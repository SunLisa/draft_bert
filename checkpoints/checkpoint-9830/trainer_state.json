{
  "best_metric": 3.661724090576172,
  "best_model_checkpoint": "./checkpoints\\checkpoint-9830",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 9830,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00508646998982706,
      "grad_norm": 1.8620541095733643,
      "learning_rate": 4.999491353001017e-05,
      "loss": 5.5501,
      "step": 10
    },
    {
      "epoch": 0.01017293997965412,
      "grad_norm": 1.6996562480926514,
      "learning_rate": 4.998982706002035e-05,
      "loss": 5.4904,
      "step": 20
    },
    {
      "epoch": 0.015259409969481181,
      "grad_norm": 1.4883601665496826,
      "learning_rate": 4.998474059003052e-05,
      "loss": 5.4464,
      "step": 30
    },
    {
      "epoch": 0.02034587995930824,
      "grad_norm": 1.4940239191055298,
      "learning_rate": 4.997965412004069e-05,
      "loss": 5.4135,
      "step": 40
    },
    {
      "epoch": 0.0254323499491353,
      "grad_norm": 1.167168140411377,
      "learning_rate": 4.9974567650050865e-05,
      "loss": 5.3982,
      "step": 50
    },
    {
      "epoch": 0.030518819938962362,
      "grad_norm": 1.5297801494598389,
      "learning_rate": 4.996948118006104e-05,
      "loss": 5.3627,
      "step": 60
    },
    {
      "epoch": 0.03560528992878942,
      "grad_norm": 1.281337022781372,
      "learning_rate": 4.996439471007121e-05,
      "loss": 5.3323,
      "step": 70
    },
    {
      "epoch": 0.04069175991861648,
      "grad_norm": 1.1677004098892212,
      "learning_rate": 4.995930824008139e-05,
      "loss": 5.3231,
      "step": 80
    },
    {
      "epoch": 0.04577822990844354,
      "grad_norm": 1.1952483654022217,
      "learning_rate": 4.995422177009156e-05,
      "loss": 5.2891,
      "step": 90
    },
    {
      "epoch": 0.0508646998982706,
      "grad_norm": 1.2573658227920532,
      "learning_rate": 4.994913530010173e-05,
      "loss": 5.2795,
      "step": 100
    },
    {
      "epoch": 0.05595116988809766,
      "grad_norm": 1.1157411336898804,
      "learning_rate": 4.9944048830111905e-05,
      "loss": 5.267,
      "step": 110
    },
    {
      "epoch": 0.061037639877924724,
      "grad_norm": 1.198099970817566,
      "learning_rate": 4.9938962360122075e-05,
      "loss": 5.2233,
      "step": 120
    },
    {
      "epoch": 0.06612410986775177,
      "grad_norm": 1.4037412405014038,
      "learning_rate": 4.9933875890132245e-05,
      "loss": 5.2106,
      "step": 130
    },
    {
      "epoch": 0.07121057985757884,
      "grad_norm": 1.3378289937973022,
      "learning_rate": 4.992878942014242e-05,
      "loss": 5.1885,
      "step": 140
    },
    {
      "epoch": 0.0762970498474059,
      "grad_norm": 1.568644404411316,
      "learning_rate": 4.99237029501526e-05,
      "loss": 5.1683,
      "step": 150
    },
    {
      "epoch": 0.08138351983723296,
      "grad_norm": 1.299017310142517,
      "learning_rate": 4.9918616480162775e-05,
      "loss": 5.156,
      "step": 160
    },
    {
      "epoch": 0.08646998982706001,
      "grad_norm": 1.2730480432510376,
      "learning_rate": 4.9913530010172945e-05,
      "loss": 5.097,
      "step": 170
    },
    {
      "epoch": 0.09155645981688708,
      "grad_norm": 1.2603338956832886,
      "learning_rate": 4.9908443540183115e-05,
      "loss": 5.0891,
      "step": 180
    },
    {
      "epoch": 0.09664292980671414,
      "grad_norm": 1.0943132638931274,
      "learning_rate": 4.990335707019329e-05,
      "loss": 5.0726,
      "step": 190
    },
    {
      "epoch": 0.1017293997965412,
      "grad_norm": 1.2997273206710815,
      "learning_rate": 4.989827060020346e-05,
      "loss": 5.0575,
      "step": 200
    },
    {
      "epoch": 0.10681586978636826,
      "grad_norm": 1.1497079133987427,
      "learning_rate": 4.989318413021363e-05,
      "loss": 5.0453,
      "step": 210
    },
    {
      "epoch": 0.11190233977619532,
      "grad_norm": 1.0320709943771362,
      "learning_rate": 4.988809766022381e-05,
      "loss": 5.0447,
      "step": 220
    },
    {
      "epoch": 0.11698880976602238,
      "grad_norm": 1.5202912092208862,
      "learning_rate": 4.988301119023398e-05,
      "loss": 4.9879,
      "step": 230
    },
    {
      "epoch": 0.12207527975584945,
      "grad_norm": 1.2593244314193726,
      "learning_rate": 4.9877924720244154e-05,
      "loss": 4.9822,
      "step": 240
    },
    {
      "epoch": 0.1271617497456765,
      "grad_norm": 0.9961100220680237,
      "learning_rate": 4.987283825025433e-05,
      "loss": 4.9659,
      "step": 250
    },
    {
      "epoch": 0.13224821973550355,
      "grad_norm": 1.2668153047561646,
      "learning_rate": 4.98677517802645e-05,
      "loss": 4.9602,
      "step": 260
    },
    {
      "epoch": 0.1373346897253306,
      "grad_norm": 1.0524022579193115,
      "learning_rate": 4.986266531027467e-05,
      "loss": 4.952,
      "step": 270
    },
    {
      "epoch": 0.14242115971515767,
      "grad_norm": 1.0568088293075562,
      "learning_rate": 4.985757884028485e-05,
      "loss": 4.887,
      "step": 280
    },
    {
      "epoch": 0.14750762970498474,
      "grad_norm": 1.2231923341751099,
      "learning_rate": 4.985249237029502e-05,
      "loss": 4.909,
      "step": 290
    },
    {
      "epoch": 0.1525940996948118,
      "grad_norm": 1.086974024772644,
      "learning_rate": 4.984740590030519e-05,
      "loss": 4.891,
      "step": 300
    },
    {
      "epoch": 0.15768056968463887,
      "grad_norm": 1.3009490966796875,
      "learning_rate": 4.9842319430315364e-05,
      "loss": 4.8309,
      "step": 310
    },
    {
      "epoch": 0.16276703967446593,
      "grad_norm": 1.2265081405639648,
      "learning_rate": 4.9837232960325534e-05,
      "loss": 4.865,
      "step": 320
    },
    {
      "epoch": 0.167853509664293,
      "grad_norm": 0.9750216603279114,
      "learning_rate": 4.983214649033571e-05,
      "loss": 4.8418,
      "step": 330
    },
    {
      "epoch": 0.17293997965412003,
      "grad_norm": 1.1377900838851929,
      "learning_rate": 4.982706002034588e-05,
      "loss": 4.7897,
      "step": 340
    },
    {
      "epoch": 0.1780264496439471,
      "grad_norm": 1.2639412879943848,
      "learning_rate": 4.982197355035606e-05,
      "loss": 4.7862,
      "step": 350
    },
    {
      "epoch": 0.18311291963377416,
      "grad_norm": 1.2019010782241821,
      "learning_rate": 4.981688708036623e-05,
      "loss": 4.7572,
      "step": 360
    },
    {
      "epoch": 0.18819938962360122,
      "grad_norm": 1.1009970903396606,
      "learning_rate": 4.9811800610376404e-05,
      "loss": 4.7336,
      "step": 370
    },
    {
      "epoch": 0.19328585961342828,
      "grad_norm": 1.0368852615356445,
      "learning_rate": 4.9806714140386574e-05,
      "loss": 4.7757,
      "step": 380
    },
    {
      "epoch": 0.19837232960325535,
      "grad_norm": 1.0208802223205566,
      "learning_rate": 4.9801627670396743e-05,
      "loss": 4.7595,
      "step": 390
    },
    {
      "epoch": 0.2034587995930824,
      "grad_norm": 1.1146231889724731,
      "learning_rate": 4.979654120040692e-05,
      "loss": 4.6913,
      "step": 400
    },
    {
      "epoch": 0.20854526958290945,
      "grad_norm": 1.0009827613830566,
      "learning_rate": 4.979145473041709e-05,
      "loss": 4.6887,
      "step": 410
    },
    {
      "epoch": 0.2136317395727365,
      "grad_norm": 1.2371834516525269,
      "learning_rate": 4.978636826042727e-05,
      "loss": 4.6956,
      "step": 420
    },
    {
      "epoch": 0.21871820956256358,
      "grad_norm": 1.3951903581619263,
      "learning_rate": 4.9781281790437437e-05,
      "loss": 4.6473,
      "step": 430
    },
    {
      "epoch": 0.22380467955239064,
      "grad_norm": 0.9142779111862183,
      "learning_rate": 4.977619532044761e-05,
      "loss": 4.6879,
      "step": 440
    },
    {
      "epoch": 0.2288911495422177,
      "grad_norm": 1.055975079536438,
      "learning_rate": 4.977110885045779e-05,
      "loss": 4.6322,
      "step": 450
    },
    {
      "epoch": 0.23397761953204477,
      "grad_norm": 1.063486933708191,
      "learning_rate": 4.976602238046796e-05,
      "loss": 4.5838,
      "step": 460
    },
    {
      "epoch": 0.23906408952187183,
      "grad_norm": 1.26181161403656,
      "learning_rate": 4.976093591047813e-05,
      "loss": 4.5956,
      "step": 470
    },
    {
      "epoch": 0.2441505595116989,
      "grad_norm": 1.114978313446045,
      "learning_rate": 4.9755849440488306e-05,
      "loss": 4.6266,
      "step": 480
    },
    {
      "epoch": 0.24923702950152593,
      "grad_norm": 1.1034096479415894,
      "learning_rate": 4.9750762970498476e-05,
      "loss": 4.5799,
      "step": 490
    },
    {
      "epoch": 0.254323499491353,
      "grad_norm": 0.9284713268280029,
      "learning_rate": 4.9745676500508646e-05,
      "loss": 4.5832,
      "step": 500
    },
    {
      "epoch": 0.25940996948118006,
      "grad_norm": 1.045884370803833,
      "learning_rate": 4.974059003051882e-05,
      "loss": 4.5394,
      "step": 510
    },
    {
      "epoch": 0.2644964394710071,
      "grad_norm": 1.037023901939392,
      "learning_rate": 4.973550356052899e-05,
      "loss": 4.593,
      "step": 520
    },
    {
      "epoch": 0.2695829094608342,
      "grad_norm": 0.9548038840293884,
      "learning_rate": 4.973041709053917e-05,
      "loss": 4.5065,
      "step": 530
    },
    {
      "epoch": 0.2746693794506612,
      "grad_norm": 0.9394946694374084,
      "learning_rate": 4.9725330620549346e-05,
      "loss": 4.575,
      "step": 540
    },
    {
      "epoch": 0.2797558494404883,
      "grad_norm": 1.1619436740875244,
      "learning_rate": 4.9720244150559516e-05,
      "loss": 4.4672,
      "step": 550
    },
    {
      "epoch": 0.28484231943031535,
      "grad_norm": 1.001769781112671,
      "learning_rate": 4.9715157680569686e-05,
      "loss": 4.5327,
      "step": 560
    },
    {
      "epoch": 0.28992878942014244,
      "grad_norm": 1.232015609741211,
      "learning_rate": 4.971007121057986e-05,
      "loss": 4.5427,
      "step": 570
    },
    {
      "epoch": 0.2950152594099695,
      "grad_norm": 0.9298741221427917,
      "learning_rate": 4.970498474059003e-05,
      "loss": 4.5001,
      "step": 580
    },
    {
      "epoch": 0.30010172939979657,
      "grad_norm": 0.8731594681739807,
      "learning_rate": 4.96998982706002e-05,
      "loss": 4.4653,
      "step": 590
    },
    {
      "epoch": 0.3051881993896236,
      "grad_norm": 0.8989977836608887,
      "learning_rate": 4.969481180061038e-05,
      "loss": 4.457,
      "step": 600
    },
    {
      "epoch": 0.31027466937945064,
      "grad_norm": 1.0350463390350342,
      "learning_rate": 4.968972533062055e-05,
      "loss": 4.4715,
      "step": 610
    },
    {
      "epoch": 0.31536113936927773,
      "grad_norm": 1.2084124088287354,
      "learning_rate": 4.9684638860630726e-05,
      "loss": 4.5121,
      "step": 620
    },
    {
      "epoch": 0.32044760935910477,
      "grad_norm": 1.0157567262649536,
      "learning_rate": 4.96795523906409e-05,
      "loss": 4.4614,
      "step": 630
    },
    {
      "epoch": 0.32553407934893186,
      "grad_norm": 0.9925084710121155,
      "learning_rate": 4.967446592065107e-05,
      "loss": 4.3935,
      "step": 640
    },
    {
      "epoch": 0.3306205493387589,
      "grad_norm": 1.1453547477722168,
      "learning_rate": 4.966937945066124e-05,
      "loss": 4.51,
      "step": 650
    },
    {
      "epoch": 0.335707019328586,
      "grad_norm": 1.079935908317566,
      "learning_rate": 4.966429298067142e-05,
      "loss": 4.4414,
      "step": 660
    },
    {
      "epoch": 0.340793489318413,
      "grad_norm": 1.014600157737732,
      "learning_rate": 4.965920651068159e-05,
      "loss": 4.4151,
      "step": 670
    },
    {
      "epoch": 0.34587995930824006,
      "grad_norm": 0.9856159687042236,
      "learning_rate": 4.9654120040691765e-05,
      "loss": 4.3928,
      "step": 680
    },
    {
      "epoch": 0.35096642929806715,
      "grad_norm": 0.9867738485336304,
      "learning_rate": 4.9649033570701935e-05,
      "loss": 4.365,
      "step": 690
    },
    {
      "epoch": 0.3560528992878942,
      "grad_norm": 1.2926799058914185,
      "learning_rate": 4.9643947100712105e-05,
      "loss": 4.3946,
      "step": 700
    },
    {
      "epoch": 0.3611393692777213,
      "grad_norm": 1.19403874874115,
      "learning_rate": 4.963886063072228e-05,
      "loss": 4.4638,
      "step": 710
    },
    {
      "epoch": 0.3662258392675483,
      "grad_norm": 1.2185667753219604,
      "learning_rate": 4.963377416073245e-05,
      "loss": 4.3881,
      "step": 720
    },
    {
      "epoch": 0.3713123092573754,
      "grad_norm": 1.1601144075393677,
      "learning_rate": 4.962868769074263e-05,
      "loss": 4.4004,
      "step": 730
    },
    {
      "epoch": 0.37639877924720244,
      "grad_norm": 1.0725337266921997,
      "learning_rate": 4.9623601220752805e-05,
      "loss": 4.3193,
      "step": 740
    },
    {
      "epoch": 0.3814852492370295,
      "grad_norm": 0.8986203670501709,
      "learning_rate": 4.9618514750762975e-05,
      "loss": 4.3496,
      "step": 750
    },
    {
      "epoch": 0.38657171922685657,
      "grad_norm": 1.0403578281402588,
      "learning_rate": 4.9613428280773145e-05,
      "loss": 4.3318,
      "step": 760
    },
    {
      "epoch": 0.3916581892166836,
      "grad_norm": 0.9969229698181152,
      "learning_rate": 4.960834181078332e-05,
      "loss": 4.4305,
      "step": 770
    },
    {
      "epoch": 0.3967446592065107,
      "grad_norm": 1.0671964883804321,
      "learning_rate": 4.960325534079349e-05,
      "loss": 4.3708,
      "step": 780
    },
    {
      "epoch": 0.40183112919633773,
      "grad_norm": 0.81570965051651,
      "learning_rate": 4.959816887080366e-05,
      "loss": 4.3945,
      "step": 790
    },
    {
      "epoch": 0.4069175991861648,
      "grad_norm": 0.9248974323272705,
      "learning_rate": 4.959308240081384e-05,
      "loss": 4.3491,
      "step": 800
    },
    {
      "epoch": 0.41200406917599186,
      "grad_norm": 1.1376482248306274,
      "learning_rate": 4.958799593082401e-05,
      "loss": 4.3475,
      "step": 810
    },
    {
      "epoch": 0.4170905391658189,
      "grad_norm": 1.1099718809127808,
      "learning_rate": 4.9582909460834184e-05,
      "loss": 4.3141,
      "step": 820
    },
    {
      "epoch": 0.422177009155646,
      "grad_norm": 1.182560920715332,
      "learning_rate": 4.957782299084436e-05,
      "loss": 4.2648,
      "step": 830
    },
    {
      "epoch": 0.427263479145473,
      "grad_norm": 1.4488362073898315,
      "learning_rate": 4.957273652085453e-05,
      "loss": 4.293,
      "step": 840
    },
    {
      "epoch": 0.4323499491353001,
      "grad_norm": 1.1328459978103638,
      "learning_rate": 4.95676500508647e-05,
      "loss": 4.3389,
      "step": 850
    },
    {
      "epoch": 0.43743641912512715,
      "grad_norm": 1.3857777118682861,
      "learning_rate": 4.956256358087488e-05,
      "loss": 4.336,
      "step": 860
    },
    {
      "epoch": 0.44252288911495424,
      "grad_norm": 1.0977919101715088,
      "learning_rate": 4.955747711088505e-05,
      "loss": 4.2598,
      "step": 870
    },
    {
      "epoch": 0.4476093591047813,
      "grad_norm": 1.1394001245498657,
      "learning_rate": 4.955239064089522e-05,
      "loss": 4.2904,
      "step": 880
    },
    {
      "epoch": 0.4526958290946083,
      "grad_norm": 1.243005394935608,
      "learning_rate": 4.9547304170905394e-05,
      "loss": 4.3366,
      "step": 890
    },
    {
      "epoch": 0.4577822990844354,
      "grad_norm": 0.9567817449569702,
      "learning_rate": 4.9542217700915564e-05,
      "loss": 4.263,
      "step": 900
    },
    {
      "epoch": 0.46286876907426244,
      "grad_norm": 1.205608606338501,
      "learning_rate": 4.953713123092574e-05,
      "loss": 4.2712,
      "step": 910
    },
    {
      "epoch": 0.46795523906408953,
      "grad_norm": 1.319305181503296,
      "learning_rate": 4.953204476093592e-05,
      "loss": 4.2366,
      "step": 920
    },
    {
      "epoch": 0.47304170905391657,
      "grad_norm": 1.4482612609863281,
      "learning_rate": 4.952695829094609e-05,
      "loss": 4.222,
      "step": 930
    },
    {
      "epoch": 0.47812817904374366,
      "grad_norm": 1.2469608783721924,
      "learning_rate": 4.952187182095626e-05,
      "loss": 4.2576,
      "step": 940
    },
    {
      "epoch": 0.4832146490335707,
      "grad_norm": 1.030442476272583,
      "learning_rate": 4.9516785350966434e-05,
      "loss": 4.2806,
      "step": 950
    },
    {
      "epoch": 0.4883011190233978,
      "grad_norm": 1.0235463380813599,
      "learning_rate": 4.9511698880976604e-05,
      "loss": 4.2612,
      "step": 960
    },
    {
      "epoch": 0.4933875890132248,
      "grad_norm": 1.7610021829605103,
      "learning_rate": 4.950661241098678e-05,
      "loss": 4.245,
      "step": 970
    },
    {
      "epoch": 0.49847405900305186,
      "grad_norm": 0.9532315135002136,
      "learning_rate": 4.950152594099695e-05,
      "loss": 4.2459,
      "step": 980
    },
    {
      "epoch": 0.503560528992879,
      "grad_norm": 1.209675669670105,
      "learning_rate": 4.949643947100712e-05,
      "loss": 4.1973,
      "step": 990
    },
    {
      "epoch": 0.508646998982706,
      "grad_norm": 1.302587628364563,
      "learning_rate": 4.94913530010173e-05,
      "loss": 4.2045,
      "step": 1000
    },
    {
      "epoch": 0.513733468972533,
      "grad_norm": 1.2250295877456665,
      "learning_rate": 4.9486266531027467e-05,
      "loss": 4.2151,
      "step": 1010
    },
    {
      "epoch": 0.5188199389623601,
      "grad_norm": 1.3869332075119019,
      "learning_rate": 4.948118006103764e-05,
      "loss": 4.2411,
      "step": 1020
    },
    {
      "epoch": 0.5239064089521872,
      "grad_norm": 1.1010462045669556,
      "learning_rate": 4.947609359104782e-05,
      "loss": 4.2137,
      "step": 1030
    },
    {
      "epoch": 0.5289928789420142,
      "grad_norm": 1.1491479873657227,
      "learning_rate": 4.947100712105799e-05,
      "loss": 4.1989,
      "step": 1040
    },
    {
      "epoch": 0.5340793489318413,
      "grad_norm": 1.1577527523040771,
      "learning_rate": 4.946592065106816e-05,
      "loss": 4.1901,
      "step": 1050
    },
    {
      "epoch": 0.5391658189216684,
      "grad_norm": 1.1797277927398682,
      "learning_rate": 4.9460834181078336e-05,
      "loss": 4.2162,
      "step": 1060
    },
    {
      "epoch": 0.5442522889114955,
      "grad_norm": 1.1767712831497192,
      "learning_rate": 4.9455747711088506e-05,
      "loss": 4.1767,
      "step": 1070
    },
    {
      "epoch": 0.5493387589013224,
      "grad_norm": 1.3309577703475952,
      "learning_rate": 4.9450661241098676e-05,
      "loss": 4.2057,
      "step": 1080
    },
    {
      "epoch": 0.5544252288911495,
      "grad_norm": 1.1619929075241089,
      "learning_rate": 4.944557477110885e-05,
      "loss": 4.1858,
      "step": 1090
    },
    {
      "epoch": 0.5595116988809766,
      "grad_norm": 1.109459638595581,
      "learning_rate": 4.944048830111902e-05,
      "loss": 4.2067,
      "step": 1100
    },
    {
      "epoch": 0.5645981688708036,
      "grad_norm": 1.047372579574585,
      "learning_rate": 4.94354018311292e-05,
      "loss": 4.137,
      "step": 1110
    },
    {
      "epoch": 0.5696846388606307,
      "grad_norm": 1.406684398651123,
      "learning_rate": 4.9430315361139376e-05,
      "loss": 4.1788,
      "step": 1120
    },
    {
      "epoch": 0.5747711088504578,
      "grad_norm": 1.426543951034546,
      "learning_rate": 4.9425228891149546e-05,
      "loss": 4.1054,
      "step": 1130
    },
    {
      "epoch": 0.5798575788402849,
      "grad_norm": 1.2268894910812378,
      "learning_rate": 4.9420142421159716e-05,
      "loss": 4.1661,
      "step": 1140
    },
    {
      "epoch": 0.5849440488301119,
      "grad_norm": 1.3415323495864868,
      "learning_rate": 4.941505595116989e-05,
      "loss": 4.2086,
      "step": 1150
    },
    {
      "epoch": 0.590030518819939,
      "grad_norm": 1.4937567710876465,
      "learning_rate": 4.940996948118006e-05,
      "loss": 4.1673,
      "step": 1160
    },
    {
      "epoch": 0.595116988809766,
      "grad_norm": 1.4333189725875854,
      "learning_rate": 4.940488301119023e-05,
      "loss": 4.1634,
      "step": 1170
    },
    {
      "epoch": 0.6002034587995931,
      "grad_norm": 1.1926484107971191,
      "learning_rate": 4.939979654120041e-05,
      "loss": 4.1754,
      "step": 1180
    },
    {
      "epoch": 0.6052899287894201,
      "grad_norm": 1.205100178718567,
      "learning_rate": 4.939471007121058e-05,
      "loss": 4.1628,
      "step": 1190
    },
    {
      "epoch": 0.6103763987792472,
      "grad_norm": 1.8868303298950195,
      "learning_rate": 4.9389623601220756e-05,
      "loss": 4.1402,
      "step": 1200
    },
    {
      "epoch": 0.6154628687690743,
      "grad_norm": 1.4437249898910522,
      "learning_rate": 4.938453713123093e-05,
      "loss": 4.2068,
      "step": 1210
    },
    {
      "epoch": 0.6205493387589013,
      "grad_norm": 1.2796233892440796,
      "learning_rate": 4.93794506612411e-05,
      "loss": 4.2437,
      "step": 1220
    },
    {
      "epoch": 0.6256358087487284,
      "grad_norm": 1.2472829818725586,
      "learning_rate": 4.937436419125128e-05,
      "loss": 4.1613,
      "step": 1230
    },
    {
      "epoch": 0.6307222787385555,
      "grad_norm": 1.3386597633361816,
      "learning_rate": 4.936927772126145e-05,
      "loss": 4.2188,
      "step": 1240
    },
    {
      "epoch": 0.6358087487283826,
      "grad_norm": 1.3510419130325317,
      "learning_rate": 4.936419125127162e-05,
      "loss": 4.2129,
      "step": 1250
    },
    {
      "epoch": 0.6408952187182095,
      "grad_norm": 1.7777752876281738,
      "learning_rate": 4.9359104781281795e-05,
      "loss": 4.1499,
      "step": 1260
    },
    {
      "epoch": 0.6459816887080366,
      "grad_norm": 1.20978581905365,
      "learning_rate": 4.9354018311291965e-05,
      "loss": 4.1134,
      "step": 1270
    },
    {
      "epoch": 0.6510681586978637,
      "grad_norm": 1.3169583082199097,
      "learning_rate": 4.9348931841302135e-05,
      "loss": 4.207,
      "step": 1280
    },
    {
      "epoch": 0.6561546286876907,
      "grad_norm": 0.9704428911209106,
      "learning_rate": 4.934384537131231e-05,
      "loss": 4.1063,
      "step": 1290
    },
    {
      "epoch": 0.6612410986775178,
      "grad_norm": 1.3254092931747437,
      "learning_rate": 4.933875890132248e-05,
      "loss": 4.2037,
      "step": 1300
    },
    {
      "epoch": 0.6663275686673449,
      "grad_norm": 1.7731354236602783,
      "learning_rate": 4.933367243133266e-05,
      "loss": 4.1308,
      "step": 1310
    },
    {
      "epoch": 0.671414038657172,
      "grad_norm": 1.3588954210281372,
      "learning_rate": 4.9328585961342835e-05,
      "loss": 4.1326,
      "step": 1320
    },
    {
      "epoch": 0.676500508646999,
      "grad_norm": 1.3305162191390991,
      "learning_rate": 4.9323499491353005e-05,
      "loss": 4.1789,
      "step": 1330
    },
    {
      "epoch": 0.681586978636826,
      "grad_norm": 1.146964430809021,
      "learning_rate": 4.9318413021363175e-05,
      "loss": 4.1495,
      "step": 1340
    },
    {
      "epoch": 0.6866734486266531,
      "grad_norm": 1.1409286260604858,
      "learning_rate": 4.931332655137335e-05,
      "loss": 4.1293,
      "step": 1350
    },
    {
      "epoch": 0.6917599186164801,
      "grad_norm": 1.3366755247116089,
      "learning_rate": 4.930824008138352e-05,
      "loss": 4.2419,
      "step": 1360
    },
    {
      "epoch": 0.6968463886063072,
      "grad_norm": 1.4700900316238403,
      "learning_rate": 4.930315361139369e-05,
      "loss": 4.101,
      "step": 1370
    },
    {
      "epoch": 0.7019328585961343,
      "grad_norm": 1.2512423992156982,
      "learning_rate": 4.929806714140387e-05,
      "loss": 4.0918,
      "step": 1380
    },
    {
      "epoch": 0.7070193285859614,
      "grad_norm": 1.7508658170700073,
      "learning_rate": 4.929298067141404e-05,
      "loss": 4.1454,
      "step": 1390
    },
    {
      "epoch": 0.7121057985757884,
      "grad_norm": 1.0262482166290283,
      "learning_rate": 4.9287894201424214e-05,
      "loss": 4.0968,
      "step": 1400
    },
    {
      "epoch": 0.7171922685656155,
      "grad_norm": 1.9106734991073608,
      "learning_rate": 4.928280773143439e-05,
      "loss": 4.0764,
      "step": 1410
    },
    {
      "epoch": 0.7222787385554426,
      "grad_norm": 1.514243483543396,
      "learning_rate": 4.927772126144456e-05,
      "loss": 4.124,
      "step": 1420
    },
    {
      "epoch": 0.7273652085452695,
      "grad_norm": 1.5909725427627563,
      "learning_rate": 4.927263479145473e-05,
      "loss": 4.1942,
      "step": 1430
    },
    {
      "epoch": 0.7324516785350966,
      "grad_norm": 1.5087953805923462,
      "learning_rate": 4.926754832146491e-05,
      "loss": 4.1029,
      "step": 1440
    },
    {
      "epoch": 0.7375381485249237,
      "grad_norm": 1.303483009338379,
      "learning_rate": 4.926246185147508e-05,
      "loss": 4.0675,
      "step": 1450
    },
    {
      "epoch": 0.7426246185147508,
      "grad_norm": 1.4629945755004883,
      "learning_rate": 4.925737538148525e-05,
      "loss": 4.1283,
      "step": 1460
    },
    {
      "epoch": 0.7477110885045778,
      "grad_norm": 1.9691299200057983,
      "learning_rate": 4.9252288911495424e-05,
      "loss": 4.0744,
      "step": 1470
    },
    {
      "epoch": 0.7527975584944049,
      "grad_norm": 1.4583765268325806,
      "learning_rate": 4.9247202441505594e-05,
      "loss": 4.0836,
      "step": 1480
    },
    {
      "epoch": 0.757884028484232,
      "grad_norm": 1.8962498903274536,
      "learning_rate": 4.924211597151577e-05,
      "loss": 4.0617,
      "step": 1490
    },
    {
      "epoch": 0.762970498474059,
      "grad_norm": 1.1788734197616577,
      "learning_rate": 4.923702950152595e-05,
      "loss": 4.1733,
      "step": 1500
    },
    {
      "epoch": 0.768056968463886,
      "grad_norm": 1.4367045164108276,
      "learning_rate": 4.923194303153612e-05,
      "loss": 4.1613,
      "step": 1510
    },
    {
      "epoch": 0.7731434384537131,
      "grad_norm": 1.4754124879837036,
      "learning_rate": 4.9226856561546294e-05,
      "loss": 4.0269,
      "step": 1520
    },
    {
      "epoch": 0.7782299084435402,
      "grad_norm": 1.3124659061431885,
      "learning_rate": 4.9221770091556464e-05,
      "loss": 4.114,
      "step": 1530
    },
    {
      "epoch": 0.7833163784333672,
      "grad_norm": 1.8541167974472046,
      "learning_rate": 4.9216683621566634e-05,
      "loss": 4.1116,
      "step": 1540
    },
    {
      "epoch": 0.7884028484231943,
      "grad_norm": 1.3308602571487427,
      "learning_rate": 4.921159715157681e-05,
      "loss": 4.034,
      "step": 1550
    },
    {
      "epoch": 0.7934893184130214,
      "grad_norm": 1.438094973564148,
      "learning_rate": 4.920651068158698e-05,
      "loss": 4.0614,
      "step": 1560
    },
    {
      "epoch": 0.7985757884028484,
      "grad_norm": 1.3125865459442139,
      "learning_rate": 4.920142421159715e-05,
      "loss": 4.02,
      "step": 1570
    },
    {
      "epoch": 0.8036622583926755,
      "grad_norm": 1.5567948818206787,
      "learning_rate": 4.919633774160733e-05,
      "loss": 4.0785,
      "step": 1580
    },
    {
      "epoch": 0.8087487283825026,
      "grad_norm": 1.516724944114685,
      "learning_rate": 4.91912512716175e-05,
      "loss": 4.1005,
      "step": 1590
    },
    {
      "epoch": 0.8138351983723296,
      "grad_norm": 1.7937555313110352,
      "learning_rate": 4.918616480162767e-05,
      "loss": 4.0661,
      "step": 1600
    },
    {
      "epoch": 0.8189216683621566,
      "grad_norm": 1.5719882249832153,
      "learning_rate": 4.918107833163785e-05,
      "loss": 4.1163,
      "step": 1610
    },
    {
      "epoch": 0.8240081383519837,
      "grad_norm": 1.4146252870559692,
      "learning_rate": 4.917599186164802e-05,
      "loss": 4.0523,
      "step": 1620
    },
    {
      "epoch": 0.8290946083418108,
      "grad_norm": 1.4866914749145508,
      "learning_rate": 4.917090539165819e-05,
      "loss": 4.1138,
      "step": 1630
    },
    {
      "epoch": 0.8341810783316378,
      "grad_norm": 1.7472399473190308,
      "learning_rate": 4.9165818921668366e-05,
      "loss": 4.1069,
      "step": 1640
    },
    {
      "epoch": 0.8392675483214649,
      "grad_norm": 1.3671644926071167,
      "learning_rate": 4.9160732451678536e-05,
      "loss": 4.0771,
      "step": 1650
    },
    {
      "epoch": 0.844354018311292,
      "grad_norm": 1.6440402269363403,
      "learning_rate": 4.9155645981688706e-05,
      "loss": 4.1624,
      "step": 1660
    },
    {
      "epoch": 0.8494404883011191,
      "grad_norm": 1.6369292736053467,
      "learning_rate": 4.915055951169888e-05,
      "loss": 4.0863,
      "step": 1670
    },
    {
      "epoch": 0.854526958290946,
      "grad_norm": 1.1553452014923096,
      "learning_rate": 4.914547304170905e-05,
      "loss": 4.0794,
      "step": 1680
    },
    {
      "epoch": 0.8596134282807731,
      "grad_norm": 1.8268574476242065,
      "learning_rate": 4.914038657171923e-05,
      "loss": 4.0721,
      "step": 1690
    },
    {
      "epoch": 0.8646998982706002,
      "grad_norm": 1.668223261833191,
      "learning_rate": 4.9135300101729406e-05,
      "loss": 4.0295,
      "step": 1700
    },
    {
      "epoch": 0.8697863682604272,
      "grad_norm": 1.6191165447235107,
      "learning_rate": 4.9130213631739576e-05,
      "loss": 4.0596,
      "step": 1710
    },
    {
      "epoch": 0.8748728382502543,
      "grad_norm": 1.6551761627197266,
      "learning_rate": 4.9125127161749746e-05,
      "loss": 4.0029,
      "step": 1720
    },
    {
      "epoch": 0.8799593082400814,
      "grad_norm": 2.0020599365234375,
      "learning_rate": 4.912004069175992e-05,
      "loss": 3.9949,
      "step": 1730
    },
    {
      "epoch": 0.8850457782299085,
      "grad_norm": 1.9126626253128052,
      "learning_rate": 4.911495422177009e-05,
      "loss": 4.076,
      "step": 1740
    },
    {
      "epoch": 0.8901322482197355,
      "grad_norm": 1.9451290369033813,
      "learning_rate": 4.910986775178026e-05,
      "loss": 4.0856,
      "step": 1750
    },
    {
      "epoch": 0.8952187182095626,
      "grad_norm": 1.8924111127853394,
      "learning_rate": 4.910478128179044e-05,
      "loss": 4.1022,
      "step": 1760
    },
    {
      "epoch": 0.9003051881993896,
      "grad_norm": 1.7400778532028198,
      "learning_rate": 4.909969481180061e-05,
      "loss": 4.0749,
      "step": 1770
    },
    {
      "epoch": 0.9053916581892166,
      "grad_norm": 1.6534168720245361,
      "learning_rate": 4.9094608341810786e-05,
      "loss": 4.0948,
      "step": 1780
    },
    {
      "epoch": 0.9104781281790437,
      "grad_norm": 1.277776837348938,
      "learning_rate": 4.908952187182096e-05,
      "loss": 4.0708,
      "step": 1790
    },
    {
      "epoch": 0.9155645981688708,
      "grad_norm": 1.594847559928894,
      "learning_rate": 4.908443540183113e-05,
      "loss": 4.0642,
      "step": 1800
    },
    {
      "epoch": 0.9206510681586979,
      "grad_norm": 1.8652358055114746,
      "learning_rate": 4.907934893184131e-05,
      "loss": 4.005,
      "step": 1810
    },
    {
      "epoch": 0.9257375381485249,
      "grad_norm": 1.9682148694992065,
      "learning_rate": 4.907426246185148e-05,
      "loss": 4.0495,
      "step": 1820
    },
    {
      "epoch": 0.930824008138352,
      "grad_norm": 1.3807790279388428,
      "learning_rate": 4.906917599186165e-05,
      "loss": 4.032,
      "step": 1830
    },
    {
      "epoch": 0.9359104781281791,
      "grad_norm": 1.3387295007705688,
      "learning_rate": 4.9064089521871825e-05,
      "loss": 4.1224,
      "step": 1840
    },
    {
      "epoch": 0.940996948118006,
      "grad_norm": 2.138181686401367,
      "learning_rate": 4.9059003051881995e-05,
      "loss": 4.0615,
      "step": 1850
    },
    {
      "epoch": 0.9460834181078331,
      "grad_norm": 1.384210467338562,
      "learning_rate": 4.9053916581892165e-05,
      "loss": 4.0101,
      "step": 1860
    },
    {
      "epoch": 0.9511698880976602,
      "grad_norm": 1.8288390636444092,
      "learning_rate": 4.904883011190234e-05,
      "loss": 4.0019,
      "step": 1870
    },
    {
      "epoch": 0.9562563580874873,
      "grad_norm": 2.013861894607544,
      "learning_rate": 4.904374364191252e-05,
      "loss": 3.9878,
      "step": 1880
    },
    {
      "epoch": 0.9613428280773143,
      "grad_norm": 1.9177788496017456,
      "learning_rate": 4.903865717192269e-05,
      "loss": 3.9973,
      "step": 1890
    },
    {
      "epoch": 0.9664292980671414,
      "grad_norm": 1.5360263586044312,
      "learning_rate": 4.9033570701932865e-05,
      "loss": 4.1192,
      "step": 1900
    },
    {
      "epoch": 0.9715157680569685,
      "grad_norm": 2.2354724407196045,
      "learning_rate": 4.9028484231943035e-05,
      "loss": 4.0219,
      "step": 1910
    },
    {
      "epoch": 0.9766022380467956,
      "grad_norm": 1.7626396417617798,
      "learning_rate": 4.9023397761953205e-05,
      "loss": 3.9792,
      "step": 1920
    },
    {
      "epoch": 0.9816887080366226,
      "grad_norm": 1.419806718826294,
      "learning_rate": 4.901831129196338e-05,
      "loss": 4.0552,
      "step": 1930
    },
    {
      "epoch": 0.9867751780264497,
      "grad_norm": 2.033162832260132,
      "learning_rate": 4.901322482197355e-05,
      "loss": 3.9787,
      "step": 1940
    },
    {
      "epoch": 0.9918616480162767,
      "grad_norm": 1.4936879873275757,
      "learning_rate": 4.900813835198372e-05,
      "loss": 4.0087,
      "step": 1950
    },
    {
      "epoch": 0.9969481180061037,
      "grad_norm": 2.586871385574341,
      "learning_rate": 4.90030518819939e-05,
      "loss": 4.0202,
      "step": 1960
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.987011432647705,
      "eval_runtime": 2.6538,
      "eval_samples_per_second": 1045.666,
      "eval_steps_per_second": 130.755,
      "step": 1966
    },
    {
      "epoch": 1.002034587995931,
      "grad_norm": 1.7257380485534668,
      "learning_rate": 4.899796541200407e-05,
      "loss": 3.9627,
      "step": 1970
    },
    {
      "epoch": 1.007121057985758,
      "grad_norm": 1.7950177192687988,
      "learning_rate": 4.8992878942014244e-05,
      "loss": 4.0469,
      "step": 1980
    },
    {
      "epoch": 1.0122075279755849,
      "grad_norm": 1.6862667798995972,
      "learning_rate": 4.898779247202442e-05,
      "loss": 4.0318,
      "step": 1990
    },
    {
      "epoch": 1.017293997965412,
      "grad_norm": 1.936666488647461,
      "learning_rate": 4.898270600203459e-05,
      "loss": 4.0466,
      "step": 2000
    },
    {
      "epoch": 1.022380467955239,
      "grad_norm": 1.9143457412719727,
      "learning_rate": 4.897761953204476e-05,
      "loss": 4.0069,
      "step": 2010
    },
    {
      "epoch": 1.027466937945066,
      "grad_norm": 1.6920356750488281,
      "learning_rate": 4.897253306205494e-05,
      "loss": 4.0069,
      "step": 2020
    },
    {
      "epoch": 1.0325534079348933,
      "grad_norm": 1.9220759868621826,
      "learning_rate": 4.896744659206511e-05,
      "loss": 3.9474,
      "step": 2030
    },
    {
      "epoch": 1.0376398779247202,
      "grad_norm": 1.7476314306259155,
      "learning_rate": 4.8962360122075284e-05,
      "loss": 4.0032,
      "step": 2040
    },
    {
      "epoch": 1.0427263479145472,
      "grad_norm": 2.980959177017212,
      "learning_rate": 4.8957273652085454e-05,
      "loss": 4.0237,
      "step": 2050
    },
    {
      "epoch": 1.0478128179043744,
      "grad_norm": 1.7879271507263184,
      "learning_rate": 4.8952187182095624e-05,
      "loss": 4.0446,
      "step": 2060
    },
    {
      "epoch": 1.0528992878942014,
      "grad_norm": 2.1408605575561523,
      "learning_rate": 4.89471007121058e-05,
      "loss": 4.0334,
      "step": 2070
    },
    {
      "epoch": 1.0579857578840284,
      "grad_norm": 1.8115652799606323,
      "learning_rate": 4.894201424211598e-05,
      "loss": 4.0494,
      "step": 2080
    },
    {
      "epoch": 1.0630722278738556,
      "grad_norm": 1.5957456827163696,
      "learning_rate": 4.893692777212615e-05,
      "loss": 4.0583,
      "step": 2090
    },
    {
      "epoch": 1.0681586978636826,
      "grad_norm": 1.920944333076477,
      "learning_rate": 4.8931841302136324e-05,
      "loss": 4.0091,
      "step": 2100
    },
    {
      "epoch": 1.0732451678535098,
      "grad_norm": 1.4710344076156616,
      "learning_rate": 4.8926754832146494e-05,
      "loss": 4.0048,
      "step": 2110
    },
    {
      "epoch": 1.0783316378433367,
      "grad_norm": 2.189177989959717,
      "learning_rate": 4.8921668362156664e-05,
      "loss": 3.9725,
      "step": 2120
    },
    {
      "epoch": 1.0834181078331637,
      "grad_norm": 2.994528293609619,
      "learning_rate": 4.891658189216684e-05,
      "loss": 3.9856,
      "step": 2130
    },
    {
      "epoch": 1.088504577822991,
      "grad_norm": 2.0357632637023926,
      "learning_rate": 4.891149542217701e-05,
      "loss": 4.0452,
      "step": 2140
    },
    {
      "epoch": 1.093591047812818,
      "grad_norm": 1.451012372970581,
      "learning_rate": 4.890640895218718e-05,
      "loss": 3.9638,
      "step": 2150
    },
    {
      "epoch": 1.0986775178026449,
      "grad_norm": 1.7088543176651,
      "learning_rate": 4.890132248219736e-05,
      "loss": 3.922,
      "step": 2160
    },
    {
      "epoch": 1.103763987792472,
      "grad_norm": 1.6088171005249023,
      "learning_rate": 4.889623601220753e-05,
      "loss": 4.0466,
      "step": 2170
    },
    {
      "epoch": 1.108850457782299,
      "grad_norm": 2.0513412952423096,
      "learning_rate": 4.88911495422177e-05,
      "loss": 3.9645,
      "step": 2180
    },
    {
      "epoch": 1.113936927772126,
      "grad_norm": 2.1487507820129395,
      "learning_rate": 4.888606307222788e-05,
      "loss": 3.9821,
      "step": 2190
    },
    {
      "epoch": 1.1190233977619533,
      "grad_norm": 2.049086570739746,
      "learning_rate": 4.888097660223805e-05,
      "loss": 3.972,
      "step": 2200
    },
    {
      "epoch": 1.1241098677517802,
      "grad_norm": 1.9366912841796875,
      "learning_rate": 4.887589013224822e-05,
      "loss": 4.0042,
      "step": 2210
    },
    {
      "epoch": 1.1291963377416074,
      "grad_norm": 1.9115527868270874,
      "learning_rate": 4.8870803662258396e-05,
      "loss": 3.8792,
      "step": 2220
    },
    {
      "epoch": 1.1342828077314344,
      "grad_norm": 2.470325469970703,
      "learning_rate": 4.8865717192268566e-05,
      "loss": 3.9574,
      "step": 2230
    },
    {
      "epoch": 1.1393692777212614,
      "grad_norm": 2.3042659759521484,
      "learning_rate": 4.8860630722278736e-05,
      "loss": 4.0026,
      "step": 2240
    },
    {
      "epoch": 1.1444557477110886,
      "grad_norm": 2.3065719604492188,
      "learning_rate": 4.885554425228891e-05,
      "loss": 4.0114,
      "step": 2250
    },
    {
      "epoch": 1.1495422177009156,
      "grad_norm": 2.1007080078125,
      "learning_rate": 4.885045778229908e-05,
      "loss": 4.0336,
      "step": 2260
    },
    {
      "epoch": 1.1546286876907426,
      "grad_norm": 2.091237783432007,
      "learning_rate": 4.884537131230926e-05,
      "loss": 3.9483,
      "step": 2270
    },
    {
      "epoch": 1.1597151576805698,
      "grad_norm": 2.5165157318115234,
      "learning_rate": 4.8840284842319436e-05,
      "loss": 3.9733,
      "step": 2280
    },
    {
      "epoch": 1.1648016276703967,
      "grad_norm": 1.9170767068862915,
      "learning_rate": 4.8835198372329606e-05,
      "loss": 3.9042,
      "step": 2290
    },
    {
      "epoch": 1.1698880976602237,
      "grad_norm": 4.967627048492432,
      "learning_rate": 4.883011190233978e-05,
      "loss": 4.0384,
      "step": 2300
    },
    {
      "epoch": 1.174974567650051,
      "grad_norm": 2.1323320865631104,
      "learning_rate": 4.882502543234995e-05,
      "loss": 3.9462,
      "step": 2310
    },
    {
      "epoch": 1.180061037639878,
      "grad_norm": 1.661591649055481,
      "learning_rate": 4.881993896236012e-05,
      "loss": 3.9326,
      "step": 2320
    },
    {
      "epoch": 1.1851475076297049,
      "grad_norm": 1.5081074237823486,
      "learning_rate": 4.88148524923703e-05,
      "loss": 3.961,
      "step": 2330
    },
    {
      "epoch": 1.190233977619532,
      "grad_norm": 1.7913029193878174,
      "learning_rate": 4.880976602238047e-05,
      "loss": 3.9422,
      "step": 2340
    },
    {
      "epoch": 1.195320447609359,
      "grad_norm": 3.078305721282959,
      "learning_rate": 4.880467955239064e-05,
      "loss": 3.9303,
      "step": 2350
    },
    {
      "epoch": 1.200406917599186,
      "grad_norm": 1.8169665336608887,
      "learning_rate": 4.8799593082400816e-05,
      "loss": 3.9232,
      "step": 2360
    },
    {
      "epoch": 1.2054933875890133,
      "grad_norm": 2.2445638179779053,
      "learning_rate": 4.879450661241099e-05,
      "loss": 4.0543,
      "step": 2370
    },
    {
      "epoch": 1.2105798575788402,
      "grad_norm": 2.0995900630950928,
      "learning_rate": 4.878942014242116e-05,
      "loss": 3.9662,
      "step": 2380
    },
    {
      "epoch": 1.2156663275686674,
      "grad_norm": 2.412574052810669,
      "learning_rate": 4.878433367243134e-05,
      "loss": 3.9576,
      "step": 2390
    },
    {
      "epoch": 1.2207527975584944,
      "grad_norm": 2.9330434799194336,
      "learning_rate": 4.877924720244151e-05,
      "loss": 3.964,
      "step": 2400
    },
    {
      "epoch": 1.2258392675483214,
      "grad_norm": 2.936581611633301,
      "learning_rate": 4.877416073245168e-05,
      "loss": 3.8388,
      "step": 2410
    },
    {
      "epoch": 1.2309257375381486,
      "grad_norm": 3.362020254135132,
      "learning_rate": 4.8769074262461855e-05,
      "loss": 3.956,
      "step": 2420
    },
    {
      "epoch": 1.2360122075279756,
      "grad_norm": 2.030841588973999,
      "learning_rate": 4.8763987792472025e-05,
      "loss": 3.891,
      "step": 2430
    },
    {
      "epoch": 1.2410986775178026,
      "grad_norm": 2.020311117172241,
      "learning_rate": 4.8758901322482195e-05,
      "loss": 3.8952,
      "step": 2440
    },
    {
      "epoch": 1.2461851475076298,
      "grad_norm": 1.9764037132263184,
      "learning_rate": 4.875381485249237e-05,
      "loss": 3.9623,
      "step": 2450
    },
    {
      "epoch": 1.2512716174974567,
      "grad_norm": 1.9649181365966797,
      "learning_rate": 4.874872838250255e-05,
      "loss": 3.9677,
      "step": 2460
    },
    {
      "epoch": 1.2563580874872837,
      "grad_norm": 2.5071680545806885,
      "learning_rate": 4.874364191251272e-05,
      "loss": 4.0276,
      "step": 2470
    },
    {
      "epoch": 1.261444557477111,
      "grad_norm": 2.068830966949463,
      "learning_rate": 4.8738555442522895e-05,
      "loss": 3.9646,
      "step": 2480
    },
    {
      "epoch": 1.266531027466938,
      "grad_norm": 2.0060958862304688,
      "learning_rate": 4.8733468972533065e-05,
      "loss": 3.9964,
      "step": 2490
    },
    {
      "epoch": 1.2716174974567651,
      "grad_norm": 1.7277652025222778,
      "learning_rate": 4.8728382502543235e-05,
      "loss": 3.9813,
      "step": 2500
    },
    {
      "epoch": 1.276703967446592,
      "grad_norm": 2.3274965286254883,
      "learning_rate": 4.872329603255341e-05,
      "loss": 3.9548,
      "step": 2510
    },
    {
      "epoch": 1.281790437436419,
      "grad_norm": 2.2281136512756348,
      "learning_rate": 4.871820956256358e-05,
      "loss": 3.9165,
      "step": 2520
    },
    {
      "epoch": 1.286876907426246,
      "grad_norm": 2.5854625701904297,
      "learning_rate": 4.871312309257375e-05,
      "loss": 4.0116,
      "step": 2530
    },
    {
      "epoch": 1.2919633774160733,
      "grad_norm": 2.83828067779541,
      "learning_rate": 4.870803662258393e-05,
      "loss": 3.9706,
      "step": 2540
    },
    {
      "epoch": 1.2970498474059002,
      "grad_norm": 2.1678407192230225,
      "learning_rate": 4.8702950152594105e-05,
      "loss": 3.9299,
      "step": 2550
    },
    {
      "epoch": 1.3021363173957274,
      "grad_norm": 1.9562280178070068,
      "learning_rate": 4.8697863682604274e-05,
      "loss": 3.8692,
      "step": 2560
    },
    {
      "epoch": 1.3072227873855544,
      "grad_norm": 2.868701457977295,
      "learning_rate": 4.869277721261445e-05,
      "loss": 3.9135,
      "step": 2570
    },
    {
      "epoch": 1.3123092573753814,
      "grad_norm": 2.8124570846557617,
      "learning_rate": 4.868769074262462e-05,
      "loss": 3.9744,
      "step": 2580
    },
    {
      "epoch": 1.3173957273652086,
      "grad_norm": 2.3565664291381836,
      "learning_rate": 4.86826042726348e-05,
      "loss": 3.8869,
      "step": 2590
    },
    {
      "epoch": 1.3224821973550356,
      "grad_norm": 2.8008124828338623,
      "learning_rate": 4.867751780264497e-05,
      "loss": 3.9142,
      "step": 2600
    },
    {
      "epoch": 1.3275686673448628,
      "grad_norm": 2.322633743286133,
      "learning_rate": 4.867243133265514e-05,
      "loss": 3.9832,
      "step": 2610
    },
    {
      "epoch": 1.3326551373346898,
      "grad_norm": 3.3832225799560547,
      "learning_rate": 4.8667344862665314e-05,
      "loss": 3.9179,
      "step": 2620
    },
    {
      "epoch": 1.3377416073245167,
      "grad_norm": 2.094360113143921,
      "learning_rate": 4.8662258392675484e-05,
      "loss": 4.0058,
      "step": 2630
    },
    {
      "epoch": 1.3428280773143437,
      "grad_norm": 2.2931244373321533,
      "learning_rate": 4.8657171922685654e-05,
      "loss": 3.9254,
      "step": 2640
    },
    {
      "epoch": 1.347914547304171,
      "grad_norm": 2.352853775024414,
      "learning_rate": 4.865208545269583e-05,
      "loss": 3.9809,
      "step": 2650
    },
    {
      "epoch": 1.353001017293998,
      "grad_norm": 2.848780870437622,
      "learning_rate": 4.864699898270601e-05,
      "loss": 3.9095,
      "step": 2660
    },
    {
      "epoch": 1.3580874872838251,
      "grad_norm": 2.9247162342071533,
      "learning_rate": 4.864191251271618e-05,
      "loss": 3.9776,
      "step": 2670
    },
    {
      "epoch": 1.363173957273652,
      "grad_norm": 2.6121084690093994,
      "learning_rate": 4.8636826042726354e-05,
      "loss": 3.8975,
      "step": 2680
    },
    {
      "epoch": 1.368260427263479,
      "grad_norm": 2.5569818019866943,
      "learning_rate": 4.8631739572736524e-05,
      "loss": 3.9456,
      "step": 2690
    },
    {
      "epoch": 1.3733468972533063,
      "grad_norm": 2.369204044342041,
      "learning_rate": 4.8626653102746694e-05,
      "loss": 3.8224,
      "step": 2700
    },
    {
      "epoch": 1.3784333672431333,
      "grad_norm": 2.3101742267608643,
      "learning_rate": 4.862156663275687e-05,
      "loss": 3.9542,
      "step": 2710
    },
    {
      "epoch": 1.3835198372329605,
      "grad_norm": 3.3624606132507324,
      "learning_rate": 4.861648016276704e-05,
      "loss": 3.913,
      "step": 2720
    },
    {
      "epoch": 1.3886063072227874,
      "grad_norm": 3.4250571727752686,
      "learning_rate": 4.861139369277721e-05,
      "loss": 3.956,
      "step": 2730
    },
    {
      "epoch": 1.3936927772126144,
      "grad_norm": 2.3920998573303223,
      "learning_rate": 4.860630722278739e-05,
      "loss": 3.8821,
      "step": 2740
    },
    {
      "epoch": 1.3987792472024414,
      "grad_norm": 3.124845504760742,
      "learning_rate": 4.860122075279756e-05,
      "loss": 3.9093,
      "step": 2750
    },
    {
      "epoch": 1.4038657171922686,
      "grad_norm": 2.3216707706451416,
      "learning_rate": 4.859613428280773e-05,
      "loss": 3.9168,
      "step": 2760
    },
    {
      "epoch": 1.4089521871820956,
      "grad_norm": 2.610485076904297,
      "learning_rate": 4.859104781281791e-05,
      "loss": 3.9388,
      "step": 2770
    },
    {
      "epoch": 1.4140386571719228,
      "grad_norm": 2.4689815044403076,
      "learning_rate": 4.858596134282808e-05,
      "loss": 3.9255,
      "step": 2780
    },
    {
      "epoch": 1.4191251271617498,
      "grad_norm": 2.6871395111083984,
      "learning_rate": 4.858087487283825e-05,
      "loss": 3.8903,
      "step": 2790
    },
    {
      "epoch": 1.4242115971515767,
      "grad_norm": 3.648493766784668,
      "learning_rate": 4.8575788402848426e-05,
      "loss": 3.9589,
      "step": 2800
    },
    {
      "epoch": 1.4292980671414037,
      "grad_norm": 2.795381784439087,
      "learning_rate": 4.8570701932858596e-05,
      "loss": 3.8458,
      "step": 2810
    },
    {
      "epoch": 1.434384537131231,
      "grad_norm": 2.672447681427002,
      "learning_rate": 4.8565615462868766e-05,
      "loss": 3.8601,
      "step": 2820
    },
    {
      "epoch": 1.439471007121058,
      "grad_norm": 2.0746052265167236,
      "learning_rate": 4.856052899287894e-05,
      "loss": 3.9126,
      "step": 2830
    },
    {
      "epoch": 1.4445574771108851,
      "grad_norm": 2.3404409885406494,
      "learning_rate": 4.855544252288912e-05,
      "loss": 3.8788,
      "step": 2840
    },
    {
      "epoch": 1.449643947100712,
      "grad_norm": 2.185244083404541,
      "learning_rate": 4.8550356052899296e-05,
      "loss": 3.922,
      "step": 2850
    },
    {
      "epoch": 1.454730417090539,
      "grad_norm": 1.6734259128570557,
      "learning_rate": 4.8545269582909466e-05,
      "loss": 3.981,
      "step": 2860
    },
    {
      "epoch": 1.4598168870803663,
      "grad_norm": 4.593475341796875,
      "learning_rate": 4.8540183112919636e-05,
      "loss": 3.8676,
      "step": 2870
    },
    {
      "epoch": 1.4649033570701933,
      "grad_norm": 1.9578518867492676,
      "learning_rate": 4.853509664292981e-05,
      "loss": 3.8711,
      "step": 2880
    },
    {
      "epoch": 1.4699898270600205,
      "grad_norm": 2.0782599449157715,
      "learning_rate": 4.853001017293998e-05,
      "loss": 3.9,
      "step": 2890
    },
    {
      "epoch": 1.4750762970498474,
      "grad_norm": 2.2813661098480225,
      "learning_rate": 4.852492370295015e-05,
      "loss": 3.8609,
      "step": 2900
    },
    {
      "epoch": 1.4801627670396744,
      "grad_norm": 2.7098612785339355,
      "learning_rate": 4.851983723296033e-05,
      "loss": 3.8749,
      "step": 2910
    },
    {
      "epoch": 1.4852492370295014,
      "grad_norm": 2.0379207134246826,
      "learning_rate": 4.85147507629705e-05,
      "loss": 3.8275,
      "step": 2920
    },
    {
      "epoch": 1.4903357070193286,
      "grad_norm": 2.3257503509521484,
      "learning_rate": 4.850966429298067e-05,
      "loss": 3.9086,
      "step": 2930
    },
    {
      "epoch": 1.4954221770091556,
      "grad_norm": 2.545694351196289,
      "learning_rate": 4.8504577822990846e-05,
      "loss": 3.9452,
      "step": 2940
    },
    {
      "epoch": 1.5005086469989828,
      "grad_norm": 2.1215734481811523,
      "learning_rate": 4.849949135300102e-05,
      "loss": 3.8586,
      "step": 2950
    },
    {
      "epoch": 1.5055951169888098,
      "grad_norm": 2.360004186630249,
      "learning_rate": 4.849440488301119e-05,
      "loss": 3.8503,
      "step": 2960
    },
    {
      "epoch": 1.5106815869786367,
      "grad_norm": 2.5942797660827637,
      "learning_rate": 4.848931841302137e-05,
      "loss": 3.9301,
      "step": 2970
    },
    {
      "epoch": 1.5157680569684637,
      "grad_norm": 3.23411226272583,
      "learning_rate": 4.848423194303154e-05,
      "loss": 3.8558,
      "step": 2980
    },
    {
      "epoch": 1.520854526958291,
      "grad_norm": 1.8477108478546143,
      "learning_rate": 4.847914547304171e-05,
      "loss": 3.9348,
      "step": 2990
    },
    {
      "epoch": 1.5259409969481181,
      "grad_norm": 2.799643039703369,
      "learning_rate": 4.8474059003051885e-05,
      "loss": 3.8646,
      "step": 3000
    },
    {
      "epoch": 1.5310274669379451,
      "grad_norm": 2.920898199081421,
      "learning_rate": 4.8468972533062055e-05,
      "loss": 3.8698,
      "step": 3010
    },
    {
      "epoch": 1.536113936927772,
      "grad_norm": 2.8635525703430176,
      "learning_rate": 4.8463886063072225e-05,
      "loss": 3.8772,
      "step": 3020
    },
    {
      "epoch": 1.541200406917599,
      "grad_norm": 2.574653387069702,
      "learning_rate": 4.84587995930824e-05,
      "loss": 3.9236,
      "step": 3030
    },
    {
      "epoch": 1.5462868769074263,
      "grad_norm": 2.6386682987213135,
      "learning_rate": 4.845371312309258e-05,
      "loss": 3.8992,
      "step": 3040
    },
    {
      "epoch": 1.5513733468972533,
      "grad_norm": 2.2035412788391113,
      "learning_rate": 4.844862665310275e-05,
      "loss": 3.9024,
      "step": 3050
    },
    {
      "epoch": 1.5564598168870805,
      "grad_norm": 3.2510619163513184,
      "learning_rate": 4.8443540183112925e-05,
      "loss": 3.874,
      "step": 3060
    },
    {
      "epoch": 1.5615462868769074,
      "grad_norm": 2.594196081161499,
      "learning_rate": 4.8438453713123095e-05,
      "loss": 3.8774,
      "step": 3070
    },
    {
      "epoch": 1.5666327568667344,
      "grad_norm": 2.3913445472717285,
      "learning_rate": 4.8433367243133265e-05,
      "loss": 3.942,
      "step": 3080
    },
    {
      "epoch": 1.5717192268565614,
      "grad_norm": 2.5080785751342773,
      "learning_rate": 4.842828077314344e-05,
      "loss": 3.8608,
      "step": 3090
    },
    {
      "epoch": 1.5768056968463886,
      "grad_norm": 3.553894281387329,
      "learning_rate": 4.842319430315361e-05,
      "loss": 3.8844,
      "step": 3100
    },
    {
      "epoch": 1.5818921668362158,
      "grad_norm": 2.492377996444702,
      "learning_rate": 4.841810783316379e-05,
      "loss": 3.8377,
      "step": 3110
    },
    {
      "epoch": 1.5869786368260428,
      "grad_norm": 3.0864450931549072,
      "learning_rate": 4.841302136317396e-05,
      "loss": 3.8689,
      "step": 3120
    },
    {
      "epoch": 1.5920651068158698,
      "grad_norm": 2.9161736965179443,
      "learning_rate": 4.8407934893184135e-05,
      "loss": 3.8766,
      "step": 3130
    },
    {
      "epoch": 1.5971515768056967,
      "grad_norm": 3.6836953163146973,
      "learning_rate": 4.840284842319431e-05,
      "loss": 3.8788,
      "step": 3140
    },
    {
      "epoch": 1.602238046795524,
      "grad_norm": 1.6159393787384033,
      "learning_rate": 4.839776195320448e-05,
      "loss": 3.9041,
      "step": 3150
    },
    {
      "epoch": 1.607324516785351,
      "grad_norm": 2.0169739723205566,
      "learning_rate": 4.839267548321465e-05,
      "loss": 3.9405,
      "step": 3160
    },
    {
      "epoch": 1.6124109867751781,
      "grad_norm": 2.210364818572998,
      "learning_rate": 4.838758901322483e-05,
      "loss": 3.8437,
      "step": 3170
    },
    {
      "epoch": 1.6174974567650051,
      "grad_norm": 2.516774892807007,
      "learning_rate": 4.8382502543235e-05,
      "loss": 3.8978,
      "step": 3180
    },
    {
      "epoch": 1.622583926754832,
      "grad_norm": 2.3684165477752686,
      "learning_rate": 4.837741607324517e-05,
      "loss": 3.7832,
      "step": 3190
    },
    {
      "epoch": 1.627670396744659,
      "grad_norm": 2.970245122909546,
      "learning_rate": 4.8372329603255344e-05,
      "loss": 3.8783,
      "step": 3200
    },
    {
      "epoch": 1.6327568667344863,
      "grad_norm": 2.5396718978881836,
      "learning_rate": 4.8367243133265514e-05,
      "loss": 3.8753,
      "step": 3210
    },
    {
      "epoch": 1.6378433367243135,
      "grad_norm": 2.4317190647125244,
      "learning_rate": 4.836215666327569e-05,
      "loss": 3.8845,
      "step": 3220
    },
    {
      "epoch": 1.6429298067141405,
      "grad_norm": 2.0661675930023193,
      "learning_rate": 4.835707019328586e-05,
      "loss": 3.9072,
      "step": 3230
    },
    {
      "epoch": 1.6480162767039674,
      "grad_norm": 3.3051681518554688,
      "learning_rate": 4.835198372329604e-05,
      "loss": 3.856,
      "step": 3240
    },
    {
      "epoch": 1.6531027466937944,
      "grad_norm": 2.246716022491455,
      "learning_rate": 4.834689725330621e-05,
      "loss": 3.881,
      "step": 3250
    },
    {
      "epoch": 1.6581892166836214,
      "grad_norm": 3.5119080543518066,
      "learning_rate": 4.8341810783316384e-05,
      "loss": 3.8539,
      "step": 3260
    },
    {
      "epoch": 1.6632756866734486,
      "grad_norm": 3.4491398334503174,
      "learning_rate": 4.8336724313326554e-05,
      "loss": 3.8632,
      "step": 3270
    },
    {
      "epoch": 1.6683621566632758,
      "grad_norm": 2.0863068103790283,
      "learning_rate": 4.8331637843336724e-05,
      "loss": 3.8179,
      "step": 3280
    },
    {
      "epoch": 1.6734486266531028,
      "grad_norm": 3.18312931060791,
      "learning_rate": 4.83265513733469e-05,
      "loss": 3.8362,
      "step": 3290
    },
    {
      "epoch": 1.6785350966429298,
      "grad_norm": 3.3505122661590576,
      "learning_rate": 4.832146490335707e-05,
      "loss": 3.8541,
      "step": 3300
    },
    {
      "epoch": 1.6836215666327567,
      "grad_norm": 2.358457326889038,
      "learning_rate": 4.831637843336724e-05,
      "loss": 3.9114,
      "step": 3310
    },
    {
      "epoch": 1.688708036622584,
      "grad_norm": 2.8709821701049805,
      "learning_rate": 4.831129196337742e-05,
      "loss": 3.8215,
      "step": 3320
    },
    {
      "epoch": 1.693794506612411,
      "grad_norm": 3.471684694290161,
      "learning_rate": 4.830620549338759e-05,
      "loss": 3.8808,
      "step": 3330
    },
    {
      "epoch": 1.6988809766022381,
      "grad_norm": 2.349759817123413,
      "learning_rate": 4.830111902339776e-05,
      "loss": 3.8793,
      "step": 3340
    },
    {
      "epoch": 1.7039674465920651,
      "grad_norm": 3.098581075668335,
      "learning_rate": 4.829603255340794e-05,
      "loss": 3.9247,
      "step": 3350
    },
    {
      "epoch": 1.709053916581892,
      "grad_norm": 3.3885202407836914,
      "learning_rate": 4.829094608341811e-05,
      "loss": 3.8473,
      "step": 3360
    },
    {
      "epoch": 1.714140386571719,
      "grad_norm": 2.7713000774383545,
      "learning_rate": 4.828585961342828e-05,
      "loss": 3.8542,
      "step": 3370
    },
    {
      "epoch": 1.7192268565615463,
      "grad_norm": 2.0381813049316406,
      "learning_rate": 4.8280773143438456e-05,
      "loss": 3.8307,
      "step": 3380
    },
    {
      "epoch": 1.7243133265513735,
      "grad_norm": 2.5406320095062256,
      "learning_rate": 4.8275686673448626e-05,
      "loss": 3.8529,
      "step": 3390
    },
    {
      "epoch": 1.7293997965412005,
      "grad_norm": 4.186591625213623,
      "learning_rate": 4.82706002034588e-05,
      "loss": 3.8466,
      "step": 3400
    },
    {
      "epoch": 1.7344862665310274,
      "grad_norm": 2.4478073120117188,
      "learning_rate": 4.826551373346897e-05,
      "loss": 3.8878,
      "step": 3410
    },
    {
      "epoch": 1.7395727365208544,
      "grad_norm": 3.5367841720581055,
      "learning_rate": 4.826042726347915e-05,
      "loss": 3.9177,
      "step": 3420
    },
    {
      "epoch": 1.7446592065106816,
      "grad_norm": 2.907881498336792,
      "learning_rate": 4.8255340793489326e-05,
      "loss": 3.8506,
      "step": 3430
    },
    {
      "epoch": 1.7497456765005086,
      "grad_norm": 3.356942892074585,
      "learning_rate": 4.8250254323499496e-05,
      "loss": 3.8961,
      "step": 3440
    },
    {
      "epoch": 1.7548321464903358,
      "grad_norm": 2.443355083465576,
      "learning_rate": 4.8245167853509666e-05,
      "loss": 3.8786,
      "step": 3450
    },
    {
      "epoch": 1.7599186164801628,
      "grad_norm": 2.973762035369873,
      "learning_rate": 4.824008138351984e-05,
      "loss": 3.8106,
      "step": 3460
    },
    {
      "epoch": 1.7650050864699898,
      "grad_norm": 2.508446455001831,
      "learning_rate": 4.823499491353001e-05,
      "loss": 3.8803,
      "step": 3470
    },
    {
      "epoch": 1.7700915564598168,
      "grad_norm": 2.3010764122009277,
      "learning_rate": 4.822990844354018e-05,
      "loss": 3.8068,
      "step": 3480
    },
    {
      "epoch": 1.775178026449644,
      "grad_norm": 2.415297269821167,
      "learning_rate": 4.822482197355036e-05,
      "loss": 3.82,
      "step": 3490
    },
    {
      "epoch": 1.7802644964394712,
      "grad_norm": 3.997767448425293,
      "learning_rate": 4.821973550356053e-05,
      "loss": 3.8387,
      "step": 3500
    },
    {
      "epoch": 1.7853509664292981,
      "grad_norm": 3.0259130001068115,
      "learning_rate": 4.8214649033570706e-05,
      "loss": 3.7925,
      "step": 3510
    },
    {
      "epoch": 1.7904374364191251,
      "grad_norm": 2.2652039527893066,
      "learning_rate": 4.820956256358088e-05,
      "loss": 3.8723,
      "step": 3520
    },
    {
      "epoch": 1.795523906408952,
      "grad_norm": 2.7250819206237793,
      "learning_rate": 4.820447609359105e-05,
      "loss": 3.7803,
      "step": 3530
    },
    {
      "epoch": 1.8006103763987793,
      "grad_norm": 3.3924481868743896,
      "learning_rate": 4.819938962360122e-05,
      "loss": 3.8226,
      "step": 3540
    },
    {
      "epoch": 1.8056968463886063,
      "grad_norm": 2.620333433151245,
      "learning_rate": 4.81943031536114e-05,
      "loss": 3.7568,
      "step": 3550
    },
    {
      "epoch": 1.8107833163784335,
      "grad_norm": 2.371976852416992,
      "learning_rate": 4.818921668362157e-05,
      "loss": 3.8788,
      "step": 3560
    },
    {
      "epoch": 1.8158697863682605,
      "grad_norm": 2.5696659088134766,
      "learning_rate": 4.818413021363174e-05,
      "loss": 3.8142,
      "step": 3570
    },
    {
      "epoch": 1.8209562563580874,
      "grad_norm": 2.553067684173584,
      "learning_rate": 4.8179043743641915e-05,
      "loss": 3.7704,
      "step": 3580
    },
    {
      "epoch": 1.8260427263479144,
      "grad_norm": 3.435072183609009,
      "learning_rate": 4.8173957273652085e-05,
      "loss": 3.7701,
      "step": 3590
    },
    {
      "epoch": 1.8311291963377416,
      "grad_norm": 3.1979541778564453,
      "learning_rate": 4.8168870803662255e-05,
      "loss": 3.8175,
      "step": 3600
    },
    {
      "epoch": 1.8362156663275688,
      "grad_norm": 2.2606077194213867,
      "learning_rate": 4.816378433367243e-05,
      "loss": 3.8208,
      "step": 3610
    },
    {
      "epoch": 1.8413021363173958,
      "grad_norm": 2.1618783473968506,
      "learning_rate": 4.815869786368261e-05,
      "loss": 3.8127,
      "step": 3620
    },
    {
      "epoch": 1.8463886063072228,
      "grad_norm": 2.9220941066741943,
      "learning_rate": 4.815361139369278e-05,
      "loss": 3.8216,
      "step": 3630
    },
    {
      "epoch": 1.8514750762970498,
      "grad_norm": 2.887899160385132,
      "learning_rate": 4.8148524923702955e-05,
      "loss": 3.853,
      "step": 3640
    },
    {
      "epoch": 1.8565615462868768,
      "grad_norm": 3.247743606567383,
      "learning_rate": 4.8143438453713125e-05,
      "loss": 3.764,
      "step": 3650
    },
    {
      "epoch": 1.861648016276704,
      "grad_norm": 2.6104135513305664,
      "learning_rate": 4.81383519837233e-05,
      "loss": 3.7908,
      "step": 3660
    },
    {
      "epoch": 1.8667344862665312,
      "grad_norm": 3.5066661834716797,
      "learning_rate": 4.813326551373347e-05,
      "loss": 3.8373,
      "step": 3670
    },
    {
      "epoch": 1.8718209562563581,
      "grad_norm": 3.195223331451416,
      "learning_rate": 4.812817904374364e-05,
      "loss": 3.8645,
      "step": 3680
    },
    {
      "epoch": 1.8769074262461851,
      "grad_norm": 2.857661008834839,
      "learning_rate": 4.812309257375382e-05,
      "loss": 3.8556,
      "step": 3690
    },
    {
      "epoch": 1.881993896236012,
      "grad_norm": 2.8105034828186035,
      "learning_rate": 4.811800610376399e-05,
      "loss": 3.8007,
      "step": 3700
    },
    {
      "epoch": 1.8870803662258393,
      "grad_norm": 3.4952337741851807,
      "learning_rate": 4.8112919633774165e-05,
      "loss": 3.7928,
      "step": 3710
    },
    {
      "epoch": 1.8921668362156663,
      "grad_norm": 3.942455768585205,
      "learning_rate": 4.810783316378434e-05,
      "loss": 3.9054,
      "step": 3720
    },
    {
      "epoch": 1.8972533062054935,
      "grad_norm": 2.816209554672241,
      "learning_rate": 4.810274669379451e-05,
      "loss": 3.7837,
      "step": 3730
    },
    {
      "epoch": 1.9023397761953205,
      "grad_norm": 3.0272772312164307,
      "learning_rate": 4.809766022380468e-05,
      "loss": 3.8001,
      "step": 3740
    },
    {
      "epoch": 1.9074262461851474,
      "grad_norm": 3.9427623748779297,
      "learning_rate": 4.809257375381486e-05,
      "loss": 3.8422,
      "step": 3750
    },
    {
      "epoch": 1.9125127161749744,
      "grad_norm": 3.2254321575164795,
      "learning_rate": 4.808748728382503e-05,
      "loss": 3.8652,
      "step": 3760
    },
    {
      "epoch": 1.9175991861648016,
      "grad_norm": 2.9322938919067383,
      "learning_rate": 4.80824008138352e-05,
      "loss": 3.7948,
      "step": 3770
    },
    {
      "epoch": 1.9226856561546288,
      "grad_norm": 3.0695714950561523,
      "learning_rate": 4.8077314343845374e-05,
      "loss": 3.7794,
      "step": 3780
    },
    {
      "epoch": 1.9277721261444558,
      "grad_norm": 2.085655450820923,
      "learning_rate": 4.8072227873855544e-05,
      "loss": 3.7606,
      "step": 3790
    },
    {
      "epoch": 1.9328585961342828,
      "grad_norm": 5.870607852935791,
      "learning_rate": 4.806714140386572e-05,
      "loss": 3.7976,
      "step": 3800
    },
    {
      "epoch": 1.9379450661241098,
      "grad_norm": 3.239985704421997,
      "learning_rate": 4.80620549338759e-05,
      "loss": 3.792,
      "step": 3810
    },
    {
      "epoch": 1.943031536113937,
      "grad_norm": 2.699589490890503,
      "learning_rate": 4.805696846388607e-05,
      "loss": 3.7408,
      "step": 3820
    },
    {
      "epoch": 1.948118006103764,
      "grad_norm": 2.924009084701538,
      "learning_rate": 4.805188199389624e-05,
      "loss": 3.846,
      "step": 3830
    },
    {
      "epoch": 1.9532044760935912,
      "grad_norm": 3.431859016418457,
      "learning_rate": 4.8046795523906414e-05,
      "loss": 3.8322,
      "step": 3840
    },
    {
      "epoch": 1.9582909460834181,
      "grad_norm": 4.013307094573975,
      "learning_rate": 4.8041709053916584e-05,
      "loss": 3.7754,
      "step": 3850
    },
    {
      "epoch": 1.9633774160732451,
      "grad_norm": 3.6749062538146973,
      "learning_rate": 4.8036622583926754e-05,
      "loss": 3.9102,
      "step": 3860
    },
    {
      "epoch": 1.968463886063072,
      "grad_norm": 5.978215217590332,
      "learning_rate": 4.803153611393693e-05,
      "loss": 3.8084,
      "step": 3870
    },
    {
      "epoch": 1.9735503560528993,
      "grad_norm": 3.1038668155670166,
      "learning_rate": 4.80264496439471e-05,
      "loss": 3.8361,
      "step": 3880
    },
    {
      "epoch": 1.9786368260427265,
      "grad_norm": 4.393378257751465,
      "learning_rate": 4.802136317395727e-05,
      "loss": 3.8265,
      "step": 3890
    },
    {
      "epoch": 1.9837232960325535,
      "grad_norm": 3.2907605171203613,
      "learning_rate": 4.801627670396745e-05,
      "loss": 3.7734,
      "step": 3900
    },
    {
      "epoch": 1.9888097660223805,
      "grad_norm": 3.6339643001556396,
      "learning_rate": 4.801119023397762e-05,
      "loss": 3.8448,
      "step": 3910
    },
    {
      "epoch": 1.9938962360122074,
      "grad_norm": 4.090737819671631,
      "learning_rate": 4.80061037639878e-05,
      "loss": 3.7712,
      "step": 3920
    },
    {
      "epoch": 1.9989827060020344,
      "grad_norm": 2.9533932209014893,
      "learning_rate": 4.800101729399797e-05,
      "loss": 3.8954,
      "step": 3930
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.826873540878296,
      "eval_runtime": 2.6248,
      "eval_samples_per_second": 1057.207,
      "eval_steps_per_second": 132.198,
      "step": 3932
    },
    {
      "epoch": 2.004069175991862,
      "grad_norm": 3.4182682037353516,
      "learning_rate": 4.799593082400814e-05,
      "loss": 3.7912,
      "step": 3940
    },
    {
      "epoch": 2.009155645981689,
      "grad_norm": 3.128972053527832,
      "learning_rate": 4.7990844354018317e-05,
      "loss": 3.71,
      "step": 3950
    },
    {
      "epoch": 2.014242115971516,
      "grad_norm": 3.345815420150757,
      "learning_rate": 4.7985757884028486e-05,
      "loss": 3.8257,
      "step": 3960
    },
    {
      "epoch": 2.019328585961343,
      "grad_norm": 3.755927562713623,
      "learning_rate": 4.7980671414038656e-05,
      "loss": 3.8506,
      "step": 3970
    },
    {
      "epoch": 2.0244150559511698,
      "grad_norm": 3.6383376121520996,
      "learning_rate": 4.797558494404883e-05,
      "loss": 3.8179,
      "step": 3980
    },
    {
      "epoch": 2.0295015259409968,
      "grad_norm": 3.574094772338867,
      "learning_rate": 4.7970498474059e-05,
      "loss": 3.6932,
      "step": 3990
    },
    {
      "epoch": 2.034587995930824,
      "grad_norm": 5.699164867401123,
      "learning_rate": 4.796541200406918e-05,
      "loss": 3.7792,
      "step": 4000
    },
    {
      "epoch": 2.039674465920651,
      "grad_norm": 2.5300450325012207,
      "learning_rate": 4.7960325534079356e-05,
      "loss": 3.9256,
      "step": 4010
    },
    {
      "epoch": 2.044760935910478,
      "grad_norm": 2.5193402767181396,
      "learning_rate": 4.7955239064089526e-05,
      "loss": 3.7365,
      "step": 4020
    },
    {
      "epoch": 2.049847405900305,
      "grad_norm": 3.6600091457366943,
      "learning_rate": 4.7950152594099696e-05,
      "loss": 3.724,
      "step": 4030
    },
    {
      "epoch": 2.054933875890132,
      "grad_norm": 2.681989908218384,
      "learning_rate": 4.794506612410987e-05,
      "loss": 3.795,
      "step": 4040
    },
    {
      "epoch": 2.0600203458799595,
      "grad_norm": 3.5618276596069336,
      "learning_rate": 4.793997965412004e-05,
      "loss": 3.7844,
      "step": 4050
    },
    {
      "epoch": 2.0651068158697865,
      "grad_norm": 2.793653964996338,
      "learning_rate": 4.793489318413021e-05,
      "loss": 3.75,
      "step": 4060
    },
    {
      "epoch": 2.0701932858596135,
      "grad_norm": 2.8173346519470215,
      "learning_rate": 4.792980671414039e-05,
      "loss": 3.8715,
      "step": 4070
    },
    {
      "epoch": 2.0752797558494405,
      "grad_norm": 2.6671271324157715,
      "learning_rate": 4.792472024415056e-05,
      "loss": 3.8105,
      "step": 4080
    },
    {
      "epoch": 2.0803662258392674,
      "grad_norm": 3.1799519062042236,
      "learning_rate": 4.7919633774160736e-05,
      "loss": 3.7614,
      "step": 4090
    },
    {
      "epoch": 2.0854526958290944,
      "grad_norm": 2.8281126022338867,
      "learning_rate": 4.791454730417091e-05,
      "loss": 3.7336,
      "step": 4100
    },
    {
      "epoch": 2.090539165818922,
      "grad_norm": 3.216190814971924,
      "learning_rate": 4.790946083418108e-05,
      "loss": 3.7321,
      "step": 4110
    },
    {
      "epoch": 2.095625635808749,
      "grad_norm": 3.920994758605957,
      "learning_rate": 4.790437436419125e-05,
      "loss": 3.8055,
      "step": 4120
    },
    {
      "epoch": 2.100712105798576,
      "grad_norm": 2.9823126792907715,
      "learning_rate": 4.789928789420143e-05,
      "loss": 3.8428,
      "step": 4130
    },
    {
      "epoch": 2.105798575788403,
      "grad_norm": 3.5184895992279053,
      "learning_rate": 4.78942014242116e-05,
      "loss": 3.8003,
      "step": 4140
    },
    {
      "epoch": 2.1108850457782298,
      "grad_norm": 2.820263147354126,
      "learning_rate": 4.788911495422177e-05,
      "loss": 3.8142,
      "step": 4150
    },
    {
      "epoch": 2.1159715157680568,
      "grad_norm": 4.79014778137207,
      "learning_rate": 4.7884028484231945e-05,
      "loss": 3.7358,
      "step": 4160
    },
    {
      "epoch": 2.121057985757884,
      "grad_norm": 2.4953773021698,
      "learning_rate": 4.7878942014242115e-05,
      "loss": 3.7089,
      "step": 4170
    },
    {
      "epoch": 2.126144455747711,
      "grad_norm": 3.46655011177063,
      "learning_rate": 4.787385554425229e-05,
      "loss": 3.7416,
      "step": 4180
    },
    {
      "epoch": 2.131230925737538,
      "grad_norm": 3.9183313846588135,
      "learning_rate": 4.786876907426246e-05,
      "loss": 3.8816,
      "step": 4190
    },
    {
      "epoch": 2.136317395727365,
      "grad_norm": 3.1138110160827637,
      "learning_rate": 4.786368260427264e-05,
      "loss": 3.7934,
      "step": 4200
    },
    {
      "epoch": 2.141403865717192,
      "grad_norm": 3.0591883659362793,
      "learning_rate": 4.7858596134282815e-05,
      "loss": 3.6891,
      "step": 4210
    },
    {
      "epoch": 2.1464903357070195,
      "grad_norm": 2.8712549209594727,
      "learning_rate": 4.7853509664292985e-05,
      "loss": 3.7532,
      "step": 4220
    },
    {
      "epoch": 2.1515768056968465,
      "grad_norm": 4.400040149688721,
      "learning_rate": 4.7848423194303155e-05,
      "loss": 3.7919,
      "step": 4230
    },
    {
      "epoch": 2.1566632756866735,
      "grad_norm": 3.4683656692504883,
      "learning_rate": 4.784333672431333e-05,
      "loss": 3.7651,
      "step": 4240
    },
    {
      "epoch": 2.1617497456765005,
      "grad_norm": 4.072911262512207,
      "learning_rate": 4.78382502543235e-05,
      "loss": 3.6897,
      "step": 4250
    },
    {
      "epoch": 2.1668362156663274,
      "grad_norm": 3.559427261352539,
      "learning_rate": 4.783316378433367e-05,
      "loss": 3.8219,
      "step": 4260
    },
    {
      "epoch": 2.1719226856561544,
      "grad_norm": 2.260404109954834,
      "learning_rate": 4.782807731434385e-05,
      "loss": 3.775,
      "step": 4270
    },
    {
      "epoch": 2.177009155645982,
      "grad_norm": 3.906188488006592,
      "learning_rate": 4.782299084435402e-05,
      "loss": 3.7592,
      "step": 4280
    },
    {
      "epoch": 2.182095625635809,
      "grad_norm": 3.158396005630493,
      "learning_rate": 4.7817904374364195e-05,
      "loss": 3.7613,
      "step": 4290
    },
    {
      "epoch": 2.187182095625636,
      "grad_norm": 2.6394829750061035,
      "learning_rate": 4.781281790437437e-05,
      "loss": 3.7168,
      "step": 4300
    },
    {
      "epoch": 2.192268565615463,
      "grad_norm": 2.850581169128418,
      "learning_rate": 4.780773143438454e-05,
      "loss": 3.762,
      "step": 4310
    },
    {
      "epoch": 2.1973550356052898,
      "grad_norm": 2.5697829723358154,
      "learning_rate": 4.780264496439471e-05,
      "loss": 3.8284,
      "step": 4320
    },
    {
      "epoch": 2.202441505595117,
      "grad_norm": 3.659562587738037,
      "learning_rate": 4.779755849440489e-05,
      "loss": 3.7848,
      "step": 4330
    },
    {
      "epoch": 2.207527975584944,
      "grad_norm": 3.7429816722869873,
      "learning_rate": 4.779247202441506e-05,
      "loss": 3.874,
      "step": 4340
    },
    {
      "epoch": 2.212614445574771,
      "grad_norm": 3.0052616596221924,
      "learning_rate": 4.778738555442523e-05,
      "loss": 3.7768,
      "step": 4350
    },
    {
      "epoch": 2.217700915564598,
      "grad_norm": 3.3453288078308105,
      "learning_rate": 4.7782299084435404e-05,
      "loss": 3.8275,
      "step": 4360
    },
    {
      "epoch": 2.222787385554425,
      "grad_norm": 2.9370498657226562,
      "learning_rate": 4.7777212614445574e-05,
      "loss": 3.6486,
      "step": 4370
    },
    {
      "epoch": 2.227873855544252,
      "grad_norm": 3.7593250274658203,
      "learning_rate": 4.777212614445575e-05,
      "loss": 3.7846,
      "step": 4380
    },
    {
      "epoch": 2.2329603255340795,
      "grad_norm": 4.049661636352539,
      "learning_rate": 4.776703967446593e-05,
      "loss": 3.7725,
      "step": 4390
    },
    {
      "epoch": 2.2380467955239065,
      "grad_norm": 2.790114164352417,
      "learning_rate": 4.77619532044761e-05,
      "loss": 3.7094,
      "step": 4400
    },
    {
      "epoch": 2.2431332655137335,
      "grad_norm": 3.980050802230835,
      "learning_rate": 4.775686673448627e-05,
      "loss": 3.8624,
      "step": 4410
    },
    {
      "epoch": 2.2482197355035605,
      "grad_norm": 2.8510401248931885,
      "learning_rate": 4.7751780264496444e-05,
      "loss": 3.7021,
      "step": 4420
    },
    {
      "epoch": 2.2533062054933874,
      "grad_norm": 2.858358860015869,
      "learning_rate": 4.7746693794506614e-05,
      "loss": 3.7423,
      "step": 4430
    },
    {
      "epoch": 2.258392675483215,
      "grad_norm": 3.4576849937438965,
      "learning_rate": 4.7741607324516784e-05,
      "loss": 3.738,
      "step": 4440
    },
    {
      "epoch": 2.263479145473042,
      "grad_norm": 3.941326856613159,
      "learning_rate": 4.773652085452696e-05,
      "loss": 3.7473,
      "step": 4450
    },
    {
      "epoch": 2.268565615462869,
      "grad_norm": 3.9448628425598145,
      "learning_rate": 4.773143438453713e-05,
      "loss": 3.7758,
      "step": 4460
    },
    {
      "epoch": 2.273652085452696,
      "grad_norm": 3.5503225326538086,
      "learning_rate": 4.772634791454731e-05,
      "loss": 3.8198,
      "step": 4470
    },
    {
      "epoch": 2.278738555442523,
      "grad_norm": 3.3230912685394287,
      "learning_rate": 4.7721261444557483e-05,
      "loss": 3.7652,
      "step": 4480
    },
    {
      "epoch": 2.2838250254323498,
      "grad_norm": 3.039677381515503,
      "learning_rate": 4.7716174974567653e-05,
      "loss": 3.7527,
      "step": 4490
    },
    {
      "epoch": 2.288911495422177,
      "grad_norm": 4.640956878662109,
      "learning_rate": 4.771108850457783e-05,
      "loss": 3.7348,
      "step": 4500
    },
    {
      "epoch": 2.293997965412004,
      "grad_norm": 3.3063101768493652,
      "learning_rate": 4.7706002034588e-05,
      "loss": 3.7194,
      "step": 4510
    },
    {
      "epoch": 2.299084435401831,
      "grad_norm": 2.76145601272583,
      "learning_rate": 4.770091556459817e-05,
      "loss": 3.7902,
      "step": 4520
    },
    {
      "epoch": 2.304170905391658,
      "grad_norm": 3.799846649169922,
      "learning_rate": 4.7695829094608347e-05,
      "loss": 3.7634,
      "step": 4530
    },
    {
      "epoch": 2.309257375381485,
      "grad_norm": 3.5113580226898193,
      "learning_rate": 4.7690742624618516e-05,
      "loss": 3.717,
      "step": 4540
    },
    {
      "epoch": 2.3143438453713125,
      "grad_norm": 5.49561882019043,
      "learning_rate": 4.7685656154628686e-05,
      "loss": 3.6792,
      "step": 4550
    },
    {
      "epoch": 2.3194303153611395,
      "grad_norm": 4.949704170227051,
      "learning_rate": 4.768056968463886e-05,
      "loss": 3.7393,
      "step": 4560
    },
    {
      "epoch": 2.3245167853509665,
      "grad_norm": 2.8186323642730713,
      "learning_rate": 4.767548321464903e-05,
      "loss": 3.6468,
      "step": 4570
    },
    {
      "epoch": 2.3296032553407935,
      "grad_norm": 4.201939582824707,
      "learning_rate": 4.767039674465921e-05,
      "loss": 3.6618,
      "step": 4580
    },
    {
      "epoch": 2.3346897253306205,
      "grad_norm": 3.32413387298584,
      "learning_rate": 4.7665310274669386e-05,
      "loss": 3.8188,
      "step": 4590
    },
    {
      "epoch": 2.3397761953204474,
      "grad_norm": 2.8796443939208984,
      "learning_rate": 4.7660223804679556e-05,
      "loss": 3.7142,
      "step": 4600
    },
    {
      "epoch": 2.3448626653102744,
      "grad_norm": 3.977274179458618,
      "learning_rate": 4.7655137334689726e-05,
      "loss": 3.7936,
      "step": 4610
    },
    {
      "epoch": 2.349949135300102,
      "grad_norm": 3.2712597846984863,
      "learning_rate": 4.76500508646999e-05,
      "loss": 3.7186,
      "step": 4620
    },
    {
      "epoch": 2.355035605289929,
      "grad_norm": 4.308432579040527,
      "learning_rate": 4.764496439471007e-05,
      "loss": 3.7061,
      "step": 4630
    },
    {
      "epoch": 2.360122075279756,
      "grad_norm": 3.5347342491149902,
      "learning_rate": 4.763987792472024e-05,
      "loss": 3.769,
      "step": 4640
    },
    {
      "epoch": 2.365208545269583,
      "grad_norm": 4.926098823547363,
      "learning_rate": 4.763479145473042e-05,
      "loss": 3.6827,
      "step": 4650
    },
    {
      "epoch": 2.3702950152594098,
      "grad_norm": 3.7818284034729004,
      "learning_rate": 4.762970498474059e-05,
      "loss": 3.8166,
      "step": 4660
    },
    {
      "epoch": 2.375381485249237,
      "grad_norm": 4.909131050109863,
      "learning_rate": 4.7624618514750766e-05,
      "loss": 3.7323,
      "step": 4670
    },
    {
      "epoch": 2.380467955239064,
      "grad_norm": 3.9240024089813232,
      "learning_rate": 4.761953204476094e-05,
      "loss": 3.7847,
      "step": 4680
    },
    {
      "epoch": 2.385554425228891,
      "grad_norm": 3.7451188564300537,
      "learning_rate": 4.761444557477111e-05,
      "loss": 3.7233,
      "step": 4690
    },
    {
      "epoch": 2.390640895218718,
      "grad_norm": 4.937544345855713,
      "learning_rate": 4.760935910478128e-05,
      "loss": 3.7559,
      "step": 4700
    },
    {
      "epoch": 2.395727365208545,
      "grad_norm": 4.20212459564209,
      "learning_rate": 4.760427263479146e-05,
      "loss": 3.7641,
      "step": 4710
    },
    {
      "epoch": 2.400813835198372,
      "grad_norm": 4.265017509460449,
      "learning_rate": 4.759918616480163e-05,
      "loss": 3.7119,
      "step": 4720
    },
    {
      "epoch": 2.4059003051881995,
      "grad_norm": 3.5301215648651123,
      "learning_rate": 4.7594099694811805e-05,
      "loss": 3.7692,
      "step": 4730
    },
    {
      "epoch": 2.4109867751780265,
      "grad_norm": 3.5779762268066406,
      "learning_rate": 4.7589013224821975e-05,
      "loss": 3.7138,
      "step": 4740
    },
    {
      "epoch": 2.4160732451678535,
      "grad_norm": 4.699209213256836,
      "learning_rate": 4.7583926754832145e-05,
      "loss": 3.7531,
      "step": 4750
    },
    {
      "epoch": 2.4211597151576805,
      "grad_norm": 3.0192880630493164,
      "learning_rate": 4.757884028484232e-05,
      "loss": 3.7693,
      "step": 4760
    },
    {
      "epoch": 2.4262461851475075,
      "grad_norm": 4.206872940063477,
      "learning_rate": 4.75737538148525e-05,
      "loss": 3.7236,
      "step": 4770
    },
    {
      "epoch": 2.431332655137335,
      "grad_norm": 4.3301825523376465,
      "learning_rate": 4.756866734486267e-05,
      "loss": 3.7487,
      "step": 4780
    },
    {
      "epoch": 2.436419125127162,
      "grad_norm": 3.8758509159088135,
      "learning_rate": 4.7563580874872845e-05,
      "loss": 3.7628,
      "step": 4790
    },
    {
      "epoch": 2.441505595116989,
      "grad_norm": 2.3153724670410156,
      "learning_rate": 4.7558494404883015e-05,
      "loss": 3.8001,
      "step": 4800
    },
    {
      "epoch": 2.446592065106816,
      "grad_norm": 4.33902645111084,
      "learning_rate": 4.7553407934893185e-05,
      "loss": 3.8024,
      "step": 4810
    },
    {
      "epoch": 2.451678535096643,
      "grad_norm": 3.399277687072754,
      "learning_rate": 4.754832146490336e-05,
      "loss": 3.79,
      "step": 4820
    },
    {
      "epoch": 2.4567650050864698,
      "grad_norm": 3.587228775024414,
      "learning_rate": 4.754323499491353e-05,
      "loss": 3.6889,
      "step": 4830
    },
    {
      "epoch": 2.461851475076297,
      "grad_norm": 4.076161861419678,
      "learning_rate": 4.75381485249237e-05,
      "loss": 3.6825,
      "step": 4840
    },
    {
      "epoch": 2.466937945066124,
      "grad_norm": 3.5534796714782715,
      "learning_rate": 4.753306205493388e-05,
      "loss": 3.7125,
      "step": 4850
    },
    {
      "epoch": 2.472024415055951,
      "grad_norm": 5.541903972625732,
      "learning_rate": 4.752797558494405e-05,
      "loss": 3.7919,
      "step": 4860
    },
    {
      "epoch": 2.477110885045778,
      "grad_norm": 6.1370930671691895,
      "learning_rate": 4.7522889114954225e-05,
      "loss": 3.7082,
      "step": 4870
    },
    {
      "epoch": 2.482197355035605,
      "grad_norm": 3.4931039810180664,
      "learning_rate": 4.75178026449644e-05,
      "loss": 3.6556,
      "step": 4880
    },
    {
      "epoch": 2.4872838250254325,
      "grad_norm": 3.6662137508392334,
      "learning_rate": 4.751271617497457e-05,
      "loss": 3.6762,
      "step": 4890
    },
    {
      "epoch": 2.4923702950152595,
      "grad_norm": 3.529069662094116,
      "learning_rate": 4.750762970498474e-05,
      "loss": 3.7542,
      "step": 4900
    },
    {
      "epoch": 2.4974567650050865,
      "grad_norm": 3.9258742332458496,
      "learning_rate": 4.750254323499492e-05,
      "loss": 3.7284,
      "step": 4910
    },
    {
      "epoch": 2.5025432349949135,
      "grad_norm": 4.791749954223633,
      "learning_rate": 4.749745676500509e-05,
      "loss": 3.7941,
      "step": 4920
    },
    {
      "epoch": 2.5076297049847405,
      "grad_norm": 3.613046407699585,
      "learning_rate": 4.749237029501526e-05,
      "loss": 3.7212,
      "step": 4930
    },
    {
      "epoch": 2.5127161749745675,
      "grad_norm": 6.143880367279053,
      "learning_rate": 4.7487283825025434e-05,
      "loss": 3.7759,
      "step": 4940
    },
    {
      "epoch": 2.517802644964395,
      "grad_norm": 4.9501566886901855,
      "learning_rate": 4.7482197355035604e-05,
      "loss": 3.7085,
      "step": 4950
    },
    {
      "epoch": 2.522889114954222,
      "grad_norm": 3.5245087146759033,
      "learning_rate": 4.747711088504578e-05,
      "loss": 3.7845,
      "step": 4960
    },
    {
      "epoch": 2.527975584944049,
      "grad_norm": 3.879873275756836,
      "learning_rate": 4.747202441505596e-05,
      "loss": 3.7845,
      "step": 4970
    },
    {
      "epoch": 2.533062054933876,
      "grad_norm": 2.863471508026123,
      "learning_rate": 4.746693794506613e-05,
      "loss": 3.7087,
      "step": 4980
    },
    {
      "epoch": 2.538148524923703,
      "grad_norm": 4.742125511169434,
      "learning_rate": 4.74618514750763e-05,
      "loss": 3.6959,
      "step": 4990
    },
    {
      "epoch": 2.5432349949135302,
      "grad_norm": 4.197792053222656,
      "learning_rate": 4.7456765005086474e-05,
      "loss": 3.6799,
      "step": 5000
    },
    {
      "epoch": 2.548321464903357,
      "grad_norm": 4.584874153137207,
      "learning_rate": 4.7451678535096644e-05,
      "loss": 3.6834,
      "step": 5010
    },
    {
      "epoch": 2.553407934893184,
      "grad_norm": 3.9623186588287354,
      "learning_rate": 4.744659206510682e-05,
      "loss": 3.6645,
      "step": 5020
    },
    {
      "epoch": 2.558494404883011,
      "grad_norm": 4.095483779907227,
      "learning_rate": 4.744150559511699e-05,
      "loss": 3.729,
      "step": 5030
    },
    {
      "epoch": 2.563580874872838,
      "grad_norm": 3.7427215576171875,
      "learning_rate": 4.743641912512716e-05,
      "loss": 3.6687,
      "step": 5040
    },
    {
      "epoch": 2.568667344862665,
      "grad_norm": 3.2752528190612793,
      "learning_rate": 4.743133265513734e-05,
      "loss": 3.6605,
      "step": 5050
    },
    {
      "epoch": 2.573753814852492,
      "grad_norm": 5.092625141143799,
      "learning_rate": 4.7426246185147514e-05,
      "loss": 3.7804,
      "step": 5060
    },
    {
      "epoch": 2.5788402848423195,
      "grad_norm": 5.100876808166504,
      "learning_rate": 4.7421159715157683e-05,
      "loss": 3.7609,
      "step": 5070
    },
    {
      "epoch": 2.5839267548321465,
      "grad_norm": 5.389939308166504,
      "learning_rate": 4.741607324516786e-05,
      "loss": 3.6929,
      "step": 5080
    },
    {
      "epoch": 2.5890132248219735,
      "grad_norm": 3.626095771789551,
      "learning_rate": 4.741098677517803e-05,
      "loss": 3.6352,
      "step": 5090
    },
    {
      "epoch": 2.5940996948118005,
      "grad_norm": 4.17099142074585,
      "learning_rate": 4.74059003051882e-05,
      "loss": 3.7135,
      "step": 5100
    },
    {
      "epoch": 2.599186164801628,
      "grad_norm": 3.0728600025177,
      "learning_rate": 4.7400813835198377e-05,
      "loss": 3.6777,
      "step": 5110
    },
    {
      "epoch": 2.604272634791455,
      "grad_norm": 5.30855655670166,
      "learning_rate": 4.7395727365208546e-05,
      "loss": 3.683,
      "step": 5120
    },
    {
      "epoch": 2.609359104781282,
      "grad_norm": 3.810598611831665,
      "learning_rate": 4.7390640895218716e-05,
      "loss": 3.707,
      "step": 5130
    },
    {
      "epoch": 2.614445574771109,
      "grad_norm": 5.050060272216797,
      "learning_rate": 4.738555442522889e-05,
      "loss": 3.7768,
      "step": 5140
    },
    {
      "epoch": 2.619532044760936,
      "grad_norm": 5.928835868835449,
      "learning_rate": 4.738046795523906e-05,
      "loss": 3.7884,
      "step": 5150
    },
    {
      "epoch": 2.624618514750763,
      "grad_norm": 3.9667060375213623,
      "learning_rate": 4.737538148524924e-05,
      "loss": 3.7595,
      "step": 5160
    },
    {
      "epoch": 2.62970498474059,
      "grad_norm": 4.3744587898254395,
      "learning_rate": 4.7370295015259416e-05,
      "loss": 3.678,
      "step": 5170
    },
    {
      "epoch": 2.634791454730417,
      "grad_norm": 4.936386585235596,
      "learning_rate": 4.7365208545269586e-05,
      "loss": 3.7201,
      "step": 5180
    },
    {
      "epoch": 2.639877924720244,
      "grad_norm": 4.088891983032227,
      "learning_rate": 4.7360122075279756e-05,
      "loss": 3.7093,
      "step": 5190
    },
    {
      "epoch": 2.644964394710071,
      "grad_norm": 4.086247444152832,
      "learning_rate": 4.735503560528993e-05,
      "loss": 3.6612,
      "step": 5200
    },
    {
      "epoch": 2.650050864699898,
      "grad_norm": 6.153862476348877,
      "learning_rate": 4.73499491353001e-05,
      "loss": 3.7143,
      "step": 5210
    },
    {
      "epoch": 2.6551373346897256,
      "grad_norm": 5.14927339553833,
      "learning_rate": 4.734486266531027e-05,
      "loss": 3.6996,
      "step": 5220
    },
    {
      "epoch": 2.6602238046795526,
      "grad_norm": 5.2147088050842285,
      "learning_rate": 4.733977619532045e-05,
      "loss": 3.6849,
      "step": 5230
    },
    {
      "epoch": 2.6653102746693795,
      "grad_norm": 3.6904585361480713,
      "learning_rate": 4.733468972533062e-05,
      "loss": 3.6646,
      "step": 5240
    },
    {
      "epoch": 2.6703967446592065,
      "grad_norm": 4.170338153839111,
      "learning_rate": 4.7329603255340796e-05,
      "loss": 3.6966,
      "step": 5250
    },
    {
      "epoch": 2.6754832146490335,
      "grad_norm": 4.927977085113525,
      "learning_rate": 4.732451678535097e-05,
      "loss": 3.7143,
      "step": 5260
    },
    {
      "epoch": 2.6805696846388605,
      "grad_norm": 4.0680832862854,
      "learning_rate": 4.731943031536114e-05,
      "loss": 3.6841,
      "step": 5270
    },
    {
      "epoch": 2.6856561546286875,
      "grad_norm": 6.402088642120361,
      "learning_rate": 4.731434384537132e-05,
      "loss": 3.6357,
      "step": 5280
    },
    {
      "epoch": 2.690742624618515,
      "grad_norm": 3.656703472137451,
      "learning_rate": 4.730925737538149e-05,
      "loss": 3.6916,
      "step": 5290
    },
    {
      "epoch": 2.695829094608342,
      "grad_norm": 3.723432779312134,
      "learning_rate": 4.730417090539166e-05,
      "loss": 3.7321,
      "step": 5300
    },
    {
      "epoch": 2.700915564598169,
      "grad_norm": 4.878834247589111,
      "learning_rate": 4.7299084435401835e-05,
      "loss": 3.6424,
      "step": 5310
    },
    {
      "epoch": 2.706002034587996,
      "grad_norm": 3.647526741027832,
      "learning_rate": 4.7293997965412005e-05,
      "loss": 3.804,
      "step": 5320
    },
    {
      "epoch": 2.7110885045778232,
      "grad_norm": 4.02158260345459,
      "learning_rate": 4.7288911495422175e-05,
      "loss": 3.6634,
      "step": 5330
    },
    {
      "epoch": 2.7161749745676502,
      "grad_norm": 3.5641396045684814,
      "learning_rate": 4.728382502543235e-05,
      "loss": 3.7005,
      "step": 5340
    },
    {
      "epoch": 2.721261444557477,
      "grad_norm": 3.2118380069732666,
      "learning_rate": 4.727873855544253e-05,
      "loss": 3.7093,
      "step": 5350
    },
    {
      "epoch": 2.726347914547304,
      "grad_norm": 3.5591046810150146,
      "learning_rate": 4.72736520854527e-05,
      "loss": 3.6659,
      "step": 5360
    },
    {
      "epoch": 2.731434384537131,
      "grad_norm": 5.539649486541748,
      "learning_rate": 4.7268565615462875e-05,
      "loss": 3.6466,
      "step": 5370
    },
    {
      "epoch": 2.736520854526958,
      "grad_norm": 4.554442882537842,
      "learning_rate": 4.7263479145473045e-05,
      "loss": 3.6131,
      "step": 5380
    },
    {
      "epoch": 2.741607324516785,
      "grad_norm": 3.602660655975342,
      "learning_rate": 4.7258392675483215e-05,
      "loss": 3.6875,
      "step": 5390
    },
    {
      "epoch": 2.7466937945066126,
      "grad_norm": 5.642316818237305,
      "learning_rate": 4.725330620549339e-05,
      "loss": 3.6964,
      "step": 5400
    },
    {
      "epoch": 2.7517802644964395,
      "grad_norm": 6.344845771789551,
      "learning_rate": 4.724821973550356e-05,
      "loss": 3.6677,
      "step": 5410
    },
    {
      "epoch": 2.7568667344862665,
      "grad_norm": 4.779513359069824,
      "learning_rate": 4.724313326551373e-05,
      "loss": 3.7205,
      "step": 5420
    },
    {
      "epoch": 2.7619532044760935,
      "grad_norm": 4.098911762237549,
      "learning_rate": 4.723804679552391e-05,
      "loss": 3.8005,
      "step": 5430
    },
    {
      "epoch": 2.767039674465921,
      "grad_norm": 4.061922550201416,
      "learning_rate": 4.7232960325534085e-05,
      "loss": 3.7158,
      "step": 5440
    },
    {
      "epoch": 2.772126144455748,
      "grad_norm": 3.6565074920654297,
      "learning_rate": 4.7227873855544255e-05,
      "loss": 3.7754,
      "step": 5450
    },
    {
      "epoch": 2.777212614445575,
      "grad_norm": 3.252622127532959,
      "learning_rate": 4.722278738555443e-05,
      "loss": 3.7673,
      "step": 5460
    },
    {
      "epoch": 2.782299084435402,
      "grad_norm": 3.1711196899414062,
      "learning_rate": 4.72177009155646e-05,
      "loss": 3.7207,
      "step": 5470
    },
    {
      "epoch": 2.787385554425229,
      "grad_norm": 4.77832555770874,
      "learning_rate": 4.721261444557477e-05,
      "loss": 3.6313,
      "step": 5480
    },
    {
      "epoch": 2.792472024415056,
      "grad_norm": 5.02147912979126,
      "learning_rate": 4.720752797558495e-05,
      "loss": 3.6641,
      "step": 5490
    },
    {
      "epoch": 2.797558494404883,
      "grad_norm": 4.071934223175049,
      "learning_rate": 4.720244150559512e-05,
      "loss": 3.6561,
      "step": 5500
    },
    {
      "epoch": 2.8026449643947102,
      "grad_norm": 5.00059175491333,
      "learning_rate": 4.719735503560529e-05,
      "loss": 3.6793,
      "step": 5510
    },
    {
      "epoch": 2.807731434384537,
      "grad_norm": 4.873887538909912,
      "learning_rate": 4.7192268565615464e-05,
      "loss": 3.6595,
      "step": 5520
    },
    {
      "epoch": 2.812817904374364,
      "grad_norm": 5.991937637329102,
      "learning_rate": 4.7187182095625634e-05,
      "loss": 3.6569,
      "step": 5530
    },
    {
      "epoch": 2.817904374364191,
      "grad_norm": 3.432481288909912,
      "learning_rate": 4.718209562563581e-05,
      "loss": 3.6634,
      "step": 5540
    },
    {
      "epoch": 2.822990844354018,
      "grad_norm": 5.015426158905029,
      "learning_rate": 4.717700915564599e-05,
      "loss": 3.6655,
      "step": 5550
    },
    {
      "epoch": 2.8280773143438456,
      "grad_norm": 3.983628988265991,
      "learning_rate": 4.717192268565616e-05,
      "loss": 3.6695,
      "step": 5560
    },
    {
      "epoch": 2.8331637843336726,
      "grad_norm": 4.249433517456055,
      "learning_rate": 4.7166836215666334e-05,
      "loss": 3.6598,
      "step": 5570
    },
    {
      "epoch": 2.8382502543234995,
      "grad_norm": 4.466683387756348,
      "learning_rate": 4.7161749745676504e-05,
      "loss": 3.6617,
      "step": 5580
    },
    {
      "epoch": 2.8433367243133265,
      "grad_norm": 4.560993194580078,
      "learning_rate": 4.7156663275686674e-05,
      "loss": 3.6239,
      "step": 5590
    },
    {
      "epoch": 2.8484231943031535,
      "grad_norm": 4.590268611907959,
      "learning_rate": 4.715157680569685e-05,
      "loss": 3.6689,
      "step": 5600
    },
    {
      "epoch": 2.8535096642929805,
      "grad_norm": 4.271324634552002,
      "learning_rate": 4.714649033570702e-05,
      "loss": 3.6668,
      "step": 5610
    },
    {
      "epoch": 2.8585961342828075,
      "grad_norm": 3.659992218017578,
      "learning_rate": 4.714140386571719e-05,
      "loss": 3.6803,
      "step": 5620
    },
    {
      "epoch": 2.863682604272635,
      "grad_norm": 6.149805545806885,
      "learning_rate": 4.713631739572737e-05,
      "loss": 3.6442,
      "step": 5630
    },
    {
      "epoch": 2.868769074262462,
      "grad_norm": 5.277060031890869,
      "learning_rate": 4.7131230925737544e-05,
      "loss": 3.6845,
      "step": 5640
    },
    {
      "epoch": 2.873855544252289,
      "grad_norm": 5.318338394165039,
      "learning_rate": 4.7126144455747713e-05,
      "loss": 3.6114,
      "step": 5650
    },
    {
      "epoch": 2.878942014242116,
      "grad_norm": 4.43471622467041,
      "learning_rate": 4.712105798575789e-05,
      "loss": 3.6988,
      "step": 5660
    },
    {
      "epoch": 2.8840284842319432,
      "grad_norm": 3.6855530738830566,
      "learning_rate": 4.711597151576806e-05,
      "loss": 3.6825,
      "step": 5670
    },
    {
      "epoch": 2.8891149542217702,
      "grad_norm": 3.5933384895324707,
      "learning_rate": 4.711088504577823e-05,
      "loss": 3.6766,
      "step": 5680
    },
    {
      "epoch": 2.894201424211597,
      "grad_norm": 4.526597499847412,
      "learning_rate": 4.7105798575788407e-05,
      "loss": 3.6725,
      "step": 5690
    },
    {
      "epoch": 2.899287894201424,
      "grad_norm": 5.999857425689697,
      "learning_rate": 4.7100712105798576e-05,
      "loss": 3.6934,
      "step": 5700
    },
    {
      "epoch": 2.904374364191251,
      "grad_norm": 4.9649553298950195,
      "learning_rate": 4.7095625635808746e-05,
      "loss": 3.677,
      "step": 5710
    },
    {
      "epoch": 2.909460834181078,
      "grad_norm": 4.035357475280762,
      "learning_rate": 4.709053916581892e-05,
      "loss": 3.6663,
      "step": 5720
    },
    {
      "epoch": 2.914547304170905,
      "grad_norm": 5.042098522186279,
      "learning_rate": 4.70854526958291e-05,
      "loss": 3.7438,
      "step": 5730
    },
    {
      "epoch": 2.9196337741607326,
      "grad_norm": 3.8188817501068115,
      "learning_rate": 4.708036622583927e-05,
      "loss": 3.6689,
      "step": 5740
    },
    {
      "epoch": 2.9247202441505595,
      "grad_norm": 5.365772247314453,
      "learning_rate": 4.7075279755849446e-05,
      "loss": 3.7026,
      "step": 5750
    },
    {
      "epoch": 2.9298067141403865,
      "grad_norm": 3.420614719390869,
      "learning_rate": 4.7070193285859616e-05,
      "loss": 3.6605,
      "step": 5760
    },
    {
      "epoch": 2.9348931841302135,
      "grad_norm": 5.011106967926025,
      "learning_rate": 4.7065106815869786e-05,
      "loss": 3.636,
      "step": 5770
    },
    {
      "epoch": 2.939979654120041,
      "grad_norm": 3.5583465099334717,
      "learning_rate": 4.706002034587996e-05,
      "loss": 3.7717,
      "step": 5780
    },
    {
      "epoch": 2.945066124109868,
      "grad_norm": 6.052684783935547,
      "learning_rate": 4.705493387589013e-05,
      "loss": 3.6722,
      "step": 5790
    },
    {
      "epoch": 2.950152594099695,
      "grad_norm": 3.774580955505371,
      "learning_rate": 4.70498474059003e-05,
      "loss": 3.6608,
      "step": 5800
    },
    {
      "epoch": 2.955239064089522,
      "grad_norm": 3.4495961666107178,
      "learning_rate": 4.704476093591048e-05,
      "loss": 3.6252,
      "step": 5810
    },
    {
      "epoch": 2.960325534079349,
      "grad_norm": 5.737781047821045,
      "learning_rate": 4.703967446592065e-05,
      "loss": 3.6989,
      "step": 5820
    },
    {
      "epoch": 2.965412004069176,
      "grad_norm": 7.032559394836426,
      "learning_rate": 4.7034587995930826e-05,
      "loss": 3.6611,
      "step": 5830
    },
    {
      "epoch": 2.970498474059003,
      "grad_norm": 3.6125881671905518,
      "learning_rate": 4.7029501525941e-05,
      "loss": 3.67,
      "step": 5840
    },
    {
      "epoch": 2.9755849440488302,
      "grad_norm": 4.724600315093994,
      "learning_rate": 4.702441505595117e-05,
      "loss": 3.5268,
      "step": 5850
    },
    {
      "epoch": 2.980671414038657,
      "grad_norm": 5.115473747253418,
      "learning_rate": 4.701932858596135e-05,
      "loss": 3.5838,
      "step": 5860
    },
    {
      "epoch": 2.985757884028484,
      "grad_norm": 4.640131950378418,
      "learning_rate": 4.701424211597152e-05,
      "loss": 3.5698,
      "step": 5870
    },
    {
      "epoch": 2.990844354018311,
      "grad_norm": 4.753575801849365,
      "learning_rate": 4.700915564598169e-05,
      "loss": 3.7128,
      "step": 5880
    },
    {
      "epoch": 2.9959308240081386,
      "grad_norm": 4.5460591316223145,
      "learning_rate": 4.7004069175991865e-05,
      "loss": 3.6432,
      "step": 5890
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.730624198913574,
      "eval_runtime": 2.6319,
      "eval_samples_per_second": 1054.387,
      "eval_steps_per_second": 131.846,
      "step": 5898
    },
    {
      "epoch": 3.0010172939979656,
      "grad_norm": 7.507416248321533,
      "learning_rate": 4.6998982706002035e-05,
      "loss": 3.6238,
      "step": 5900
    },
    {
      "epoch": 3.0061037639877926,
      "grad_norm": 4.895421981811523,
      "learning_rate": 4.6993896236012205e-05,
      "loss": 3.609,
      "step": 5910
    },
    {
      "epoch": 3.0111902339776195,
      "grad_norm": 4.378381252288818,
      "learning_rate": 4.698880976602238e-05,
      "loss": 3.6303,
      "step": 5920
    },
    {
      "epoch": 3.0162767039674465,
      "grad_norm": 4.391936779022217,
      "learning_rate": 4.698372329603256e-05,
      "loss": 3.6101,
      "step": 5930
    },
    {
      "epoch": 3.0213631739572735,
      "grad_norm": 3.6512954235076904,
      "learning_rate": 4.697863682604273e-05,
      "loss": 3.6587,
      "step": 5940
    },
    {
      "epoch": 3.026449643947101,
      "grad_norm": 3.6170618534088135,
      "learning_rate": 4.6973550356052905e-05,
      "loss": 3.663,
      "step": 5950
    },
    {
      "epoch": 3.031536113936928,
      "grad_norm": 4.112524032592773,
      "learning_rate": 4.6968463886063075e-05,
      "loss": 3.6767,
      "step": 5960
    },
    {
      "epoch": 3.036622583926755,
      "grad_norm": 4.816345691680908,
      "learning_rate": 4.6963377416073245e-05,
      "loss": 3.6566,
      "step": 5970
    },
    {
      "epoch": 3.041709053916582,
      "grad_norm": 5.210585594177246,
      "learning_rate": 4.695829094608342e-05,
      "loss": 3.5746,
      "step": 5980
    },
    {
      "epoch": 3.046795523906409,
      "grad_norm": 5.842030048370361,
      "learning_rate": 4.695320447609359e-05,
      "loss": 3.6868,
      "step": 5990
    },
    {
      "epoch": 3.051881993896236,
      "grad_norm": 5.47335147857666,
      "learning_rate": 4.694811800610376e-05,
      "loss": 3.5736,
      "step": 6000
    },
    {
      "epoch": 3.0569684638860632,
      "grad_norm": 5.056438446044922,
      "learning_rate": 4.694303153611394e-05,
      "loss": 3.6288,
      "step": 6010
    },
    {
      "epoch": 3.0620549338758902,
      "grad_norm": 5.013584613800049,
      "learning_rate": 4.6937945066124115e-05,
      "loss": 3.5711,
      "step": 6020
    },
    {
      "epoch": 3.067141403865717,
      "grad_norm": 5.635212421417236,
      "learning_rate": 4.6932858596134285e-05,
      "loss": 3.648,
      "step": 6030
    },
    {
      "epoch": 3.072227873855544,
      "grad_norm": 4.481797218322754,
      "learning_rate": 4.692777212614446e-05,
      "loss": 3.6144,
      "step": 6040
    },
    {
      "epoch": 3.077314343845371,
      "grad_norm": 5.8804192543029785,
      "learning_rate": 4.692268565615463e-05,
      "loss": 3.6266,
      "step": 6050
    },
    {
      "epoch": 3.082400813835198,
      "grad_norm": 7.3049750328063965,
      "learning_rate": 4.69175991861648e-05,
      "loss": 3.5545,
      "step": 6060
    },
    {
      "epoch": 3.0874872838250256,
      "grad_norm": 7.520923137664795,
      "learning_rate": 4.691251271617498e-05,
      "loss": 3.6626,
      "step": 6070
    },
    {
      "epoch": 3.0925737538148526,
      "grad_norm": 4.732873439788818,
      "learning_rate": 4.690742624618515e-05,
      "loss": 3.6745,
      "step": 6080
    },
    {
      "epoch": 3.0976602238046795,
      "grad_norm": 5.350921630859375,
      "learning_rate": 4.6902339776195324e-05,
      "loss": 3.657,
      "step": 6090
    },
    {
      "epoch": 3.1027466937945065,
      "grad_norm": 4.853120803833008,
      "learning_rate": 4.6897253306205494e-05,
      "loss": 3.6241,
      "step": 6100
    },
    {
      "epoch": 3.1078331637843335,
      "grad_norm": 5.193929672241211,
      "learning_rate": 4.6892166836215664e-05,
      "loss": 3.6805,
      "step": 6110
    },
    {
      "epoch": 3.112919633774161,
      "grad_norm": 5.137939453125,
      "learning_rate": 4.688708036622584e-05,
      "loss": 3.6463,
      "step": 6120
    },
    {
      "epoch": 3.118006103763988,
      "grad_norm": 5.308433532714844,
      "learning_rate": 4.688199389623602e-05,
      "loss": 3.5716,
      "step": 6130
    },
    {
      "epoch": 3.123092573753815,
      "grad_norm": 5.362980842590332,
      "learning_rate": 4.687690742624619e-05,
      "loss": 3.5878,
      "step": 6140
    },
    {
      "epoch": 3.128179043743642,
      "grad_norm": 6.189334392547607,
      "learning_rate": 4.6871820956256364e-05,
      "loss": 3.6959,
      "step": 6150
    },
    {
      "epoch": 3.133265513733469,
      "grad_norm": 5.566260814666748,
      "learning_rate": 4.6866734486266534e-05,
      "loss": 3.6474,
      "step": 6160
    },
    {
      "epoch": 3.138351983723296,
      "grad_norm": 3.9197475910186768,
      "learning_rate": 4.6861648016276704e-05,
      "loss": 3.5929,
      "step": 6170
    },
    {
      "epoch": 3.1434384537131232,
      "grad_norm": 5.2231855392456055,
      "learning_rate": 4.685656154628688e-05,
      "loss": 3.6423,
      "step": 6180
    },
    {
      "epoch": 3.1485249237029502,
      "grad_norm": 4.13824462890625,
      "learning_rate": 4.685147507629705e-05,
      "loss": 3.6855,
      "step": 6190
    },
    {
      "epoch": 3.153611393692777,
      "grad_norm": 6.090406894683838,
      "learning_rate": 4.684638860630722e-05,
      "loss": 3.6057,
      "step": 6200
    },
    {
      "epoch": 3.158697863682604,
      "grad_norm": 4.644675254821777,
      "learning_rate": 4.68413021363174e-05,
      "loss": 3.6638,
      "step": 6210
    },
    {
      "epoch": 3.163784333672431,
      "grad_norm": 6.911463737487793,
      "learning_rate": 4.6836215666327574e-05,
      "loss": 3.6513,
      "step": 6220
    },
    {
      "epoch": 3.1688708036622586,
      "grad_norm": 5.354273796081543,
      "learning_rate": 4.6831129196337743e-05,
      "loss": 3.6928,
      "step": 6230
    },
    {
      "epoch": 3.1739572736520856,
      "grad_norm": 5.094912528991699,
      "learning_rate": 4.682604272634792e-05,
      "loss": 3.6879,
      "step": 6240
    },
    {
      "epoch": 3.1790437436419126,
      "grad_norm": 5.294932842254639,
      "learning_rate": 4.682095625635809e-05,
      "loss": 3.623,
      "step": 6250
    },
    {
      "epoch": 3.1841302136317395,
      "grad_norm": 4.585472106933594,
      "learning_rate": 4.681586978636826e-05,
      "loss": 3.6347,
      "step": 6260
    },
    {
      "epoch": 3.1892166836215665,
      "grad_norm": 3.752584457397461,
      "learning_rate": 4.6810783316378437e-05,
      "loss": 3.645,
      "step": 6270
    },
    {
      "epoch": 3.1943031536113935,
      "grad_norm": 6.681651592254639,
      "learning_rate": 4.6805696846388606e-05,
      "loss": 3.6406,
      "step": 6280
    },
    {
      "epoch": 3.199389623601221,
      "grad_norm": 3.9756886959075928,
      "learning_rate": 4.6800610376398776e-05,
      "loss": 3.5795,
      "step": 6290
    },
    {
      "epoch": 3.204476093591048,
      "grad_norm": 7.231886863708496,
      "learning_rate": 4.679552390640895e-05,
      "loss": 3.6665,
      "step": 6300
    },
    {
      "epoch": 3.209562563580875,
      "grad_norm": 5.2829413414001465,
      "learning_rate": 4.679043743641913e-05,
      "loss": 3.6035,
      "step": 6310
    },
    {
      "epoch": 3.214649033570702,
      "grad_norm": 3.9279534816741943,
      "learning_rate": 4.67853509664293e-05,
      "loss": 3.6748,
      "step": 6320
    },
    {
      "epoch": 3.219735503560529,
      "grad_norm": 4.4516072273254395,
      "learning_rate": 4.6780264496439476e-05,
      "loss": 3.5935,
      "step": 6330
    },
    {
      "epoch": 3.2248219735503563,
      "grad_norm": 5.494381427764893,
      "learning_rate": 4.6775178026449646e-05,
      "loss": 3.613,
      "step": 6340
    },
    {
      "epoch": 3.2299084435401832,
      "grad_norm": 6.77787446975708,
      "learning_rate": 4.677009155645982e-05,
      "loss": 3.5552,
      "step": 6350
    },
    {
      "epoch": 3.2349949135300102,
      "grad_norm": 4.624700546264648,
      "learning_rate": 4.676500508646999e-05,
      "loss": 3.6108,
      "step": 6360
    },
    {
      "epoch": 3.240081383519837,
      "grad_norm": 5.207804203033447,
      "learning_rate": 4.675991861648016e-05,
      "loss": 3.6278,
      "step": 6370
    },
    {
      "epoch": 3.245167853509664,
      "grad_norm": 4.456101417541504,
      "learning_rate": 4.675483214649034e-05,
      "loss": 3.5611,
      "step": 6380
    },
    {
      "epoch": 3.250254323499491,
      "grad_norm": 5.631757736206055,
      "learning_rate": 4.674974567650051e-05,
      "loss": 3.641,
      "step": 6390
    },
    {
      "epoch": 3.2553407934893186,
      "grad_norm": 5.219798564910889,
      "learning_rate": 4.6744659206510686e-05,
      "loss": 3.6026,
      "step": 6400
    },
    {
      "epoch": 3.2604272634791456,
      "grad_norm": 4.668636322021484,
      "learning_rate": 4.673957273652086e-05,
      "loss": 3.6479,
      "step": 6410
    },
    {
      "epoch": 3.2655137334689726,
      "grad_norm": 5.387055397033691,
      "learning_rate": 4.673448626653103e-05,
      "loss": 3.5609,
      "step": 6420
    },
    {
      "epoch": 3.2706002034587995,
      "grad_norm": 4.472424507141113,
      "learning_rate": 4.67293997965412e-05,
      "loss": 3.5468,
      "step": 6430
    },
    {
      "epoch": 3.2756866734486265,
      "grad_norm": 5.355788230895996,
      "learning_rate": 4.672431332655138e-05,
      "loss": 3.6089,
      "step": 6440
    },
    {
      "epoch": 3.280773143438454,
      "grad_norm": 4.767132759094238,
      "learning_rate": 4.671922685656155e-05,
      "loss": 3.6801,
      "step": 6450
    },
    {
      "epoch": 3.285859613428281,
      "grad_norm": 6.642704486846924,
      "learning_rate": 4.671414038657172e-05,
      "loss": 3.6679,
      "step": 6460
    },
    {
      "epoch": 3.290946083418108,
      "grad_norm": 5.752387523651123,
      "learning_rate": 4.6709053916581895e-05,
      "loss": 3.5664,
      "step": 6470
    },
    {
      "epoch": 3.296032553407935,
      "grad_norm": 6.090226650238037,
      "learning_rate": 4.6703967446592065e-05,
      "loss": 3.6715,
      "step": 6480
    },
    {
      "epoch": 3.301119023397762,
      "grad_norm": 5.08964204788208,
      "learning_rate": 4.6698880976602235e-05,
      "loss": 3.5957,
      "step": 6490
    },
    {
      "epoch": 3.306205493387589,
      "grad_norm": 5.29610013961792,
      "learning_rate": 4.669379450661241e-05,
      "loss": 3.6722,
      "step": 6500
    },
    {
      "epoch": 3.311291963377416,
      "grad_norm": 5.590677738189697,
      "learning_rate": 4.668870803662259e-05,
      "loss": 3.6655,
      "step": 6510
    },
    {
      "epoch": 3.3163784333672433,
      "grad_norm": 4.272968292236328,
      "learning_rate": 4.668362156663276e-05,
      "loss": 3.6091,
      "step": 6520
    },
    {
      "epoch": 3.3214649033570702,
      "grad_norm": 6.740396976470947,
      "learning_rate": 4.6678535096642935e-05,
      "loss": 3.5491,
      "step": 6530
    },
    {
      "epoch": 3.326551373346897,
      "grad_norm": 7.082432746887207,
      "learning_rate": 4.6673448626653105e-05,
      "loss": 3.5512,
      "step": 6540
    },
    {
      "epoch": 3.331637843336724,
      "grad_norm": 4.9167022705078125,
      "learning_rate": 4.6668362156663275e-05,
      "loss": 3.5549,
      "step": 6550
    },
    {
      "epoch": 3.3367243133265516,
      "grad_norm": 5.1121063232421875,
      "learning_rate": 4.666327568667345e-05,
      "loss": 3.6098,
      "step": 6560
    },
    {
      "epoch": 3.3418107833163786,
      "grad_norm": 4.417415618896484,
      "learning_rate": 4.665818921668362e-05,
      "loss": 3.6504,
      "step": 6570
    },
    {
      "epoch": 3.3468972533062056,
      "grad_norm": 5.0213541984558105,
      "learning_rate": 4.665310274669379e-05,
      "loss": 3.5584,
      "step": 6580
    },
    {
      "epoch": 3.3519837232960326,
      "grad_norm": 4.2303032875061035,
      "learning_rate": 4.664801627670397e-05,
      "loss": 3.6434,
      "step": 6590
    },
    {
      "epoch": 3.3570701932858595,
      "grad_norm": 7.07000207901001,
      "learning_rate": 4.6642929806714145e-05,
      "loss": 3.6364,
      "step": 6600
    },
    {
      "epoch": 3.3621566632756865,
      "grad_norm": 5.448245525360107,
      "learning_rate": 4.6637843336724315e-05,
      "loss": 3.5681,
      "step": 6610
    },
    {
      "epoch": 3.3672431332655135,
      "grad_norm": 5.562067031860352,
      "learning_rate": 4.663275686673449e-05,
      "loss": 3.6292,
      "step": 6620
    },
    {
      "epoch": 3.372329603255341,
      "grad_norm": 4.17584753036499,
      "learning_rate": 4.662767039674466e-05,
      "loss": 3.6419,
      "step": 6630
    },
    {
      "epoch": 3.377416073245168,
      "grad_norm": 5.500147819519043,
      "learning_rate": 4.662258392675484e-05,
      "loss": 3.6058,
      "step": 6640
    },
    {
      "epoch": 3.382502543234995,
      "grad_norm": 4.1581807136535645,
      "learning_rate": 4.661749745676501e-05,
      "loss": 3.5954,
      "step": 6650
    },
    {
      "epoch": 3.387589013224822,
      "grad_norm": 4.956427097320557,
      "learning_rate": 4.661241098677518e-05,
      "loss": 3.5831,
      "step": 6660
    },
    {
      "epoch": 3.392675483214649,
      "grad_norm": 5.724091053009033,
      "learning_rate": 4.6607324516785354e-05,
      "loss": 3.6244,
      "step": 6670
    },
    {
      "epoch": 3.3977619532044763,
      "grad_norm": 4.715976715087891,
      "learning_rate": 4.6602238046795524e-05,
      "loss": 3.5964,
      "step": 6680
    },
    {
      "epoch": 3.4028484231943033,
      "grad_norm": 5.500229835510254,
      "learning_rate": 4.65971515768057e-05,
      "loss": 3.6604,
      "step": 6690
    },
    {
      "epoch": 3.4079348931841302,
      "grad_norm": 4.800648212432861,
      "learning_rate": 4.659206510681588e-05,
      "loss": 3.6522,
      "step": 6700
    },
    {
      "epoch": 3.413021363173957,
      "grad_norm": 6.145252704620361,
      "learning_rate": 4.658697863682605e-05,
      "loss": 3.5853,
      "step": 6710
    },
    {
      "epoch": 3.418107833163784,
      "grad_norm": 5.1746110916137695,
      "learning_rate": 4.658189216683622e-05,
      "loss": 3.5512,
      "step": 6720
    },
    {
      "epoch": 3.423194303153611,
      "grad_norm": 5.844448089599609,
      "learning_rate": 4.6576805696846394e-05,
      "loss": 3.6089,
      "step": 6730
    },
    {
      "epoch": 3.4282807731434386,
      "grad_norm": 4.023530960083008,
      "learning_rate": 4.6571719226856564e-05,
      "loss": 3.6283,
      "step": 6740
    },
    {
      "epoch": 3.4333672431332656,
      "grad_norm": 6.939926624298096,
      "learning_rate": 4.6566632756866734e-05,
      "loss": 3.6125,
      "step": 6750
    },
    {
      "epoch": 3.4384537131230926,
      "grad_norm": 5.898573398590088,
      "learning_rate": 4.656154628687691e-05,
      "loss": 3.5964,
      "step": 6760
    },
    {
      "epoch": 3.4435401831129195,
      "grad_norm": 4.974893093109131,
      "learning_rate": 4.655645981688708e-05,
      "loss": 3.5804,
      "step": 6770
    },
    {
      "epoch": 3.4486266531027465,
      "grad_norm": 4.803226470947266,
      "learning_rate": 4.655137334689725e-05,
      "loss": 3.628,
      "step": 6780
    },
    {
      "epoch": 3.453713123092574,
      "grad_norm": 4.777872562408447,
      "learning_rate": 4.654628687690743e-05,
      "loss": 3.6318,
      "step": 6790
    },
    {
      "epoch": 3.458799593082401,
      "grad_norm": 5.87904691696167,
      "learning_rate": 4.6541200406917604e-05,
      "loss": 3.5689,
      "step": 6800
    },
    {
      "epoch": 3.463886063072228,
      "grad_norm": 4.298388481140137,
      "learning_rate": 4.6536113936927773e-05,
      "loss": 3.6352,
      "step": 6810
    },
    {
      "epoch": 3.468972533062055,
      "grad_norm": 5.30334997177124,
      "learning_rate": 4.653102746693795e-05,
      "loss": 3.5961,
      "step": 6820
    },
    {
      "epoch": 3.474059003051882,
      "grad_norm": 6.82050895690918,
      "learning_rate": 4.652594099694812e-05,
      "loss": 3.6312,
      "step": 6830
    },
    {
      "epoch": 3.479145473041709,
      "grad_norm": 4.372466087341309,
      "learning_rate": 4.652085452695829e-05,
      "loss": 3.5823,
      "step": 6840
    },
    {
      "epoch": 3.4842319430315363,
      "grad_norm": 5.053226947784424,
      "learning_rate": 4.6515768056968467e-05,
      "loss": 3.5941,
      "step": 6850
    },
    {
      "epoch": 3.4893184130213633,
      "grad_norm": 6.829438209533691,
      "learning_rate": 4.6510681586978636e-05,
      "loss": 3.5406,
      "step": 6860
    },
    {
      "epoch": 3.4944048830111902,
      "grad_norm": 8.250497817993164,
      "learning_rate": 4.6505595116988806e-05,
      "loss": 3.6266,
      "step": 6870
    },
    {
      "epoch": 3.499491353001017,
      "grad_norm": 6.642141342163086,
      "learning_rate": 4.650050864699898e-05,
      "loss": 3.6075,
      "step": 6880
    },
    {
      "epoch": 3.504577822990844,
      "grad_norm": 4.867504119873047,
      "learning_rate": 4.649542217700916e-05,
      "loss": 3.6243,
      "step": 6890
    },
    {
      "epoch": 3.5096642929806716,
      "grad_norm": 5.355265140533447,
      "learning_rate": 4.6490335707019336e-05,
      "loss": 3.5015,
      "step": 6900
    },
    {
      "epoch": 3.5147507629704986,
      "grad_norm": 5.4089202880859375,
      "learning_rate": 4.6485249237029506e-05,
      "loss": 3.5895,
      "step": 6910
    },
    {
      "epoch": 3.5198372329603256,
      "grad_norm": 6.7899250984191895,
      "learning_rate": 4.6480162767039676e-05,
      "loss": 3.565,
      "step": 6920
    },
    {
      "epoch": 3.5249237029501526,
      "grad_norm": 4.2368483543396,
      "learning_rate": 4.647507629704985e-05,
      "loss": 3.5896,
      "step": 6930
    },
    {
      "epoch": 3.5300101729399795,
      "grad_norm": 7.55795431137085,
      "learning_rate": 4.646998982706002e-05,
      "loss": 3.4915,
      "step": 6940
    },
    {
      "epoch": 3.5350966429298065,
      "grad_norm": 5.70027494430542,
      "learning_rate": 4.646490335707019e-05,
      "loss": 3.5925,
      "step": 6950
    },
    {
      "epoch": 3.5401831129196335,
      "grad_norm": 5.808229446411133,
      "learning_rate": 4.645981688708037e-05,
      "loss": 3.6594,
      "step": 6960
    },
    {
      "epoch": 3.545269582909461,
      "grad_norm": 6.569754600524902,
      "learning_rate": 4.645473041709054e-05,
      "loss": 3.6093,
      "step": 6970
    },
    {
      "epoch": 3.550356052899288,
      "grad_norm": 5.768550395965576,
      "learning_rate": 4.6449643947100716e-05,
      "loss": 3.5639,
      "step": 6980
    },
    {
      "epoch": 3.555442522889115,
      "grad_norm": 5.893528938293457,
      "learning_rate": 4.644455747711089e-05,
      "loss": 3.5655,
      "step": 6990
    },
    {
      "epoch": 3.560528992878942,
      "grad_norm": 4.518820285797119,
      "learning_rate": 4.643947100712106e-05,
      "loss": 3.4665,
      "step": 7000
    },
    {
      "epoch": 3.5656154628687693,
      "grad_norm": 4.769210338592529,
      "learning_rate": 4.643438453713123e-05,
      "loss": 3.5411,
      "step": 7010
    },
    {
      "epoch": 3.5707019328585963,
      "grad_norm": 9.2518892288208,
      "learning_rate": 4.642929806714141e-05,
      "loss": 3.5721,
      "step": 7020
    },
    {
      "epoch": 3.5757884028484233,
      "grad_norm": 5.777797698974609,
      "learning_rate": 4.642421159715158e-05,
      "loss": 3.5474,
      "step": 7030
    },
    {
      "epoch": 3.5808748728382502,
      "grad_norm": 7.147435665130615,
      "learning_rate": 4.641912512716175e-05,
      "loss": 3.5347,
      "step": 7040
    },
    {
      "epoch": 3.585961342828077,
      "grad_norm": 5.864285469055176,
      "learning_rate": 4.6414038657171925e-05,
      "loss": 3.5141,
      "step": 7050
    },
    {
      "epoch": 3.591047812817904,
      "grad_norm": 6.365981101989746,
      "learning_rate": 4.6408952187182095e-05,
      "loss": 3.5434,
      "step": 7060
    },
    {
      "epoch": 3.596134282807731,
      "grad_norm": 7.023739337921143,
      "learning_rate": 4.640386571719227e-05,
      "loss": 3.5912,
      "step": 7070
    },
    {
      "epoch": 3.6012207527975586,
      "grad_norm": 4.688153266906738,
      "learning_rate": 4.639877924720244e-05,
      "loss": 3.6213,
      "step": 7080
    },
    {
      "epoch": 3.6063072227873856,
      "grad_norm": 6.201676368713379,
      "learning_rate": 4.639369277721262e-05,
      "loss": 3.5101,
      "step": 7090
    },
    {
      "epoch": 3.6113936927772126,
      "grad_norm": 6.988996982574463,
      "learning_rate": 4.638860630722279e-05,
      "loss": 3.5803,
      "step": 7100
    },
    {
      "epoch": 3.6164801627670395,
      "grad_norm": 4.757587432861328,
      "learning_rate": 4.6383519837232965e-05,
      "loss": 3.6021,
      "step": 7110
    },
    {
      "epoch": 3.621566632756867,
      "grad_norm": 6.333174705505371,
      "learning_rate": 4.6378433367243135e-05,
      "loss": 3.5773,
      "step": 7120
    },
    {
      "epoch": 3.626653102746694,
      "grad_norm": 4.091790199279785,
      "learning_rate": 4.6373346897253305e-05,
      "loss": 3.6065,
      "step": 7130
    },
    {
      "epoch": 3.631739572736521,
      "grad_norm": 5.441139221191406,
      "learning_rate": 4.636826042726348e-05,
      "loss": 3.5047,
      "step": 7140
    },
    {
      "epoch": 3.636826042726348,
      "grad_norm": 6.429261684417725,
      "learning_rate": 4.636317395727365e-05,
      "loss": 3.5854,
      "step": 7150
    },
    {
      "epoch": 3.641912512716175,
      "grad_norm": 5.253026008605957,
      "learning_rate": 4.635808748728383e-05,
      "loss": 3.501,
      "step": 7160
    },
    {
      "epoch": 3.646998982706002,
      "grad_norm": 6.200937271118164,
      "learning_rate": 4.6353001017294e-05,
      "loss": 3.5381,
      "step": 7170
    },
    {
      "epoch": 3.652085452695829,
      "grad_norm": 7.587273120880127,
      "learning_rate": 4.6347914547304175e-05,
      "loss": 3.5828,
      "step": 7180
    },
    {
      "epoch": 3.6571719226856563,
      "grad_norm": 11.000535011291504,
      "learning_rate": 4.634282807731435e-05,
      "loss": 3.5786,
      "step": 7190
    },
    {
      "epoch": 3.6622583926754833,
      "grad_norm": 6.3573198318481445,
      "learning_rate": 4.633774160732452e-05,
      "loss": 3.5805,
      "step": 7200
    },
    {
      "epoch": 3.6673448626653102,
      "grad_norm": 5.1232523918151855,
      "learning_rate": 4.633265513733469e-05,
      "loss": 3.5712,
      "step": 7210
    },
    {
      "epoch": 3.672431332655137,
      "grad_norm": 5.091216564178467,
      "learning_rate": 4.632756866734487e-05,
      "loss": 3.597,
      "step": 7220
    },
    {
      "epoch": 3.6775178026449646,
      "grad_norm": 6.637453079223633,
      "learning_rate": 4.632248219735504e-05,
      "loss": 3.5548,
      "step": 7230
    },
    {
      "epoch": 3.6826042726347916,
      "grad_norm": 5.850083827972412,
      "learning_rate": 4.631739572736521e-05,
      "loss": 3.5054,
      "step": 7240
    },
    {
      "epoch": 3.6876907426246186,
      "grad_norm": 5.623889923095703,
      "learning_rate": 4.6312309257375384e-05,
      "loss": 3.587,
      "step": 7250
    },
    {
      "epoch": 3.6927772126144456,
      "grad_norm": 6.446109771728516,
      "learning_rate": 4.6307222787385554e-05,
      "loss": 3.5791,
      "step": 7260
    },
    {
      "epoch": 3.6978636826042726,
      "grad_norm": 5.402917861938477,
      "learning_rate": 4.630213631739573e-05,
      "loss": 3.5435,
      "step": 7270
    },
    {
      "epoch": 3.7029501525940995,
      "grad_norm": 5.547103404998779,
      "learning_rate": 4.629704984740591e-05,
      "loss": 3.5471,
      "step": 7280
    },
    {
      "epoch": 3.7080366225839265,
      "grad_norm": 8.38454818725586,
      "learning_rate": 4.629196337741608e-05,
      "loss": 3.5268,
      "step": 7290
    },
    {
      "epoch": 3.713123092573754,
      "grad_norm": 6.1707305908203125,
      "learning_rate": 4.628687690742625e-05,
      "loss": 3.5909,
      "step": 7300
    },
    {
      "epoch": 3.718209562563581,
      "grad_norm": 7.111429691314697,
      "learning_rate": 4.6281790437436424e-05,
      "loss": 3.5725,
      "step": 7310
    },
    {
      "epoch": 3.723296032553408,
      "grad_norm": 4.615469932556152,
      "learning_rate": 4.6276703967446594e-05,
      "loss": 3.6313,
      "step": 7320
    },
    {
      "epoch": 3.728382502543235,
      "grad_norm": 6.948646068572998,
      "learning_rate": 4.6271617497456764e-05,
      "loss": 3.584,
      "step": 7330
    },
    {
      "epoch": 3.7334689725330623,
      "grad_norm": 4.330513000488281,
      "learning_rate": 4.626653102746694e-05,
      "loss": 3.5477,
      "step": 7340
    },
    {
      "epoch": 3.7385554425228893,
      "grad_norm": 5.677266597747803,
      "learning_rate": 4.626144455747711e-05,
      "loss": 3.6447,
      "step": 7350
    },
    {
      "epoch": 3.7436419125127163,
      "grad_norm": 6.416383266448975,
      "learning_rate": 4.625635808748729e-05,
      "loss": 3.5246,
      "step": 7360
    },
    {
      "epoch": 3.7487283825025433,
      "grad_norm": 4.948240280151367,
      "learning_rate": 4.6251271617497464e-05,
      "loss": 3.5381,
      "step": 7370
    },
    {
      "epoch": 3.7538148524923702,
      "grad_norm": 5.02435827255249,
      "learning_rate": 4.6246185147507634e-05,
      "loss": 3.5443,
      "step": 7380
    },
    {
      "epoch": 3.758901322482197,
      "grad_norm": 4.8628153800964355,
      "learning_rate": 4.6241098677517803e-05,
      "loss": 3.5125,
      "step": 7390
    },
    {
      "epoch": 3.763987792472024,
      "grad_norm": 5.5221381187438965,
      "learning_rate": 4.623601220752798e-05,
      "loss": 3.5262,
      "step": 7400
    },
    {
      "epoch": 3.7690742624618516,
      "grad_norm": 9.55009651184082,
      "learning_rate": 4.623092573753815e-05,
      "loss": 3.5853,
      "step": 7410
    },
    {
      "epoch": 3.7741607324516786,
      "grad_norm": 5.79356050491333,
      "learning_rate": 4.622583926754832e-05,
      "loss": 3.5956,
      "step": 7420
    },
    {
      "epoch": 3.7792472024415056,
      "grad_norm": 4.877356052398682,
      "learning_rate": 4.6220752797558497e-05,
      "loss": 3.5158,
      "step": 7430
    },
    {
      "epoch": 3.7843336724313326,
      "grad_norm": 5.197232246398926,
      "learning_rate": 4.6215666327568666e-05,
      "loss": 3.6362,
      "step": 7440
    },
    {
      "epoch": 3.78942014242116,
      "grad_norm": 5.297779083251953,
      "learning_rate": 4.621057985757884e-05,
      "loss": 3.5105,
      "step": 7450
    },
    {
      "epoch": 3.794506612410987,
      "grad_norm": 5.04917049407959,
      "learning_rate": 4.620549338758901e-05,
      "loss": 3.5439,
      "step": 7460
    },
    {
      "epoch": 3.799593082400814,
      "grad_norm": 5.650892734527588,
      "learning_rate": 4.620040691759919e-05,
      "loss": 3.4941,
      "step": 7470
    },
    {
      "epoch": 3.804679552390641,
      "grad_norm": 5.529642581939697,
      "learning_rate": 4.6195320447609366e-05,
      "loss": 3.5082,
      "step": 7480
    },
    {
      "epoch": 3.809766022380468,
      "grad_norm": 5.204840660095215,
      "learning_rate": 4.6190233977619536e-05,
      "loss": 3.5312,
      "step": 7490
    },
    {
      "epoch": 3.814852492370295,
      "grad_norm": 5.493813991546631,
      "learning_rate": 4.6185147507629706e-05,
      "loss": 3.5126,
      "step": 7500
    },
    {
      "epoch": 3.819938962360122,
      "grad_norm": 6.871865749359131,
      "learning_rate": 4.618006103763988e-05,
      "loss": 3.5112,
      "step": 7510
    },
    {
      "epoch": 3.8250254323499493,
      "grad_norm": 6.430351734161377,
      "learning_rate": 4.617497456765005e-05,
      "loss": 3.5615,
      "step": 7520
    },
    {
      "epoch": 3.8301119023397763,
      "grad_norm": 5.755605697631836,
      "learning_rate": 4.616988809766022e-05,
      "loss": 3.5298,
      "step": 7530
    },
    {
      "epoch": 3.8351983723296033,
      "grad_norm": 5.163219928741455,
      "learning_rate": 4.61648016276704e-05,
      "loss": 3.5213,
      "step": 7540
    },
    {
      "epoch": 3.8402848423194302,
      "grad_norm": 5.366949081420898,
      "learning_rate": 4.615971515768057e-05,
      "loss": 3.5481,
      "step": 7550
    },
    {
      "epoch": 3.845371312309257,
      "grad_norm": 7.023998737335205,
      "learning_rate": 4.6154628687690746e-05,
      "loss": 3.6106,
      "step": 7560
    },
    {
      "epoch": 3.8504577822990846,
      "grad_norm": 5.381234645843506,
      "learning_rate": 4.614954221770092e-05,
      "loss": 3.5839,
      "step": 7570
    },
    {
      "epoch": 3.8555442522889116,
      "grad_norm": 9.15304183959961,
      "learning_rate": 4.614445574771109e-05,
      "loss": 3.4529,
      "step": 7580
    },
    {
      "epoch": 3.8606307222787386,
      "grad_norm": 4.431382179260254,
      "learning_rate": 4.613936927772126e-05,
      "loss": 3.5682,
      "step": 7590
    },
    {
      "epoch": 3.8657171922685656,
      "grad_norm": 6.945425510406494,
      "learning_rate": 4.613428280773144e-05,
      "loss": 3.5706,
      "step": 7600
    },
    {
      "epoch": 3.8708036622583926,
      "grad_norm": 9.154135704040527,
      "learning_rate": 4.612919633774161e-05,
      "loss": 3.5444,
      "step": 7610
    },
    {
      "epoch": 3.8758901322482195,
      "grad_norm": 7.302424430847168,
      "learning_rate": 4.612410986775178e-05,
      "loss": 3.5419,
      "step": 7620
    },
    {
      "epoch": 3.8809766022380465,
      "grad_norm": 5.929433345794678,
      "learning_rate": 4.6119023397761955e-05,
      "loss": 3.5432,
      "step": 7630
    },
    {
      "epoch": 3.886063072227874,
      "grad_norm": 6.965738296508789,
      "learning_rate": 4.6113936927772125e-05,
      "loss": 3.5536,
      "step": 7640
    },
    {
      "epoch": 3.891149542217701,
      "grad_norm": 6.285124778747559,
      "learning_rate": 4.61088504577823e-05,
      "loss": 3.4841,
      "step": 7650
    },
    {
      "epoch": 3.896236012207528,
      "grad_norm": 6.06207275390625,
      "learning_rate": 4.610376398779248e-05,
      "loss": 3.6346,
      "step": 7660
    },
    {
      "epoch": 3.901322482197355,
      "grad_norm": 4.360729694366455,
      "learning_rate": 4.609867751780265e-05,
      "loss": 3.5855,
      "step": 7670
    },
    {
      "epoch": 3.9064089521871823,
      "grad_norm": 6.817298889160156,
      "learning_rate": 4.609359104781282e-05,
      "loss": 3.5162,
      "step": 7680
    },
    {
      "epoch": 3.9114954221770093,
      "grad_norm": 5.378087997436523,
      "learning_rate": 4.6088504577822995e-05,
      "loss": 3.5715,
      "step": 7690
    },
    {
      "epoch": 3.9165818921668363,
      "grad_norm": 5.7994489669799805,
      "learning_rate": 4.6083418107833165e-05,
      "loss": 3.5501,
      "step": 7700
    },
    {
      "epoch": 3.9216683621566633,
      "grad_norm": 6.699179172515869,
      "learning_rate": 4.607833163784334e-05,
      "loss": 3.5948,
      "step": 7710
    },
    {
      "epoch": 3.9267548321464902,
      "grad_norm": 6.175057888031006,
      "learning_rate": 4.607324516785351e-05,
      "loss": 3.5615,
      "step": 7720
    },
    {
      "epoch": 3.931841302136317,
      "grad_norm": 6.6745452880859375,
      "learning_rate": 4.606815869786368e-05,
      "loss": 3.4739,
      "step": 7730
    },
    {
      "epoch": 3.936927772126144,
      "grad_norm": 6.519194602966309,
      "learning_rate": 4.606307222787386e-05,
      "loss": 3.5052,
      "step": 7740
    },
    {
      "epoch": 3.9420142421159716,
      "grad_norm": 5.105235576629639,
      "learning_rate": 4.605798575788403e-05,
      "loss": 3.5508,
      "step": 7750
    },
    {
      "epoch": 3.9471007121057986,
      "grad_norm": 7.490808486938477,
      "learning_rate": 4.6052899287894205e-05,
      "loss": 3.4862,
      "step": 7760
    },
    {
      "epoch": 3.9521871820956256,
      "grad_norm": 6.9258294105529785,
      "learning_rate": 4.604781281790438e-05,
      "loss": 3.4812,
      "step": 7770
    },
    {
      "epoch": 3.9572736520854526,
      "grad_norm": 4.654047012329102,
      "learning_rate": 4.604272634791455e-05,
      "loss": 3.525,
      "step": 7780
    },
    {
      "epoch": 3.96236012207528,
      "grad_norm": 6.667936325073242,
      "learning_rate": 4.603763987792472e-05,
      "loss": 3.5566,
      "step": 7790
    },
    {
      "epoch": 3.967446592065107,
      "grad_norm": 6.412913799285889,
      "learning_rate": 4.60325534079349e-05,
      "loss": 3.4717,
      "step": 7800
    },
    {
      "epoch": 3.972533062054934,
      "grad_norm": 5.949088096618652,
      "learning_rate": 4.602746693794507e-05,
      "loss": 3.5673,
      "step": 7810
    },
    {
      "epoch": 3.977619532044761,
      "grad_norm": 4.598972797393799,
      "learning_rate": 4.602238046795524e-05,
      "loss": 3.5397,
      "step": 7820
    },
    {
      "epoch": 3.982706002034588,
      "grad_norm": 7.5904130935668945,
      "learning_rate": 4.6017293997965414e-05,
      "loss": 3.5587,
      "step": 7830
    },
    {
      "epoch": 3.987792472024415,
      "grad_norm": 6.036080837249756,
      "learning_rate": 4.6012207527975584e-05,
      "loss": 3.5251,
      "step": 7840
    },
    {
      "epoch": 3.992878942014242,
      "grad_norm": 5.858404636383057,
      "learning_rate": 4.600712105798576e-05,
      "loss": 3.5263,
      "step": 7850
    },
    {
      "epoch": 3.9979654120040693,
      "grad_norm": 6.2696332931518555,
      "learning_rate": 4.600203458799594e-05,
      "loss": 3.4358,
      "step": 7860
    },
    {
      "epoch": 4.0,
      "eval_loss": 3.6870806217193604,
      "eval_runtime": 2.6793,
      "eval_samples_per_second": 1035.721,
      "eval_steps_per_second": 129.512,
      "step": 7864
    },
    {
      "epoch": 4.003051881993896,
      "grad_norm": 5.84808874130249,
      "learning_rate": 4.599694811800611e-05,
      "loss": 3.5769,
      "step": 7870
    },
    {
      "epoch": 4.008138351983724,
      "grad_norm": 6.910951137542725,
      "learning_rate": 4.599186164801628e-05,
      "loss": 3.5624,
      "step": 7880
    },
    {
      "epoch": 4.013224821973551,
      "grad_norm": 7.399456977844238,
      "learning_rate": 4.5986775178026454e-05,
      "loss": 3.5466,
      "step": 7890
    },
    {
      "epoch": 4.018311291963378,
      "grad_norm": 6.991793155670166,
      "learning_rate": 4.5981688708036624e-05,
      "loss": 3.5271,
      "step": 7900
    },
    {
      "epoch": 4.023397761953205,
      "grad_norm": 6.519759178161621,
      "learning_rate": 4.5976602238046794e-05,
      "loss": 3.4815,
      "step": 7910
    },
    {
      "epoch": 4.028484231943032,
      "grad_norm": 6.074951171875,
      "learning_rate": 4.597151576805697e-05,
      "loss": 3.5704,
      "step": 7920
    },
    {
      "epoch": 4.033570701932859,
      "grad_norm": 5.455128192901611,
      "learning_rate": 4.596642929806714e-05,
      "loss": 3.5322,
      "step": 7930
    },
    {
      "epoch": 4.038657171922686,
      "grad_norm": 5.989661693572998,
      "learning_rate": 4.596134282807732e-05,
      "loss": 3.4652,
      "step": 7940
    },
    {
      "epoch": 4.043743641912513,
      "grad_norm": 6.176759243011475,
      "learning_rate": 4.5956256358087494e-05,
      "loss": 3.5253,
      "step": 7950
    },
    {
      "epoch": 4.0488301119023395,
      "grad_norm": 6.516461372375488,
      "learning_rate": 4.5951169888097664e-05,
      "loss": 3.5593,
      "step": 7960
    },
    {
      "epoch": 4.0539165818921665,
      "grad_norm": 5.6406402587890625,
      "learning_rate": 4.594608341810784e-05,
      "loss": 3.5227,
      "step": 7970
    },
    {
      "epoch": 4.0590030518819935,
      "grad_norm": 7.355774402618408,
      "learning_rate": 4.594099694811801e-05,
      "loss": 3.4906,
      "step": 7980
    },
    {
      "epoch": 4.064089521871821,
      "grad_norm": 7.388492584228516,
      "learning_rate": 4.593591047812818e-05,
      "loss": 3.4895,
      "step": 7990
    },
    {
      "epoch": 4.069175991861648,
      "grad_norm": 7.947268962860107,
      "learning_rate": 4.593082400813836e-05,
      "loss": 3.5441,
      "step": 8000
    },
    {
      "epoch": 4.074262461851475,
      "grad_norm": 7.20912504196167,
      "learning_rate": 4.5925737538148527e-05,
      "loss": 3.5519,
      "step": 8010
    },
    {
      "epoch": 4.079348931841302,
      "grad_norm": 6.576728820800781,
      "learning_rate": 4.5920651068158696e-05,
      "loss": 3.5611,
      "step": 8020
    },
    {
      "epoch": 4.084435401831129,
      "grad_norm": 6.395181655883789,
      "learning_rate": 4.591556459816887e-05,
      "loss": 3.5403,
      "step": 8030
    },
    {
      "epoch": 4.089521871820956,
      "grad_norm": 5.22281551361084,
      "learning_rate": 4.591047812817904e-05,
      "loss": 3.5103,
      "step": 8040
    },
    {
      "epoch": 4.094608341810783,
      "grad_norm": 7.34916877746582,
      "learning_rate": 4.590539165818922e-05,
      "loss": 3.5208,
      "step": 8050
    },
    {
      "epoch": 4.09969481180061,
      "grad_norm": 5.591099262237549,
      "learning_rate": 4.5900305188199396e-05,
      "loss": 3.5609,
      "step": 8060
    },
    {
      "epoch": 4.104781281790437,
      "grad_norm": 5.333564758300781,
      "learning_rate": 4.5895218718209566e-05,
      "loss": 3.5316,
      "step": 8070
    },
    {
      "epoch": 4.109867751780264,
      "grad_norm": 6.185909271240234,
      "learning_rate": 4.5890132248219736e-05,
      "loss": 3.4652,
      "step": 8080
    },
    {
      "epoch": 4.114954221770091,
      "grad_norm": 5.670868873596191,
      "learning_rate": 4.588504577822991e-05,
      "loss": 3.5385,
      "step": 8090
    },
    {
      "epoch": 4.120040691759919,
      "grad_norm": 5.862897872924805,
      "learning_rate": 4.587995930824008e-05,
      "loss": 3.4664,
      "step": 8100
    },
    {
      "epoch": 4.125127161749746,
      "grad_norm": 5.833090782165527,
      "learning_rate": 4.587487283825025e-05,
      "loss": 3.5046,
      "step": 8110
    },
    {
      "epoch": 4.130213631739573,
      "grad_norm": 6.568278789520264,
      "learning_rate": 4.586978636826043e-05,
      "loss": 3.5422,
      "step": 8120
    },
    {
      "epoch": 4.1353001017294,
      "grad_norm": 8.386506080627441,
      "learning_rate": 4.58646998982706e-05,
      "loss": 3.4919,
      "step": 8130
    },
    {
      "epoch": 4.140386571719227,
      "grad_norm": 5.642545223236084,
      "learning_rate": 4.5859613428280776e-05,
      "loss": 3.552,
      "step": 8140
    },
    {
      "epoch": 4.145473041709054,
      "grad_norm": 6.534317493438721,
      "learning_rate": 4.585452695829095e-05,
      "loss": 3.4837,
      "step": 8150
    },
    {
      "epoch": 4.150559511698881,
      "grad_norm": 7.1066179275512695,
      "learning_rate": 4.584944048830112e-05,
      "loss": 3.4524,
      "step": 8160
    },
    {
      "epoch": 4.155645981688708,
      "grad_norm": 5.316425323486328,
      "learning_rate": 4.584435401831129e-05,
      "loss": 3.5209,
      "step": 8170
    },
    {
      "epoch": 4.160732451678535,
      "grad_norm": 6.180997371673584,
      "learning_rate": 4.583926754832147e-05,
      "loss": 3.5224,
      "step": 8180
    },
    {
      "epoch": 4.165818921668362,
      "grad_norm": 6.601441860198975,
      "learning_rate": 4.583418107833164e-05,
      "loss": 3.5603,
      "step": 8190
    },
    {
      "epoch": 4.170905391658189,
      "grad_norm": 5.998661994934082,
      "learning_rate": 4.582909460834181e-05,
      "loss": 3.5143,
      "step": 8200
    },
    {
      "epoch": 4.175991861648017,
      "grad_norm": 5.674927711486816,
      "learning_rate": 4.5824008138351985e-05,
      "loss": 3.4611,
      "step": 8210
    },
    {
      "epoch": 4.181078331637844,
      "grad_norm": 6.2485671043396,
      "learning_rate": 4.5818921668362155e-05,
      "loss": 3.4643,
      "step": 8220
    },
    {
      "epoch": 4.186164801627671,
      "grad_norm": 7.158141613006592,
      "learning_rate": 4.581383519837233e-05,
      "loss": 3.4639,
      "step": 8230
    },
    {
      "epoch": 4.191251271617498,
      "grad_norm": 6.6262431144714355,
      "learning_rate": 4.580874872838251e-05,
      "loss": 3.4649,
      "step": 8240
    },
    {
      "epoch": 4.196337741607325,
      "grad_norm": 7.603287220001221,
      "learning_rate": 4.580366225839268e-05,
      "loss": 3.5326,
      "step": 8250
    },
    {
      "epoch": 4.201424211597152,
      "grad_norm": 6.2046403884887695,
      "learning_rate": 4.5798575788402855e-05,
      "loss": 3.4912,
      "step": 8260
    },
    {
      "epoch": 4.206510681586979,
      "grad_norm": 6.46549654006958,
      "learning_rate": 4.5793489318413025e-05,
      "loss": 3.4364,
      "step": 8270
    },
    {
      "epoch": 4.211597151576806,
      "grad_norm": 9.815648078918457,
      "learning_rate": 4.5788402848423195e-05,
      "loss": 3.4726,
      "step": 8280
    },
    {
      "epoch": 4.216683621566633,
      "grad_norm": 6.973529815673828,
      "learning_rate": 4.578331637843337e-05,
      "loss": 3.5368,
      "step": 8290
    },
    {
      "epoch": 4.2217700915564595,
      "grad_norm": 6.640503883361816,
      "learning_rate": 4.577822990844354e-05,
      "loss": 3.5042,
      "step": 8300
    },
    {
      "epoch": 4.2268565615462865,
      "grad_norm": 6.75279426574707,
      "learning_rate": 4.577314343845371e-05,
      "loss": 3.4623,
      "step": 8310
    },
    {
      "epoch": 4.2319430315361135,
      "grad_norm": 7.035434722900391,
      "learning_rate": 4.576805696846389e-05,
      "loss": 3.4708,
      "step": 8320
    },
    {
      "epoch": 4.237029501525941,
      "grad_norm": 8.213186264038086,
      "learning_rate": 4.5762970498474065e-05,
      "loss": 3.461,
      "step": 8330
    },
    {
      "epoch": 4.242115971515768,
      "grad_norm": 7.839914798736572,
      "learning_rate": 4.5757884028484235e-05,
      "loss": 3.4438,
      "step": 8340
    },
    {
      "epoch": 4.247202441505595,
      "grad_norm": 6.409915924072266,
      "learning_rate": 4.575279755849441e-05,
      "loss": 3.4713,
      "step": 8350
    },
    {
      "epoch": 4.252288911495422,
      "grad_norm": 7.141901016235352,
      "learning_rate": 4.574771108850458e-05,
      "loss": 3.5132,
      "step": 8360
    },
    {
      "epoch": 4.257375381485249,
      "grad_norm": 9.487360000610352,
      "learning_rate": 4.574262461851475e-05,
      "loss": 3.4465,
      "step": 8370
    },
    {
      "epoch": 4.262461851475076,
      "grad_norm": 6.028769493103027,
      "learning_rate": 4.573753814852493e-05,
      "loss": 3.5287,
      "step": 8380
    },
    {
      "epoch": 4.267548321464903,
      "grad_norm": 8.045287132263184,
      "learning_rate": 4.57324516785351e-05,
      "loss": 3.4913,
      "step": 8390
    },
    {
      "epoch": 4.27263479145473,
      "grad_norm": 7.016541957855225,
      "learning_rate": 4.572736520854527e-05,
      "loss": 3.5024,
      "step": 8400
    },
    {
      "epoch": 4.277721261444557,
      "grad_norm": 8.580831527709961,
      "learning_rate": 4.5722278738555444e-05,
      "loss": 3.4194,
      "step": 8410
    },
    {
      "epoch": 4.282807731434384,
      "grad_norm": 6.608240127563477,
      "learning_rate": 4.5717192268565614e-05,
      "loss": 3.5327,
      "step": 8420
    },
    {
      "epoch": 4.287894201424211,
      "grad_norm": 6.825939178466797,
      "learning_rate": 4.571210579857579e-05,
      "loss": 3.5088,
      "step": 8430
    },
    {
      "epoch": 4.292980671414039,
      "grad_norm": 7.93143367767334,
      "learning_rate": 4.570701932858597e-05,
      "loss": 3.5188,
      "step": 8440
    },
    {
      "epoch": 4.298067141403866,
      "grad_norm": 7.117186546325684,
      "learning_rate": 4.570193285859614e-05,
      "loss": 3.3578,
      "step": 8450
    },
    {
      "epoch": 4.303153611393693,
      "grad_norm": 6.295094013214111,
      "learning_rate": 4.569684638860631e-05,
      "loss": 3.5015,
      "step": 8460
    },
    {
      "epoch": 4.30824008138352,
      "grad_norm": 7.897289752960205,
      "learning_rate": 4.5691759918616484e-05,
      "loss": 3.4597,
      "step": 8470
    },
    {
      "epoch": 4.313326551373347,
      "grad_norm": 6.831140041351318,
      "learning_rate": 4.5686673448626654e-05,
      "loss": 3.4947,
      "step": 8480
    },
    {
      "epoch": 4.318413021363174,
      "grad_norm": 9.467386245727539,
      "learning_rate": 4.5681586978636824e-05,
      "loss": 3.5195,
      "step": 8490
    },
    {
      "epoch": 4.323499491353001,
      "grad_norm": 7.796557426452637,
      "learning_rate": 4.5676500508647e-05,
      "loss": 3.4879,
      "step": 8500
    },
    {
      "epoch": 4.328585961342828,
      "grad_norm": 6.665657997131348,
      "learning_rate": 4.567141403865717e-05,
      "loss": 3.5127,
      "step": 8510
    },
    {
      "epoch": 4.333672431332655,
      "grad_norm": 6.197167873382568,
      "learning_rate": 4.566632756866735e-05,
      "loss": 3.4602,
      "step": 8520
    },
    {
      "epoch": 4.338758901322482,
      "grad_norm": 7.088810443878174,
      "learning_rate": 4.5661241098677524e-05,
      "loss": 3.4184,
      "step": 8530
    },
    {
      "epoch": 4.343845371312309,
      "grad_norm": 7.874248027801514,
      "learning_rate": 4.5656154628687694e-05,
      "loss": 3.4504,
      "step": 8540
    },
    {
      "epoch": 4.348931841302137,
      "grad_norm": 6.594128131866455,
      "learning_rate": 4.565106815869787e-05,
      "loss": 3.5293,
      "step": 8550
    },
    {
      "epoch": 4.354018311291964,
      "grad_norm": 7.080340385437012,
      "learning_rate": 4.564598168870804e-05,
      "loss": 3.434,
      "step": 8560
    },
    {
      "epoch": 4.359104781281791,
      "grad_norm": 5.372812271118164,
      "learning_rate": 4.564089521871821e-05,
      "loss": 3.5188,
      "step": 8570
    },
    {
      "epoch": 4.364191251271618,
      "grad_norm": 6.204227447509766,
      "learning_rate": 4.563580874872839e-05,
      "loss": 3.4287,
      "step": 8580
    },
    {
      "epoch": 4.369277721261445,
      "grad_norm": 8.148781776428223,
      "learning_rate": 4.5630722278738557e-05,
      "loss": 3.4803,
      "step": 8590
    },
    {
      "epoch": 4.374364191251272,
      "grad_norm": 6.666170597076416,
      "learning_rate": 4.5625635808748726e-05,
      "loss": 3.4848,
      "step": 8600
    },
    {
      "epoch": 4.379450661241099,
      "grad_norm": 8.510690689086914,
      "learning_rate": 4.56205493387589e-05,
      "loss": 3.4648,
      "step": 8610
    },
    {
      "epoch": 4.384537131230926,
      "grad_norm": 8.282868385314941,
      "learning_rate": 4.561546286876908e-05,
      "loss": 3.4719,
      "step": 8620
    },
    {
      "epoch": 4.389623601220753,
      "grad_norm": 5.057900428771973,
      "learning_rate": 4.561037639877925e-05,
      "loss": 3.478,
      "step": 8630
    },
    {
      "epoch": 4.3947100712105795,
      "grad_norm": 8.918657302856445,
      "learning_rate": 4.5605289928789426e-05,
      "loss": 3.4614,
      "step": 8640
    },
    {
      "epoch": 4.3997965412004065,
      "grad_norm": 7.845905780792236,
      "learning_rate": 4.5600203458799596e-05,
      "loss": 3.4778,
      "step": 8650
    },
    {
      "epoch": 4.404883011190234,
      "grad_norm": 9.447675704956055,
      "learning_rate": 4.5595116988809766e-05,
      "loss": 3.5066,
      "step": 8660
    },
    {
      "epoch": 4.409969481180061,
      "grad_norm": 5.6529154777526855,
      "learning_rate": 4.559003051881994e-05,
      "loss": 3.3809,
      "step": 8670
    },
    {
      "epoch": 4.415055951169888,
      "grad_norm": 9.205937385559082,
      "learning_rate": 4.558494404883011e-05,
      "loss": 3.458,
      "step": 8680
    },
    {
      "epoch": 4.420142421159715,
      "grad_norm": 6.617910861968994,
      "learning_rate": 4.557985757884028e-05,
      "loss": 3.43,
      "step": 8690
    },
    {
      "epoch": 4.425228891149542,
      "grad_norm": 7.692637920379639,
      "learning_rate": 4.557477110885046e-05,
      "loss": 3.5498,
      "step": 8700
    },
    {
      "epoch": 4.430315361139369,
      "grad_norm": 8.768918991088867,
      "learning_rate": 4.556968463886063e-05,
      "loss": 3.4414,
      "step": 8710
    },
    {
      "epoch": 4.435401831129196,
      "grad_norm": 8.039145469665527,
      "learning_rate": 4.5564598168870806e-05,
      "loss": 3.4992,
      "step": 8720
    },
    {
      "epoch": 4.440488301119023,
      "grad_norm": 8.133491516113281,
      "learning_rate": 4.555951169888098e-05,
      "loss": 3.4608,
      "step": 8730
    },
    {
      "epoch": 4.44557477110885,
      "grad_norm": 6.4011149406433105,
      "learning_rate": 4.555442522889115e-05,
      "loss": 3.4328,
      "step": 8740
    },
    {
      "epoch": 4.450661241098677,
      "grad_norm": 6.637654781341553,
      "learning_rate": 4.554933875890132e-05,
      "loss": 3.3785,
      "step": 8750
    },
    {
      "epoch": 4.455747711088504,
      "grad_norm": 7.677149295806885,
      "learning_rate": 4.55442522889115e-05,
      "loss": 3.4849,
      "step": 8760
    },
    {
      "epoch": 4.460834181078331,
      "grad_norm": 7.639240741729736,
      "learning_rate": 4.553916581892167e-05,
      "loss": 3.4859,
      "step": 8770
    },
    {
      "epoch": 4.465920651068159,
      "grad_norm": 8.05338191986084,
      "learning_rate": 4.5534079348931846e-05,
      "loss": 3.4196,
      "step": 8780
    },
    {
      "epoch": 4.471007121057986,
      "grad_norm": 6.806941986083984,
      "learning_rate": 4.5528992878942015e-05,
      "loss": 3.5578,
      "step": 8790
    },
    {
      "epoch": 4.476093591047813,
      "grad_norm": 7.233952045440674,
      "learning_rate": 4.5523906408952185e-05,
      "loss": 3.4033,
      "step": 8800
    },
    {
      "epoch": 4.48118006103764,
      "grad_norm": 9.229225158691406,
      "learning_rate": 4.551881993896236e-05,
      "loss": 3.4055,
      "step": 8810
    },
    {
      "epoch": 4.486266531027467,
      "grad_norm": 8.897475242614746,
      "learning_rate": 4.551373346897254e-05,
      "loss": 3.4048,
      "step": 8820
    },
    {
      "epoch": 4.491353001017294,
      "grad_norm": 7.134532928466797,
      "learning_rate": 4.550864699898271e-05,
      "loss": 3.5022,
      "step": 8830
    },
    {
      "epoch": 4.496439471007121,
      "grad_norm": 6.268679618835449,
      "learning_rate": 4.5503560528992885e-05,
      "loss": 3.4535,
      "step": 8840
    },
    {
      "epoch": 4.501525940996948,
      "grad_norm": 8.39194107055664,
      "learning_rate": 4.5498474059003055e-05,
      "loss": 3.4864,
      "step": 8850
    },
    {
      "epoch": 4.506612410986775,
      "grad_norm": 9.036888122558594,
      "learning_rate": 4.5493387589013225e-05,
      "loss": 3.4852,
      "step": 8860
    },
    {
      "epoch": 4.511698880976602,
      "grad_norm": 6.8496503829956055,
      "learning_rate": 4.54883011190234e-05,
      "loss": 3.4557,
      "step": 8870
    },
    {
      "epoch": 4.51678535096643,
      "grad_norm": 8.66197395324707,
      "learning_rate": 4.548321464903357e-05,
      "loss": 3.4212,
      "step": 8880
    },
    {
      "epoch": 4.521871820956257,
      "grad_norm": 9.332901954650879,
      "learning_rate": 4.547812817904374e-05,
      "loss": 3.5132,
      "step": 8890
    },
    {
      "epoch": 4.526958290946084,
      "grad_norm": 7.599575519561768,
      "learning_rate": 4.547304170905392e-05,
      "loss": 3.4446,
      "step": 8900
    },
    {
      "epoch": 4.532044760935911,
      "grad_norm": 7.490322113037109,
      "learning_rate": 4.5467955239064095e-05,
      "loss": 3.462,
      "step": 8910
    },
    {
      "epoch": 4.537131230925738,
      "grad_norm": 8.404818534851074,
      "learning_rate": 4.5462868769074265e-05,
      "loss": 3.4751,
      "step": 8920
    },
    {
      "epoch": 4.542217700915565,
      "grad_norm": 6.408419609069824,
      "learning_rate": 4.545778229908444e-05,
      "loss": 3.4726,
      "step": 8930
    },
    {
      "epoch": 4.547304170905392,
      "grad_norm": 5.526950359344482,
      "learning_rate": 4.545269582909461e-05,
      "loss": 3.4758,
      "step": 8940
    },
    {
      "epoch": 4.552390640895219,
      "grad_norm": 8.67048168182373,
      "learning_rate": 4.544760935910478e-05,
      "loss": 3.4411,
      "step": 8950
    },
    {
      "epoch": 4.557477110885046,
      "grad_norm": 7.330310821533203,
      "learning_rate": 4.544252288911496e-05,
      "loss": 3.4614,
      "step": 8960
    },
    {
      "epoch": 4.562563580874873,
      "grad_norm": 9.197126388549805,
      "learning_rate": 4.543743641912513e-05,
      "loss": 3.5036,
      "step": 8970
    },
    {
      "epoch": 4.5676500508646996,
      "grad_norm": 6.245562553405762,
      "learning_rate": 4.54323499491353e-05,
      "loss": 3.4846,
      "step": 8980
    },
    {
      "epoch": 4.5727365208545265,
      "grad_norm": 10.178180694580078,
      "learning_rate": 4.5427263479145474e-05,
      "loss": 3.4501,
      "step": 8990
    },
    {
      "epoch": 4.577822990844354,
      "grad_norm": 6.755914211273193,
      "learning_rate": 4.5422177009155644e-05,
      "loss": 3.4573,
      "step": 9000
    },
    {
      "epoch": 4.582909460834181,
      "grad_norm": 11.36359691619873,
      "learning_rate": 4.541709053916582e-05,
      "loss": 3.4791,
      "step": 9010
    },
    {
      "epoch": 4.587995930824008,
      "grad_norm": 6.885517120361328,
      "learning_rate": 4.5412004069176e-05,
      "loss": 3.5291,
      "step": 9020
    },
    {
      "epoch": 4.593082400813835,
      "grad_norm": 8.114654541015625,
      "learning_rate": 4.540691759918617e-05,
      "loss": 3.4486,
      "step": 9030
    },
    {
      "epoch": 4.598168870803662,
      "grad_norm": 7.776980876922607,
      "learning_rate": 4.540183112919634e-05,
      "loss": 3.3762,
      "step": 9040
    },
    {
      "epoch": 4.603255340793489,
      "grad_norm": 9.37743854522705,
      "learning_rate": 4.5396744659206514e-05,
      "loss": 3.4698,
      "step": 9050
    },
    {
      "epoch": 4.608341810783316,
      "grad_norm": 7.510167598724365,
      "learning_rate": 4.5391658189216684e-05,
      "loss": 3.4499,
      "step": 9060
    },
    {
      "epoch": 4.613428280773143,
      "grad_norm": 8.669599533081055,
      "learning_rate": 4.538657171922686e-05,
      "loss": 3.4765,
      "step": 9070
    },
    {
      "epoch": 4.61851475076297,
      "grad_norm": 7.406222820281982,
      "learning_rate": 4.538148524923703e-05,
      "loss": 3.4554,
      "step": 9080
    },
    {
      "epoch": 4.623601220752797,
      "grad_norm": 8.941712379455566,
      "learning_rate": 4.53763987792472e-05,
      "loss": 3.4292,
      "step": 9090
    },
    {
      "epoch": 4.628687690742625,
      "grad_norm": 8.585077285766602,
      "learning_rate": 4.537131230925738e-05,
      "loss": 3.4978,
      "step": 9100
    },
    {
      "epoch": 4.633774160732452,
      "grad_norm": 8.143572807312012,
      "learning_rate": 4.5366225839267554e-05,
      "loss": 3.4796,
      "step": 9110
    },
    {
      "epoch": 4.638860630722279,
      "grad_norm": 8.052303314208984,
      "learning_rate": 4.5361139369277724e-05,
      "loss": 3.4769,
      "step": 9120
    },
    {
      "epoch": 4.643947100712106,
      "grad_norm": 8.309174537658691,
      "learning_rate": 4.53560528992879e-05,
      "loss": 3.4884,
      "step": 9130
    },
    {
      "epoch": 4.649033570701933,
      "grad_norm": 7.226728439331055,
      "learning_rate": 4.535096642929807e-05,
      "loss": 3.4919,
      "step": 9140
    },
    {
      "epoch": 4.65412004069176,
      "grad_norm": 10.464227676391602,
      "learning_rate": 4.534587995930824e-05,
      "loss": 3.3851,
      "step": 9150
    },
    {
      "epoch": 4.659206510681587,
      "grad_norm": 8.3024320602417,
      "learning_rate": 4.534079348931842e-05,
      "loss": 3.4285,
      "step": 9160
    },
    {
      "epoch": 4.664292980671414,
      "grad_norm": 6.473715305328369,
      "learning_rate": 4.5335707019328587e-05,
      "loss": 3.5036,
      "step": 9170
    },
    {
      "epoch": 4.669379450661241,
      "grad_norm": 7.222052097320557,
      "learning_rate": 4.5330620549338756e-05,
      "loss": 3.4126,
      "step": 9180
    },
    {
      "epoch": 4.674465920651068,
      "grad_norm": 9.240386962890625,
      "learning_rate": 4.532553407934893e-05,
      "loss": 3.3971,
      "step": 9190
    },
    {
      "epoch": 4.679552390640895,
      "grad_norm": 7.217928409576416,
      "learning_rate": 4.532044760935911e-05,
      "loss": 3.5304,
      "step": 9200
    },
    {
      "epoch": 4.684638860630722,
      "grad_norm": 7.457093715667725,
      "learning_rate": 4.531536113936928e-05,
      "loss": 3.4025,
      "step": 9210
    },
    {
      "epoch": 4.689725330620549,
      "grad_norm": 7.842870235443115,
      "learning_rate": 4.5310274669379456e-05,
      "loss": 3.4071,
      "step": 9220
    },
    {
      "epoch": 4.694811800610377,
      "grad_norm": 6.347442150115967,
      "learning_rate": 4.5305188199389626e-05,
      "loss": 3.4498,
      "step": 9230
    },
    {
      "epoch": 4.699898270600204,
      "grad_norm": 7.595486164093018,
      "learning_rate": 4.5300101729399796e-05,
      "loss": 3.4964,
      "step": 9240
    },
    {
      "epoch": 4.704984740590031,
      "grad_norm": 7.422082901000977,
      "learning_rate": 4.529501525940997e-05,
      "loss": 3.5127,
      "step": 9250
    },
    {
      "epoch": 4.710071210579858,
      "grad_norm": 7.308662414550781,
      "learning_rate": 4.528992878942014e-05,
      "loss": 3.4028,
      "step": 9260
    },
    {
      "epoch": 4.715157680569685,
      "grad_norm": 9.710973739624023,
      "learning_rate": 4.528484231943031e-05,
      "loss": 3.4357,
      "step": 9270
    },
    {
      "epoch": 4.720244150559512,
      "grad_norm": 9.333261489868164,
      "learning_rate": 4.527975584944049e-05,
      "loss": 3.4566,
      "step": 9280
    },
    {
      "epoch": 4.725330620549339,
      "grad_norm": 7.892265319824219,
      "learning_rate": 4.5274669379450666e-05,
      "loss": 3.4917,
      "step": 9290
    },
    {
      "epoch": 4.730417090539166,
      "grad_norm": 6.595106601715088,
      "learning_rate": 4.5269582909460836e-05,
      "loss": 3.4522,
      "step": 9300
    },
    {
      "epoch": 4.735503560528993,
      "grad_norm": 6.381352424621582,
      "learning_rate": 4.526449643947101e-05,
      "loss": 3.4132,
      "step": 9310
    },
    {
      "epoch": 4.7405900305188196,
      "grad_norm": 8.090741157531738,
      "learning_rate": 4.525940996948118e-05,
      "loss": 3.4253,
      "step": 9320
    },
    {
      "epoch": 4.745676500508647,
      "grad_norm": 6.785665512084961,
      "learning_rate": 4.525432349949136e-05,
      "loss": 3.3968,
      "step": 9330
    },
    {
      "epoch": 4.750762970498474,
      "grad_norm": 6.873204708099365,
      "learning_rate": 4.524923702950153e-05,
      "loss": 3.3375,
      "step": 9340
    },
    {
      "epoch": 4.755849440488301,
      "grad_norm": 9.545615196228027,
      "learning_rate": 4.52441505595117e-05,
      "loss": 3.5353,
      "step": 9350
    },
    {
      "epoch": 4.760935910478128,
      "grad_norm": 10.11694049835205,
      "learning_rate": 4.5239064089521876e-05,
      "loss": 3.4221,
      "step": 9360
    },
    {
      "epoch": 4.766022380467955,
      "grad_norm": 11.527960777282715,
      "learning_rate": 4.5233977619532045e-05,
      "loss": 3.3752,
      "step": 9370
    },
    {
      "epoch": 4.771108850457782,
      "grad_norm": 7.576038360595703,
      "learning_rate": 4.5228891149542215e-05,
      "loss": 3.5052,
      "step": 9380
    },
    {
      "epoch": 4.776195320447609,
      "grad_norm": 7.101188659667969,
      "learning_rate": 4.522380467955239e-05,
      "loss": 3.4659,
      "step": 9390
    },
    {
      "epoch": 4.781281790437436,
      "grad_norm": 7.593052864074707,
      "learning_rate": 4.521871820956257e-05,
      "loss": 3.3832,
      "step": 9400
    },
    {
      "epoch": 4.786368260427263,
      "grad_norm": 9.175793647766113,
      "learning_rate": 4.521363173957274e-05,
      "loss": 3.4087,
      "step": 9410
    },
    {
      "epoch": 4.79145473041709,
      "grad_norm": 10.864426612854004,
      "learning_rate": 4.5208545269582915e-05,
      "loss": 3.4016,
      "step": 9420
    },
    {
      "epoch": 4.796541200406917,
      "grad_norm": 9.779570579528809,
      "learning_rate": 4.5203458799593085e-05,
      "loss": 3.4035,
      "step": 9430
    },
    {
      "epoch": 4.801627670396744,
      "grad_norm": 6.005212306976318,
      "learning_rate": 4.5198372329603255e-05,
      "loss": 3.38,
      "step": 9440
    },
    {
      "epoch": 4.806714140386572,
      "grad_norm": 9.447188377380371,
      "learning_rate": 4.519328585961343e-05,
      "loss": 3.445,
      "step": 9450
    },
    {
      "epoch": 4.811800610376399,
      "grad_norm": 7.931705474853516,
      "learning_rate": 4.51881993896236e-05,
      "loss": 3.4025,
      "step": 9460
    },
    {
      "epoch": 4.816887080366226,
      "grad_norm": 11.030824661254883,
      "learning_rate": 4.518311291963377e-05,
      "loss": 3.3486,
      "step": 9470
    },
    {
      "epoch": 4.821973550356053,
      "grad_norm": 7.932216644287109,
      "learning_rate": 4.517802644964395e-05,
      "loss": 3.4224,
      "step": 9480
    },
    {
      "epoch": 4.82706002034588,
      "grad_norm": 7.7786335945129395,
      "learning_rate": 4.5172939979654125e-05,
      "loss": 3.4066,
      "step": 9490
    },
    {
      "epoch": 4.832146490335707,
      "grad_norm": 6.774055480957031,
      "learning_rate": 4.5167853509664295e-05,
      "loss": 3.4878,
      "step": 9500
    },
    {
      "epoch": 4.837232960325534,
      "grad_norm": 6.631634712219238,
      "learning_rate": 4.516276703967447e-05,
      "loss": 3.4049,
      "step": 9510
    },
    {
      "epoch": 4.842319430315361,
      "grad_norm": 8.098694801330566,
      "learning_rate": 4.515768056968464e-05,
      "loss": 3.4764,
      "step": 9520
    },
    {
      "epoch": 4.847405900305188,
      "grad_norm": 5.958906173706055,
      "learning_rate": 4.515259409969481e-05,
      "loss": 3.4607,
      "step": 9530
    },
    {
      "epoch": 4.852492370295015,
      "grad_norm": 9.672557830810547,
      "learning_rate": 4.514750762970499e-05,
      "loss": 3.3551,
      "step": 9540
    },
    {
      "epoch": 4.857578840284843,
      "grad_norm": 9.587404251098633,
      "learning_rate": 4.514242115971516e-05,
      "loss": 3.4037,
      "step": 9550
    },
    {
      "epoch": 4.86266531027467,
      "grad_norm": 9.31063175201416,
      "learning_rate": 4.513733468972533e-05,
      "loss": 3.4018,
      "step": 9560
    },
    {
      "epoch": 4.867751780264497,
      "grad_norm": 7.712965488433838,
      "learning_rate": 4.5132248219735504e-05,
      "loss": 3.4885,
      "step": 9570
    },
    {
      "epoch": 4.872838250254324,
      "grad_norm": 7.883378982543945,
      "learning_rate": 4.512716174974568e-05,
      "loss": 3.3177,
      "step": 9580
    },
    {
      "epoch": 4.877924720244151,
      "grad_norm": 9.153311729431152,
      "learning_rate": 4.512207527975586e-05,
      "loss": 3.4079,
      "step": 9590
    },
    {
      "epoch": 4.883011190233978,
      "grad_norm": 11.107531547546387,
      "learning_rate": 4.511698880976603e-05,
      "loss": 3.4774,
      "step": 9600
    },
    {
      "epoch": 4.888097660223805,
      "grad_norm": 7.391555309295654,
      "learning_rate": 4.51119023397762e-05,
      "loss": 3.4974,
      "step": 9610
    },
    {
      "epoch": 4.893184130213632,
      "grad_norm": 7.610074520111084,
      "learning_rate": 4.5106815869786374e-05,
      "loss": 3.4145,
      "step": 9620
    },
    {
      "epoch": 4.898270600203459,
      "grad_norm": 6.261350631713867,
      "learning_rate": 4.5101729399796544e-05,
      "loss": 3.4093,
      "step": 9630
    },
    {
      "epoch": 4.903357070193286,
      "grad_norm": 9.30160903930664,
      "learning_rate": 4.5096642929806714e-05,
      "loss": 3.4554,
      "step": 9640
    },
    {
      "epoch": 4.908443540183113,
      "grad_norm": 8.21364974975586,
      "learning_rate": 4.509155645981689e-05,
      "loss": 3.4233,
      "step": 9650
    },
    {
      "epoch": 4.9135300101729396,
      "grad_norm": 6.8808512687683105,
      "learning_rate": 4.508646998982706e-05,
      "loss": 3.3496,
      "step": 9660
    },
    {
      "epoch": 4.918616480162767,
      "grad_norm": 8.322468757629395,
      "learning_rate": 4.508138351983723e-05,
      "loss": 3.509,
      "step": 9670
    },
    {
      "epoch": 4.923702950152594,
      "grad_norm": 10.318218231201172,
      "learning_rate": 4.507629704984741e-05,
      "loss": 3.3569,
      "step": 9680
    },
    {
      "epoch": 4.928789420142421,
      "grad_norm": 7.555917739868164,
      "learning_rate": 4.5071210579857584e-05,
      "loss": 3.4327,
      "step": 9690
    },
    {
      "epoch": 4.933875890132248,
      "grad_norm": 9.784575462341309,
      "learning_rate": 4.5066124109867754e-05,
      "loss": 3.4172,
      "step": 9700
    },
    {
      "epoch": 4.938962360122075,
      "grad_norm": 8.84133243560791,
      "learning_rate": 4.506103763987793e-05,
      "loss": 3.3369,
      "step": 9710
    },
    {
      "epoch": 4.944048830111902,
      "grad_norm": 6.447965145111084,
      "learning_rate": 4.50559511698881e-05,
      "loss": 3.4442,
      "step": 9720
    },
    {
      "epoch": 4.949135300101729,
      "grad_norm": 6.56115198135376,
      "learning_rate": 4.505086469989827e-05,
      "loss": 3.4005,
      "step": 9730
    },
    {
      "epoch": 4.954221770091556,
      "grad_norm": 6.356902122497559,
      "learning_rate": 4.504577822990845e-05,
      "loss": 3.3982,
      "step": 9740
    },
    {
      "epoch": 4.959308240081383,
      "grad_norm": 7.716780662536621,
      "learning_rate": 4.5040691759918617e-05,
      "loss": 3.3916,
      "step": 9750
    },
    {
      "epoch": 4.96439471007121,
      "grad_norm": 6.3376007080078125,
      "learning_rate": 4.5035605289928786e-05,
      "loss": 3.4535,
      "step": 9760
    },
    {
      "epoch": 4.969481180061038,
      "grad_norm": 7.64890718460083,
      "learning_rate": 4.503051881993896e-05,
      "loss": 3.4119,
      "step": 9770
    },
    {
      "epoch": 4.974567650050865,
      "grad_norm": 10.527449607849121,
      "learning_rate": 4.502543234994914e-05,
      "loss": 3.2871,
      "step": 9780
    },
    {
      "epoch": 4.979654120040692,
      "grad_norm": 9.034046173095703,
      "learning_rate": 4.502034587995931e-05,
      "loss": 3.4036,
      "step": 9790
    },
    {
      "epoch": 4.984740590030519,
      "grad_norm": 7.88875675201416,
      "learning_rate": 4.5015259409969486e-05,
      "loss": 3.3779,
      "step": 9800
    },
    {
      "epoch": 4.989827060020346,
      "grad_norm": 8.219017028808594,
      "learning_rate": 4.5010172939979656e-05,
      "loss": 3.4746,
      "step": 9810
    },
    {
      "epoch": 4.994913530010173,
      "grad_norm": 8.629941940307617,
      "learning_rate": 4.5005086469989826e-05,
      "loss": 3.4406,
      "step": 9820
    },
    {
      "epoch": 5.0,
      "grad_norm": 10.693283081054688,
      "learning_rate": 4.5e-05,
      "loss": 3.4115,
      "step": 9830
    },
    {
      "epoch": 5.0,
      "eval_loss": 3.661724090576172,
      "eval_runtime": 2.6905,
      "eval_samples_per_second": 1031.412,
      "eval_steps_per_second": 128.973,
      "step": 9830
    }
  ],
  "logging_steps": 10,
  "max_steps": 98300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3267499008000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
