{
  "best_metric": 3.661724090576172,
  "best_model_checkpoint": "./checkpoints\\checkpoint-9830",
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 98300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00508646998982706,
      "grad_norm": 1.8620541095733643,
      "learning_rate": 4.999491353001017e-05,
      "loss": 5.5501,
      "step": 10
    },
    {
      "epoch": 0.01017293997965412,
      "grad_norm": 1.6996562480926514,
      "learning_rate": 4.998982706002035e-05,
      "loss": 5.4904,
      "step": 20
    },
    {
      "epoch": 0.015259409969481181,
      "grad_norm": 1.4883601665496826,
      "learning_rate": 4.998474059003052e-05,
      "loss": 5.4464,
      "step": 30
    },
    {
      "epoch": 0.02034587995930824,
      "grad_norm": 1.4940239191055298,
      "learning_rate": 4.997965412004069e-05,
      "loss": 5.4135,
      "step": 40
    },
    {
      "epoch": 0.0254323499491353,
      "grad_norm": 1.167168140411377,
      "learning_rate": 4.9974567650050865e-05,
      "loss": 5.3982,
      "step": 50
    },
    {
      "epoch": 0.030518819938962362,
      "grad_norm": 1.5297801494598389,
      "learning_rate": 4.996948118006104e-05,
      "loss": 5.3627,
      "step": 60
    },
    {
      "epoch": 0.03560528992878942,
      "grad_norm": 1.281337022781372,
      "learning_rate": 4.996439471007121e-05,
      "loss": 5.3323,
      "step": 70
    },
    {
      "epoch": 0.04069175991861648,
      "grad_norm": 1.1677004098892212,
      "learning_rate": 4.995930824008139e-05,
      "loss": 5.3231,
      "step": 80
    },
    {
      "epoch": 0.04577822990844354,
      "grad_norm": 1.1952483654022217,
      "learning_rate": 4.995422177009156e-05,
      "loss": 5.2891,
      "step": 90
    },
    {
      "epoch": 0.0508646998982706,
      "grad_norm": 1.2573658227920532,
      "learning_rate": 4.994913530010173e-05,
      "loss": 5.2795,
      "step": 100
    },
    {
      "epoch": 0.05595116988809766,
      "grad_norm": 1.1157411336898804,
      "learning_rate": 4.9944048830111905e-05,
      "loss": 5.267,
      "step": 110
    },
    {
      "epoch": 0.061037639877924724,
      "grad_norm": 1.198099970817566,
      "learning_rate": 4.9938962360122075e-05,
      "loss": 5.2233,
      "step": 120
    },
    {
      "epoch": 0.06612410986775177,
      "grad_norm": 1.4037412405014038,
      "learning_rate": 4.9933875890132245e-05,
      "loss": 5.2106,
      "step": 130
    },
    {
      "epoch": 0.07121057985757884,
      "grad_norm": 1.3378289937973022,
      "learning_rate": 4.992878942014242e-05,
      "loss": 5.1885,
      "step": 140
    },
    {
      "epoch": 0.0762970498474059,
      "grad_norm": 1.568644404411316,
      "learning_rate": 4.99237029501526e-05,
      "loss": 5.1683,
      "step": 150
    },
    {
      "epoch": 0.08138351983723296,
      "grad_norm": 1.299017310142517,
      "learning_rate": 4.9918616480162775e-05,
      "loss": 5.156,
      "step": 160
    },
    {
      "epoch": 0.08646998982706001,
      "grad_norm": 1.2730480432510376,
      "learning_rate": 4.9913530010172945e-05,
      "loss": 5.097,
      "step": 170
    },
    {
      "epoch": 0.09155645981688708,
      "grad_norm": 1.2603338956832886,
      "learning_rate": 4.9908443540183115e-05,
      "loss": 5.0891,
      "step": 180
    },
    {
      "epoch": 0.09664292980671414,
      "grad_norm": 1.0943132638931274,
      "learning_rate": 4.990335707019329e-05,
      "loss": 5.0726,
      "step": 190
    },
    {
      "epoch": 0.1017293997965412,
      "grad_norm": 1.2997273206710815,
      "learning_rate": 4.989827060020346e-05,
      "loss": 5.0575,
      "step": 200
    },
    {
      "epoch": 0.10681586978636826,
      "grad_norm": 1.1497079133987427,
      "learning_rate": 4.989318413021363e-05,
      "loss": 5.0453,
      "step": 210
    },
    {
      "epoch": 0.11190233977619532,
      "grad_norm": 1.0320709943771362,
      "learning_rate": 4.988809766022381e-05,
      "loss": 5.0447,
      "step": 220
    },
    {
      "epoch": 0.11698880976602238,
      "grad_norm": 1.5202912092208862,
      "learning_rate": 4.988301119023398e-05,
      "loss": 4.9879,
      "step": 230
    },
    {
      "epoch": 0.12207527975584945,
      "grad_norm": 1.2593244314193726,
      "learning_rate": 4.9877924720244154e-05,
      "loss": 4.9822,
      "step": 240
    },
    {
      "epoch": 0.1271617497456765,
      "grad_norm": 0.9961100220680237,
      "learning_rate": 4.987283825025433e-05,
      "loss": 4.9659,
      "step": 250
    },
    {
      "epoch": 0.13224821973550355,
      "grad_norm": 1.2668153047561646,
      "learning_rate": 4.98677517802645e-05,
      "loss": 4.9602,
      "step": 260
    },
    {
      "epoch": 0.1373346897253306,
      "grad_norm": 1.0524022579193115,
      "learning_rate": 4.986266531027467e-05,
      "loss": 4.952,
      "step": 270
    },
    {
      "epoch": 0.14242115971515767,
      "grad_norm": 1.0568088293075562,
      "learning_rate": 4.985757884028485e-05,
      "loss": 4.887,
      "step": 280
    },
    {
      "epoch": 0.14750762970498474,
      "grad_norm": 1.2231923341751099,
      "learning_rate": 4.985249237029502e-05,
      "loss": 4.909,
      "step": 290
    },
    {
      "epoch": 0.1525940996948118,
      "grad_norm": 1.086974024772644,
      "learning_rate": 4.984740590030519e-05,
      "loss": 4.891,
      "step": 300
    },
    {
      "epoch": 0.15768056968463887,
      "grad_norm": 1.3009490966796875,
      "learning_rate": 4.9842319430315364e-05,
      "loss": 4.8309,
      "step": 310
    },
    {
      "epoch": 0.16276703967446593,
      "grad_norm": 1.2265081405639648,
      "learning_rate": 4.9837232960325534e-05,
      "loss": 4.865,
      "step": 320
    },
    {
      "epoch": 0.167853509664293,
      "grad_norm": 0.9750216603279114,
      "learning_rate": 4.983214649033571e-05,
      "loss": 4.8418,
      "step": 330
    },
    {
      "epoch": 0.17293997965412003,
      "grad_norm": 1.1377900838851929,
      "learning_rate": 4.982706002034588e-05,
      "loss": 4.7897,
      "step": 340
    },
    {
      "epoch": 0.1780264496439471,
      "grad_norm": 1.2639412879943848,
      "learning_rate": 4.982197355035606e-05,
      "loss": 4.7862,
      "step": 350
    },
    {
      "epoch": 0.18311291963377416,
      "grad_norm": 1.2019010782241821,
      "learning_rate": 4.981688708036623e-05,
      "loss": 4.7572,
      "step": 360
    },
    {
      "epoch": 0.18819938962360122,
      "grad_norm": 1.1009970903396606,
      "learning_rate": 4.9811800610376404e-05,
      "loss": 4.7336,
      "step": 370
    },
    {
      "epoch": 0.19328585961342828,
      "grad_norm": 1.0368852615356445,
      "learning_rate": 4.9806714140386574e-05,
      "loss": 4.7757,
      "step": 380
    },
    {
      "epoch": 0.19837232960325535,
      "grad_norm": 1.0208802223205566,
      "learning_rate": 4.9801627670396743e-05,
      "loss": 4.7595,
      "step": 390
    },
    {
      "epoch": 0.2034587995930824,
      "grad_norm": 1.1146231889724731,
      "learning_rate": 4.979654120040692e-05,
      "loss": 4.6913,
      "step": 400
    },
    {
      "epoch": 0.20854526958290945,
      "grad_norm": 1.0009827613830566,
      "learning_rate": 4.979145473041709e-05,
      "loss": 4.6887,
      "step": 410
    },
    {
      "epoch": 0.2136317395727365,
      "grad_norm": 1.2371834516525269,
      "learning_rate": 4.978636826042727e-05,
      "loss": 4.6956,
      "step": 420
    },
    {
      "epoch": 0.21871820956256358,
      "grad_norm": 1.3951903581619263,
      "learning_rate": 4.9781281790437437e-05,
      "loss": 4.6473,
      "step": 430
    },
    {
      "epoch": 0.22380467955239064,
      "grad_norm": 0.9142779111862183,
      "learning_rate": 4.977619532044761e-05,
      "loss": 4.6879,
      "step": 440
    },
    {
      "epoch": 0.2288911495422177,
      "grad_norm": 1.055975079536438,
      "learning_rate": 4.977110885045779e-05,
      "loss": 4.6322,
      "step": 450
    },
    {
      "epoch": 0.23397761953204477,
      "grad_norm": 1.063486933708191,
      "learning_rate": 4.976602238046796e-05,
      "loss": 4.5838,
      "step": 460
    },
    {
      "epoch": 0.23906408952187183,
      "grad_norm": 1.26181161403656,
      "learning_rate": 4.976093591047813e-05,
      "loss": 4.5956,
      "step": 470
    },
    {
      "epoch": 0.2441505595116989,
      "grad_norm": 1.114978313446045,
      "learning_rate": 4.9755849440488306e-05,
      "loss": 4.6266,
      "step": 480
    },
    {
      "epoch": 0.24923702950152593,
      "grad_norm": 1.1034096479415894,
      "learning_rate": 4.9750762970498476e-05,
      "loss": 4.5799,
      "step": 490
    },
    {
      "epoch": 0.254323499491353,
      "grad_norm": 0.9284713268280029,
      "learning_rate": 4.9745676500508646e-05,
      "loss": 4.5832,
      "step": 500
    },
    {
      "epoch": 0.25940996948118006,
      "grad_norm": 1.045884370803833,
      "learning_rate": 4.974059003051882e-05,
      "loss": 4.5394,
      "step": 510
    },
    {
      "epoch": 0.2644964394710071,
      "grad_norm": 1.037023901939392,
      "learning_rate": 4.973550356052899e-05,
      "loss": 4.593,
      "step": 520
    },
    {
      "epoch": 0.2695829094608342,
      "grad_norm": 0.9548038840293884,
      "learning_rate": 4.973041709053917e-05,
      "loss": 4.5065,
      "step": 530
    },
    {
      "epoch": 0.2746693794506612,
      "grad_norm": 0.9394946694374084,
      "learning_rate": 4.9725330620549346e-05,
      "loss": 4.575,
      "step": 540
    },
    {
      "epoch": 0.2797558494404883,
      "grad_norm": 1.1619436740875244,
      "learning_rate": 4.9720244150559516e-05,
      "loss": 4.4672,
      "step": 550
    },
    {
      "epoch": 0.28484231943031535,
      "grad_norm": 1.001769781112671,
      "learning_rate": 4.9715157680569686e-05,
      "loss": 4.5327,
      "step": 560
    },
    {
      "epoch": 0.28992878942014244,
      "grad_norm": 1.232015609741211,
      "learning_rate": 4.971007121057986e-05,
      "loss": 4.5427,
      "step": 570
    },
    {
      "epoch": 0.2950152594099695,
      "grad_norm": 0.9298741221427917,
      "learning_rate": 4.970498474059003e-05,
      "loss": 4.5001,
      "step": 580
    },
    {
      "epoch": 0.30010172939979657,
      "grad_norm": 0.8731594681739807,
      "learning_rate": 4.96998982706002e-05,
      "loss": 4.4653,
      "step": 590
    },
    {
      "epoch": 0.3051881993896236,
      "grad_norm": 0.8989977836608887,
      "learning_rate": 4.969481180061038e-05,
      "loss": 4.457,
      "step": 600
    },
    {
      "epoch": 0.31027466937945064,
      "grad_norm": 1.0350463390350342,
      "learning_rate": 4.968972533062055e-05,
      "loss": 4.4715,
      "step": 610
    },
    {
      "epoch": 0.31536113936927773,
      "grad_norm": 1.2084124088287354,
      "learning_rate": 4.9684638860630726e-05,
      "loss": 4.5121,
      "step": 620
    },
    {
      "epoch": 0.32044760935910477,
      "grad_norm": 1.0157567262649536,
      "learning_rate": 4.96795523906409e-05,
      "loss": 4.4614,
      "step": 630
    },
    {
      "epoch": 0.32553407934893186,
      "grad_norm": 0.9925084710121155,
      "learning_rate": 4.967446592065107e-05,
      "loss": 4.3935,
      "step": 640
    },
    {
      "epoch": 0.3306205493387589,
      "grad_norm": 1.1453547477722168,
      "learning_rate": 4.966937945066124e-05,
      "loss": 4.51,
      "step": 650
    },
    {
      "epoch": 0.335707019328586,
      "grad_norm": 1.079935908317566,
      "learning_rate": 4.966429298067142e-05,
      "loss": 4.4414,
      "step": 660
    },
    {
      "epoch": 0.340793489318413,
      "grad_norm": 1.014600157737732,
      "learning_rate": 4.965920651068159e-05,
      "loss": 4.4151,
      "step": 670
    },
    {
      "epoch": 0.34587995930824006,
      "grad_norm": 0.9856159687042236,
      "learning_rate": 4.9654120040691765e-05,
      "loss": 4.3928,
      "step": 680
    },
    {
      "epoch": 0.35096642929806715,
      "grad_norm": 0.9867738485336304,
      "learning_rate": 4.9649033570701935e-05,
      "loss": 4.365,
      "step": 690
    },
    {
      "epoch": 0.3560528992878942,
      "grad_norm": 1.2926799058914185,
      "learning_rate": 4.9643947100712105e-05,
      "loss": 4.3946,
      "step": 700
    },
    {
      "epoch": 0.3611393692777213,
      "grad_norm": 1.19403874874115,
      "learning_rate": 4.963886063072228e-05,
      "loss": 4.4638,
      "step": 710
    },
    {
      "epoch": 0.3662258392675483,
      "grad_norm": 1.2185667753219604,
      "learning_rate": 4.963377416073245e-05,
      "loss": 4.3881,
      "step": 720
    },
    {
      "epoch": 0.3713123092573754,
      "grad_norm": 1.1601144075393677,
      "learning_rate": 4.962868769074263e-05,
      "loss": 4.4004,
      "step": 730
    },
    {
      "epoch": 0.37639877924720244,
      "grad_norm": 1.0725337266921997,
      "learning_rate": 4.9623601220752805e-05,
      "loss": 4.3193,
      "step": 740
    },
    {
      "epoch": 0.3814852492370295,
      "grad_norm": 0.8986203670501709,
      "learning_rate": 4.9618514750762975e-05,
      "loss": 4.3496,
      "step": 750
    },
    {
      "epoch": 0.38657171922685657,
      "grad_norm": 1.0403578281402588,
      "learning_rate": 4.9613428280773145e-05,
      "loss": 4.3318,
      "step": 760
    },
    {
      "epoch": 0.3916581892166836,
      "grad_norm": 0.9969229698181152,
      "learning_rate": 4.960834181078332e-05,
      "loss": 4.4305,
      "step": 770
    },
    {
      "epoch": 0.3967446592065107,
      "grad_norm": 1.0671964883804321,
      "learning_rate": 4.960325534079349e-05,
      "loss": 4.3708,
      "step": 780
    },
    {
      "epoch": 0.40183112919633773,
      "grad_norm": 0.81570965051651,
      "learning_rate": 4.959816887080366e-05,
      "loss": 4.3945,
      "step": 790
    },
    {
      "epoch": 0.4069175991861648,
      "grad_norm": 0.9248974323272705,
      "learning_rate": 4.959308240081384e-05,
      "loss": 4.3491,
      "step": 800
    },
    {
      "epoch": 0.41200406917599186,
      "grad_norm": 1.1376482248306274,
      "learning_rate": 4.958799593082401e-05,
      "loss": 4.3475,
      "step": 810
    },
    {
      "epoch": 0.4170905391658189,
      "grad_norm": 1.1099718809127808,
      "learning_rate": 4.9582909460834184e-05,
      "loss": 4.3141,
      "step": 820
    },
    {
      "epoch": 0.422177009155646,
      "grad_norm": 1.182560920715332,
      "learning_rate": 4.957782299084436e-05,
      "loss": 4.2648,
      "step": 830
    },
    {
      "epoch": 0.427263479145473,
      "grad_norm": 1.4488362073898315,
      "learning_rate": 4.957273652085453e-05,
      "loss": 4.293,
      "step": 840
    },
    {
      "epoch": 0.4323499491353001,
      "grad_norm": 1.1328459978103638,
      "learning_rate": 4.95676500508647e-05,
      "loss": 4.3389,
      "step": 850
    },
    {
      "epoch": 0.43743641912512715,
      "grad_norm": 1.3857777118682861,
      "learning_rate": 4.956256358087488e-05,
      "loss": 4.336,
      "step": 860
    },
    {
      "epoch": 0.44252288911495424,
      "grad_norm": 1.0977919101715088,
      "learning_rate": 4.955747711088505e-05,
      "loss": 4.2598,
      "step": 870
    },
    {
      "epoch": 0.4476093591047813,
      "grad_norm": 1.1394001245498657,
      "learning_rate": 4.955239064089522e-05,
      "loss": 4.2904,
      "step": 880
    },
    {
      "epoch": 0.4526958290946083,
      "grad_norm": 1.243005394935608,
      "learning_rate": 4.9547304170905394e-05,
      "loss": 4.3366,
      "step": 890
    },
    {
      "epoch": 0.4577822990844354,
      "grad_norm": 0.9567817449569702,
      "learning_rate": 4.9542217700915564e-05,
      "loss": 4.263,
      "step": 900
    },
    {
      "epoch": 0.46286876907426244,
      "grad_norm": 1.205608606338501,
      "learning_rate": 4.953713123092574e-05,
      "loss": 4.2712,
      "step": 910
    },
    {
      "epoch": 0.46795523906408953,
      "grad_norm": 1.319305181503296,
      "learning_rate": 4.953204476093592e-05,
      "loss": 4.2366,
      "step": 920
    },
    {
      "epoch": 0.47304170905391657,
      "grad_norm": 1.4482612609863281,
      "learning_rate": 4.952695829094609e-05,
      "loss": 4.222,
      "step": 930
    },
    {
      "epoch": 0.47812817904374366,
      "grad_norm": 1.2469608783721924,
      "learning_rate": 4.952187182095626e-05,
      "loss": 4.2576,
      "step": 940
    },
    {
      "epoch": 0.4832146490335707,
      "grad_norm": 1.030442476272583,
      "learning_rate": 4.9516785350966434e-05,
      "loss": 4.2806,
      "step": 950
    },
    {
      "epoch": 0.4883011190233978,
      "grad_norm": 1.0235463380813599,
      "learning_rate": 4.9511698880976604e-05,
      "loss": 4.2612,
      "step": 960
    },
    {
      "epoch": 0.4933875890132248,
      "grad_norm": 1.7610021829605103,
      "learning_rate": 4.950661241098678e-05,
      "loss": 4.245,
      "step": 970
    },
    {
      "epoch": 0.49847405900305186,
      "grad_norm": 0.9532315135002136,
      "learning_rate": 4.950152594099695e-05,
      "loss": 4.2459,
      "step": 980
    },
    {
      "epoch": 0.503560528992879,
      "grad_norm": 1.209675669670105,
      "learning_rate": 4.949643947100712e-05,
      "loss": 4.1973,
      "step": 990
    },
    {
      "epoch": 0.508646998982706,
      "grad_norm": 1.302587628364563,
      "learning_rate": 4.94913530010173e-05,
      "loss": 4.2045,
      "step": 1000
    },
    {
      "epoch": 0.513733468972533,
      "grad_norm": 1.2250295877456665,
      "learning_rate": 4.9486266531027467e-05,
      "loss": 4.2151,
      "step": 1010
    },
    {
      "epoch": 0.5188199389623601,
      "grad_norm": 1.3869332075119019,
      "learning_rate": 4.948118006103764e-05,
      "loss": 4.2411,
      "step": 1020
    },
    {
      "epoch": 0.5239064089521872,
      "grad_norm": 1.1010462045669556,
      "learning_rate": 4.947609359104782e-05,
      "loss": 4.2137,
      "step": 1030
    },
    {
      "epoch": 0.5289928789420142,
      "grad_norm": 1.1491479873657227,
      "learning_rate": 4.947100712105799e-05,
      "loss": 4.1989,
      "step": 1040
    },
    {
      "epoch": 0.5340793489318413,
      "grad_norm": 1.1577527523040771,
      "learning_rate": 4.946592065106816e-05,
      "loss": 4.1901,
      "step": 1050
    },
    {
      "epoch": 0.5391658189216684,
      "grad_norm": 1.1797277927398682,
      "learning_rate": 4.9460834181078336e-05,
      "loss": 4.2162,
      "step": 1060
    },
    {
      "epoch": 0.5442522889114955,
      "grad_norm": 1.1767712831497192,
      "learning_rate": 4.9455747711088506e-05,
      "loss": 4.1767,
      "step": 1070
    },
    {
      "epoch": 0.5493387589013224,
      "grad_norm": 1.3309577703475952,
      "learning_rate": 4.9450661241098676e-05,
      "loss": 4.2057,
      "step": 1080
    },
    {
      "epoch": 0.5544252288911495,
      "grad_norm": 1.1619929075241089,
      "learning_rate": 4.944557477110885e-05,
      "loss": 4.1858,
      "step": 1090
    },
    {
      "epoch": 0.5595116988809766,
      "grad_norm": 1.109459638595581,
      "learning_rate": 4.944048830111902e-05,
      "loss": 4.2067,
      "step": 1100
    },
    {
      "epoch": 0.5645981688708036,
      "grad_norm": 1.047372579574585,
      "learning_rate": 4.94354018311292e-05,
      "loss": 4.137,
      "step": 1110
    },
    {
      "epoch": 0.5696846388606307,
      "grad_norm": 1.406684398651123,
      "learning_rate": 4.9430315361139376e-05,
      "loss": 4.1788,
      "step": 1120
    },
    {
      "epoch": 0.5747711088504578,
      "grad_norm": 1.426543951034546,
      "learning_rate": 4.9425228891149546e-05,
      "loss": 4.1054,
      "step": 1130
    },
    {
      "epoch": 0.5798575788402849,
      "grad_norm": 1.2268894910812378,
      "learning_rate": 4.9420142421159716e-05,
      "loss": 4.1661,
      "step": 1140
    },
    {
      "epoch": 0.5849440488301119,
      "grad_norm": 1.3415323495864868,
      "learning_rate": 4.941505595116989e-05,
      "loss": 4.2086,
      "step": 1150
    },
    {
      "epoch": 0.590030518819939,
      "grad_norm": 1.4937567710876465,
      "learning_rate": 4.940996948118006e-05,
      "loss": 4.1673,
      "step": 1160
    },
    {
      "epoch": 0.595116988809766,
      "grad_norm": 1.4333189725875854,
      "learning_rate": 4.940488301119023e-05,
      "loss": 4.1634,
      "step": 1170
    },
    {
      "epoch": 0.6002034587995931,
      "grad_norm": 1.1926484107971191,
      "learning_rate": 4.939979654120041e-05,
      "loss": 4.1754,
      "step": 1180
    },
    {
      "epoch": 0.6052899287894201,
      "grad_norm": 1.205100178718567,
      "learning_rate": 4.939471007121058e-05,
      "loss": 4.1628,
      "step": 1190
    },
    {
      "epoch": 0.6103763987792472,
      "grad_norm": 1.8868303298950195,
      "learning_rate": 4.9389623601220756e-05,
      "loss": 4.1402,
      "step": 1200
    },
    {
      "epoch": 0.6154628687690743,
      "grad_norm": 1.4437249898910522,
      "learning_rate": 4.938453713123093e-05,
      "loss": 4.2068,
      "step": 1210
    },
    {
      "epoch": 0.6205493387589013,
      "grad_norm": 1.2796233892440796,
      "learning_rate": 4.93794506612411e-05,
      "loss": 4.2437,
      "step": 1220
    },
    {
      "epoch": 0.6256358087487284,
      "grad_norm": 1.2472829818725586,
      "learning_rate": 4.937436419125128e-05,
      "loss": 4.1613,
      "step": 1230
    },
    {
      "epoch": 0.6307222787385555,
      "grad_norm": 1.3386597633361816,
      "learning_rate": 4.936927772126145e-05,
      "loss": 4.2188,
      "step": 1240
    },
    {
      "epoch": 0.6358087487283826,
      "grad_norm": 1.3510419130325317,
      "learning_rate": 4.936419125127162e-05,
      "loss": 4.2129,
      "step": 1250
    },
    {
      "epoch": 0.6408952187182095,
      "grad_norm": 1.7777752876281738,
      "learning_rate": 4.9359104781281795e-05,
      "loss": 4.1499,
      "step": 1260
    },
    {
      "epoch": 0.6459816887080366,
      "grad_norm": 1.20978581905365,
      "learning_rate": 4.9354018311291965e-05,
      "loss": 4.1134,
      "step": 1270
    },
    {
      "epoch": 0.6510681586978637,
      "grad_norm": 1.3169583082199097,
      "learning_rate": 4.9348931841302135e-05,
      "loss": 4.207,
      "step": 1280
    },
    {
      "epoch": 0.6561546286876907,
      "grad_norm": 0.9704428911209106,
      "learning_rate": 4.934384537131231e-05,
      "loss": 4.1063,
      "step": 1290
    },
    {
      "epoch": 0.6612410986775178,
      "grad_norm": 1.3254092931747437,
      "learning_rate": 4.933875890132248e-05,
      "loss": 4.2037,
      "step": 1300
    },
    {
      "epoch": 0.6663275686673449,
      "grad_norm": 1.7731354236602783,
      "learning_rate": 4.933367243133266e-05,
      "loss": 4.1308,
      "step": 1310
    },
    {
      "epoch": 0.671414038657172,
      "grad_norm": 1.3588954210281372,
      "learning_rate": 4.9328585961342835e-05,
      "loss": 4.1326,
      "step": 1320
    },
    {
      "epoch": 0.676500508646999,
      "grad_norm": 1.3305162191390991,
      "learning_rate": 4.9323499491353005e-05,
      "loss": 4.1789,
      "step": 1330
    },
    {
      "epoch": 0.681586978636826,
      "grad_norm": 1.146964430809021,
      "learning_rate": 4.9318413021363175e-05,
      "loss": 4.1495,
      "step": 1340
    },
    {
      "epoch": 0.6866734486266531,
      "grad_norm": 1.1409286260604858,
      "learning_rate": 4.931332655137335e-05,
      "loss": 4.1293,
      "step": 1350
    },
    {
      "epoch": 0.6917599186164801,
      "grad_norm": 1.3366755247116089,
      "learning_rate": 4.930824008138352e-05,
      "loss": 4.2419,
      "step": 1360
    },
    {
      "epoch": 0.6968463886063072,
      "grad_norm": 1.4700900316238403,
      "learning_rate": 4.930315361139369e-05,
      "loss": 4.101,
      "step": 1370
    },
    {
      "epoch": 0.7019328585961343,
      "grad_norm": 1.2512423992156982,
      "learning_rate": 4.929806714140387e-05,
      "loss": 4.0918,
      "step": 1380
    },
    {
      "epoch": 0.7070193285859614,
      "grad_norm": 1.7508658170700073,
      "learning_rate": 4.929298067141404e-05,
      "loss": 4.1454,
      "step": 1390
    },
    {
      "epoch": 0.7121057985757884,
      "grad_norm": 1.0262482166290283,
      "learning_rate": 4.9287894201424214e-05,
      "loss": 4.0968,
      "step": 1400
    },
    {
      "epoch": 0.7171922685656155,
      "grad_norm": 1.9106734991073608,
      "learning_rate": 4.928280773143439e-05,
      "loss": 4.0764,
      "step": 1410
    },
    {
      "epoch": 0.7222787385554426,
      "grad_norm": 1.514243483543396,
      "learning_rate": 4.927772126144456e-05,
      "loss": 4.124,
      "step": 1420
    },
    {
      "epoch": 0.7273652085452695,
      "grad_norm": 1.5909725427627563,
      "learning_rate": 4.927263479145473e-05,
      "loss": 4.1942,
      "step": 1430
    },
    {
      "epoch": 0.7324516785350966,
      "grad_norm": 1.5087953805923462,
      "learning_rate": 4.926754832146491e-05,
      "loss": 4.1029,
      "step": 1440
    },
    {
      "epoch": 0.7375381485249237,
      "grad_norm": 1.303483009338379,
      "learning_rate": 4.926246185147508e-05,
      "loss": 4.0675,
      "step": 1450
    },
    {
      "epoch": 0.7426246185147508,
      "grad_norm": 1.4629945755004883,
      "learning_rate": 4.925737538148525e-05,
      "loss": 4.1283,
      "step": 1460
    },
    {
      "epoch": 0.7477110885045778,
      "grad_norm": 1.9691299200057983,
      "learning_rate": 4.9252288911495424e-05,
      "loss": 4.0744,
      "step": 1470
    },
    {
      "epoch": 0.7527975584944049,
      "grad_norm": 1.4583765268325806,
      "learning_rate": 4.9247202441505594e-05,
      "loss": 4.0836,
      "step": 1480
    },
    {
      "epoch": 0.757884028484232,
      "grad_norm": 1.8962498903274536,
      "learning_rate": 4.924211597151577e-05,
      "loss": 4.0617,
      "step": 1490
    },
    {
      "epoch": 0.762970498474059,
      "grad_norm": 1.1788734197616577,
      "learning_rate": 4.923702950152595e-05,
      "loss": 4.1733,
      "step": 1500
    },
    {
      "epoch": 0.768056968463886,
      "grad_norm": 1.4367045164108276,
      "learning_rate": 4.923194303153612e-05,
      "loss": 4.1613,
      "step": 1510
    },
    {
      "epoch": 0.7731434384537131,
      "grad_norm": 1.4754124879837036,
      "learning_rate": 4.9226856561546294e-05,
      "loss": 4.0269,
      "step": 1520
    },
    {
      "epoch": 0.7782299084435402,
      "grad_norm": 1.3124659061431885,
      "learning_rate": 4.9221770091556464e-05,
      "loss": 4.114,
      "step": 1530
    },
    {
      "epoch": 0.7833163784333672,
      "grad_norm": 1.8541167974472046,
      "learning_rate": 4.9216683621566634e-05,
      "loss": 4.1116,
      "step": 1540
    },
    {
      "epoch": 0.7884028484231943,
      "grad_norm": 1.3308602571487427,
      "learning_rate": 4.921159715157681e-05,
      "loss": 4.034,
      "step": 1550
    },
    {
      "epoch": 0.7934893184130214,
      "grad_norm": 1.438094973564148,
      "learning_rate": 4.920651068158698e-05,
      "loss": 4.0614,
      "step": 1560
    },
    {
      "epoch": 0.7985757884028484,
      "grad_norm": 1.3125865459442139,
      "learning_rate": 4.920142421159715e-05,
      "loss": 4.02,
      "step": 1570
    },
    {
      "epoch": 0.8036622583926755,
      "grad_norm": 1.5567948818206787,
      "learning_rate": 4.919633774160733e-05,
      "loss": 4.0785,
      "step": 1580
    },
    {
      "epoch": 0.8087487283825026,
      "grad_norm": 1.516724944114685,
      "learning_rate": 4.91912512716175e-05,
      "loss": 4.1005,
      "step": 1590
    },
    {
      "epoch": 0.8138351983723296,
      "grad_norm": 1.7937555313110352,
      "learning_rate": 4.918616480162767e-05,
      "loss": 4.0661,
      "step": 1600
    },
    {
      "epoch": 0.8189216683621566,
      "grad_norm": 1.5719882249832153,
      "learning_rate": 4.918107833163785e-05,
      "loss": 4.1163,
      "step": 1610
    },
    {
      "epoch": 0.8240081383519837,
      "grad_norm": 1.4146252870559692,
      "learning_rate": 4.917599186164802e-05,
      "loss": 4.0523,
      "step": 1620
    },
    {
      "epoch": 0.8290946083418108,
      "grad_norm": 1.4866914749145508,
      "learning_rate": 4.917090539165819e-05,
      "loss": 4.1138,
      "step": 1630
    },
    {
      "epoch": 0.8341810783316378,
      "grad_norm": 1.7472399473190308,
      "learning_rate": 4.9165818921668366e-05,
      "loss": 4.1069,
      "step": 1640
    },
    {
      "epoch": 0.8392675483214649,
      "grad_norm": 1.3671644926071167,
      "learning_rate": 4.9160732451678536e-05,
      "loss": 4.0771,
      "step": 1650
    },
    {
      "epoch": 0.844354018311292,
      "grad_norm": 1.6440402269363403,
      "learning_rate": 4.9155645981688706e-05,
      "loss": 4.1624,
      "step": 1660
    },
    {
      "epoch": 0.8494404883011191,
      "grad_norm": 1.6369292736053467,
      "learning_rate": 4.915055951169888e-05,
      "loss": 4.0863,
      "step": 1670
    },
    {
      "epoch": 0.854526958290946,
      "grad_norm": 1.1553452014923096,
      "learning_rate": 4.914547304170905e-05,
      "loss": 4.0794,
      "step": 1680
    },
    {
      "epoch": 0.8596134282807731,
      "grad_norm": 1.8268574476242065,
      "learning_rate": 4.914038657171923e-05,
      "loss": 4.0721,
      "step": 1690
    },
    {
      "epoch": 0.8646998982706002,
      "grad_norm": 1.668223261833191,
      "learning_rate": 4.9135300101729406e-05,
      "loss": 4.0295,
      "step": 1700
    },
    {
      "epoch": 0.8697863682604272,
      "grad_norm": 1.6191165447235107,
      "learning_rate": 4.9130213631739576e-05,
      "loss": 4.0596,
      "step": 1710
    },
    {
      "epoch": 0.8748728382502543,
      "grad_norm": 1.6551761627197266,
      "learning_rate": 4.9125127161749746e-05,
      "loss": 4.0029,
      "step": 1720
    },
    {
      "epoch": 0.8799593082400814,
      "grad_norm": 2.0020599365234375,
      "learning_rate": 4.912004069175992e-05,
      "loss": 3.9949,
      "step": 1730
    },
    {
      "epoch": 0.8850457782299085,
      "grad_norm": 1.9126626253128052,
      "learning_rate": 4.911495422177009e-05,
      "loss": 4.076,
      "step": 1740
    },
    {
      "epoch": 0.8901322482197355,
      "grad_norm": 1.9451290369033813,
      "learning_rate": 4.910986775178026e-05,
      "loss": 4.0856,
      "step": 1750
    },
    {
      "epoch": 0.8952187182095626,
      "grad_norm": 1.8924111127853394,
      "learning_rate": 4.910478128179044e-05,
      "loss": 4.1022,
      "step": 1760
    },
    {
      "epoch": 0.9003051881993896,
      "grad_norm": 1.7400778532028198,
      "learning_rate": 4.909969481180061e-05,
      "loss": 4.0749,
      "step": 1770
    },
    {
      "epoch": 0.9053916581892166,
      "grad_norm": 1.6534168720245361,
      "learning_rate": 4.9094608341810786e-05,
      "loss": 4.0948,
      "step": 1780
    },
    {
      "epoch": 0.9104781281790437,
      "grad_norm": 1.277776837348938,
      "learning_rate": 4.908952187182096e-05,
      "loss": 4.0708,
      "step": 1790
    },
    {
      "epoch": 0.9155645981688708,
      "grad_norm": 1.594847559928894,
      "learning_rate": 4.908443540183113e-05,
      "loss": 4.0642,
      "step": 1800
    },
    {
      "epoch": 0.9206510681586979,
      "grad_norm": 1.8652358055114746,
      "learning_rate": 4.907934893184131e-05,
      "loss": 4.005,
      "step": 1810
    },
    {
      "epoch": 0.9257375381485249,
      "grad_norm": 1.9682148694992065,
      "learning_rate": 4.907426246185148e-05,
      "loss": 4.0495,
      "step": 1820
    },
    {
      "epoch": 0.930824008138352,
      "grad_norm": 1.3807790279388428,
      "learning_rate": 4.906917599186165e-05,
      "loss": 4.032,
      "step": 1830
    },
    {
      "epoch": 0.9359104781281791,
      "grad_norm": 1.3387295007705688,
      "learning_rate": 4.9064089521871825e-05,
      "loss": 4.1224,
      "step": 1840
    },
    {
      "epoch": 0.940996948118006,
      "grad_norm": 2.138181686401367,
      "learning_rate": 4.9059003051881995e-05,
      "loss": 4.0615,
      "step": 1850
    },
    {
      "epoch": 0.9460834181078331,
      "grad_norm": 1.384210467338562,
      "learning_rate": 4.9053916581892165e-05,
      "loss": 4.0101,
      "step": 1860
    },
    {
      "epoch": 0.9511698880976602,
      "grad_norm": 1.8288390636444092,
      "learning_rate": 4.904883011190234e-05,
      "loss": 4.0019,
      "step": 1870
    },
    {
      "epoch": 0.9562563580874873,
      "grad_norm": 2.013861894607544,
      "learning_rate": 4.904374364191252e-05,
      "loss": 3.9878,
      "step": 1880
    },
    {
      "epoch": 0.9613428280773143,
      "grad_norm": 1.9177788496017456,
      "learning_rate": 4.903865717192269e-05,
      "loss": 3.9973,
      "step": 1890
    },
    {
      "epoch": 0.9664292980671414,
      "grad_norm": 1.5360263586044312,
      "learning_rate": 4.9033570701932865e-05,
      "loss": 4.1192,
      "step": 1900
    },
    {
      "epoch": 0.9715157680569685,
      "grad_norm": 2.2354724407196045,
      "learning_rate": 4.9028484231943035e-05,
      "loss": 4.0219,
      "step": 1910
    },
    {
      "epoch": 0.9766022380467956,
      "grad_norm": 1.7626396417617798,
      "learning_rate": 4.9023397761953205e-05,
      "loss": 3.9792,
      "step": 1920
    },
    {
      "epoch": 0.9816887080366226,
      "grad_norm": 1.419806718826294,
      "learning_rate": 4.901831129196338e-05,
      "loss": 4.0552,
      "step": 1930
    },
    {
      "epoch": 0.9867751780264497,
      "grad_norm": 2.033162832260132,
      "learning_rate": 4.901322482197355e-05,
      "loss": 3.9787,
      "step": 1940
    },
    {
      "epoch": 0.9918616480162767,
      "grad_norm": 1.4936879873275757,
      "learning_rate": 4.900813835198372e-05,
      "loss": 4.0087,
      "step": 1950
    },
    {
      "epoch": 0.9969481180061037,
      "grad_norm": 2.586871385574341,
      "learning_rate": 4.90030518819939e-05,
      "loss": 4.0202,
      "step": 1960
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.987011432647705,
      "eval_runtime": 2.6538,
      "eval_samples_per_second": 1045.666,
      "eval_steps_per_second": 130.755,
      "step": 1966
    },
    {
      "epoch": 1.002034587995931,
      "grad_norm": 1.7257380485534668,
      "learning_rate": 4.899796541200407e-05,
      "loss": 3.9627,
      "step": 1970
    },
    {
      "epoch": 1.007121057985758,
      "grad_norm": 1.7950177192687988,
      "learning_rate": 4.8992878942014244e-05,
      "loss": 4.0469,
      "step": 1980
    },
    {
      "epoch": 1.0122075279755849,
      "grad_norm": 1.6862667798995972,
      "learning_rate": 4.898779247202442e-05,
      "loss": 4.0318,
      "step": 1990
    },
    {
      "epoch": 1.017293997965412,
      "grad_norm": 1.936666488647461,
      "learning_rate": 4.898270600203459e-05,
      "loss": 4.0466,
      "step": 2000
    },
    {
      "epoch": 1.022380467955239,
      "grad_norm": 1.9143457412719727,
      "learning_rate": 4.897761953204476e-05,
      "loss": 4.0069,
      "step": 2010
    },
    {
      "epoch": 1.027466937945066,
      "grad_norm": 1.6920356750488281,
      "learning_rate": 4.897253306205494e-05,
      "loss": 4.0069,
      "step": 2020
    },
    {
      "epoch": 1.0325534079348933,
      "grad_norm": 1.9220759868621826,
      "learning_rate": 4.896744659206511e-05,
      "loss": 3.9474,
      "step": 2030
    },
    {
      "epoch": 1.0376398779247202,
      "grad_norm": 1.7476314306259155,
      "learning_rate": 4.8962360122075284e-05,
      "loss": 4.0032,
      "step": 2040
    },
    {
      "epoch": 1.0427263479145472,
      "grad_norm": 2.980959177017212,
      "learning_rate": 4.8957273652085454e-05,
      "loss": 4.0237,
      "step": 2050
    },
    {
      "epoch": 1.0478128179043744,
      "grad_norm": 1.7879271507263184,
      "learning_rate": 4.8952187182095624e-05,
      "loss": 4.0446,
      "step": 2060
    },
    {
      "epoch": 1.0528992878942014,
      "grad_norm": 2.1408605575561523,
      "learning_rate": 4.89471007121058e-05,
      "loss": 4.0334,
      "step": 2070
    },
    {
      "epoch": 1.0579857578840284,
      "grad_norm": 1.8115652799606323,
      "learning_rate": 4.894201424211598e-05,
      "loss": 4.0494,
      "step": 2080
    },
    {
      "epoch": 1.0630722278738556,
      "grad_norm": 1.5957456827163696,
      "learning_rate": 4.893692777212615e-05,
      "loss": 4.0583,
      "step": 2090
    },
    {
      "epoch": 1.0681586978636826,
      "grad_norm": 1.920944333076477,
      "learning_rate": 4.8931841302136324e-05,
      "loss": 4.0091,
      "step": 2100
    },
    {
      "epoch": 1.0732451678535098,
      "grad_norm": 1.4710344076156616,
      "learning_rate": 4.8926754832146494e-05,
      "loss": 4.0048,
      "step": 2110
    },
    {
      "epoch": 1.0783316378433367,
      "grad_norm": 2.189177989959717,
      "learning_rate": 4.8921668362156664e-05,
      "loss": 3.9725,
      "step": 2120
    },
    {
      "epoch": 1.0834181078331637,
      "grad_norm": 2.994528293609619,
      "learning_rate": 4.891658189216684e-05,
      "loss": 3.9856,
      "step": 2130
    },
    {
      "epoch": 1.088504577822991,
      "grad_norm": 2.0357632637023926,
      "learning_rate": 4.891149542217701e-05,
      "loss": 4.0452,
      "step": 2140
    },
    {
      "epoch": 1.093591047812818,
      "grad_norm": 1.451012372970581,
      "learning_rate": 4.890640895218718e-05,
      "loss": 3.9638,
      "step": 2150
    },
    {
      "epoch": 1.0986775178026449,
      "grad_norm": 1.7088543176651,
      "learning_rate": 4.890132248219736e-05,
      "loss": 3.922,
      "step": 2160
    },
    {
      "epoch": 1.103763987792472,
      "grad_norm": 1.6088171005249023,
      "learning_rate": 4.889623601220753e-05,
      "loss": 4.0466,
      "step": 2170
    },
    {
      "epoch": 1.108850457782299,
      "grad_norm": 2.0513412952423096,
      "learning_rate": 4.88911495422177e-05,
      "loss": 3.9645,
      "step": 2180
    },
    {
      "epoch": 1.113936927772126,
      "grad_norm": 2.1487507820129395,
      "learning_rate": 4.888606307222788e-05,
      "loss": 3.9821,
      "step": 2190
    },
    {
      "epoch": 1.1190233977619533,
      "grad_norm": 2.049086570739746,
      "learning_rate": 4.888097660223805e-05,
      "loss": 3.972,
      "step": 2200
    },
    {
      "epoch": 1.1241098677517802,
      "grad_norm": 1.9366912841796875,
      "learning_rate": 4.887589013224822e-05,
      "loss": 4.0042,
      "step": 2210
    },
    {
      "epoch": 1.1291963377416074,
      "grad_norm": 1.9115527868270874,
      "learning_rate": 4.8870803662258396e-05,
      "loss": 3.8792,
      "step": 2220
    },
    {
      "epoch": 1.1342828077314344,
      "grad_norm": 2.470325469970703,
      "learning_rate": 4.8865717192268566e-05,
      "loss": 3.9574,
      "step": 2230
    },
    {
      "epoch": 1.1393692777212614,
      "grad_norm": 2.3042659759521484,
      "learning_rate": 4.8860630722278736e-05,
      "loss": 4.0026,
      "step": 2240
    },
    {
      "epoch": 1.1444557477110886,
      "grad_norm": 2.3065719604492188,
      "learning_rate": 4.885554425228891e-05,
      "loss": 4.0114,
      "step": 2250
    },
    {
      "epoch": 1.1495422177009156,
      "grad_norm": 2.1007080078125,
      "learning_rate": 4.885045778229908e-05,
      "loss": 4.0336,
      "step": 2260
    },
    {
      "epoch": 1.1546286876907426,
      "grad_norm": 2.091237783432007,
      "learning_rate": 4.884537131230926e-05,
      "loss": 3.9483,
      "step": 2270
    },
    {
      "epoch": 1.1597151576805698,
      "grad_norm": 2.5165157318115234,
      "learning_rate": 4.8840284842319436e-05,
      "loss": 3.9733,
      "step": 2280
    },
    {
      "epoch": 1.1648016276703967,
      "grad_norm": 1.9170767068862915,
      "learning_rate": 4.8835198372329606e-05,
      "loss": 3.9042,
      "step": 2290
    },
    {
      "epoch": 1.1698880976602237,
      "grad_norm": 4.967627048492432,
      "learning_rate": 4.883011190233978e-05,
      "loss": 4.0384,
      "step": 2300
    },
    {
      "epoch": 1.174974567650051,
      "grad_norm": 2.1323320865631104,
      "learning_rate": 4.882502543234995e-05,
      "loss": 3.9462,
      "step": 2310
    },
    {
      "epoch": 1.180061037639878,
      "grad_norm": 1.661591649055481,
      "learning_rate": 4.881993896236012e-05,
      "loss": 3.9326,
      "step": 2320
    },
    {
      "epoch": 1.1851475076297049,
      "grad_norm": 1.5081074237823486,
      "learning_rate": 4.88148524923703e-05,
      "loss": 3.961,
      "step": 2330
    },
    {
      "epoch": 1.190233977619532,
      "grad_norm": 1.7913029193878174,
      "learning_rate": 4.880976602238047e-05,
      "loss": 3.9422,
      "step": 2340
    },
    {
      "epoch": 1.195320447609359,
      "grad_norm": 3.078305721282959,
      "learning_rate": 4.880467955239064e-05,
      "loss": 3.9303,
      "step": 2350
    },
    {
      "epoch": 1.200406917599186,
      "grad_norm": 1.8169665336608887,
      "learning_rate": 4.8799593082400816e-05,
      "loss": 3.9232,
      "step": 2360
    },
    {
      "epoch": 1.2054933875890133,
      "grad_norm": 2.2445638179779053,
      "learning_rate": 4.879450661241099e-05,
      "loss": 4.0543,
      "step": 2370
    },
    {
      "epoch": 1.2105798575788402,
      "grad_norm": 2.0995900630950928,
      "learning_rate": 4.878942014242116e-05,
      "loss": 3.9662,
      "step": 2380
    },
    {
      "epoch": 1.2156663275686674,
      "grad_norm": 2.412574052810669,
      "learning_rate": 4.878433367243134e-05,
      "loss": 3.9576,
      "step": 2390
    },
    {
      "epoch": 1.2207527975584944,
      "grad_norm": 2.9330434799194336,
      "learning_rate": 4.877924720244151e-05,
      "loss": 3.964,
      "step": 2400
    },
    {
      "epoch": 1.2258392675483214,
      "grad_norm": 2.936581611633301,
      "learning_rate": 4.877416073245168e-05,
      "loss": 3.8388,
      "step": 2410
    },
    {
      "epoch": 1.2309257375381486,
      "grad_norm": 3.362020254135132,
      "learning_rate": 4.8769074262461855e-05,
      "loss": 3.956,
      "step": 2420
    },
    {
      "epoch": 1.2360122075279756,
      "grad_norm": 2.030841588973999,
      "learning_rate": 4.8763987792472025e-05,
      "loss": 3.891,
      "step": 2430
    },
    {
      "epoch": 1.2410986775178026,
      "grad_norm": 2.020311117172241,
      "learning_rate": 4.8758901322482195e-05,
      "loss": 3.8952,
      "step": 2440
    },
    {
      "epoch": 1.2461851475076298,
      "grad_norm": 1.9764037132263184,
      "learning_rate": 4.875381485249237e-05,
      "loss": 3.9623,
      "step": 2450
    },
    {
      "epoch": 1.2512716174974567,
      "grad_norm": 1.9649181365966797,
      "learning_rate": 4.874872838250255e-05,
      "loss": 3.9677,
      "step": 2460
    },
    {
      "epoch": 1.2563580874872837,
      "grad_norm": 2.5071680545806885,
      "learning_rate": 4.874364191251272e-05,
      "loss": 4.0276,
      "step": 2470
    },
    {
      "epoch": 1.261444557477111,
      "grad_norm": 2.068830966949463,
      "learning_rate": 4.8738555442522895e-05,
      "loss": 3.9646,
      "step": 2480
    },
    {
      "epoch": 1.266531027466938,
      "grad_norm": 2.0060958862304688,
      "learning_rate": 4.8733468972533065e-05,
      "loss": 3.9964,
      "step": 2490
    },
    {
      "epoch": 1.2716174974567651,
      "grad_norm": 1.7277652025222778,
      "learning_rate": 4.8728382502543235e-05,
      "loss": 3.9813,
      "step": 2500
    },
    {
      "epoch": 1.276703967446592,
      "grad_norm": 2.3274965286254883,
      "learning_rate": 4.872329603255341e-05,
      "loss": 3.9548,
      "step": 2510
    },
    {
      "epoch": 1.281790437436419,
      "grad_norm": 2.2281136512756348,
      "learning_rate": 4.871820956256358e-05,
      "loss": 3.9165,
      "step": 2520
    },
    {
      "epoch": 1.286876907426246,
      "grad_norm": 2.5854625701904297,
      "learning_rate": 4.871312309257375e-05,
      "loss": 4.0116,
      "step": 2530
    },
    {
      "epoch": 1.2919633774160733,
      "grad_norm": 2.83828067779541,
      "learning_rate": 4.870803662258393e-05,
      "loss": 3.9706,
      "step": 2540
    },
    {
      "epoch": 1.2970498474059002,
      "grad_norm": 2.1678407192230225,
      "learning_rate": 4.8702950152594105e-05,
      "loss": 3.9299,
      "step": 2550
    },
    {
      "epoch": 1.3021363173957274,
      "grad_norm": 1.9562280178070068,
      "learning_rate": 4.8697863682604274e-05,
      "loss": 3.8692,
      "step": 2560
    },
    {
      "epoch": 1.3072227873855544,
      "grad_norm": 2.868701457977295,
      "learning_rate": 4.869277721261445e-05,
      "loss": 3.9135,
      "step": 2570
    },
    {
      "epoch": 1.3123092573753814,
      "grad_norm": 2.8124570846557617,
      "learning_rate": 4.868769074262462e-05,
      "loss": 3.9744,
      "step": 2580
    },
    {
      "epoch": 1.3173957273652086,
      "grad_norm": 2.3565664291381836,
      "learning_rate": 4.86826042726348e-05,
      "loss": 3.8869,
      "step": 2590
    },
    {
      "epoch": 1.3224821973550356,
      "grad_norm": 2.8008124828338623,
      "learning_rate": 4.867751780264497e-05,
      "loss": 3.9142,
      "step": 2600
    },
    {
      "epoch": 1.3275686673448628,
      "grad_norm": 2.322633743286133,
      "learning_rate": 4.867243133265514e-05,
      "loss": 3.9832,
      "step": 2610
    },
    {
      "epoch": 1.3326551373346898,
      "grad_norm": 3.3832225799560547,
      "learning_rate": 4.8667344862665314e-05,
      "loss": 3.9179,
      "step": 2620
    },
    {
      "epoch": 1.3377416073245167,
      "grad_norm": 2.094360113143921,
      "learning_rate": 4.8662258392675484e-05,
      "loss": 4.0058,
      "step": 2630
    },
    {
      "epoch": 1.3428280773143437,
      "grad_norm": 2.2931244373321533,
      "learning_rate": 4.8657171922685654e-05,
      "loss": 3.9254,
      "step": 2640
    },
    {
      "epoch": 1.347914547304171,
      "grad_norm": 2.352853775024414,
      "learning_rate": 4.865208545269583e-05,
      "loss": 3.9809,
      "step": 2650
    },
    {
      "epoch": 1.353001017293998,
      "grad_norm": 2.848780870437622,
      "learning_rate": 4.864699898270601e-05,
      "loss": 3.9095,
      "step": 2660
    },
    {
      "epoch": 1.3580874872838251,
      "grad_norm": 2.9247162342071533,
      "learning_rate": 4.864191251271618e-05,
      "loss": 3.9776,
      "step": 2670
    },
    {
      "epoch": 1.363173957273652,
      "grad_norm": 2.6121084690093994,
      "learning_rate": 4.8636826042726354e-05,
      "loss": 3.8975,
      "step": 2680
    },
    {
      "epoch": 1.368260427263479,
      "grad_norm": 2.5569818019866943,
      "learning_rate": 4.8631739572736524e-05,
      "loss": 3.9456,
      "step": 2690
    },
    {
      "epoch": 1.3733468972533063,
      "grad_norm": 2.369204044342041,
      "learning_rate": 4.8626653102746694e-05,
      "loss": 3.8224,
      "step": 2700
    },
    {
      "epoch": 1.3784333672431333,
      "grad_norm": 2.3101742267608643,
      "learning_rate": 4.862156663275687e-05,
      "loss": 3.9542,
      "step": 2710
    },
    {
      "epoch": 1.3835198372329605,
      "grad_norm": 3.3624606132507324,
      "learning_rate": 4.861648016276704e-05,
      "loss": 3.913,
      "step": 2720
    },
    {
      "epoch": 1.3886063072227874,
      "grad_norm": 3.4250571727752686,
      "learning_rate": 4.861139369277721e-05,
      "loss": 3.956,
      "step": 2730
    },
    {
      "epoch": 1.3936927772126144,
      "grad_norm": 2.3920998573303223,
      "learning_rate": 4.860630722278739e-05,
      "loss": 3.8821,
      "step": 2740
    },
    {
      "epoch": 1.3987792472024414,
      "grad_norm": 3.124845504760742,
      "learning_rate": 4.860122075279756e-05,
      "loss": 3.9093,
      "step": 2750
    },
    {
      "epoch": 1.4038657171922686,
      "grad_norm": 2.3216707706451416,
      "learning_rate": 4.859613428280773e-05,
      "loss": 3.9168,
      "step": 2760
    },
    {
      "epoch": 1.4089521871820956,
      "grad_norm": 2.610485076904297,
      "learning_rate": 4.859104781281791e-05,
      "loss": 3.9388,
      "step": 2770
    },
    {
      "epoch": 1.4140386571719228,
      "grad_norm": 2.4689815044403076,
      "learning_rate": 4.858596134282808e-05,
      "loss": 3.9255,
      "step": 2780
    },
    {
      "epoch": 1.4191251271617498,
      "grad_norm": 2.6871395111083984,
      "learning_rate": 4.858087487283825e-05,
      "loss": 3.8903,
      "step": 2790
    },
    {
      "epoch": 1.4242115971515767,
      "grad_norm": 3.648493766784668,
      "learning_rate": 4.8575788402848426e-05,
      "loss": 3.9589,
      "step": 2800
    },
    {
      "epoch": 1.4292980671414037,
      "grad_norm": 2.795381784439087,
      "learning_rate": 4.8570701932858596e-05,
      "loss": 3.8458,
      "step": 2810
    },
    {
      "epoch": 1.434384537131231,
      "grad_norm": 2.672447681427002,
      "learning_rate": 4.8565615462868766e-05,
      "loss": 3.8601,
      "step": 2820
    },
    {
      "epoch": 1.439471007121058,
      "grad_norm": 2.0746052265167236,
      "learning_rate": 4.856052899287894e-05,
      "loss": 3.9126,
      "step": 2830
    },
    {
      "epoch": 1.4445574771108851,
      "grad_norm": 2.3404409885406494,
      "learning_rate": 4.855544252288912e-05,
      "loss": 3.8788,
      "step": 2840
    },
    {
      "epoch": 1.449643947100712,
      "grad_norm": 2.185244083404541,
      "learning_rate": 4.8550356052899296e-05,
      "loss": 3.922,
      "step": 2850
    },
    {
      "epoch": 1.454730417090539,
      "grad_norm": 1.6734259128570557,
      "learning_rate": 4.8545269582909466e-05,
      "loss": 3.981,
      "step": 2860
    },
    {
      "epoch": 1.4598168870803663,
      "grad_norm": 4.593475341796875,
      "learning_rate": 4.8540183112919636e-05,
      "loss": 3.8676,
      "step": 2870
    },
    {
      "epoch": 1.4649033570701933,
      "grad_norm": 1.9578518867492676,
      "learning_rate": 4.853509664292981e-05,
      "loss": 3.8711,
      "step": 2880
    },
    {
      "epoch": 1.4699898270600205,
      "grad_norm": 2.0782599449157715,
      "learning_rate": 4.853001017293998e-05,
      "loss": 3.9,
      "step": 2890
    },
    {
      "epoch": 1.4750762970498474,
      "grad_norm": 2.2813661098480225,
      "learning_rate": 4.852492370295015e-05,
      "loss": 3.8609,
      "step": 2900
    },
    {
      "epoch": 1.4801627670396744,
      "grad_norm": 2.7098612785339355,
      "learning_rate": 4.851983723296033e-05,
      "loss": 3.8749,
      "step": 2910
    },
    {
      "epoch": 1.4852492370295014,
      "grad_norm": 2.0379207134246826,
      "learning_rate": 4.85147507629705e-05,
      "loss": 3.8275,
      "step": 2920
    },
    {
      "epoch": 1.4903357070193286,
      "grad_norm": 2.3257503509521484,
      "learning_rate": 4.850966429298067e-05,
      "loss": 3.9086,
      "step": 2930
    },
    {
      "epoch": 1.4954221770091556,
      "grad_norm": 2.545694351196289,
      "learning_rate": 4.8504577822990846e-05,
      "loss": 3.9452,
      "step": 2940
    },
    {
      "epoch": 1.5005086469989828,
      "grad_norm": 2.1215734481811523,
      "learning_rate": 4.849949135300102e-05,
      "loss": 3.8586,
      "step": 2950
    },
    {
      "epoch": 1.5055951169888098,
      "grad_norm": 2.360004186630249,
      "learning_rate": 4.849440488301119e-05,
      "loss": 3.8503,
      "step": 2960
    },
    {
      "epoch": 1.5106815869786367,
      "grad_norm": 2.5942797660827637,
      "learning_rate": 4.848931841302137e-05,
      "loss": 3.9301,
      "step": 2970
    },
    {
      "epoch": 1.5157680569684637,
      "grad_norm": 3.23411226272583,
      "learning_rate": 4.848423194303154e-05,
      "loss": 3.8558,
      "step": 2980
    },
    {
      "epoch": 1.520854526958291,
      "grad_norm": 1.8477108478546143,
      "learning_rate": 4.847914547304171e-05,
      "loss": 3.9348,
      "step": 2990
    },
    {
      "epoch": 1.5259409969481181,
      "grad_norm": 2.799643039703369,
      "learning_rate": 4.8474059003051885e-05,
      "loss": 3.8646,
      "step": 3000
    },
    {
      "epoch": 1.5310274669379451,
      "grad_norm": 2.920898199081421,
      "learning_rate": 4.8468972533062055e-05,
      "loss": 3.8698,
      "step": 3010
    },
    {
      "epoch": 1.536113936927772,
      "grad_norm": 2.8635525703430176,
      "learning_rate": 4.8463886063072225e-05,
      "loss": 3.8772,
      "step": 3020
    },
    {
      "epoch": 1.541200406917599,
      "grad_norm": 2.574653387069702,
      "learning_rate": 4.84587995930824e-05,
      "loss": 3.9236,
      "step": 3030
    },
    {
      "epoch": 1.5462868769074263,
      "grad_norm": 2.6386682987213135,
      "learning_rate": 4.845371312309258e-05,
      "loss": 3.8992,
      "step": 3040
    },
    {
      "epoch": 1.5513733468972533,
      "grad_norm": 2.2035412788391113,
      "learning_rate": 4.844862665310275e-05,
      "loss": 3.9024,
      "step": 3050
    },
    {
      "epoch": 1.5564598168870805,
      "grad_norm": 3.2510619163513184,
      "learning_rate": 4.8443540183112925e-05,
      "loss": 3.874,
      "step": 3060
    },
    {
      "epoch": 1.5615462868769074,
      "grad_norm": 2.594196081161499,
      "learning_rate": 4.8438453713123095e-05,
      "loss": 3.8774,
      "step": 3070
    },
    {
      "epoch": 1.5666327568667344,
      "grad_norm": 2.3913445472717285,
      "learning_rate": 4.8433367243133265e-05,
      "loss": 3.942,
      "step": 3080
    },
    {
      "epoch": 1.5717192268565614,
      "grad_norm": 2.5080785751342773,
      "learning_rate": 4.842828077314344e-05,
      "loss": 3.8608,
      "step": 3090
    },
    {
      "epoch": 1.5768056968463886,
      "grad_norm": 3.553894281387329,
      "learning_rate": 4.842319430315361e-05,
      "loss": 3.8844,
      "step": 3100
    },
    {
      "epoch": 1.5818921668362158,
      "grad_norm": 2.492377996444702,
      "learning_rate": 4.841810783316379e-05,
      "loss": 3.8377,
      "step": 3110
    },
    {
      "epoch": 1.5869786368260428,
      "grad_norm": 3.0864450931549072,
      "learning_rate": 4.841302136317396e-05,
      "loss": 3.8689,
      "step": 3120
    },
    {
      "epoch": 1.5920651068158698,
      "grad_norm": 2.9161736965179443,
      "learning_rate": 4.8407934893184135e-05,
      "loss": 3.8766,
      "step": 3130
    },
    {
      "epoch": 1.5971515768056967,
      "grad_norm": 3.6836953163146973,
      "learning_rate": 4.840284842319431e-05,
      "loss": 3.8788,
      "step": 3140
    },
    {
      "epoch": 1.602238046795524,
      "grad_norm": 1.6159393787384033,
      "learning_rate": 4.839776195320448e-05,
      "loss": 3.9041,
      "step": 3150
    },
    {
      "epoch": 1.607324516785351,
      "grad_norm": 2.0169739723205566,
      "learning_rate": 4.839267548321465e-05,
      "loss": 3.9405,
      "step": 3160
    },
    {
      "epoch": 1.6124109867751781,
      "grad_norm": 2.210364818572998,
      "learning_rate": 4.838758901322483e-05,
      "loss": 3.8437,
      "step": 3170
    },
    {
      "epoch": 1.6174974567650051,
      "grad_norm": 2.516774892807007,
      "learning_rate": 4.8382502543235e-05,
      "loss": 3.8978,
      "step": 3180
    },
    {
      "epoch": 1.622583926754832,
      "grad_norm": 2.3684165477752686,
      "learning_rate": 4.837741607324517e-05,
      "loss": 3.7832,
      "step": 3190
    },
    {
      "epoch": 1.627670396744659,
      "grad_norm": 2.970245122909546,
      "learning_rate": 4.8372329603255344e-05,
      "loss": 3.8783,
      "step": 3200
    },
    {
      "epoch": 1.6327568667344863,
      "grad_norm": 2.5396718978881836,
      "learning_rate": 4.8367243133265514e-05,
      "loss": 3.8753,
      "step": 3210
    },
    {
      "epoch": 1.6378433367243135,
      "grad_norm": 2.4317190647125244,
      "learning_rate": 4.836215666327569e-05,
      "loss": 3.8845,
      "step": 3220
    },
    {
      "epoch": 1.6429298067141405,
      "grad_norm": 2.0661675930023193,
      "learning_rate": 4.835707019328586e-05,
      "loss": 3.9072,
      "step": 3230
    },
    {
      "epoch": 1.6480162767039674,
      "grad_norm": 3.3051681518554688,
      "learning_rate": 4.835198372329604e-05,
      "loss": 3.856,
      "step": 3240
    },
    {
      "epoch": 1.6531027466937944,
      "grad_norm": 2.246716022491455,
      "learning_rate": 4.834689725330621e-05,
      "loss": 3.881,
      "step": 3250
    },
    {
      "epoch": 1.6581892166836214,
      "grad_norm": 3.5119080543518066,
      "learning_rate": 4.8341810783316384e-05,
      "loss": 3.8539,
      "step": 3260
    },
    {
      "epoch": 1.6632756866734486,
      "grad_norm": 3.4491398334503174,
      "learning_rate": 4.8336724313326554e-05,
      "loss": 3.8632,
      "step": 3270
    },
    {
      "epoch": 1.6683621566632758,
      "grad_norm": 2.0863068103790283,
      "learning_rate": 4.8331637843336724e-05,
      "loss": 3.8179,
      "step": 3280
    },
    {
      "epoch": 1.6734486266531028,
      "grad_norm": 3.18312931060791,
      "learning_rate": 4.83265513733469e-05,
      "loss": 3.8362,
      "step": 3290
    },
    {
      "epoch": 1.6785350966429298,
      "grad_norm": 3.3505122661590576,
      "learning_rate": 4.832146490335707e-05,
      "loss": 3.8541,
      "step": 3300
    },
    {
      "epoch": 1.6836215666327567,
      "grad_norm": 2.358457326889038,
      "learning_rate": 4.831637843336724e-05,
      "loss": 3.9114,
      "step": 3310
    },
    {
      "epoch": 1.688708036622584,
      "grad_norm": 2.8709821701049805,
      "learning_rate": 4.831129196337742e-05,
      "loss": 3.8215,
      "step": 3320
    },
    {
      "epoch": 1.693794506612411,
      "grad_norm": 3.471684694290161,
      "learning_rate": 4.830620549338759e-05,
      "loss": 3.8808,
      "step": 3330
    },
    {
      "epoch": 1.6988809766022381,
      "grad_norm": 2.349759817123413,
      "learning_rate": 4.830111902339776e-05,
      "loss": 3.8793,
      "step": 3340
    },
    {
      "epoch": 1.7039674465920651,
      "grad_norm": 3.098581075668335,
      "learning_rate": 4.829603255340794e-05,
      "loss": 3.9247,
      "step": 3350
    },
    {
      "epoch": 1.709053916581892,
      "grad_norm": 3.3885202407836914,
      "learning_rate": 4.829094608341811e-05,
      "loss": 3.8473,
      "step": 3360
    },
    {
      "epoch": 1.714140386571719,
      "grad_norm": 2.7713000774383545,
      "learning_rate": 4.828585961342828e-05,
      "loss": 3.8542,
      "step": 3370
    },
    {
      "epoch": 1.7192268565615463,
      "grad_norm": 2.0381813049316406,
      "learning_rate": 4.8280773143438456e-05,
      "loss": 3.8307,
      "step": 3380
    },
    {
      "epoch": 1.7243133265513735,
      "grad_norm": 2.5406320095062256,
      "learning_rate": 4.8275686673448626e-05,
      "loss": 3.8529,
      "step": 3390
    },
    {
      "epoch": 1.7293997965412005,
      "grad_norm": 4.186591625213623,
      "learning_rate": 4.82706002034588e-05,
      "loss": 3.8466,
      "step": 3400
    },
    {
      "epoch": 1.7344862665310274,
      "grad_norm": 2.4478073120117188,
      "learning_rate": 4.826551373346897e-05,
      "loss": 3.8878,
      "step": 3410
    },
    {
      "epoch": 1.7395727365208544,
      "grad_norm": 3.5367841720581055,
      "learning_rate": 4.826042726347915e-05,
      "loss": 3.9177,
      "step": 3420
    },
    {
      "epoch": 1.7446592065106816,
      "grad_norm": 2.907881498336792,
      "learning_rate": 4.8255340793489326e-05,
      "loss": 3.8506,
      "step": 3430
    },
    {
      "epoch": 1.7497456765005086,
      "grad_norm": 3.356942892074585,
      "learning_rate": 4.8250254323499496e-05,
      "loss": 3.8961,
      "step": 3440
    },
    {
      "epoch": 1.7548321464903358,
      "grad_norm": 2.443355083465576,
      "learning_rate": 4.8245167853509666e-05,
      "loss": 3.8786,
      "step": 3450
    },
    {
      "epoch": 1.7599186164801628,
      "grad_norm": 2.973762035369873,
      "learning_rate": 4.824008138351984e-05,
      "loss": 3.8106,
      "step": 3460
    },
    {
      "epoch": 1.7650050864699898,
      "grad_norm": 2.508446455001831,
      "learning_rate": 4.823499491353001e-05,
      "loss": 3.8803,
      "step": 3470
    },
    {
      "epoch": 1.7700915564598168,
      "grad_norm": 2.3010764122009277,
      "learning_rate": 4.822990844354018e-05,
      "loss": 3.8068,
      "step": 3480
    },
    {
      "epoch": 1.775178026449644,
      "grad_norm": 2.415297269821167,
      "learning_rate": 4.822482197355036e-05,
      "loss": 3.82,
      "step": 3490
    },
    {
      "epoch": 1.7802644964394712,
      "grad_norm": 3.997767448425293,
      "learning_rate": 4.821973550356053e-05,
      "loss": 3.8387,
      "step": 3500
    },
    {
      "epoch": 1.7853509664292981,
      "grad_norm": 3.0259130001068115,
      "learning_rate": 4.8214649033570706e-05,
      "loss": 3.7925,
      "step": 3510
    },
    {
      "epoch": 1.7904374364191251,
      "grad_norm": 2.2652039527893066,
      "learning_rate": 4.820956256358088e-05,
      "loss": 3.8723,
      "step": 3520
    },
    {
      "epoch": 1.795523906408952,
      "grad_norm": 2.7250819206237793,
      "learning_rate": 4.820447609359105e-05,
      "loss": 3.7803,
      "step": 3530
    },
    {
      "epoch": 1.8006103763987793,
      "grad_norm": 3.3924481868743896,
      "learning_rate": 4.819938962360122e-05,
      "loss": 3.8226,
      "step": 3540
    },
    {
      "epoch": 1.8056968463886063,
      "grad_norm": 2.620333433151245,
      "learning_rate": 4.81943031536114e-05,
      "loss": 3.7568,
      "step": 3550
    },
    {
      "epoch": 1.8107833163784335,
      "grad_norm": 2.371976852416992,
      "learning_rate": 4.818921668362157e-05,
      "loss": 3.8788,
      "step": 3560
    },
    {
      "epoch": 1.8158697863682605,
      "grad_norm": 2.5696659088134766,
      "learning_rate": 4.818413021363174e-05,
      "loss": 3.8142,
      "step": 3570
    },
    {
      "epoch": 1.8209562563580874,
      "grad_norm": 2.553067684173584,
      "learning_rate": 4.8179043743641915e-05,
      "loss": 3.7704,
      "step": 3580
    },
    {
      "epoch": 1.8260427263479144,
      "grad_norm": 3.435072183609009,
      "learning_rate": 4.8173957273652085e-05,
      "loss": 3.7701,
      "step": 3590
    },
    {
      "epoch": 1.8311291963377416,
      "grad_norm": 3.1979541778564453,
      "learning_rate": 4.8168870803662255e-05,
      "loss": 3.8175,
      "step": 3600
    },
    {
      "epoch": 1.8362156663275688,
      "grad_norm": 2.2606077194213867,
      "learning_rate": 4.816378433367243e-05,
      "loss": 3.8208,
      "step": 3610
    },
    {
      "epoch": 1.8413021363173958,
      "grad_norm": 2.1618783473968506,
      "learning_rate": 4.815869786368261e-05,
      "loss": 3.8127,
      "step": 3620
    },
    {
      "epoch": 1.8463886063072228,
      "grad_norm": 2.9220941066741943,
      "learning_rate": 4.815361139369278e-05,
      "loss": 3.8216,
      "step": 3630
    },
    {
      "epoch": 1.8514750762970498,
      "grad_norm": 2.887899160385132,
      "learning_rate": 4.8148524923702955e-05,
      "loss": 3.853,
      "step": 3640
    },
    {
      "epoch": 1.8565615462868768,
      "grad_norm": 3.247743606567383,
      "learning_rate": 4.8143438453713125e-05,
      "loss": 3.764,
      "step": 3650
    },
    {
      "epoch": 1.861648016276704,
      "grad_norm": 2.6104135513305664,
      "learning_rate": 4.81383519837233e-05,
      "loss": 3.7908,
      "step": 3660
    },
    {
      "epoch": 1.8667344862665312,
      "grad_norm": 3.5066661834716797,
      "learning_rate": 4.813326551373347e-05,
      "loss": 3.8373,
      "step": 3670
    },
    {
      "epoch": 1.8718209562563581,
      "grad_norm": 3.195223331451416,
      "learning_rate": 4.812817904374364e-05,
      "loss": 3.8645,
      "step": 3680
    },
    {
      "epoch": 1.8769074262461851,
      "grad_norm": 2.857661008834839,
      "learning_rate": 4.812309257375382e-05,
      "loss": 3.8556,
      "step": 3690
    },
    {
      "epoch": 1.881993896236012,
      "grad_norm": 2.8105034828186035,
      "learning_rate": 4.811800610376399e-05,
      "loss": 3.8007,
      "step": 3700
    },
    {
      "epoch": 1.8870803662258393,
      "grad_norm": 3.4952337741851807,
      "learning_rate": 4.8112919633774165e-05,
      "loss": 3.7928,
      "step": 3710
    },
    {
      "epoch": 1.8921668362156663,
      "grad_norm": 3.942455768585205,
      "learning_rate": 4.810783316378434e-05,
      "loss": 3.9054,
      "step": 3720
    },
    {
      "epoch": 1.8972533062054935,
      "grad_norm": 2.816209554672241,
      "learning_rate": 4.810274669379451e-05,
      "loss": 3.7837,
      "step": 3730
    },
    {
      "epoch": 1.9023397761953205,
      "grad_norm": 3.0272772312164307,
      "learning_rate": 4.809766022380468e-05,
      "loss": 3.8001,
      "step": 3740
    },
    {
      "epoch": 1.9074262461851474,
      "grad_norm": 3.9427623748779297,
      "learning_rate": 4.809257375381486e-05,
      "loss": 3.8422,
      "step": 3750
    },
    {
      "epoch": 1.9125127161749744,
      "grad_norm": 3.2254321575164795,
      "learning_rate": 4.808748728382503e-05,
      "loss": 3.8652,
      "step": 3760
    },
    {
      "epoch": 1.9175991861648016,
      "grad_norm": 2.9322938919067383,
      "learning_rate": 4.80824008138352e-05,
      "loss": 3.7948,
      "step": 3770
    },
    {
      "epoch": 1.9226856561546288,
      "grad_norm": 3.0695714950561523,
      "learning_rate": 4.8077314343845374e-05,
      "loss": 3.7794,
      "step": 3780
    },
    {
      "epoch": 1.9277721261444558,
      "grad_norm": 2.085655450820923,
      "learning_rate": 4.8072227873855544e-05,
      "loss": 3.7606,
      "step": 3790
    },
    {
      "epoch": 1.9328585961342828,
      "grad_norm": 5.870607852935791,
      "learning_rate": 4.806714140386572e-05,
      "loss": 3.7976,
      "step": 3800
    },
    {
      "epoch": 1.9379450661241098,
      "grad_norm": 3.239985704421997,
      "learning_rate": 4.80620549338759e-05,
      "loss": 3.792,
      "step": 3810
    },
    {
      "epoch": 1.943031536113937,
      "grad_norm": 2.699589490890503,
      "learning_rate": 4.805696846388607e-05,
      "loss": 3.7408,
      "step": 3820
    },
    {
      "epoch": 1.948118006103764,
      "grad_norm": 2.924009084701538,
      "learning_rate": 4.805188199389624e-05,
      "loss": 3.846,
      "step": 3830
    },
    {
      "epoch": 1.9532044760935912,
      "grad_norm": 3.431859016418457,
      "learning_rate": 4.8046795523906414e-05,
      "loss": 3.8322,
      "step": 3840
    },
    {
      "epoch": 1.9582909460834181,
      "grad_norm": 4.013307094573975,
      "learning_rate": 4.8041709053916584e-05,
      "loss": 3.7754,
      "step": 3850
    },
    {
      "epoch": 1.9633774160732451,
      "grad_norm": 3.6749062538146973,
      "learning_rate": 4.8036622583926754e-05,
      "loss": 3.9102,
      "step": 3860
    },
    {
      "epoch": 1.968463886063072,
      "grad_norm": 5.978215217590332,
      "learning_rate": 4.803153611393693e-05,
      "loss": 3.8084,
      "step": 3870
    },
    {
      "epoch": 1.9735503560528993,
      "grad_norm": 3.1038668155670166,
      "learning_rate": 4.80264496439471e-05,
      "loss": 3.8361,
      "step": 3880
    },
    {
      "epoch": 1.9786368260427265,
      "grad_norm": 4.393378257751465,
      "learning_rate": 4.802136317395727e-05,
      "loss": 3.8265,
      "step": 3890
    },
    {
      "epoch": 1.9837232960325535,
      "grad_norm": 3.2907605171203613,
      "learning_rate": 4.801627670396745e-05,
      "loss": 3.7734,
      "step": 3900
    },
    {
      "epoch": 1.9888097660223805,
      "grad_norm": 3.6339643001556396,
      "learning_rate": 4.801119023397762e-05,
      "loss": 3.8448,
      "step": 3910
    },
    {
      "epoch": 1.9938962360122074,
      "grad_norm": 4.090737819671631,
      "learning_rate": 4.80061037639878e-05,
      "loss": 3.7712,
      "step": 3920
    },
    {
      "epoch": 1.9989827060020344,
      "grad_norm": 2.9533932209014893,
      "learning_rate": 4.800101729399797e-05,
      "loss": 3.8954,
      "step": 3930
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.826873540878296,
      "eval_runtime": 2.6248,
      "eval_samples_per_second": 1057.207,
      "eval_steps_per_second": 132.198,
      "step": 3932
    },
    {
      "epoch": 2.004069175991862,
      "grad_norm": 3.4182682037353516,
      "learning_rate": 4.799593082400814e-05,
      "loss": 3.7912,
      "step": 3940
    },
    {
      "epoch": 2.009155645981689,
      "grad_norm": 3.128972053527832,
      "learning_rate": 4.7990844354018317e-05,
      "loss": 3.71,
      "step": 3950
    },
    {
      "epoch": 2.014242115971516,
      "grad_norm": 3.345815420150757,
      "learning_rate": 4.7985757884028486e-05,
      "loss": 3.8257,
      "step": 3960
    },
    {
      "epoch": 2.019328585961343,
      "grad_norm": 3.755927562713623,
      "learning_rate": 4.7980671414038656e-05,
      "loss": 3.8506,
      "step": 3970
    },
    {
      "epoch": 2.0244150559511698,
      "grad_norm": 3.6383376121520996,
      "learning_rate": 4.797558494404883e-05,
      "loss": 3.8179,
      "step": 3980
    },
    {
      "epoch": 2.0295015259409968,
      "grad_norm": 3.574094772338867,
      "learning_rate": 4.7970498474059e-05,
      "loss": 3.6932,
      "step": 3990
    },
    {
      "epoch": 2.034587995930824,
      "grad_norm": 5.699164867401123,
      "learning_rate": 4.796541200406918e-05,
      "loss": 3.7792,
      "step": 4000
    },
    {
      "epoch": 2.039674465920651,
      "grad_norm": 2.5300450325012207,
      "learning_rate": 4.7960325534079356e-05,
      "loss": 3.9256,
      "step": 4010
    },
    {
      "epoch": 2.044760935910478,
      "grad_norm": 2.5193402767181396,
      "learning_rate": 4.7955239064089526e-05,
      "loss": 3.7365,
      "step": 4020
    },
    {
      "epoch": 2.049847405900305,
      "grad_norm": 3.6600091457366943,
      "learning_rate": 4.7950152594099696e-05,
      "loss": 3.724,
      "step": 4030
    },
    {
      "epoch": 2.054933875890132,
      "grad_norm": 2.681989908218384,
      "learning_rate": 4.794506612410987e-05,
      "loss": 3.795,
      "step": 4040
    },
    {
      "epoch": 2.0600203458799595,
      "grad_norm": 3.5618276596069336,
      "learning_rate": 4.793997965412004e-05,
      "loss": 3.7844,
      "step": 4050
    },
    {
      "epoch": 2.0651068158697865,
      "grad_norm": 2.793653964996338,
      "learning_rate": 4.793489318413021e-05,
      "loss": 3.75,
      "step": 4060
    },
    {
      "epoch": 2.0701932858596135,
      "grad_norm": 2.8173346519470215,
      "learning_rate": 4.792980671414039e-05,
      "loss": 3.8715,
      "step": 4070
    },
    {
      "epoch": 2.0752797558494405,
      "grad_norm": 2.6671271324157715,
      "learning_rate": 4.792472024415056e-05,
      "loss": 3.8105,
      "step": 4080
    },
    {
      "epoch": 2.0803662258392674,
      "grad_norm": 3.1799519062042236,
      "learning_rate": 4.7919633774160736e-05,
      "loss": 3.7614,
      "step": 4090
    },
    {
      "epoch": 2.0854526958290944,
      "grad_norm": 2.8281126022338867,
      "learning_rate": 4.791454730417091e-05,
      "loss": 3.7336,
      "step": 4100
    },
    {
      "epoch": 2.090539165818922,
      "grad_norm": 3.216190814971924,
      "learning_rate": 4.790946083418108e-05,
      "loss": 3.7321,
      "step": 4110
    },
    {
      "epoch": 2.095625635808749,
      "grad_norm": 3.920994758605957,
      "learning_rate": 4.790437436419125e-05,
      "loss": 3.8055,
      "step": 4120
    },
    {
      "epoch": 2.100712105798576,
      "grad_norm": 2.9823126792907715,
      "learning_rate": 4.789928789420143e-05,
      "loss": 3.8428,
      "step": 4130
    },
    {
      "epoch": 2.105798575788403,
      "grad_norm": 3.5184895992279053,
      "learning_rate": 4.78942014242116e-05,
      "loss": 3.8003,
      "step": 4140
    },
    {
      "epoch": 2.1108850457782298,
      "grad_norm": 2.820263147354126,
      "learning_rate": 4.788911495422177e-05,
      "loss": 3.8142,
      "step": 4150
    },
    {
      "epoch": 2.1159715157680568,
      "grad_norm": 4.79014778137207,
      "learning_rate": 4.7884028484231945e-05,
      "loss": 3.7358,
      "step": 4160
    },
    {
      "epoch": 2.121057985757884,
      "grad_norm": 2.4953773021698,
      "learning_rate": 4.7878942014242115e-05,
      "loss": 3.7089,
      "step": 4170
    },
    {
      "epoch": 2.126144455747711,
      "grad_norm": 3.46655011177063,
      "learning_rate": 4.787385554425229e-05,
      "loss": 3.7416,
      "step": 4180
    },
    {
      "epoch": 2.131230925737538,
      "grad_norm": 3.9183313846588135,
      "learning_rate": 4.786876907426246e-05,
      "loss": 3.8816,
      "step": 4190
    },
    {
      "epoch": 2.136317395727365,
      "grad_norm": 3.1138110160827637,
      "learning_rate": 4.786368260427264e-05,
      "loss": 3.7934,
      "step": 4200
    },
    {
      "epoch": 2.141403865717192,
      "grad_norm": 3.0591883659362793,
      "learning_rate": 4.7858596134282815e-05,
      "loss": 3.6891,
      "step": 4210
    },
    {
      "epoch": 2.1464903357070195,
      "grad_norm": 2.8712549209594727,
      "learning_rate": 4.7853509664292985e-05,
      "loss": 3.7532,
      "step": 4220
    },
    {
      "epoch": 2.1515768056968465,
      "grad_norm": 4.400040149688721,
      "learning_rate": 4.7848423194303155e-05,
      "loss": 3.7919,
      "step": 4230
    },
    {
      "epoch": 2.1566632756866735,
      "grad_norm": 3.4683656692504883,
      "learning_rate": 4.784333672431333e-05,
      "loss": 3.7651,
      "step": 4240
    },
    {
      "epoch": 2.1617497456765005,
      "grad_norm": 4.072911262512207,
      "learning_rate": 4.78382502543235e-05,
      "loss": 3.6897,
      "step": 4250
    },
    {
      "epoch": 2.1668362156663274,
      "grad_norm": 3.559427261352539,
      "learning_rate": 4.783316378433367e-05,
      "loss": 3.8219,
      "step": 4260
    },
    {
      "epoch": 2.1719226856561544,
      "grad_norm": 2.260404109954834,
      "learning_rate": 4.782807731434385e-05,
      "loss": 3.775,
      "step": 4270
    },
    {
      "epoch": 2.177009155645982,
      "grad_norm": 3.906188488006592,
      "learning_rate": 4.782299084435402e-05,
      "loss": 3.7592,
      "step": 4280
    },
    {
      "epoch": 2.182095625635809,
      "grad_norm": 3.158396005630493,
      "learning_rate": 4.7817904374364195e-05,
      "loss": 3.7613,
      "step": 4290
    },
    {
      "epoch": 2.187182095625636,
      "grad_norm": 2.6394829750061035,
      "learning_rate": 4.781281790437437e-05,
      "loss": 3.7168,
      "step": 4300
    },
    {
      "epoch": 2.192268565615463,
      "grad_norm": 2.850581169128418,
      "learning_rate": 4.780773143438454e-05,
      "loss": 3.762,
      "step": 4310
    },
    {
      "epoch": 2.1973550356052898,
      "grad_norm": 2.5697829723358154,
      "learning_rate": 4.780264496439471e-05,
      "loss": 3.8284,
      "step": 4320
    },
    {
      "epoch": 2.202441505595117,
      "grad_norm": 3.659562587738037,
      "learning_rate": 4.779755849440489e-05,
      "loss": 3.7848,
      "step": 4330
    },
    {
      "epoch": 2.207527975584944,
      "grad_norm": 3.7429816722869873,
      "learning_rate": 4.779247202441506e-05,
      "loss": 3.874,
      "step": 4340
    },
    {
      "epoch": 2.212614445574771,
      "grad_norm": 3.0052616596221924,
      "learning_rate": 4.778738555442523e-05,
      "loss": 3.7768,
      "step": 4350
    },
    {
      "epoch": 2.217700915564598,
      "grad_norm": 3.3453288078308105,
      "learning_rate": 4.7782299084435404e-05,
      "loss": 3.8275,
      "step": 4360
    },
    {
      "epoch": 2.222787385554425,
      "grad_norm": 2.9370498657226562,
      "learning_rate": 4.7777212614445574e-05,
      "loss": 3.6486,
      "step": 4370
    },
    {
      "epoch": 2.227873855544252,
      "grad_norm": 3.7593250274658203,
      "learning_rate": 4.777212614445575e-05,
      "loss": 3.7846,
      "step": 4380
    },
    {
      "epoch": 2.2329603255340795,
      "grad_norm": 4.049661636352539,
      "learning_rate": 4.776703967446593e-05,
      "loss": 3.7725,
      "step": 4390
    },
    {
      "epoch": 2.2380467955239065,
      "grad_norm": 2.790114164352417,
      "learning_rate": 4.77619532044761e-05,
      "loss": 3.7094,
      "step": 4400
    },
    {
      "epoch": 2.2431332655137335,
      "grad_norm": 3.980050802230835,
      "learning_rate": 4.775686673448627e-05,
      "loss": 3.8624,
      "step": 4410
    },
    {
      "epoch": 2.2482197355035605,
      "grad_norm": 2.8510401248931885,
      "learning_rate": 4.7751780264496444e-05,
      "loss": 3.7021,
      "step": 4420
    },
    {
      "epoch": 2.2533062054933874,
      "grad_norm": 2.858358860015869,
      "learning_rate": 4.7746693794506614e-05,
      "loss": 3.7423,
      "step": 4430
    },
    {
      "epoch": 2.258392675483215,
      "grad_norm": 3.4576849937438965,
      "learning_rate": 4.7741607324516784e-05,
      "loss": 3.738,
      "step": 4440
    },
    {
      "epoch": 2.263479145473042,
      "grad_norm": 3.941326856613159,
      "learning_rate": 4.773652085452696e-05,
      "loss": 3.7473,
      "step": 4450
    },
    {
      "epoch": 2.268565615462869,
      "grad_norm": 3.9448628425598145,
      "learning_rate": 4.773143438453713e-05,
      "loss": 3.7758,
      "step": 4460
    },
    {
      "epoch": 2.273652085452696,
      "grad_norm": 3.5503225326538086,
      "learning_rate": 4.772634791454731e-05,
      "loss": 3.8198,
      "step": 4470
    },
    {
      "epoch": 2.278738555442523,
      "grad_norm": 3.3230912685394287,
      "learning_rate": 4.7721261444557483e-05,
      "loss": 3.7652,
      "step": 4480
    },
    {
      "epoch": 2.2838250254323498,
      "grad_norm": 3.039677381515503,
      "learning_rate": 4.7716174974567653e-05,
      "loss": 3.7527,
      "step": 4490
    },
    {
      "epoch": 2.288911495422177,
      "grad_norm": 4.640956878662109,
      "learning_rate": 4.771108850457783e-05,
      "loss": 3.7348,
      "step": 4500
    },
    {
      "epoch": 2.293997965412004,
      "grad_norm": 3.3063101768493652,
      "learning_rate": 4.7706002034588e-05,
      "loss": 3.7194,
      "step": 4510
    },
    {
      "epoch": 2.299084435401831,
      "grad_norm": 2.76145601272583,
      "learning_rate": 4.770091556459817e-05,
      "loss": 3.7902,
      "step": 4520
    },
    {
      "epoch": 2.304170905391658,
      "grad_norm": 3.799846649169922,
      "learning_rate": 4.7695829094608347e-05,
      "loss": 3.7634,
      "step": 4530
    },
    {
      "epoch": 2.309257375381485,
      "grad_norm": 3.5113580226898193,
      "learning_rate": 4.7690742624618516e-05,
      "loss": 3.717,
      "step": 4540
    },
    {
      "epoch": 2.3143438453713125,
      "grad_norm": 5.49561882019043,
      "learning_rate": 4.7685656154628686e-05,
      "loss": 3.6792,
      "step": 4550
    },
    {
      "epoch": 2.3194303153611395,
      "grad_norm": 4.949704170227051,
      "learning_rate": 4.768056968463886e-05,
      "loss": 3.7393,
      "step": 4560
    },
    {
      "epoch": 2.3245167853509665,
      "grad_norm": 2.8186323642730713,
      "learning_rate": 4.767548321464903e-05,
      "loss": 3.6468,
      "step": 4570
    },
    {
      "epoch": 2.3296032553407935,
      "grad_norm": 4.201939582824707,
      "learning_rate": 4.767039674465921e-05,
      "loss": 3.6618,
      "step": 4580
    },
    {
      "epoch": 2.3346897253306205,
      "grad_norm": 3.32413387298584,
      "learning_rate": 4.7665310274669386e-05,
      "loss": 3.8188,
      "step": 4590
    },
    {
      "epoch": 2.3397761953204474,
      "grad_norm": 2.8796443939208984,
      "learning_rate": 4.7660223804679556e-05,
      "loss": 3.7142,
      "step": 4600
    },
    {
      "epoch": 2.3448626653102744,
      "grad_norm": 3.977274179458618,
      "learning_rate": 4.7655137334689726e-05,
      "loss": 3.7936,
      "step": 4610
    },
    {
      "epoch": 2.349949135300102,
      "grad_norm": 3.2712597846984863,
      "learning_rate": 4.76500508646999e-05,
      "loss": 3.7186,
      "step": 4620
    },
    {
      "epoch": 2.355035605289929,
      "grad_norm": 4.308432579040527,
      "learning_rate": 4.764496439471007e-05,
      "loss": 3.7061,
      "step": 4630
    },
    {
      "epoch": 2.360122075279756,
      "grad_norm": 3.5347342491149902,
      "learning_rate": 4.763987792472024e-05,
      "loss": 3.769,
      "step": 4640
    },
    {
      "epoch": 2.365208545269583,
      "grad_norm": 4.926098823547363,
      "learning_rate": 4.763479145473042e-05,
      "loss": 3.6827,
      "step": 4650
    },
    {
      "epoch": 2.3702950152594098,
      "grad_norm": 3.7818284034729004,
      "learning_rate": 4.762970498474059e-05,
      "loss": 3.8166,
      "step": 4660
    },
    {
      "epoch": 2.375381485249237,
      "grad_norm": 4.909131050109863,
      "learning_rate": 4.7624618514750766e-05,
      "loss": 3.7323,
      "step": 4670
    },
    {
      "epoch": 2.380467955239064,
      "grad_norm": 3.9240024089813232,
      "learning_rate": 4.761953204476094e-05,
      "loss": 3.7847,
      "step": 4680
    },
    {
      "epoch": 2.385554425228891,
      "grad_norm": 3.7451188564300537,
      "learning_rate": 4.761444557477111e-05,
      "loss": 3.7233,
      "step": 4690
    },
    {
      "epoch": 2.390640895218718,
      "grad_norm": 4.937544345855713,
      "learning_rate": 4.760935910478128e-05,
      "loss": 3.7559,
      "step": 4700
    },
    {
      "epoch": 2.395727365208545,
      "grad_norm": 4.20212459564209,
      "learning_rate": 4.760427263479146e-05,
      "loss": 3.7641,
      "step": 4710
    },
    {
      "epoch": 2.400813835198372,
      "grad_norm": 4.265017509460449,
      "learning_rate": 4.759918616480163e-05,
      "loss": 3.7119,
      "step": 4720
    },
    {
      "epoch": 2.4059003051881995,
      "grad_norm": 3.5301215648651123,
      "learning_rate": 4.7594099694811805e-05,
      "loss": 3.7692,
      "step": 4730
    },
    {
      "epoch": 2.4109867751780265,
      "grad_norm": 3.5779762268066406,
      "learning_rate": 4.7589013224821975e-05,
      "loss": 3.7138,
      "step": 4740
    },
    {
      "epoch": 2.4160732451678535,
      "grad_norm": 4.699209213256836,
      "learning_rate": 4.7583926754832145e-05,
      "loss": 3.7531,
      "step": 4750
    },
    {
      "epoch": 2.4211597151576805,
      "grad_norm": 3.0192880630493164,
      "learning_rate": 4.757884028484232e-05,
      "loss": 3.7693,
      "step": 4760
    },
    {
      "epoch": 2.4262461851475075,
      "grad_norm": 4.206872940063477,
      "learning_rate": 4.75737538148525e-05,
      "loss": 3.7236,
      "step": 4770
    },
    {
      "epoch": 2.431332655137335,
      "grad_norm": 4.3301825523376465,
      "learning_rate": 4.756866734486267e-05,
      "loss": 3.7487,
      "step": 4780
    },
    {
      "epoch": 2.436419125127162,
      "grad_norm": 3.8758509159088135,
      "learning_rate": 4.7563580874872845e-05,
      "loss": 3.7628,
      "step": 4790
    },
    {
      "epoch": 2.441505595116989,
      "grad_norm": 2.3153724670410156,
      "learning_rate": 4.7558494404883015e-05,
      "loss": 3.8001,
      "step": 4800
    },
    {
      "epoch": 2.446592065106816,
      "grad_norm": 4.33902645111084,
      "learning_rate": 4.7553407934893185e-05,
      "loss": 3.8024,
      "step": 4810
    },
    {
      "epoch": 2.451678535096643,
      "grad_norm": 3.399277687072754,
      "learning_rate": 4.754832146490336e-05,
      "loss": 3.79,
      "step": 4820
    },
    {
      "epoch": 2.4567650050864698,
      "grad_norm": 3.587228775024414,
      "learning_rate": 4.754323499491353e-05,
      "loss": 3.6889,
      "step": 4830
    },
    {
      "epoch": 2.461851475076297,
      "grad_norm": 4.076161861419678,
      "learning_rate": 4.75381485249237e-05,
      "loss": 3.6825,
      "step": 4840
    },
    {
      "epoch": 2.466937945066124,
      "grad_norm": 3.5534796714782715,
      "learning_rate": 4.753306205493388e-05,
      "loss": 3.7125,
      "step": 4850
    },
    {
      "epoch": 2.472024415055951,
      "grad_norm": 5.541903972625732,
      "learning_rate": 4.752797558494405e-05,
      "loss": 3.7919,
      "step": 4860
    },
    {
      "epoch": 2.477110885045778,
      "grad_norm": 6.1370930671691895,
      "learning_rate": 4.7522889114954225e-05,
      "loss": 3.7082,
      "step": 4870
    },
    {
      "epoch": 2.482197355035605,
      "grad_norm": 3.4931039810180664,
      "learning_rate": 4.75178026449644e-05,
      "loss": 3.6556,
      "step": 4880
    },
    {
      "epoch": 2.4872838250254325,
      "grad_norm": 3.6662137508392334,
      "learning_rate": 4.751271617497457e-05,
      "loss": 3.6762,
      "step": 4890
    },
    {
      "epoch": 2.4923702950152595,
      "grad_norm": 3.529069662094116,
      "learning_rate": 4.750762970498474e-05,
      "loss": 3.7542,
      "step": 4900
    },
    {
      "epoch": 2.4974567650050865,
      "grad_norm": 3.9258742332458496,
      "learning_rate": 4.750254323499492e-05,
      "loss": 3.7284,
      "step": 4910
    },
    {
      "epoch": 2.5025432349949135,
      "grad_norm": 4.791749954223633,
      "learning_rate": 4.749745676500509e-05,
      "loss": 3.7941,
      "step": 4920
    },
    {
      "epoch": 2.5076297049847405,
      "grad_norm": 3.613046407699585,
      "learning_rate": 4.749237029501526e-05,
      "loss": 3.7212,
      "step": 4930
    },
    {
      "epoch": 2.5127161749745675,
      "grad_norm": 6.143880367279053,
      "learning_rate": 4.7487283825025434e-05,
      "loss": 3.7759,
      "step": 4940
    },
    {
      "epoch": 2.517802644964395,
      "grad_norm": 4.9501566886901855,
      "learning_rate": 4.7482197355035604e-05,
      "loss": 3.7085,
      "step": 4950
    },
    {
      "epoch": 2.522889114954222,
      "grad_norm": 3.5245087146759033,
      "learning_rate": 4.747711088504578e-05,
      "loss": 3.7845,
      "step": 4960
    },
    {
      "epoch": 2.527975584944049,
      "grad_norm": 3.879873275756836,
      "learning_rate": 4.747202441505596e-05,
      "loss": 3.7845,
      "step": 4970
    },
    {
      "epoch": 2.533062054933876,
      "grad_norm": 2.863471508026123,
      "learning_rate": 4.746693794506613e-05,
      "loss": 3.7087,
      "step": 4980
    },
    {
      "epoch": 2.538148524923703,
      "grad_norm": 4.742125511169434,
      "learning_rate": 4.74618514750763e-05,
      "loss": 3.6959,
      "step": 4990
    },
    {
      "epoch": 2.5432349949135302,
      "grad_norm": 4.197792053222656,
      "learning_rate": 4.7456765005086474e-05,
      "loss": 3.6799,
      "step": 5000
    },
    {
      "epoch": 2.548321464903357,
      "grad_norm": 4.584874153137207,
      "learning_rate": 4.7451678535096644e-05,
      "loss": 3.6834,
      "step": 5010
    },
    {
      "epoch": 2.553407934893184,
      "grad_norm": 3.9623186588287354,
      "learning_rate": 4.744659206510682e-05,
      "loss": 3.6645,
      "step": 5020
    },
    {
      "epoch": 2.558494404883011,
      "grad_norm": 4.095483779907227,
      "learning_rate": 4.744150559511699e-05,
      "loss": 3.729,
      "step": 5030
    },
    {
      "epoch": 2.563580874872838,
      "grad_norm": 3.7427215576171875,
      "learning_rate": 4.743641912512716e-05,
      "loss": 3.6687,
      "step": 5040
    },
    {
      "epoch": 2.568667344862665,
      "grad_norm": 3.2752528190612793,
      "learning_rate": 4.743133265513734e-05,
      "loss": 3.6605,
      "step": 5050
    },
    {
      "epoch": 2.573753814852492,
      "grad_norm": 5.092625141143799,
      "learning_rate": 4.7426246185147514e-05,
      "loss": 3.7804,
      "step": 5060
    },
    {
      "epoch": 2.5788402848423195,
      "grad_norm": 5.100876808166504,
      "learning_rate": 4.7421159715157683e-05,
      "loss": 3.7609,
      "step": 5070
    },
    {
      "epoch": 2.5839267548321465,
      "grad_norm": 5.389939308166504,
      "learning_rate": 4.741607324516786e-05,
      "loss": 3.6929,
      "step": 5080
    },
    {
      "epoch": 2.5890132248219735,
      "grad_norm": 3.626095771789551,
      "learning_rate": 4.741098677517803e-05,
      "loss": 3.6352,
      "step": 5090
    },
    {
      "epoch": 2.5940996948118005,
      "grad_norm": 4.17099142074585,
      "learning_rate": 4.74059003051882e-05,
      "loss": 3.7135,
      "step": 5100
    },
    {
      "epoch": 2.599186164801628,
      "grad_norm": 3.0728600025177,
      "learning_rate": 4.7400813835198377e-05,
      "loss": 3.6777,
      "step": 5110
    },
    {
      "epoch": 2.604272634791455,
      "grad_norm": 5.30855655670166,
      "learning_rate": 4.7395727365208546e-05,
      "loss": 3.683,
      "step": 5120
    },
    {
      "epoch": 2.609359104781282,
      "grad_norm": 3.810598611831665,
      "learning_rate": 4.7390640895218716e-05,
      "loss": 3.707,
      "step": 5130
    },
    {
      "epoch": 2.614445574771109,
      "grad_norm": 5.050060272216797,
      "learning_rate": 4.738555442522889e-05,
      "loss": 3.7768,
      "step": 5140
    },
    {
      "epoch": 2.619532044760936,
      "grad_norm": 5.928835868835449,
      "learning_rate": 4.738046795523906e-05,
      "loss": 3.7884,
      "step": 5150
    },
    {
      "epoch": 2.624618514750763,
      "grad_norm": 3.9667060375213623,
      "learning_rate": 4.737538148524924e-05,
      "loss": 3.7595,
      "step": 5160
    },
    {
      "epoch": 2.62970498474059,
      "grad_norm": 4.3744587898254395,
      "learning_rate": 4.7370295015259416e-05,
      "loss": 3.678,
      "step": 5170
    },
    {
      "epoch": 2.634791454730417,
      "grad_norm": 4.936386585235596,
      "learning_rate": 4.7365208545269586e-05,
      "loss": 3.7201,
      "step": 5180
    },
    {
      "epoch": 2.639877924720244,
      "grad_norm": 4.088891983032227,
      "learning_rate": 4.7360122075279756e-05,
      "loss": 3.7093,
      "step": 5190
    },
    {
      "epoch": 2.644964394710071,
      "grad_norm": 4.086247444152832,
      "learning_rate": 4.735503560528993e-05,
      "loss": 3.6612,
      "step": 5200
    },
    {
      "epoch": 2.650050864699898,
      "grad_norm": 6.153862476348877,
      "learning_rate": 4.73499491353001e-05,
      "loss": 3.7143,
      "step": 5210
    },
    {
      "epoch": 2.6551373346897256,
      "grad_norm": 5.14927339553833,
      "learning_rate": 4.734486266531027e-05,
      "loss": 3.6996,
      "step": 5220
    },
    {
      "epoch": 2.6602238046795526,
      "grad_norm": 5.2147088050842285,
      "learning_rate": 4.733977619532045e-05,
      "loss": 3.6849,
      "step": 5230
    },
    {
      "epoch": 2.6653102746693795,
      "grad_norm": 3.6904585361480713,
      "learning_rate": 4.733468972533062e-05,
      "loss": 3.6646,
      "step": 5240
    },
    {
      "epoch": 2.6703967446592065,
      "grad_norm": 4.170338153839111,
      "learning_rate": 4.7329603255340796e-05,
      "loss": 3.6966,
      "step": 5250
    },
    {
      "epoch": 2.6754832146490335,
      "grad_norm": 4.927977085113525,
      "learning_rate": 4.732451678535097e-05,
      "loss": 3.7143,
      "step": 5260
    },
    {
      "epoch": 2.6805696846388605,
      "grad_norm": 4.0680832862854,
      "learning_rate": 4.731943031536114e-05,
      "loss": 3.6841,
      "step": 5270
    },
    {
      "epoch": 2.6856561546286875,
      "grad_norm": 6.402088642120361,
      "learning_rate": 4.731434384537132e-05,
      "loss": 3.6357,
      "step": 5280
    },
    {
      "epoch": 2.690742624618515,
      "grad_norm": 3.656703472137451,
      "learning_rate": 4.730925737538149e-05,
      "loss": 3.6916,
      "step": 5290
    },
    {
      "epoch": 2.695829094608342,
      "grad_norm": 3.723432779312134,
      "learning_rate": 4.730417090539166e-05,
      "loss": 3.7321,
      "step": 5300
    },
    {
      "epoch": 2.700915564598169,
      "grad_norm": 4.878834247589111,
      "learning_rate": 4.7299084435401835e-05,
      "loss": 3.6424,
      "step": 5310
    },
    {
      "epoch": 2.706002034587996,
      "grad_norm": 3.647526741027832,
      "learning_rate": 4.7293997965412005e-05,
      "loss": 3.804,
      "step": 5320
    },
    {
      "epoch": 2.7110885045778232,
      "grad_norm": 4.02158260345459,
      "learning_rate": 4.7288911495422175e-05,
      "loss": 3.6634,
      "step": 5330
    },
    {
      "epoch": 2.7161749745676502,
      "grad_norm": 3.5641396045684814,
      "learning_rate": 4.728382502543235e-05,
      "loss": 3.7005,
      "step": 5340
    },
    {
      "epoch": 2.721261444557477,
      "grad_norm": 3.2118380069732666,
      "learning_rate": 4.727873855544253e-05,
      "loss": 3.7093,
      "step": 5350
    },
    {
      "epoch": 2.726347914547304,
      "grad_norm": 3.5591046810150146,
      "learning_rate": 4.72736520854527e-05,
      "loss": 3.6659,
      "step": 5360
    },
    {
      "epoch": 2.731434384537131,
      "grad_norm": 5.539649486541748,
      "learning_rate": 4.7268565615462875e-05,
      "loss": 3.6466,
      "step": 5370
    },
    {
      "epoch": 2.736520854526958,
      "grad_norm": 4.554442882537842,
      "learning_rate": 4.7263479145473045e-05,
      "loss": 3.6131,
      "step": 5380
    },
    {
      "epoch": 2.741607324516785,
      "grad_norm": 3.602660655975342,
      "learning_rate": 4.7258392675483215e-05,
      "loss": 3.6875,
      "step": 5390
    },
    {
      "epoch": 2.7466937945066126,
      "grad_norm": 5.642316818237305,
      "learning_rate": 4.725330620549339e-05,
      "loss": 3.6964,
      "step": 5400
    },
    {
      "epoch": 2.7517802644964395,
      "grad_norm": 6.344845771789551,
      "learning_rate": 4.724821973550356e-05,
      "loss": 3.6677,
      "step": 5410
    },
    {
      "epoch": 2.7568667344862665,
      "grad_norm": 4.779513359069824,
      "learning_rate": 4.724313326551373e-05,
      "loss": 3.7205,
      "step": 5420
    },
    {
      "epoch": 2.7619532044760935,
      "grad_norm": 4.098911762237549,
      "learning_rate": 4.723804679552391e-05,
      "loss": 3.8005,
      "step": 5430
    },
    {
      "epoch": 2.767039674465921,
      "grad_norm": 4.061922550201416,
      "learning_rate": 4.7232960325534085e-05,
      "loss": 3.7158,
      "step": 5440
    },
    {
      "epoch": 2.772126144455748,
      "grad_norm": 3.6565074920654297,
      "learning_rate": 4.7227873855544255e-05,
      "loss": 3.7754,
      "step": 5450
    },
    {
      "epoch": 2.777212614445575,
      "grad_norm": 3.252622127532959,
      "learning_rate": 4.722278738555443e-05,
      "loss": 3.7673,
      "step": 5460
    },
    {
      "epoch": 2.782299084435402,
      "grad_norm": 3.1711196899414062,
      "learning_rate": 4.72177009155646e-05,
      "loss": 3.7207,
      "step": 5470
    },
    {
      "epoch": 2.787385554425229,
      "grad_norm": 4.77832555770874,
      "learning_rate": 4.721261444557477e-05,
      "loss": 3.6313,
      "step": 5480
    },
    {
      "epoch": 2.792472024415056,
      "grad_norm": 5.02147912979126,
      "learning_rate": 4.720752797558495e-05,
      "loss": 3.6641,
      "step": 5490
    },
    {
      "epoch": 2.797558494404883,
      "grad_norm": 4.071934223175049,
      "learning_rate": 4.720244150559512e-05,
      "loss": 3.6561,
      "step": 5500
    },
    {
      "epoch": 2.8026449643947102,
      "grad_norm": 5.00059175491333,
      "learning_rate": 4.719735503560529e-05,
      "loss": 3.6793,
      "step": 5510
    },
    {
      "epoch": 2.807731434384537,
      "grad_norm": 4.873887538909912,
      "learning_rate": 4.7192268565615464e-05,
      "loss": 3.6595,
      "step": 5520
    },
    {
      "epoch": 2.812817904374364,
      "grad_norm": 5.991937637329102,
      "learning_rate": 4.7187182095625634e-05,
      "loss": 3.6569,
      "step": 5530
    },
    {
      "epoch": 2.817904374364191,
      "grad_norm": 3.432481288909912,
      "learning_rate": 4.718209562563581e-05,
      "loss": 3.6634,
      "step": 5540
    },
    {
      "epoch": 2.822990844354018,
      "grad_norm": 5.015426158905029,
      "learning_rate": 4.717700915564599e-05,
      "loss": 3.6655,
      "step": 5550
    },
    {
      "epoch": 2.8280773143438456,
      "grad_norm": 3.983628988265991,
      "learning_rate": 4.717192268565616e-05,
      "loss": 3.6695,
      "step": 5560
    },
    {
      "epoch": 2.8331637843336726,
      "grad_norm": 4.249433517456055,
      "learning_rate": 4.7166836215666334e-05,
      "loss": 3.6598,
      "step": 5570
    },
    {
      "epoch": 2.8382502543234995,
      "grad_norm": 4.466683387756348,
      "learning_rate": 4.7161749745676504e-05,
      "loss": 3.6617,
      "step": 5580
    },
    {
      "epoch": 2.8433367243133265,
      "grad_norm": 4.560993194580078,
      "learning_rate": 4.7156663275686674e-05,
      "loss": 3.6239,
      "step": 5590
    },
    {
      "epoch": 2.8484231943031535,
      "grad_norm": 4.590268611907959,
      "learning_rate": 4.715157680569685e-05,
      "loss": 3.6689,
      "step": 5600
    },
    {
      "epoch": 2.8535096642929805,
      "grad_norm": 4.271324634552002,
      "learning_rate": 4.714649033570702e-05,
      "loss": 3.6668,
      "step": 5610
    },
    {
      "epoch": 2.8585961342828075,
      "grad_norm": 3.659992218017578,
      "learning_rate": 4.714140386571719e-05,
      "loss": 3.6803,
      "step": 5620
    },
    {
      "epoch": 2.863682604272635,
      "grad_norm": 6.149805545806885,
      "learning_rate": 4.713631739572737e-05,
      "loss": 3.6442,
      "step": 5630
    },
    {
      "epoch": 2.868769074262462,
      "grad_norm": 5.277060031890869,
      "learning_rate": 4.7131230925737544e-05,
      "loss": 3.6845,
      "step": 5640
    },
    {
      "epoch": 2.873855544252289,
      "grad_norm": 5.318338394165039,
      "learning_rate": 4.7126144455747713e-05,
      "loss": 3.6114,
      "step": 5650
    },
    {
      "epoch": 2.878942014242116,
      "grad_norm": 4.43471622467041,
      "learning_rate": 4.712105798575789e-05,
      "loss": 3.6988,
      "step": 5660
    },
    {
      "epoch": 2.8840284842319432,
      "grad_norm": 3.6855530738830566,
      "learning_rate": 4.711597151576806e-05,
      "loss": 3.6825,
      "step": 5670
    },
    {
      "epoch": 2.8891149542217702,
      "grad_norm": 3.5933384895324707,
      "learning_rate": 4.711088504577823e-05,
      "loss": 3.6766,
      "step": 5680
    },
    {
      "epoch": 2.894201424211597,
      "grad_norm": 4.526597499847412,
      "learning_rate": 4.7105798575788407e-05,
      "loss": 3.6725,
      "step": 5690
    },
    {
      "epoch": 2.899287894201424,
      "grad_norm": 5.999857425689697,
      "learning_rate": 4.7100712105798576e-05,
      "loss": 3.6934,
      "step": 5700
    },
    {
      "epoch": 2.904374364191251,
      "grad_norm": 4.9649553298950195,
      "learning_rate": 4.7095625635808746e-05,
      "loss": 3.677,
      "step": 5710
    },
    {
      "epoch": 2.909460834181078,
      "grad_norm": 4.035357475280762,
      "learning_rate": 4.709053916581892e-05,
      "loss": 3.6663,
      "step": 5720
    },
    {
      "epoch": 2.914547304170905,
      "grad_norm": 5.042098522186279,
      "learning_rate": 4.70854526958291e-05,
      "loss": 3.7438,
      "step": 5730
    },
    {
      "epoch": 2.9196337741607326,
      "grad_norm": 3.8188817501068115,
      "learning_rate": 4.708036622583927e-05,
      "loss": 3.6689,
      "step": 5740
    },
    {
      "epoch": 2.9247202441505595,
      "grad_norm": 5.365772247314453,
      "learning_rate": 4.7075279755849446e-05,
      "loss": 3.7026,
      "step": 5750
    },
    {
      "epoch": 2.9298067141403865,
      "grad_norm": 3.420614719390869,
      "learning_rate": 4.7070193285859616e-05,
      "loss": 3.6605,
      "step": 5760
    },
    {
      "epoch": 2.9348931841302135,
      "grad_norm": 5.011106967926025,
      "learning_rate": 4.7065106815869786e-05,
      "loss": 3.636,
      "step": 5770
    },
    {
      "epoch": 2.939979654120041,
      "grad_norm": 3.5583465099334717,
      "learning_rate": 4.706002034587996e-05,
      "loss": 3.7717,
      "step": 5780
    },
    {
      "epoch": 2.945066124109868,
      "grad_norm": 6.052684783935547,
      "learning_rate": 4.705493387589013e-05,
      "loss": 3.6722,
      "step": 5790
    },
    {
      "epoch": 2.950152594099695,
      "grad_norm": 3.774580955505371,
      "learning_rate": 4.70498474059003e-05,
      "loss": 3.6608,
      "step": 5800
    },
    {
      "epoch": 2.955239064089522,
      "grad_norm": 3.4495961666107178,
      "learning_rate": 4.704476093591048e-05,
      "loss": 3.6252,
      "step": 5810
    },
    {
      "epoch": 2.960325534079349,
      "grad_norm": 5.737781047821045,
      "learning_rate": 4.703967446592065e-05,
      "loss": 3.6989,
      "step": 5820
    },
    {
      "epoch": 2.965412004069176,
      "grad_norm": 7.032559394836426,
      "learning_rate": 4.7034587995930826e-05,
      "loss": 3.6611,
      "step": 5830
    },
    {
      "epoch": 2.970498474059003,
      "grad_norm": 3.6125881671905518,
      "learning_rate": 4.7029501525941e-05,
      "loss": 3.67,
      "step": 5840
    },
    {
      "epoch": 2.9755849440488302,
      "grad_norm": 4.724600315093994,
      "learning_rate": 4.702441505595117e-05,
      "loss": 3.5268,
      "step": 5850
    },
    {
      "epoch": 2.980671414038657,
      "grad_norm": 5.115473747253418,
      "learning_rate": 4.701932858596135e-05,
      "loss": 3.5838,
      "step": 5860
    },
    {
      "epoch": 2.985757884028484,
      "grad_norm": 4.640131950378418,
      "learning_rate": 4.701424211597152e-05,
      "loss": 3.5698,
      "step": 5870
    },
    {
      "epoch": 2.990844354018311,
      "grad_norm": 4.753575801849365,
      "learning_rate": 4.700915564598169e-05,
      "loss": 3.7128,
      "step": 5880
    },
    {
      "epoch": 2.9959308240081386,
      "grad_norm": 4.5460591316223145,
      "learning_rate": 4.7004069175991865e-05,
      "loss": 3.6432,
      "step": 5890
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.730624198913574,
      "eval_runtime": 2.6319,
      "eval_samples_per_second": 1054.387,
      "eval_steps_per_second": 131.846,
      "step": 5898
    },
    {
      "epoch": 3.0010172939979656,
      "grad_norm": 7.507416248321533,
      "learning_rate": 4.6998982706002035e-05,
      "loss": 3.6238,
      "step": 5900
    },
    {
      "epoch": 3.0061037639877926,
      "grad_norm": 4.895421981811523,
      "learning_rate": 4.6993896236012205e-05,
      "loss": 3.609,
      "step": 5910
    },
    {
      "epoch": 3.0111902339776195,
      "grad_norm": 4.378381252288818,
      "learning_rate": 4.698880976602238e-05,
      "loss": 3.6303,
      "step": 5920
    },
    {
      "epoch": 3.0162767039674465,
      "grad_norm": 4.391936779022217,
      "learning_rate": 4.698372329603256e-05,
      "loss": 3.6101,
      "step": 5930
    },
    {
      "epoch": 3.0213631739572735,
      "grad_norm": 3.6512954235076904,
      "learning_rate": 4.697863682604273e-05,
      "loss": 3.6587,
      "step": 5940
    },
    {
      "epoch": 3.026449643947101,
      "grad_norm": 3.6170618534088135,
      "learning_rate": 4.6973550356052905e-05,
      "loss": 3.663,
      "step": 5950
    },
    {
      "epoch": 3.031536113936928,
      "grad_norm": 4.112524032592773,
      "learning_rate": 4.6968463886063075e-05,
      "loss": 3.6767,
      "step": 5960
    },
    {
      "epoch": 3.036622583926755,
      "grad_norm": 4.816345691680908,
      "learning_rate": 4.6963377416073245e-05,
      "loss": 3.6566,
      "step": 5970
    },
    {
      "epoch": 3.041709053916582,
      "grad_norm": 5.210585594177246,
      "learning_rate": 4.695829094608342e-05,
      "loss": 3.5746,
      "step": 5980
    },
    {
      "epoch": 3.046795523906409,
      "grad_norm": 5.842030048370361,
      "learning_rate": 4.695320447609359e-05,
      "loss": 3.6868,
      "step": 5990
    },
    {
      "epoch": 3.051881993896236,
      "grad_norm": 5.47335147857666,
      "learning_rate": 4.694811800610376e-05,
      "loss": 3.5736,
      "step": 6000
    },
    {
      "epoch": 3.0569684638860632,
      "grad_norm": 5.056438446044922,
      "learning_rate": 4.694303153611394e-05,
      "loss": 3.6288,
      "step": 6010
    },
    {
      "epoch": 3.0620549338758902,
      "grad_norm": 5.013584613800049,
      "learning_rate": 4.6937945066124115e-05,
      "loss": 3.5711,
      "step": 6020
    },
    {
      "epoch": 3.067141403865717,
      "grad_norm": 5.635212421417236,
      "learning_rate": 4.6932858596134285e-05,
      "loss": 3.648,
      "step": 6030
    },
    {
      "epoch": 3.072227873855544,
      "grad_norm": 4.481797218322754,
      "learning_rate": 4.692777212614446e-05,
      "loss": 3.6144,
      "step": 6040
    },
    {
      "epoch": 3.077314343845371,
      "grad_norm": 5.8804192543029785,
      "learning_rate": 4.692268565615463e-05,
      "loss": 3.6266,
      "step": 6050
    },
    {
      "epoch": 3.082400813835198,
      "grad_norm": 7.3049750328063965,
      "learning_rate": 4.69175991861648e-05,
      "loss": 3.5545,
      "step": 6060
    },
    {
      "epoch": 3.0874872838250256,
      "grad_norm": 7.520923137664795,
      "learning_rate": 4.691251271617498e-05,
      "loss": 3.6626,
      "step": 6070
    },
    {
      "epoch": 3.0925737538148526,
      "grad_norm": 4.732873439788818,
      "learning_rate": 4.690742624618515e-05,
      "loss": 3.6745,
      "step": 6080
    },
    {
      "epoch": 3.0976602238046795,
      "grad_norm": 5.350921630859375,
      "learning_rate": 4.6902339776195324e-05,
      "loss": 3.657,
      "step": 6090
    },
    {
      "epoch": 3.1027466937945065,
      "grad_norm": 4.853120803833008,
      "learning_rate": 4.6897253306205494e-05,
      "loss": 3.6241,
      "step": 6100
    },
    {
      "epoch": 3.1078331637843335,
      "grad_norm": 5.193929672241211,
      "learning_rate": 4.6892166836215664e-05,
      "loss": 3.6805,
      "step": 6110
    },
    {
      "epoch": 3.112919633774161,
      "grad_norm": 5.137939453125,
      "learning_rate": 4.688708036622584e-05,
      "loss": 3.6463,
      "step": 6120
    },
    {
      "epoch": 3.118006103763988,
      "grad_norm": 5.308433532714844,
      "learning_rate": 4.688199389623602e-05,
      "loss": 3.5716,
      "step": 6130
    },
    {
      "epoch": 3.123092573753815,
      "grad_norm": 5.362980842590332,
      "learning_rate": 4.687690742624619e-05,
      "loss": 3.5878,
      "step": 6140
    },
    {
      "epoch": 3.128179043743642,
      "grad_norm": 6.189334392547607,
      "learning_rate": 4.6871820956256364e-05,
      "loss": 3.6959,
      "step": 6150
    },
    {
      "epoch": 3.133265513733469,
      "grad_norm": 5.566260814666748,
      "learning_rate": 4.6866734486266534e-05,
      "loss": 3.6474,
      "step": 6160
    },
    {
      "epoch": 3.138351983723296,
      "grad_norm": 3.9197475910186768,
      "learning_rate": 4.6861648016276704e-05,
      "loss": 3.5929,
      "step": 6170
    },
    {
      "epoch": 3.1434384537131232,
      "grad_norm": 5.2231855392456055,
      "learning_rate": 4.685656154628688e-05,
      "loss": 3.6423,
      "step": 6180
    },
    {
      "epoch": 3.1485249237029502,
      "grad_norm": 4.13824462890625,
      "learning_rate": 4.685147507629705e-05,
      "loss": 3.6855,
      "step": 6190
    },
    {
      "epoch": 3.153611393692777,
      "grad_norm": 6.090406894683838,
      "learning_rate": 4.684638860630722e-05,
      "loss": 3.6057,
      "step": 6200
    },
    {
      "epoch": 3.158697863682604,
      "grad_norm": 4.644675254821777,
      "learning_rate": 4.68413021363174e-05,
      "loss": 3.6638,
      "step": 6210
    },
    {
      "epoch": 3.163784333672431,
      "grad_norm": 6.911463737487793,
      "learning_rate": 4.6836215666327574e-05,
      "loss": 3.6513,
      "step": 6220
    },
    {
      "epoch": 3.1688708036622586,
      "grad_norm": 5.354273796081543,
      "learning_rate": 4.6831129196337743e-05,
      "loss": 3.6928,
      "step": 6230
    },
    {
      "epoch": 3.1739572736520856,
      "grad_norm": 5.094912528991699,
      "learning_rate": 4.682604272634792e-05,
      "loss": 3.6879,
      "step": 6240
    },
    {
      "epoch": 3.1790437436419126,
      "grad_norm": 5.294932842254639,
      "learning_rate": 4.682095625635809e-05,
      "loss": 3.623,
      "step": 6250
    },
    {
      "epoch": 3.1841302136317395,
      "grad_norm": 4.585472106933594,
      "learning_rate": 4.681586978636826e-05,
      "loss": 3.6347,
      "step": 6260
    },
    {
      "epoch": 3.1892166836215665,
      "grad_norm": 3.752584457397461,
      "learning_rate": 4.6810783316378437e-05,
      "loss": 3.645,
      "step": 6270
    },
    {
      "epoch": 3.1943031536113935,
      "grad_norm": 6.681651592254639,
      "learning_rate": 4.6805696846388606e-05,
      "loss": 3.6406,
      "step": 6280
    },
    {
      "epoch": 3.199389623601221,
      "grad_norm": 3.9756886959075928,
      "learning_rate": 4.6800610376398776e-05,
      "loss": 3.5795,
      "step": 6290
    },
    {
      "epoch": 3.204476093591048,
      "grad_norm": 7.231886863708496,
      "learning_rate": 4.679552390640895e-05,
      "loss": 3.6665,
      "step": 6300
    },
    {
      "epoch": 3.209562563580875,
      "grad_norm": 5.2829413414001465,
      "learning_rate": 4.679043743641913e-05,
      "loss": 3.6035,
      "step": 6310
    },
    {
      "epoch": 3.214649033570702,
      "grad_norm": 3.9279534816741943,
      "learning_rate": 4.67853509664293e-05,
      "loss": 3.6748,
      "step": 6320
    },
    {
      "epoch": 3.219735503560529,
      "grad_norm": 4.4516072273254395,
      "learning_rate": 4.6780264496439476e-05,
      "loss": 3.5935,
      "step": 6330
    },
    {
      "epoch": 3.2248219735503563,
      "grad_norm": 5.494381427764893,
      "learning_rate": 4.6775178026449646e-05,
      "loss": 3.613,
      "step": 6340
    },
    {
      "epoch": 3.2299084435401832,
      "grad_norm": 6.77787446975708,
      "learning_rate": 4.677009155645982e-05,
      "loss": 3.5552,
      "step": 6350
    },
    {
      "epoch": 3.2349949135300102,
      "grad_norm": 4.624700546264648,
      "learning_rate": 4.676500508646999e-05,
      "loss": 3.6108,
      "step": 6360
    },
    {
      "epoch": 3.240081383519837,
      "grad_norm": 5.207804203033447,
      "learning_rate": 4.675991861648016e-05,
      "loss": 3.6278,
      "step": 6370
    },
    {
      "epoch": 3.245167853509664,
      "grad_norm": 4.456101417541504,
      "learning_rate": 4.675483214649034e-05,
      "loss": 3.5611,
      "step": 6380
    },
    {
      "epoch": 3.250254323499491,
      "grad_norm": 5.631757736206055,
      "learning_rate": 4.674974567650051e-05,
      "loss": 3.641,
      "step": 6390
    },
    {
      "epoch": 3.2553407934893186,
      "grad_norm": 5.219798564910889,
      "learning_rate": 4.6744659206510686e-05,
      "loss": 3.6026,
      "step": 6400
    },
    {
      "epoch": 3.2604272634791456,
      "grad_norm": 4.668636322021484,
      "learning_rate": 4.673957273652086e-05,
      "loss": 3.6479,
      "step": 6410
    },
    {
      "epoch": 3.2655137334689726,
      "grad_norm": 5.387055397033691,
      "learning_rate": 4.673448626653103e-05,
      "loss": 3.5609,
      "step": 6420
    },
    {
      "epoch": 3.2706002034587995,
      "grad_norm": 4.472424507141113,
      "learning_rate": 4.67293997965412e-05,
      "loss": 3.5468,
      "step": 6430
    },
    {
      "epoch": 3.2756866734486265,
      "grad_norm": 5.355788230895996,
      "learning_rate": 4.672431332655138e-05,
      "loss": 3.6089,
      "step": 6440
    },
    {
      "epoch": 3.280773143438454,
      "grad_norm": 4.767132759094238,
      "learning_rate": 4.671922685656155e-05,
      "loss": 3.6801,
      "step": 6450
    },
    {
      "epoch": 3.285859613428281,
      "grad_norm": 6.642704486846924,
      "learning_rate": 4.671414038657172e-05,
      "loss": 3.6679,
      "step": 6460
    },
    {
      "epoch": 3.290946083418108,
      "grad_norm": 5.752387523651123,
      "learning_rate": 4.6709053916581895e-05,
      "loss": 3.5664,
      "step": 6470
    },
    {
      "epoch": 3.296032553407935,
      "grad_norm": 6.090226650238037,
      "learning_rate": 4.6703967446592065e-05,
      "loss": 3.6715,
      "step": 6480
    },
    {
      "epoch": 3.301119023397762,
      "grad_norm": 5.08964204788208,
      "learning_rate": 4.6698880976602235e-05,
      "loss": 3.5957,
      "step": 6490
    },
    {
      "epoch": 3.306205493387589,
      "grad_norm": 5.29610013961792,
      "learning_rate": 4.669379450661241e-05,
      "loss": 3.6722,
      "step": 6500
    },
    {
      "epoch": 3.311291963377416,
      "grad_norm": 5.590677738189697,
      "learning_rate": 4.668870803662259e-05,
      "loss": 3.6655,
      "step": 6510
    },
    {
      "epoch": 3.3163784333672433,
      "grad_norm": 4.272968292236328,
      "learning_rate": 4.668362156663276e-05,
      "loss": 3.6091,
      "step": 6520
    },
    {
      "epoch": 3.3214649033570702,
      "grad_norm": 6.740396976470947,
      "learning_rate": 4.6678535096642935e-05,
      "loss": 3.5491,
      "step": 6530
    },
    {
      "epoch": 3.326551373346897,
      "grad_norm": 7.082432746887207,
      "learning_rate": 4.6673448626653105e-05,
      "loss": 3.5512,
      "step": 6540
    },
    {
      "epoch": 3.331637843336724,
      "grad_norm": 4.9167022705078125,
      "learning_rate": 4.6668362156663275e-05,
      "loss": 3.5549,
      "step": 6550
    },
    {
      "epoch": 3.3367243133265516,
      "grad_norm": 5.1121063232421875,
      "learning_rate": 4.666327568667345e-05,
      "loss": 3.6098,
      "step": 6560
    },
    {
      "epoch": 3.3418107833163786,
      "grad_norm": 4.417415618896484,
      "learning_rate": 4.665818921668362e-05,
      "loss": 3.6504,
      "step": 6570
    },
    {
      "epoch": 3.3468972533062056,
      "grad_norm": 5.0213541984558105,
      "learning_rate": 4.665310274669379e-05,
      "loss": 3.5584,
      "step": 6580
    },
    {
      "epoch": 3.3519837232960326,
      "grad_norm": 4.2303032875061035,
      "learning_rate": 4.664801627670397e-05,
      "loss": 3.6434,
      "step": 6590
    },
    {
      "epoch": 3.3570701932858595,
      "grad_norm": 7.07000207901001,
      "learning_rate": 4.6642929806714145e-05,
      "loss": 3.6364,
      "step": 6600
    },
    {
      "epoch": 3.3621566632756865,
      "grad_norm": 5.448245525360107,
      "learning_rate": 4.6637843336724315e-05,
      "loss": 3.5681,
      "step": 6610
    },
    {
      "epoch": 3.3672431332655135,
      "grad_norm": 5.562067031860352,
      "learning_rate": 4.663275686673449e-05,
      "loss": 3.6292,
      "step": 6620
    },
    {
      "epoch": 3.372329603255341,
      "grad_norm": 4.17584753036499,
      "learning_rate": 4.662767039674466e-05,
      "loss": 3.6419,
      "step": 6630
    },
    {
      "epoch": 3.377416073245168,
      "grad_norm": 5.500147819519043,
      "learning_rate": 4.662258392675484e-05,
      "loss": 3.6058,
      "step": 6640
    },
    {
      "epoch": 3.382502543234995,
      "grad_norm": 4.1581807136535645,
      "learning_rate": 4.661749745676501e-05,
      "loss": 3.5954,
      "step": 6650
    },
    {
      "epoch": 3.387589013224822,
      "grad_norm": 4.956427097320557,
      "learning_rate": 4.661241098677518e-05,
      "loss": 3.5831,
      "step": 6660
    },
    {
      "epoch": 3.392675483214649,
      "grad_norm": 5.724091053009033,
      "learning_rate": 4.6607324516785354e-05,
      "loss": 3.6244,
      "step": 6670
    },
    {
      "epoch": 3.3977619532044763,
      "grad_norm": 4.715976715087891,
      "learning_rate": 4.6602238046795524e-05,
      "loss": 3.5964,
      "step": 6680
    },
    {
      "epoch": 3.4028484231943033,
      "grad_norm": 5.500229835510254,
      "learning_rate": 4.65971515768057e-05,
      "loss": 3.6604,
      "step": 6690
    },
    {
      "epoch": 3.4079348931841302,
      "grad_norm": 4.800648212432861,
      "learning_rate": 4.659206510681588e-05,
      "loss": 3.6522,
      "step": 6700
    },
    {
      "epoch": 3.413021363173957,
      "grad_norm": 6.145252704620361,
      "learning_rate": 4.658697863682605e-05,
      "loss": 3.5853,
      "step": 6710
    },
    {
      "epoch": 3.418107833163784,
      "grad_norm": 5.1746110916137695,
      "learning_rate": 4.658189216683622e-05,
      "loss": 3.5512,
      "step": 6720
    },
    {
      "epoch": 3.423194303153611,
      "grad_norm": 5.844448089599609,
      "learning_rate": 4.6576805696846394e-05,
      "loss": 3.6089,
      "step": 6730
    },
    {
      "epoch": 3.4282807731434386,
      "grad_norm": 4.023530960083008,
      "learning_rate": 4.6571719226856564e-05,
      "loss": 3.6283,
      "step": 6740
    },
    {
      "epoch": 3.4333672431332656,
      "grad_norm": 6.939926624298096,
      "learning_rate": 4.6566632756866734e-05,
      "loss": 3.6125,
      "step": 6750
    },
    {
      "epoch": 3.4384537131230926,
      "grad_norm": 5.898573398590088,
      "learning_rate": 4.656154628687691e-05,
      "loss": 3.5964,
      "step": 6760
    },
    {
      "epoch": 3.4435401831129195,
      "grad_norm": 4.974893093109131,
      "learning_rate": 4.655645981688708e-05,
      "loss": 3.5804,
      "step": 6770
    },
    {
      "epoch": 3.4486266531027465,
      "grad_norm": 4.803226470947266,
      "learning_rate": 4.655137334689725e-05,
      "loss": 3.628,
      "step": 6780
    },
    {
      "epoch": 3.453713123092574,
      "grad_norm": 4.777872562408447,
      "learning_rate": 4.654628687690743e-05,
      "loss": 3.6318,
      "step": 6790
    },
    {
      "epoch": 3.458799593082401,
      "grad_norm": 5.87904691696167,
      "learning_rate": 4.6541200406917604e-05,
      "loss": 3.5689,
      "step": 6800
    },
    {
      "epoch": 3.463886063072228,
      "grad_norm": 4.298388481140137,
      "learning_rate": 4.6536113936927773e-05,
      "loss": 3.6352,
      "step": 6810
    },
    {
      "epoch": 3.468972533062055,
      "grad_norm": 5.30334997177124,
      "learning_rate": 4.653102746693795e-05,
      "loss": 3.5961,
      "step": 6820
    },
    {
      "epoch": 3.474059003051882,
      "grad_norm": 6.82050895690918,
      "learning_rate": 4.652594099694812e-05,
      "loss": 3.6312,
      "step": 6830
    },
    {
      "epoch": 3.479145473041709,
      "grad_norm": 4.372466087341309,
      "learning_rate": 4.652085452695829e-05,
      "loss": 3.5823,
      "step": 6840
    },
    {
      "epoch": 3.4842319430315363,
      "grad_norm": 5.053226947784424,
      "learning_rate": 4.6515768056968467e-05,
      "loss": 3.5941,
      "step": 6850
    },
    {
      "epoch": 3.4893184130213633,
      "grad_norm": 6.829438209533691,
      "learning_rate": 4.6510681586978636e-05,
      "loss": 3.5406,
      "step": 6860
    },
    {
      "epoch": 3.4944048830111902,
      "grad_norm": 8.250497817993164,
      "learning_rate": 4.6505595116988806e-05,
      "loss": 3.6266,
      "step": 6870
    },
    {
      "epoch": 3.499491353001017,
      "grad_norm": 6.642141342163086,
      "learning_rate": 4.650050864699898e-05,
      "loss": 3.6075,
      "step": 6880
    },
    {
      "epoch": 3.504577822990844,
      "grad_norm": 4.867504119873047,
      "learning_rate": 4.649542217700916e-05,
      "loss": 3.6243,
      "step": 6890
    },
    {
      "epoch": 3.5096642929806716,
      "grad_norm": 5.355265140533447,
      "learning_rate": 4.6490335707019336e-05,
      "loss": 3.5015,
      "step": 6900
    },
    {
      "epoch": 3.5147507629704986,
      "grad_norm": 5.4089202880859375,
      "learning_rate": 4.6485249237029506e-05,
      "loss": 3.5895,
      "step": 6910
    },
    {
      "epoch": 3.5198372329603256,
      "grad_norm": 6.7899250984191895,
      "learning_rate": 4.6480162767039676e-05,
      "loss": 3.565,
      "step": 6920
    },
    {
      "epoch": 3.5249237029501526,
      "grad_norm": 4.2368483543396,
      "learning_rate": 4.647507629704985e-05,
      "loss": 3.5896,
      "step": 6930
    },
    {
      "epoch": 3.5300101729399795,
      "grad_norm": 7.55795431137085,
      "learning_rate": 4.646998982706002e-05,
      "loss": 3.4915,
      "step": 6940
    },
    {
      "epoch": 3.5350966429298065,
      "grad_norm": 5.70027494430542,
      "learning_rate": 4.646490335707019e-05,
      "loss": 3.5925,
      "step": 6950
    },
    {
      "epoch": 3.5401831129196335,
      "grad_norm": 5.808229446411133,
      "learning_rate": 4.645981688708037e-05,
      "loss": 3.6594,
      "step": 6960
    },
    {
      "epoch": 3.545269582909461,
      "grad_norm": 6.569754600524902,
      "learning_rate": 4.645473041709054e-05,
      "loss": 3.6093,
      "step": 6970
    },
    {
      "epoch": 3.550356052899288,
      "grad_norm": 5.768550395965576,
      "learning_rate": 4.6449643947100716e-05,
      "loss": 3.5639,
      "step": 6980
    },
    {
      "epoch": 3.555442522889115,
      "grad_norm": 5.893528938293457,
      "learning_rate": 4.644455747711089e-05,
      "loss": 3.5655,
      "step": 6990
    },
    {
      "epoch": 3.560528992878942,
      "grad_norm": 4.518820285797119,
      "learning_rate": 4.643947100712106e-05,
      "loss": 3.4665,
      "step": 7000
    },
    {
      "epoch": 3.5656154628687693,
      "grad_norm": 4.769210338592529,
      "learning_rate": 4.643438453713123e-05,
      "loss": 3.5411,
      "step": 7010
    },
    {
      "epoch": 3.5707019328585963,
      "grad_norm": 9.2518892288208,
      "learning_rate": 4.642929806714141e-05,
      "loss": 3.5721,
      "step": 7020
    },
    {
      "epoch": 3.5757884028484233,
      "grad_norm": 5.777797698974609,
      "learning_rate": 4.642421159715158e-05,
      "loss": 3.5474,
      "step": 7030
    },
    {
      "epoch": 3.5808748728382502,
      "grad_norm": 7.147435665130615,
      "learning_rate": 4.641912512716175e-05,
      "loss": 3.5347,
      "step": 7040
    },
    {
      "epoch": 3.585961342828077,
      "grad_norm": 5.864285469055176,
      "learning_rate": 4.6414038657171925e-05,
      "loss": 3.5141,
      "step": 7050
    },
    {
      "epoch": 3.591047812817904,
      "grad_norm": 6.365981101989746,
      "learning_rate": 4.6408952187182095e-05,
      "loss": 3.5434,
      "step": 7060
    },
    {
      "epoch": 3.596134282807731,
      "grad_norm": 7.023739337921143,
      "learning_rate": 4.640386571719227e-05,
      "loss": 3.5912,
      "step": 7070
    },
    {
      "epoch": 3.6012207527975586,
      "grad_norm": 4.688153266906738,
      "learning_rate": 4.639877924720244e-05,
      "loss": 3.6213,
      "step": 7080
    },
    {
      "epoch": 3.6063072227873856,
      "grad_norm": 6.201676368713379,
      "learning_rate": 4.639369277721262e-05,
      "loss": 3.5101,
      "step": 7090
    },
    {
      "epoch": 3.6113936927772126,
      "grad_norm": 6.988996982574463,
      "learning_rate": 4.638860630722279e-05,
      "loss": 3.5803,
      "step": 7100
    },
    {
      "epoch": 3.6164801627670395,
      "grad_norm": 4.757587432861328,
      "learning_rate": 4.6383519837232965e-05,
      "loss": 3.6021,
      "step": 7110
    },
    {
      "epoch": 3.621566632756867,
      "grad_norm": 6.333174705505371,
      "learning_rate": 4.6378433367243135e-05,
      "loss": 3.5773,
      "step": 7120
    },
    {
      "epoch": 3.626653102746694,
      "grad_norm": 4.091790199279785,
      "learning_rate": 4.6373346897253305e-05,
      "loss": 3.6065,
      "step": 7130
    },
    {
      "epoch": 3.631739572736521,
      "grad_norm": 5.441139221191406,
      "learning_rate": 4.636826042726348e-05,
      "loss": 3.5047,
      "step": 7140
    },
    {
      "epoch": 3.636826042726348,
      "grad_norm": 6.429261684417725,
      "learning_rate": 4.636317395727365e-05,
      "loss": 3.5854,
      "step": 7150
    },
    {
      "epoch": 3.641912512716175,
      "grad_norm": 5.253026008605957,
      "learning_rate": 4.635808748728383e-05,
      "loss": 3.501,
      "step": 7160
    },
    {
      "epoch": 3.646998982706002,
      "grad_norm": 6.200937271118164,
      "learning_rate": 4.6353001017294e-05,
      "loss": 3.5381,
      "step": 7170
    },
    {
      "epoch": 3.652085452695829,
      "grad_norm": 7.587273120880127,
      "learning_rate": 4.6347914547304175e-05,
      "loss": 3.5828,
      "step": 7180
    },
    {
      "epoch": 3.6571719226856563,
      "grad_norm": 11.000535011291504,
      "learning_rate": 4.634282807731435e-05,
      "loss": 3.5786,
      "step": 7190
    },
    {
      "epoch": 3.6622583926754833,
      "grad_norm": 6.3573198318481445,
      "learning_rate": 4.633774160732452e-05,
      "loss": 3.5805,
      "step": 7200
    },
    {
      "epoch": 3.6673448626653102,
      "grad_norm": 5.1232523918151855,
      "learning_rate": 4.633265513733469e-05,
      "loss": 3.5712,
      "step": 7210
    },
    {
      "epoch": 3.672431332655137,
      "grad_norm": 5.091216564178467,
      "learning_rate": 4.632756866734487e-05,
      "loss": 3.597,
      "step": 7220
    },
    {
      "epoch": 3.6775178026449646,
      "grad_norm": 6.637453079223633,
      "learning_rate": 4.632248219735504e-05,
      "loss": 3.5548,
      "step": 7230
    },
    {
      "epoch": 3.6826042726347916,
      "grad_norm": 5.850083827972412,
      "learning_rate": 4.631739572736521e-05,
      "loss": 3.5054,
      "step": 7240
    },
    {
      "epoch": 3.6876907426246186,
      "grad_norm": 5.623889923095703,
      "learning_rate": 4.6312309257375384e-05,
      "loss": 3.587,
      "step": 7250
    },
    {
      "epoch": 3.6927772126144456,
      "grad_norm": 6.446109771728516,
      "learning_rate": 4.6307222787385554e-05,
      "loss": 3.5791,
      "step": 7260
    },
    {
      "epoch": 3.6978636826042726,
      "grad_norm": 5.402917861938477,
      "learning_rate": 4.630213631739573e-05,
      "loss": 3.5435,
      "step": 7270
    },
    {
      "epoch": 3.7029501525940995,
      "grad_norm": 5.547103404998779,
      "learning_rate": 4.629704984740591e-05,
      "loss": 3.5471,
      "step": 7280
    },
    {
      "epoch": 3.7080366225839265,
      "grad_norm": 8.38454818725586,
      "learning_rate": 4.629196337741608e-05,
      "loss": 3.5268,
      "step": 7290
    },
    {
      "epoch": 3.713123092573754,
      "grad_norm": 6.1707305908203125,
      "learning_rate": 4.628687690742625e-05,
      "loss": 3.5909,
      "step": 7300
    },
    {
      "epoch": 3.718209562563581,
      "grad_norm": 7.111429691314697,
      "learning_rate": 4.6281790437436424e-05,
      "loss": 3.5725,
      "step": 7310
    },
    {
      "epoch": 3.723296032553408,
      "grad_norm": 4.615469932556152,
      "learning_rate": 4.6276703967446594e-05,
      "loss": 3.6313,
      "step": 7320
    },
    {
      "epoch": 3.728382502543235,
      "grad_norm": 6.948646068572998,
      "learning_rate": 4.6271617497456764e-05,
      "loss": 3.584,
      "step": 7330
    },
    {
      "epoch": 3.7334689725330623,
      "grad_norm": 4.330513000488281,
      "learning_rate": 4.626653102746694e-05,
      "loss": 3.5477,
      "step": 7340
    },
    {
      "epoch": 3.7385554425228893,
      "grad_norm": 5.677266597747803,
      "learning_rate": 4.626144455747711e-05,
      "loss": 3.6447,
      "step": 7350
    },
    {
      "epoch": 3.7436419125127163,
      "grad_norm": 6.416383266448975,
      "learning_rate": 4.625635808748729e-05,
      "loss": 3.5246,
      "step": 7360
    },
    {
      "epoch": 3.7487283825025433,
      "grad_norm": 4.948240280151367,
      "learning_rate": 4.6251271617497464e-05,
      "loss": 3.5381,
      "step": 7370
    },
    {
      "epoch": 3.7538148524923702,
      "grad_norm": 5.02435827255249,
      "learning_rate": 4.6246185147507634e-05,
      "loss": 3.5443,
      "step": 7380
    },
    {
      "epoch": 3.758901322482197,
      "grad_norm": 4.8628153800964355,
      "learning_rate": 4.6241098677517803e-05,
      "loss": 3.5125,
      "step": 7390
    },
    {
      "epoch": 3.763987792472024,
      "grad_norm": 5.5221381187438965,
      "learning_rate": 4.623601220752798e-05,
      "loss": 3.5262,
      "step": 7400
    },
    {
      "epoch": 3.7690742624618516,
      "grad_norm": 9.55009651184082,
      "learning_rate": 4.623092573753815e-05,
      "loss": 3.5853,
      "step": 7410
    },
    {
      "epoch": 3.7741607324516786,
      "grad_norm": 5.79356050491333,
      "learning_rate": 4.622583926754832e-05,
      "loss": 3.5956,
      "step": 7420
    },
    {
      "epoch": 3.7792472024415056,
      "grad_norm": 4.877356052398682,
      "learning_rate": 4.6220752797558497e-05,
      "loss": 3.5158,
      "step": 7430
    },
    {
      "epoch": 3.7843336724313326,
      "grad_norm": 5.197232246398926,
      "learning_rate": 4.6215666327568666e-05,
      "loss": 3.6362,
      "step": 7440
    },
    {
      "epoch": 3.78942014242116,
      "grad_norm": 5.297779083251953,
      "learning_rate": 4.621057985757884e-05,
      "loss": 3.5105,
      "step": 7450
    },
    {
      "epoch": 3.794506612410987,
      "grad_norm": 5.04917049407959,
      "learning_rate": 4.620549338758901e-05,
      "loss": 3.5439,
      "step": 7460
    },
    {
      "epoch": 3.799593082400814,
      "grad_norm": 5.650892734527588,
      "learning_rate": 4.620040691759919e-05,
      "loss": 3.4941,
      "step": 7470
    },
    {
      "epoch": 3.804679552390641,
      "grad_norm": 5.529642581939697,
      "learning_rate": 4.6195320447609366e-05,
      "loss": 3.5082,
      "step": 7480
    },
    {
      "epoch": 3.809766022380468,
      "grad_norm": 5.204840660095215,
      "learning_rate": 4.6190233977619536e-05,
      "loss": 3.5312,
      "step": 7490
    },
    {
      "epoch": 3.814852492370295,
      "grad_norm": 5.493813991546631,
      "learning_rate": 4.6185147507629706e-05,
      "loss": 3.5126,
      "step": 7500
    },
    {
      "epoch": 3.819938962360122,
      "grad_norm": 6.871865749359131,
      "learning_rate": 4.618006103763988e-05,
      "loss": 3.5112,
      "step": 7510
    },
    {
      "epoch": 3.8250254323499493,
      "grad_norm": 6.430351734161377,
      "learning_rate": 4.617497456765005e-05,
      "loss": 3.5615,
      "step": 7520
    },
    {
      "epoch": 3.8301119023397763,
      "grad_norm": 5.755605697631836,
      "learning_rate": 4.616988809766022e-05,
      "loss": 3.5298,
      "step": 7530
    },
    {
      "epoch": 3.8351983723296033,
      "grad_norm": 5.163219928741455,
      "learning_rate": 4.61648016276704e-05,
      "loss": 3.5213,
      "step": 7540
    },
    {
      "epoch": 3.8402848423194302,
      "grad_norm": 5.366949081420898,
      "learning_rate": 4.615971515768057e-05,
      "loss": 3.5481,
      "step": 7550
    },
    {
      "epoch": 3.845371312309257,
      "grad_norm": 7.023998737335205,
      "learning_rate": 4.6154628687690746e-05,
      "loss": 3.6106,
      "step": 7560
    },
    {
      "epoch": 3.8504577822990846,
      "grad_norm": 5.381234645843506,
      "learning_rate": 4.614954221770092e-05,
      "loss": 3.5839,
      "step": 7570
    },
    {
      "epoch": 3.8555442522889116,
      "grad_norm": 9.15304183959961,
      "learning_rate": 4.614445574771109e-05,
      "loss": 3.4529,
      "step": 7580
    },
    {
      "epoch": 3.8606307222787386,
      "grad_norm": 4.431382179260254,
      "learning_rate": 4.613936927772126e-05,
      "loss": 3.5682,
      "step": 7590
    },
    {
      "epoch": 3.8657171922685656,
      "grad_norm": 6.945425510406494,
      "learning_rate": 4.613428280773144e-05,
      "loss": 3.5706,
      "step": 7600
    },
    {
      "epoch": 3.8708036622583926,
      "grad_norm": 9.154135704040527,
      "learning_rate": 4.612919633774161e-05,
      "loss": 3.5444,
      "step": 7610
    },
    {
      "epoch": 3.8758901322482195,
      "grad_norm": 7.302424430847168,
      "learning_rate": 4.612410986775178e-05,
      "loss": 3.5419,
      "step": 7620
    },
    {
      "epoch": 3.8809766022380465,
      "grad_norm": 5.929433345794678,
      "learning_rate": 4.6119023397761955e-05,
      "loss": 3.5432,
      "step": 7630
    },
    {
      "epoch": 3.886063072227874,
      "grad_norm": 6.965738296508789,
      "learning_rate": 4.6113936927772125e-05,
      "loss": 3.5536,
      "step": 7640
    },
    {
      "epoch": 3.891149542217701,
      "grad_norm": 6.285124778747559,
      "learning_rate": 4.61088504577823e-05,
      "loss": 3.4841,
      "step": 7650
    },
    {
      "epoch": 3.896236012207528,
      "grad_norm": 6.06207275390625,
      "learning_rate": 4.610376398779248e-05,
      "loss": 3.6346,
      "step": 7660
    },
    {
      "epoch": 3.901322482197355,
      "grad_norm": 4.360729694366455,
      "learning_rate": 4.609867751780265e-05,
      "loss": 3.5855,
      "step": 7670
    },
    {
      "epoch": 3.9064089521871823,
      "grad_norm": 6.817298889160156,
      "learning_rate": 4.609359104781282e-05,
      "loss": 3.5162,
      "step": 7680
    },
    {
      "epoch": 3.9114954221770093,
      "grad_norm": 5.378087997436523,
      "learning_rate": 4.6088504577822995e-05,
      "loss": 3.5715,
      "step": 7690
    },
    {
      "epoch": 3.9165818921668363,
      "grad_norm": 5.7994489669799805,
      "learning_rate": 4.6083418107833165e-05,
      "loss": 3.5501,
      "step": 7700
    },
    {
      "epoch": 3.9216683621566633,
      "grad_norm": 6.699179172515869,
      "learning_rate": 4.607833163784334e-05,
      "loss": 3.5948,
      "step": 7710
    },
    {
      "epoch": 3.9267548321464902,
      "grad_norm": 6.175057888031006,
      "learning_rate": 4.607324516785351e-05,
      "loss": 3.5615,
      "step": 7720
    },
    {
      "epoch": 3.931841302136317,
      "grad_norm": 6.6745452880859375,
      "learning_rate": 4.606815869786368e-05,
      "loss": 3.4739,
      "step": 7730
    },
    {
      "epoch": 3.936927772126144,
      "grad_norm": 6.519194602966309,
      "learning_rate": 4.606307222787386e-05,
      "loss": 3.5052,
      "step": 7740
    },
    {
      "epoch": 3.9420142421159716,
      "grad_norm": 5.105235576629639,
      "learning_rate": 4.605798575788403e-05,
      "loss": 3.5508,
      "step": 7750
    },
    {
      "epoch": 3.9471007121057986,
      "grad_norm": 7.490808486938477,
      "learning_rate": 4.6052899287894205e-05,
      "loss": 3.4862,
      "step": 7760
    },
    {
      "epoch": 3.9521871820956256,
      "grad_norm": 6.9258294105529785,
      "learning_rate": 4.604781281790438e-05,
      "loss": 3.4812,
      "step": 7770
    },
    {
      "epoch": 3.9572736520854526,
      "grad_norm": 4.654047012329102,
      "learning_rate": 4.604272634791455e-05,
      "loss": 3.525,
      "step": 7780
    },
    {
      "epoch": 3.96236012207528,
      "grad_norm": 6.667936325073242,
      "learning_rate": 4.603763987792472e-05,
      "loss": 3.5566,
      "step": 7790
    },
    {
      "epoch": 3.967446592065107,
      "grad_norm": 6.412913799285889,
      "learning_rate": 4.60325534079349e-05,
      "loss": 3.4717,
      "step": 7800
    },
    {
      "epoch": 3.972533062054934,
      "grad_norm": 5.949088096618652,
      "learning_rate": 4.602746693794507e-05,
      "loss": 3.5673,
      "step": 7810
    },
    {
      "epoch": 3.977619532044761,
      "grad_norm": 4.598972797393799,
      "learning_rate": 4.602238046795524e-05,
      "loss": 3.5397,
      "step": 7820
    },
    {
      "epoch": 3.982706002034588,
      "grad_norm": 7.5904130935668945,
      "learning_rate": 4.6017293997965414e-05,
      "loss": 3.5587,
      "step": 7830
    },
    {
      "epoch": 3.987792472024415,
      "grad_norm": 6.036080837249756,
      "learning_rate": 4.6012207527975584e-05,
      "loss": 3.5251,
      "step": 7840
    },
    {
      "epoch": 3.992878942014242,
      "grad_norm": 5.858404636383057,
      "learning_rate": 4.600712105798576e-05,
      "loss": 3.5263,
      "step": 7850
    },
    {
      "epoch": 3.9979654120040693,
      "grad_norm": 6.2696332931518555,
      "learning_rate": 4.600203458799594e-05,
      "loss": 3.4358,
      "step": 7860
    },
    {
      "epoch": 4.0,
      "eval_loss": 3.6870806217193604,
      "eval_runtime": 2.6793,
      "eval_samples_per_second": 1035.721,
      "eval_steps_per_second": 129.512,
      "step": 7864
    },
    {
      "epoch": 4.003051881993896,
      "grad_norm": 5.84808874130249,
      "learning_rate": 4.599694811800611e-05,
      "loss": 3.5769,
      "step": 7870
    },
    {
      "epoch": 4.008138351983724,
      "grad_norm": 6.910951137542725,
      "learning_rate": 4.599186164801628e-05,
      "loss": 3.5624,
      "step": 7880
    },
    {
      "epoch": 4.013224821973551,
      "grad_norm": 7.399456977844238,
      "learning_rate": 4.5986775178026454e-05,
      "loss": 3.5466,
      "step": 7890
    },
    {
      "epoch": 4.018311291963378,
      "grad_norm": 6.991793155670166,
      "learning_rate": 4.5981688708036624e-05,
      "loss": 3.5271,
      "step": 7900
    },
    {
      "epoch": 4.023397761953205,
      "grad_norm": 6.519759178161621,
      "learning_rate": 4.5976602238046794e-05,
      "loss": 3.4815,
      "step": 7910
    },
    {
      "epoch": 4.028484231943032,
      "grad_norm": 6.074951171875,
      "learning_rate": 4.597151576805697e-05,
      "loss": 3.5704,
      "step": 7920
    },
    {
      "epoch": 4.033570701932859,
      "grad_norm": 5.455128192901611,
      "learning_rate": 4.596642929806714e-05,
      "loss": 3.5322,
      "step": 7930
    },
    {
      "epoch": 4.038657171922686,
      "grad_norm": 5.989661693572998,
      "learning_rate": 4.596134282807732e-05,
      "loss": 3.4652,
      "step": 7940
    },
    {
      "epoch": 4.043743641912513,
      "grad_norm": 6.176759243011475,
      "learning_rate": 4.5956256358087494e-05,
      "loss": 3.5253,
      "step": 7950
    },
    {
      "epoch": 4.0488301119023395,
      "grad_norm": 6.516461372375488,
      "learning_rate": 4.5951169888097664e-05,
      "loss": 3.5593,
      "step": 7960
    },
    {
      "epoch": 4.0539165818921665,
      "grad_norm": 5.6406402587890625,
      "learning_rate": 4.594608341810784e-05,
      "loss": 3.5227,
      "step": 7970
    },
    {
      "epoch": 4.0590030518819935,
      "grad_norm": 7.355774402618408,
      "learning_rate": 4.594099694811801e-05,
      "loss": 3.4906,
      "step": 7980
    },
    {
      "epoch": 4.064089521871821,
      "grad_norm": 7.388492584228516,
      "learning_rate": 4.593591047812818e-05,
      "loss": 3.4895,
      "step": 7990
    },
    {
      "epoch": 4.069175991861648,
      "grad_norm": 7.947268962860107,
      "learning_rate": 4.593082400813836e-05,
      "loss": 3.5441,
      "step": 8000
    },
    {
      "epoch": 4.074262461851475,
      "grad_norm": 7.20912504196167,
      "learning_rate": 4.5925737538148527e-05,
      "loss": 3.5519,
      "step": 8010
    },
    {
      "epoch": 4.079348931841302,
      "grad_norm": 6.576728820800781,
      "learning_rate": 4.5920651068158696e-05,
      "loss": 3.5611,
      "step": 8020
    },
    {
      "epoch": 4.084435401831129,
      "grad_norm": 6.395181655883789,
      "learning_rate": 4.591556459816887e-05,
      "loss": 3.5403,
      "step": 8030
    },
    {
      "epoch": 4.089521871820956,
      "grad_norm": 5.22281551361084,
      "learning_rate": 4.591047812817904e-05,
      "loss": 3.5103,
      "step": 8040
    },
    {
      "epoch": 4.094608341810783,
      "grad_norm": 7.34916877746582,
      "learning_rate": 4.590539165818922e-05,
      "loss": 3.5208,
      "step": 8050
    },
    {
      "epoch": 4.09969481180061,
      "grad_norm": 5.591099262237549,
      "learning_rate": 4.5900305188199396e-05,
      "loss": 3.5609,
      "step": 8060
    },
    {
      "epoch": 4.104781281790437,
      "grad_norm": 5.333564758300781,
      "learning_rate": 4.5895218718209566e-05,
      "loss": 3.5316,
      "step": 8070
    },
    {
      "epoch": 4.109867751780264,
      "grad_norm": 6.185909271240234,
      "learning_rate": 4.5890132248219736e-05,
      "loss": 3.4652,
      "step": 8080
    },
    {
      "epoch": 4.114954221770091,
      "grad_norm": 5.670868873596191,
      "learning_rate": 4.588504577822991e-05,
      "loss": 3.5385,
      "step": 8090
    },
    {
      "epoch": 4.120040691759919,
      "grad_norm": 5.862897872924805,
      "learning_rate": 4.587995930824008e-05,
      "loss": 3.4664,
      "step": 8100
    },
    {
      "epoch": 4.125127161749746,
      "grad_norm": 5.833090782165527,
      "learning_rate": 4.587487283825025e-05,
      "loss": 3.5046,
      "step": 8110
    },
    {
      "epoch": 4.130213631739573,
      "grad_norm": 6.568278789520264,
      "learning_rate": 4.586978636826043e-05,
      "loss": 3.5422,
      "step": 8120
    },
    {
      "epoch": 4.1353001017294,
      "grad_norm": 8.386506080627441,
      "learning_rate": 4.58646998982706e-05,
      "loss": 3.4919,
      "step": 8130
    },
    {
      "epoch": 4.140386571719227,
      "grad_norm": 5.642545223236084,
      "learning_rate": 4.5859613428280776e-05,
      "loss": 3.552,
      "step": 8140
    },
    {
      "epoch": 4.145473041709054,
      "grad_norm": 6.534317493438721,
      "learning_rate": 4.585452695829095e-05,
      "loss": 3.4837,
      "step": 8150
    },
    {
      "epoch": 4.150559511698881,
      "grad_norm": 7.1066179275512695,
      "learning_rate": 4.584944048830112e-05,
      "loss": 3.4524,
      "step": 8160
    },
    {
      "epoch": 4.155645981688708,
      "grad_norm": 5.316425323486328,
      "learning_rate": 4.584435401831129e-05,
      "loss": 3.5209,
      "step": 8170
    },
    {
      "epoch": 4.160732451678535,
      "grad_norm": 6.180997371673584,
      "learning_rate": 4.583926754832147e-05,
      "loss": 3.5224,
      "step": 8180
    },
    {
      "epoch": 4.165818921668362,
      "grad_norm": 6.601441860198975,
      "learning_rate": 4.583418107833164e-05,
      "loss": 3.5603,
      "step": 8190
    },
    {
      "epoch": 4.170905391658189,
      "grad_norm": 5.998661994934082,
      "learning_rate": 4.582909460834181e-05,
      "loss": 3.5143,
      "step": 8200
    },
    {
      "epoch": 4.175991861648017,
      "grad_norm": 5.674927711486816,
      "learning_rate": 4.5824008138351985e-05,
      "loss": 3.4611,
      "step": 8210
    },
    {
      "epoch": 4.181078331637844,
      "grad_norm": 6.2485671043396,
      "learning_rate": 4.5818921668362155e-05,
      "loss": 3.4643,
      "step": 8220
    },
    {
      "epoch": 4.186164801627671,
      "grad_norm": 7.158141613006592,
      "learning_rate": 4.581383519837233e-05,
      "loss": 3.4639,
      "step": 8230
    },
    {
      "epoch": 4.191251271617498,
      "grad_norm": 6.6262431144714355,
      "learning_rate": 4.580874872838251e-05,
      "loss": 3.4649,
      "step": 8240
    },
    {
      "epoch": 4.196337741607325,
      "grad_norm": 7.603287220001221,
      "learning_rate": 4.580366225839268e-05,
      "loss": 3.5326,
      "step": 8250
    },
    {
      "epoch": 4.201424211597152,
      "grad_norm": 6.2046403884887695,
      "learning_rate": 4.5798575788402855e-05,
      "loss": 3.4912,
      "step": 8260
    },
    {
      "epoch": 4.206510681586979,
      "grad_norm": 6.46549654006958,
      "learning_rate": 4.5793489318413025e-05,
      "loss": 3.4364,
      "step": 8270
    },
    {
      "epoch": 4.211597151576806,
      "grad_norm": 9.815648078918457,
      "learning_rate": 4.5788402848423195e-05,
      "loss": 3.4726,
      "step": 8280
    },
    {
      "epoch": 4.216683621566633,
      "grad_norm": 6.973529815673828,
      "learning_rate": 4.578331637843337e-05,
      "loss": 3.5368,
      "step": 8290
    },
    {
      "epoch": 4.2217700915564595,
      "grad_norm": 6.640503883361816,
      "learning_rate": 4.577822990844354e-05,
      "loss": 3.5042,
      "step": 8300
    },
    {
      "epoch": 4.2268565615462865,
      "grad_norm": 6.75279426574707,
      "learning_rate": 4.577314343845371e-05,
      "loss": 3.4623,
      "step": 8310
    },
    {
      "epoch": 4.2319430315361135,
      "grad_norm": 7.035434722900391,
      "learning_rate": 4.576805696846389e-05,
      "loss": 3.4708,
      "step": 8320
    },
    {
      "epoch": 4.237029501525941,
      "grad_norm": 8.213186264038086,
      "learning_rate": 4.5762970498474065e-05,
      "loss": 3.461,
      "step": 8330
    },
    {
      "epoch": 4.242115971515768,
      "grad_norm": 7.839914798736572,
      "learning_rate": 4.5757884028484235e-05,
      "loss": 3.4438,
      "step": 8340
    },
    {
      "epoch": 4.247202441505595,
      "grad_norm": 6.409915924072266,
      "learning_rate": 4.575279755849441e-05,
      "loss": 3.4713,
      "step": 8350
    },
    {
      "epoch": 4.252288911495422,
      "grad_norm": 7.141901016235352,
      "learning_rate": 4.574771108850458e-05,
      "loss": 3.5132,
      "step": 8360
    },
    {
      "epoch": 4.257375381485249,
      "grad_norm": 9.487360000610352,
      "learning_rate": 4.574262461851475e-05,
      "loss": 3.4465,
      "step": 8370
    },
    {
      "epoch": 4.262461851475076,
      "grad_norm": 6.028769493103027,
      "learning_rate": 4.573753814852493e-05,
      "loss": 3.5287,
      "step": 8380
    },
    {
      "epoch": 4.267548321464903,
      "grad_norm": 8.045287132263184,
      "learning_rate": 4.57324516785351e-05,
      "loss": 3.4913,
      "step": 8390
    },
    {
      "epoch": 4.27263479145473,
      "grad_norm": 7.016541957855225,
      "learning_rate": 4.572736520854527e-05,
      "loss": 3.5024,
      "step": 8400
    },
    {
      "epoch": 4.277721261444557,
      "grad_norm": 8.580831527709961,
      "learning_rate": 4.5722278738555444e-05,
      "loss": 3.4194,
      "step": 8410
    },
    {
      "epoch": 4.282807731434384,
      "grad_norm": 6.608240127563477,
      "learning_rate": 4.5717192268565614e-05,
      "loss": 3.5327,
      "step": 8420
    },
    {
      "epoch": 4.287894201424211,
      "grad_norm": 6.825939178466797,
      "learning_rate": 4.571210579857579e-05,
      "loss": 3.5088,
      "step": 8430
    },
    {
      "epoch": 4.292980671414039,
      "grad_norm": 7.93143367767334,
      "learning_rate": 4.570701932858597e-05,
      "loss": 3.5188,
      "step": 8440
    },
    {
      "epoch": 4.298067141403866,
      "grad_norm": 7.117186546325684,
      "learning_rate": 4.570193285859614e-05,
      "loss": 3.3578,
      "step": 8450
    },
    {
      "epoch": 4.303153611393693,
      "grad_norm": 6.295094013214111,
      "learning_rate": 4.569684638860631e-05,
      "loss": 3.5015,
      "step": 8460
    },
    {
      "epoch": 4.30824008138352,
      "grad_norm": 7.897289752960205,
      "learning_rate": 4.5691759918616484e-05,
      "loss": 3.4597,
      "step": 8470
    },
    {
      "epoch": 4.313326551373347,
      "grad_norm": 6.831140041351318,
      "learning_rate": 4.5686673448626654e-05,
      "loss": 3.4947,
      "step": 8480
    },
    {
      "epoch": 4.318413021363174,
      "grad_norm": 9.467386245727539,
      "learning_rate": 4.5681586978636824e-05,
      "loss": 3.5195,
      "step": 8490
    },
    {
      "epoch": 4.323499491353001,
      "grad_norm": 7.796557426452637,
      "learning_rate": 4.5676500508647e-05,
      "loss": 3.4879,
      "step": 8500
    },
    {
      "epoch": 4.328585961342828,
      "grad_norm": 6.665657997131348,
      "learning_rate": 4.567141403865717e-05,
      "loss": 3.5127,
      "step": 8510
    },
    {
      "epoch": 4.333672431332655,
      "grad_norm": 6.197167873382568,
      "learning_rate": 4.566632756866735e-05,
      "loss": 3.4602,
      "step": 8520
    },
    {
      "epoch": 4.338758901322482,
      "grad_norm": 7.088810443878174,
      "learning_rate": 4.5661241098677524e-05,
      "loss": 3.4184,
      "step": 8530
    },
    {
      "epoch": 4.343845371312309,
      "grad_norm": 7.874248027801514,
      "learning_rate": 4.5656154628687694e-05,
      "loss": 3.4504,
      "step": 8540
    },
    {
      "epoch": 4.348931841302137,
      "grad_norm": 6.594128131866455,
      "learning_rate": 4.565106815869787e-05,
      "loss": 3.5293,
      "step": 8550
    },
    {
      "epoch": 4.354018311291964,
      "grad_norm": 7.080340385437012,
      "learning_rate": 4.564598168870804e-05,
      "loss": 3.434,
      "step": 8560
    },
    {
      "epoch": 4.359104781281791,
      "grad_norm": 5.372812271118164,
      "learning_rate": 4.564089521871821e-05,
      "loss": 3.5188,
      "step": 8570
    },
    {
      "epoch": 4.364191251271618,
      "grad_norm": 6.204227447509766,
      "learning_rate": 4.563580874872839e-05,
      "loss": 3.4287,
      "step": 8580
    },
    {
      "epoch": 4.369277721261445,
      "grad_norm": 8.148781776428223,
      "learning_rate": 4.5630722278738557e-05,
      "loss": 3.4803,
      "step": 8590
    },
    {
      "epoch": 4.374364191251272,
      "grad_norm": 6.666170597076416,
      "learning_rate": 4.5625635808748726e-05,
      "loss": 3.4848,
      "step": 8600
    },
    {
      "epoch": 4.379450661241099,
      "grad_norm": 8.510690689086914,
      "learning_rate": 4.56205493387589e-05,
      "loss": 3.4648,
      "step": 8610
    },
    {
      "epoch": 4.384537131230926,
      "grad_norm": 8.282868385314941,
      "learning_rate": 4.561546286876908e-05,
      "loss": 3.4719,
      "step": 8620
    },
    {
      "epoch": 4.389623601220753,
      "grad_norm": 5.057900428771973,
      "learning_rate": 4.561037639877925e-05,
      "loss": 3.478,
      "step": 8630
    },
    {
      "epoch": 4.3947100712105795,
      "grad_norm": 8.918657302856445,
      "learning_rate": 4.5605289928789426e-05,
      "loss": 3.4614,
      "step": 8640
    },
    {
      "epoch": 4.3997965412004065,
      "grad_norm": 7.845905780792236,
      "learning_rate": 4.5600203458799596e-05,
      "loss": 3.4778,
      "step": 8650
    },
    {
      "epoch": 4.404883011190234,
      "grad_norm": 9.447675704956055,
      "learning_rate": 4.5595116988809766e-05,
      "loss": 3.5066,
      "step": 8660
    },
    {
      "epoch": 4.409969481180061,
      "grad_norm": 5.6529154777526855,
      "learning_rate": 4.559003051881994e-05,
      "loss": 3.3809,
      "step": 8670
    },
    {
      "epoch": 4.415055951169888,
      "grad_norm": 9.205937385559082,
      "learning_rate": 4.558494404883011e-05,
      "loss": 3.458,
      "step": 8680
    },
    {
      "epoch": 4.420142421159715,
      "grad_norm": 6.617910861968994,
      "learning_rate": 4.557985757884028e-05,
      "loss": 3.43,
      "step": 8690
    },
    {
      "epoch": 4.425228891149542,
      "grad_norm": 7.692637920379639,
      "learning_rate": 4.557477110885046e-05,
      "loss": 3.5498,
      "step": 8700
    },
    {
      "epoch": 4.430315361139369,
      "grad_norm": 8.768918991088867,
      "learning_rate": 4.556968463886063e-05,
      "loss": 3.4414,
      "step": 8710
    },
    {
      "epoch": 4.435401831129196,
      "grad_norm": 8.039145469665527,
      "learning_rate": 4.5564598168870806e-05,
      "loss": 3.4992,
      "step": 8720
    },
    {
      "epoch": 4.440488301119023,
      "grad_norm": 8.133491516113281,
      "learning_rate": 4.555951169888098e-05,
      "loss": 3.4608,
      "step": 8730
    },
    {
      "epoch": 4.44557477110885,
      "grad_norm": 6.4011149406433105,
      "learning_rate": 4.555442522889115e-05,
      "loss": 3.4328,
      "step": 8740
    },
    {
      "epoch": 4.450661241098677,
      "grad_norm": 6.637654781341553,
      "learning_rate": 4.554933875890132e-05,
      "loss": 3.3785,
      "step": 8750
    },
    {
      "epoch": 4.455747711088504,
      "grad_norm": 7.677149295806885,
      "learning_rate": 4.55442522889115e-05,
      "loss": 3.4849,
      "step": 8760
    },
    {
      "epoch": 4.460834181078331,
      "grad_norm": 7.639240741729736,
      "learning_rate": 4.553916581892167e-05,
      "loss": 3.4859,
      "step": 8770
    },
    {
      "epoch": 4.465920651068159,
      "grad_norm": 8.05338191986084,
      "learning_rate": 4.5534079348931846e-05,
      "loss": 3.4196,
      "step": 8780
    },
    {
      "epoch": 4.471007121057986,
      "grad_norm": 6.806941986083984,
      "learning_rate": 4.5528992878942015e-05,
      "loss": 3.5578,
      "step": 8790
    },
    {
      "epoch": 4.476093591047813,
      "grad_norm": 7.233952045440674,
      "learning_rate": 4.5523906408952185e-05,
      "loss": 3.4033,
      "step": 8800
    },
    {
      "epoch": 4.48118006103764,
      "grad_norm": 9.229225158691406,
      "learning_rate": 4.551881993896236e-05,
      "loss": 3.4055,
      "step": 8810
    },
    {
      "epoch": 4.486266531027467,
      "grad_norm": 8.897475242614746,
      "learning_rate": 4.551373346897254e-05,
      "loss": 3.4048,
      "step": 8820
    },
    {
      "epoch": 4.491353001017294,
      "grad_norm": 7.134532928466797,
      "learning_rate": 4.550864699898271e-05,
      "loss": 3.5022,
      "step": 8830
    },
    {
      "epoch": 4.496439471007121,
      "grad_norm": 6.268679618835449,
      "learning_rate": 4.5503560528992885e-05,
      "loss": 3.4535,
      "step": 8840
    },
    {
      "epoch": 4.501525940996948,
      "grad_norm": 8.39194107055664,
      "learning_rate": 4.5498474059003055e-05,
      "loss": 3.4864,
      "step": 8850
    },
    {
      "epoch": 4.506612410986775,
      "grad_norm": 9.036888122558594,
      "learning_rate": 4.5493387589013225e-05,
      "loss": 3.4852,
      "step": 8860
    },
    {
      "epoch": 4.511698880976602,
      "grad_norm": 6.8496503829956055,
      "learning_rate": 4.54883011190234e-05,
      "loss": 3.4557,
      "step": 8870
    },
    {
      "epoch": 4.51678535096643,
      "grad_norm": 8.66197395324707,
      "learning_rate": 4.548321464903357e-05,
      "loss": 3.4212,
      "step": 8880
    },
    {
      "epoch": 4.521871820956257,
      "grad_norm": 9.332901954650879,
      "learning_rate": 4.547812817904374e-05,
      "loss": 3.5132,
      "step": 8890
    },
    {
      "epoch": 4.526958290946084,
      "grad_norm": 7.599575519561768,
      "learning_rate": 4.547304170905392e-05,
      "loss": 3.4446,
      "step": 8900
    },
    {
      "epoch": 4.532044760935911,
      "grad_norm": 7.490322113037109,
      "learning_rate": 4.5467955239064095e-05,
      "loss": 3.462,
      "step": 8910
    },
    {
      "epoch": 4.537131230925738,
      "grad_norm": 8.404818534851074,
      "learning_rate": 4.5462868769074265e-05,
      "loss": 3.4751,
      "step": 8920
    },
    {
      "epoch": 4.542217700915565,
      "grad_norm": 6.408419609069824,
      "learning_rate": 4.545778229908444e-05,
      "loss": 3.4726,
      "step": 8930
    },
    {
      "epoch": 4.547304170905392,
      "grad_norm": 5.526950359344482,
      "learning_rate": 4.545269582909461e-05,
      "loss": 3.4758,
      "step": 8940
    },
    {
      "epoch": 4.552390640895219,
      "grad_norm": 8.67048168182373,
      "learning_rate": 4.544760935910478e-05,
      "loss": 3.4411,
      "step": 8950
    },
    {
      "epoch": 4.557477110885046,
      "grad_norm": 7.330310821533203,
      "learning_rate": 4.544252288911496e-05,
      "loss": 3.4614,
      "step": 8960
    },
    {
      "epoch": 4.562563580874873,
      "grad_norm": 9.197126388549805,
      "learning_rate": 4.543743641912513e-05,
      "loss": 3.5036,
      "step": 8970
    },
    {
      "epoch": 4.5676500508646996,
      "grad_norm": 6.245562553405762,
      "learning_rate": 4.54323499491353e-05,
      "loss": 3.4846,
      "step": 8980
    },
    {
      "epoch": 4.5727365208545265,
      "grad_norm": 10.178180694580078,
      "learning_rate": 4.5427263479145474e-05,
      "loss": 3.4501,
      "step": 8990
    },
    {
      "epoch": 4.577822990844354,
      "grad_norm": 6.755914211273193,
      "learning_rate": 4.5422177009155644e-05,
      "loss": 3.4573,
      "step": 9000
    },
    {
      "epoch": 4.582909460834181,
      "grad_norm": 11.36359691619873,
      "learning_rate": 4.541709053916582e-05,
      "loss": 3.4791,
      "step": 9010
    },
    {
      "epoch": 4.587995930824008,
      "grad_norm": 6.885517120361328,
      "learning_rate": 4.5412004069176e-05,
      "loss": 3.5291,
      "step": 9020
    },
    {
      "epoch": 4.593082400813835,
      "grad_norm": 8.114654541015625,
      "learning_rate": 4.540691759918617e-05,
      "loss": 3.4486,
      "step": 9030
    },
    {
      "epoch": 4.598168870803662,
      "grad_norm": 7.776980876922607,
      "learning_rate": 4.540183112919634e-05,
      "loss": 3.3762,
      "step": 9040
    },
    {
      "epoch": 4.603255340793489,
      "grad_norm": 9.37743854522705,
      "learning_rate": 4.5396744659206514e-05,
      "loss": 3.4698,
      "step": 9050
    },
    {
      "epoch": 4.608341810783316,
      "grad_norm": 7.510167598724365,
      "learning_rate": 4.5391658189216684e-05,
      "loss": 3.4499,
      "step": 9060
    },
    {
      "epoch": 4.613428280773143,
      "grad_norm": 8.669599533081055,
      "learning_rate": 4.538657171922686e-05,
      "loss": 3.4765,
      "step": 9070
    },
    {
      "epoch": 4.61851475076297,
      "grad_norm": 7.406222820281982,
      "learning_rate": 4.538148524923703e-05,
      "loss": 3.4554,
      "step": 9080
    },
    {
      "epoch": 4.623601220752797,
      "grad_norm": 8.941712379455566,
      "learning_rate": 4.53763987792472e-05,
      "loss": 3.4292,
      "step": 9090
    },
    {
      "epoch": 4.628687690742625,
      "grad_norm": 8.585077285766602,
      "learning_rate": 4.537131230925738e-05,
      "loss": 3.4978,
      "step": 9100
    },
    {
      "epoch": 4.633774160732452,
      "grad_norm": 8.143572807312012,
      "learning_rate": 4.5366225839267554e-05,
      "loss": 3.4796,
      "step": 9110
    },
    {
      "epoch": 4.638860630722279,
      "grad_norm": 8.052303314208984,
      "learning_rate": 4.5361139369277724e-05,
      "loss": 3.4769,
      "step": 9120
    },
    {
      "epoch": 4.643947100712106,
      "grad_norm": 8.309174537658691,
      "learning_rate": 4.53560528992879e-05,
      "loss": 3.4884,
      "step": 9130
    },
    {
      "epoch": 4.649033570701933,
      "grad_norm": 7.226728439331055,
      "learning_rate": 4.535096642929807e-05,
      "loss": 3.4919,
      "step": 9140
    },
    {
      "epoch": 4.65412004069176,
      "grad_norm": 10.464227676391602,
      "learning_rate": 4.534587995930824e-05,
      "loss": 3.3851,
      "step": 9150
    },
    {
      "epoch": 4.659206510681587,
      "grad_norm": 8.3024320602417,
      "learning_rate": 4.534079348931842e-05,
      "loss": 3.4285,
      "step": 9160
    },
    {
      "epoch": 4.664292980671414,
      "grad_norm": 6.473715305328369,
      "learning_rate": 4.5335707019328587e-05,
      "loss": 3.5036,
      "step": 9170
    },
    {
      "epoch": 4.669379450661241,
      "grad_norm": 7.222052097320557,
      "learning_rate": 4.5330620549338756e-05,
      "loss": 3.4126,
      "step": 9180
    },
    {
      "epoch": 4.674465920651068,
      "grad_norm": 9.240386962890625,
      "learning_rate": 4.532553407934893e-05,
      "loss": 3.3971,
      "step": 9190
    },
    {
      "epoch": 4.679552390640895,
      "grad_norm": 7.217928409576416,
      "learning_rate": 4.532044760935911e-05,
      "loss": 3.5304,
      "step": 9200
    },
    {
      "epoch": 4.684638860630722,
      "grad_norm": 7.457093715667725,
      "learning_rate": 4.531536113936928e-05,
      "loss": 3.4025,
      "step": 9210
    },
    {
      "epoch": 4.689725330620549,
      "grad_norm": 7.842870235443115,
      "learning_rate": 4.5310274669379456e-05,
      "loss": 3.4071,
      "step": 9220
    },
    {
      "epoch": 4.694811800610377,
      "grad_norm": 6.347442150115967,
      "learning_rate": 4.5305188199389626e-05,
      "loss": 3.4498,
      "step": 9230
    },
    {
      "epoch": 4.699898270600204,
      "grad_norm": 7.595486164093018,
      "learning_rate": 4.5300101729399796e-05,
      "loss": 3.4964,
      "step": 9240
    },
    {
      "epoch": 4.704984740590031,
      "grad_norm": 7.422082901000977,
      "learning_rate": 4.529501525940997e-05,
      "loss": 3.5127,
      "step": 9250
    },
    {
      "epoch": 4.710071210579858,
      "grad_norm": 7.308662414550781,
      "learning_rate": 4.528992878942014e-05,
      "loss": 3.4028,
      "step": 9260
    },
    {
      "epoch": 4.715157680569685,
      "grad_norm": 9.710973739624023,
      "learning_rate": 4.528484231943031e-05,
      "loss": 3.4357,
      "step": 9270
    },
    {
      "epoch": 4.720244150559512,
      "grad_norm": 9.333261489868164,
      "learning_rate": 4.527975584944049e-05,
      "loss": 3.4566,
      "step": 9280
    },
    {
      "epoch": 4.725330620549339,
      "grad_norm": 7.892265319824219,
      "learning_rate": 4.5274669379450666e-05,
      "loss": 3.4917,
      "step": 9290
    },
    {
      "epoch": 4.730417090539166,
      "grad_norm": 6.595106601715088,
      "learning_rate": 4.5269582909460836e-05,
      "loss": 3.4522,
      "step": 9300
    },
    {
      "epoch": 4.735503560528993,
      "grad_norm": 6.381352424621582,
      "learning_rate": 4.526449643947101e-05,
      "loss": 3.4132,
      "step": 9310
    },
    {
      "epoch": 4.7405900305188196,
      "grad_norm": 8.090741157531738,
      "learning_rate": 4.525940996948118e-05,
      "loss": 3.4253,
      "step": 9320
    },
    {
      "epoch": 4.745676500508647,
      "grad_norm": 6.785665512084961,
      "learning_rate": 4.525432349949136e-05,
      "loss": 3.3968,
      "step": 9330
    },
    {
      "epoch": 4.750762970498474,
      "grad_norm": 6.873204708099365,
      "learning_rate": 4.524923702950153e-05,
      "loss": 3.3375,
      "step": 9340
    },
    {
      "epoch": 4.755849440488301,
      "grad_norm": 9.545615196228027,
      "learning_rate": 4.52441505595117e-05,
      "loss": 3.5353,
      "step": 9350
    },
    {
      "epoch": 4.760935910478128,
      "grad_norm": 10.11694049835205,
      "learning_rate": 4.5239064089521876e-05,
      "loss": 3.4221,
      "step": 9360
    },
    {
      "epoch": 4.766022380467955,
      "grad_norm": 11.527960777282715,
      "learning_rate": 4.5233977619532045e-05,
      "loss": 3.3752,
      "step": 9370
    },
    {
      "epoch": 4.771108850457782,
      "grad_norm": 7.576038360595703,
      "learning_rate": 4.5228891149542215e-05,
      "loss": 3.5052,
      "step": 9380
    },
    {
      "epoch": 4.776195320447609,
      "grad_norm": 7.101188659667969,
      "learning_rate": 4.522380467955239e-05,
      "loss": 3.4659,
      "step": 9390
    },
    {
      "epoch": 4.781281790437436,
      "grad_norm": 7.593052864074707,
      "learning_rate": 4.521871820956257e-05,
      "loss": 3.3832,
      "step": 9400
    },
    {
      "epoch": 4.786368260427263,
      "grad_norm": 9.175793647766113,
      "learning_rate": 4.521363173957274e-05,
      "loss": 3.4087,
      "step": 9410
    },
    {
      "epoch": 4.79145473041709,
      "grad_norm": 10.864426612854004,
      "learning_rate": 4.5208545269582915e-05,
      "loss": 3.4016,
      "step": 9420
    },
    {
      "epoch": 4.796541200406917,
      "grad_norm": 9.779570579528809,
      "learning_rate": 4.5203458799593085e-05,
      "loss": 3.4035,
      "step": 9430
    },
    {
      "epoch": 4.801627670396744,
      "grad_norm": 6.005212306976318,
      "learning_rate": 4.5198372329603255e-05,
      "loss": 3.38,
      "step": 9440
    },
    {
      "epoch": 4.806714140386572,
      "grad_norm": 9.447188377380371,
      "learning_rate": 4.519328585961343e-05,
      "loss": 3.445,
      "step": 9450
    },
    {
      "epoch": 4.811800610376399,
      "grad_norm": 7.931705474853516,
      "learning_rate": 4.51881993896236e-05,
      "loss": 3.4025,
      "step": 9460
    },
    {
      "epoch": 4.816887080366226,
      "grad_norm": 11.030824661254883,
      "learning_rate": 4.518311291963377e-05,
      "loss": 3.3486,
      "step": 9470
    },
    {
      "epoch": 4.821973550356053,
      "grad_norm": 7.932216644287109,
      "learning_rate": 4.517802644964395e-05,
      "loss": 3.4224,
      "step": 9480
    },
    {
      "epoch": 4.82706002034588,
      "grad_norm": 7.7786335945129395,
      "learning_rate": 4.5172939979654125e-05,
      "loss": 3.4066,
      "step": 9490
    },
    {
      "epoch": 4.832146490335707,
      "grad_norm": 6.774055480957031,
      "learning_rate": 4.5167853509664295e-05,
      "loss": 3.4878,
      "step": 9500
    },
    {
      "epoch": 4.837232960325534,
      "grad_norm": 6.631634712219238,
      "learning_rate": 4.516276703967447e-05,
      "loss": 3.4049,
      "step": 9510
    },
    {
      "epoch": 4.842319430315361,
      "grad_norm": 8.098694801330566,
      "learning_rate": 4.515768056968464e-05,
      "loss": 3.4764,
      "step": 9520
    },
    {
      "epoch": 4.847405900305188,
      "grad_norm": 5.958906173706055,
      "learning_rate": 4.515259409969481e-05,
      "loss": 3.4607,
      "step": 9530
    },
    {
      "epoch": 4.852492370295015,
      "grad_norm": 9.672557830810547,
      "learning_rate": 4.514750762970499e-05,
      "loss": 3.3551,
      "step": 9540
    },
    {
      "epoch": 4.857578840284843,
      "grad_norm": 9.587404251098633,
      "learning_rate": 4.514242115971516e-05,
      "loss": 3.4037,
      "step": 9550
    },
    {
      "epoch": 4.86266531027467,
      "grad_norm": 9.31063175201416,
      "learning_rate": 4.513733468972533e-05,
      "loss": 3.4018,
      "step": 9560
    },
    {
      "epoch": 4.867751780264497,
      "grad_norm": 7.712965488433838,
      "learning_rate": 4.5132248219735504e-05,
      "loss": 3.4885,
      "step": 9570
    },
    {
      "epoch": 4.872838250254324,
      "grad_norm": 7.883378982543945,
      "learning_rate": 4.512716174974568e-05,
      "loss": 3.3177,
      "step": 9580
    },
    {
      "epoch": 4.877924720244151,
      "grad_norm": 9.153311729431152,
      "learning_rate": 4.512207527975586e-05,
      "loss": 3.4079,
      "step": 9590
    },
    {
      "epoch": 4.883011190233978,
      "grad_norm": 11.107531547546387,
      "learning_rate": 4.511698880976603e-05,
      "loss": 3.4774,
      "step": 9600
    },
    {
      "epoch": 4.888097660223805,
      "grad_norm": 7.391555309295654,
      "learning_rate": 4.51119023397762e-05,
      "loss": 3.4974,
      "step": 9610
    },
    {
      "epoch": 4.893184130213632,
      "grad_norm": 7.610074520111084,
      "learning_rate": 4.5106815869786374e-05,
      "loss": 3.4145,
      "step": 9620
    },
    {
      "epoch": 4.898270600203459,
      "grad_norm": 6.261350631713867,
      "learning_rate": 4.5101729399796544e-05,
      "loss": 3.4093,
      "step": 9630
    },
    {
      "epoch": 4.903357070193286,
      "grad_norm": 9.30160903930664,
      "learning_rate": 4.5096642929806714e-05,
      "loss": 3.4554,
      "step": 9640
    },
    {
      "epoch": 4.908443540183113,
      "grad_norm": 8.21364974975586,
      "learning_rate": 4.509155645981689e-05,
      "loss": 3.4233,
      "step": 9650
    },
    {
      "epoch": 4.9135300101729396,
      "grad_norm": 6.8808512687683105,
      "learning_rate": 4.508646998982706e-05,
      "loss": 3.3496,
      "step": 9660
    },
    {
      "epoch": 4.918616480162767,
      "grad_norm": 8.322468757629395,
      "learning_rate": 4.508138351983723e-05,
      "loss": 3.509,
      "step": 9670
    },
    {
      "epoch": 4.923702950152594,
      "grad_norm": 10.318218231201172,
      "learning_rate": 4.507629704984741e-05,
      "loss": 3.3569,
      "step": 9680
    },
    {
      "epoch": 4.928789420142421,
      "grad_norm": 7.555917739868164,
      "learning_rate": 4.5071210579857584e-05,
      "loss": 3.4327,
      "step": 9690
    },
    {
      "epoch": 4.933875890132248,
      "grad_norm": 9.784575462341309,
      "learning_rate": 4.5066124109867754e-05,
      "loss": 3.4172,
      "step": 9700
    },
    {
      "epoch": 4.938962360122075,
      "grad_norm": 8.84133243560791,
      "learning_rate": 4.506103763987793e-05,
      "loss": 3.3369,
      "step": 9710
    },
    {
      "epoch": 4.944048830111902,
      "grad_norm": 6.447965145111084,
      "learning_rate": 4.50559511698881e-05,
      "loss": 3.4442,
      "step": 9720
    },
    {
      "epoch": 4.949135300101729,
      "grad_norm": 6.56115198135376,
      "learning_rate": 4.505086469989827e-05,
      "loss": 3.4005,
      "step": 9730
    },
    {
      "epoch": 4.954221770091556,
      "grad_norm": 6.356902122497559,
      "learning_rate": 4.504577822990845e-05,
      "loss": 3.3982,
      "step": 9740
    },
    {
      "epoch": 4.959308240081383,
      "grad_norm": 7.716780662536621,
      "learning_rate": 4.5040691759918617e-05,
      "loss": 3.3916,
      "step": 9750
    },
    {
      "epoch": 4.96439471007121,
      "grad_norm": 6.3376007080078125,
      "learning_rate": 4.5035605289928786e-05,
      "loss": 3.4535,
      "step": 9760
    },
    {
      "epoch": 4.969481180061038,
      "grad_norm": 7.64890718460083,
      "learning_rate": 4.503051881993896e-05,
      "loss": 3.4119,
      "step": 9770
    },
    {
      "epoch": 4.974567650050865,
      "grad_norm": 10.527449607849121,
      "learning_rate": 4.502543234994914e-05,
      "loss": 3.2871,
      "step": 9780
    },
    {
      "epoch": 4.979654120040692,
      "grad_norm": 9.034046173095703,
      "learning_rate": 4.502034587995931e-05,
      "loss": 3.4036,
      "step": 9790
    },
    {
      "epoch": 4.984740590030519,
      "grad_norm": 7.88875675201416,
      "learning_rate": 4.5015259409969486e-05,
      "loss": 3.3779,
      "step": 9800
    },
    {
      "epoch": 4.989827060020346,
      "grad_norm": 8.219017028808594,
      "learning_rate": 4.5010172939979656e-05,
      "loss": 3.4746,
      "step": 9810
    },
    {
      "epoch": 4.994913530010173,
      "grad_norm": 8.629941940307617,
      "learning_rate": 4.5005086469989826e-05,
      "loss": 3.4406,
      "step": 9820
    },
    {
      "epoch": 5.0,
      "grad_norm": 10.693283081054688,
      "learning_rate": 4.5e-05,
      "loss": 3.4115,
      "step": 9830
    },
    {
      "epoch": 5.0,
      "eval_loss": 3.661724090576172,
      "eval_runtime": 2.6905,
      "eval_samples_per_second": 1031.412,
      "eval_steps_per_second": 128.973,
      "step": 9830
    },
    {
      "epoch": 5.005086469989827,
      "grad_norm": 8.713690757751465,
      "learning_rate": 4.499491353001017e-05,
      "loss": 3.3567,
      "step": 9840
    },
    {
      "epoch": 5.010172939979654,
      "grad_norm": 9.43446159362793,
      "learning_rate": 4.498982706002034e-05,
      "loss": 3.4551,
      "step": 9850
    },
    {
      "epoch": 5.015259409969481,
      "grad_norm": 8.01353645324707,
      "learning_rate": 4.498474059003052e-05,
      "loss": 3.367,
      "step": 9860
    },
    {
      "epoch": 5.020345879959308,
      "grad_norm": 6.540533542633057,
      "learning_rate": 4.4979654120040696e-05,
      "loss": 3.3769,
      "step": 9870
    },
    {
      "epoch": 5.025432349949135,
      "grad_norm": 7.901946544647217,
      "learning_rate": 4.497456765005087e-05,
      "loss": 3.3792,
      "step": 9880
    },
    {
      "epoch": 5.030518819938963,
      "grad_norm": 7.830153465270996,
      "learning_rate": 4.496948118006104e-05,
      "loss": 3.4682,
      "step": 9890
    },
    {
      "epoch": 5.03560528992879,
      "grad_norm": 12.979973793029785,
      "learning_rate": 4.496439471007121e-05,
      "loss": 3.3579,
      "step": 9900
    },
    {
      "epoch": 5.040691759918617,
      "grad_norm": 10.153520584106445,
      "learning_rate": 4.495930824008139e-05,
      "loss": 3.3168,
      "step": 9910
    },
    {
      "epoch": 5.045778229908444,
      "grad_norm": 6.942941188812256,
      "learning_rate": 4.495422177009156e-05,
      "loss": 3.4051,
      "step": 9920
    },
    {
      "epoch": 5.050864699898271,
      "grad_norm": 8.764078140258789,
      "learning_rate": 4.494913530010173e-05,
      "loss": 3.4106,
      "step": 9930
    },
    {
      "epoch": 5.055951169888098,
      "grad_norm": 9.122148513793945,
      "learning_rate": 4.4944048830111906e-05,
      "loss": 3.4033,
      "step": 9940
    },
    {
      "epoch": 5.061037639877925,
      "grad_norm": 10.862384796142578,
      "learning_rate": 4.4938962360122075e-05,
      "loss": 3.3227,
      "step": 9950
    },
    {
      "epoch": 5.066124109867752,
      "grad_norm": 10.244147300720215,
      "learning_rate": 4.493387589013225e-05,
      "loss": 3.4371,
      "step": 9960
    },
    {
      "epoch": 5.071210579857579,
      "grad_norm": 8.090278625488281,
      "learning_rate": 4.492878942014242e-05,
      "loss": 3.3857,
      "step": 9970
    },
    {
      "epoch": 5.076297049847406,
      "grad_norm": 7.132679462432861,
      "learning_rate": 4.49237029501526e-05,
      "loss": 3.3866,
      "step": 9980
    },
    {
      "epoch": 5.081383519837233,
      "grad_norm": 5.883156776428223,
      "learning_rate": 4.491861648016277e-05,
      "loss": 3.4224,
      "step": 9990
    },
    {
      "epoch": 5.0864699898270604,
      "grad_norm": 9.080748558044434,
      "learning_rate": 4.4913530010172945e-05,
      "loss": 3.3937,
      "step": 10000
    },
    {
      "epoch": 5.091556459816887,
      "grad_norm": 9.243337631225586,
      "learning_rate": 4.4908443540183115e-05,
      "loss": 3.454,
      "step": 10010
    },
    {
      "epoch": 5.096642929806714,
      "grad_norm": 7.22979211807251,
      "learning_rate": 4.4903357070193285e-05,
      "loss": 3.3609,
      "step": 10020
    },
    {
      "epoch": 5.101729399796541,
      "grad_norm": 7.994319438934326,
      "learning_rate": 4.489827060020346e-05,
      "loss": 3.3368,
      "step": 10030
    },
    {
      "epoch": 5.106815869786368,
      "grad_norm": 8.279252052307129,
      "learning_rate": 4.489318413021363e-05,
      "loss": 3.4099,
      "step": 10040
    },
    {
      "epoch": 5.111902339776195,
      "grad_norm": 7.638004779815674,
      "learning_rate": 4.48880976602238e-05,
      "loss": 3.4506,
      "step": 10050
    },
    {
      "epoch": 5.116988809766022,
      "grad_norm": 9.001969337463379,
      "learning_rate": 4.488301119023398e-05,
      "loss": 3.3768,
      "step": 10060
    },
    {
      "epoch": 5.122075279755849,
      "grad_norm": 7.916812419891357,
      "learning_rate": 4.4877924720244155e-05,
      "loss": 3.4134,
      "step": 10070
    },
    {
      "epoch": 5.127161749745676,
      "grad_norm": 8.516066551208496,
      "learning_rate": 4.4872838250254325e-05,
      "loss": 3.3901,
      "step": 10080
    },
    {
      "epoch": 5.132248219735503,
      "grad_norm": 9.184781074523926,
      "learning_rate": 4.48677517802645e-05,
      "loss": 3.4362,
      "step": 10090
    },
    {
      "epoch": 5.13733468972533,
      "grad_norm": 7.7425737380981445,
      "learning_rate": 4.486266531027467e-05,
      "loss": 3.3927,
      "step": 10100
    },
    {
      "epoch": 5.142421159715157,
      "grad_norm": 8.394432067871094,
      "learning_rate": 4.485757884028484e-05,
      "loss": 3.411,
      "step": 10110
    },
    {
      "epoch": 5.147507629704985,
      "grad_norm": 6.657287120819092,
      "learning_rate": 4.485249237029502e-05,
      "loss": 3.3021,
      "step": 10120
    },
    {
      "epoch": 5.152594099694812,
      "grad_norm": 7.865348815917969,
      "learning_rate": 4.484740590030519e-05,
      "loss": 3.3083,
      "step": 10130
    },
    {
      "epoch": 5.157680569684639,
      "grad_norm": 6.853671073913574,
      "learning_rate": 4.4842319430315364e-05,
      "loss": 3.3371,
      "step": 10140
    },
    {
      "epoch": 5.162767039674466,
      "grad_norm": 7.6596455574035645,
      "learning_rate": 4.4837232960325534e-05,
      "loss": 3.3637,
      "step": 10150
    },
    {
      "epoch": 5.167853509664293,
      "grad_norm": 8.701679229736328,
      "learning_rate": 4.483214649033571e-05,
      "loss": 3.3786,
      "step": 10160
    },
    {
      "epoch": 5.17293997965412,
      "grad_norm": 8.074728965759277,
      "learning_rate": 4.482706002034589e-05,
      "loss": 3.4009,
      "step": 10170
    },
    {
      "epoch": 5.178026449643947,
      "grad_norm": 8.530977249145508,
      "learning_rate": 4.482197355035606e-05,
      "loss": 3.3302,
      "step": 10180
    },
    {
      "epoch": 5.183112919633774,
      "grad_norm": 8.249604225158691,
      "learning_rate": 4.481688708036623e-05,
      "loss": 3.4108,
      "step": 10190
    },
    {
      "epoch": 5.188199389623601,
      "grad_norm": 9.414680480957031,
      "learning_rate": 4.4811800610376404e-05,
      "loss": 3.3209,
      "step": 10200
    },
    {
      "epoch": 5.193285859613428,
      "grad_norm": 10.902302742004395,
      "learning_rate": 4.4806714140386574e-05,
      "loss": 3.4182,
      "step": 10210
    },
    {
      "epoch": 5.198372329603256,
      "grad_norm": 11.286271095275879,
      "learning_rate": 4.4801627670396744e-05,
      "loss": 3.3084,
      "step": 10220
    },
    {
      "epoch": 5.203458799593083,
      "grad_norm": 10.430606842041016,
      "learning_rate": 4.479654120040692e-05,
      "loss": 3.3516,
      "step": 10230
    },
    {
      "epoch": 5.20854526958291,
      "grad_norm": 11.544814109802246,
      "learning_rate": 4.479145473041709e-05,
      "loss": 3.3677,
      "step": 10240
    },
    {
      "epoch": 5.213631739572737,
      "grad_norm": 11.880499839782715,
      "learning_rate": 4.478636826042727e-05,
      "loss": 3.3907,
      "step": 10250
    },
    {
      "epoch": 5.218718209562564,
      "grad_norm": 10.1436185836792,
      "learning_rate": 4.4781281790437444e-05,
      "loss": 3.3853,
      "step": 10260
    },
    {
      "epoch": 5.223804679552391,
      "grad_norm": 9.843362808227539,
      "learning_rate": 4.4776195320447614e-05,
      "loss": 3.3577,
      "step": 10270
    },
    {
      "epoch": 5.228891149542218,
      "grad_norm": 8.799412727355957,
      "learning_rate": 4.4771108850457784e-05,
      "loss": 3.4787,
      "step": 10280
    },
    {
      "epoch": 5.233977619532045,
      "grad_norm": 9.022526741027832,
      "learning_rate": 4.476602238046796e-05,
      "loss": 3.3859,
      "step": 10290
    },
    {
      "epoch": 5.239064089521872,
      "grad_norm": 8.957947731018066,
      "learning_rate": 4.476093591047813e-05,
      "loss": 3.3976,
      "step": 10300
    },
    {
      "epoch": 5.244150559511699,
      "grad_norm": 8.797022819519043,
      "learning_rate": 4.47558494404883e-05,
      "loss": 3.4001,
      "step": 10310
    },
    {
      "epoch": 5.249237029501526,
      "grad_norm": 8.072952270507812,
      "learning_rate": 4.475076297049848e-05,
      "loss": 3.4157,
      "step": 10320
    },
    {
      "epoch": 5.254323499491353,
      "grad_norm": 10.067475318908691,
      "learning_rate": 4.4745676500508647e-05,
      "loss": 3.3319,
      "step": 10330
    },
    {
      "epoch": 5.2594099694811804,
      "grad_norm": 7.069119930267334,
      "learning_rate": 4.4740590030518816e-05,
      "loss": 3.3853,
      "step": 10340
    },
    {
      "epoch": 5.264496439471007,
      "grad_norm": 9.937731742858887,
      "learning_rate": 4.473550356052899e-05,
      "loss": 3.3271,
      "step": 10350
    },
    {
      "epoch": 5.269582909460834,
      "grad_norm": 6.256550312042236,
      "learning_rate": 4.473041709053917e-05,
      "loss": 3.3476,
      "step": 10360
    },
    {
      "epoch": 5.274669379450661,
      "grad_norm": 8.040237426757812,
      "learning_rate": 4.472533062054934e-05,
      "loss": 3.3952,
      "step": 10370
    },
    {
      "epoch": 5.279755849440488,
      "grad_norm": 7.727762699127197,
      "learning_rate": 4.4720244150559516e-05,
      "loss": 3.4012,
      "step": 10380
    },
    {
      "epoch": 5.284842319430315,
      "grad_norm": 10.036988258361816,
      "learning_rate": 4.4715157680569686e-05,
      "loss": 3.3454,
      "step": 10390
    },
    {
      "epoch": 5.289928789420142,
      "grad_norm": 10.328217506408691,
      "learning_rate": 4.471007121057986e-05,
      "loss": 3.2787,
      "step": 10400
    },
    {
      "epoch": 5.295015259409969,
      "grad_norm": 9.366949081420898,
      "learning_rate": 4.470498474059003e-05,
      "loss": 3.4425,
      "step": 10410
    },
    {
      "epoch": 5.300101729399796,
      "grad_norm": 7.286594867706299,
      "learning_rate": 4.46998982706002e-05,
      "loss": 3.3952,
      "step": 10420
    },
    {
      "epoch": 5.305188199389623,
      "grad_norm": 7.556516170501709,
      "learning_rate": 4.469481180061038e-05,
      "loss": 3.4198,
      "step": 10430
    },
    {
      "epoch": 5.31027466937945,
      "grad_norm": 7.925772190093994,
      "learning_rate": 4.468972533062055e-05,
      "loss": 3.4135,
      "step": 10440
    },
    {
      "epoch": 5.315361139369278,
      "grad_norm": 9.027365684509277,
      "learning_rate": 4.4684638860630726e-05,
      "loss": 3.4831,
      "step": 10450
    },
    {
      "epoch": 5.320447609359105,
      "grad_norm": 9.765704154968262,
      "learning_rate": 4.46795523906409e-05,
      "loss": 3.4176,
      "step": 10460
    },
    {
      "epoch": 5.325534079348932,
      "grad_norm": 8.96181583404541,
      "learning_rate": 4.467446592065107e-05,
      "loss": 3.3573,
      "step": 10470
    },
    {
      "epoch": 5.330620549338759,
      "grad_norm": 6.161792278289795,
      "learning_rate": 4.466937945066124e-05,
      "loss": 3.2636,
      "step": 10480
    },
    {
      "epoch": 5.335707019328586,
      "grad_norm": 9.588943481445312,
      "learning_rate": 4.466429298067142e-05,
      "loss": 3.3311,
      "step": 10490
    },
    {
      "epoch": 5.340793489318413,
      "grad_norm": 6.478540897369385,
      "learning_rate": 4.465920651068159e-05,
      "loss": 3.2994,
      "step": 10500
    },
    {
      "epoch": 5.34587995930824,
      "grad_norm": 10.064840316772461,
      "learning_rate": 4.465412004069176e-05,
      "loss": 3.3418,
      "step": 10510
    },
    {
      "epoch": 5.350966429298067,
      "grad_norm": 8.378392219543457,
      "learning_rate": 4.4649033570701936e-05,
      "loss": 3.4545,
      "step": 10520
    },
    {
      "epoch": 5.356052899287894,
      "grad_norm": 11.477513313293457,
      "learning_rate": 4.4643947100712105e-05,
      "loss": 3.3481,
      "step": 10530
    },
    {
      "epoch": 5.361139369277721,
      "grad_norm": 8.817432403564453,
      "learning_rate": 4.463886063072228e-05,
      "loss": 3.4073,
      "step": 10540
    },
    {
      "epoch": 5.366225839267548,
      "grad_norm": 6.793882369995117,
      "learning_rate": 4.463377416073246e-05,
      "loss": 3.3836,
      "step": 10550
    },
    {
      "epoch": 5.371312309257376,
      "grad_norm": 10.672224998474121,
      "learning_rate": 4.462868769074263e-05,
      "loss": 3.3329,
      "step": 10560
    },
    {
      "epoch": 5.376398779247203,
      "grad_norm": 9.395589828491211,
      "learning_rate": 4.46236012207528e-05,
      "loss": 3.3458,
      "step": 10570
    },
    {
      "epoch": 5.38148524923703,
      "grad_norm": 10.1043119430542,
      "learning_rate": 4.4618514750762975e-05,
      "loss": 3.2803,
      "step": 10580
    },
    {
      "epoch": 5.386571719226857,
      "grad_norm": 9.046292304992676,
      "learning_rate": 4.4613428280773145e-05,
      "loss": 3.3648,
      "step": 10590
    },
    {
      "epoch": 5.391658189216684,
      "grad_norm": 7.7065653800964355,
      "learning_rate": 4.4608341810783315e-05,
      "loss": 3.416,
      "step": 10600
    },
    {
      "epoch": 5.396744659206511,
      "grad_norm": 7.565491676330566,
      "learning_rate": 4.460325534079349e-05,
      "loss": 3.3861,
      "step": 10610
    },
    {
      "epoch": 5.401831129196338,
      "grad_norm": 9.166823387145996,
      "learning_rate": 4.459816887080366e-05,
      "loss": 3.3158,
      "step": 10620
    },
    {
      "epoch": 5.406917599186165,
      "grad_norm": 8.307760238647461,
      "learning_rate": 4.459308240081383e-05,
      "loss": 3.3959,
      "step": 10630
    },
    {
      "epoch": 5.412004069175992,
      "grad_norm": 7.466367244720459,
      "learning_rate": 4.458799593082401e-05,
      "loss": 3.3988,
      "step": 10640
    },
    {
      "epoch": 5.417090539165819,
      "grad_norm": 9.6941556930542,
      "learning_rate": 4.4582909460834185e-05,
      "loss": 3.3354,
      "step": 10650
    },
    {
      "epoch": 5.422177009155646,
      "grad_norm": 7.696808338165283,
      "learning_rate": 4.4577822990844355e-05,
      "loss": 3.3803,
      "step": 10660
    },
    {
      "epoch": 5.4272634791454735,
      "grad_norm": 10.51921558380127,
      "learning_rate": 4.457273652085453e-05,
      "loss": 3.3332,
      "step": 10670
    },
    {
      "epoch": 5.4323499491353004,
      "grad_norm": 7.727941513061523,
      "learning_rate": 4.45676500508647e-05,
      "loss": 3.2982,
      "step": 10680
    },
    {
      "epoch": 5.437436419125127,
      "grad_norm": 8.127115249633789,
      "learning_rate": 4.456256358087488e-05,
      "loss": 3.419,
      "step": 10690
    },
    {
      "epoch": 5.442522889114954,
      "grad_norm": 8.31723690032959,
      "learning_rate": 4.455747711088505e-05,
      "loss": 3.3302,
      "step": 10700
    },
    {
      "epoch": 5.447609359104781,
      "grad_norm": 6.789867877960205,
      "learning_rate": 4.455239064089522e-05,
      "loss": 3.3278,
      "step": 10710
    },
    {
      "epoch": 5.452695829094608,
      "grad_norm": 8.108121871948242,
      "learning_rate": 4.4547304170905394e-05,
      "loss": 3.3845,
      "step": 10720
    },
    {
      "epoch": 5.457782299084435,
      "grad_norm": 11.251033782958984,
      "learning_rate": 4.4542217700915564e-05,
      "loss": 3.3138,
      "step": 10730
    },
    {
      "epoch": 5.462868769074262,
      "grad_norm": 9.589756965637207,
      "learning_rate": 4.453713123092574e-05,
      "loss": 3.3205,
      "step": 10740
    },
    {
      "epoch": 5.467955239064089,
      "grad_norm": 8.040104866027832,
      "learning_rate": 4.453204476093592e-05,
      "loss": 3.3558,
      "step": 10750
    },
    {
      "epoch": 5.473041709053916,
      "grad_norm": 10.341806411743164,
      "learning_rate": 4.452695829094609e-05,
      "loss": 3.3474,
      "step": 10760
    },
    {
      "epoch": 5.478128179043743,
      "grad_norm": 9.915574073791504,
      "learning_rate": 4.452187182095626e-05,
      "loss": 3.4175,
      "step": 10770
    },
    {
      "epoch": 5.48321464903357,
      "grad_norm": 8.605934143066406,
      "learning_rate": 4.4516785350966434e-05,
      "loss": 3.3643,
      "step": 10780
    },
    {
      "epoch": 5.488301119023398,
      "grad_norm": 7.384767532348633,
      "learning_rate": 4.4511698880976604e-05,
      "loss": 3.4015,
      "step": 10790
    },
    {
      "epoch": 5.493387589013225,
      "grad_norm": 8.929535865783691,
      "learning_rate": 4.4506612410986774e-05,
      "loss": 3.4034,
      "step": 10800
    },
    {
      "epoch": 5.498474059003052,
      "grad_norm": 12.214426040649414,
      "learning_rate": 4.450152594099695e-05,
      "loss": 3.2972,
      "step": 10810
    },
    {
      "epoch": 5.503560528992879,
      "grad_norm": 8.479266166687012,
      "learning_rate": 4.449643947100712e-05,
      "loss": 3.3664,
      "step": 10820
    },
    {
      "epoch": 5.508646998982706,
      "grad_norm": 10.129794120788574,
      "learning_rate": 4.44913530010173e-05,
      "loss": 3.3409,
      "step": 10830
    },
    {
      "epoch": 5.513733468972533,
      "grad_norm": 10.696216583251953,
      "learning_rate": 4.4486266531027474e-05,
      "loss": 3.3247,
      "step": 10840
    },
    {
      "epoch": 5.51881993896236,
      "grad_norm": 7.036331653594971,
      "learning_rate": 4.4481180061037644e-05,
      "loss": 3.2461,
      "step": 10850
    },
    {
      "epoch": 5.523906408952187,
      "grad_norm": 8.595849990844727,
      "learning_rate": 4.4476093591047814e-05,
      "loss": 3.3705,
      "step": 10860
    },
    {
      "epoch": 5.528992878942014,
      "grad_norm": 8.81847095489502,
      "learning_rate": 4.447100712105799e-05,
      "loss": 3.3813,
      "step": 10870
    },
    {
      "epoch": 5.534079348931841,
      "grad_norm": 10.922606468200684,
      "learning_rate": 4.446592065106816e-05,
      "loss": 3.2739,
      "step": 10880
    },
    {
      "epoch": 5.539165818921669,
      "grad_norm": 8.586981773376465,
      "learning_rate": 4.446083418107833e-05,
      "loss": 3.3377,
      "step": 10890
    },
    {
      "epoch": 5.544252288911496,
      "grad_norm": 9.336477279663086,
      "learning_rate": 4.445574771108851e-05,
      "loss": 3.3135,
      "step": 10900
    },
    {
      "epoch": 5.549338758901323,
      "grad_norm": 11.866600036621094,
      "learning_rate": 4.4450661241098677e-05,
      "loss": 3.3214,
      "step": 10910
    },
    {
      "epoch": 5.55442522889115,
      "grad_norm": 10.178399085998535,
      "learning_rate": 4.444557477110885e-05,
      "loss": 3.3016,
      "step": 10920
    },
    {
      "epoch": 5.559511698880977,
      "grad_norm": 9.476956367492676,
      "learning_rate": 4.444048830111902e-05,
      "loss": 3.2604,
      "step": 10930
    },
    {
      "epoch": 5.564598168870804,
      "grad_norm": 6.972438812255859,
      "learning_rate": 4.44354018311292e-05,
      "loss": 3.2877,
      "step": 10940
    },
    {
      "epoch": 5.569684638860631,
      "grad_norm": 10.653018951416016,
      "learning_rate": 4.4430315361139376e-05,
      "loss": 3.3605,
      "step": 10950
    },
    {
      "epoch": 5.574771108850458,
      "grad_norm": 11.971806526184082,
      "learning_rate": 4.4425228891149546e-05,
      "loss": 3.3612,
      "step": 10960
    },
    {
      "epoch": 5.579857578840285,
      "grad_norm": 7.843995571136475,
      "learning_rate": 4.4420142421159716e-05,
      "loss": 3.3774,
      "step": 10970
    },
    {
      "epoch": 5.584944048830112,
      "grad_norm": 9.943848609924316,
      "learning_rate": 4.441505595116989e-05,
      "loss": 3.3472,
      "step": 10980
    },
    {
      "epoch": 5.590030518819939,
      "grad_norm": 14.10664176940918,
      "learning_rate": 4.440996948118006e-05,
      "loss": 3.3259,
      "step": 10990
    },
    {
      "epoch": 5.595116988809766,
      "grad_norm": 7.178832530975342,
      "learning_rate": 4.440488301119023e-05,
      "loss": 3.3622,
      "step": 11000
    },
    {
      "epoch": 5.6002034587995935,
      "grad_norm": 9.936934471130371,
      "learning_rate": 4.439979654120041e-05,
      "loss": 3.3481,
      "step": 11010
    },
    {
      "epoch": 5.6052899287894205,
      "grad_norm": 8.469064712524414,
      "learning_rate": 4.439471007121058e-05,
      "loss": 3.3023,
      "step": 11020
    },
    {
      "epoch": 5.610376398779247,
      "grad_norm": 8.676024436950684,
      "learning_rate": 4.4389623601220756e-05,
      "loss": 3.3629,
      "step": 11030
    },
    {
      "epoch": 5.615462868769074,
      "grad_norm": 7.189399242401123,
      "learning_rate": 4.438453713123093e-05,
      "loss": 3.4442,
      "step": 11040
    },
    {
      "epoch": 5.620549338758901,
      "grad_norm": 9.27315616607666,
      "learning_rate": 4.43794506612411e-05,
      "loss": 3.3294,
      "step": 11050
    },
    {
      "epoch": 5.625635808748728,
      "grad_norm": 8.393766403198242,
      "learning_rate": 4.437436419125127e-05,
      "loss": 3.3567,
      "step": 11060
    },
    {
      "epoch": 5.630722278738555,
      "grad_norm": 7.41504430770874,
      "learning_rate": 4.436927772126145e-05,
      "loss": 3.3956,
      "step": 11070
    },
    {
      "epoch": 5.635808748728382,
      "grad_norm": 9.979848861694336,
      "learning_rate": 4.436419125127162e-05,
      "loss": 3.2792,
      "step": 11080
    },
    {
      "epoch": 5.640895218718209,
      "grad_norm": 6.212418556213379,
      "learning_rate": 4.435910478128179e-05,
      "loss": 3.3702,
      "step": 11090
    },
    {
      "epoch": 5.645981688708036,
      "grad_norm": 10.287408828735352,
      "learning_rate": 4.4354018311291966e-05,
      "loss": 3.3723,
      "step": 11100
    },
    {
      "epoch": 5.651068158697864,
      "grad_norm": 8.051026344299316,
      "learning_rate": 4.4348931841302135e-05,
      "loss": 3.2955,
      "step": 11110
    },
    {
      "epoch": 5.656154628687691,
      "grad_norm": 7.832154750823975,
      "learning_rate": 4.434384537131231e-05,
      "loss": 3.3624,
      "step": 11120
    },
    {
      "epoch": 5.661241098677518,
      "grad_norm": 8.182462692260742,
      "learning_rate": 4.433875890132249e-05,
      "loss": 3.4099,
      "step": 11130
    },
    {
      "epoch": 5.666327568667345,
      "grad_norm": 12.512518882751465,
      "learning_rate": 4.433367243133266e-05,
      "loss": 3.3099,
      "step": 11140
    },
    {
      "epoch": 5.671414038657172,
      "grad_norm": 10.205362319946289,
      "learning_rate": 4.432858596134283e-05,
      "loss": 3.282,
      "step": 11150
    },
    {
      "epoch": 5.676500508646999,
      "grad_norm": 8.304862976074219,
      "learning_rate": 4.4323499491353005e-05,
      "loss": 3.3172,
      "step": 11160
    },
    {
      "epoch": 5.681586978636826,
      "grad_norm": 9.468347549438477,
      "learning_rate": 4.4318413021363175e-05,
      "loss": 3.3067,
      "step": 11170
    },
    {
      "epoch": 5.686673448626653,
      "grad_norm": 7.776686191558838,
      "learning_rate": 4.4313326551373345e-05,
      "loss": 3.3339,
      "step": 11180
    },
    {
      "epoch": 5.69175991861648,
      "grad_norm": 10.3855562210083,
      "learning_rate": 4.430824008138352e-05,
      "loss": 3.3381,
      "step": 11190
    },
    {
      "epoch": 5.696846388606307,
      "grad_norm": 9.17280101776123,
      "learning_rate": 4.430315361139369e-05,
      "loss": 3.3213,
      "step": 11200
    },
    {
      "epoch": 5.701932858596134,
      "grad_norm": 9.951476097106934,
      "learning_rate": 4.429806714140387e-05,
      "loss": 3.2647,
      "step": 11210
    },
    {
      "epoch": 5.707019328585961,
      "grad_norm": 9.065881729125977,
      "learning_rate": 4.4292980671414045e-05,
      "loss": 3.3519,
      "step": 11220
    },
    {
      "epoch": 5.712105798575788,
      "grad_norm": 9.589483261108398,
      "learning_rate": 4.4287894201424215e-05,
      "loss": 3.2593,
      "step": 11230
    },
    {
      "epoch": 5.717192268565616,
      "grad_norm": 10.739326477050781,
      "learning_rate": 4.428280773143439e-05,
      "loss": 3.3704,
      "step": 11240
    },
    {
      "epoch": 5.722278738555443,
      "grad_norm": 11.001755714416504,
      "learning_rate": 4.427772126144456e-05,
      "loss": 3.248,
      "step": 11250
    },
    {
      "epoch": 5.72736520854527,
      "grad_norm": 10.168893814086914,
      "learning_rate": 4.427263479145473e-05,
      "loss": 3.3879,
      "step": 11260
    },
    {
      "epoch": 5.732451678535097,
      "grad_norm": 11.36638355255127,
      "learning_rate": 4.426754832146491e-05,
      "loss": 3.2766,
      "step": 11270
    },
    {
      "epoch": 5.737538148524924,
      "grad_norm": 9.25913143157959,
      "learning_rate": 4.426246185147508e-05,
      "loss": 3.3521,
      "step": 11280
    },
    {
      "epoch": 5.742624618514751,
      "grad_norm": 11.19470500946045,
      "learning_rate": 4.425737538148525e-05,
      "loss": 3.3163,
      "step": 11290
    },
    {
      "epoch": 5.747711088504578,
      "grad_norm": 8.661599159240723,
      "learning_rate": 4.4252288911495424e-05,
      "loss": 3.33,
      "step": 11300
    },
    {
      "epoch": 5.752797558494405,
      "grad_norm": 11.455594062805176,
      "learning_rate": 4.4247202441505594e-05,
      "loss": 3.2608,
      "step": 11310
    },
    {
      "epoch": 5.757884028484232,
      "grad_norm": 13.132927894592285,
      "learning_rate": 4.424211597151577e-05,
      "loss": 3.358,
      "step": 11320
    },
    {
      "epoch": 5.762970498474059,
      "grad_norm": 10.351884841918945,
      "learning_rate": 4.423702950152595e-05,
      "loss": 3.3075,
      "step": 11330
    },
    {
      "epoch": 5.7680569684638865,
      "grad_norm": 7.469217777252197,
      "learning_rate": 4.423194303153612e-05,
      "loss": 3.3535,
      "step": 11340
    },
    {
      "epoch": 5.7731434384537135,
      "grad_norm": 8.530325889587402,
      "learning_rate": 4.422685656154629e-05,
      "loss": 3.2923,
      "step": 11350
    },
    {
      "epoch": 5.7782299084435405,
      "grad_norm": 8.323394775390625,
      "learning_rate": 4.4221770091556464e-05,
      "loss": 3.3881,
      "step": 11360
    },
    {
      "epoch": 5.783316378433367,
      "grad_norm": 9.261088371276855,
      "learning_rate": 4.4216683621566634e-05,
      "loss": 3.3316,
      "step": 11370
    },
    {
      "epoch": 5.788402848423194,
      "grad_norm": 8.136610984802246,
      "learning_rate": 4.4211597151576804e-05,
      "loss": 3.3056,
      "step": 11380
    },
    {
      "epoch": 5.793489318413021,
      "grad_norm": 10.949090957641602,
      "learning_rate": 4.420651068158698e-05,
      "loss": 3.2797,
      "step": 11390
    },
    {
      "epoch": 5.798575788402848,
      "grad_norm": 8.96713638305664,
      "learning_rate": 4.420142421159715e-05,
      "loss": 3.3156,
      "step": 11400
    },
    {
      "epoch": 5.803662258392675,
      "grad_norm": 10.787271499633789,
      "learning_rate": 4.419633774160733e-05,
      "loss": 3.2873,
      "step": 11410
    },
    {
      "epoch": 5.808748728382502,
      "grad_norm": 7.385814189910889,
      "learning_rate": 4.4191251271617504e-05,
      "loss": 3.2587,
      "step": 11420
    },
    {
      "epoch": 5.813835198372329,
      "grad_norm": 9.083758354187012,
      "learning_rate": 4.4186164801627674e-05,
      "loss": 3.3147,
      "step": 11430
    },
    {
      "epoch": 5.818921668362156,
      "grad_norm": 12.711030960083008,
      "learning_rate": 4.4181078331637844e-05,
      "loss": 3.3086,
      "step": 11440
    },
    {
      "epoch": 5.824008138351983,
      "grad_norm": 14.534794807434082,
      "learning_rate": 4.417599186164802e-05,
      "loss": 3.2666,
      "step": 11450
    },
    {
      "epoch": 5.829094608341811,
      "grad_norm": 9.501992225646973,
      "learning_rate": 4.417090539165819e-05,
      "loss": 3.3682,
      "step": 11460
    },
    {
      "epoch": 5.834181078331638,
      "grad_norm": 10.890825271606445,
      "learning_rate": 4.416581892166836e-05,
      "loss": 3.3062,
      "step": 11470
    },
    {
      "epoch": 5.839267548321465,
      "grad_norm": 8.138911247253418,
      "learning_rate": 4.416073245167854e-05,
      "loss": 3.2592,
      "step": 11480
    },
    {
      "epoch": 5.844354018311292,
      "grad_norm": 9.661840438842773,
      "learning_rate": 4.4155645981688707e-05,
      "loss": 3.3259,
      "step": 11490
    },
    {
      "epoch": 5.849440488301119,
      "grad_norm": 14.291275024414062,
      "learning_rate": 4.415055951169888e-05,
      "loss": 3.2726,
      "step": 11500
    },
    {
      "epoch": 5.854526958290946,
      "grad_norm": 11.070219039916992,
      "learning_rate": 4.414547304170906e-05,
      "loss": 3.2797,
      "step": 11510
    },
    {
      "epoch": 5.859613428280773,
      "grad_norm": 10.900617599487305,
      "learning_rate": 4.414038657171923e-05,
      "loss": 3.2692,
      "step": 11520
    },
    {
      "epoch": 5.8646998982706,
      "grad_norm": 11.795979499816895,
      "learning_rate": 4.4135300101729407e-05,
      "loss": 3.2836,
      "step": 11530
    },
    {
      "epoch": 5.869786368260427,
      "grad_norm": 8.795612335205078,
      "learning_rate": 4.4130213631739576e-05,
      "loss": 3.3342,
      "step": 11540
    },
    {
      "epoch": 5.874872838250254,
      "grad_norm": 7.593344211578369,
      "learning_rate": 4.4125127161749746e-05,
      "loss": 3.3271,
      "step": 11550
    },
    {
      "epoch": 5.879959308240082,
      "grad_norm": 9.538450241088867,
      "learning_rate": 4.412004069175992e-05,
      "loss": 3.3349,
      "step": 11560
    },
    {
      "epoch": 5.885045778229909,
      "grad_norm": 7.954007148742676,
      "learning_rate": 4.411495422177009e-05,
      "loss": 3.2953,
      "step": 11570
    },
    {
      "epoch": 5.890132248219736,
      "grad_norm": 11.583616256713867,
      "learning_rate": 4.410986775178026e-05,
      "loss": 3.336,
      "step": 11580
    },
    {
      "epoch": 5.895218718209563,
      "grad_norm": 9.837143898010254,
      "learning_rate": 4.410478128179044e-05,
      "loss": 3.3383,
      "step": 11590
    },
    {
      "epoch": 5.90030518819939,
      "grad_norm": 12.638315200805664,
      "learning_rate": 4.409969481180061e-05,
      "loss": 3.2655,
      "step": 11600
    },
    {
      "epoch": 5.905391658189217,
      "grad_norm": 9.379345893859863,
      "learning_rate": 4.4094608341810786e-05,
      "loss": 3.3039,
      "step": 11610
    },
    {
      "epoch": 5.910478128179044,
      "grad_norm": 9.075811386108398,
      "learning_rate": 4.408952187182096e-05,
      "loss": 3.2518,
      "step": 11620
    },
    {
      "epoch": 5.915564598168871,
      "grad_norm": 8.431933403015137,
      "learning_rate": 4.408443540183113e-05,
      "loss": 3.2116,
      "step": 11630
    },
    {
      "epoch": 5.920651068158698,
      "grad_norm": 11.366869926452637,
      "learning_rate": 4.40793489318413e-05,
      "loss": 3.1926,
      "step": 11640
    },
    {
      "epoch": 5.925737538148525,
      "grad_norm": 15.916213035583496,
      "learning_rate": 4.407426246185148e-05,
      "loss": 3.2732,
      "step": 11650
    },
    {
      "epoch": 5.930824008138352,
      "grad_norm": 7.625041484832764,
      "learning_rate": 4.406917599186165e-05,
      "loss": 3.2034,
      "step": 11660
    },
    {
      "epoch": 5.935910478128179,
      "grad_norm": 9.71884536743164,
      "learning_rate": 4.406408952187182e-05,
      "loss": 3.3009,
      "step": 11670
    },
    {
      "epoch": 5.940996948118006,
      "grad_norm": 8.393194198608398,
      "learning_rate": 4.4059003051881996e-05,
      "loss": 3.3858,
      "step": 11680
    },
    {
      "epoch": 5.9460834181078335,
      "grad_norm": 8.3996000289917,
      "learning_rate": 4.4053916581892165e-05,
      "loss": 3.3133,
      "step": 11690
    },
    {
      "epoch": 5.9511698880976605,
      "grad_norm": 10.36633014678955,
      "learning_rate": 4.404883011190234e-05,
      "loss": 3.308,
      "step": 11700
    },
    {
      "epoch": 5.956256358087487,
      "grad_norm": 10.311779975891113,
      "learning_rate": 4.404374364191252e-05,
      "loss": 3.3094,
      "step": 11710
    },
    {
      "epoch": 5.961342828077314,
      "grad_norm": 9.835495948791504,
      "learning_rate": 4.403865717192269e-05,
      "loss": 3.3277,
      "step": 11720
    },
    {
      "epoch": 5.966429298067141,
      "grad_norm": 11.286917686462402,
      "learning_rate": 4.403357070193286e-05,
      "loss": 3.2753,
      "step": 11730
    },
    {
      "epoch": 5.971515768056968,
      "grad_norm": 9.897188186645508,
      "learning_rate": 4.4028484231943035e-05,
      "loss": 3.2703,
      "step": 11740
    },
    {
      "epoch": 5.976602238046795,
      "grad_norm": 8.362552642822266,
      "learning_rate": 4.4023397761953205e-05,
      "loss": 3.3437,
      "step": 11750
    },
    {
      "epoch": 5.981688708036622,
      "grad_norm": 8.75931453704834,
      "learning_rate": 4.401831129196338e-05,
      "loss": 3.2791,
      "step": 11760
    },
    {
      "epoch": 5.986775178026449,
      "grad_norm": 9.499463081359863,
      "learning_rate": 4.401322482197355e-05,
      "loss": 3.2209,
      "step": 11770
    },
    {
      "epoch": 5.991861648016277,
      "grad_norm": 9.673983573913574,
      "learning_rate": 4.400813835198372e-05,
      "loss": 3.2796,
      "step": 11780
    },
    {
      "epoch": 5.996948118006104,
      "grad_norm": 7.451428413391113,
      "learning_rate": 4.40030518819939e-05,
      "loss": 3.266,
      "step": 11790
    },
    {
      "epoch": 6.0,
      "eval_loss": 3.6665639877319336,
      "eval_runtime": 2.6754,
      "eval_samples_per_second": 1037.212,
      "eval_steps_per_second": 129.698,
      "step": 11796
    },
    {
      "epoch": 6.002034587995931,
      "grad_norm": 8.92768669128418,
      "learning_rate": 4.3997965412004075e-05,
      "loss": 3.3084,
      "step": 11800
    },
    {
      "epoch": 6.007121057985758,
      "grad_norm": 11.104836463928223,
      "learning_rate": 4.3992878942014245e-05,
      "loss": 3.3312,
      "step": 11810
    },
    {
      "epoch": 6.012207527975585,
      "grad_norm": 12.759325981140137,
      "learning_rate": 4.398779247202442e-05,
      "loss": 3.2813,
      "step": 11820
    },
    {
      "epoch": 6.017293997965412,
      "grad_norm": 8.804829597473145,
      "learning_rate": 4.398270600203459e-05,
      "loss": 3.2912,
      "step": 11830
    },
    {
      "epoch": 6.022380467955239,
      "grad_norm": 15.439751625061035,
      "learning_rate": 4.397761953204476e-05,
      "loss": 3.2942,
      "step": 11840
    },
    {
      "epoch": 6.027466937945066,
      "grad_norm": 13.220057487487793,
      "learning_rate": 4.397253306205494e-05,
      "loss": 3.222,
      "step": 11850
    },
    {
      "epoch": 6.032553407934893,
      "grad_norm": 12.170092582702637,
      "learning_rate": 4.396744659206511e-05,
      "loss": 3.2893,
      "step": 11860
    },
    {
      "epoch": 6.03763987792472,
      "grad_norm": 12.46107006072998,
      "learning_rate": 4.396236012207528e-05,
      "loss": 3.271,
      "step": 11870
    },
    {
      "epoch": 6.042726347914547,
      "grad_norm": 11.578186988830566,
      "learning_rate": 4.3957273652085454e-05,
      "loss": 3.3021,
      "step": 11880
    },
    {
      "epoch": 6.047812817904374,
      "grad_norm": 12.344486236572266,
      "learning_rate": 4.3952187182095624e-05,
      "loss": 3.2935,
      "step": 11890
    },
    {
      "epoch": 6.052899287894202,
      "grad_norm": 8.262415885925293,
      "learning_rate": 4.39471007121058e-05,
      "loss": 3.2814,
      "step": 11900
    },
    {
      "epoch": 6.057985757884029,
      "grad_norm": 9.596832275390625,
      "learning_rate": 4.394201424211598e-05,
      "loss": 3.2428,
      "step": 11910
    },
    {
      "epoch": 6.063072227873856,
      "grad_norm": 13.289778709411621,
      "learning_rate": 4.393692777212615e-05,
      "loss": 3.317,
      "step": 11920
    },
    {
      "epoch": 6.068158697863683,
      "grad_norm": 9.876460075378418,
      "learning_rate": 4.393184130213632e-05,
      "loss": 3.2512,
      "step": 11930
    },
    {
      "epoch": 6.07324516785351,
      "grad_norm": 8.781933784484863,
      "learning_rate": 4.3926754832146494e-05,
      "loss": 3.2863,
      "step": 11940
    },
    {
      "epoch": 6.078331637843337,
      "grad_norm": 10.647642135620117,
      "learning_rate": 4.3921668362156664e-05,
      "loss": 3.2777,
      "step": 11950
    },
    {
      "epoch": 6.083418107833164,
      "grad_norm": 10.620988845825195,
      "learning_rate": 4.3916581892166834e-05,
      "loss": 3.2082,
      "step": 11960
    },
    {
      "epoch": 6.088504577822991,
      "grad_norm": 10.659385681152344,
      "learning_rate": 4.391149542217701e-05,
      "loss": 3.237,
      "step": 11970
    },
    {
      "epoch": 6.093591047812818,
      "grad_norm": 12.371952056884766,
      "learning_rate": 4.390640895218718e-05,
      "loss": 3.2784,
      "step": 11980
    },
    {
      "epoch": 6.098677517802645,
      "grad_norm": 12.78773307800293,
      "learning_rate": 4.390132248219736e-05,
      "loss": 3.3085,
      "step": 11990
    },
    {
      "epoch": 6.103763987792472,
      "grad_norm": 9.820123672485352,
      "learning_rate": 4.3896236012207534e-05,
      "loss": 3.2844,
      "step": 12000
    },
    {
      "epoch": 6.1088504577822995,
      "grad_norm": 10.491037368774414,
      "learning_rate": 4.3891149542217704e-05,
      "loss": 3.2636,
      "step": 12010
    },
    {
      "epoch": 6.1139369277721265,
      "grad_norm": 8.540008544921875,
      "learning_rate": 4.388606307222788e-05,
      "loss": 3.2791,
      "step": 12020
    },
    {
      "epoch": 6.1190233977619535,
      "grad_norm": 9.657414436340332,
      "learning_rate": 4.388097660223805e-05,
      "loss": 3.3095,
      "step": 12030
    },
    {
      "epoch": 6.1241098677517805,
      "grad_norm": 14.037581443786621,
      "learning_rate": 4.387589013224822e-05,
      "loss": 3.3076,
      "step": 12040
    },
    {
      "epoch": 6.129196337741607,
      "grad_norm": 11.339468002319336,
      "learning_rate": 4.38708036622584e-05,
      "loss": 3.3389,
      "step": 12050
    },
    {
      "epoch": 6.134282807731434,
      "grad_norm": 11.757417678833008,
      "learning_rate": 4.386571719226857e-05,
      "loss": 3.2416,
      "step": 12060
    },
    {
      "epoch": 6.139369277721261,
      "grad_norm": 9.51999282836914,
      "learning_rate": 4.3860630722278737e-05,
      "loss": 3.3082,
      "step": 12070
    },
    {
      "epoch": 6.144455747711088,
      "grad_norm": 8.990324020385742,
      "learning_rate": 4.385554425228891e-05,
      "loss": 3.1877,
      "step": 12080
    },
    {
      "epoch": 6.149542217700915,
      "grad_norm": 12.7012300491333,
      "learning_rate": 4.385045778229909e-05,
      "loss": 3.2471,
      "step": 12090
    },
    {
      "epoch": 6.154628687690742,
      "grad_norm": 10.557493209838867,
      "learning_rate": 4.384537131230926e-05,
      "loss": 3.2291,
      "step": 12100
    },
    {
      "epoch": 6.159715157680569,
      "grad_norm": 9.388824462890625,
      "learning_rate": 4.3840284842319437e-05,
      "loss": 3.1833,
      "step": 12110
    },
    {
      "epoch": 6.164801627670396,
      "grad_norm": 9.015096664428711,
      "learning_rate": 4.3835198372329606e-05,
      "loss": 3.1978,
      "step": 12120
    },
    {
      "epoch": 6.169888097660224,
      "grad_norm": 10.556726455688477,
      "learning_rate": 4.3830111902339776e-05,
      "loss": 3.2429,
      "step": 12130
    },
    {
      "epoch": 6.174974567650051,
      "grad_norm": 11.089271545410156,
      "learning_rate": 4.382502543234995e-05,
      "loss": 3.2679,
      "step": 12140
    },
    {
      "epoch": 6.180061037639878,
      "grad_norm": 12.755763053894043,
      "learning_rate": 4.381993896236012e-05,
      "loss": 3.3139,
      "step": 12150
    },
    {
      "epoch": 6.185147507629705,
      "grad_norm": 13.850837707519531,
      "learning_rate": 4.381485249237029e-05,
      "loss": 3.2934,
      "step": 12160
    },
    {
      "epoch": 6.190233977619532,
      "grad_norm": 10.467589378356934,
      "learning_rate": 4.380976602238047e-05,
      "loss": 3.2907,
      "step": 12170
    },
    {
      "epoch": 6.195320447609359,
      "grad_norm": 11.08360767364502,
      "learning_rate": 4.3804679552390646e-05,
      "loss": 3.1974,
      "step": 12180
    },
    {
      "epoch": 6.200406917599186,
      "grad_norm": 6.810572147369385,
      "learning_rate": 4.3799593082400816e-05,
      "loss": 3.287,
      "step": 12190
    },
    {
      "epoch": 6.205493387589013,
      "grad_norm": 10.481377601623535,
      "learning_rate": 4.379450661241099e-05,
      "loss": 3.2086,
      "step": 12200
    },
    {
      "epoch": 6.21057985757884,
      "grad_norm": 10.150228500366211,
      "learning_rate": 4.378942014242116e-05,
      "loss": 3.3225,
      "step": 12210
    },
    {
      "epoch": 6.215666327568667,
      "grad_norm": 9.468871116638184,
      "learning_rate": 4.378433367243133e-05,
      "loss": 3.3584,
      "step": 12220
    },
    {
      "epoch": 6.220752797558495,
      "grad_norm": 12.20047378540039,
      "learning_rate": 4.377924720244151e-05,
      "loss": 3.2903,
      "step": 12230
    },
    {
      "epoch": 6.225839267548322,
      "grad_norm": 11.462540626525879,
      "learning_rate": 4.377416073245168e-05,
      "loss": 3.2592,
      "step": 12240
    },
    {
      "epoch": 6.230925737538149,
      "grad_norm": 11.656315803527832,
      "learning_rate": 4.376907426246185e-05,
      "loss": 3.3149,
      "step": 12250
    },
    {
      "epoch": 6.236012207527976,
      "grad_norm": 10.437939643859863,
      "learning_rate": 4.3763987792472026e-05,
      "loss": 3.2424,
      "step": 12260
    },
    {
      "epoch": 6.241098677517803,
      "grad_norm": 14.722661018371582,
      "learning_rate": 4.3758901322482195e-05,
      "loss": 3.2586,
      "step": 12270
    },
    {
      "epoch": 6.24618514750763,
      "grad_norm": 8.842120170593262,
      "learning_rate": 4.375381485249237e-05,
      "loss": 3.1785,
      "step": 12280
    },
    {
      "epoch": 6.251271617497457,
      "grad_norm": 16.828960418701172,
      "learning_rate": 4.374872838250255e-05,
      "loss": 3.1595,
      "step": 12290
    },
    {
      "epoch": 6.256358087487284,
      "grad_norm": 9.829050064086914,
      "learning_rate": 4.374364191251272e-05,
      "loss": 3.2849,
      "step": 12300
    },
    {
      "epoch": 6.261444557477111,
      "grad_norm": 8.185510635375977,
      "learning_rate": 4.3738555442522895e-05,
      "loss": 3.2653,
      "step": 12310
    },
    {
      "epoch": 6.266531027466938,
      "grad_norm": 9.910697937011719,
      "learning_rate": 4.3733468972533065e-05,
      "loss": 3.2805,
      "step": 12320
    },
    {
      "epoch": 6.271617497456765,
      "grad_norm": 11.709479331970215,
      "learning_rate": 4.3728382502543235e-05,
      "loss": 3.2948,
      "step": 12330
    },
    {
      "epoch": 6.276703967446592,
      "grad_norm": 10.404139518737793,
      "learning_rate": 4.372329603255341e-05,
      "loss": 3.2896,
      "step": 12340
    },
    {
      "epoch": 6.2817904374364195,
      "grad_norm": 10.194877624511719,
      "learning_rate": 4.371820956256358e-05,
      "loss": 3.25,
      "step": 12350
    },
    {
      "epoch": 6.2868769074262465,
      "grad_norm": 12.115843772888184,
      "learning_rate": 4.371312309257375e-05,
      "loss": 3.2309,
      "step": 12360
    },
    {
      "epoch": 6.2919633774160735,
      "grad_norm": 10.375568389892578,
      "learning_rate": 4.370803662258393e-05,
      "loss": 3.2788,
      "step": 12370
    },
    {
      "epoch": 6.2970498474059005,
      "grad_norm": 11.693881034851074,
      "learning_rate": 4.3702950152594105e-05,
      "loss": 3.3067,
      "step": 12380
    },
    {
      "epoch": 6.302136317395727,
      "grad_norm": 7.646602630615234,
      "learning_rate": 4.3697863682604275e-05,
      "loss": 3.2518,
      "step": 12390
    },
    {
      "epoch": 6.307222787385554,
      "grad_norm": 10.19802474975586,
      "learning_rate": 4.369277721261445e-05,
      "loss": 3.1901,
      "step": 12400
    },
    {
      "epoch": 6.312309257375381,
      "grad_norm": 11.075265884399414,
      "learning_rate": 4.368769074262462e-05,
      "loss": 3.3191,
      "step": 12410
    },
    {
      "epoch": 6.317395727365208,
      "grad_norm": 11.427128791809082,
      "learning_rate": 4.368260427263479e-05,
      "loss": 3.2159,
      "step": 12420
    },
    {
      "epoch": 6.322482197355035,
      "grad_norm": 9.94471263885498,
      "learning_rate": 4.367751780264497e-05,
      "loss": 3.3084,
      "step": 12430
    },
    {
      "epoch": 6.327568667344862,
      "grad_norm": 11.320552825927734,
      "learning_rate": 4.367243133265514e-05,
      "loss": 3.2476,
      "step": 12440
    },
    {
      "epoch": 6.332655137334689,
      "grad_norm": 12.766264915466309,
      "learning_rate": 4.366734486266531e-05,
      "loss": 3.2618,
      "step": 12450
    },
    {
      "epoch": 6.337741607324517,
      "grad_norm": 11.984681129455566,
      "learning_rate": 4.3662258392675484e-05,
      "loss": 3.2113,
      "step": 12460
    },
    {
      "epoch": 6.342828077314344,
      "grad_norm": 7.671189308166504,
      "learning_rate": 4.365717192268566e-05,
      "loss": 3.2194,
      "step": 12470
    },
    {
      "epoch": 6.347914547304171,
      "grad_norm": 8.380943298339844,
      "learning_rate": 4.365208545269583e-05,
      "loss": 3.2416,
      "step": 12480
    },
    {
      "epoch": 6.353001017293998,
      "grad_norm": 9.585579872131348,
      "learning_rate": 4.364699898270601e-05,
      "loss": 3.2211,
      "step": 12490
    },
    {
      "epoch": 6.358087487283825,
      "grad_norm": 9.270358085632324,
      "learning_rate": 4.364191251271618e-05,
      "loss": 3.2466,
      "step": 12500
    },
    {
      "epoch": 6.363173957273652,
      "grad_norm": 13.5767240524292,
      "learning_rate": 4.363682604272635e-05,
      "loss": 3.2135,
      "step": 12510
    },
    {
      "epoch": 6.368260427263479,
      "grad_norm": 11.323988914489746,
      "learning_rate": 4.3631739572736524e-05,
      "loss": 3.402,
      "step": 12520
    },
    {
      "epoch": 6.373346897253306,
      "grad_norm": 10.207348823547363,
      "learning_rate": 4.3626653102746694e-05,
      "loss": 3.2322,
      "step": 12530
    },
    {
      "epoch": 6.378433367243133,
      "grad_norm": 11.793749809265137,
      "learning_rate": 4.3621566632756864e-05,
      "loss": 3.1975,
      "step": 12540
    },
    {
      "epoch": 6.38351983723296,
      "grad_norm": 10.125690460205078,
      "learning_rate": 4.361648016276704e-05,
      "loss": 3.2221,
      "step": 12550
    },
    {
      "epoch": 6.388606307222787,
      "grad_norm": 11.624287605285645,
      "learning_rate": 4.361139369277721e-05,
      "loss": 3.2828,
      "step": 12560
    },
    {
      "epoch": 6.393692777212614,
      "grad_norm": 14.066250801086426,
      "learning_rate": 4.360630722278739e-05,
      "loss": 3.2339,
      "step": 12570
    },
    {
      "epoch": 6.398779247202442,
      "grad_norm": 10.392916679382324,
      "learning_rate": 4.3601220752797564e-05,
      "loss": 3.2153,
      "step": 12580
    },
    {
      "epoch": 6.403865717192269,
      "grad_norm": 13.055791854858398,
      "learning_rate": 4.3596134282807734e-05,
      "loss": 3.2246,
      "step": 12590
    },
    {
      "epoch": 6.408952187182096,
      "grad_norm": 9.581912994384766,
      "learning_rate": 4.359104781281791e-05,
      "loss": 3.2021,
      "step": 12600
    },
    {
      "epoch": 6.414038657171923,
      "grad_norm": 11.372106552124023,
      "learning_rate": 4.358596134282808e-05,
      "loss": 3.1808,
      "step": 12610
    },
    {
      "epoch": 6.41912512716175,
      "grad_norm": 12.767346382141113,
      "learning_rate": 4.358087487283825e-05,
      "loss": 3.2362,
      "step": 12620
    },
    {
      "epoch": 6.424211597151577,
      "grad_norm": 9.470026969909668,
      "learning_rate": 4.357578840284843e-05,
      "loss": 3.2806,
      "step": 12630
    },
    {
      "epoch": 6.429298067141404,
      "grad_norm": 13.293411254882812,
      "learning_rate": 4.35707019328586e-05,
      "loss": 3.2239,
      "step": 12640
    },
    {
      "epoch": 6.434384537131231,
      "grad_norm": 11.859395980834961,
      "learning_rate": 4.356561546286877e-05,
      "loss": 3.2283,
      "step": 12650
    },
    {
      "epoch": 6.439471007121058,
      "grad_norm": 11.766693115234375,
      "learning_rate": 4.356052899287894e-05,
      "loss": 3.2965,
      "step": 12660
    },
    {
      "epoch": 6.444557477110885,
      "grad_norm": 11.173050880432129,
      "learning_rate": 4.355544252288912e-05,
      "loss": 3.3127,
      "step": 12670
    },
    {
      "epoch": 6.4496439471007125,
      "grad_norm": 11.033978462219238,
      "learning_rate": 4.355035605289929e-05,
      "loss": 3.2873,
      "step": 12680
    },
    {
      "epoch": 6.4547304170905395,
      "grad_norm": 9.976672172546387,
      "learning_rate": 4.3545269582909467e-05,
      "loss": 3.3471,
      "step": 12690
    },
    {
      "epoch": 6.4598168870803665,
      "grad_norm": 8.389979362487793,
      "learning_rate": 4.3540183112919636e-05,
      "loss": 3.2729,
      "step": 12700
    },
    {
      "epoch": 6.4649033570701935,
      "grad_norm": 16.390270233154297,
      "learning_rate": 4.3535096642929806e-05,
      "loss": 3.2838,
      "step": 12710
    },
    {
      "epoch": 6.4699898270600205,
      "grad_norm": 9.303053855895996,
      "learning_rate": 4.353001017293998e-05,
      "loss": 3.3231,
      "step": 12720
    },
    {
      "epoch": 6.475076297049847,
      "grad_norm": 11.123842239379883,
      "learning_rate": 4.352492370295015e-05,
      "loss": 3.2038,
      "step": 12730
    },
    {
      "epoch": 6.480162767039674,
      "grad_norm": 10.659820556640625,
      "learning_rate": 4.351983723296032e-05,
      "loss": 3.2844,
      "step": 12740
    },
    {
      "epoch": 6.485249237029501,
      "grad_norm": 12.965475082397461,
      "learning_rate": 4.35147507629705e-05,
      "loss": 3.2352,
      "step": 12750
    },
    {
      "epoch": 6.490335707019328,
      "grad_norm": 10.997183799743652,
      "learning_rate": 4.3509664292980676e-05,
      "loss": 3.246,
      "step": 12760
    },
    {
      "epoch": 6.495422177009155,
      "grad_norm": 13.63547134399414,
      "learning_rate": 4.3504577822990846e-05,
      "loss": 3.2743,
      "step": 12770
    },
    {
      "epoch": 6.500508646998982,
      "grad_norm": 13.303077697753906,
      "learning_rate": 4.349949135300102e-05,
      "loss": 3.2641,
      "step": 12780
    },
    {
      "epoch": 6.505595116988809,
      "grad_norm": 11.430062294006348,
      "learning_rate": 4.349440488301119e-05,
      "loss": 3.2273,
      "step": 12790
    },
    {
      "epoch": 6.510681586978637,
      "grad_norm": 10.979966163635254,
      "learning_rate": 4.348931841302136e-05,
      "loss": 3.2379,
      "step": 12800
    },
    {
      "epoch": 6.515768056968464,
      "grad_norm": 12.225619316101074,
      "learning_rate": 4.348423194303154e-05,
      "loss": 3.2257,
      "step": 12810
    },
    {
      "epoch": 6.520854526958291,
      "grad_norm": 11.77514362335205,
      "learning_rate": 4.347914547304171e-05,
      "loss": 3.2263,
      "step": 12820
    },
    {
      "epoch": 6.525940996948118,
      "grad_norm": 9.936267852783203,
      "learning_rate": 4.3474059003051886e-05,
      "loss": 3.2223,
      "step": 12830
    },
    {
      "epoch": 6.531027466937945,
      "grad_norm": 11.874371528625488,
      "learning_rate": 4.3468972533062056e-05,
      "loss": 3.2626,
      "step": 12840
    },
    {
      "epoch": 6.536113936927772,
      "grad_norm": 13.542184829711914,
      "learning_rate": 4.346388606307223e-05,
      "loss": 3.0533,
      "step": 12850
    },
    {
      "epoch": 6.541200406917599,
      "grad_norm": 10.162274360656738,
      "learning_rate": 4.34587995930824e-05,
      "loss": 3.2407,
      "step": 12860
    },
    {
      "epoch": 6.546286876907426,
      "grad_norm": 9.996756553649902,
      "learning_rate": 4.345371312309258e-05,
      "loss": 3.1943,
      "step": 12870
    },
    {
      "epoch": 6.551373346897253,
      "grad_norm": 13.290634155273438,
      "learning_rate": 4.344862665310275e-05,
      "loss": 3.277,
      "step": 12880
    },
    {
      "epoch": 6.55645981688708,
      "grad_norm": 10.582927703857422,
      "learning_rate": 4.3443540183112925e-05,
      "loss": 3.1901,
      "step": 12890
    },
    {
      "epoch": 6.561546286876908,
      "grad_norm": 12.714545249938965,
      "learning_rate": 4.3438453713123095e-05,
      "loss": 3.2259,
      "step": 12900
    },
    {
      "epoch": 6.566632756866735,
      "grad_norm": 9.669217109680176,
      "learning_rate": 4.3433367243133265e-05,
      "loss": 3.2268,
      "step": 12910
    },
    {
      "epoch": 6.571719226856562,
      "grad_norm": 10.973793983459473,
      "learning_rate": 4.342828077314344e-05,
      "loss": 3.1711,
      "step": 12920
    },
    {
      "epoch": 6.576805696846389,
      "grad_norm": 11.460936546325684,
      "learning_rate": 4.342319430315361e-05,
      "loss": 3.1746,
      "step": 12930
    },
    {
      "epoch": 6.581892166836216,
      "grad_norm": 8.077754974365234,
      "learning_rate": 4.341810783316378e-05,
      "loss": 3.277,
      "step": 12940
    },
    {
      "epoch": 6.586978636826043,
      "grad_norm": 12.262907981872559,
      "learning_rate": 4.341302136317396e-05,
      "loss": 3.207,
      "step": 12950
    },
    {
      "epoch": 6.59206510681587,
      "grad_norm": 13.164629936218262,
      "learning_rate": 4.3407934893184135e-05,
      "loss": 3.1639,
      "step": 12960
    },
    {
      "epoch": 6.597151576805697,
      "grad_norm": 12.98508358001709,
      "learning_rate": 4.3402848423194305e-05,
      "loss": 3.2564,
      "step": 12970
    },
    {
      "epoch": 6.602238046795524,
      "grad_norm": 12.724855422973633,
      "learning_rate": 4.339776195320448e-05,
      "loss": 3.2613,
      "step": 12980
    },
    {
      "epoch": 6.607324516785351,
      "grad_norm": 13.869009971618652,
      "learning_rate": 4.339267548321465e-05,
      "loss": 3.1498,
      "step": 12990
    },
    {
      "epoch": 6.612410986775178,
      "grad_norm": 10.372666358947754,
      "learning_rate": 4.338758901322482e-05,
      "loss": 3.1964,
      "step": 13000
    },
    {
      "epoch": 6.617497456765005,
      "grad_norm": 8.399933815002441,
      "learning_rate": 4.3382502543235e-05,
      "loss": 3.2306,
      "step": 13010
    },
    {
      "epoch": 6.622583926754832,
      "grad_norm": 12.580220222473145,
      "learning_rate": 4.337741607324517e-05,
      "loss": 3.2353,
      "step": 13020
    },
    {
      "epoch": 6.6276703967446595,
      "grad_norm": 11.342060089111328,
      "learning_rate": 4.337232960325534e-05,
      "loss": 3.1855,
      "step": 13030
    },
    {
      "epoch": 6.6327568667344865,
      "grad_norm": 15.268062591552734,
      "learning_rate": 4.3367243133265514e-05,
      "loss": 3.1595,
      "step": 13040
    },
    {
      "epoch": 6.6378433367243135,
      "grad_norm": 10.943838119506836,
      "learning_rate": 4.336215666327569e-05,
      "loss": 3.1842,
      "step": 13050
    },
    {
      "epoch": 6.6429298067141405,
      "grad_norm": 10.212411880493164,
      "learning_rate": 4.335707019328586e-05,
      "loss": 3.2522,
      "step": 13060
    },
    {
      "epoch": 6.648016276703967,
      "grad_norm": 9.586613655090332,
      "learning_rate": 4.335198372329604e-05,
      "loss": 3.145,
      "step": 13070
    },
    {
      "epoch": 6.653102746693794,
      "grad_norm": 13.448043823242188,
      "learning_rate": 4.334689725330621e-05,
      "loss": 3.2332,
      "step": 13080
    },
    {
      "epoch": 6.658189216683621,
      "grad_norm": 13.557765007019043,
      "learning_rate": 4.334181078331638e-05,
      "loss": 3.2488,
      "step": 13090
    },
    {
      "epoch": 6.663275686673448,
      "grad_norm": 12.310768127441406,
      "learning_rate": 4.3336724313326554e-05,
      "loss": 3.2519,
      "step": 13100
    },
    {
      "epoch": 6.668362156663275,
      "grad_norm": 11.596999168395996,
      "learning_rate": 4.3331637843336724e-05,
      "loss": 3.259,
      "step": 13110
    },
    {
      "epoch": 6.673448626653103,
      "grad_norm": 11.129977226257324,
      "learning_rate": 4.33265513733469e-05,
      "loss": 3.1986,
      "step": 13120
    },
    {
      "epoch": 6.67853509664293,
      "grad_norm": 10.85417652130127,
      "learning_rate": 4.332146490335707e-05,
      "loss": 3.2239,
      "step": 13130
    },
    {
      "epoch": 6.683621566632757,
      "grad_norm": 10.424202919006348,
      "learning_rate": 4.331637843336725e-05,
      "loss": 3.2226,
      "step": 13140
    },
    {
      "epoch": 6.688708036622584,
      "grad_norm": 11.110891342163086,
      "learning_rate": 4.3311291963377424e-05,
      "loss": 3.2696,
      "step": 13150
    },
    {
      "epoch": 6.693794506612411,
      "grad_norm": 12.576324462890625,
      "learning_rate": 4.3306205493387594e-05,
      "loss": 3.1529,
      "step": 13160
    },
    {
      "epoch": 6.698880976602238,
      "grad_norm": 13.022037506103516,
      "learning_rate": 4.3301119023397764e-05,
      "loss": 3.2463,
      "step": 13170
    },
    {
      "epoch": 6.703967446592065,
      "grad_norm": 10.296320915222168,
      "learning_rate": 4.329603255340794e-05,
      "loss": 3.2263,
      "step": 13180
    },
    {
      "epoch": 6.709053916581892,
      "grad_norm": 11.199825286865234,
      "learning_rate": 4.329094608341811e-05,
      "loss": 3.2178,
      "step": 13190
    },
    {
      "epoch": 6.714140386571719,
      "grad_norm": 11.349441528320312,
      "learning_rate": 4.328585961342828e-05,
      "loss": 3.2155,
      "step": 13200
    },
    {
      "epoch": 6.719226856561546,
      "grad_norm": 8.889259338378906,
      "learning_rate": 4.328077314343846e-05,
      "loss": 3.1847,
      "step": 13210
    },
    {
      "epoch": 6.724313326551373,
      "grad_norm": 15.080031394958496,
      "learning_rate": 4.327568667344863e-05,
      "loss": 3.1787,
      "step": 13220
    },
    {
      "epoch": 6.7293997965412,
      "grad_norm": 13.261422157287598,
      "learning_rate": 4.32706002034588e-05,
      "loss": 3.2623,
      "step": 13230
    },
    {
      "epoch": 6.734486266531027,
      "grad_norm": 13.4135103225708,
      "learning_rate": 4.326551373346897e-05,
      "loss": 3.138,
      "step": 13240
    },
    {
      "epoch": 6.739572736520855,
      "grad_norm": 10.068618774414062,
      "learning_rate": 4.326042726347915e-05,
      "loss": 3.2563,
      "step": 13250
    },
    {
      "epoch": 6.744659206510682,
      "grad_norm": 11.3948335647583,
      "learning_rate": 4.325534079348932e-05,
      "loss": 3.2078,
      "step": 13260
    },
    {
      "epoch": 6.749745676500509,
      "grad_norm": 8.03414535522461,
      "learning_rate": 4.3250254323499497e-05,
      "loss": 3.1753,
      "step": 13270
    },
    {
      "epoch": 6.754832146490336,
      "grad_norm": 16.495262145996094,
      "learning_rate": 4.3245167853509666e-05,
      "loss": 3.2322,
      "step": 13280
    },
    {
      "epoch": 6.759918616480163,
      "grad_norm": 14.192139625549316,
      "learning_rate": 4.3240081383519836e-05,
      "loss": 3.1767,
      "step": 13290
    },
    {
      "epoch": 6.76500508646999,
      "grad_norm": 13.336214065551758,
      "learning_rate": 4.323499491353001e-05,
      "loss": 3.2891,
      "step": 13300
    },
    {
      "epoch": 6.770091556459817,
      "grad_norm": 11.138673782348633,
      "learning_rate": 4.322990844354018e-05,
      "loss": 3.194,
      "step": 13310
    },
    {
      "epoch": 6.775178026449644,
      "grad_norm": 13.310430526733398,
      "learning_rate": 4.322482197355035e-05,
      "loss": 3.1146,
      "step": 13320
    },
    {
      "epoch": 6.780264496439471,
      "grad_norm": 10.071840286254883,
      "learning_rate": 4.321973550356053e-05,
      "loss": 3.2367,
      "step": 13330
    },
    {
      "epoch": 6.785350966429298,
      "grad_norm": 8.101580619812012,
      "learning_rate": 4.3214649033570706e-05,
      "loss": 3.2247,
      "step": 13340
    },
    {
      "epoch": 6.790437436419126,
      "grad_norm": 13.314878463745117,
      "learning_rate": 4.3209562563580876e-05,
      "loss": 3.1345,
      "step": 13350
    },
    {
      "epoch": 6.7955239064089525,
      "grad_norm": 12.384883880615234,
      "learning_rate": 4.320447609359105e-05,
      "loss": 3.1763,
      "step": 13360
    },
    {
      "epoch": 6.8006103763987795,
      "grad_norm": 12.097447395324707,
      "learning_rate": 4.319938962360122e-05,
      "loss": 3.2699,
      "step": 13370
    },
    {
      "epoch": 6.8056968463886065,
      "grad_norm": 11.867907524108887,
      "learning_rate": 4.31943031536114e-05,
      "loss": 3.1509,
      "step": 13380
    },
    {
      "epoch": 6.8107833163784335,
      "grad_norm": 17.24781036376953,
      "learning_rate": 4.318921668362157e-05,
      "loss": 3.1562,
      "step": 13390
    },
    {
      "epoch": 6.8158697863682605,
      "grad_norm": 9.532815933227539,
      "learning_rate": 4.318413021363174e-05,
      "loss": 3.2563,
      "step": 13400
    },
    {
      "epoch": 6.820956256358087,
      "grad_norm": 12.62606143951416,
      "learning_rate": 4.3179043743641916e-05,
      "loss": 3.1896,
      "step": 13410
    },
    {
      "epoch": 6.826042726347914,
      "grad_norm": 10.735611915588379,
      "learning_rate": 4.3173957273652086e-05,
      "loss": 3.194,
      "step": 13420
    },
    {
      "epoch": 6.831129196337741,
      "grad_norm": 11.94498062133789,
      "learning_rate": 4.316887080366226e-05,
      "loss": 3.2106,
      "step": 13430
    },
    {
      "epoch": 6.836215666327568,
      "grad_norm": 12.905476570129395,
      "learning_rate": 4.316378433367244e-05,
      "loss": 3.1545,
      "step": 13440
    },
    {
      "epoch": 6.841302136317395,
      "grad_norm": 12.400548934936523,
      "learning_rate": 4.315869786368261e-05,
      "loss": 3.1735,
      "step": 13450
    },
    {
      "epoch": 6.846388606307222,
      "grad_norm": 13.932846069335938,
      "learning_rate": 4.315361139369278e-05,
      "loss": 3.223,
      "step": 13460
    },
    {
      "epoch": 6.85147507629705,
      "grad_norm": 10.51653003692627,
      "learning_rate": 4.3148524923702955e-05,
      "loss": 3.159,
      "step": 13470
    },
    {
      "epoch": 6.856561546286877,
      "grad_norm": 12.383691787719727,
      "learning_rate": 4.3143438453713125e-05,
      "loss": 3.1821,
      "step": 13480
    },
    {
      "epoch": 6.861648016276704,
      "grad_norm": 12.677413940429688,
      "learning_rate": 4.3138351983723295e-05,
      "loss": 3.128,
      "step": 13490
    },
    {
      "epoch": 6.866734486266531,
      "grad_norm": 17.795982360839844,
      "learning_rate": 4.313326551373347e-05,
      "loss": 3.2253,
      "step": 13500
    },
    {
      "epoch": 6.871820956256358,
      "grad_norm": 14.110898971557617,
      "learning_rate": 4.312817904374364e-05,
      "loss": 3.1758,
      "step": 13510
    },
    {
      "epoch": 6.876907426246185,
      "grad_norm": 9.910618782043457,
      "learning_rate": 4.312309257375381e-05,
      "loss": 3.1443,
      "step": 13520
    },
    {
      "epoch": 6.881993896236012,
      "grad_norm": 11.029361724853516,
      "learning_rate": 4.311800610376399e-05,
      "loss": 3.1306,
      "step": 13530
    },
    {
      "epoch": 6.887080366225839,
      "grad_norm": 11.703958511352539,
      "learning_rate": 4.3112919633774165e-05,
      "loss": 3.199,
      "step": 13540
    },
    {
      "epoch": 6.892166836215666,
      "grad_norm": 10.050275802612305,
      "learning_rate": 4.3107833163784335e-05,
      "loss": 3.1808,
      "step": 13550
    },
    {
      "epoch": 6.897253306205493,
      "grad_norm": 11.304215431213379,
      "learning_rate": 4.310274669379451e-05,
      "loss": 3.1882,
      "step": 13560
    },
    {
      "epoch": 6.902339776195321,
      "grad_norm": 13.691685676574707,
      "learning_rate": 4.309766022380468e-05,
      "loss": 3.1841,
      "step": 13570
    },
    {
      "epoch": 6.907426246185148,
      "grad_norm": 17.383460998535156,
      "learning_rate": 4.309257375381485e-05,
      "loss": 3.1927,
      "step": 13580
    },
    {
      "epoch": 6.912512716174975,
      "grad_norm": 13.849498748779297,
      "learning_rate": 4.308748728382503e-05,
      "loss": 3.233,
      "step": 13590
    },
    {
      "epoch": 6.917599186164802,
      "grad_norm": 11.017163276672363,
      "learning_rate": 4.30824008138352e-05,
      "loss": 3.2595,
      "step": 13600
    },
    {
      "epoch": 6.922685656154629,
      "grad_norm": 10.699933052062988,
      "learning_rate": 4.307731434384537e-05,
      "loss": 3.2411,
      "step": 13610
    },
    {
      "epoch": 6.927772126144456,
      "grad_norm": 13.152884483337402,
      "learning_rate": 4.3072227873855544e-05,
      "loss": 3.1756,
      "step": 13620
    },
    {
      "epoch": 6.932858596134283,
      "grad_norm": 10.675054550170898,
      "learning_rate": 4.306714140386572e-05,
      "loss": 3.2188,
      "step": 13630
    },
    {
      "epoch": 6.93794506612411,
      "grad_norm": 10.713194847106934,
      "learning_rate": 4.30620549338759e-05,
      "loss": 3.1647,
      "step": 13640
    },
    {
      "epoch": 6.943031536113937,
      "grad_norm": 9.366776466369629,
      "learning_rate": 4.305696846388607e-05,
      "loss": 3.2546,
      "step": 13650
    },
    {
      "epoch": 6.948118006103764,
      "grad_norm": 10.866145133972168,
      "learning_rate": 4.305188199389624e-05,
      "loss": 3.1756,
      "step": 13660
    },
    {
      "epoch": 6.953204476093591,
      "grad_norm": 12.500100135803223,
      "learning_rate": 4.3046795523906414e-05,
      "loss": 3.1845,
      "step": 13670
    },
    {
      "epoch": 6.958290946083418,
      "grad_norm": 13.523908615112305,
      "learning_rate": 4.3041709053916584e-05,
      "loss": 3.1854,
      "step": 13680
    },
    {
      "epoch": 6.963377416073245,
      "grad_norm": 12.461912155151367,
      "learning_rate": 4.3036622583926754e-05,
      "loss": 3.1714,
      "step": 13690
    },
    {
      "epoch": 6.9684638860630725,
      "grad_norm": 9.463894844055176,
      "learning_rate": 4.303153611393693e-05,
      "loss": 3.2473,
      "step": 13700
    },
    {
      "epoch": 6.9735503560528995,
      "grad_norm": 13.426480293273926,
      "learning_rate": 4.30264496439471e-05,
      "loss": 3.2172,
      "step": 13710
    },
    {
      "epoch": 6.9786368260427265,
      "grad_norm": 10.739567756652832,
      "learning_rate": 4.302136317395728e-05,
      "loss": 3.2579,
      "step": 13720
    },
    {
      "epoch": 6.9837232960325535,
      "grad_norm": 12.784322738647461,
      "learning_rate": 4.3016276703967454e-05,
      "loss": 3.2568,
      "step": 13730
    },
    {
      "epoch": 6.9888097660223805,
      "grad_norm": 12.60083293914795,
      "learning_rate": 4.3011190233977624e-05,
      "loss": 3.1608,
      "step": 13740
    },
    {
      "epoch": 6.9938962360122074,
      "grad_norm": 11.553544044494629,
      "learning_rate": 4.3006103763987794e-05,
      "loss": 3.1453,
      "step": 13750
    },
    {
      "epoch": 6.998982706002034,
      "grad_norm": 10.258931159973145,
      "learning_rate": 4.300101729399797e-05,
      "loss": 3.152,
      "step": 13760
    },
    {
      "epoch": 7.0,
      "eval_loss": 3.6671340465545654,
      "eval_runtime": 3.5272,
      "eval_samples_per_second": 786.754,
      "eval_steps_per_second": 98.38,
      "step": 13762
    },
    {
      "epoch": 7.004069175991861,
      "grad_norm": 11.5941743850708,
      "learning_rate": 4.299593082400814e-05,
      "loss": 3.1832,
      "step": 13770
    },
    {
      "epoch": 7.009155645981688,
      "grad_norm": 15.35818099975586,
      "learning_rate": 4.299084435401831e-05,
      "loss": 3.1543,
      "step": 13780
    },
    {
      "epoch": 7.014242115971515,
      "grad_norm": 12.676278114318848,
      "learning_rate": 4.298575788402849e-05,
      "loss": 3.2173,
      "step": 13790
    },
    {
      "epoch": 7.019328585961343,
      "grad_norm": 10.982734680175781,
      "learning_rate": 4.298067141403866e-05,
      "loss": 3.1071,
      "step": 13800
    },
    {
      "epoch": 7.02441505595117,
      "grad_norm": 10.946977615356445,
      "learning_rate": 4.2975584944048833e-05,
      "loss": 3.1506,
      "step": 13810
    },
    {
      "epoch": 7.029501525940997,
      "grad_norm": 13.051501274108887,
      "learning_rate": 4.2970498474059e-05,
      "loss": 3.1698,
      "step": 13820
    },
    {
      "epoch": 7.034587995930824,
      "grad_norm": 14.462674140930176,
      "learning_rate": 4.296541200406918e-05,
      "loss": 3.1678,
      "step": 13830
    },
    {
      "epoch": 7.039674465920651,
      "grad_norm": 11.077228546142578,
      "learning_rate": 4.296032553407935e-05,
      "loss": 3.1898,
      "step": 13840
    },
    {
      "epoch": 7.044760935910478,
      "grad_norm": 13.068549156188965,
      "learning_rate": 4.2955239064089527e-05,
      "loss": 3.1737,
      "step": 13850
    },
    {
      "epoch": 7.049847405900305,
      "grad_norm": 12.239466667175293,
      "learning_rate": 4.2950152594099696e-05,
      "loss": 3.0558,
      "step": 13860
    },
    {
      "epoch": 7.054933875890132,
      "grad_norm": 9.303457260131836,
      "learning_rate": 4.2945066124109866e-05,
      "loss": 3.1632,
      "step": 13870
    },
    {
      "epoch": 7.060020345879959,
      "grad_norm": 11.344688415527344,
      "learning_rate": 4.293997965412004e-05,
      "loss": 3.144,
      "step": 13880
    },
    {
      "epoch": 7.065106815869786,
      "grad_norm": 15.662213325500488,
      "learning_rate": 4.293489318413021e-05,
      "loss": 3.1709,
      "step": 13890
    },
    {
      "epoch": 7.070193285859613,
      "grad_norm": 8.776938438415527,
      "learning_rate": 4.292980671414038e-05,
      "loss": 3.1276,
      "step": 13900
    },
    {
      "epoch": 7.075279755849441,
      "grad_norm": 12.234952926635742,
      "learning_rate": 4.292472024415056e-05,
      "loss": 3.1592,
      "step": 13910
    },
    {
      "epoch": 7.080366225839268,
      "grad_norm": 9.527645111083984,
      "learning_rate": 4.2919633774160736e-05,
      "loss": 3.1701,
      "step": 13920
    },
    {
      "epoch": 7.085452695829095,
      "grad_norm": 14.235998153686523,
      "learning_rate": 4.291454730417091e-05,
      "loss": 3.1763,
      "step": 13930
    },
    {
      "epoch": 7.090539165818922,
      "grad_norm": 12.754864692687988,
      "learning_rate": 4.290946083418108e-05,
      "loss": 3.222,
      "step": 13940
    },
    {
      "epoch": 7.095625635808749,
      "grad_norm": 12.526369094848633,
      "learning_rate": 4.290437436419125e-05,
      "loss": 3.1916,
      "step": 13950
    },
    {
      "epoch": 7.100712105798576,
      "grad_norm": 13.054576873779297,
      "learning_rate": 4.289928789420143e-05,
      "loss": 3.098,
      "step": 13960
    },
    {
      "epoch": 7.105798575788403,
      "grad_norm": 11.275128364562988,
      "learning_rate": 4.28942014242116e-05,
      "loss": 3.1031,
      "step": 13970
    },
    {
      "epoch": 7.11088504577823,
      "grad_norm": 10.300225257873535,
      "learning_rate": 4.288911495422177e-05,
      "loss": 3.1337,
      "step": 13980
    },
    {
      "epoch": 7.115971515768057,
      "grad_norm": 17.400615692138672,
      "learning_rate": 4.2884028484231946e-05,
      "loss": 3.2343,
      "step": 13990
    },
    {
      "epoch": 7.121057985757884,
      "grad_norm": 9.169463157653809,
      "learning_rate": 4.2878942014242116e-05,
      "loss": 3.1406,
      "step": 14000
    },
    {
      "epoch": 7.126144455747711,
      "grad_norm": 12.050277709960938,
      "learning_rate": 4.287385554425229e-05,
      "loss": 3.1378,
      "step": 14010
    },
    {
      "epoch": 7.131230925737539,
      "grad_norm": 16.103662490844727,
      "learning_rate": 4.286876907426247e-05,
      "loss": 3.163,
      "step": 14020
    },
    {
      "epoch": 7.136317395727366,
      "grad_norm": 13.772439002990723,
      "learning_rate": 4.286368260427264e-05,
      "loss": 3.1328,
      "step": 14030
    },
    {
      "epoch": 7.1414038657171925,
      "grad_norm": 12.79511833190918,
      "learning_rate": 4.285859613428281e-05,
      "loss": 3.1369,
      "step": 14040
    },
    {
      "epoch": 7.1464903357070195,
      "grad_norm": 12.766855239868164,
      "learning_rate": 4.2853509664292985e-05,
      "loss": 3.1229,
      "step": 14050
    },
    {
      "epoch": 7.1515768056968465,
      "grad_norm": 11.497024536132812,
      "learning_rate": 4.2848423194303155e-05,
      "loss": 3.1251,
      "step": 14060
    },
    {
      "epoch": 7.1566632756866735,
      "grad_norm": 13.164114952087402,
      "learning_rate": 4.2843336724313325e-05,
      "loss": 3.1601,
      "step": 14070
    },
    {
      "epoch": 7.1617497456765005,
      "grad_norm": 12.722779273986816,
      "learning_rate": 4.28382502543235e-05,
      "loss": 3.0843,
      "step": 14080
    },
    {
      "epoch": 7.1668362156663274,
      "grad_norm": 13.412485122680664,
      "learning_rate": 4.283316378433367e-05,
      "loss": 3.1694,
      "step": 14090
    },
    {
      "epoch": 7.171922685656154,
      "grad_norm": 17.61063003540039,
      "learning_rate": 4.282807731434385e-05,
      "loss": 3.0875,
      "step": 14100
    },
    {
      "epoch": 7.177009155645981,
      "grad_norm": 12.588896751403809,
      "learning_rate": 4.2822990844354025e-05,
      "loss": 3.1712,
      "step": 14110
    },
    {
      "epoch": 7.182095625635808,
      "grad_norm": 12.112273216247559,
      "learning_rate": 4.2817904374364195e-05,
      "loss": 3.2182,
      "step": 14120
    },
    {
      "epoch": 7.187182095625635,
      "grad_norm": 13.229724884033203,
      "learning_rate": 4.2812817904374365e-05,
      "loss": 3.1769,
      "step": 14130
    },
    {
      "epoch": 7.192268565615463,
      "grad_norm": 10.960392951965332,
      "learning_rate": 4.280773143438454e-05,
      "loss": 3.2156,
      "step": 14140
    },
    {
      "epoch": 7.19735503560529,
      "grad_norm": 11.743688583374023,
      "learning_rate": 4.280264496439471e-05,
      "loss": 3.2742,
      "step": 14150
    },
    {
      "epoch": 7.202441505595117,
      "grad_norm": 9.819112777709961,
      "learning_rate": 4.279755849440488e-05,
      "loss": 3.1402,
      "step": 14160
    },
    {
      "epoch": 7.207527975584944,
      "grad_norm": 12.61091136932373,
      "learning_rate": 4.279247202441506e-05,
      "loss": 3.1448,
      "step": 14170
    },
    {
      "epoch": 7.212614445574771,
      "grad_norm": 12.436691284179688,
      "learning_rate": 4.278738555442523e-05,
      "loss": 3.0793,
      "step": 14180
    },
    {
      "epoch": 7.217700915564598,
      "grad_norm": 10.88539981842041,
      "learning_rate": 4.2782299084435405e-05,
      "loss": 3.1051,
      "step": 14190
    },
    {
      "epoch": 7.222787385554425,
      "grad_norm": 9.222299575805664,
      "learning_rate": 4.2777212614445574e-05,
      "loss": 3.1065,
      "step": 14200
    },
    {
      "epoch": 7.227873855544252,
      "grad_norm": 11.611506462097168,
      "learning_rate": 4.277212614445575e-05,
      "loss": 3.1444,
      "step": 14210
    },
    {
      "epoch": 7.232960325534079,
      "grad_norm": 14.176562309265137,
      "learning_rate": 4.276703967446593e-05,
      "loss": 3.1488,
      "step": 14220
    },
    {
      "epoch": 7.238046795523906,
      "grad_norm": 14.568534851074219,
      "learning_rate": 4.27619532044761e-05,
      "loss": 3.1337,
      "step": 14230
    },
    {
      "epoch": 7.243133265513733,
      "grad_norm": 13.909802436828613,
      "learning_rate": 4.275686673448627e-05,
      "loss": 3.2357,
      "step": 14240
    },
    {
      "epoch": 7.248219735503561,
      "grad_norm": 9.760618209838867,
      "learning_rate": 4.2751780264496444e-05,
      "loss": 3.1387,
      "step": 14250
    },
    {
      "epoch": 7.253306205493388,
      "grad_norm": 11.027158737182617,
      "learning_rate": 4.2746693794506614e-05,
      "loss": 3.1669,
      "step": 14260
    },
    {
      "epoch": 7.258392675483215,
      "grad_norm": 18.723779678344727,
      "learning_rate": 4.2741607324516784e-05,
      "loss": 3.1914,
      "step": 14270
    },
    {
      "epoch": 7.263479145473042,
      "grad_norm": 13.623268127441406,
      "learning_rate": 4.273652085452696e-05,
      "loss": 3.1685,
      "step": 14280
    },
    {
      "epoch": 7.268565615462869,
      "grad_norm": 17.841869354248047,
      "learning_rate": 4.273143438453713e-05,
      "loss": 3.2134,
      "step": 14290
    },
    {
      "epoch": 7.273652085452696,
      "grad_norm": 18.71294403076172,
      "learning_rate": 4.272634791454731e-05,
      "loss": 3.2272,
      "step": 14300
    },
    {
      "epoch": 7.278738555442523,
      "grad_norm": 18.633392333984375,
      "learning_rate": 4.2721261444557484e-05,
      "loss": 3.0639,
      "step": 14310
    },
    {
      "epoch": 7.28382502543235,
      "grad_norm": 16.586023330688477,
      "learning_rate": 4.2716174974567654e-05,
      "loss": 3.1534,
      "step": 14320
    },
    {
      "epoch": 7.288911495422177,
      "grad_norm": 14.14828872680664,
      "learning_rate": 4.2711088504577824e-05,
      "loss": 3.0799,
      "step": 14330
    },
    {
      "epoch": 7.293997965412004,
      "grad_norm": 14.213943481445312,
      "learning_rate": 4.2706002034588e-05,
      "loss": 3.1296,
      "step": 14340
    },
    {
      "epoch": 7.299084435401831,
      "grad_norm": 14.60071086883545,
      "learning_rate": 4.270091556459817e-05,
      "loss": 3.1788,
      "step": 14350
    },
    {
      "epoch": 7.304170905391659,
      "grad_norm": 12.082286834716797,
      "learning_rate": 4.269582909460834e-05,
      "loss": 3.1568,
      "step": 14360
    },
    {
      "epoch": 7.309257375381486,
      "grad_norm": 17.383146286010742,
      "learning_rate": 4.269074262461852e-05,
      "loss": 3.115,
      "step": 14370
    },
    {
      "epoch": 7.3143438453713125,
      "grad_norm": 10.862799644470215,
      "learning_rate": 4.268565615462869e-05,
      "loss": 3.1859,
      "step": 14380
    },
    {
      "epoch": 7.3194303153611395,
      "grad_norm": 13.931792259216309,
      "learning_rate": 4.2680569684638863e-05,
      "loss": 3.0904,
      "step": 14390
    },
    {
      "epoch": 7.3245167853509665,
      "grad_norm": 14.339019775390625,
      "learning_rate": 4.267548321464904e-05,
      "loss": 3.0807,
      "step": 14400
    },
    {
      "epoch": 7.3296032553407935,
      "grad_norm": 10.957094192504883,
      "learning_rate": 4.267039674465921e-05,
      "loss": 3.1869,
      "step": 14410
    },
    {
      "epoch": 7.3346897253306205,
      "grad_norm": 14.775540351867676,
      "learning_rate": 4.266531027466938e-05,
      "loss": 3.1146,
      "step": 14420
    },
    {
      "epoch": 7.3397761953204474,
      "grad_norm": 11.085357666015625,
      "learning_rate": 4.2660223804679557e-05,
      "loss": 3.1948,
      "step": 14430
    },
    {
      "epoch": 7.344862665310274,
      "grad_norm": 12.785884857177734,
      "learning_rate": 4.2655137334689726e-05,
      "loss": 3.1398,
      "step": 14440
    },
    {
      "epoch": 7.349949135300101,
      "grad_norm": 13.623847961425781,
      "learning_rate": 4.26500508646999e-05,
      "loss": 3.1509,
      "step": 14450
    },
    {
      "epoch": 7.355035605289928,
      "grad_norm": 12.327385902404785,
      "learning_rate": 4.264496439471007e-05,
      "loss": 3.1291,
      "step": 14460
    },
    {
      "epoch": 7.360122075279756,
      "grad_norm": 14.8341064453125,
      "learning_rate": 4.263987792472024e-05,
      "loss": 3.0594,
      "step": 14470
    },
    {
      "epoch": 7.365208545269583,
      "grad_norm": 17.314939498901367,
      "learning_rate": 4.263479145473042e-05,
      "loss": 3.08,
      "step": 14480
    },
    {
      "epoch": 7.37029501525941,
      "grad_norm": 18.213871002197266,
      "learning_rate": 4.262970498474059e-05,
      "loss": 3.1665,
      "step": 14490
    },
    {
      "epoch": 7.375381485249237,
      "grad_norm": 12.707700729370117,
      "learning_rate": 4.2624618514750766e-05,
      "loss": 3.0909,
      "step": 14500
    },
    {
      "epoch": 7.380467955239064,
      "grad_norm": 14.163589477539062,
      "learning_rate": 4.261953204476094e-05,
      "loss": 3.1474,
      "step": 14510
    },
    {
      "epoch": 7.385554425228891,
      "grad_norm": 11.90768051147461,
      "learning_rate": 4.261444557477111e-05,
      "loss": 3.1287,
      "step": 14520
    },
    {
      "epoch": 7.390640895218718,
      "grad_norm": 12.82662296295166,
      "learning_rate": 4.260935910478128e-05,
      "loss": 3.1558,
      "step": 14530
    },
    {
      "epoch": 7.395727365208545,
      "grad_norm": 12.264856338500977,
      "learning_rate": 4.260427263479146e-05,
      "loss": 3.1263,
      "step": 14540
    },
    {
      "epoch": 7.400813835198372,
      "grad_norm": 16.402250289916992,
      "learning_rate": 4.259918616480163e-05,
      "loss": 3.1685,
      "step": 14550
    },
    {
      "epoch": 7.405900305188199,
      "grad_norm": 16.401206970214844,
      "learning_rate": 4.25940996948118e-05,
      "loss": 3.0859,
      "step": 14560
    },
    {
      "epoch": 7.410986775178026,
      "grad_norm": 14.27571964263916,
      "learning_rate": 4.2589013224821976e-05,
      "loss": 3.1113,
      "step": 14570
    },
    {
      "epoch": 7.416073245167853,
      "grad_norm": 14.453121185302734,
      "learning_rate": 4.2583926754832146e-05,
      "loss": 3.1678,
      "step": 14580
    },
    {
      "epoch": 7.421159715157681,
      "grad_norm": 13.699223518371582,
      "learning_rate": 4.257884028484232e-05,
      "loss": 3.1274,
      "step": 14590
    },
    {
      "epoch": 7.426246185147508,
      "grad_norm": 10.896016120910645,
      "learning_rate": 4.25737538148525e-05,
      "loss": 3.1134,
      "step": 14600
    },
    {
      "epoch": 7.431332655137335,
      "grad_norm": 13.978320121765137,
      "learning_rate": 4.256866734486267e-05,
      "loss": 3.0863,
      "step": 14610
    },
    {
      "epoch": 7.436419125127162,
      "grad_norm": 12.28719425201416,
      "learning_rate": 4.256358087487284e-05,
      "loss": 3.1178,
      "step": 14620
    },
    {
      "epoch": 7.441505595116989,
      "grad_norm": 12.902549743652344,
      "learning_rate": 4.2558494404883015e-05,
      "loss": 3.124,
      "step": 14630
    },
    {
      "epoch": 7.446592065106816,
      "grad_norm": 13.359137535095215,
      "learning_rate": 4.2553407934893185e-05,
      "loss": 3.1728,
      "step": 14640
    },
    {
      "epoch": 7.451678535096643,
      "grad_norm": 12.950918197631836,
      "learning_rate": 4.2548321464903355e-05,
      "loss": 3.0339,
      "step": 14650
    },
    {
      "epoch": 7.45676500508647,
      "grad_norm": 12.69457721710205,
      "learning_rate": 4.254323499491353e-05,
      "loss": 3.1195,
      "step": 14660
    },
    {
      "epoch": 7.461851475076297,
      "grad_norm": 11.209867477416992,
      "learning_rate": 4.25381485249237e-05,
      "loss": 3.2216,
      "step": 14670
    },
    {
      "epoch": 7.466937945066124,
      "grad_norm": 14.507238388061523,
      "learning_rate": 4.253306205493388e-05,
      "loss": 3.0856,
      "step": 14680
    },
    {
      "epoch": 7.472024415055952,
      "grad_norm": 14.675912857055664,
      "learning_rate": 4.2527975584944055e-05,
      "loss": 3.1398,
      "step": 14690
    },
    {
      "epoch": 7.477110885045779,
      "grad_norm": 17.426368713378906,
      "learning_rate": 4.2522889114954225e-05,
      "loss": 3.1454,
      "step": 14700
    },
    {
      "epoch": 7.482197355035606,
      "grad_norm": 12.300128936767578,
      "learning_rate": 4.2517802644964395e-05,
      "loss": 3.1199,
      "step": 14710
    },
    {
      "epoch": 7.4872838250254325,
      "grad_norm": 14.484045028686523,
      "learning_rate": 4.251271617497457e-05,
      "loss": 3.1939,
      "step": 14720
    },
    {
      "epoch": 7.4923702950152595,
      "grad_norm": 11.753674507141113,
      "learning_rate": 4.250762970498474e-05,
      "loss": 3.1198,
      "step": 14730
    },
    {
      "epoch": 7.4974567650050865,
      "grad_norm": 13.926111221313477,
      "learning_rate": 4.250254323499492e-05,
      "loss": 3.1162,
      "step": 14740
    },
    {
      "epoch": 7.5025432349949135,
      "grad_norm": 11.53599739074707,
      "learning_rate": 4.249745676500509e-05,
      "loss": 3.1822,
      "step": 14750
    },
    {
      "epoch": 7.5076297049847405,
      "grad_norm": 16.282089233398438,
      "learning_rate": 4.249237029501526e-05,
      "loss": 3.1576,
      "step": 14760
    },
    {
      "epoch": 7.5127161749745675,
      "grad_norm": 13.711163520812988,
      "learning_rate": 4.2487283825025435e-05,
      "loss": 3.1381,
      "step": 14770
    },
    {
      "epoch": 7.517802644964394,
      "grad_norm": 10.966619491577148,
      "learning_rate": 4.2482197355035604e-05,
      "loss": 3.1868,
      "step": 14780
    },
    {
      "epoch": 7.522889114954221,
      "grad_norm": 12.827094078063965,
      "learning_rate": 4.247711088504578e-05,
      "loss": 3.0348,
      "step": 14790
    },
    {
      "epoch": 7.527975584944048,
      "grad_norm": 13.764524459838867,
      "learning_rate": 4.247202441505596e-05,
      "loss": 3.1161,
      "step": 14800
    },
    {
      "epoch": 7.533062054933876,
      "grad_norm": 13.45412540435791,
      "learning_rate": 4.246693794506613e-05,
      "loss": 3.0782,
      "step": 14810
    },
    {
      "epoch": 7.538148524923703,
      "grad_norm": 14.882996559143066,
      "learning_rate": 4.24618514750763e-05,
      "loss": 3.1937,
      "step": 14820
    },
    {
      "epoch": 7.54323499491353,
      "grad_norm": 17.153528213500977,
      "learning_rate": 4.2456765005086474e-05,
      "loss": 3.1149,
      "step": 14830
    },
    {
      "epoch": 7.548321464903357,
      "grad_norm": 16.371015548706055,
      "learning_rate": 4.2451678535096644e-05,
      "loss": 3.1369,
      "step": 14840
    },
    {
      "epoch": 7.553407934893184,
      "grad_norm": 15.135963439941406,
      "learning_rate": 4.2446592065106814e-05,
      "loss": 3.0992,
      "step": 14850
    },
    {
      "epoch": 7.558494404883011,
      "grad_norm": 18.329097747802734,
      "learning_rate": 4.244150559511699e-05,
      "loss": 3.1539,
      "step": 14860
    },
    {
      "epoch": 7.563580874872838,
      "grad_norm": 12.744379997253418,
      "learning_rate": 4.243641912512716e-05,
      "loss": 3.1457,
      "step": 14870
    },
    {
      "epoch": 7.568667344862665,
      "grad_norm": 14.317986488342285,
      "learning_rate": 4.243133265513734e-05,
      "loss": 3.1445,
      "step": 14880
    },
    {
      "epoch": 7.573753814852492,
      "grad_norm": 11.666365623474121,
      "learning_rate": 4.2426246185147514e-05,
      "loss": 3.1158,
      "step": 14890
    },
    {
      "epoch": 7.578840284842319,
      "grad_norm": 11.018901824951172,
      "learning_rate": 4.2421159715157684e-05,
      "loss": 3.0915,
      "step": 14900
    },
    {
      "epoch": 7.583926754832147,
      "grad_norm": 12.043902397155762,
      "learning_rate": 4.2416073245167854e-05,
      "loss": 3.1124,
      "step": 14910
    },
    {
      "epoch": 7.589013224821974,
      "grad_norm": 12.533592224121094,
      "learning_rate": 4.241098677517803e-05,
      "loss": 3.1635,
      "step": 14920
    },
    {
      "epoch": 7.594099694811801,
      "grad_norm": 16.176721572875977,
      "learning_rate": 4.24059003051882e-05,
      "loss": 3.1795,
      "step": 14930
    },
    {
      "epoch": 7.599186164801628,
      "grad_norm": 12.955118179321289,
      "learning_rate": 4.240081383519837e-05,
      "loss": 3.1189,
      "step": 14940
    },
    {
      "epoch": 7.604272634791455,
      "grad_norm": 10.760583877563477,
      "learning_rate": 4.239572736520855e-05,
      "loss": 3.1158,
      "step": 14950
    },
    {
      "epoch": 7.609359104781282,
      "grad_norm": 11.775084495544434,
      "learning_rate": 4.239064089521872e-05,
      "loss": 3.0234,
      "step": 14960
    },
    {
      "epoch": 7.614445574771109,
      "grad_norm": 13.297082901000977,
      "learning_rate": 4.2385554425228893e-05,
      "loss": 3.1535,
      "step": 14970
    },
    {
      "epoch": 7.619532044760936,
      "grad_norm": 16.160003662109375,
      "learning_rate": 4.238046795523907e-05,
      "loss": 3.0799,
      "step": 14980
    },
    {
      "epoch": 7.624618514750763,
      "grad_norm": 13.48473834991455,
      "learning_rate": 4.237538148524924e-05,
      "loss": 3.108,
      "step": 14990
    },
    {
      "epoch": 7.62970498474059,
      "grad_norm": 12.865646362304688,
      "learning_rate": 4.237029501525942e-05,
      "loss": 3.1305,
      "step": 15000
    },
    {
      "epoch": 7.634791454730417,
      "grad_norm": 11.849969863891602,
      "learning_rate": 4.2365208545269587e-05,
      "loss": 3.1121,
      "step": 15010
    },
    {
      "epoch": 7.639877924720244,
      "grad_norm": 13.343849182128906,
      "learning_rate": 4.2360122075279756e-05,
      "loss": 3.1001,
      "step": 15020
    },
    {
      "epoch": 7.644964394710071,
      "grad_norm": 12.646707534790039,
      "learning_rate": 4.235503560528993e-05,
      "loss": 3.0467,
      "step": 15030
    },
    {
      "epoch": 7.650050864699899,
      "grad_norm": 13.415116310119629,
      "learning_rate": 4.23499491353001e-05,
      "loss": 3.2047,
      "step": 15040
    },
    {
      "epoch": 7.655137334689726,
      "grad_norm": 11.298495292663574,
      "learning_rate": 4.234486266531027e-05,
      "loss": 3.2142,
      "step": 15050
    },
    {
      "epoch": 7.6602238046795526,
      "grad_norm": 13.055991172790527,
      "learning_rate": 4.233977619532045e-05,
      "loss": 3.0784,
      "step": 15060
    },
    {
      "epoch": 7.6653102746693795,
      "grad_norm": 14.5846586227417,
      "learning_rate": 4.2334689725330626e-05,
      "loss": 3.012,
      "step": 15070
    },
    {
      "epoch": 7.6703967446592065,
      "grad_norm": 13.172313690185547,
      "learning_rate": 4.2329603255340796e-05,
      "loss": 3.073,
      "step": 15080
    },
    {
      "epoch": 7.6754832146490335,
      "grad_norm": 11.72507381439209,
      "learning_rate": 4.232451678535097e-05,
      "loss": 3.0777,
      "step": 15090
    },
    {
      "epoch": 7.6805696846388605,
      "grad_norm": 12.766185760498047,
      "learning_rate": 4.231943031536114e-05,
      "loss": 3.1773,
      "step": 15100
    },
    {
      "epoch": 7.6856561546286875,
      "grad_norm": 13.787973403930664,
      "learning_rate": 4.231434384537131e-05,
      "loss": 3.1351,
      "step": 15110
    },
    {
      "epoch": 7.690742624618514,
      "grad_norm": 20.632734298706055,
      "learning_rate": 4.230925737538149e-05,
      "loss": 3.1055,
      "step": 15120
    },
    {
      "epoch": 7.695829094608342,
      "grad_norm": 10.45496654510498,
      "learning_rate": 4.230417090539166e-05,
      "loss": 3.1359,
      "step": 15130
    },
    {
      "epoch": 7.700915564598169,
      "grad_norm": 12.674958229064941,
      "learning_rate": 4.229908443540183e-05,
      "loss": 3.1586,
      "step": 15140
    },
    {
      "epoch": 7.706002034587996,
      "grad_norm": 12.822318077087402,
      "learning_rate": 4.2293997965412006e-05,
      "loss": 3.1695,
      "step": 15150
    },
    {
      "epoch": 7.711088504577823,
      "grad_norm": 12.608235359191895,
      "learning_rate": 4.2288911495422176e-05,
      "loss": 3.0211,
      "step": 15160
    },
    {
      "epoch": 7.71617497456765,
      "grad_norm": 14.259943962097168,
      "learning_rate": 4.228382502543235e-05,
      "loss": 3.1182,
      "step": 15170
    },
    {
      "epoch": 7.721261444557477,
      "grad_norm": 11.790283203125,
      "learning_rate": 4.227873855544253e-05,
      "loss": 3.1739,
      "step": 15180
    },
    {
      "epoch": 7.726347914547304,
      "grad_norm": 13.168200492858887,
      "learning_rate": 4.22736520854527e-05,
      "loss": 3.1573,
      "step": 15190
    },
    {
      "epoch": 7.731434384537131,
      "grad_norm": 16.35390281677246,
      "learning_rate": 4.226856561546287e-05,
      "loss": 3.0771,
      "step": 15200
    },
    {
      "epoch": 7.736520854526958,
      "grad_norm": 12.763242721557617,
      "learning_rate": 4.2263479145473045e-05,
      "loss": 3.0374,
      "step": 15210
    },
    {
      "epoch": 7.741607324516785,
      "grad_norm": 14.98311996459961,
      "learning_rate": 4.2258392675483215e-05,
      "loss": 3.1253,
      "step": 15220
    },
    {
      "epoch": 7.746693794506612,
      "grad_norm": 17.178354263305664,
      "learning_rate": 4.2253306205493385e-05,
      "loss": 3.0396,
      "step": 15230
    },
    {
      "epoch": 7.751780264496439,
      "grad_norm": 11.178521156311035,
      "learning_rate": 4.224821973550356e-05,
      "loss": 3.1013,
      "step": 15240
    },
    {
      "epoch": 7.756866734486266,
      "grad_norm": 10.84069538116455,
      "learning_rate": 4.224313326551373e-05,
      "loss": 3.092,
      "step": 15250
    },
    {
      "epoch": 7.761953204476094,
      "grad_norm": 12.018199920654297,
      "learning_rate": 4.223804679552391e-05,
      "loss": 3.0974,
      "step": 15260
    },
    {
      "epoch": 7.767039674465921,
      "grad_norm": 13.51475715637207,
      "learning_rate": 4.2232960325534085e-05,
      "loss": 3.0437,
      "step": 15270
    },
    {
      "epoch": 7.772126144455748,
      "grad_norm": 10.817480087280273,
      "learning_rate": 4.2227873855544255e-05,
      "loss": 3.143,
      "step": 15280
    },
    {
      "epoch": 7.777212614445575,
      "grad_norm": 11.931266784667969,
      "learning_rate": 4.222278738555443e-05,
      "loss": 3.1056,
      "step": 15290
    },
    {
      "epoch": 7.782299084435402,
      "grad_norm": 13.275988578796387,
      "learning_rate": 4.22177009155646e-05,
      "loss": 3.1208,
      "step": 15300
    },
    {
      "epoch": 7.787385554425229,
      "grad_norm": 14.619327545166016,
      "learning_rate": 4.221261444557477e-05,
      "loss": 3.0382,
      "step": 15310
    },
    {
      "epoch": 7.792472024415056,
      "grad_norm": 13.204493522644043,
      "learning_rate": 4.220752797558495e-05,
      "loss": 3.0894,
      "step": 15320
    },
    {
      "epoch": 7.797558494404883,
      "grad_norm": 15.460308074951172,
      "learning_rate": 4.220244150559512e-05,
      "loss": 3.1449,
      "step": 15330
    },
    {
      "epoch": 7.80264496439471,
      "grad_norm": 15.320133209228516,
      "learning_rate": 4.219735503560529e-05,
      "loss": 3.0926,
      "step": 15340
    },
    {
      "epoch": 7.807731434384537,
      "grad_norm": 19.33114242553711,
      "learning_rate": 4.2192268565615465e-05,
      "loss": 3.0881,
      "step": 15350
    },
    {
      "epoch": 7.812817904374365,
      "grad_norm": 12.794787406921387,
      "learning_rate": 4.218718209562564e-05,
      "loss": 3.1182,
      "step": 15360
    },
    {
      "epoch": 7.817904374364192,
      "grad_norm": 13.668291091918945,
      "learning_rate": 4.218209562563581e-05,
      "loss": 3.0886,
      "step": 15370
    },
    {
      "epoch": 7.822990844354019,
      "grad_norm": 13.859365463256836,
      "learning_rate": 4.217700915564599e-05,
      "loss": 3.0834,
      "step": 15380
    },
    {
      "epoch": 7.828077314343846,
      "grad_norm": 12.810121536254883,
      "learning_rate": 4.217192268565616e-05,
      "loss": 3.166,
      "step": 15390
    },
    {
      "epoch": 7.8331637843336726,
      "grad_norm": 14.90147876739502,
      "learning_rate": 4.216683621566633e-05,
      "loss": 3.1434,
      "step": 15400
    },
    {
      "epoch": 7.8382502543234995,
      "grad_norm": 12.19420337677002,
      "learning_rate": 4.2161749745676504e-05,
      "loss": 3.1797,
      "step": 15410
    },
    {
      "epoch": 7.8433367243133265,
      "grad_norm": 20.570558547973633,
      "learning_rate": 4.2156663275686674e-05,
      "loss": 3.092,
      "step": 15420
    },
    {
      "epoch": 7.8484231943031535,
      "grad_norm": 15.110376358032227,
      "learning_rate": 4.2151576805696844e-05,
      "loss": 3.0657,
      "step": 15430
    },
    {
      "epoch": 7.8535096642929805,
      "grad_norm": 14.438309669494629,
      "learning_rate": 4.214649033570702e-05,
      "loss": 3.1599,
      "step": 15440
    },
    {
      "epoch": 7.8585961342828075,
      "grad_norm": 11.084553718566895,
      "learning_rate": 4.214140386571719e-05,
      "loss": 3.0636,
      "step": 15450
    },
    {
      "epoch": 7.863682604272634,
      "grad_norm": 16.198122024536133,
      "learning_rate": 4.213631739572737e-05,
      "loss": 3.1204,
      "step": 15460
    },
    {
      "epoch": 7.868769074262461,
      "grad_norm": 16.28600311279297,
      "learning_rate": 4.2131230925737544e-05,
      "loss": 3.143,
      "step": 15470
    },
    {
      "epoch": 7.873855544252289,
      "grad_norm": 14.360342979431152,
      "learning_rate": 4.2126144455747714e-05,
      "loss": 3.0917,
      "step": 15480
    },
    {
      "epoch": 7.878942014242116,
      "grad_norm": 14.336257934570312,
      "learning_rate": 4.2121057985757884e-05,
      "loss": 3.0343,
      "step": 15490
    },
    {
      "epoch": 7.884028484231943,
      "grad_norm": 14.251696586608887,
      "learning_rate": 4.211597151576806e-05,
      "loss": 3.1491,
      "step": 15500
    },
    {
      "epoch": 7.88911495422177,
      "grad_norm": 12.308575630187988,
      "learning_rate": 4.211088504577823e-05,
      "loss": 3.0973,
      "step": 15510
    },
    {
      "epoch": 7.894201424211597,
      "grad_norm": 11.95673942565918,
      "learning_rate": 4.21057985757884e-05,
      "loss": 3.0855,
      "step": 15520
    },
    {
      "epoch": 7.899287894201424,
      "grad_norm": 11.142349243164062,
      "learning_rate": 4.210071210579858e-05,
      "loss": 3.0459,
      "step": 15530
    },
    {
      "epoch": 7.904374364191251,
      "grad_norm": 16.442302703857422,
      "learning_rate": 4.209562563580875e-05,
      "loss": 3.1911,
      "step": 15540
    },
    {
      "epoch": 7.909460834181078,
      "grad_norm": 12.998251914978027,
      "learning_rate": 4.2090539165818923e-05,
      "loss": 3.1741,
      "step": 15550
    },
    {
      "epoch": 7.914547304170905,
      "grad_norm": 15.988588333129883,
      "learning_rate": 4.20854526958291e-05,
      "loss": 3.1103,
      "step": 15560
    },
    {
      "epoch": 7.919633774160732,
      "grad_norm": 12.63163948059082,
      "learning_rate": 4.208036622583927e-05,
      "loss": 3.189,
      "step": 15570
    },
    {
      "epoch": 7.92472024415056,
      "grad_norm": 13.404878616333008,
      "learning_rate": 4.207527975584945e-05,
      "loss": 3.138,
      "step": 15580
    },
    {
      "epoch": 7.929806714140387,
      "grad_norm": 12.717023849487305,
      "learning_rate": 4.2070193285859617e-05,
      "loss": 3.0766,
      "step": 15590
    },
    {
      "epoch": 7.934893184130214,
      "grad_norm": 15.099451065063477,
      "learning_rate": 4.2065106815869786e-05,
      "loss": 3.0228,
      "step": 15600
    },
    {
      "epoch": 7.939979654120041,
      "grad_norm": 17.43771743774414,
      "learning_rate": 4.206002034587996e-05,
      "loss": 3.1099,
      "step": 15610
    },
    {
      "epoch": 7.945066124109868,
      "grad_norm": 13.83143138885498,
      "learning_rate": 4.205493387589013e-05,
      "loss": 3.1068,
      "step": 15620
    },
    {
      "epoch": 7.950152594099695,
      "grad_norm": 13.133318901062012,
      "learning_rate": 4.20498474059003e-05,
      "loss": 3.0664,
      "step": 15630
    },
    {
      "epoch": 7.955239064089522,
      "grad_norm": 13.834551811218262,
      "learning_rate": 4.204476093591048e-05,
      "loss": 3.0972,
      "step": 15640
    },
    {
      "epoch": 7.960325534079349,
      "grad_norm": 14.416163444519043,
      "learning_rate": 4.2039674465920656e-05,
      "loss": 3.0985,
      "step": 15650
    },
    {
      "epoch": 7.965412004069176,
      "grad_norm": 13.6142578125,
      "learning_rate": 4.2034587995930826e-05,
      "loss": 3.07,
      "step": 15660
    },
    {
      "epoch": 7.970498474059003,
      "grad_norm": 14.86745834350586,
      "learning_rate": 4.2029501525941e-05,
      "loss": 3.1132,
      "step": 15670
    },
    {
      "epoch": 7.97558494404883,
      "grad_norm": 14.024497985839844,
      "learning_rate": 4.202441505595117e-05,
      "loss": 3.0128,
      "step": 15680
    },
    {
      "epoch": 7.980671414038657,
      "grad_norm": 13.328909873962402,
      "learning_rate": 4.201932858596134e-05,
      "loss": 3.1484,
      "step": 15690
    },
    {
      "epoch": 7.985757884028484,
      "grad_norm": 11.27929973602295,
      "learning_rate": 4.201424211597152e-05,
      "loss": 3.0767,
      "step": 15700
    },
    {
      "epoch": 7.990844354018312,
      "grad_norm": 14.646596908569336,
      "learning_rate": 4.200915564598169e-05,
      "loss": 3.1256,
      "step": 15710
    },
    {
      "epoch": 7.995930824008139,
      "grad_norm": 20.515239715576172,
      "learning_rate": 4.200406917599186e-05,
      "loss": 3.155,
      "step": 15720
    },
    {
      "epoch": 8.0,
      "eval_loss": 3.6886837482452393,
      "eval_runtime": 2.8378,
      "eval_samples_per_second": 977.885,
      "eval_steps_per_second": 122.28,
      "step": 15728
    },
    {
      "epoch": 8.001017293997965,
      "grad_norm": 12.637009620666504,
      "learning_rate": 4.1998982706002036e-05,
      "loss": 3.102,
      "step": 15730
    },
    {
      "epoch": 8.006103763987792,
      "grad_norm": 13.154071807861328,
      "learning_rate": 4.199389623601221e-05,
      "loss": 2.9916,
      "step": 15740
    },
    {
      "epoch": 8.011190233977619,
      "grad_norm": 14.610821723937988,
      "learning_rate": 4.198880976602238e-05,
      "loss": 3.0655,
      "step": 15750
    },
    {
      "epoch": 8.016276703967447,
      "grad_norm": 14.439972877502441,
      "learning_rate": 4.198372329603256e-05,
      "loss": 3.0511,
      "step": 15760
    },
    {
      "epoch": 8.021363173957274,
      "grad_norm": 15.612303733825684,
      "learning_rate": 4.197863682604273e-05,
      "loss": 3.043,
      "step": 15770
    },
    {
      "epoch": 8.026449643947101,
      "grad_norm": 14.108195304870605,
      "learning_rate": 4.19735503560529e-05,
      "loss": 3.0339,
      "step": 15780
    },
    {
      "epoch": 8.031536113936928,
      "grad_norm": 11.365588188171387,
      "learning_rate": 4.1968463886063075e-05,
      "loss": 3.0687,
      "step": 15790
    },
    {
      "epoch": 8.036622583926755,
      "grad_norm": 13.361722946166992,
      "learning_rate": 4.1963377416073245e-05,
      "loss": 3.0641,
      "step": 15800
    },
    {
      "epoch": 8.041709053916582,
      "grad_norm": 12.121826171875,
      "learning_rate": 4.195829094608342e-05,
      "loss": 3.0501,
      "step": 15810
    },
    {
      "epoch": 8.04679552390641,
      "grad_norm": 13.435758590698242,
      "learning_rate": 4.195320447609359e-05,
      "loss": 3.0012,
      "step": 15820
    },
    {
      "epoch": 8.051881993896236,
      "grad_norm": 13.5836763381958,
      "learning_rate": 4.194811800610376e-05,
      "loss": 3.1009,
      "step": 15830
    },
    {
      "epoch": 8.056968463886063,
      "grad_norm": 14.182551383972168,
      "learning_rate": 4.194303153611394e-05,
      "loss": 3.1167,
      "step": 15840
    },
    {
      "epoch": 8.06205493387589,
      "grad_norm": 18.525503158569336,
      "learning_rate": 4.1937945066124115e-05,
      "loss": 3.0816,
      "step": 15850
    },
    {
      "epoch": 8.067141403865717,
      "grad_norm": 14.17665958404541,
      "learning_rate": 4.1932858596134285e-05,
      "loss": 3.0807,
      "step": 15860
    },
    {
      "epoch": 8.072227873855544,
      "grad_norm": 13.49341869354248,
      "learning_rate": 4.192777212614446e-05,
      "loss": 3.0692,
      "step": 15870
    },
    {
      "epoch": 8.077314343845371,
      "grad_norm": 13.0217924118042,
      "learning_rate": 4.192268565615463e-05,
      "loss": 2.9911,
      "step": 15880
    },
    {
      "epoch": 8.082400813835198,
      "grad_norm": 15.189833641052246,
      "learning_rate": 4.19175991861648e-05,
      "loss": 3.0423,
      "step": 15890
    },
    {
      "epoch": 8.087487283825025,
      "grad_norm": 17.26044273376465,
      "learning_rate": 4.191251271617498e-05,
      "loss": 3.0759,
      "step": 15900
    },
    {
      "epoch": 8.092573753814852,
      "grad_norm": 11.384991645812988,
      "learning_rate": 4.190742624618515e-05,
      "loss": 3.0636,
      "step": 15910
    },
    {
      "epoch": 8.097660223804679,
      "grad_norm": 12.222345352172852,
      "learning_rate": 4.190233977619532e-05,
      "loss": 3.1303,
      "step": 15920
    },
    {
      "epoch": 8.102746693794506,
      "grad_norm": 18.95274543762207,
      "learning_rate": 4.1897253306205495e-05,
      "loss": 3.0172,
      "step": 15930
    },
    {
      "epoch": 8.107833163784333,
      "grad_norm": 14.993569374084473,
      "learning_rate": 4.189216683621567e-05,
      "loss": 3.0705,
      "step": 15940
    },
    {
      "epoch": 8.11291963377416,
      "grad_norm": 14.437776565551758,
      "learning_rate": 4.188708036622584e-05,
      "loss": 3.1773,
      "step": 15950
    },
    {
      "epoch": 8.118006103763987,
      "grad_norm": 14.799071311950684,
      "learning_rate": 4.188199389623602e-05,
      "loss": 3.0196,
      "step": 15960
    },
    {
      "epoch": 8.123092573753814,
      "grad_norm": 14.746389389038086,
      "learning_rate": 4.187690742624619e-05,
      "loss": 3.1117,
      "step": 15970
    },
    {
      "epoch": 8.128179043743643,
      "grad_norm": 16.07479476928711,
      "learning_rate": 4.187182095625636e-05,
      "loss": 2.9889,
      "step": 15980
    },
    {
      "epoch": 8.13326551373347,
      "grad_norm": 11.944884300231934,
      "learning_rate": 4.1866734486266534e-05,
      "loss": 3.0337,
      "step": 15990
    },
    {
      "epoch": 8.138351983723297,
      "grad_norm": 14.749932289123535,
      "learning_rate": 4.1861648016276704e-05,
      "loss": 3.1376,
      "step": 16000
    },
    {
      "epoch": 8.143438453713124,
      "grad_norm": 15.342531204223633,
      "learning_rate": 4.1856561546286874e-05,
      "loss": 3.0001,
      "step": 16010
    },
    {
      "epoch": 8.14852492370295,
      "grad_norm": 14.266587257385254,
      "learning_rate": 4.185147507629705e-05,
      "loss": 2.9839,
      "step": 16020
    },
    {
      "epoch": 8.153611393692778,
      "grad_norm": 14.076037406921387,
      "learning_rate": 4.184638860630723e-05,
      "loss": 3.0671,
      "step": 16030
    },
    {
      "epoch": 8.158697863682605,
      "grad_norm": 13.248229026794434,
      "learning_rate": 4.18413021363174e-05,
      "loss": 3.0434,
      "step": 16040
    },
    {
      "epoch": 8.163784333672432,
      "grad_norm": 15.478142738342285,
      "learning_rate": 4.1836215666327574e-05,
      "loss": 3.0572,
      "step": 16050
    },
    {
      "epoch": 8.168870803662259,
      "grad_norm": 16.607067108154297,
      "learning_rate": 4.1831129196337744e-05,
      "loss": 3.1647,
      "step": 16060
    },
    {
      "epoch": 8.173957273652086,
      "grad_norm": 14.282708168029785,
      "learning_rate": 4.182604272634792e-05,
      "loss": 3.0236,
      "step": 16070
    },
    {
      "epoch": 8.179043743641913,
      "grad_norm": 13.37510871887207,
      "learning_rate": 4.182095625635809e-05,
      "loss": 3.0515,
      "step": 16080
    },
    {
      "epoch": 8.18413021363174,
      "grad_norm": 14.094108581542969,
      "learning_rate": 4.181586978636826e-05,
      "loss": 3.0619,
      "step": 16090
    },
    {
      "epoch": 8.189216683621567,
      "grad_norm": 17.17620849609375,
      "learning_rate": 4.181078331637844e-05,
      "loss": 2.9814,
      "step": 16100
    },
    {
      "epoch": 8.194303153611393,
      "grad_norm": 15.298035621643066,
      "learning_rate": 4.180569684638861e-05,
      "loss": 2.9841,
      "step": 16110
    },
    {
      "epoch": 8.19938962360122,
      "grad_norm": 15.488139152526855,
      "learning_rate": 4.180061037639878e-05,
      "loss": 3.0332,
      "step": 16120
    },
    {
      "epoch": 8.204476093591047,
      "grad_norm": 17.153911590576172,
      "learning_rate": 4.1795523906408953e-05,
      "loss": 3.001,
      "step": 16130
    },
    {
      "epoch": 8.209562563580874,
      "grad_norm": 13.907596588134766,
      "learning_rate": 4.179043743641913e-05,
      "loss": 3.0585,
      "step": 16140
    },
    {
      "epoch": 8.214649033570701,
      "grad_norm": 15.55334186553955,
      "learning_rate": 4.17853509664293e-05,
      "loss": 3.0952,
      "step": 16150
    },
    {
      "epoch": 8.219735503560528,
      "grad_norm": 11.37863826751709,
      "learning_rate": 4.178026449643948e-05,
      "loss": 3.0835,
      "step": 16160
    },
    {
      "epoch": 8.224821973550355,
      "grad_norm": 13.455548286437988,
      "learning_rate": 4.1775178026449647e-05,
      "loss": 3.074,
      "step": 16170
    },
    {
      "epoch": 8.229908443540182,
      "grad_norm": 15.89254379272461,
      "learning_rate": 4.1770091556459816e-05,
      "loss": 2.9566,
      "step": 16180
    },
    {
      "epoch": 8.23499491353001,
      "grad_norm": 15.12623119354248,
      "learning_rate": 4.176500508646999e-05,
      "loss": 3.1398,
      "step": 16190
    },
    {
      "epoch": 8.240081383519838,
      "grad_norm": 13.66268539428711,
      "learning_rate": 4.175991861648016e-05,
      "loss": 3.0789,
      "step": 16200
    },
    {
      "epoch": 8.245167853509665,
      "grad_norm": 16.75746726989746,
      "learning_rate": 4.175483214649033e-05,
      "loss": 3.1244,
      "step": 16210
    },
    {
      "epoch": 8.250254323499492,
      "grad_norm": 20.295333862304688,
      "learning_rate": 4.174974567650051e-05,
      "loss": 3.0858,
      "step": 16220
    },
    {
      "epoch": 8.255340793489319,
      "grad_norm": 13.554240226745605,
      "learning_rate": 4.1744659206510686e-05,
      "loss": 3.0089,
      "step": 16230
    },
    {
      "epoch": 8.260427263479146,
      "grad_norm": 16.820676803588867,
      "learning_rate": 4.1739572736520856e-05,
      "loss": 3.0357,
      "step": 16240
    },
    {
      "epoch": 8.265513733468973,
      "grad_norm": 16.717021942138672,
      "learning_rate": 4.173448626653103e-05,
      "loss": 3.0597,
      "step": 16250
    },
    {
      "epoch": 8.2706002034588,
      "grad_norm": 11.681320190429688,
      "learning_rate": 4.17293997965412e-05,
      "loss": 3.0346,
      "step": 16260
    },
    {
      "epoch": 8.275686673448627,
      "grad_norm": 12.46370792388916,
      "learning_rate": 4.172431332655137e-05,
      "loss": 3.1423,
      "step": 16270
    },
    {
      "epoch": 8.280773143438454,
      "grad_norm": 13.78438663482666,
      "learning_rate": 4.171922685656155e-05,
      "loss": 3.0632,
      "step": 16280
    },
    {
      "epoch": 8.285859613428281,
      "grad_norm": 14.294203758239746,
      "learning_rate": 4.171414038657172e-05,
      "loss": 3.0625,
      "step": 16290
    },
    {
      "epoch": 8.290946083418108,
      "grad_norm": 14.598856925964355,
      "learning_rate": 4.170905391658189e-05,
      "loss": 3.1277,
      "step": 16300
    },
    {
      "epoch": 8.296032553407935,
      "grad_norm": 11.459656715393066,
      "learning_rate": 4.1703967446592066e-05,
      "loss": 3.0275,
      "step": 16310
    },
    {
      "epoch": 8.301119023397762,
      "grad_norm": 15.358277320861816,
      "learning_rate": 4.169888097660224e-05,
      "loss": 3.1004,
      "step": 16320
    },
    {
      "epoch": 8.306205493387589,
      "grad_norm": 16.611087799072266,
      "learning_rate": 4.169379450661241e-05,
      "loss": 2.9989,
      "step": 16330
    },
    {
      "epoch": 8.311291963377416,
      "grad_norm": 12.898659706115723,
      "learning_rate": 4.168870803662259e-05,
      "loss": 3.0253,
      "step": 16340
    },
    {
      "epoch": 8.316378433367243,
      "grad_norm": 13.557995796203613,
      "learning_rate": 4.168362156663276e-05,
      "loss": 3.0147,
      "step": 16350
    },
    {
      "epoch": 8.32146490335707,
      "grad_norm": 25.38555145263672,
      "learning_rate": 4.1678535096642936e-05,
      "loss": 3.0785,
      "step": 16360
    },
    {
      "epoch": 8.326551373346897,
      "grad_norm": 13.375015258789062,
      "learning_rate": 4.1673448626653105e-05,
      "loss": 3.0061,
      "step": 16370
    },
    {
      "epoch": 8.331637843336724,
      "grad_norm": 16.212560653686523,
      "learning_rate": 4.1668362156663275e-05,
      "loss": 2.934,
      "step": 16380
    },
    {
      "epoch": 8.33672431332655,
      "grad_norm": 13.167869567871094,
      "learning_rate": 4.166327568667345e-05,
      "loss": 3.0356,
      "step": 16390
    },
    {
      "epoch": 8.341810783316378,
      "grad_norm": 15.039643287658691,
      "learning_rate": 4.165818921668362e-05,
      "loss": 2.9529,
      "step": 16400
    },
    {
      "epoch": 8.346897253306205,
      "grad_norm": 13.939568519592285,
      "learning_rate": 4.165310274669379e-05,
      "loss": 3.0303,
      "step": 16410
    },
    {
      "epoch": 8.351983723296033,
      "grad_norm": 13.812487602233887,
      "learning_rate": 4.164801627670397e-05,
      "loss": 3.1332,
      "step": 16420
    },
    {
      "epoch": 8.35707019328586,
      "grad_norm": 14.777557373046875,
      "learning_rate": 4.1642929806714145e-05,
      "loss": 3.0485,
      "step": 16430
    },
    {
      "epoch": 8.362156663275687,
      "grad_norm": 14.81912612915039,
      "learning_rate": 4.1637843336724315e-05,
      "loss": 2.9901,
      "step": 16440
    },
    {
      "epoch": 8.367243133265514,
      "grad_norm": 16.416484832763672,
      "learning_rate": 4.163275686673449e-05,
      "loss": 2.9761,
      "step": 16450
    },
    {
      "epoch": 8.372329603255341,
      "grad_norm": 13.858380317687988,
      "learning_rate": 4.162767039674466e-05,
      "loss": 3.0685,
      "step": 16460
    },
    {
      "epoch": 8.377416073245168,
      "grad_norm": 13.638654708862305,
      "learning_rate": 4.162258392675483e-05,
      "loss": 3.0385,
      "step": 16470
    },
    {
      "epoch": 8.382502543234995,
      "grad_norm": 16.713104248046875,
      "learning_rate": 4.161749745676501e-05,
      "loss": 3.0905,
      "step": 16480
    },
    {
      "epoch": 8.387589013224822,
      "grad_norm": 11.733059883117676,
      "learning_rate": 4.161241098677518e-05,
      "loss": 3.1385,
      "step": 16490
    },
    {
      "epoch": 8.39267548321465,
      "grad_norm": 13.933062553405762,
      "learning_rate": 4.160732451678535e-05,
      "loss": 3.1508,
      "step": 16500
    },
    {
      "epoch": 8.397761953204476,
      "grad_norm": 20.838167190551758,
      "learning_rate": 4.1602238046795525e-05,
      "loss": 3.0273,
      "step": 16510
    },
    {
      "epoch": 8.402848423194303,
      "grad_norm": 11.769674301147461,
      "learning_rate": 4.15971515768057e-05,
      "loss": 3.0678,
      "step": 16520
    },
    {
      "epoch": 8.40793489318413,
      "grad_norm": 13.92647647857666,
      "learning_rate": 4.159206510681587e-05,
      "loss": 3.0044,
      "step": 16530
    },
    {
      "epoch": 8.413021363173957,
      "grad_norm": 15.483283042907715,
      "learning_rate": 4.158697863682605e-05,
      "loss": 3.0909,
      "step": 16540
    },
    {
      "epoch": 8.418107833163784,
      "grad_norm": 12.035265922546387,
      "learning_rate": 4.158189216683622e-05,
      "loss": 3.0194,
      "step": 16550
    },
    {
      "epoch": 8.423194303153611,
      "grad_norm": 14.679373741149902,
      "learning_rate": 4.157680569684639e-05,
      "loss": 3.0988,
      "step": 16560
    },
    {
      "epoch": 8.428280773143438,
      "grad_norm": 17.07221221923828,
      "learning_rate": 4.1571719226856564e-05,
      "loss": 3.0838,
      "step": 16570
    },
    {
      "epoch": 8.433367243133265,
      "grad_norm": 13.36583137512207,
      "learning_rate": 4.1566632756866734e-05,
      "loss": 3.0579,
      "step": 16580
    },
    {
      "epoch": 8.438453713123092,
      "grad_norm": 16.28850746154785,
      "learning_rate": 4.1561546286876904e-05,
      "loss": 3.0554,
      "step": 16590
    },
    {
      "epoch": 8.443540183112919,
      "grad_norm": 10.76805591583252,
      "learning_rate": 4.155645981688708e-05,
      "loss": 3.017,
      "step": 16600
    },
    {
      "epoch": 8.448626653102746,
      "grad_norm": 16.492406845092773,
      "learning_rate": 4.155137334689726e-05,
      "loss": 3.0171,
      "step": 16610
    },
    {
      "epoch": 8.453713123092573,
      "grad_norm": 15.080432891845703,
      "learning_rate": 4.1546286876907434e-05,
      "loss": 3.0135,
      "step": 16620
    },
    {
      "epoch": 8.4587995930824,
      "grad_norm": 14.18274211883545,
      "learning_rate": 4.1541200406917604e-05,
      "loss": 3.0303,
      "step": 16630
    },
    {
      "epoch": 8.463886063072227,
      "grad_norm": 14.142437934875488,
      "learning_rate": 4.1536113936927774e-05,
      "loss": 3.0064,
      "step": 16640
    },
    {
      "epoch": 8.468972533062056,
      "grad_norm": 16.109556198120117,
      "learning_rate": 4.153102746693795e-05,
      "loss": 2.9954,
      "step": 16650
    },
    {
      "epoch": 8.474059003051883,
      "grad_norm": 15.874693870544434,
      "learning_rate": 4.152594099694812e-05,
      "loss": 3.1269,
      "step": 16660
    },
    {
      "epoch": 8.47914547304171,
      "grad_norm": 12.8213529586792,
      "learning_rate": 4.152085452695829e-05,
      "loss": 3.0353,
      "step": 16670
    },
    {
      "epoch": 8.484231943031537,
      "grad_norm": 13.78437614440918,
      "learning_rate": 4.151576805696847e-05,
      "loss": 3.1192,
      "step": 16680
    },
    {
      "epoch": 8.489318413021364,
      "grad_norm": 15.1554536819458,
      "learning_rate": 4.151068158697864e-05,
      "loss": 2.9926,
      "step": 16690
    },
    {
      "epoch": 8.49440488301119,
      "grad_norm": 12.597220420837402,
      "learning_rate": 4.1505595116988814e-05,
      "loss": 3.1206,
      "step": 16700
    },
    {
      "epoch": 8.499491353001018,
      "grad_norm": 22.364492416381836,
      "learning_rate": 4.1500508646998983e-05,
      "loss": 3.1177,
      "step": 16710
    },
    {
      "epoch": 8.504577822990845,
      "grad_norm": 13.480934143066406,
      "learning_rate": 4.149542217700916e-05,
      "loss": 2.9575,
      "step": 16720
    },
    {
      "epoch": 8.509664292980672,
      "grad_norm": 12.310460090637207,
      "learning_rate": 4.149033570701933e-05,
      "loss": 3.0806,
      "step": 16730
    },
    {
      "epoch": 8.514750762970499,
      "grad_norm": 16.259090423583984,
      "learning_rate": 4.148524923702951e-05,
      "loss": 3.0897,
      "step": 16740
    },
    {
      "epoch": 8.519837232960326,
      "grad_norm": 17.375322341918945,
      "learning_rate": 4.1480162767039677e-05,
      "loss": 2.9969,
      "step": 16750
    },
    {
      "epoch": 8.524923702950153,
      "grad_norm": 18.764488220214844,
      "learning_rate": 4.1475076297049846e-05,
      "loss": 3.1115,
      "step": 16760
    },
    {
      "epoch": 8.53001017293998,
      "grad_norm": 17.071884155273438,
      "learning_rate": 4.146998982706002e-05,
      "loss": 3.107,
      "step": 16770
    },
    {
      "epoch": 8.535096642929807,
      "grad_norm": 15.243307113647461,
      "learning_rate": 4.146490335707019e-05,
      "loss": 3.0189,
      "step": 16780
    },
    {
      "epoch": 8.540183112919634,
      "grad_norm": 15.025164604187012,
      "learning_rate": 4.145981688708036e-05,
      "loss": 3.0411,
      "step": 16790
    },
    {
      "epoch": 8.54526958290946,
      "grad_norm": 16.52695655822754,
      "learning_rate": 4.145473041709054e-05,
      "loss": 3.0492,
      "step": 16800
    },
    {
      "epoch": 8.550356052899287,
      "grad_norm": 12.826531410217285,
      "learning_rate": 4.1449643947100716e-05,
      "loss": 3.0147,
      "step": 16810
    },
    {
      "epoch": 8.555442522889114,
      "grad_norm": 15.599091529846191,
      "learning_rate": 4.1444557477110886e-05,
      "loss": 3.0085,
      "step": 16820
    },
    {
      "epoch": 8.560528992878941,
      "grad_norm": 15.831518173217773,
      "learning_rate": 4.143947100712106e-05,
      "loss": 3.0036,
      "step": 16830
    },
    {
      "epoch": 8.565615462868768,
      "grad_norm": 12.98510456085205,
      "learning_rate": 4.143438453713123e-05,
      "loss": 2.9525,
      "step": 16840
    },
    {
      "epoch": 8.570701932858595,
      "grad_norm": 18.806354522705078,
      "learning_rate": 4.14292980671414e-05,
      "loss": 2.9826,
      "step": 16850
    },
    {
      "epoch": 8.575788402848422,
      "grad_norm": 13.773913383483887,
      "learning_rate": 4.142421159715158e-05,
      "loss": 3.0533,
      "step": 16860
    },
    {
      "epoch": 8.580874872838251,
      "grad_norm": 13.1165771484375,
      "learning_rate": 4.141912512716175e-05,
      "loss": 3.0068,
      "step": 16870
    },
    {
      "epoch": 8.585961342828078,
      "grad_norm": 12.612248420715332,
      "learning_rate": 4.1414038657171926e-05,
      "loss": 2.9753,
      "step": 16880
    },
    {
      "epoch": 8.591047812817905,
      "grad_norm": 14.30902099609375,
      "learning_rate": 4.1408952187182096e-05,
      "loss": 3.0349,
      "step": 16890
    },
    {
      "epoch": 8.596134282807732,
      "grad_norm": 16.993919372558594,
      "learning_rate": 4.140386571719227e-05,
      "loss": 3.0095,
      "step": 16900
    },
    {
      "epoch": 8.601220752797559,
      "grad_norm": 14.538847923278809,
      "learning_rate": 4.139877924720245e-05,
      "loss": 3.0296,
      "step": 16910
    },
    {
      "epoch": 8.606307222787386,
      "grad_norm": 14.64323902130127,
      "learning_rate": 4.139369277721262e-05,
      "loss": 2.9864,
      "step": 16920
    },
    {
      "epoch": 8.611393692777213,
      "grad_norm": 15.42750072479248,
      "learning_rate": 4.138860630722279e-05,
      "loss": 3.0241,
      "step": 16930
    },
    {
      "epoch": 8.61648016276704,
      "grad_norm": 17.518138885498047,
      "learning_rate": 4.1383519837232966e-05,
      "loss": 2.9962,
      "step": 16940
    },
    {
      "epoch": 8.621566632756867,
      "grad_norm": 13.055644989013672,
      "learning_rate": 4.1378433367243135e-05,
      "loss": 2.951,
      "step": 16950
    },
    {
      "epoch": 8.626653102746694,
      "grad_norm": 12.5352783203125,
      "learning_rate": 4.1373346897253305e-05,
      "loss": 2.947,
      "step": 16960
    },
    {
      "epoch": 8.631739572736521,
      "grad_norm": 18.288393020629883,
      "learning_rate": 4.136826042726348e-05,
      "loss": 2.9885,
      "step": 16970
    },
    {
      "epoch": 8.636826042726348,
      "grad_norm": 19.644878387451172,
      "learning_rate": 4.136317395727365e-05,
      "loss": 2.9957,
      "step": 16980
    },
    {
      "epoch": 8.641912512716175,
      "grad_norm": 15.78522777557373,
      "learning_rate": 4.135808748728383e-05,
      "loss": 3.0027,
      "step": 16990
    },
    {
      "epoch": 8.646998982706002,
      "grad_norm": 18.540203094482422,
      "learning_rate": 4.1353001017294005e-05,
      "loss": 2.9913,
      "step": 17000
    },
    {
      "epoch": 8.652085452695829,
      "grad_norm": 19.732337951660156,
      "learning_rate": 4.1347914547304175e-05,
      "loss": 2.9948,
      "step": 17010
    },
    {
      "epoch": 8.657171922685656,
      "grad_norm": 17.2221622467041,
      "learning_rate": 4.1342828077314345e-05,
      "loss": 2.9803,
      "step": 17020
    },
    {
      "epoch": 8.662258392675483,
      "grad_norm": 13.746020317077637,
      "learning_rate": 4.133774160732452e-05,
      "loss": 2.9785,
      "step": 17030
    },
    {
      "epoch": 8.66734486266531,
      "grad_norm": 14.687827110290527,
      "learning_rate": 4.133265513733469e-05,
      "loss": 2.9938,
      "step": 17040
    },
    {
      "epoch": 8.672431332655137,
      "grad_norm": 13.609103202819824,
      "learning_rate": 4.132756866734486e-05,
      "loss": 3.0515,
      "step": 17050
    },
    {
      "epoch": 8.677517802644964,
      "grad_norm": 17.70267677307129,
      "learning_rate": 4.132248219735504e-05,
      "loss": 2.9667,
      "step": 17060
    },
    {
      "epoch": 8.68260427263479,
      "grad_norm": 16.130685806274414,
      "learning_rate": 4.131739572736521e-05,
      "loss": 3.0844,
      "step": 17070
    },
    {
      "epoch": 8.687690742624618,
      "grad_norm": 12.200199127197266,
      "learning_rate": 4.131230925737538e-05,
      "loss": 2.9329,
      "step": 17080
    },
    {
      "epoch": 8.692777212614445,
      "grad_norm": 17.966093063354492,
      "learning_rate": 4.1307222787385555e-05,
      "loss": 3.0054,
      "step": 17090
    },
    {
      "epoch": 8.697863682604273,
      "grad_norm": 16.68718719482422,
      "learning_rate": 4.130213631739573e-05,
      "loss": 2.9983,
      "step": 17100
    },
    {
      "epoch": 8.7029501525941,
      "grad_norm": 19.042388916015625,
      "learning_rate": 4.12970498474059e-05,
      "loss": 3.0208,
      "step": 17110
    },
    {
      "epoch": 8.708036622583927,
      "grad_norm": 13.261035919189453,
      "learning_rate": 4.129196337741608e-05,
      "loss": 2.9522,
      "step": 17120
    },
    {
      "epoch": 8.713123092573754,
      "grad_norm": 11.589232444763184,
      "learning_rate": 4.128687690742625e-05,
      "loss": 3.0428,
      "step": 17130
    },
    {
      "epoch": 8.718209562563581,
      "grad_norm": 16.871801376342773,
      "learning_rate": 4.128179043743642e-05,
      "loss": 2.9713,
      "step": 17140
    },
    {
      "epoch": 8.723296032553408,
      "grad_norm": 13.429302215576172,
      "learning_rate": 4.1276703967446594e-05,
      "loss": 2.9787,
      "step": 17150
    },
    {
      "epoch": 8.728382502543235,
      "grad_norm": 17.409996032714844,
      "learning_rate": 4.1271617497456764e-05,
      "loss": 2.9877,
      "step": 17160
    },
    {
      "epoch": 8.733468972533062,
      "grad_norm": 14.578534126281738,
      "learning_rate": 4.126653102746694e-05,
      "loss": 3.0175,
      "step": 17170
    },
    {
      "epoch": 8.73855544252289,
      "grad_norm": 17.02524757385254,
      "learning_rate": 4.126144455747711e-05,
      "loss": 2.9489,
      "step": 17180
    },
    {
      "epoch": 8.743641912512716,
      "grad_norm": 12.304388999938965,
      "learning_rate": 4.125635808748729e-05,
      "loss": 3.1309,
      "step": 17190
    },
    {
      "epoch": 8.748728382502543,
      "grad_norm": 17.316852569580078,
      "learning_rate": 4.1251271617497464e-05,
      "loss": 2.9786,
      "step": 17200
    },
    {
      "epoch": 8.75381485249237,
      "grad_norm": 14.008768081665039,
      "learning_rate": 4.1246185147507634e-05,
      "loss": 3.0297,
      "step": 17210
    },
    {
      "epoch": 8.758901322482197,
      "grad_norm": 16.45051383972168,
      "learning_rate": 4.1241098677517804e-05,
      "loss": 3.0585,
      "step": 17220
    },
    {
      "epoch": 8.763987792472024,
      "grad_norm": 13.709582328796387,
      "learning_rate": 4.123601220752798e-05,
      "loss": 3.0679,
      "step": 17230
    },
    {
      "epoch": 8.769074262461851,
      "grad_norm": 19.018735885620117,
      "learning_rate": 4.123092573753815e-05,
      "loss": 3.0674,
      "step": 17240
    },
    {
      "epoch": 8.774160732451678,
      "grad_norm": 13.945225715637207,
      "learning_rate": 4.122583926754832e-05,
      "loss": 3.0523,
      "step": 17250
    },
    {
      "epoch": 8.779247202441505,
      "grad_norm": 14.848626136779785,
      "learning_rate": 4.12207527975585e-05,
      "loss": 2.9132,
      "step": 17260
    },
    {
      "epoch": 8.784333672431332,
      "grad_norm": 24.497724533081055,
      "learning_rate": 4.121566632756867e-05,
      "loss": 3.0031,
      "step": 17270
    },
    {
      "epoch": 8.789420142421159,
      "grad_norm": 18.333389282226562,
      "learning_rate": 4.1210579857578844e-05,
      "loss": 3.0125,
      "step": 17280
    },
    {
      "epoch": 8.794506612410986,
      "grad_norm": 18.646453857421875,
      "learning_rate": 4.120549338758902e-05,
      "loss": 2.9911,
      "step": 17290
    },
    {
      "epoch": 8.799593082400813,
      "grad_norm": 21.026926040649414,
      "learning_rate": 4.120040691759919e-05,
      "loss": 2.958,
      "step": 17300
    },
    {
      "epoch": 8.804679552390642,
      "grad_norm": 13.026596069335938,
      "learning_rate": 4.119532044760936e-05,
      "loss": 3.0883,
      "step": 17310
    },
    {
      "epoch": 8.809766022380469,
      "grad_norm": 18.60308837890625,
      "learning_rate": 4.119023397761954e-05,
      "loss": 3.0383,
      "step": 17320
    },
    {
      "epoch": 8.814852492370296,
      "grad_norm": 18.5765380859375,
      "learning_rate": 4.1185147507629707e-05,
      "loss": 2.97,
      "step": 17330
    },
    {
      "epoch": 8.819938962360123,
      "grad_norm": 16.967010498046875,
      "learning_rate": 4.1180061037639876e-05,
      "loss": 2.9671,
      "step": 17340
    },
    {
      "epoch": 8.82502543234995,
      "grad_norm": 16.30205535888672,
      "learning_rate": 4.117497456765005e-05,
      "loss": 3.0091,
      "step": 17350
    },
    {
      "epoch": 8.830111902339777,
      "grad_norm": 14.021199226379395,
      "learning_rate": 4.116988809766022e-05,
      "loss": 2.9617,
      "step": 17360
    },
    {
      "epoch": 8.835198372329604,
      "grad_norm": 14.643684387207031,
      "learning_rate": 4.116480162767039e-05,
      "loss": 3.0512,
      "step": 17370
    },
    {
      "epoch": 8.84028484231943,
      "grad_norm": 19.06390953063965,
      "learning_rate": 4.115971515768057e-05,
      "loss": 3.0328,
      "step": 17380
    },
    {
      "epoch": 8.845371312309258,
      "grad_norm": 19.76906394958496,
      "learning_rate": 4.1154628687690746e-05,
      "loss": 3.0718,
      "step": 17390
    },
    {
      "epoch": 8.850457782299085,
      "grad_norm": 17.803279876708984,
      "learning_rate": 4.1149542217700916e-05,
      "loss": 3.0509,
      "step": 17400
    },
    {
      "epoch": 8.855544252288912,
      "grad_norm": 14.254117965698242,
      "learning_rate": 4.114445574771109e-05,
      "loss": 2.9799,
      "step": 17410
    },
    {
      "epoch": 8.860630722278739,
      "grad_norm": 16.01664161682129,
      "learning_rate": 4.113936927772126e-05,
      "loss": 2.926,
      "step": 17420
    },
    {
      "epoch": 8.865717192268566,
      "grad_norm": 15.49747371673584,
      "learning_rate": 4.113428280773144e-05,
      "loss": 3.0189,
      "step": 17430
    },
    {
      "epoch": 8.870803662258393,
      "grad_norm": 14.62973403930664,
      "learning_rate": 4.112919633774161e-05,
      "loss": 3.0117,
      "step": 17440
    },
    {
      "epoch": 8.87589013224822,
      "grad_norm": 17.434131622314453,
      "learning_rate": 4.112410986775178e-05,
      "loss": 2.8963,
      "step": 17450
    },
    {
      "epoch": 8.880976602238047,
      "grad_norm": 14.772350311279297,
      "learning_rate": 4.1119023397761956e-05,
      "loss": 3.01,
      "step": 17460
    },
    {
      "epoch": 8.886063072227874,
      "grad_norm": 19.00459861755371,
      "learning_rate": 4.1113936927772126e-05,
      "loss": 3.0375,
      "step": 17470
    },
    {
      "epoch": 8.8911495422177,
      "grad_norm": 19.23999786376953,
      "learning_rate": 4.11088504577823e-05,
      "loss": 3.0601,
      "step": 17480
    },
    {
      "epoch": 8.896236012207527,
      "grad_norm": 15.944080352783203,
      "learning_rate": 4.110376398779248e-05,
      "loss": 3.0169,
      "step": 17490
    },
    {
      "epoch": 8.901322482197354,
      "grad_norm": 14.699912071228027,
      "learning_rate": 4.109867751780265e-05,
      "loss": 2.9884,
      "step": 17500
    },
    {
      "epoch": 8.906408952187181,
      "grad_norm": 14.12640380859375,
      "learning_rate": 4.109359104781282e-05,
      "loss": 2.9221,
      "step": 17510
    },
    {
      "epoch": 8.911495422177008,
      "grad_norm": 19.828914642333984,
      "learning_rate": 4.1088504577822996e-05,
      "loss": 3.0007,
      "step": 17520
    },
    {
      "epoch": 8.916581892166835,
      "grad_norm": 15.575589179992676,
      "learning_rate": 4.1083418107833165e-05,
      "loss": 2.9911,
      "step": 17530
    },
    {
      "epoch": 8.921668362156662,
      "grad_norm": 16.805150985717773,
      "learning_rate": 4.1078331637843335e-05,
      "loss": 3.0275,
      "step": 17540
    },
    {
      "epoch": 8.926754832146491,
      "grad_norm": 16.529909133911133,
      "learning_rate": 4.107324516785351e-05,
      "loss": 3.1156,
      "step": 17550
    },
    {
      "epoch": 8.931841302136318,
      "grad_norm": 15.297626495361328,
      "learning_rate": 4.106815869786368e-05,
      "loss": 2.8938,
      "step": 17560
    },
    {
      "epoch": 8.936927772126145,
      "grad_norm": 16.43935775756836,
      "learning_rate": 4.106307222787386e-05,
      "loss": 2.953,
      "step": 17570
    },
    {
      "epoch": 8.942014242115972,
      "grad_norm": 17.998027801513672,
      "learning_rate": 4.1057985757884035e-05,
      "loss": 2.9528,
      "step": 17580
    },
    {
      "epoch": 8.947100712105799,
      "grad_norm": 16.05047035217285,
      "learning_rate": 4.1052899287894205e-05,
      "loss": 3.0238,
      "step": 17590
    },
    {
      "epoch": 8.952187182095626,
      "grad_norm": 13.945396423339844,
      "learning_rate": 4.1047812817904375e-05,
      "loss": 3.0263,
      "step": 17600
    },
    {
      "epoch": 8.957273652085453,
      "grad_norm": 15.978719711303711,
      "learning_rate": 4.104272634791455e-05,
      "loss": 2.9673,
      "step": 17610
    },
    {
      "epoch": 8.96236012207528,
      "grad_norm": 17.608144760131836,
      "learning_rate": 4.103763987792472e-05,
      "loss": 2.995,
      "step": 17620
    },
    {
      "epoch": 8.967446592065107,
      "grad_norm": 14.457584381103516,
      "learning_rate": 4.103255340793489e-05,
      "loss": 2.988,
      "step": 17630
    },
    {
      "epoch": 8.972533062054934,
      "grad_norm": 19.13253402709961,
      "learning_rate": 4.102746693794507e-05,
      "loss": 2.9775,
      "step": 17640
    },
    {
      "epoch": 8.977619532044761,
      "grad_norm": 18.01675033569336,
      "learning_rate": 4.102238046795524e-05,
      "loss": 2.9688,
      "step": 17650
    },
    {
      "epoch": 8.982706002034588,
      "grad_norm": 14.064876556396484,
      "learning_rate": 4.1017293997965415e-05,
      "loss": 2.978,
      "step": 17660
    },
    {
      "epoch": 8.987792472024415,
      "grad_norm": 15.310554504394531,
      "learning_rate": 4.1012207527975585e-05,
      "loss": 3.0058,
      "step": 17670
    },
    {
      "epoch": 8.992878942014242,
      "grad_norm": 19.683517456054688,
      "learning_rate": 4.100712105798576e-05,
      "loss": 3.0683,
      "step": 17680
    },
    {
      "epoch": 8.997965412004069,
      "grad_norm": 13.94625186920166,
      "learning_rate": 4.100203458799594e-05,
      "loss": 2.9837,
      "step": 17690
    },
    {
      "epoch": 9.0,
      "eval_loss": 3.7183122634887695,
      "eval_runtime": 2.9106,
      "eval_samples_per_second": 953.404,
      "eval_steps_per_second": 119.218,
      "step": 17694
    },
    {
      "epoch": 9.003051881993896,
      "grad_norm": 18.454280853271484,
      "learning_rate": 4.099694811800611e-05,
      "loss": 2.933,
      "step": 17700
    },
    {
      "epoch": 9.008138351983723,
      "grad_norm": 14.50883674621582,
      "learning_rate": 4.099186164801628e-05,
      "loss": 2.9529,
      "step": 17710
    },
    {
      "epoch": 9.01322482197355,
      "grad_norm": 16.110599517822266,
      "learning_rate": 4.0986775178026454e-05,
      "loss": 2.9714,
      "step": 17720
    },
    {
      "epoch": 9.018311291963377,
      "grad_norm": 15.333687782287598,
      "learning_rate": 4.0981688708036624e-05,
      "loss": 2.946,
      "step": 17730
    },
    {
      "epoch": 9.023397761953204,
      "grad_norm": 14.150404930114746,
      "learning_rate": 4.0976602238046794e-05,
      "loss": 2.9463,
      "step": 17740
    },
    {
      "epoch": 9.02848423194303,
      "grad_norm": 14.914074897766113,
      "learning_rate": 4.097151576805697e-05,
      "loss": 3.012,
      "step": 17750
    },
    {
      "epoch": 9.033570701932858,
      "grad_norm": 18.10265350341797,
      "learning_rate": 4.096642929806714e-05,
      "loss": 2.8984,
      "step": 17760
    },
    {
      "epoch": 9.038657171922686,
      "grad_norm": 18.1214599609375,
      "learning_rate": 4.096134282807732e-05,
      "loss": 2.9925,
      "step": 17770
    },
    {
      "epoch": 9.043743641912513,
      "grad_norm": 13.03000259399414,
      "learning_rate": 4.0956256358087494e-05,
      "loss": 2.9845,
      "step": 17780
    },
    {
      "epoch": 9.04883011190234,
      "grad_norm": 16.291845321655273,
      "learning_rate": 4.0951169888097664e-05,
      "loss": 2.8951,
      "step": 17790
    },
    {
      "epoch": 9.053916581892167,
      "grad_norm": 17.42450523376465,
      "learning_rate": 4.0946083418107834e-05,
      "loss": 2.9871,
      "step": 17800
    },
    {
      "epoch": 9.059003051881994,
      "grad_norm": 18.21952247619629,
      "learning_rate": 4.094099694811801e-05,
      "loss": 2.9081,
      "step": 17810
    },
    {
      "epoch": 9.064089521871821,
      "grad_norm": 17.570072174072266,
      "learning_rate": 4.093591047812818e-05,
      "loss": 2.9425,
      "step": 17820
    },
    {
      "epoch": 9.069175991861648,
      "grad_norm": 15.641007423400879,
      "learning_rate": 4.093082400813835e-05,
      "loss": 2.8796,
      "step": 17830
    },
    {
      "epoch": 9.074262461851475,
      "grad_norm": 16.065670013427734,
      "learning_rate": 4.092573753814853e-05,
      "loss": 3.0316,
      "step": 17840
    },
    {
      "epoch": 9.079348931841302,
      "grad_norm": 16.48870086669922,
      "learning_rate": 4.09206510681587e-05,
      "loss": 2.9591,
      "step": 17850
    },
    {
      "epoch": 9.08443540183113,
      "grad_norm": 13.766641616821289,
      "learning_rate": 4.0915564598168874e-05,
      "loss": 2.9496,
      "step": 17860
    },
    {
      "epoch": 9.089521871820956,
      "grad_norm": 20.930837631225586,
      "learning_rate": 4.091047812817905e-05,
      "loss": 2.9928,
      "step": 17870
    },
    {
      "epoch": 9.094608341810783,
      "grad_norm": 13.781905174255371,
      "learning_rate": 4.090539165818922e-05,
      "loss": 3.0275,
      "step": 17880
    },
    {
      "epoch": 9.09969481180061,
      "grad_norm": 13.578851699829102,
      "learning_rate": 4.090030518819939e-05,
      "loss": 3.0594,
      "step": 17890
    },
    {
      "epoch": 9.104781281790437,
      "grad_norm": 14.986557960510254,
      "learning_rate": 4.089521871820957e-05,
      "loss": 2.999,
      "step": 17900
    },
    {
      "epoch": 9.109867751780264,
      "grad_norm": 15.237399101257324,
      "learning_rate": 4.0890132248219737e-05,
      "loss": 2.9674,
      "step": 17910
    },
    {
      "epoch": 9.114954221770091,
      "grad_norm": 14.75436782836914,
      "learning_rate": 4.0885045778229906e-05,
      "loss": 2.974,
      "step": 17920
    },
    {
      "epoch": 9.120040691759918,
      "grad_norm": 15.31865406036377,
      "learning_rate": 4.087995930824008e-05,
      "loss": 2.9652,
      "step": 17930
    },
    {
      "epoch": 9.125127161749745,
      "grad_norm": 16.118675231933594,
      "learning_rate": 4.087487283825025e-05,
      "loss": 2.9799,
      "step": 17940
    },
    {
      "epoch": 9.130213631739572,
      "grad_norm": 18.892824172973633,
      "learning_rate": 4.086978636826043e-05,
      "loss": 2.9762,
      "step": 17950
    },
    {
      "epoch": 9.135300101729399,
      "grad_norm": 20.832950592041016,
      "learning_rate": 4.0864699898270606e-05,
      "loss": 2.9927,
      "step": 17960
    },
    {
      "epoch": 9.140386571719226,
      "grad_norm": 19.512086868286133,
      "learning_rate": 4.0859613428280776e-05,
      "loss": 2.9402,
      "step": 17970
    },
    {
      "epoch": 9.145473041709053,
      "grad_norm": 15.901588439941406,
      "learning_rate": 4.085452695829095e-05,
      "loss": 2.917,
      "step": 17980
    },
    {
      "epoch": 9.150559511698882,
      "grad_norm": 19.68896484375,
      "learning_rate": 4.084944048830112e-05,
      "loss": 2.933,
      "step": 17990
    },
    {
      "epoch": 9.155645981688709,
      "grad_norm": 16.916797637939453,
      "learning_rate": 4.084435401831129e-05,
      "loss": 2.996,
      "step": 18000
    },
    {
      "epoch": 9.160732451678536,
      "grad_norm": 16.84360122680664,
      "learning_rate": 4.083926754832147e-05,
      "loss": 2.94,
      "step": 18010
    },
    {
      "epoch": 9.165818921668363,
      "grad_norm": 16.119056701660156,
      "learning_rate": 4.083418107833164e-05,
      "loss": 2.9247,
      "step": 18020
    },
    {
      "epoch": 9.17090539165819,
      "grad_norm": 14.363480567932129,
      "learning_rate": 4.082909460834181e-05,
      "loss": 2.9406,
      "step": 18030
    },
    {
      "epoch": 9.175991861648017,
      "grad_norm": 15.330048561096191,
      "learning_rate": 4.0824008138351986e-05,
      "loss": 2.9722,
      "step": 18040
    },
    {
      "epoch": 9.181078331637844,
      "grad_norm": 20.768091201782227,
      "learning_rate": 4.0818921668362156e-05,
      "loss": 2.9627,
      "step": 18050
    },
    {
      "epoch": 9.18616480162767,
      "grad_norm": 16.57724380493164,
      "learning_rate": 4.081383519837233e-05,
      "loss": 2.9844,
      "step": 18060
    },
    {
      "epoch": 9.191251271617498,
      "grad_norm": 15.851431846618652,
      "learning_rate": 4.080874872838251e-05,
      "loss": 2.987,
      "step": 18070
    },
    {
      "epoch": 9.196337741607325,
      "grad_norm": 15.169150352478027,
      "learning_rate": 4.080366225839268e-05,
      "loss": 2.9148,
      "step": 18080
    },
    {
      "epoch": 9.201424211597152,
      "grad_norm": 15.662875175476074,
      "learning_rate": 4.079857578840285e-05,
      "loss": 2.9116,
      "step": 18090
    },
    {
      "epoch": 9.206510681586979,
      "grad_norm": 15.29304027557373,
      "learning_rate": 4.0793489318413026e-05,
      "loss": 2.9521,
      "step": 18100
    },
    {
      "epoch": 9.211597151576806,
      "grad_norm": 16.365304946899414,
      "learning_rate": 4.0788402848423195e-05,
      "loss": 2.9267,
      "step": 18110
    },
    {
      "epoch": 9.216683621566633,
      "grad_norm": 16.032255172729492,
      "learning_rate": 4.0783316378433365e-05,
      "loss": 2.9785,
      "step": 18120
    },
    {
      "epoch": 9.22177009155646,
      "grad_norm": 15.317227363586426,
      "learning_rate": 4.077822990844354e-05,
      "loss": 2.9955,
      "step": 18130
    },
    {
      "epoch": 9.226856561546287,
      "grad_norm": 16.586889266967773,
      "learning_rate": 4.077314343845371e-05,
      "loss": 3.0048,
      "step": 18140
    },
    {
      "epoch": 9.231943031536114,
      "grad_norm": 18.557212829589844,
      "learning_rate": 4.076805696846389e-05,
      "loss": 2.9404,
      "step": 18150
    },
    {
      "epoch": 9.23702950152594,
      "grad_norm": 15.288494110107422,
      "learning_rate": 4.0762970498474065e-05,
      "loss": 2.8985,
      "step": 18160
    },
    {
      "epoch": 9.242115971515767,
      "grad_norm": 14.917366981506348,
      "learning_rate": 4.0757884028484235e-05,
      "loss": 2.9277,
      "step": 18170
    },
    {
      "epoch": 9.247202441505594,
      "grad_norm": 15.915492057800293,
      "learning_rate": 4.0752797558494405e-05,
      "loss": 2.9622,
      "step": 18180
    },
    {
      "epoch": 9.252288911495421,
      "grad_norm": 20.345518112182617,
      "learning_rate": 4.074771108850458e-05,
      "loss": 2.9537,
      "step": 18190
    },
    {
      "epoch": 9.257375381485248,
      "grad_norm": 21.820980072021484,
      "learning_rate": 4.074262461851475e-05,
      "loss": 2.9858,
      "step": 18200
    },
    {
      "epoch": 9.262461851475077,
      "grad_norm": 12.905560493469238,
      "learning_rate": 4.073753814852492e-05,
      "loss": 2.9804,
      "step": 18210
    },
    {
      "epoch": 9.267548321464904,
      "grad_norm": 17.12042236328125,
      "learning_rate": 4.07324516785351e-05,
      "loss": 2.8876,
      "step": 18220
    },
    {
      "epoch": 9.272634791454731,
      "grad_norm": 16.71328353881836,
      "learning_rate": 4.072736520854527e-05,
      "loss": 2.9916,
      "step": 18230
    },
    {
      "epoch": 9.277721261444558,
      "grad_norm": 19.84445571899414,
      "learning_rate": 4.0722278738555445e-05,
      "loss": 2.9434,
      "step": 18240
    },
    {
      "epoch": 9.282807731434385,
      "grad_norm": 16.3077392578125,
      "learning_rate": 4.071719226856562e-05,
      "loss": 2.9798,
      "step": 18250
    },
    {
      "epoch": 9.287894201424212,
      "grad_norm": 20.115154266357422,
      "learning_rate": 4.071210579857579e-05,
      "loss": 2.9842,
      "step": 18260
    },
    {
      "epoch": 9.292980671414039,
      "grad_norm": 18.290964126586914,
      "learning_rate": 4.070701932858597e-05,
      "loss": 2.9583,
      "step": 18270
    },
    {
      "epoch": 9.298067141403866,
      "grad_norm": 14.67371654510498,
      "learning_rate": 4.070193285859614e-05,
      "loss": 2.9644,
      "step": 18280
    },
    {
      "epoch": 9.303153611393693,
      "grad_norm": 18.61212921142578,
      "learning_rate": 4.069684638860631e-05,
      "loss": 2.9364,
      "step": 18290
    },
    {
      "epoch": 9.30824008138352,
      "grad_norm": 23.834959030151367,
      "learning_rate": 4.0691759918616484e-05,
      "loss": 3.0034,
      "step": 18300
    },
    {
      "epoch": 9.313326551373347,
      "grad_norm": 19.664264678955078,
      "learning_rate": 4.0686673448626654e-05,
      "loss": 2.9383,
      "step": 18310
    },
    {
      "epoch": 9.318413021363174,
      "grad_norm": 20.907453536987305,
      "learning_rate": 4.0681586978636824e-05,
      "loss": 2.9417,
      "step": 18320
    },
    {
      "epoch": 9.323499491353001,
      "grad_norm": 17.546676635742188,
      "learning_rate": 4.0676500508647e-05,
      "loss": 3.0025,
      "step": 18330
    },
    {
      "epoch": 9.328585961342828,
      "grad_norm": 19.969505310058594,
      "learning_rate": 4.067141403865717e-05,
      "loss": 2.981,
      "step": 18340
    },
    {
      "epoch": 9.333672431332655,
      "grad_norm": 15.900032997131348,
      "learning_rate": 4.066632756866735e-05,
      "loss": 2.9505,
      "step": 18350
    },
    {
      "epoch": 9.338758901322482,
      "grad_norm": 15.800183296203613,
      "learning_rate": 4.0661241098677524e-05,
      "loss": 2.938,
      "step": 18360
    },
    {
      "epoch": 9.343845371312309,
      "grad_norm": 20.828676223754883,
      "learning_rate": 4.0656154628687694e-05,
      "loss": 2.9889,
      "step": 18370
    },
    {
      "epoch": 9.348931841302136,
      "grad_norm": 14.792842864990234,
      "learning_rate": 4.0651068158697864e-05,
      "loss": 2.9639,
      "step": 18380
    },
    {
      "epoch": 9.354018311291963,
      "grad_norm": 17.09352684020996,
      "learning_rate": 4.064598168870804e-05,
      "loss": 2.9265,
      "step": 18390
    },
    {
      "epoch": 9.35910478128179,
      "grad_norm": 20.13121795654297,
      "learning_rate": 4.064089521871821e-05,
      "loss": 2.916,
      "step": 18400
    },
    {
      "epoch": 9.364191251271617,
      "grad_norm": 23.156763076782227,
      "learning_rate": 4.063580874872838e-05,
      "loss": 2.8726,
      "step": 18410
    },
    {
      "epoch": 9.369277721261444,
      "grad_norm": 18.474090576171875,
      "learning_rate": 4.063072227873856e-05,
      "loss": 2.9542,
      "step": 18420
    },
    {
      "epoch": 9.37436419125127,
      "grad_norm": 19.42500114440918,
      "learning_rate": 4.062563580874873e-05,
      "loss": 2.9261,
      "step": 18430
    },
    {
      "epoch": 9.3794506612411,
      "grad_norm": 12.497570991516113,
      "learning_rate": 4.0620549338758904e-05,
      "loss": 2.906,
      "step": 18440
    },
    {
      "epoch": 9.384537131230926,
      "grad_norm": 17.645301818847656,
      "learning_rate": 4.061546286876908e-05,
      "loss": 2.8682,
      "step": 18450
    },
    {
      "epoch": 9.389623601220753,
      "grad_norm": 15.854520797729492,
      "learning_rate": 4.061037639877925e-05,
      "loss": 2.9123,
      "step": 18460
    },
    {
      "epoch": 9.39471007121058,
      "grad_norm": 17.46116065979004,
      "learning_rate": 4.060528992878942e-05,
      "loss": 3.0157,
      "step": 18470
    },
    {
      "epoch": 9.399796541200407,
      "grad_norm": 15.73112964630127,
      "learning_rate": 4.06002034587996e-05,
      "loss": 2.9013,
      "step": 18480
    },
    {
      "epoch": 9.404883011190234,
      "grad_norm": 15.326108932495117,
      "learning_rate": 4.0595116988809767e-05,
      "loss": 2.9187,
      "step": 18490
    },
    {
      "epoch": 9.409969481180061,
      "grad_norm": 18.726722717285156,
      "learning_rate": 4.059003051881994e-05,
      "loss": 2.9967,
      "step": 18500
    },
    {
      "epoch": 9.415055951169888,
      "grad_norm": 16.572744369506836,
      "learning_rate": 4.058494404883011e-05,
      "loss": 2.9525,
      "step": 18510
    },
    {
      "epoch": 9.420142421159715,
      "grad_norm": 17.1090145111084,
      "learning_rate": 4.057985757884028e-05,
      "loss": 2.8725,
      "step": 18520
    },
    {
      "epoch": 9.425228891149542,
      "grad_norm": 14.077458381652832,
      "learning_rate": 4.057477110885046e-05,
      "loss": 2.9598,
      "step": 18530
    },
    {
      "epoch": 9.43031536113937,
      "grad_norm": 15.97276782989502,
      "learning_rate": 4.0569684638860636e-05,
      "loss": 2.9507,
      "step": 18540
    },
    {
      "epoch": 9.435401831129196,
      "grad_norm": 21.822298049926758,
      "learning_rate": 4.0564598168870806e-05,
      "loss": 2.962,
      "step": 18550
    },
    {
      "epoch": 9.440488301119023,
      "grad_norm": 19.274145126342773,
      "learning_rate": 4.055951169888098e-05,
      "loss": 3.048,
      "step": 18560
    },
    {
      "epoch": 9.44557477110885,
      "grad_norm": 15.33523178100586,
      "learning_rate": 4.055442522889115e-05,
      "loss": 2.8952,
      "step": 18570
    },
    {
      "epoch": 9.450661241098677,
      "grad_norm": 16.490903854370117,
      "learning_rate": 4.054933875890132e-05,
      "loss": 2.9428,
      "step": 18580
    },
    {
      "epoch": 9.455747711088504,
      "grad_norm": 16.900876998901367,
      "learning_rate": 4.05442522889115e-05,
      "loss": 2.9021,
      "step": 18590
    },
    {
      "epoch": 9.460834181078331,
      "grad_norm": 19.610706329345703,
      "learning_rate": 4.053916581892167e-05,
      "loss": 2.8817,
      "step": 18600
    },
    {
      "epoch": 9.465920651068158,
      "grad_norm": 24.000659942626953,
      "learning_rate": 4.053407934893184e-05,
      "loss": 2.9141,
      "step": 18610
    },
    {
      "epoch": 9.471007121057985,
      "grad_norm": 15.27361011505127,
      "learning_rate": 4.0528992878942016e-05,
      "loss": 2.9429,
      "step": 18620
    },
    {
      "epoch": 9.476093591047812,
      "grad_norm": 16.9083309173584,
      "learning_rate": 4.052390640895219e-05,
      "loss": 2.8996,
      "step": 18630
    },
    {
      "epoch": 9.481180061037639,
      "grad_norm": 20.501615524291992,
      "learning_rate": 4.051881993896236e-05,
      "loss": 2.8723,
      "step": 18640
    },
    {
      "epoch": 9.486266531027466,
      "grad_norm": 16.975452423095703,
      "learning_rate": 4.051373346897254e-05,
      "loss": 2.9301,
      "step": 18650
    },
    {
      "epoch": 9.491353001017295,
      "grad_norm": 15.710579872131348,
      "learning_rate": 4.050864699898271e-05,
      "loss": 2.9941,
      "step": 18660
    },
    {
      "epoch": 9.496439471007122,
      "grad_norm": 15.361702919006348,
      "learning_rate": 4.050356052899288e-05,
      "loss": 2.9013,
      "step": 18670
    },
    {
      "epoch": 9.501525940996949,
      "grad_norm": 14.02274227142334,
      "learning_rate": 4.0498474059003056e-05,
      "loss": 2.9055,
      "step": 18680
    },
    {
      "epoch": 9.506612410986776,
      "grad_norm": 14.502226829528809,
      "learning_rate": 4.0493387589013225e-05,
      "loss": 2.9159,
      "step": 18690
    },
    {
      "epoch": 9.511698880976603,
      "grad_norm": 20.545177459716797,
      "learning_rate": 4.0488301119023395e-05,
      "loss": 2.8606,
      "step": 18700
    },
    {
      "epoch": 9.51678535096643,
      "grad_norm": 17.92603874206543,
      "learning_rate": 4.048321464903357e-05,
      "loss": 2.9468,
      "step": 18710
    },
    {
      "epoch": 9.521871820956257,
      "grad_norm": 13.47028923034668,
      "learning_rate": 4.047812817904374e-05,
      "loss": 2.9278,
      "step": 18720
    },
    {
      "epoch": 9.526958290946084,
      "grad_norm": 20.139577865600586,
      "learning_rate": 4.047304170905392e-05,
      "loss": 2.9501,
      "step": 18730
    },
    {
      "epoch": 9.53204476093591,
      "grad_norm": 18.77730941772461,
      "learning_rate": 4.0467955239064095e-05,
      "loss": 2.9695,
      "step": 18740
    },
    {
      "epoch": 9.537131230925738,
      "grad_norm": 16.45660972595215,
      "learning_rate": 4.0462868769074265e-05,
      "loss": 2.962,
      "step": 18750
    },
    {
      "epoch": 9.542217700915565,
      "grad_norm": 13.813652038574219,
      "learning_rate": 4.0457782299084435e-05,
      "loss": 2.9445,
      "step": 18760
    },
    {
      "epoch": 9.547304170905392,
      "grad_norm": 17.206418991088867,
      "learning_rate": 4.045269582909461e-05,
      "loss": 2.8864,
      "step": 18770
    },
    {
      "epoch": 9.552390640895219,
      "grad_norm": 14.297285079956055,
      "learning_rate": 4.044760935910478e-05,
      "loss": 2.8801,
      "step": 18780
    },
    {
      "epoch": 9.557477110885046,
      "grad_norm": 16.152755737304688,
      "learning_rate": 4.044252288911496e-05,
      "loss": 2.9623,
      "step": 18790
    },
    {
      "epoch": 9.562563580874873,
      "grad_norm": 19.248554229736328,
      "learning_rate": 4.043743641912513e-05,
      "loss": 2.876,
      "step": 18800
    },
    {
      "epoch": 9.5676500508647,
      "grad_norm": 17.153831481933594,
      "learning_rate": 4.04323499491353e-05,
      "loss": 2.9177,
      "step": 18810
    },
    {
      "epoch": 9.572736520854527,
      "grad_norm": 15.795903205871582,
      "learning_rate": 4.0427263479145475e-05,
      "loss": 2.9476,
      "step": 18820
    },
    {
      "epoch": 9.577822990844354,
      "grad_norm": 17.050241470336914,
      "learning_rate": 4.042217700915565e-05,
      "loss": 2.9539,
      "step": 18830
    },
    {
      "epoch": 9.58290946083418,
      "grad_norm": 14.726469993591309,
      "learning_rate": 4.041709053916582e-05,
      "loss": 2.8964,
      "step": 18840
    },
    {
      "epoch": 9.587995930824007,
      "grad_norm": 15.652382850646973,
      "learning_rate": 4.0412004069176e-05,
      "loss": 2.9902,
      "step": 18850
    },
    {
      "epoch": 9.593082400813834,
      "grad_norm": 17.891935348510742,
      "learning_rate": 4.040691759918617e-05,
      "loss": 2.8783,
      "step": 18860
    },
    {
      "epoch": 9.598168870803661,
      "grad_norm": 15.808036804199219,
      "learning_rate": 4.040183112919634e-05,
      "loss": 2.8742,
      "step": 18870
    },
    {
      "epoch": 9.603255340793488,
      "grad_norm": 17.316665649414062,
      "learning_rate": 4.0396744659206514e-05,
      "loss": 2.9965,
      "step": 18880
    },
    {
      "epoch": 9.608341810783317,
      "grad_norm": 20.307279586791992,
      "learning_rate": 4.0391658189216684e-05,
      "loss": 2.7782,
      "step": 18890
    },
    {
      "epoch": 9.613428280773144,
      "grad_norm": 15.996129035949707,
      "learning_rate": 4.0386571719226854e-05,
      "loss": 2.8843,
      "step": 18900
    },
    {
      "epoch": 9.618514750762971,
      "grad_norm": 16.578290939331055,
      "learning_rate": 4.038148524923703e-05,
      "loss": 2.9231,
      "step": 18910
    },
    {
      "epoch": 9.623601220752798,
      "grad_norm": 17.29109764099121,
      "learning_rate": 4.037639877924721e-05,
      "loss": 2.882,
      "step": 18920
    },
    {
      "epoch": 9.628687690742625,
      "grad_norm": 17.70659828186035,
      "learning_rate": 4.037131230925738e-05,
      "loss": 2.9183,
      "step": 18930
    },
    {
      "epoch": 9.633774160732452,
      "grad_norm": 20.94564437866211,
      "learning_rate": 4.0366225839267554e-05,
      "loss": 2.902,
      "step": 18940
    },
    {
      "epoch": 9.638860630722279,
      "grad_norm": 17.776447296142578,
      "learning_rate": 4.0361139369277724e-05,
      "loss": 2.998,
      "step": 18950
    },
    {
      "epoch": 9.643947100712106,
      "grad_norm": 19.010507583618164,
      "learning_rate": 4.0356052899287894e-05,
      "loss": 2.9019,
      "step": 18960
    },
    {
      "epoch": 9.649033570701933,
      "grad_norm": 17.87042808532715,
      "learning_rate": 4.035096642929807e-05,
      "loss": 2.903,
      "step": 18970
    },
    {
      "epoch": 9.65412004069176,
      "grad_norm": 14.913387298583984,
      "learning_rate": 4.034587995930824e-05,
      "loss": 3.0558,
      "step": 18980
    },
    {
      "epoch": 9.659206510681587,
      "grad_norm": 15.640864372253418,
      "learning_rate": 4.034079348931841e-05,
      "loss": 3.0669,
      "step": 18990
    },
    {
      "epoch": 9.664292980671414,
      "grad_norm": 19.772762298583984,
      "learning_rate": 4.033570701932859e-05,
      "loss": 2.9324,
      "step": 19000
    },
    {
      "epoch": 9.669379450661241,
      "grad_norm": 15.92456340789795,
      "learning_rate": 4.033062054933876e-05,
      "loss": 2.8922,
      "step": 19010
    },
    {
      "epoch": 9.674465920651068,
      "grad_norm": 14.960122108459473,
      "learning_rate": 4.0325534079348934e-05,
      "loss": 2.8984,
      "step": 19020
    },
    {
      "epoch": 9.679552390640895,
      "grad_norm": 15.790862083435059,
      "learning_rate": 4.032044760935911e-05,
      "loss": 2.9233,
      "step": 19030
    },
    {
      "epoch": 9.684638860630722,
      "grad_norm": 20.389930725097656,
      "learning_rate": 4.031536113936928e-05,
      "loss": 2.9582,
      "step": 19040
    },
    {
      "epoch": 9.689725330620549,
      "grad_norm": 18.668045043945312,
      "learning_rate": 4.031027466937946e-05,
      "loss": 2.8854,
      "step": 19050
    },
    {
      "epoch": 9.694811800610376,
      "grad_norm": 16.312334060668945,
      "learning_rate": 4.030518819938963e-05,
      "loss": 2.9483,
      "step": 19060
    },
    {
      "epoch": 9.699898270600203,
      "grad_norm": 14.768927574157715,
      "learning_rate": 4.0300101729399797e-05,
      "loss": 2.941,
      "step": 19070
    },
    {
      "epoch": 9.70498474059003,
      "grad_norm": 15.870491027832031,
      "learning_rate": 4.029501525940997e-05,
      "loss": 2.9212,
      "step": 19080
    },
    {
      "epoch": 9.710071210579857,
      "grad_norm": 22.61162757873535,
      "learning_rate": 4.028992878942014e-05,
      "loss": 2.8777,
      "step": 19090
    },
    {
      "epoch": 9.715157680569686,
      "grad_norm": 24.642166137695312,
      "learning_rate": 4.028484231943031e-05,
      "loss": 2.8429,
      "step": 19100
    },
    {
      "epoch": 9.720244150559513,
      "grad_norm": 13.845978736877441,
      "learning_rate": 4.027975584944049e-05,
      "loss": 2.8698,
      "step": 19110
    },
    {
      "epoch": 9.72533062054934,
      "grad_norm": 19.625654220581055,
      "learning_rate": 4.0274669379450666e-05,
      "loss": 2.9424,
      "step": 19120
    },
    {
      "epoch": 9.730417090539166,
      "grad_norm": 20.41069221496582,
      "learning_rate": 4.0269582909460836e-05,
      "loss": 2.8961,
      "step": 19130
    },
    {
      "epoch": 9.735503560528993,
      "grad_norm": 21.239553451538086,
      "learning_rate": 4.026449643947101e-05,
      "loss": 2.8615,
      "step": 19140
    },
    {
      "epoch": 9.74059003051882,
      "grad_norm": 19.991334915161133,
      "learning_rate": 4.025940996948118e-05,
      "loss": 2.855,
      "step": 19150
    },
    {
      "epoch": 9.745676500508647,
      "grad_norm": 16.2652645111084,
      "learning_rate": 4.025432349949135e-05,
      "loss": 2.9011,
      "step": 19160
    },
    {
      "epoch": 9.750762970498474,
      "grad_norm": 19.047325134277344,
      "learning_rate": 4.024923702950153e-05,
      "loss": 2.9517,
      "step": 19170
    },
    {
      "epoch": 9.755849440488301,
      "grad_norm": 18.952579498291016,
      "learning_rate": 4.02441505595117e-05,
      "loss": 2.9523,
      "step": 19180
    },
    {
      "epoch": 9.760935910478128,
      "grad_norm": 17.27861785888672,
      "learning_rate": 4.023906408952187e-05,
      "loss": 2.9121,
      "step": 19190
    },
    {
      "epoch": 9.766022380467955,
      "grad_norm": 15.993096351623535,
      "learning_rate": 4.0233977619532046e-05,
      "loss": 2.9738,
      "step": 19200
    },
    {
      "epoch": 9.771108850457782,
      "grad_norm": 16.08727264404297,
      "learning_rate": 4.022889114954222e-05,
      "loss": 2.9939,
      "step": 19210
    },
    {
      "epoch": 9.77619532044761,
      "grad_norm": 16.5753116607666,
      "learning_rate": 4.022380467955239e-05,
      "loss": 2.9284,
      "step": 19220
    },
    {
      "epoch": 9.781281790437436,
      "grad_norm": 17.324565887451172,
      "learning_rate": 4.021871820956257e-05,
      "loss": 2.9152,
      "step": 19230
    },
    {
      "epoch": 9.786368260427263,
      "grad_norm": 18.293746948242188,
      "learning_rate": 4.021363173957274e-05,
      "loss": 2.905,
      "step": 19240
    },
    {
      "epoch": 9.79145473041709,
      "grad_norm": 17.046674728393555,
      "learning_rate": 4.020854526958291e-05,
      "loss": 2.8464,
      "step": 19250
    },
    {
      "epoch": 9.796541200406917,
      "grad_norm": 13.681806564331055,
      "learning_rate": 4.0203458799593086e-05,
      "loss": 2.8879,
      "step": 19260
    },
    {
      "epoch": 9.801627670396744,
      "grad_norm": 13.078919410705566,
      "learning_rate": 4.0198372329603255e-05,
      "loss": 2.9668,
      "step": 19270
    },
    {
      "epoch": 9.806714140386571,
      "grad_norm": 17.744972229003906,
      "learning_rate": 4.0193285859613425e-05,
      "loss": 2.8912,
      "step": 19280
    },
    {
      "epoch": 9.811800610376398,
      "grad_norm": 15.893312454223633,
      "learning_rate": 4.01881993896236e-05,
      "loss": 2.9604,
      "step": 19290
    },
    {
      "epoch": 9.816887080366225,
      "grad_norm": 13.551118850708008,
      "learning_rate": 4.018311291963377e-05,
      "loss": 2.8877,
      "step": 19300
    },
    {
      "epoch": 9.821973550356052,
      "grad_norm": 16.98556900024414,
      "learning_rate": 4.017802644964395e-05,
      "loss": 2.9354,
      "step": 19310
    },
    {
      "epoch": 9.827060020345879,
      "grad_norm": 20.272777557373047,
      "learning_rate": 4.0172939979654125e-05,
      "loss": 2.9289,
      "step": 19320
    },
    {
      "epoch": 9.832146490335706,
      "grad_norm": 20.93497657775879,
      "learning_rate": 4.0167853509664295e-05,
      "loss": 2.9116,
      "step": 19330
    },
    {
      "epoch": 9.837232960325535,
      "grad_norm": 15.051610946655273,
      "learning_rate": 4.016276703967447e-05,
      "loss": 2.8448,
      "step": 19340
    },
    {
      "epoch": 9.842319430315362,
      "grad_norm": 20.17119789123535,
      "learning_rate": 4.015768056968464e-05,
      "loss": 2.8329,
      "step": 19350
    },
    {
      "epoch": 9.847405900305189,
      "grad_norm": 15.434674263000488,
      "learning_rate": 4.015259409969481e-05,
      "loss": 2.8713,
      "step": 19360
    },
    {
      "epoch": 9.852492370295016,
      "grad_norm": 16.53095245361328,
      "learning_rate": 4.014750762970499e-05,
      "loss": 2.9306,
      "step": 19370
    },
    {
      "epoch": 9.857578840284843,
      "grad_norm": 20.747177124023438,
      "learning_rate": 4.014242115971516e-05,
      "loss": 2.8785,
      "step": 19380
    },
    {
      "epoch": 9.86266531027467,
      "grad_norm": 17.273754119873047,
      "learning_rate": 4.013733468972533e-05,
      "loss": 2.8997,
      "step": 19390
    },
    {
      "epoch": 9.867751780264497,
      "grad_norm": 19.49944496154785,
      "learning_rate": 4.0132248219735505e-05,
      "loss": 2.8928,
      "step": 19400
    },
    {
      "epoch": 9.872838250254324,
      "grad_norm": 20.02122688293457,
      "learning_rate": 4.012716174974568e-05,
      "loss": 2.9319,
      "step": 19410
    },
    {
      "epoch": 9.87792472024415,
      "grad_norm": 16.99493980407715,
      "learning_rate": 4.012207527975585e-05,
      "loss": 2.9527,
      "step": 19420
    },
    {
      "epoch": 9.883011190233978,
      "grad_norm": 18.710264205932617,
      "learning_rate": 4.011698880976603e-05,
      "loss": 2.8469,
      "step": 19430
    },
    {
      "epoch": 9.888097660223805,
      "grad_norm": 17.227405548095703,
      "learning_rate": 4.01119023397762e-05,
      "loss": 2.9806,
      "step": 19440
    },
    {
      "epoch": 9.893184130213632,
      "grad_norm": 20.36041831970215,
      "learning_rate": 4.010681586978637e-05,
      "loss": 2.9058,
      "step": 19450
    },
    {
      "epoch": 9.898270600203459,
      "grad_norm": 16.54412841796875,
      "learning_rate": 4.0101729399796544e-05,
      "loss": 2.949,
      "step": 19460
    },
    {
      "epoch": 9.903357070193286,
      "grad_norm": 18.469436645507812,
      "learning_rate": 4.0096642929806714e-05,
      "loss": 2.8698,
      "step": 19470
    },
    {
      "epoch": 9.908443540183113,
      "grad_norm": 25.20393180847168,
      "learning_rate": 4.0091556459816884e-05,
      "loss": 2.8669,
      "step": 19480
    },
    {
      "epoch": 9.91353001017294,
      "grad_norm": 19.53472328186035,
      "learning_rate": 4.008646998982706e-05,
      "loss": 2.8227,
      "step": 19490
    },
    {
      "epoch": 9.918616480162767,
      "grad_norm": 15.464256286621094,
      "learning_rate": 4.008138351983724e-05,
      "loss": 2.8383,
      "step": 19500
    },
    {
      "epoch": 9.923702950152594,
      "grad_norm": 17.111499786376953,
      "learning_rate": 4.007629704984741e-05,
      "loss": 2.9042,
      "step": 19510
    },
    {
      "epoch": 9.92878942014242,
      "grad_norm": 20.69657325744629,
      "learning_rate": 4.0071210579857584e-05,
      "loss": 2.8677,
      "step": 19520
    },
    {
      "epoch": 9.933875890132247,
      "grad_norm": 18.039939880371094,
      "learning_rate": 4.0066124109867754e-05,
      "loss": 2.9512,
      "step": 19530
    },
    {
      "epoch": 9.938962360122074,
      "grad_norm": 15.598677635192871,
      "learning_rate": 4.0061037639877924e-05,
      "loss": 2.9478,
      "step": 19540
    },
    {
      "epoch": 9.944048830111903,
      "grad_norm": 19.700590133666992,
      "learning_rate": 4.00559511698881e-05,
      "loss": 2.833,
      "step": 19550
    },
    {
      "epoch": 9.94913530010173,
      "grad_norm": 13.382396697998047,
      "learning_rate": 4.005086469989827e-05,
      "loss": 2.8075,
      "step": 19560
    },
    {
      "epoch": 9.954221770091557,
      "grad_norm": 18.396137237548828,
      "learning_rate": 4.004577822990844e-05,
      "loss": 2.9223,
      "step": 19570
    },
    {
      "epoch": 9.959308240081384,
      "grad_norm": 20.041301727294922,
      "learning_rate": 4.004069175991862e-05,
      "loss": 2.9229,
      "step": 19580
    },
    {
      "epoch": 9.964394710071211,
      "grad_norm": 18.307695388793945,
      "learning_rate": 4.0035605289928794e-05,
      "loss": 2.9791,
      "step": 19590
    },
    {
      "epoch": 9.969481180061038,
      "grad_norm": 20.135276794433594,
      "learning_rate": 4.0030518819938964e-05,
      "loss": 2.855,
      "step": 19600
    },
    {
      "epoch": 9.974567650050865,
      "grad_norm": 16.021772384643555,
      "learning_rate": 4.002543234994914e-05,
      "loss": 2.8959,
      "step": 19610
    },
    {
      "epoch": 9.979654120040692,
      "grad_norm": 22.079059600830078,
      "learning_rate": 4.002034587995931e-05,
      "loss": 2.8879,
      "step": 19620
    },
    {
      "epoch": 9.984740590030519,
      "grad_norm": 17.6051025390625,
      "learning_rate": 4.001525940996949e-05,
      "loss": 2.8843,
      "step": 19630
    },
    {
      "epoch": 9.989827060020346,
      "grad_norm": 18.893795013427734,
      "learning_rate": 4.001017293997966e-05,
      "loss": 2.8642,
      "step": 19640
    },
    {
      "epoch": 9.994913530010173,
      "grad_norm": 20.67147445678711,
      "learning_rate": 4.0005086469989827e-05,
      "loss": 2.9085,
      "step": 19650
    },
    {
      "epoch": 10.0,
      "grad_norm": 29.296907424926758,
      "learning_rate": 4e-05,
      "loss": 2.8136,
      "step": 19660
    },
    {
      "epoch": 10.0,
      "eval_loss": 3.7677645683288574,
      "eval_runtime": 2.7321,
      "eval_samples_per_second": 1015.719,
      "eval_steps_per_second": 127.011,
      "step": 19660
    },
    {
      "epoch": 10.005086469989827,
      "grad_norm": 18.18377685546875,
      "learning_rate": 3.999491353001017e-05,
      "loss": 2.9203,
      "step": 19670
    },
    {
      "epoch": 10.010172939979654,
      "grad_norm": 15.925793647766113,
      "learning_rate": 3.998982706002034e-05,
      "loss": 2.9359,
      "step": 19680
    },
    {
      "epoch": 10.015259409969481,
      "grad_norm": 19.154146194458008,
      "learning_rate": 3.998474059003052e-05,
      "loss": 2.8098,
      "step": 19690
    },
    {
      "epoch": 10.020345879959308,
      "grad_norm": 16.8885498046875,
      "learning_rate": 3.9979654120040696e-05,
      "loss": 2.888,
      "step": 19700
    },
    {
      "epoch": 10.025432349949135,
      "grad_norm": 17.635303497314453,
      "learning_rate": 3.9974567650050866e-05,
      "loss": 2.9257,
      "step": 19710
    },
    {
      "epoch": 10.030518819938962,
      "grad_norm": 20.185277938842773,
      "learning_rate": 3.996948118006104e-05,
      "loss": 2.9496,
      "step": 19720
    },
    {
      "epoch": 10.035605289928789,
      "grad_norm": 14.616819381713867,
      "learning_rate": 3.996439471007121e-05,
      "loss": 2.8293,
      "step": 19730
    },
    {
      "epoch": 10.040691759918616,
      "grad_norm": 17.25101089477539,
      "learning_rate": 3.995930824008138e-05,
      "loss": 2.9916,
      "step": 19740
    },
    {
      "epoch": 10.045778229908443,
      "grad_norm": 19.958595275878906,
      "learning_rate": 3.995422177009156e-05,
      "loss": 2.8713,
      "step": 19750
    },
    {
      "epoch": 10.05086469989827,
      "grad_norm": 18.349884033203125,
      "learning_rate": 3.994913530010173e-05,
      "loss": 2.8755,
      "step": 19760
    },
    {
      "epoch": 10.055951169888097,
      "grad_norm": 15.645377159118652,
      "learning_rate": 3.99440488301119e-05,
      "loss": 2.8324,
      "step": 19770
    },
    {
      "epoch": 10.061037639877926,
      "grad_norm": 15.381381034851074,
      "learning_rate": 3.9938962360122076e-05,
      "loss": 2.8166,
      "step": 19780
    },
    {
      "epoch": 10.066124109867753,
      "grad_norm": 25.797138214111328,
      "learning_rate": 3.993387589013225e-05,
      "loss": 2.8835,
      "step": 19790
    },
    {
      "epoch": 10.07121057985758,
      "grad_norm": 17.1701717376709,
      "learning_rate": 3.992878942014242e-05,
      "loss": 2.8099,
      "step": 19800
    },
    {
      "epoch": 10.076297049847406,
      "grad_norm": 17.982776641845703,
      "learning_rate": 3.99237029501526e-05,
      "loss": 2.8343,
      "step": 19810
    },
    {
      "epoch": 10.081383519837233,
      "grad_norm": 19.280542373657227,
      "learning_rate": 3.991861648016277e-05,
      "loss": 2.8385,
      "step": 19820
    },
    {
      "epoch": 10.08646998982706,
      "grad_norm": 18.021438598632812,
      "learning_rate": 3.991353001017294e-05,
      "loss": 2.8249,
      "step": 19830
    },
    {
      "epoch": 10.091556459816887,
      "grad_norm": 15.672769546508789,
      "learning_rate": 3.9908443540183116e-05,
      "loss": 2.8675,
      "step": 19840
    },
    {
      "epoch": 10.096642929806714,
      "grad_norm": 18.532081604003906,
      "learning_rate": 3.9903357070193285e-05,
      "loss": 2.9038,
      "step": 19850
    },
    {
      "epoch": 10.101729399796541,
      "grad_norm": 15.670252799987793,
      "learning_rate": 3.989827060020346e-05,
      "loss": 2.8635,
      "step": 19860
    },
    {
      "epoch": 10.106815869786368,
      "grad_norm": 17.12494659423828,
      "learning_rate": 3.989318413021363e-05,
      "loss": 2.848,
      "step": 19870
    },
    {
      "epoch": 10.111902339776195,
      "grad_norm": 15.802406311035156,
      "learning_rate": 3.988809766022381e-05,
      "loss": 2.7992,
      "step": 19880
    },
    {
      "epoch": 10.116988809766022,
      "grad_norm": 23.873117446899414,
      "learning_rate": 3.9883011190233985e-05,
      "loss": 2.9176,
      "step": 19890
    },
    {
      "epoch": 10.12207527975585,
      "grad_norm": 18.575206756591797,
      "learning_rate": 3.9877924720244155e-05,
      "loss": 2.8681,
      "step": 19900
    },
    {
      "epoch": 10.127161749745676,
      "grad_norm": 13.292927742004395,
      "learning_rate": 3.9872838250254325e-05,
      "loss": 2.9312,
      "step": 19910
    },
    {
      "epoch": 10.132248219735503,
      "grad_norm": 25.11388397216797,
      "learning_rate": 3.98677517802645e-05,
      "loss": 2.8008,
      "step": 19920
    },
    {
      "epoch": 10.13733468972533,
      "grad_norm": 20.10129737854004,
      "learning_rate": 3.986266531027467e-05,
      "loss": 2.8777,
      "step": 19930
    },
    {
      "epoch": 10.142421159715157,
      "grad_norm": 16.310226440429688,
      "learning_rate": 3.985757884028484e-05,
      "loss": 2.8021,
      "step": 19940
    },
    {
      "epoch": 10.147507629704984,
      "grad_norm": 14.677746772766113,
      "learning_rate": 3.985249237029502e-05,
      "loss": 2.8474,
      "step": 19950
    },
    {
      "epoch": 10.152594099694811,
      "grad_norm": 21.19615936279297,
      "learning_rate": 3.984740590030519e-05,
      "loss": 2.823,
      "step": 19960
    },
    {
      "epoch": 10.157680569684638,
      "grad_norm": 21.007423400878906,
      "learning_rate": 3.984231943031536e-05,
      "loss": 2.9476,
      "step": 19970
    },
    {
      "epoch": 10.162767039674465,
      "grad_norm": 16.726728439331055,
      "learning_rate": 3.9837232960325535e-05,
      "loss": 2.8884,
      "step": 19980
    },
    {
      "epoch": 10.167853509664292,
      "grad_norm": 17.88982391357422,
      "learning_rate": 3.983214649033571e-05,
      "loss": 2.7206,
      "step": 19990
    },
    {
      "epoch": 10.172939979654121,
      "grad_norm": 18.51310920715332,
      "learning_rate": 3.982706002034588e-05,
      "loss": 2.9362,
      "step": 20000
    },
    {
      "epoch": 10.178026449643948,
      "grad_norm": 20.358985900878906,
      "learning_rate": 3.982197355035606e-05,
      "loss": 2.8425,
      "step": 20010
    },
    {
      "epoch": 10.183112919633775,
      "grad_norm": 16.891786575317383,
      "learning_rate": 3.981688708036623e-05,
      "loss": 2.8387,
      "step": 20020
    },
    {
      "epoch": 10.188199389623602,
      "grad_norm": 19.407140731811523,
      "learning_rate": 3.98118006103764e-05,
      "loss": 2.8701,
      "step": 20030
    },
    {
      "epoch": 10.193285859613429,
      "grad_norm": 21.520044326782227,
      "learning_rate": 3.9806714140386574e-05,
      "loss": 2.9166,
      "step": 20040
    },
    {
      "epoch": 10.198372329603256,
      "grad_norm": 21.191883087158203,
      "learning_rate": 3.9801627670396744e-05,
      "loss": 2.8819,
      "step": 20050
    },
    {
      "epoch": 10.203458799593083,
      "grad_norm": 18.964494705200195,
      "learning_rate": 3.9796541200406914e-05,
      "loss": 2.8865,
      "step": 20060
    },
    {
      "epoch": 10.20854526958291,
      "grad_norm": 21.868757247924805,
      "learning_rate": 3.979145473041709e-05,
      "loss": 2.8513,
      "step": 20070
    },
    {
      "epoch": 10.213631739572737,
      "grad_norm": 18.72809410095215,
      "learning_rate": 3.978636826042727e-05,
      "loss": 2.8816,
      "step": 20080
    },
    {
      "epoch": 10.218718209562564,
      "grad_norm": 18.93357276916504,
      "learning_rate": 3.978128179043744e-05,
      "loss": 2.9113,
      "step": 20090
    },
    {
      "epoch": 10.22380467955239,
      "grad_norm": 22.08209991455078,
      "learning_rate": 3.9776195320447614e-05,
      "loss": 2.8669,
      "step": 20100
    },
    {
      "epoch": 10.228891149542218,
      "grad_norm": 19.662019729614258,
      "learning_rate": 3.9771108850457784e-05,
      "loss": 2.8421,
      "step": 20110
    },
    {
      "epoch": 10.233977619532045,
      "grad_norm": 19.833768844604492,
      "learning_rate": 3.976602238046796e-05,
      "loss": 2.9515,
      "step": 20120
    },
    {
      "epoch": 10.239064089521872,
      "grad_norm": 20.84111213684082,
      "learning_rate": 3.976093591047813e-05,
      "loss": 2.9099,
      "step": 20130
    },
    {
      "epoch": 10.244150559511699,
      "grad_norm": 24.336381912231445,
      "learning_rate": 3.97558494404883e-05,
      "loss": 2.8757,
      "step": 20140
    },
    {
      "epoch": 10.249237029501526,
      "grad_norm": 18.02033233642578,
      "learning_rate": 3.975076297049848e-05,
      "loss": 2.8322,
      "step": 20150
    },
    {
      "epoch": 10.254323499491353,
      "grad_norm": 19.180906295776367,
      "learning_rate": 3.974567650050865e-05,
      "loss": 2.7786,
      "step": 20160
    },
    {
      "epoch": 10.25940996948118,
      "grad_norm": 19.119985580444336,
      "learning_rate": 3.9740590030518824e-05,
      "loss": 2.8012,
      "step": 20170
    },
    {
      "epoch": 10.264496439471007,
      "grad_norm": 17.395465850830078,
      "learning_rate": 3.9735503560529e-05,
      "loss": 2.8969,
      "step": 20180
    },
    {
      "epoch": 10.269582909460834,
      "grad_norm": 19.43162727355957,
      "learning_rate": 3.973041709053917e-05,
      "loss": 2.7828,
      "step": 20190
    },
    {
      "epoch": 10.27466937945066,
      "grad_norm": 19.274776458740234,
      "learning_rate": 3.972533062054934e-05,
      "loss": 2.8762,
      "step": 20200
    },
    {
      "epoch": 10.279755849440487,
      "grad_norm": 21.659709930419922,
      "learning_rate": 3.972024415055952e-05,
      "loss": 2.8106,
      "step": 20210
    },
    {
      "epoch": 10.284842319430314,
      "grad_norm": 21.937358856201172,
      "learning_rate": 3.971515768056969e-05,
      "loss": 2.8618,
      "step": 20220
    },
    {
      "epoch": 10.289928789420143,
      "grad_norm": 23.04399871826172,
      "learning_rate": 3.9710071210579857e-05,
      "loss": 2.9134,
      "step": 20230
    },
    {
      "epoch": 10.29501525940997,
      "grad_norm": 19.15633773803711,
      "learning_rate": 3.970498474059003e-05,
      "loss": 2.8915,
      "step": 20240
    },
    {
      "epoch": 10.300101729399797,
      "grad_norm": 16.65291976928711,
      "learning_rate": 3.96998982706002e-05,
      "loss": 2.8669,
      "step": 20250
    },
    {
      "epoch": 10.305188199389624,
      "grad_norm": 19.56952667236328,
      "learning_rate": 3.969481180061037e-05,
      "loss": 2.8967,
      "step": 20260
    },
    {
      "epoch": 10.310274669379451,
      "grad_norm": 22.556640625,
      "learning_rate": 3.968972533062055e-05,
      "loss": 2.8002,
      "step": 20270
    },
    {
      "epoch": 10.315361139369278,
      "grad_norm": 17.27200698852539,
      "learning_rate": 3.9684638860630726e-05,
      "loss": 2.9372,
      "step": 20280
    },
    {
      "epoch": 10.320447609359105,
      "grad_norm": 18.670583724975586,
      "learning_rate": 3.9679552390640896e-05,
      "loss": 2.8589,
      "step": 20290
    },
    {
      "epoch": 10.325534079348932,
      "grad_norm": 17.492034912109375,
      "learning_rate": 3.967446592065107e-05,
      "loss": 2.8592,
      "step": 20300
    },
    {
      "epoch": 10.330620549338759,
      "grad_norm": 14.62711238861084,
      "learning_rate": 3.966937945066124e-05,
      "loss": 2.9108,
      "step": 20310
    },
    {
      "epoch": 10.335707019328586,
      "grad_norm": 24.17778968811035,
      "learning_rate": 3.966429298067141e-05,
      "loss": 2.825,
      "step": 20320
    },
    {
      "epoch": 10.340793489318413,
      "grad_norm": 17.433515548706055,
      "learning_rate": 3.965920651068159e-05,
      "loss": 2.8341,
      "step": 20330
    },
    {
      "epoch": 10.34587995930824,
      "grad_norm": 17.840225219726562,
      "learning_rate": 3.965412004069176e-05,
      "loss": 2.795,
      "step": 20340
    },
    {
      "epoch": 10.350966429298067,
      "grad_norm": 16.259992599487305,
      "learning_rate": 3.964903357070193e-05,
      "loss": 2.9176,
      "step": 20350
    },
    {
      "epoch": 10.356052899287894,
      "grad_norm": 18.14045524597168,
      "learning_rate": 3.9643947100712106e-05,
      "loss": 2.922,
      "step": 20360
    },
    {
      "epoch": 10.361139369277721,
      "grad_norm": 22.633665084838867,
      "learning_rate": 3.963886063072228e-05,
      "loss": 2.8305,
      "step": 20370
    },
    {
      "epoch": 10.366225839267548,
      "grad_norm": 21.518077850341797,
      "learning_rate": 3.963377416073245e-05,
      "loss": 2.8633,
      "step": 20380
    },
    {
      "epoch": 10.371312309257375,
      "grad_norm": 18.127546310424805,
      "learning_rate": 3.962868769074263e-05,
      "loss": 2.9199,
      "step": 20390
    },
    {
      "epoch": 10.376398779247202,
      "grad_norm": 15.671205520629883,
      "learning_rate": 3.96236012207528e-05,
      "loss": 2.9154,
      "step": 20400
    },
    {
      "epoch": 10.381485249237029,
      "grad_norm": 16.969419479370117,
      "learning_rate": 3.9618514750762976e-05,
      "loss": 2.877,
      "step": 20410
    },
    {
      "epoch": 10.386571719226856,
      "grad_norm": 17.550962448120117,
      "learning_rate": 3.9613428280773146e-05,
      "loss": 2.7642,
      "step": 20420
    },
    {
      "epoch": 10.391658189216683,
      "grad_norm": 19.300199508666992,
      "learning_rate": 3.9608341810783315e-05,
      "loss": 2.7931,
      "step": 20430
    },
    {
      "epoch": 10.396744659206512,
      "grad_norm": 15.369332313537598,
      "learning_rate": 3.960325534079349e-05,
      "loss": 2.8585,
      "step": 20440
    },
    {
      "epoch": 10.401831129196339,
      "grad_norm": 22.732690811157227,
      "learning_rate": 3.959816887080366e-05,
      "loss": 2.8317,
      "step": 20450
    },
    {
      "epoch": 10.406917599186166,
      "grad_norm": 14.778566360473633,
      "learning_rate": 3.959308240081384e-05,
      "loss": 2.9038,
      "step": 20460
    },
    {
      "epoch": 10.412004069175993,
      "grad_norm": 19.224607467651367,
      "learning_rate": 3.9587995930824015e-05,
      "loss": 2.828,
      "step": 20470
    },
    {
      "epoch": 10.41709053916582,
      "grad_norm": 18.50887107849121,
      "learning_rate": 3.9582909460834185e-05,
      "loss": 2.7714,
      "step": 20480
    },
    {
      "epoch": 10.422177009155646,
      "grad_norm": 21.317575454711914,
      "learning_rate": 3.9577822990844355e-05,
      "loss": 2.8301,
      "step": 20490
    },
    {
      "epoch": 10.427263479145473,
      "grad_norm": 19.37515640258789,
      "learning_rate": 3.957273652085453e-05,
      "loss": 2.8219,
      "step": 20500
    },
    {
      "epoch": 10.4323499491353,
      "grad_norm": 20.749069213867188,
      "learning_rate": 3.95676500508647e-05,
      "loss": 2.894,
      "step": 20510
    },
    {
      "epoch": 10.437436419125127,
      "grad_norm": 17.157258987426758,
      "learning_rate": 3.956256358087487e-05,
      "loss": 2.8579,
      "step": 20520
    },
    {
      "epoch": 10.442522889114954,
      "grad_norm": 19.420032501220703,
      "learning_rate": 3.955747711088505e-05,
      "loss": 2.7554,
      "step": 20530
    },
    {
      "epoch": 10.447609359104781,
      "grad_norm": 15.406341552734375,
      "learning_rate": 3.955239064089522e-05,
      "loss": 2.8757,
      "step": 20540
    },
    {
      "epoch": 10.452695829094608,
      "grad_norm": 22.588266372680664,
      "learning_rate": 3.9547304170905395e-05,
      "loss": 2.8419,
      "step": 20550
    },
    {
      "epoch": 10.457782299084435,
      "grad_norm": 22.82891273498535,
      "learning_rate": 3.9542217700915565e-05,
      "loss": 2.8248,
      "step": 20560
    },
    {
      "epoch": 10.462868769074262,
      "grad_norm": 20.08200454711914,
      "learning_rate": 3.953713123092574e-05,
      "loss": 2.7399,
      "step": 20570
    },
    {
      "epoch": 10.46795523906409,
      "grad_norm": 17.433481216430664,
      "learning_rate": 3.953204476093591e-05,
      "loss": 2.833,
      "step": 20580
    },
    {
      "epoch": 10.473041709053916,
      "grad_norm": 17.02317237854004,
      "learning_rate": 3.952695829094609e-05,
      "loss": 2.7777,
      "step": 20590
    },
    {
      "epoch": 10.478128179043743,
      "grad_norm": 17.6638240814209,
      "learning_rate": 3.952187182095626e-05,
      "loss": 2.7847,
      "step": 20600
    },
    {
      "epoch": 10.48321464903357,
      "grad_norm": 16.863847732543945,
      "learning_rate": 3.951678535096643e-05,
      "loss": 2.8412,
      "step": 20610
    },
    {
      "epoch": 10.488301119023397,
      "grad_norm": 16.097501754760742,
      "learning_rate": 3.9511698880976604e-05,
      "loss": 2.8011,
      "step": 20620
    },
    {
      "epoch": 10.493387589013224,
      "grad_norm": 20.353404998779297,
      "learning_rate": 3.9506612410986774e-05,
      "loss": 2.8448,
      "step": 20630
    },
    {
      "epoch": 10.498474059003051,
      "grad_norm": 19.356243133544922,
      "learning_rate": 3.9501525940996944e-05,
      "loss": 2.8843,
      "step": 20640
    },
    {
      "epoch": 10.503560528992878,
      "grad_norm": 15.768136024475098,
      "learning_rate": 3.949643947100712e-05,
      "loss": 2.8188,
      "step": 20650
    },
    {
      "epoch": 10.508646998982705,
      "grad_norm": 16.388824462890625,
      "learning_rate": 3.94913530010173e-05,
      "loss": 2.8603,
      "step": 20660
    },
    {
      "epoch": 10.513733468972532,
      "grad_norm": 18.11664581298828,
      "learning_rate": 3.9486266531027474e-05,
      "loss": 2.8881,
      "step": 20670
    },
    {
      "epoch": 10.518819938962361,
      "grad_norm": 16.85737419128418,
      "learning_rate": 3.9481180061037644e-05,
      "loss": 2.9328,
      "step": 20680
    },
    {
      "epoch": 10.523906408952188,
      "grad_norm": 18.745359420776367,
      "learning_rate": 3.9476093591047814e-05,
      "loss": 2.8631,
      "step": 20690
    },
    {
      "epoch": 10.528992878942015,
      "grad_norm": 20.331451416015625,
      "learning_rate": 3.947100712105799e-05,
      "loss": 2.9017,
      "step": 20700
    },
    {
      "epoch": 10.534079348931842,
      "grad_norm": 20.08468246459961,
      "learning_rate": 3.946592065106816e-05,
      "loss": 2.8124,
      "step": 20710
    },
    {
      "epoch": 10.539165818921669,
      "grad_norm": 19.111045837402344,
      "learning_rate": 3.946083418107833e-05,
      "loss": 2.8562,
      "step": 20720
    },
    {
      "epoch": 10.544252288911496,
      "grad_norm": 16.866657257080078,
      "learning_rate": 3.945574771108851e-05,
      "loss": 2.9341,
      "step": 20730
    },
    {
      "epoch": 10.549338758901323,
      "grad_norm": 18.816673278808594,
      "learning_rate": 3.945066124109868e-05,
      "loss": 2.8103,
      "step": 20740
    },
    {
      "epoch": 10.55442522889115,
      "grad_norm": 16.7462100982666,
      "learning_rate": 3.9445574771108854e-05,
      "loss": 2.865,
      "step": 20750
    },
    {
      "epoch": 10.559511698880977,
      "grad_norm": 19.950210571289062,
      "learning_rate": 3.944048830111903e-05,
      "loss": 2.821,
      "step": 20760
    },
    {
      "epoch": 10.564598168870804,
      "grad_norm": 19.85320281982422,
      "learning_rate": 3.94354018311292e-05,
      "loss": 2.8123,
      "step": 20770
    },
    {
      "epoch": 10.56968463886063,
      "grad_norm": 16.810895919799805,
      "learning_rate": 3.943031536113937e-05,
      "loss": 2.7618,
      "step": 20780
    },
    {
      "epoch": 10.574771108850458,
      "grad_norm": 17.147491455078125,
      "learning_rate": 3.942522889114955e-05,
      "loss": 2.9111,
      "step": 20790
    },
    {
      "epoch": 10.579857578840285,
      "grad_norm": 18.544967651367188,
      "learning_rate": 3.942014242115972e-05,
      "loss": 2.8458,
      "step": 20800
    },
    {
      "epoch": 10.584944048830112,
      "grad_norm": 20.709184646606445,
      "learning_rate": 3.941505595116989e-05,
      "loss": 2.7879,
      "step": 20810
    },
    {
      "epoch": 10.590030518819939,
      "grad_norm": 14.161674499511719,
      "learning_rate": 3.940996948118006e-05,
      "loss": 2.8296,
      "step": 20820
    },
    {
      "epoch": 10.595116988809766,
      "grad_norm": 20.84160041809082,
      "learning_rate": 3.940488301119023e-05,
      "loss": 2.8486,
      "step": 20830
    },
    {
      "epoch": 10.600203458799593,
      "grad_norm": 19.011938095092773,
      "learning_rate": 3.939979654120041e-05,
      "loss": 2.8524,
      "step": 20840
    },
    {
      "epoch": 10.60528992878942,
      "grad_norm": 17.60879898071289,
      "learning_rate": 3.9394710071210587e-05,
      "loss": 2.7307,
      "step": 20850
    },
    {
      "epoch": 10.610376398779247,
      "grad_norm": 15.848828315734863,
      "learning_rate": 3.9389623601220756e-05,
      "loss": 2.8889,
      "step": 20860
    },
    {
      "epoch": 10.615462868769074,
      "grad_norm": 21.446651458740234,
      "learning_rate": 3.9384537131230926e-05,
      "loss": 2.7758,
      "step": 20870
    },
    {
      "epoch": 10.6205493387589,
      "grad_norm": 22.295713424682617,
      "learning_rate": 3.93794506612411e-05,
      "loss": 2.7797,
      "step": 20880
    },
    {
      "epoch": 10.62563580874873,
      "grad_norm": 21.57613754272461,
      "learning_rate": 3.937436419125127e-05,
      "loss": 2.8974,
      "step": 20890
    },
    {
      "epoch": 10.630722278738556,
      "grad_norm": 19.322357177734375,
      "learning_rate": 3.936927772126144e-05,
      "loss": 2.7606,
      "step": 20900
    },
    {
      "epoch": 10.635808748728383,
      "grad_norm": 19.7751522064209,
      "learning_rate": 3.936419125127162e-05,
      "loss": 2.8924,
      "step": 20910
    },
    {
      "epoch": 10.64089521871821,
      "grad_norm": 24.398040771484375,
      "learning_rate": 3.935910478128179e-05,
      "loss": 2.8137,
      "step": 20920
    },
    {
      "epoch": 10.645981688708037,
      "grad_norm": 18.72324562072754,
      "learning_rate": 3.9354018311291966e-05,
      "loss": 2.8214,
      "step": 20930
    },
    {
      "epoch": 10.651068158697864,
      "grad_norm": 15.596647262573242,
      "learning_rate": 3.9348931841302136e-05,
      "loss": 2.8401,
      "step": 20940
    },
    {
      "epoch": 10.656154628687691,
      "grad_norm": 18.63135528564453,
      "learning_rate": 3.934384537131231e-05,
      "loss": 2.8725,
      "step": 20950
    },
    {
      "epoch": 10.661241098677518,
      "grad_norm": 22.661664962768555,
      "learning_rate": 3.933875890132249e-05,
      "loss": 2.8447,
      "step": 20960
    },
    {
      "epoch": 10.666327568667345,
      "grad_norm": 17.18400001525879,
      "learning_rate": 3.933367243133266e-05,
      "loss": 2.7644,
      "step": 20970
    },
    {
      "epoch": 10.671414038657172,
      "grad_norm": 16.4204044342041,
      "learning_rate": 3.932858596134283e-05,
      "loss": 2.7606,
      "step": 20980
    },
    {
      "epoch": 10.676500508646999,
      "grad_norm": 17.38124656677246,
      "learning_rate": 3.9323499491353006e-05,
      "loss": 2.8651,
      "step": 20990
    },
    {
      "epoch": 10.681586978636826,
      "grad_norm": 17.613636016845703,
      "learning_rate": 3.9318413021363176e-05,
      "loss": 2.8858,
      "step": 21000
    },
    {
      "epoch": 10.686673448626653,
      "grad_norm": 18.623977661132812,
      "learning_rate": 3.9313326551373345e-05,
      "loss": 2.7277,
      "step": 21010
    },
    {
      "epoch": 10.69175991861648,
      "grad_norm": 19.75860595703125,
      "learning_rate": 3.930824008138352e-05,
      "loss": 2.7424,
      "step": 21020
    },
    {
      "epoch": 10.696846388606307,
      "grad_norm": 29.80388832092285,
      "learning_rate": 3.930315361139369e-05,
      "loss": 2.7851,
      "step": 21030
    },
    {
      "epoch": 10.701932858596134,
      "grad_norm": 18.830434799194336,
      "learning_rate": 3.929806714140387e-05,
      "loss": 2.7827,
      "step": 21040
    },
    {
      "epoch": 10.707019328585961,
      "grad_norm": 21.462419509887695,
      "learning_rate": 3.9292980671414045e-05,
      "loss": 2.8465,
      "step": 21050
    },
    {
      "epoch": 10.712105798575788,
      "grad_norm": 18.461734771728516,
      "learning_rate": 3.9287894201424215e-05,
      "loss": 2.8262,
      "step": 21060
    },
    {
      "epoch": 10.717192268565615,
      "grad_norm": 17.517822265625,
      "learning_rate": 3.9282807731434385e-05,
      "loss": 2.8704,
      "step": 21070
    },
    {
      "epoch": 10.722278738555442,
      "grad_norm": 21.218963623046875,
      "learning_rate": 3.927772126144456e-05,
      "loss": 2.8372,
      "step": 21080
    },
    {
      "epoch": 10.727365208545269,
      "grad_norm": 20.331375122070312,
      "learning_rate": 3.927263479145473e-05,
      "loss": 2.8473,
      "step": 21090
    },
    {
      "epoch": 10.732451678535096,
      "grad_norm": 19.740495681762695,
      "learning_rate": 3.92675483214649e-05,
      "loss": 2.7993,
      "step": 21100
    },
    {
      "epoch": 10.737538148524923,
      "grad_norm": 21.917640686035156,
      "learning_rate": 3.926246185147508e-05,
      "loss": 2.7657,
      "step": 21110
    },
    {
      "epoch": 10.742624618514752,
      "grad_norm": 21.129783630371094,
      "learning_rate": 3.925737538148525e-05,
      "loss": 2.8231,
      "step": 21120
    },
    {
      "epoch": 10.747711088504579,
      "grad_norm": 24.61094093322754,
      "learning_rate": 3.9252288911495425e-05,
      "loss": 2.8448,
      "step": 21130
    },
    {
      "epoch": 10.752797558494406,
      "grad_norm": 19.750410079956055,
      "learning_rate": 3.92472024415056e-05,
      "loss": 2.8067,
      "step": 21140
    },
    {
      "epoch": 10.757884028484233,
      "grad_norm": 18.977745056152344,
      "learning_rate": 3.924211597151577e-05,
      "loss": 2.7987,
      "step": 21150
    },
    {
      "epoch": 10.76297049847406,
      "grad_norm": 18.721904754638672,
      "learning_rate": 3.923702950152594e-05,
      "loss": 2.7855,
      "step": 21160
    },
    {
      "epoch": 10.768056968463886,
      "grad_norm": 17.996517181396484,
      "learning_rate": 3.923194303153612e-05,
      "loss": 2.8205,
      "step": 21170
    },
    {
      "epoch": 10.773143438453713,
      "grad_norm": 21.42969512939453,
      "learning_rate": 3.922685656154629e-05,
      "loss": 2.8088,
      "step": 21180
    },
    {
      "epoch": 10.77822990844354,
      "grad_norm": 30.439157485961914,
      "learning_rate": 3.922177009155646e-05,
      "loss": 2.8136,
      "step": 21190
    },
    {
      "epoch": 10.783316378433367,
      "grad_norm": 18.205461502075195,
      "learning_rate": 3.9216683621566634e-05,
      "loss": 2.7798,
      "step": 21200
    },
    {
      "epoch": 10.788402848423194,
      "grad_norm": 14.408455848693848,
      "learning_rate": 3.9211597151576804e-05,
      "loss": 2.8435,
      "step": 21210
    },
    {
      "epoch": 10.793489318413021,
      "grad_norm": 16.934537887573242,
      "learning_rate": 3.920651068158698e-05,
      "loss": 2.8316,
      "step": 21220
    },
    {
      "epoch": 10.798575788402848,
      "grad_norm": 21.77813720703125,
      "learning_rate": 3.920142421159715e-05,
      "loss": 2.8346,
      "step": 21230
    },
    {
      "epoch": 10.803662258392675,
      "grad_norm": 22.671348571777344,
      "learning_rate": 3.919633774160733e-05,
      "loss": 2.8108,
      "step": 21240
    },
    {
      "epoch": 10.808748728382502,
      "grad_norm": 15.916820526123047,
      "learning_rate": 3.9191251271617504e-05,
      "loss": 2.8521,
      "step": 21250
    },
    {
      "epoch": 10.81383519837233,
      "grad_norm": 20.98160743713379,
      "learning_rate": 3.9186164801627674e-05,
      "loss": 2.8194,
      "step": 21260
    },
    {
      "epoch": 10.818921668362156,
      "grad_norm": 20.483867645263672,
      "learning_rate": 3.9181078331637844e-05,
      "loss": 2.7931,
      "step": 21270
    },
    {
      "epoch": 10.824008138351983,
      "grad_norm": 21.685487747192383,
      "learning_rate": 3.917599186164802e-05,
      "loss": 2.8641,
      "step": 21280
    },
    {
      "epoch": 10.82909460834181,
      "grad_norm": 18.630462646484375,
      "learning_rate": 3.917090539165819e-05,
      "loss": 2.8431,
      "step": 21290
    },
    {
      "epoch": 10.834181078331637,
      "grad_norm": 16.780160903930664,
      "learning_rate": 3.916581892166836e-05,
      "loss": 2.8077,
      "step": 21300
    },
    {
      "epoch": 10.839267548321464,
      "grad_norm": 17.130647659301758,
      "learning_rate": 3.916073245167854e-05,
      "loss": 2.8634,
      "step": 21310
    },
    {
      "epoch": 10.844354018311291,
      "grad_norm": 16.627588272094727,
      "learning_rate": 3.915564598168871e-05,
      "loss": 2.9106,
      "step": 21320
    },
    {
      "epoch": 10.84944048830112,
      "grad_norm": 18.692598342895508,
      "learning_rate": 3.9150559511698884e-05,
      "loss": 2.8076,
      "step": 21330
    },
    {
      "epoch": 10.854526958290947,
      "grad_norm": 20.750347137451172,
      "learning_rate": 3.914547304170906e-05,
      "loss": 2.8319,
      "step": 21340
    },
    {
      "epoch": 10.859613428280774,
      "grad_norm": 17.804580688476562,
      "learning_rate": 3.914038657171923e-05,
      "loss": 2.7764,
      "step": 21350
    },
    {
      "epoch": 10.864699898270601,
      "grad_norm": 17.485136032104492,
      "learning_rate": 3.91353001017294e-05,
      "loss": 2.9032,
      "step": 21360
    },
    {
      "epoch": 10.869786368260428,
      "grad_norm": 18.388879776000977,
      "learning_rate": 3.913021363173958e-05,
      "loss": 2.9146,
      "step": 21370
    },
    {
      "epoch": 10.874872838250255,
      "grad_norm": 16.84370231628418,
      "learning_rate": 3.912512716174975e-05,
      "loss": 2.7667,
      "step": 21380
    },
    {
      "epoch": 10.879959308240082,
      "grad_norm": 19.707008361816406,
      "learning_rate": 3.912004069175992e-05,
      "loss": 2.8333,
      "step": 21390
    },
    {
      "epoch": 10.885045778229909,
      "grad_norm": 20.182153701782227,
      "learning_rate": 3.911495422177009e-05,
      "loss": 2.8038,
      "step": 21400
    },
    {
      "epoch": 10.890132248219736,
      "grad_norm": 17.892358779907227,
      "learning_rate": 3.910986775178026e-05,
      "loss": 2.7381,
      "step": 21410
    },
    {
      "epoch": 10.895218718209563,
      "grad_norm": 24.6174373626709,
      "learning_rate": 3.910478128179044e-05,
      "loss": 2.7517,
      "step": 21420
    },
    {
      "epoch": 10.90030518819939,
      "grad_norm": 19.914823532104492,
      "learning_rate": 3.9099694811800617e-05,
      "loss": 2.8781,
      "step": 21430
    },
    {
      "epoch": 10.905391658189217,
      "grad_norm": 18.1698055267334,
      "learning_rate": 3.9094608341810786e-05,
      "loss": 2.7599,
      "step": 21440
    },
    {
      "epoch": 10.910478128179044,
      "grad_norm": 20.56409454345703,
      "learning_rate": 3.9089521871820956e-05,
      "loss": 2.7336,
      "step": 21450
    },
    {
      "epoch": 10.91556459816887,
      "grad_norm": 20.26105308532715,
      "learning_rate": 3.908443540183113e-05,
      "loss": 2.724,
      "step": 21460
    },
    {
      "epoch": 10.920651068158698,
      "grad_norm": 21.467905044555664,
      "learning_rate": 3.90793489318413e-05,
      "loss": 2.7688,
      "step": 21470
    },
    {
      "epoch": 10.925737538148525,
      "grad_norm": 20.820053100585938,
      "learning_rate": 3.907426246185148e-05,
      "loss": 2.7262,
      "step": 21480
    },
    {
      "epoch": 10.930824008138352,
      "grad_norm": 22.338361740112305,
      "learning_rate": 3.906917599186165e-05,
      "loss": 2.76,
      "step": 21490
    },
    {
      "epoch": 10.935910478128179,
      "grad_norm": 23.395475387573242,
      "learning_rate": 3.906408952187182e-05,
      "loss": 2.827,
      "step": 21500
    },
    {
      "epoch": 10.940996948118006,
      "grad_norm": 18.974029541015625,
      "learning_rate": 3.9059003051881996e-05,
      "loss": 2.7653,
      "step": 21510
    },
    {
      "epoch": 10.946083418107833,
      "grad_norm": 18.909957885742188,
      "learning_rate": 3.905391658189217e-05,
      "loss": 2.8669,
      "step": 21520
    },
    {
      "epoch": 10.95116988809766,
      "grad_norm": 19.359811782836914,
      "learning_rate": 3.904883011190234e-05,
      "loss": 2.855,
      "step": 21530
    },
    {
      "epoch": 10.956256358087487,
      "grad_norm": 24.57283592224121,
      "learning_rate": 3.904374364191252e-05,
      "loss": 2.8707,
      "step": 21540
    },
    {
      "epoch": 10.961342828077314,
      "grad_norm": 21.120969772338867,
      "learning_rate": 3.903865717192269e-05,
      "loss": 2.7523,
      "step": 21550
    },
    {
      "epoch": 10.96642929806714,
      "grad_norm": 16.83354377746582,
      "learning_rate": 3.903357070193286e-05,
      "loss": 2.7945,
      "step": 21560
    },
    {
      "epoch": 10.97151576805697,
      "grad_norm": 23.331802368164062,
      "learning_rate": 3.9028484231943036e-05,
      "loss": 2.7647,
      "step": 21570
    },
    {
      "epoch": 10.976602238046796,
      "grad_norm": 20.248775482177734,
      "learning_rate": 3.9023397761953206e-05,
      "loss": 2.8384,
      "step": 21580
    },
    {
      "epoch": 10.981688708036623,
      "grad_norm": 23.18598175048828,
      "learning_rate": 3.9018311291963375e-05,
      "loss": 2.8043,
      "step": 21590
    },
    {
      "epoch": 10.98677517802645,
      "grad_norm": 17.677764892578125,
      "learning_rate": 3.901322482197355e-05,
      "loss": 2.8119,
      "step": 21600
    },
    {
      "epoch": 10.991861648016277,
      "grad_norm": 20.074981689453125,
      "learning_rate": 3.900813835198372e-05,
      "loss": 2.7433,
      "step": 21610
    },
    {
      "epoch": 10.996948118006104,
      "grad_norm": 16.33331871032715,
      "learning_rate": 3.90030518819939e-05,
      "loss": 2.7844,
      "step": 21620
    },
    {
      "epoch": 11.0,
      "eval_loss": 3.811251163482666,
      "eval_runtime": 2.7079,
      "eval_samples_per_second": 1024.764,
      "eval_steps_per_second": 128.142,
      "step": 21626
    },
    {
      "epoch": 11.002034587995931,
      "grad_norm": 21.593887329101562,
      "learning_rate": 3.8997965412004075e-05,
      "loss": 2.8452,
      "step": 21630
    },
    {
      "epoch": 11.007121057985758,
      "grad_norm": 19.762060165405273,
      "learning_rate": 3.8992878942014245e-05,
      "loss": 2.7657,
      "step": 21640
    },
    {
      "epoch": 11.012207527975585,
      "grad_norm": 19.293106079101562,
      "learning_rate": 3.8987792472024415e-05,
      "loss": 2.8465,
      "step": 21650
    },
    {
      "epoch": 11.017293997965412,
      "grad_norm": 16.828088760375977,
      "learning_rate": 3.898270600203459e-05,
      "loss": 2.7705,
      "step": 21660
    },
    {
      "epoch": 11.022380467955239,
      "grad_norm": 18.876548767089844,
      "learning_rate": 3.897761953204476e-05,
      "loss": 2.7258,
      "step": 21670
    },
    {
      "epoch": 11.027466937945066,
      "grad_norm": 27.097904205322266,
      "learning_rate": 3.897253306205493e-05,
      "loss": 2.8133,
      "step": 21680
    },
    {
      "epoch": 11.032553407934893,
      "grad_norm": 23.05379867553711,
      "learning_rate": 3.896744659206511e-05,
      "loss": 2.8256,
      "step": 21690
    },
    {
      "epoch": 11.03763987792472,
      "grad_norm": 16.600811004638672,
      "learning_rate": 3.896236012207528e-05,
      "loss": 2.7985,
      "step": 21700
    },
    {
      "epoch": 11.042726347914547,
      "grad_norm": 28.439661026000977,
      "learning_rate": 3.8957273652085455e-05,
      "loss": 2.7858,
      "step": 21710
    },
    {
      "epoch": 11.047812817904374,
      "grad_norm": 18.23356819152832,
      "learning_rate": 3.895218718209563e-05,
      "loss": 2.7719,
      "step": 21720
    },
    {
      "epoch": 11.052899287894201,
      "grad_norm": 22.59870719909668,
      "learning_rate": 3.89471007121058e-05,
      "loss": 2.8207,
      "step": 21730
    },
    {
      "epoch": 11.057985757884028,
      "grad_norm": 15.182524681091309,
      "learning_rate": 3.894201424211598e-05,
      "loss": 2.7789,
      "step": 21740
    },
    {
      "epoch": 11.063072227873855,
      "grad_norm": 16.029272079467773,
      "learning_rate": 3.893692777212615e-05,
      "loss": 2.8094,
      "step": 21750
    },
    {
      "epoch": 11.068158697863682,
      "grad_norm": 24.17420768737793,
      "learning_rate": 3.893184130213632e-05,
      "loss": 2.7802,
      "step": 21760
    },
    {
      "epoch": 11.073245167853509,
      "grad_norm": 18.245344161987305,
      "learning_rate": 3.8926754832146495e-05,
      "loss": 2.7905,
      "step": 21770
    },
    {
      "epoch": 11.078331637843336,
      "grad_norm": 20.092815399169922,
      "learning_rate": 3.8921668362156664e-05,
      "loss": 2.7789,
      "step": 21780
    },
    {
      "epoch": 11.083418107833165,
      "grad_norm": 22.9559383392334,
      "learning_rate": 3.8916581892166834e-05,
      "loss": 2.7893,
      "step": 21790
    },
    {
      "epoch": 11.088504577822992,
      "grad_norm": 22.82319450378418,
      "learning_rate": 3.891149542217701e-05,
      "loss": 2.7199,
      "step": 21800
    },
    {
      "epoch": 11.093591047812819,
      "grad_norm": 19.970014572143555,
      "learning_rate": 3.890640895218719e-05,
      "loss": 2.7576,
      "step": 21810
    },
    {
      "epoch": 11.098677517802646,
      "grad_norm": 20.524599075317383,
      "learning_rate": 3.890132248219736e-05,
      "loss": 2.7393,
      "step": 21820
    },
    {
      "epoch": 11.103763987792473,
      "grad_norm": 21.611879348754883,
      "learning_rate": 3.8896236012207534e-05,
      "loss": 2.7861,
      "step": 21830
    },
    {
      "epoch": 11.1088504577823,
      "grad_norm": 17.11014747619629,
      "learning_rate": 3.8891149542217704e-05,
      "loss": 2.9174,
      "step": 21840
    },
    {
      "epoch": 11.113936927772126,
      "grad_norm": 23.225618362426758,
      "learning_rate": 3.8886063072227874e-05,
      "loss": 2.7218,
      "step": 21850
    },
    {
      "epoch": 11.119023397761953,
      "grad_norm": 19.047767639160156,
      "learning_rate": 3.888097660223805e-05,
      "loss": 2.7958,
      "step": 21860
    },
    {
      "epoch": 11.12410986775178,
      "grad_norm": 14.95315170288086,
      "learning_rate": 3.887589013224822e-05,
      "loss": 2.7428,
      "step": 21870
    },
    {
      "epoch": 11.129196337741607,
      "grad_norm": 19.355966567993164,
      "learning_rate": 3.887080366225839e-05,
      "loss": 2.817,
      "step": 21880
    },
    {
      "epoch": 11.134282807731434,
      "grad_norm": 19.831483840942383,
      "learning_rate": 3.886571719226857e-05,
      "loss": 2.7341,
      "step": 21890
    },
    {
      "epoch": 11.139369277721261,
      "grad_norm": 19.61813735961914,
      "learning_rate": 3.886063072227874e-05,
      "loss": 2.8133,
      "step": 21900
    },
    {
      "epoch": 11.144455747711088,
      "grad_norm": 20.895130157470703,
      "learning_rate": 3.8855544252288914e-05,
      "loss": 2.7842,
      "step": 21910
    },
    {
      "epoch": 11.149542217700915,
      "grad_norm": 25.401884078979492,
      "learning_rate": 3.885045778229909e-05,
      "loss": 2.7116,
      "step": 21920
    },
    {
      "epoch": 11.154628687690742,
      "grad_norm": 16.911514282226562,
      "learning_rate": 3.884537131230926e-05,
      "loss": 2.7588,
      "step": 21930
    },
    {
      "epoch": 11.15971515768057,
      "grad_norm": 20.179412841796875,
      "learning_rate": 3.884028484231943e-05,
      "loss": 2.807,
      "step": 21940
    },
    {
      "epoch": 11.164801627670396,
      "grad_norm": 19.390766143798828,
      "learning_rate": 3.883519837232961e-05,
      "loss": 2.798,
      "step": 21950
    },
    {
      "epoch": 11.169888097660223,
      "grad_norm": 19.114694595336914,
      "learning_rate": 3.883011190233978e-05,
      "loss": 2.759,
      "step": 21960
    },
    {
      "epoch": 11.17497456765005,
      "grad_norm": 21.65285301208496,
      "learning_rate": 3.882502543234995e-05,
      "loss": 2.6984,
      "step": 21970
    },
    {
      "epoch": 11.180061037639877,
      "grad_norm": 22.838420867919922,
      "learning_rate": 3.881993896236012e-05,
      "loss": 2.7685,
      "step": 21980
    },
    {
      "epoch": 11.185147507629704,
      "grad_norm": 25.098068237304688,
      "learning_rate": 3.881485249237029e-05,
      "loss": 2.7596,
      "step": 21990
    },
    {
      "epoch": 11.190233977619531,
      "grad_norm": 19.476701736450195,
      "learning_rate": 3.880976602238047e-05,
      "loss": 2.7674,
      "step": 22000
    },
    {
      "epoch": 11.19532044760936,
      "grad_norm": 23.89430809020996,
      "learning_rate": 3.8804679552390647e-05,
      "loss": 2.7852,
      "step": 22010
    },
    {
      "epoch": 11.200406917599187,
      "grad_norm": 16.94744110107422,
      "learning_rate": 3.8799593082400816e-05,
      "loss": 2.7603,
      "step": 22020
    },
    {
      "epoch": 11.205493387589014,
      "grad_norm": 20.52509117126465,
      "learning_rate": 3.879450661241099e-05,
      "loss": 2.821,
      "step": 22030
    },
    {
      "epoch": 11.210579857578841,
      "grad_norm": 20.611591339111328,
      "learning_rate": 3.878942014242116e-05,
      "loss": 2.7983,
      "step": 22040
    },
    {
      "epoch": 11.215666327568668,
      "grad_norm": 19.779542922973633,
      "learning_rate": 3.878433367243133e-05,
      "loss": 2.7391,
      "step": 22050
    },
    {
      "epoch": 11.220752797558495,
      "grad_norm": 19.86223793029785,
      "learning_rate": 3.877924720244151e-05,
      "loss": 2.7547,
      "step": 22060
    },
    {
      "epoch": 11.225839267548322,
      "grad_norm": 14.790937423706055,
      "learning_rate": 3.877416073245168e-05,
      "loss": 2.7611,
      "step": 22070
    },
    {
      "epoch": 11.230925737538149,
      "grad_norm": 19.519271850585938,
      "learning_rate": 3.876907426246185e-05,
      "loss": 2.7289,
      "step": 22080
    },
    {
      "epoch": 11.236012207527976,
      "grad_norm": 20.854751586914062,
      "learning_rate": 3.8763987792472026e-05,
      "loss": 2.7567,
      "step": 22090
    },
    {
      "epoch": 11.241098677517803,
      "grad_norm": 21.6315975189209,
      "learning_rate": 3.87589013224822e-05,
      "loss": 2.6882,
      "step": 22100
    },
    {
      "epoch": 11.24618514750763,
      "grad_norm": 19.490419387817383,
      "learning_rate": 3.875381485249237e-05,
      "loss": 2.8005,
      "step": 22110
    },
    {
      "epoch": 11.251271617497457,
      "grad_norm": 17.121530532836914,
      "learning_rate": 3.874872838250255e-05,
      "loss": 2.7782,
      "step": 22120
    },
    {
      "epoch": 11.256358087487284,
      "grad_norm": 25.228145599365234,
      "learning_rate": 3.874364191251272e-05,
      "loss": 2.7326,
      "step": 22130
    },
    {
      "epoch": 11.26144455747711,
      "grad_norm": 24.31155776977539,
      "learning_rate": 3.873855544252289e-05,
      "loss": 2.7735,
      "step": 22140
    },
    {
      "epoch": 11.266531027466938,
      "grad_norm": 16.069686889648438,
      "learning_rate": 3.8733468972533066e-05,
      "loss": 2.8323,
      "step": 22150
    },
    {
      "epoch": 11.271617497456765,
      "grad_norm": 20.004802703857422,
      "learning_rate": 3.8728382502543236e-05,
      "loss": 2.7937,
      "step": 22160
    },
    {
      "epoch": 11.276703967446592,
      "grad_norm": 24.43902587890625,
      "learning_rate": 3.8723296032553405e-05,
      "loss": 2.6649,
      "step": 22170
    },
    {
      "epoch": 11.281790437436419,
      "grad_norm": 18.945175170898438,
      "learning_rate": 3.871820956256358e-05,
      "loss": 2.7611,
      "step": 22180
    },
    {
      "epoch": 11.286876907426246,
      "grad_norm": 21.50430679321289,
      "learning_rate": 3.871312309257375e-05,
      "loss": 2.7111,
      "step": 22190
    },
    {
      "epoch": 11.291963377416073,
      "grad_norm": 24.30327033996582,
      "learning_rate": 3.870803662258393e-05,
      "loss": 2.8235,
      "step": 22200
    },
    {
      "epoch": 11.2970498474059,
      "grad_norm": 18.87114715576172,
      "learning_rate": 3.8702950152594105e-05,
      "loss": 2.6666,
      "step": 22210
    },
    {
      "epoch": 11.302136317395727,
      "grad_norm": 19.520761489868164,
      "learning_rate": 3.8697863682604275e-05,
      "loss": 2.7097,
      "step": 22220
    },
    {
      "epoch": 11.307222787385555,
      "grad_norm": 20.831130981445312,
      "learning_rate": 3.8692777212614445e-05,
      "loss": 2.7246,
      "step": 22230
    },
    {
      "epoch": 11.312309257375382,
      "grad_norm": 21.48367691040039,
      "learning_rate": 3.868769074262462e-05,
      "loss": 2.6755,
      "step": 22240
    },
    {
      "epoch": 11.31739572736521,
      "grad_norm": 19.88458824157715,
      "learning_rate": 3.868260427263479e-05,
      "loss": 2.8043,
      "step": 22250
    },
    {
      "epoch": 11.322482197355036,
      "grad_norm": 21.049161911010742,
      "learning_rate": 3.867751780264496e-05,
      "loss": 2.8339,
      "step": 22260
    },
    {
      "epoch": 11.327568667344863,
      "grad_norm": 18.370037078857422,
      "learning_rate": 3.867243133265514e-05,
      "loss": 2.7379,
      "step": 22270
    },
    {
      "epoch": 11.33265513733469,
      "grad_norm": 20.200321197509766,
      "learning_rate": 3.866734486266531e-05,
      "loss": 2.7496,
      "step": 22280
    },
    {
      "epoch": 11.337741607324517,
      "grad_norm": 19.27316665649414,
      "learning_rate": 3.8662258392675485e-05,
      "loss": 2.742,
      "step": 22290
    },
    {
      "epoch": 11.342828077314344,
      "grad_norm": 20.586912155151367,
      "learning_rate": 3.865717192268566e-05,
      "loss": 2.7805,
      "step": 22300
    },
    {
      "epoch": 11.347914547304171,
      "grad_norm": 22.515159606933594,
      "learning_rate": 3.865208545269583e-05,
      "loss": 2.7904,
      "step": 22310
    },
    {
      "epoch": 11.353001017293998,
      "grad_norm": 19.56134033203125,
      "learning_rate": 3.864699898270601e-05,
      "loss": 2.7247,
      "step": 22320
    },
    {
      "epoch": 11.358087487283825,
      "grad_norm": 22.719402313232422,
      "learning_rate": 3.864191251271618e-05,
      "loss": 2.7587,
      "step": 22330
    },
    {
      "epoch": 11.363173957273652,
      "grad_norm": 20.554885864257812,
      "learning_rate": 3.863682604272635e-05,
      "loss": 2.6297,
      "step": 22340
    },
    {
      "epoch": 11.368260427263479,
      "grad_norm": 20.6072940826416,
      "learning_rate": 3.8631739572736525e-05,
      "loss": 2.7734,
      "step": 22350
    },
    {
      "epoch": 11.373346897253306,
      "grad_norm": 24.69867706298828,
      "learning_rate": 3.8626653102746694e-05,
      "loss": 2.7504,
      "step": 22360
    },
    {
      "epoch": 11.378433367243133,
      "grad_norm": 23.293354034423828,
      "learning_rate": 3.8621566632756864e-05,
      "loss": 2.788,
      "step": 22370
    },
    {
      "epoch": 11.38351983723296,
      "grad_norm": 19.933208465576172,
      "learning_rate": 3.861648016276704e-05,
      "loss": 2.7668,
      "step": 22380
    },
    {
      "epoch": 11.388606307222787,
      "grad_norm": 22.533483505249023,
      "learning_rate": 3.861139369277722e-05,
      "loss": 2.7624,
      "step": 22390
    },
    {
      "epoch": 11.393692777212614,
      "grad_norm": 21.236635208129883,
      "learning_rate": 3.860630722278739e-05,
      "loss": 2.7439,
      "step": 22400
    },
    {
      "epoch": 11.398779247202441,
      "grad_norm": 19.90292739868164,
      "learning_rate": 3.8601220752797564e-05,
      "loss": 2.7273,
      "step": 22410
    },
    {
      "epoch": 11.403865717192268,
      "grad_norm": 20.1163272857666,
      "learning_rate": 3.8596134282807734e-05,
      "loss": 2.7879,
      "step": 22420
    },
    {
      "epoch": 11.408952187182095,
      "grad_norm": 19.050312042236328,
      "learning_rate": 3.8591047812817904e-05,
      "loss": 2.791,
      "step": 22430
    },
    {
      "epoch": 11.414038657171922,
      "grad_norm": 15.517632484436035,
      "learning_rate": 3.858596134282808e-05,
      "loss": 2.8205,
      "step": 22440
    },
    {
      "epoch": 11.419125127161749,
      "grad_norm": 25.350234985351562,
      "learning_rate": 3.858087487283825e-05,
      "loss": 2.6783,
      "step": 22450
    },
    {
      "epoch": 11.424211597151578,
      "grad_norm": 19.562604904174805,
      "learning_rate": 3.857578840284842e-05,
      "loss": 2.7541,
      "step": 22460
    },
    {
      "epoch": 11.429298067141405,
      "grad_norm": 25.107128143310547,
      "learning_rate": 3.85707019328586e-05,
      "loss": 2.7917,
      "step": 22470
    },
    {
      "epoch": 11.434384537131232,
      "grad_norm": 18.698226928710938,
      "learning_rate": 3.8565615462868774e-05,
      "loss": 2.7796,
      "step": 22480
    },
    {
      "epoch": 11.439471007121059,
      "grad_norm": 25.090604782104492,
      "learning_rate": 3.8560528992878944e-05,
      "loss": 2.7132,
      "step": 22490
    },
    {
      "epoch": 11.444557477110886,
      "grad_norm": 21.549055099487305,
      "learning_rate": 3.855544252288912e-05,
      "loss": 2.8442,
      "step": 22500
    },
    {
      "epoch": 11.449643947100713,
      "grad_norm": 20.284669876098633,
      "learning_rate": 3.855035605289929e-05,
      "loss": 2.7727,
      "step": 22510
    },
    {
      "epoch": 11.45473041709054,
      "grad_norm": 20.13149070739746,
      "learning_rate": 3.854526958290946e-05,
      "loss": 2.7825,
      "step": 22520
    },
    {
      "epoch": 11.459816887080366,
      "grad_norm": 17.802444458007812,
      "learning_rate": 3.854018311291964e-05,
      "loss": 2.7396,
      "step": 22530
    },
    {
      "epoch": 11.464903357070193,
      "grad_norm": 16.516963958740234,
      "learning_rate": 3.853509664292981e-05,
      "loss": 2.7495,
      "step": 22540
    },
    {
      "epoch": 11.46998982706002,
      "grad_norm": 22.512104034423828,
      "learning_rate": 3.8530010172939983e-05,
      "loss": 2.6507,
      "step": 22550
    },
    {
      "epoch": 11.475076297049847,
      "grad_norm": 18.327821731567383,
      "learning_rate": 3.852492370295015e-05,
      "loss": 2.7708,
      "step": 22560
    },
    {
      "epoch": 11.480162767039674,
      "grad_norm": 22.14789390563965,
      "learning_rate": 3.851983723296032e-05,
      "loss": 2.7317,
      "step": 22570
    },
    {
      "epoch": 11.485249237029501,
      "grad_norm": 16.698305130004883,
      "learning_rate": 3.85147507629705e-05,
      "loss": 2.6706,
      "step": 22580
    },
    {
      "epoch": 11.490335707019328,
      "grad_norm": 17.78123664855957,
      "learning_rate": 3.8509664292980677e-05,
      "loss": 2.6884,
      "step": 22590
    },
    {
      "epoch": 11.495422177009155,
      "grad_norm": 20.249629974365234,
      "learning_rate": 3.8504577822990846e-05,
      "loss": 2.8051,
      "step": 22600
    },
    {
      "epoch": 11.500508646998982,
      "grad_norm": 17.304309844970703,
      "learning_rate": 3.849949135300102e-05,
      "loss": 2.7129,
      "step": 22610
    },
    {
      "epoch": 11.50559511698881,
      "grad_norm": 21.395858764648438,
      "learning_rate": 3.849440488301119e-05,
      "loss": 2.7201,
      "step": 22620
    },
    {
      "epoch": 11.510681586978636,
      "grad_norm": 19.119956970214844,
      "learning_rate": 3.848931841302136e-05,
      "loss": 2.7501,
      "step": 22630
    },
    {
      "epoch": 11.515768056968463,
      "grad_norm": 21.47637367248535,
      "learning_rate": 3.848423194303154e-05,
      "loss": 2.7498,
      "step": 22640
    },
    {
      "epoch": 11.52085452695829,
      "grad_norm": 24.9534912109375,
      "learning_rate": 3.847914547304171e-05,
      "loss": 2.6901,
      "step": 22650
    },
    {
      "epoch": 11.525940996948117,
      "grad_norm": 20.697223663330078,
      "learning_rate": 3.847405900305188e-05,
      "loss": 2.8658,
      "step": 22660
    },
    {
      "epoch": 11.531027466937946,
      "grad_norm": 21.402292251586914,
      "learning_rate": 3.8468972533062056e-05,
      "loss": 2.8396,
      "step": 22670
    },
    {
      "epoch": 11.536113936927773,
      "grad_norm": 33.00079345703125,
      "learning_rate": 3.846388606307223e-05,
      "loss": 2.7293,
      "step": 22680
    },
    {
      "epoch": 11.5412004069176,
      "grad_norm": 21.426538467407227,
      "learning_rate": 3.84587995930824e-05,
      "loss": 2.7232,
      "step": 22690
    },
    {
      "epoch": 11.546286876907427,
      "grad_norm": 25.461904525756836,
      "learning_rate": 3.845371312309258e-05,
      "loss": 2.8057,
      "step": 22700
    },
    {
      "epoch": 11.551373346897254,
      "grad_norm": 25.770858764648438,
      "learning_rate": 3.844862665310275e-05,
      "loss": 2.7756,
      "step": 22710
    },
    {
      "epoch": 11.556459816887081,
      "grad_norm": 23.4028263092041,
      "learning_rate": 3.844354018311292e-05,
      "loss": 2.69,
      "step": 22720
    },
    {
      "epoch": 11.561546286876908,
      "grad_norm": 17.35580062866211,
      "learning_rate": 3.8438453713123096e-05,
      "loss": 2.7307,
      "step": 22730
    },
    {
      "epoch": 11.566632756866735,
      "grad_norm": 20.473548889160156,
      "learning_rate": 3.8433367243133266e-05,
      "loss": 2.771,
      "step": 22740
    },
    {
      "epoch": 11.571719226856562,
      "grad_norm": 18.04205322265625,
      "learning_rate": 3.8428280773143435e-05,
      "loss": 2.7601,
      "step": 22750
    },
    {
      "epoch": 11.576805696846389,
      "grad_norm": 25.015335083007812,
      "learning_rate": 3.842319430315361e-05,
      "loss": 2.7697,
      "step": 22760
    },
    {
      "epoch": 11.581892166836216,
      "grad_norm": 17.26896095275879,
      "learning_rate": 3.841810783316379e-05,
      "loss": 2.7502,
      "step": 22770
    },
    {
      "epoch": 11.586978636826043,
      "grad_norm": 26.492660522460938,
      "learning_rate": 3.841302136317396e-05,
      "loss": 2.8235,
      "step": 22780
    },
    {
      "epoch": 11.59206510681587,
      "grad_norm": 19.748008728027344,
      "learning_rate": 3.8407934893184135e-05,
      "loss": 2.7864,
      "step": 22790
    },
    {
      "epoch": 11.597151576805697,
      "grad_norm": 19.345083236694336,
      "learning_rate": 3.8402848423194305e-05,
      "loss": 2.8116,
      "step": 22800
    },
    {
      "epoch": 11.602238046795524,
      "grad_norm": 20.157968521118164,
      "learning_rate": 3.8397761953204475e-05,
      "loss": 2.7979,
      "step": 22810
    },
    {
      "epoch": 11.60732451678535,
      "grad_norm": 15.777881622314453,
      "learning_rate": 3.839267548321465e-05,
      "loss": 2.7835,
      "step": 22820
    },
    {
      "epoch": 11.612410986775178,
      "grad_norm": 21.062009811401367,
      "learning_rate": 3.838758901322482e-05,
      "loss": 2.7771,
      "step": 22830
    },
    {
      "epoch": 11.617497456765005,
      "grad_norm": 21.31230354309082,
      "learning_rate": 3.8382502543235e-05,
      "loss": 2.7559,
      "step": 22840
    },
    {
      "epoch": 11.622583926754832,
      "grad_norm": 18.30986785888672,
      "learning_rate": 3.837741607324517e-05,
      "loss": 2.6681,
      "step": 22850
    },
    {
      "epoch": 11.627670396744659,
      "grad_norm": 17.9965877532959,
      "learning_rate": 3.837232960325534e-05,
      "loss": 2.8397,
      "step": 22860
    },
    {
      "epoch": 11.632756866734486,
      "grad_norm": 22.151126861572266,
      "learning_rate": 3.8367243133265515e-05,
      "loss": 2.6685,
      "step": 22870
    },
    {
      "epoch": 11.637843336724313,
      "grad_norm": 17.581790924072266,
      "learning_rate": 3.836215666327569e-05,
      "loss": 2.7306,
      "step": 22880
    },
    {
      "epoch": 11.64292980671414,
      "grad_norm": 20.881961822509766,
      "learning_rate": 3.835707019328586e-05,
      "loss": 2.7417,
      "step": 22890
    },
    {
      "epoch": 11.648016276703967,
      "grad_norm": 22.661828994750977,
      "learning_rate": 3.835198372329604e-05,
      "loss": 2.7683,
      "step": 22900
    },
    {
      "epoch": 11.653102746693795,
      "grad_norm": 19.697967529296875,
      "learning_rate": 3.834689725330621e-05,
      "loss": 2.8139,
      "step": 22910
    },
    {
      "epoch": 11.658189216683622,
      "grad_norm": 19.93389892578125,
      "learning_rate": 3.834181078331638e-05,
      "loss": 2.7934,
      "step": 22920
    },
    {
      "epoch": 11.66327568667345,
      "grad_norm": 22.908157348632812,
      "learning_rate": 3.8336724313326555e-05,
      "loss": 2.7157,
      "step": 22930
    },
    {
      "epoch": 11.668362156663276,
      "grad_norm": 29.387006759643555,
      "learning_rate": 3.8331637843336724e-05,
      "loss": 2.7595,
      "step": 22940
    },
    {
      "epoch": 11.673448626653103,
      "grad_norm": 25.67847442626953,
      "learning_rate": 3.8326551373346894e-05,
      "loss": 2.691,
      "step": 22950
    },
    {
      "epoch": 11.67853509664293,
      "grad_norm": 22.26468276977539,
      "learning_rate": 3.832146490335707e-05,
      "loss": 2.7461,
      "step": 22960
    },
    {
      "epoch": 11.683621566632757,
      "grad_norm": 29.15597915649414,
      "learning_rate": 3.831637843336725e-05,
      "loss": 2.7369,
      "step": 22970
    },
    {
      "epoch": 11.688708036622584,
      "grad_norm": 17.133399963378906,
      "learning_rate": 3.831129196337742e-05,
      "loss": 2.7115,
      "step": 22980
    },
    {
      "epoch": 11.693794506612411,
      "grad_norm": 22.182729721069336,
      "learning_rate": 3.8306205493387594e-05,
      "loss": 2.7471,
      "step": 22990
    },
    {
      "epoch": 11.698880976602238,
      "grad_norm": 16.213346481323242,
      "learning_rate": 3.8301119023397764e-05,
      "loss": 2.6897,
      "step": 23000
    },
    {
      "epoch": 11.703967446592065,
      "grad_norm": 20.350725173950195,
      "learning_rate": 3.8296032553407934e-05,
      "loss": 2.7303,
      "step": 23010
    },
    {
      "epoch": 11.709053916581892,
      "grad_norm": 21.296104431152344,
      "learning_rate": 3.829094608341811e-05,
      "loss": 2.7317,
      "step": 23020
    },
    {
      "epoch": 11.714140386571719,
      "grad_norm": 20.96907615661621,
      "learning_rate": 3.828585961342828e-05,
      "loss": 2.7777,
      "step": 23030
    },
    {
      "epoch": 11.719226856561546,
      "grad_norm": 18.2613582611084,
      "learning_rate": 3.828077314343845e-05,
      "loss": 2.7396,
      "step": 23040
    },
    {
      "epoch": 11.724313326551373,
      "grad_norm": 15.795475006103516,
      "learning_rate": 3.827568667344863e-05,
      "loss": 2.7238,
      "step": 23050
    },
    {
      "epoch": 11.7293997965412,
      "grad_norm": 14.279106140136719,
      "learning_rate": 3.8270600203458804e-05,
      "loss": 2.8129,
      "step": 23060
    },
    {
      "epoch": 11.734486266531027,
      "grad_norm": 23.758607864379883,
      "learning_rate": 3.8265513733468974e-05,
      "loss": 2.7146,
      "step": 23070
    },
    {
      "epoch": 11.739572736520854,
      "grad_norm": 22.895559310913086,
      "learning_rate": 3.826042726347915e-05,
      "loss": 2.7476,
      "step": 23080
    },
    {
      "epoch": 11.744659206510681,
      "grad_norm": 31.839427947998047,
      "learning_rate": 3.825534079348932e-05,
      "loss": 2.6823,
      "step": 23090
    },
    {
      "epoch": 11.749745676500508,
      "grad_norm": 22.665220260620117,
      "learning_rate": 3.82502543234995e-05,
      "loss": 2.7467,
      "step": 23100
    },
    {
      "epoch": 11.754832146490335,
      "grad_norm": 20.52402687072754,
      "learning_rate": 3.824516785350967e-05,
      "loss": 2.7313,
      "step": 23110
    },
    {
      "epoch": 11.759918616480164,
      "grad_norm": 19.31149673461914,
      "learning_rate": 3.824008138351984e-05,
      "loss": 2.7572,
      "step": 23120
    },
    {
      "epoch": 11.76500508646999,
      "grad_norm": 21.162263870239258,
      "learning_rate": 3.8234994913530013e-05,
      "loss": 2.7461,
      "step": 23130
    },
    {
      "epoch": 11.770091556459818,
      "grad_norm": 18.986370086669922,
      "learning_rate": 3.822990844354018e-05,
      "loss": 2.7199,
      "step": 23140
    },
    {
      "epoch": 11.775178026449645,
      "grad_norm": 22.819429397583008,
      "learning_rate": 3.822482197355035e-05,
      "loss": 2.7861,
      "step": 23150
    },
    {
      "epoch": 11.780264496439472,
      "grad_norm": 22.162084579467773,
      "learning_rate": 3.821973550356053e-05,
      "loss": 2.7081,
      "step": 23160
    },
    {
      "epoch": 11.785350966429299,
      "grad_norm": 20.29315948486328,
      "learning_rate": 3.8214649033570707e-05,
      "loss": 2.8154,
      "step": 23170
    },
    {
      "epoch": 11.790437436419126,
      "grad_norm": 20.034894943237305,
      "learning_rate": 3.8209562563580876e-05,
      "loss": 2.7221,
      "step": 23180
    },
    {
      "epoch": 11.795523906408953,
      "grad_norm": 22.35992431640625,
      "learning_rate": 3.820447609359105e-05,
      "loss": 2.6861,
      "step": 23190
    },
    {
      "epoch": 11.80061037639878,
      "grad_norm": 20.984586715698242,
      "learning_rate": 3.819938962360122e-05,
      "loss": 2.7608,
      "step": 23200
    },
    {
      "epoch": 11.805696846388607,
      "grad_norm": 19.524381637573242,
      "learning_rate": 3.819430315361139e-05,
      "loss": 2.7087,
      "step": 23210
    },
    {
      "epoch": 11.810783316378433,
      "grad_norm": 23.723655700683594,
      "learning_rate": 3.818921668362157e-05,
      "loss": 2.7806,
      "step": 23220
    },
    {
      "epoch": 11.81586978636826,
      "grad_norm": 21.28441619873047,
      "learning_rate": 3.818413021363174e-05,
      "loss": 2.7706,
      "step": 23230
    },
    {
      "epoch": 11.820956256358087,
      "grad_norm": 21.00736427307129,
      "learning_rate": 3.817904374364191e-05,
      "loss": 2.7479,
      "step": 23240
    },
    {
      "epoch": 11.826042726347914,
      "grad_norm": 22.646419525146484,
      "learning_rate": 3.8173957273652086e-05,
      "loss": 2.7277,
      "step": 23250
    },
    {
      "epoch": 11.831129196337741,
      "grad_norm": 22.016454696655273,
      "learning_rate": 3.816887080366226e-05,
      "loss": 2.6748,
      "step": 23260
    },
    {
      "epoch": 11.836215666327568,
      "grad_norm": 22.24337387084961,
      "learning_rate": 3.816378433367243e-05,
      "loss": 2.7517,
      "step": 23270
    },
    {
      "epoch": 11.841302136317395,
      "grad_norm": 19.767913818359375,
      "learning_rate": 3.815869786368261e-05,
      "loss": 2.6316,
      "step": 23280
    },
    {
      "epoch": 11.846388606307222,
      "grad_norm": 23.500789642333984,
      "learning_rate": 3.815361139369278e-05,
      "loss": 2.8438,
      "step": 23290
    },
    {
      "epoch": 11.85147507629705,
      "grad_norm": 27.124300003051758,
      "learning_rate": 3.814852492370295e-05,
      "loss": 2.6083,
      "step": 23300
    },
    {
      "epoch": 11.856561546286876,
      "grad_norm": 18.94940185546875,
      "learning_rate": 3.8143438453713126e-05,
      "loss": 2.7305,
      "step": 23310
    },
    {
      "epoch": 11.861648016276703,
      "grad_norm": 22.181020736694336,
      "learning_rate": 3.8138351983723296e-05,
      "loss": 2.6879,
      "step": 23320
    },
    {
      "epoch": 11.86673448626653,
      "grad_norm": 22.794937133789062,
      "learning_rate": 3.8133265513733466e-05,
      "loss": 2.7563,
      "step": 23330
    },
    {
      "epoch": 11.871820956256357,
      "grad_norm": 21.399335861206055,
      "learning_rate": 3.812817904374364e-05,
      "loss": 2.7063,
      "step": 23340
    },
    {
      "epoch": 11.876907426246184,
      "grad_norm": 21.712072372436523,
      "learning_rate": 3.812309257375382e-05,
      "loss": 2.7846,
      "step": 23350
    },
    {
      "epoch": 11.881993896236013,
      "grad_norm": 18.2066707611084,
      "learning_rate": 3.8118006103763996e-05,
      "loss": 2.7425,
      "step": 23360
    },
    {
      "epoch": 11.88708036622584,
      "grad_norm": 23.331497192382812,
      "learning_rate": 3.8112919633774165e-05,
      "loss": 2.6831,
      "step": 23370
    },
    {
      "epoch": 11.892166836215667,
      "grad_norm": 20.649757385253906,
      "learning_rate": 3.8107833163784335e-05,
      "loss": 2.7436,
      "step": 23380
    },
    {
      "epoch": 11.897253306205494,
      "grad_norm": 20.747892379760742,
      "learning_rate": 3.810274669379451e-05,
      "loss": 2.654,
      "step": 23390
    },
    {
      "epoch": 11.902339776195321,
      "grad_norm": 20.715085983276367,
      "learning_rate": 3.809766022380468e-05,
      "loss": 2.7982,
      "step": 23400
    },
    {
      "epoch": 11.907426246185148,
      "grad_norm": 19.310260772705078,
      "learning_rate": 3.809257375381485e-05,
      "loss": 2.7126,
      "step": 23410
    },
    {
      "epoch": 11.912512716174975,
      "grad_norm": 18.82754135131836,
      "learning_rate": 3.808748728382503e-05,
      "loss": 2.7064,
      "step": 23420
    },
    {
      "epoch": 11.917599186164802,
      "grad_norm": 22.9812068939209,
      "learning_rate": 3.80824008138352e-05,
      "loss": 2.7061,
      "step": 23430
    },
    {
      "epoch": 11.922685656154629,
      "grad_norm": 21.464767456054688,
      "learning_rate": 3.8077314343845375e-05,
      "loss": 2.755,
      "step": 23440
    },
    {
      "epoch": 11.927772126144456,
      "grad_norm": 23.895526885986328,
      "learning_rate": 3.8072227873855545e-05,
      "loss": 2.7285,
      "step": 23450
    },
    {
      "epoch": 11.932858596134283,
      "grad_norm": 24.303606033325195,
      "learning_rate": 3.806714140386572e-05,
      "loss": 2.7094,
      "step": 23460
    },
    {
      "epoch": 11.93794506612411,
      "grad_norm": 19.38591957092285,
      "learning_rate": 3.806205493387589e-05,
      "loss": 2.6689,
      "step": 23470
    },
    {
      "epoch": 11.943031536113937,
      "grad_norm": 18.968482971191406,
      "learning_rate": 3.805696846388607e-05,
      "loss": 2.627,
      "step": 23480
    },
    {
      "epoch": 11.948118006103764,
      "grad_norm": 18.924936294555664,
      "learning_rate": 3.805188199389624e-05,
      "loss": 2.7257,
      "step": 23490
    },
    {
      "epoch": 11.95320447609359,
      "grad_norm": 22.261009216308594,
      "learning_rate": 3.804679552390641e-05,
      "loss": 2.686,
      "step": 23500
    },
    {
      "epoch": 11.958290946083418,
      "grad_norm": 25.78392791748047,
      "learning_rate": 3.8041709053916585e-05,
      "loss": 2.7149,
      "step": 23510
    },
    {
      "epoch": 11.963377416073245,
      "grad_norm": 22.588552474975586,
      "learning_rate": 3.8036622583926754e-05,
      "loss": 2.6305,
      "step": 23520
    },
    {
      "epoch": 11.968463886063072,
      "grad_norm": 17.292003631591797,
      "learning_rate": 3.8031536113936924e-05,
      "loss": 2.6882,
      "step": 23530
    },
    {
      "epoch": 11.973550356052899,
      "grad_norm": 25.617324829101562,
      "learning_rate": 3.80264496439471e-05,
      "loss": 2.6333,
      "step": 23540
    },
    {
      "epoch": 11.978636826042726,
      "grad_norm": 19.819189071655273,
      "learning_rate": 3.802136317395728e-05,
      "loss": 2.7177,
      "step": 23550
    },
    {
      "epoch": 11.983723296032553,
      "grad_norm": 23.851879119873047,
      "learning_rate": 3.801627670396745e-05,
      "loss": 2.7401,
      "step": 23560
    },
    {
      "epoch": 11.988809766022381,
      "grad_norm": 25.103418350219727,
      "learning_rate": 3.8011190233977624e-05,
      "loss": 2.7596,
      "step": 23570
    },
    {
      "epoch": 11.993896236012208,
      "grad_norm": 24.701513290405273,
      "learning_rate": 3.8006103763987794e-05,
      "loss": 2.7381,
      "step": 23580
    },
    {
      "epoch": 11.998982706002035,
      "grad_norm": 19.66347885131836,
      "learning_rate": 3.8001017293997964e-05,
      "loss": 2.7054,
      "step": 23590
    },
    {
      "epoch": 12.0,
      "eval_loss": 3.8623926639556885,
      "eval_runtime": 2.6643,
      "eval_samples_per_second": 1041.539,
      "eval_steps_per_second": 130.239,
      "step": 23592
    },
    {
      "epoch": 12.004069175991862,
      "grad_norm": 26.70317268371582,
      "learning_rate": 3.799593082400814e-05,
      "loss": 2.6487,
      "step": 23600
    },
    {
      "epoch": 12.00915564598169,
      "grad_norm": 20.68559455871582,
      "learning_rate": 3.799084435401831e-05,
      "loss": 2.6687,
      "step": 23610
    },
    {
      "epoch": 12.014242115971516,
      "grad_norm": 26.24175453186035,
      "learning_rate": 3.798575788402848e-05,
      "loss": 2.6796,
      "step": 23620
    },
    {
      "epoch": 12.019328585961343,
      "grad_norm": 21.45163345336914,
      "learning_rate": 3.798067141403866e-05,
      "loss": 2.6963,
      "step": 23630
    },
    {
      "epoch": 12.02441505595117,
      "grad_norm": 18.995113372802734,
      "learning_rate": 3.7975584944048834e-05,
      "loss": 2.7443,
      "step": 23640
    },
    {
      "epoch": 12.029501525940997,
      "grad_norm": 17.361534118652344,
      "learning_rate": 3.797049847405901e-05,
      "loss": 2.6633,
      "step": 23650
    },
    {
      "epoch": 12.034587995930824,
      "grad_norm": 24.805551528930664,
      "learning_rate": 3.796541200406918e-05,
      "loss": 2.7086,
      "step": 23660
    },
    {
      "epoch": 12.039674465920651,
      "grad_norm": 18.2790470123291,
      "learning_rate": 3.796032553407935e-05,
      "loss": 2.7405,
      "step": 23670
    },
    {
      "epoch": 12.044760935910478,
      "grad_norm": 21.951757431030273,
      "learning_rate": 3.795523906408953e-05,
      "loss": 2.723,
      "step": 23680
    },
    {
      "epoch": 12.049847405900305,
      "grad_norm": 21.265830993652344,
      "learning_rate": 3.79501525940997e-05,
      "loss": 2.6343,
      "step": 23690
    },
    {
      "epoch": 12.054933875890132,
      "grad_norm": 19.750186920166016,
      "learning_rate": 3.794506612410987e-05,
      "loss": 2.7149,
      "step": 23700
    },
    {
      "epoch": 12.060020345879959,
      "grad_norm": 23.161848068237305,
      "learning_rate": 3.7939979654120043e-05,
      "loss": 2.7147,
      "step": 23710
    },
    {
      "epoch": 12.065106815869786,
      "grad_norm": 23.033405303955078,
      "learning_rate": 3.793489318413021e-05,
      "loss": 2.8008,
      "step": 23720
    },
    {
      "epoch": 12.070193285859613,
      "grad_norm": 23.173688888549805,
      "learning_rate": 3.792980671414039e-05,
      "loss": 2.6303,
      "step": 23730
    },
    {
      "epoch": 12.07527975584944,
      "grad_norm": 19.885400772094727,
      "learning_rate": 3.792472024415057e-05,
      "loss": 2.6032,
      "step": 23740
    },
    {
      "epoch": 12.080366225839267,
      "grad_norm": 19.40361976623535,
      "learning_rate": 3.7919633774160737e-05,
      "loss": 2.7082,
      "step": 23750
    },
    {
      "epoch": 12.085452695829094,
      "grad_norm": 21.627784729003906,
      "learning_rate": 3.7914547304170906e-05,
      "loss": 2.7283,
      "step": 23760
    },
    {
      "epoch": 12.090539165818921,
      "grad_norm": 24.499027252197266,
      "learning_rate": 3.790946083418108e-05,
      "loss": 2.7019,
      "step": 23770
    },
    {
      "epoch": 12.095625635808748,
      "grad_norm": 22.71116065979004,
      "learning_rate": 3.790437436419125e-05,
      "loss": 2.6299,
      "step": 23780
    },
    {
      "epoch": 12.100712105798575,
      "grad_norm": 20.558622360229492,
      "learning_rate": 3.789928789420142e-05,
      "loss": 2.7335,
      "step": 23790
    },
    {
      "epoch": 12.105798575788404,
      "grad_norm": 26.51136589050293,
      "learning_rate": 3.78942014242116e-05,
      "loss": 2.7475,
      "step": 23800
    },
    {
      "epoch": 12.11088504577823,
      "grad_norm": 22.635759353637695,
      "learning_rate": 3.788911495422177e-05,
      "loss": 2.7354,
      "step": 23810
    },
    {
      "epoch": 12.115971515768058,
      "grad_norm": 26.957691192626953,
      "learning_rate": 3.788402848423194e-05,
      "loss": 2.6879,
      "step": 23820
    },
    {
      "epoch": 12.121057985757885,
      "grad_norm": 24.51953887939453,
      "learning_rate": 3.7878942014242116e-05,
      "loss": 2.7158,
      "step": 23830
    },
    {
      "epoch": 12.126144455747712,
      "grad_norm": 17.373918533325195,
      "learning_rate": 3.787385554425229e-05,
      "loss": 2.6221,
      "step": 23840
    },
    {
      "epoch": 12.131230925737539,
      "grad_norm": 18.077606201171875,
      "learning_rate": 3.786876907426246e-05,
      "loss": 2.6552,
      "step": 23850
    },
    {
      "epoch": 12.136317395727366,
      "grad_norm": 20.906137466430664,
      "learning_rate": 3.786368260427264e-05,
      "loss": 2.6487,
      "step": 23860
    },
    {
      "epoch": 12.141403865717193,
      "grad_norm": 24.352737426757812,
      "learning_rate": 3.785859613428281e-05,
      "loss": 2.552,
      "step": 23870
    },
    {
      "epoch": 12.14649033570702,
      "grad_norm": 31.503618240356445,
      "learning_rate": 3.785350966429298e-05,
      "loss": 2.7072,
      "step": 23880
    },
    {
      "epoch": 12.151576805696847,
      "grad_norm": 22.457475662231445,
      "learning_rate": 3.7848423194303156e-05,
      "loss": 2.7035,
      "step": 23890
    },
    {
      "epoch": 12.156663275686673,
      "grad_norm": 26.56629753112793,
      "learning_rate": 3.7843336724313326e-05,
      "loss": 2.7215,
      "step": 23900
    },
    {
      "epoch": 12.1617497456765,
      "grad_norm": 21.491252899169922,
      "learning_rate": 3.78382502543235e-05,
      "loss": 2.6552,
      "step": 23910
    },
    {
      "epoch": 12.166836215666327,
      "grad_norm": 22.094219207763672,
      "learning_rate": 3.783316378433367e-05,
      "loss": 2.6375,
      "step": 23920
    },
    {
      "epoch": 12.171922685656154,
      "grad_norm": 27.75609588623047,
      "learning_rate": 3.782807731434385e-05,
      "loss": 2.6157,
      "step": 23930
    },
    {
      "epoch": 12.177009155645981,
      "grad_norm": 18.492998123168945,
      "learning_rate": 3.7822990844354026e-05,
      "loss": 2.686,
      "step": 23940
    },
    {
      "epoch": 12.182095625635808,
      "grad_norm": 23.334077835083008,
      "learning_rate": 3.7817904374364195e-05,
      "loss": 2.6961,
      "step": 23950
    },
    {
      "epoch": 12.187182095625635,
      "grad_norm": 20.646459579467773,
      "learning_rate": 3.7812817904374365e-05,
      "loss": 2.7084,
      "step": 23960
    },
    {
      "epoch": 12.192268565615462,
      "grad_norm": 21.071414947509766,
      "learning_rate": 3.780773143438454e-05,
      "loss": 2.7038,
      "step": 23970
    },
    {
      "epoch": 12.19735503560529,
      "grad_norm": 28.822284698486328,
      "learning_rate": 3.780264496439471e-05,
      "loss": 2.7201,
      "step": 23980
    },
    {
      "epoch": 12.202441505595116,
      "grad_norm": 22.86301040649414,
      "learning_rate": 3.779755849440488e-05,
      "loss": 2.6058,
      "step": 23990
    },
    {
      "epoch": 12.207527975584943,
      "grad_norm": 21.395156860351562,
      "learning_rate": 3.779247202441506e-05,
      "loss": 2.7269,
      "step": 24000
    },
    {
      "epoch": 12.21261444557477,
      "grad_norm": 20.536806106567383,
      "learning_rate": 3.778738555442523e-05,
      "loss": 2.7019,
      "step": 24010
    },
    {
      "epoch": 12.217700915564599,
      "grad_norm": 19.03197479248047,
      "learning_rate": 3.7782299084435405e-05,
      "loss": 2.6181,
      "step": 24020
    },
    {
      "epoch": 12.222787385554426,
      "grad_norm": 16.98179054260254,
      "learning_rate": 3.777721261444558e-05,
      "loss": 2.6874,
      "step": 24030
    },
    {
      "epoch": 12.227873855544253,
      "grad_norm": 19.362470626831055,
      "learning_rate": 3.777212614445575e-05,
      "loss": 2.7247,
      "step": 24040
    },
    {
      "epoch": 12.23296032553408,
      "grad_norm": 27.420198440551758,
      "learning_rate": 3.776703967446592e-05,
      "loss": 2.6543,
      "step": 24050
    },
    {
      "epoch": 12.238046795523907,
      "grad_norm": 21.92498207092285,
      "learning_rate": 3.77619532044761e-05,
      "loss": 2.6314,
      "step": 24060
    },
    {
      "epoch": 12.243133265513734,
      "grad_norm": 22.71173095703125,
      "learning_rate": 3.775686673448627e-05,
      "loss": 2.6576,
      "step": 24070
    },
    {
      "epoch": 12.248219735503561,
      "grad_norm": 25.536529541015625,
      "learning_rate": 3.775178026449644e-05,
      "loss": 2.7158,
      "step": 24080
    },
    {
      "epoch": 12.253306205493388,
      "grad_norm": 22.182003021240234,
      "learning_rate": 3.7746693794506615e-05,
      "loss": 2.6806,
      "step": 24090
    },
    {
      "epoch": 12.258392675483215,
      "grad_norm": 20.83319091796875,
      "learning_rate": 3.7741607324516784e-05,
      "loss": 2.7163,
      "step": 24100
    },
    {
      "epoch": 12.263479145473042,
      "grad_norm": 26.09567642211914,
      "learning_rate": 3.7736520854526954e-05,
      "loss": 2.6999,
      "step": 24110
    },
    {
      "epoch": 12.268565615462869,
      "grad_norm": 27.664403915405273,
      "learning_rate": 3.773143438453713e-05,
      "loss": 2.6283,
      "step": 24120
    },
    {
      "epoch": 12.273652085452696,
      "grad_norm": 17.31485366821289,
      "learning_rate": 3.772634791454731e-05,
      "loss": 2.6818,
      "step": 24130
    },
    {
      "epoch": 12.278738555442523,
      "grad_norm": 24.88094139099121,
      "learning_rate": 3.772126144455748e-05,
      "loss": 2.6279,
      "step": 24140
    },
    {
      "epoch": 12.28382502543235,
      "grad_norm": 22.898008346557617,
      "learning_rate": 3.7716174974567654e-05,
      "loss": 2.7411,
      "step": 24150
    },
    {
      "epoch": 12.288911495422177,
      "grad_norm": 26.145551681518555,
      "learning_rate": 3.7711088504577824e-05,
      "loss": 2.723,
      "step": 24160
    },
    {
      "epoch": 12.293997965412004,
      "grad_norm": 20.354368209838867,
      "learning_rate": 3.7706002034588e-05,
      "loss": 2.7482,
      "step": 24170
    },
    {
      "epoch": 12.29908443540183,
      "grad_norm": 22.921586990356445,
      "learning_rate": 3.770091556459817e-05,
      "loss": 2.7382,
      "step": 24180
    },
    {
      "epoch": 12.304170905391658,
      "grad_norm": 17.77687644958496,
      "learning_rate": 3.769582909460834e-05,
      "loss": 2.693,
      "step": 24190
    },
    {
      "epoch": 12.309257375381485,
      "grad_norm": 24.97138023376465,
      "learning_rate": 3.769074262461852e-05,
      "loss": 2.7307,
      "step": 24200
    },
    {
      "epoch": 12.314343845371312,
      "grad_norm": 25.086990356445312,
      "learning_rate": 3.768565615462869e-05,
      "loss": 2.6777,
      "step": 24210
    },
    {
      "epoch": 12.319430315361139,
      "grad_norm": 20.8211727142334,
      "learning_rate": 3.7680569684638864e-05,
      "loss": 2.6572,
      "step": 24220
    },
    {
      "epoch": 12.324516785350966,
      "grad_norm": 23.740684509277344,
      "learning_rate": 3.767548321464904e-05,
      "loss": 2.6726,
      "step": 24230
    },
    {
      "epoch": 12.329603255340793,
      "grad_norm": 24.469974517822266,
      "learning_rate": 3.767039674465921e-05,
      "loss": 2.6775,
      "step": 24240
    },
    {
      "epoch": 12.334689725330621,
      "grad_norm": 24.271562576293945,
      "learning_rate": 3.766531027466938e-05,
      "loss": 2.6947,
      "step": 24250
    },
    {
      "epoch": 12.339776195320448,
      "grad_norm": 22.79188346862793,
      "learning_rate": 3.766022380467956e-05,
      "loss": 2.6624,
      "step": 24260
    },
    {
      "epoch": 12.344862665310275,
      "grad_norm": 21.625656127929688,
      "learning_rate": 3.765513733468973e-05,
      "loss": 2.6764,
      "step": 24270
    },
    {
      "epoch": 12.349949135300102,
      "grad_norm": 23.045682907104492,
      "learning_rate": 3.76500508646999e-05,
      "loss": 2.6504,
      "step": 24280
    },
    {
      "epoch": 12.35503560528993,
      "grad_norm": 25.293609619140625,
      "learning_rate": 3.7644964394710073e-05,
      "loss": 2.6991,
      "step": 24290
    },
    {
      "epoch": 12.360122075279756,
      "grad_norm": 22.943178176879883,
      "learning_rate": 3.763987792472024e-05,
      "loss": 2.668,
      "step": 24300
    },
    {
      "epoch": 12.365208545269583,
      "grad_norm": 28.4421329498291,
      "learning_rate": 3.763479145473042e-05,
      "loss": 2.6566,
      "step": 24310
    },
    {
      "epoch": 12.37029501525941,
      "grad_norm": 22.331037521362305,
      "learning_rate": 3.76297049847406e-05,
      "loss": 2.6903,
      "step": 24320
    },
    {
      "epoch": 12.375381485249237,
      "grad_norm": 18.943695068359375,
      "learning_rate": 3.7624618514750767e-05,
      "loss": 2.6743,
      "step": 24330
    },
    {
      "epoch": 12.380467955239064,
      "grad_norm": 19.197933197021484,
      "learning_rate": 3.7619532044760936e-05,
      "loss": 2.6914,
      "step": 24340
    },
    {
      "epoch": 12.385554425228891,
      "grad_norm": 20.354726791381836,
      "learning_rate": 3.761444557477111e-05,
      "loss": 2.7232,
      "step": 24350
    },
    {
      "epoch": 12.390640895218718,
      "grad_norm": 17.1406192779541,
      "learning_rate": 3.760935910478128e-05,
      "loss": 2.6361,
      "step": 24360
    },
    {
      "epoch": 12.395727365208545,
      "grad_norm": 17.069555282592773,
      "learning_rate": 3.760427263479145e-05,
      "loss": 2.6069,
      "step": 24370
    },
    {
      "epoch": 12.400813835198372,
      "grad_norm": 25.281064987182617,
      "learning_rate": 3.759918616480163e-05,
      "loss": 2.7251,
      "step": 24380
    },
    {
      "epoch": 12.405900305188199,
      "grad_norm": 22.360939025878906,
      "learning_rate": 3.75940996948118e-05,
      "loss": 2.6588,
      "step": 24390
    },
    {
      "epoch": 12.410986775178026,
      "grad_norm": 20.594512939453125,
      "learning_rate": 3.7589013224821976e-05,
      "loss": 2.6553,
      "step": 24400
    },
    {
      "epoch": 12.416073245167853,
      "grad_norm": 27.09566307067871,
      "learning_rate": 3.7583926754832146e-05,
      "loss": 2.6594,
      "step": 24410
    },
    {
      "epoch": 12.42115971515768,
      "grad_norm": 20.98194122314453,
      "learning_rate": 3.757884028484232e-05,
      "loss": 2.6786,
      "step": 24420
    },
    {
      "epoch": 12.426246185147507,
      "grad_norm": 19.528127670288086,
      "learning_rate": 3.757375381485249e-05,
      "loss": 2.7964,
      "step": 24430
    },
    {
      "epoch": 12.431332655137334,
      "grad_norm": 22.109203338623047,
      "learning_rate": 3.756866734486267e-05,
      "loss": 2.6635,
      "step": 24440
    },
    {
      "epoch": 12.436419125127161,
      "grad_norm": 20.160512924194336,
      "learning_rate": 3.756358087487284e-05,
      "loss": 2.5821,
      "step": 24450
    },
    {
      "epoch": 12.44150559511699,
      "grad_norm": 18.92050552368164,
      "learning_rate": 3.7558494404883016e-05,
      "loss": 2.6252,
      "step": 24460
    },
    {
      "epoch": 12.446592065106817,
      "grad_norm": 23.506874084472656,
      "learning_rate": 3.7553407934893186e-05,
      "loss": 2.6373,
      "step": 24470
    },
    {
      "epoch": 12.451678535096644,
      "grad_norm": 22.411598205566406,
      "learning_rate": 3.7548321464903356e-05,
      "loss": 2.6864,
      "step": 24480
    },
    {
      "epoch": 12.45676500508647,
      "grad_norm": 20.031402587890625,
      "learning_rate": 3.754323499491353e-05,
      "loss": 2.683,
      "step": 24490
    },
    {
      "epoch": 12.461851475076298,
      "grad_norm": 24.06229591369629,
      "learning_rate": 3.75381485249237e-05,
      "loss": 2.6364,
      "step": 24500
    },
    {
      "epoch": 12.466937945066125,
      "grad_norm": 28.36515998840332,
      "learning_rate": 3.753306205493388e-05,
      "loss": 2.6443,
      "step": 24510
    },
    {
      "epoch": 12.472024415055952,
      "grad_norm": 18.926774978637695,
      "learning_rate": 3.7527975584944056e-05,
      "loss": 2.6128,
      "step": 24520
    },
    {
      "epoch": 12.477110885045779,
      "grad_norm": 19.445114135742188,
      "learning_rate": 3.7522889114954225e-05,
      "loss": 2.659,
      "step": 24530
    },
    {
      "epoch": 12.482197355035606,
      "grad_norm": 20.054950714111328,
      "learning_rate": 3.7517802644964395e-05,
      "loss": 2.6014,
      "step": 24540
    },
    {
      "epoch": 12.487283825025433,
      "grad_norm": 21.225915908813477,
      "learning_rate": 3.751271617497457e-05,
      "loss": 2.678,
      "step": 24550
    },
    {
      "epoch": 12.49237029501526,
      "grad_norm": 23.48853874206543,
      "learning_rate": 3.750762970498474e-05,
      "loss": 2.5976,
      "step": 24560
    },
    {
      "epoch": 12.497456765005087,
      "grad_norm": 25.87223243713379,
      "learning_rate": 3.750254323499491e-05,
      "loss": 2.6329,
      "step": 24570
    },
    {
      "epoch": 12.502543234994913,
      "grad_norm": 22.809518814086914,
      "learning_rate": 3.749745676500509e-05,
      "loss": 2.6126,
      "step": 24580
    },
    {
      "epoch": 12.50762970498474,
      "grad_norm": 21.234874725341797,
      "learning_rate": 3.749237029501526e-05,
      "loss": 2.6351,
      "step": 24590
    },
    {
      "epoch": 12.512716174974567,
      "grad_norm": 23.20268440246582,
      "learning_rate": 3.7487283825025435e-05,
      "loss": 2.6777,
      "step": 24600
    },
    {
      "epoch": 12.517802644964394,
      "grad_norm": 21.66872787475586,
      "learning_rate": 3.748219735503561e-05,
      "loss": 2.7224,
      "step": 24610
    },
    {
      "epoch": 12.522889114954221,
      "grad_norm": 17.947195053100586,
      "learning_rate": 3.747711088504578e-05,
      "loss": 2.5936,
      "step": 24620
    },
    {
      "epoch": 12.527975584944048,
      "grad_norm": 24.310420989990234,
      "learning_rate": 3.747202441505595e-05,
      "loss": 2.6347,
      "step": 24630
    },
    {
      "epoch": 12.533062054933875,
      "grad_norm": 20.42530059814453,
      "learning_rate": 3.746693794506613e-05,
      "loss": 2.6493,
      "step": 24640
    },
    {
      "epoch": 12.538148524923702,
      "grad_norm": 20.94441032409668,
      "learning_rate": 3.74618514750763e-05,
      "loss": 2.7402,
      "step": 24650
    },
    {
      "epoch": 12.54323499491353,
      "grad_norm": 24.11933708190918,
      "learning_rate": 3.745676500508647e-05,
      "loss": 2.6498,
      "step": 24660
    },
    {
      "epoch": 12.548321464903356,
      "grad_norm": 22.09236717224121,
      "learning_rate": 3.7451678535096645e-05,
      "loss": 2.6564,
      "step": 24670
    },
    {
      "epoch": 12.553407934893183,
      "grad_norm": 21.81316375732422,
      "learning_rate": 3.7446592065106814e-05,
      "loss": 2.6647,
      "step": 24680
    },
    {
      "epoch": 12.55849440488301,
      "grad_norm": 17.843427658081055,
      "learning_rate": 3.744150559511699e-05,
      "loss": 2.6802,
      "step": 24690
    },
    {
      "epoch": 12.563580874872839,
      "grad_norm": 23.21587371826172,
      "learning_rate": 3.743641912512717e-05,
      "loss": 2.6149,
      "step": 24700
    },
    {
      "epoch": 12.568667344862666,
      "grad_norm": 25.714303970336914,
      "learning_rate": 3.743133265513734e-05,
      "loss": 2.6037,
      "step": 24710
    },
    {
      "epoch": 12.573753814852493,
      "grad_norm": 21.126096725463867,
      "learning_rate": 3.7426246185147514e-05,
      "loss": 2.6385,
      "step": 24720
    },
    {
      "epoch": 12.57884028484232,
      "grad_norm": 23.971893310546875,
      "learning_rate": 3.7421159715157684e-05,
      "loss": 2.6691,
      "step": 24730
    },
    {
      "epoch": 12.583926754832147,
      "grad_norm": 23.652841567993164,
      "learning_rate": 3.7416073245167854e-05,
      "loss": 2.6843,
      "step": 24740
    },
    {
      "epoch": 12.589013224821974,
      "grad_norm": 19.25437355041504,
      "learning_rate": 3.741098677517803e-05,
      "loss": 2.6798,
      "step": 24750
    },
    {
      "epoch": 12.594099694811801,
      "grad_norm": 21.57781219482422,
      "learning_rate": 3.74059003051882e-05,
      "loss": 2.6944,
      "step": 24760
    },
    {
      "epoch": 12.599186164801628,
      "grad_norm": 23.43077850341797,
      "learning_rate": 3.740081383519837e-05,
      "loss": 2.6445,
      "step": 24770
    },
    {
      "epoch": 12.604272634791455,
      "grad_norm": 21.237403869628906,
      "learning_rate": 3.739572736520855e-05,
      "loss": 2.6342,
      "step": 24780
    },
    {
      "epoch": 12.609359104781282,
      "grad_norm": 19.181337356567383,
      "learning_rate": 3.739064089521872e-05,
      "loss": 2.7683,
      "step": 24790
    },
    {
      "epoch": 12.614445574771109,
      "grad_norm": 20.57066535949707,
      "learning_rate": 3.7385554425228894e-05,
      "loss": 2.5855,
      "step": 24800
    },
    {
      "epoch": 12.619532044760936,
      "grad_norm": 24.924972534179688,
      "learning_rate": 3.738046795523907e-05,
      "loss": 2.6895,
      "step": 24810
    },
    {
      "epoch": 12.624618514750763,
      "grad_norm": 20.67169189453125,
      "learning_rate": 3.737538148524924e-05,
      "loss": 2.7512,
      "step": 24820
    },
    {
      "epoch": 12.62970498474059,
      "grad_norm": 20.58978271484375,
      "learning_rate": 3.737029501525941e-05,
      "loss": 2.5869,
      "step": 24830
    },
    {
      "epoch": 12.634791454730417,
      "grad_norm": 22.06312370300293,
      "learning_rate": 3.736520854526959e-05,
      "loss": 2.6543,
      "step": 24840
    },
    {
      "epoch": 12.639877924720244,
      "grad_norm": 22.81588363647461,
      "learning_rate": 3.736012207527976e-05,
      "loss": 2.6253,
      "step": 24850
    },
    {
      "epoch": 12.64496439471007,
      "grad_norm": 19.669845581054688,
      "learning_rate": 3.735503560528993e-05,
      "loss": 2.6522,
      "step": 24860
    },
    {
      "epoch": 12.650050864699898,
      "grad_norm": 28.33985137939453,
      "learning_rate": 3.7349949135300103e-05,
      "loss": 2.6599,
      "step": 24870
    },
    {
      "epoch": 12.655137334689725,
      "grad_norm": 18.95823097229004,
      "learning_rate": 3.734486266531027e-05,
      "loss": 2.6693,
      "step": 24880
    },
    {
      "epoch": 12.660223804679552,
      "grad_norm": 27.874004364013672,
      "learning_rate": 3.733977619532045e-05,
      "loss": 2.7514,
      "step": 24890
    },
    {
      "epoch": 12.665310274669379,
      "grad_norm": 25.99468421936035,
      "learning_rate": 3.733468972533063e-05,
      "loss": 2.6171,
      "step": 24900
    },
    {
      "epoch": 12.670396744659207,
      "grad_norm": 18.89939308166504,
      "learning_rate": 3.7329603255340797e-05,
      "loss": 2.6823,
      "step": 24910
    },
    {
      "epoch": 12.675483214649034,
      "grad_norm": 22.55258560180664,
      "learning_rate": 3.7324516785350966e-05,
      "loss": 2.689,
      "step": 24920
    },
    {
      "epoch": 12.680569684638861,
      "grad_norm": 26.52766990661621,
      "learning_rate": 3.731943031536114e-05,
      "loss": 2.6582,
      "step": 24930
    },
    {
      "epoch": 12.685656154628688,
      "grad_norm": 17.65053367614746,
      "learning_rate": 3.731434384537131e-05,
      "loss": 2.6948,
      "step": 24940
    },
    {
      "epoch": 12.690742624618515,
      "grad_norm": 23.282258987426758,
      "learning_rate": 3.730925737538148e-05,
      "loss": 2.7284,
      "step": 24950
    },
    {
      "epoch": 12.695829094608342,
      "grad_norm": 22.486879348754883,
      "learning_rate": 3.730417090539166e-05,
      "loss": 2.5976,
      "step": 24960
    },
    {
      "epoch": 12.70091556459817,
      "grad_norm": 21.598499298095703,
      "learning_rate": 3.729908443540183e-05,
      "loss": 2.7351,
      "step": 24970
    },
    {
      "epoch": 12.706002034587996,
      "grad_norm": 22.200281143188477,
      "learning_rate": 3.7293997965412006e-05,
      "loss": 2.7368,
      "step": 24980
    },
    {
      "epoch": 12.711088504577823,
      "grad_norm": 21.99577522277832,
      "learning_rate": 3.728891149542218e-05,
      "loss": 2.5789,
      "step": 24990
    },
    {
      "epoch": 12.71617497456765,
      "grad_norm": 24.53858184814453,
      "learning_rate": 3.728382502543235e-05,
      "loss": 2.6917,
      "step": 25000
    },
    {
      "epoch": 12.721261444557477,
      "grad_norm": 20.829282760620117,
      "learning_rate": 3.727873855544253e-05,
      "loss": 2.6782,
      "step": 25010
    },
    {
      "epoch": 12.726347914547304,
      "grad_norm": 23.8604793548584,
      "learning_rate": 3.72736520854527e-05,
      "loss": 2.6167,
      "step": 25020
    },
    {
      "epoch": 12.731434384537131,
      "grad_norm": 27.544677734375,
      "learning_rate": 3.726856561546287e-05,
      "loss": 2.7297,
      "step": 25030
    },
    {
      "epoch": 12.736520854526958,
      "grad_norm": 19.844562530517578,
      "learning_rate": 3.7263479145473046e-05,
      "loss": 2.6554,
      "step": 25040
    },
    {
      "epoch": 12.741607324516785,
      "grad_norm": 24.370452880859375,
      "learning_rate": 3.7258392675483216e-05,
      "loss": 2.6613,
      "step": 25050
    },
    {
      "epoch": 12.746693794506612,
      "grad_norm": 21.31599235534668,
      "learning_rate": 3.7253306205493386e-05,
      "loss": 2.6974,
      "step": 25060
    },
    {
      "epoch": 12.751780264496439,
      "grad_norm": 22.880102157592773,
      "learning_rate": 3.724821973550356e-05,
      "loss": 2.747,
      "step": 25070
    },
    {
      "epoch": 12.756866734486266,
      "grad_norm": 31.16554069519043,
      "learning_rate": 3.724313326551373e-05,
      "loss": 2.6776,
      "step": 25080
    },
    {
      "epoch": 12.761953204476093,
      "grad_norm": 28.132469177246094,
      "learning_rate": 3.723804679552391e-05,
      "loss": 2.5811,
      "step": 25090
    },
    {
      "epoch": 12.76703967446592,
      "grad_norm": 22.301923751831055,
      "learning_rate": 3.7232960325534086e-05,
      "loss": 2.6336,
      "step": 25100
    },
    {
      "epoch": 12.772126144455747,
      "grad_norm": 21.70692253112793,
      "learning_rate": 3.7227873855544255e-05,
      "loss": 2.6613,
      "step": 25110
    },
    {
      "epoch": 12.777212614445574,
      "grad_norm": 20.610441207885742,
      "learning_rate": 3.7222787385554425e-05,
      "loss": 2.5987,
      "step": 25120
    },
    {
      "epoch": 12.782299084435401,
      "grad_norm": 21.1705265045166,
      "learning_rate": 3.72177009155646e-05,
      "loss": 2.6515,
      "step": 25130
    },
    {
      "epoch": 12.787385554425228,
      "grad_norm": 20.157901763916016,
      "learning_rate": 3.721261444557477e-05,
      "loss": 2.591,
      "step": 25140
    },
    {
      "epoch": 12.792472024415057,
      "grad_norm": 24.142879486083984,
      "learning_rate": 3.720752797558494e-05,
      "loss": 2.7299,
      "step": 25150
    },
    {
      "epoch": 12.797558494404884,
      "grad_norm": 18.37208366394043,
      "learning_rate": 3.720244150559512e-05,
      "loss": 2.7312,
      "step": 25160
    },
    {
      "epoch": 12.80264496439471,
      "grad_norm": 22.99932289123535,
      "learning_rate": 3.719735503560529e-05,
      "loss": 2.58,
      "step": 25170
    },
    {
      "epoch": 12.807731434384538,
      "grad_norm": 25.201074600219727,
      "learning_rate": 3.7192268565615465e-05,
      "loss": 2.5283,
      "step": 25180
    },
    {
      "epoch": 12.812817904374365,
      "grad_norm": 22.763626098632812,
      "learning_rate": 3.718718209562564e-05,
      "loss": 2.6805,
      "step": 25190
    },
    {
      "epoch": 12.817904374364192,
      "grad_norm": 20.80044937133789,
      "learning_rate": 3.718209562563581e-05,
      "loss": 2.6068,
      "step": 25200
    },
    {
      "epoch": 12.822990844354019,
      "grad_norm": 22.18815803527832,
      "learning_rate": 3.717700915564598e-05,
      "loss": 2.6633,
      "step": 25210
    },
    {
      "epoch": 12.828077314343846,
      "grad_norm": 24.550472259521484,
      "learning_rate": 3.717192268565616e-05,
      "loss": 2.6291,
      "step": 25220
    },
    {
      "epoch": 12.833163784333673,
      "grad_norm": 23.55223274230957,
      "learning_rate": 3.716683621566633e-05,
      "loss": 2.6466,
      "step": 25230
    },
    {
      "epoch": 12.8382502543235,
      "grad_norm": 22.447574615478516,
      "learning_rate": 3.71617497456765e-05,
      "loss": 2.6176,
      "step": 25240
    },
    {
      "epoch": 12.843336724313327,
      "grad_norm": 23.457305908203125,
      "learning_rate": 3.7156663275686675e-05,
      "loss": 2.712,
      "step": 25250
    },
    {
      "epoch": 12.848423194303153,
      "grad_norm": 18.966291427612305,
      "learning_rate": 3.7151576805696844e-05,
      "loss": 2.5782,
      "step": 25260
    },
    {
      "epoch": 12.85350966429298,
      "grad_norm": 20.153675079345703,
      "learning_rate": 3.714649033570702e-05,
      "loss": 2.6955,
      "step": 25270
    },
    {
      "epoch": 12.858596134282807,
      "grad_norm": 28.175172805786133,
      "learning_rate": 3.71414038657172e-05,
      "loss": 2.7313,
      "step": 25280
    },
    {
      "epoch": 12.863682604272634,
      "grad_norm": 18.49806785583496,
      "learning_rate": 3.713631739572737e-05,
      "loss": 2.6518,
      "step": 25290
    },
    {
      "epoch": 12.868769074262461,
      "grad_norm": 25.882240295410156,
      "learning_rate": 3.7131230925737544e-05,
      "loss": 2.6223,
      "step": 25300
    },
    {
      "epoch": 12.873855544252288,
      "grad_norm": 21.466245651245117,
      "learning_rate": 3.7126144455747714e-05,
      "loss": 2.657,
      "step": 25310
    },
    {
      "epoch": 12.878942014242115,
      "grad_norm": 22.680082321166992,
      "learning_rate": 3.7121057985757884e-05,
      "loss": 2.6471,
      "step": 25320
    },
    {
      "epoch": 12.884028484231942,
      "grad_norm": 26.989643096923828,
      "learning_rate": 3.711597151576806e-05,
      "loss": 2.6722,
      "step": 25330
    },
    {
      "epoch": 12.88911495422177,
      "grad_norm": 26.384349822998047,
      "learning_rate": 3.711088504577823e-05,
      "loss": 2.7022,
      "step": 25340
    },
    {
      "epoch": 12.894201424211598,
      "grad_norm": 24.425857543945312,
      "learning_rate": 3.71057985757884e-05,
      "loss": 2.6411,
      "step": 25350
    },
    {
      "epoch": 12.899287894201425,
      "grad_norm": 19.937095642089844,
      "learning_rate": 3.710071210579858e-05,
      "loss": 2.6515,
      "step": 25360
    },
    {
      "epoch": 12.904374364191252,
      "grad_norm": 19.059383392333984,
      "learning_rate": 3.7095625635808754e-05,
      "loss": 2.6536,
      "step": 25370
    },
    {
      "epoch": 12.909460834181079,
      "grad_norm": 22.17498016357422,
      "learning_rate": 3.7090539165818924e-05,
      "loss": 2.6157,
      "step": 25380
    },
    {
      "epoch": 12.914547304170906,
      "grad_norm": 29.505123138427734,
      "learning_rate": 3.70854526958291e-05,
      "loss": 2.6891,
      "step": 25390
    },
    {
      "epoch": 12.919633774160733,
      "grad_norm": 22.275041580200195,
      "learning_rate": 3.708036622583927e-05,
      "loss": 2.5663,
      "step": 25400
    },
    {
      "epoch": 12.92472024415056,
      "grad_norm": 21.534814834594727,
      "learning_rate": 3.707527975584944e-05,
      "loss": 2.6853,
      "step": 25410
    },
    {
      "epoch": 12.929806714140387,
      "grad_norm": 23.416303634643555,
      "learning_rate": 3.707019328585962e-05,
      "loss": 2.6067,
      "step": 25420
    },
    {
      "epoch": 12.934893184130214,
      "grad_norm": 20.509572982788086,
      "learning_rate": 3.706510681586979e-05,
      "loss": 2.6883,
      "step": 25430
    },
    {
      "epoch": 12.939979654120041,
      "grad_norm": 26.657068252563477,
      "learning_rate": 3.706002034587996e-05,
      "loss": 2.6847,
      "step": 25440
    },
    {
      "epoch": 12.945066124109868,
      "grad_norm": 20.854970932006836,
      "learning_rate": 3.7054933875890133e-05,
      "loss": 2.5869,
      "step": 25450
    },
    {
      "epoch": 12.950152594099695,
      "grad_norm": 19.749906539916992,
      "learning_rate": 3.70498474059003e-05,
      "loss": 2.6267,
      "step": 25460
    },
    {
      "epoch": 12.955239064089522,
      "grad_norm": 22.988479614257812,
      "learning_rate": 3.704476093591048e-05,
      "loss": 2.6466,
      "step": 25470
    },
    {
      "epoch": 12.960325534079349,
      "grad_norm": 26.169923782348633,
      "learning_rate": 3.703967446592066e-05,
      "loss": 2.643,
      "step": 25480
    },
    {
      "epoch": 12.965412004069176,
      "grad_norm": 26.803163528442383,
      "learning_rate": 3.7034587995930827e-05,
      "loss": 2.6381,
      "step": 25490
    },
    {
      "epoch": 12.970498474059003,
      "grad_norm": 27.490921020507812,
      "learning_rate": 3.7029501525940996e-05,
      "loss": 2.6527,
      "step": 25500
    },
    {
      "epoch": 12.97558494404883,
      "grad_norm": 26.54038429260254,
      "learning_rate": 3.702441505595117e-05,
      "loss": 2.7059,
      "step": 25510
    },
    {
      "epoch": 12.980671414038657,
      "grad_norm": 21.748716354370117,
      "learning_rate": 3.701932858596134e-05,
      "loss": 2.7008,
      "step": 25520
    },
    {
      "epoch": 12.985757884028484,
      "grad_norm": 26.420860290527344,
      "learning_rate": 3.701424211597152e-05,
      "loss": 2.6232,
      "step": 25530
    },
    {
      "epoch": 12.99084435401831,
      "grad_norm": 18.97555160522461,
      "learning_rate": 3.700915564598169e-05,
      "loss": 2.5388,
      "step": 25540
    },
    {
      "epoch": 12.995930824008138,
      "grad_norm": 21.048908233642578,
      "learning_rate": 3.700406917599186e-05,
      "loss": 2.6254,
      "step": 25550
    },
    {
      "epoch": 13.0,
      "eval_loss": 3.907058000564575,
      "eval_runtime": 2.7934,
      "eval_samples_per_second": 993.42,
      "eval_steps_per_second": 124.222,
      "step": 25558
    },
    {
      "epoch": 13.001017293997965,
      "grad_norm": 22.064136505126953,
      "learning_rate": 3.6998982706002036e-05,
      "loss": 2.5843,
      "step": 25560
    },
    {
      "epoch": 13.006103763987792,
      "grad_norm": 20.373769760131836,
      "learning_rate": 3.699389623601221e-05,
      "loss": 2.5813,
      "step": 25570
    },
    {
      "epoch": 13.011190233977619,
      "grad_norm": 25.27885627746582,
      "learning_rate": 3.698880976602238e-05,
      "loss": 2.5958,
      "step": 25580
    },
    {
      "epoch": 13.016276703967447,
      "grad_norm": 26.86949920654297,
      "learning_rate": 3.698372329603256e-05,
      "loss": 2.6822,
      "step": 25590
    },
    {
      "epoch": 13.021363173957274,
      "grad_norm": 25.2540225982666,
      "learning_rate": 3.697863682604273e-05,
      "loss": 2.5865,
      "step": 25600
    },
    {
      "epoch": 13.026449643947101,
      "grad_norm": 19.449764251708984,
      "learning_rate": 3.69735503560529e-05,
      "loss": 2.5968,
      "step": 25610
    },
    {
      "epoch": 13.031536113936928,
      "grad_norm": 34.91796112060547,
      "learning_rate": 3.6968463886063076e-05,
      "loss": 2.6192,
      "step": 25620
    },
    {
      "epoch": 13.036622583926755,
      "grad_norm": 20.014921188354492,
      "learning_rate": 3.6963377416073246e-05,
      "loss": 2.5921,
      "step": 25630
    },
    {
      "epoch": 13.041709053916582,
      "grad_norm": 25.591821670532227,
      "learning_rate": 3.6958290946083416e-05,
      "loss": 2.5944,
      "step": 25640
    },
    {
      "epoch": 13.04679552390641,
      "grad_norm": 30.4376277923584,
      "learning_rate": 3.695320447609359e-05,
      "loss": 2.6309,
      "step": 25650
    },
    {
      "epoch": 13.051881993896236,
      "grad_norm": 22.628299713134766,
      "learning_rate": 3.694811800610377e-05,
      "loss": 2.5605,
      "step": 25660
    },
    {
      "epoch": 13.056968463886063,
      "grad_norm": 23.349363327026367,
      "learning_rate": 3.694303153611394e-05,
      "loss": 2.5528,
      "step": 25670
    },
    {
      "epoch": 13.06205493387589,
      "grad_norm": 28.75684928894043,
      "learning_rate": 3.6937945066124116e-05,
      "loss": 2.6186,
      "step": 25680
    },
    {
      "epoch": 13.067141403865717,
      "grad_norm": 25.3458194732666,
      "learning_rate": 3.6932858596134285e-05,
      "loss": 2.5429,
      "step": 25690
    },
    {
      "epoch": 13.072227873855544,
      "grad_norm": 21.42618179321289,
      "learning_rate": 3.6927772126144455e-05,
      "loss": 2.6592,
      "step": 25700
    },
    {
      "epoch": 13.077314343845371,
      "grad_norm": 21.313079833984375,
      "learning_rate": 3.692268565615463e-05,
      "loss": 2.6048,
      "step": 25710
    },
    {
      "epoch": 13.082400813835198,
      "grad_norm": 23.926769256591797,
      "learning_rate": 3.69175991861648e-05,
      "loss": 2.6293,
      "step": 25720
    },
    {
      "epoch": 13.087487283825025,
      "grad_norm": 21.372238159179688,
      "learning_rate": 3.691251271617497e-05,
      "loss": 2.5948,
      "step": 25730
    },
    {
      "epoch": 13.092573753814852,
      "grad_norm": 28.90158462524414,
      "learning_rate": 3.690742624618515e-05,
      "loss": 2.7208,
      "step": 25740
    },
    {
      "epoch": 13.097660223804679,
      "grad_norm": 20.24724006652832,
      "learning_rate": 3.690233977619532e-05,
      "loss": 2.5773,
      "step": 25750
    },
    {
      "epoch": 13.102746693794506,
      "grad_norm": 23.627534866333008,
      "learning_rate": 3.6897253306205495e-05,
      "loss": 2.6517,
      "step": 25760
    },
    {
      "epoch": 13.107833163784333,
      "grad_norm": 20.310455322265625,
      "learning_rate": 3.689216683621567e-05,
      "loss": 2.604,
      "step": 25770
    },
    {
      "epoch": 13.11291963377416,
      "grad_norm": 26.187301635742188,
      "learning_rate": 3.688708036622584e-05,
      "loss": 2.6763,
      "step": 25780
    },
    {
      "epoch": 13.118006103763987,
      "grad_norm": 23.127992630004883,
      "learning_rate": 3.688199389623601e-05,
      "loss": 2.554,
      "step": 25790
    },
    {
      "epoch": 13.123092573753814,
      "grad_norm": 19.29962158203125,
      "learning_rate": 3.687690742624619e-05,
      "loss": 2.631,
      "step": 25800
    },
    {
      "epoch": 13.128179043743643,
      "grad_norm": 23.144943237304688,
      "learning_rate": 3.687182095625636e-05,
      "loss": 2.6579,
      "step": 25810
    },
    {
      "epoch": 13.13326551373347,
      "grad_norm": 25.57448387145996,
      "learning_rate": 3.6866734486266535e-05,
      "loss": 2.6333,
      "step": 25820
    },
    {
      "epoch": 13.138351983723297,
      "grad_norm": 22.928030014038086,
      "learning_rate": 3.6861648016276705e-05,
      "loss": 2.6104,
      "step": 25830
    },
    {
      "epoch": 13.143438453713124,
      "grad_norm": 25.050880432128906,
      "learning_rate": 3.6856561546286874e-05,
      "loss": 2.5401,
      "step": 25840
    },
    {
      "epoch": 13.14852492370295,
      "grad_norm": 21.83374786376953,
      "learning_rate": 3.685147507629705e-05,
      "loss": 2.6394,
      "step": 25850
    },
    {
      "epoch": 13.153611393692778,
      "grad_norm": 21.636831283569336,
      "learning_rate": 3.684638860630723e-05,
      "loss": 2.6692,
      "step": 25860
    },
    {
      "epoch": 13.158697863682605,
      "grad_norm": 22.00147247314453,
      "learning_rate": 3.68413021363174e-05,
      "loss": 2.617,
      "step": 25870
    },
    {
      "epoch": 13.163784333672432,
      "grad_norm": 23.989299774169922,
      "learning_rate": 3.6836215666327574e-05,
      "loss": 2.5729,
      "step": 25880
    },
    {
      "epoch": 13.168870803662259,
      "grad_norm": 19.287565231323242,
      "learning_rate": 3.6831129196337744e-05,
      "loss": 2.6212,
      "step": 25890
    },
    {
      "epoch": 13.173957273652086,
      "grad_norm": 28.198503494262695,
      "learning_rate": 3.6826042726347914e-05,
      "loss": 2.673,
      "step": 25900
    },
    {
      "epoch": 13.179043743641913,
      "grad_norm": 27.29502296447754,
      "learning_rate": 3.682095625635809e-05,
      "loss": 2.6044,
      "step": 25910
    },
    {
      "epoch": 13.18413021363174,
      "grad_norm": 16.768999099731445,
      "learning_rate": 3.681586978636826e-05,
      "loss": 2.6931,
      "step": 25920
    },
    {
      "epoch": 13.189216683621567,
      "grad_norm": 23.61721420288086,
      "learning_rate": 3.681078331637843e-05,
      "loss": 2.6087,
      "step": 25930
    },
    {
      "epoch": 13.194303153611393,
      "grad_norm": 23.196989059448242,
      "learning_rate": 3.680569684638861e-05,
      "loss": 2.6462,
      "step": 25940
    },
    {
      "epoch": 13.19938962360122,
      "grad_norm": 23.65170669555664,
      "learning_rate": 3.6800610376398784e-05,
      "loss": 2.6351,
      "step": 25950
    },
    {
      "epoch": 13.204476093591047,
      "grad_norm": 20.449548721313477,
      "learning_rate": 3.6795523906408954e-05,
      "loss": 2.5874,
      "step": 25960
    },
    {
      "epoch": 13.209562563580874,
      "grad_norm": 18.707420349121094,
      "learning_rate": 3.679043743641913e-05,
      "loss": 2.5954,
      "step": 25970
    },
    {
      "epoch": 13.214649033570701,
      "grad_norm": 20.432140350341797,
      "learning_rate": 3.67853509664293e-05,
      "loss": 2.5991,
      "step": 25980
    },
    {
      "epoch": 13.219735503560528,
      "grad_norm": 24.322324752807617,
      "learning_rate": 3.678026449643947e-05,
      "loss": 2.6369,
      "step": 25990
    },
    {
      "epoch": 13.224821973550355,
      "grad_norm": 24.700586318969727,
      "learning_rate": 3.677517802644965e-05,
      "loss": 2.5251,
      "step": 26000
    },
    {
      "epoch": 13.229908443540182,
      "grad_norm": 24.353574752807617,
      "learning_rate": 3.677009155645982e-05,
      "loss": 2.6017,
      "step": 26010
    },
    {
      "epoch": 13.23499491353001,
      "grad_norm": 19.794065475463867,
      "learning_rate": 3.676500508646999e-05,
      "loss": 2.5898,
      "step": 26020
    },
    {
      "epoch": 13.240081383519838,
      "grad_norm": 26.691761016845703,
      "learning_rate": 3.6759918616480163e-05,
      "loss": 2.5575,
      "step": 26030
    },
    {
      "epoch": 13.245167853509665,
      "grad_norm": 23.383060455322266,
      "learning_rate": 3.675483214649033e-05,
      "loss": 2.5311,
      "step": 26040
    },
    {
      "epoch": 13.250254323499492,
      "grad_norm": 23.012496948242188,
      "learning_rate": 3.674974567650051e-05,
      "loss": 2.6199,
      "step": 26050
    },
    {
      "epoch": 13.255340793489319,
      "grad_norm": 25.542760848999023,
      "learning_rate": 3.674465920651069e-05,
      "loss": 2.5765,
      "step": 26060
    },
    {
      "epoch": 13.260427263479146,
      "grad_norm": 25.613088607788086,
      "learning_rate": 3.6739572736520857e-05,
      "loss": 2.6063,
      "step": 26070
    },
    {
      "epoch": 13.265513733468973,
      "grad_norm": 20.242536544799805,
      "learning_rate": 3.673448626653103e-05,
      "loss": 2.5708,
      "step": 26080
    },
    {
      "epoch": 13.2706002034588,
      "grad_norm": 24.606990814208984,
      "learning_rate": 3.67293997965412e-05,
      "loss": 2.5917,
      "step": 26090
    },
    {
      "epoch": 13.275686673448627,
      "grad_norm": 21.046586990356445,
      "learning_rate": 3.672431332655137e-05,
      "loss": 2.6012,
      "step": 26100
    },
    {
      "epoch": 13.280773143438454,
      "grad_norm": 24.321327209472656,
      "learning_rate": 3.671922685656155e-05,
      "loss": 2.6044,
      "step": 26110
    },
    {
      "epoch": 13.285859613428281,
      "grad_norm": 22.746185302734375,
      "learning_rate": 3.671414038657172e-05,
      "loss": 2.5254,
      "step": 26120
    },
    {
      "epoch": 13.290946083418108,
      "grad_norm": 25.327484130859375,
      "learning_rate": 3.670905391658189e-05,
      "loss": 2.5694,
      "step": 26130
    },
    {
      "epoch": 13.296032553407935,
      "grad_norm": 22.689809799194336,
      "learning_rate": 3.6703967446592066e-05,
      "loss": 2.6237,
      "step": 26140
    },
    {
      "epoch": 13.301119023397762,
      "grad_norm": 28.951805114746094,
      "learning_rate": 3.669888097660224e-05,
      "loss": 2.5656,
      "step": 26150
    },
    {
      "epoch": 13.306205493387589,
      "grad_norm": 23.03537368774414,
      "learning_rate": 3.669379450661241e-05,
      "loss": 2.6501,
      "step": 26160
    },
    {
      "epoch": 13.311291963377416,
      "grad_norm": 23.498558044433594,
      "learning_rate": 3.668870803662259e-05,
      "loss": 2.6539,
      "step": 26170
    },
    {
      "epoch": 13.316378433367243,
      "grad_norm": 24.992084503173828,
      "learning_rate": 3.668362156663276e-05,
      "loss": 2.5796,
      "step": 26180
    },
    {
      "epoch": 13.32146490335707,
      "grad_norm": 21.656827926635742,
      "learning_rate": 3.667853509664293e-05,
      "loss": 2.611,
      "step": 26190
    },
    {
      "epoch": 13.326551373346897,
      "grad_norm": 24.153078079223633,
      "learning_rate": 3.6673448626653106e-05,
      "loss": 2.6455,
      "step": 26200
    },
    {
      "epoch": 13.331637843336724,
      "grad_norm": 24.31046485900879,
      "learning_rate": 3.6668362156663276e-05,
      "loss": 2.5677,
      "step": 26210
    },
    {
      "epoch": 13.33672431332655,
      "grad_norm": 26.329803466796875,
      "learning_rate": 3.6663275686673446e-05,
      "loss": 2.5867,
      "step": 26220
    },
    {
      "epoch": 13.341810783316378,
      "grad_norm": 22.12615203857422,
      "learning_rate": 3.665818921668362e-05,
      "loss": 2.5576,
      "step": 26230
    },
    {
      "epoch": 13.346897253306205,
      "grad_norm": 26.441978454589844,
      "learning_rate": 3.66531027466938e-05,
      "loss": 2.6273,
      "step": 26240
    },
    {
      "epoch": 13.351983723296033,
      "grad_norm": 28.908397674560547,
      "learning_rate": 3.664801627670397e-05,
      "loss": 2.6179,
      "step": 26250
    },
    {
      "epoch": 13.35707019328586,
      "grad_norm": 21.238527297973633,
      "learning_rate": 3.6642929806714146e-05,
      "loss": 2.516,
      "step": 26260
    },
    {
      "epoch": 13.362156663275687,
      "grad_norm": 21.869691848754883,
      "learning_rate": 3.6637843336724315e-05,
      "loss": 2.5656,
      "step": 26270
    },
    {
      "epoch": 13.367243133265514,
      "grad_norm": 23.98820686340332,
      "learning_rate": 3.6632756866734485e-05,
      "loss": 2.7057,
      "step": 26280
    },
    {
      "epoch": 13.372329603255341,
      "grad_norm": 21.344167709350586,
      "learning_rate": 3.662767039674466e-05,
      "loss": 2.5358,
      "step": 26290
    },
    {
      "epoch": 13.377416073245168,
      "grad_norm": 16.837575912475586,
      "learning_rate": 3.662258392675483e-05,
      "loss": 2.5551,
      "step": 26300
    },
    {
      "epoch": 13.382502543234995,
      "grad_norm": 28.475513458251953,
      "learning_rate": 3.6617497456765e-05,
      "loss": 2.5848,
      "step": 26310
    },
    {
      "epoch": 13.387589013224822,
      "grad_norm": 28.159542083740234,
      "learning_rate": 3.661241098677518e-05,
      "loss": 2.4862,
      "step": 26320
    },
    {
      "epoch": 13.39267548321465,
      "grad_norm": 19.358606338500977,
      "learning_rate": 3.6607324516785355e-05,
      "loss": 2.6403,
      "step": 26330
    },
    {
      "epoch": 13.397761953204476,
      "grad_norm": 21.93395233154297,
      "learning_rate": 3.6602238046795525e-05,
      "loss": 2.6101,
      "step": 26340
    },
    {
      "epoch": 13.402848423194303,
      "grad_norm": 18.71138572692871,
      "learning_rate": 3.65971515768057e-05,
      "loss": 2.6128,
      "step": 26350
    },
    {
      "epoch": 13.40793489318413,
      "grad_norm": 29.981664657592773,
      "learning_rate": 3.659206510681587e-05,
      "loss": 2.611,
      "step": 26360
    },
    {
      "epoch": 13.413021363173957,
      "grad_norm": 22.071285247802734,
      "learning_rate": 3.658697863682605e-05,
      "loss": 2.5766,
      "step": 26370
    },
    {
      "epoch": 13.418107833163784,
      "grad_norm": 26.060420989990234,
      "learning_rate": 3.658189216683622e-05,
      "loss": 2.5634,
      "step": 26380
    },
    {
      "epoch": 13.423194303153611,
      "grad_norm": 19.506860733032227,
      "learning_rate": 3.657680569684639e-05,
      "loss": 2.6238,
      "step": 26390
    },
    {
      "epoch": 13.428280773143438,
      "grad_norm": 26.830459594726562,
      "learning_rate": 3.6571719226856565e-05,
      "loss": 2.6266,
      "step": 26400
    },
    {
      "epoch": 13.433367243133265,
      "grad_norm": 25.984798431396484,
      "learning_rate": 3.6566632756866735e-05,
      "loss": 2.6285,
      "step": 26410
    },
    {
      "epoch": 13.438453713123092,
      "grad_norm": 23.99202537536621,
      "learning_rate": 3.6561546286876904e-05,
      "loss": 2.5804,
      "step": 26420
    },
    {
      "epoch": 13.443540183112919,
      "grad_norm": 29.93266487121582,
      "learning_rate": 3.655645981688708e-05,
      "loss": 2.6532,
      "step": 26430
    },
    {
      "epoch": 13.448626653102746,
      "grad_norm": 29.159942626953125,
      "learning_rate": 3.655137334689726e-05,
      "loss": 2.6656,
      "step": 26440
    },
    {
      "epoch": 13.453713123092573,
      "grad_norm": 28.23113250732422,
      "learning_rate": 3.654628687690743e-05,
      "loss": 2.5689,
      "step": 26450
    },
    {
      "epoch": 13.4587995930824,
      "grad_norm": 21.669830322265625,
      "learning_rate": 3.6541200406917604e-05,
      "loss": 2.594,
      "step": 26460
    },
    {
      "epoch": 13.463886063072227,
      "grad_norm": 19.242361068725586,
      "learning_rate": 3.6536113936927774e-05,
      "loss": 2.5598,
      "step": 26470
    },
    {
      "epoch": 13.468972533062056,
      "grad_norm": 21.808565139770508,
      "learning_rate": 3.6531027466937944e-05,
      "loss": 2.6104,
      "step": 26480
    },
    {
      "epoch": 13.474059003051883,
      "grad_norm": 23.268491744995117,
      "learning_rate": 3.652594099694812e-05,
      "loss": 2.7054,
      "step": 26490
    },
    {
      "epoch": 13.47914547304171,
      "grad_norm": 24.73192596435547,
      "learning_rate": 3.652085452695829e-05,
      "loss": 2.6173,
      "step": 26500
    },
    {
      "epoch": 13.484231943031537,
      "grad_norm": 21.63174819946289,
      "learning_rate": 3.651576805696846e-05,
      "loss": 2.5928,
      "step": 26510
    },
    {
      "epoch": 13.489318413021364,
      "grad_norm": 24.554481506347656,
      "learning_rate": 3.651068158697864e-05,
      "loss": 2.5647,
      "step": 26520
    },
    {
      "epoch": 13.49440488301119,
      "grad_norm": 30.621871948242188,
      "learning_rate": 3.6505595116988814e-05,
      "loss": 2.513,
      "step": 26530
    },
    {
      "epoch": 13.499491353001018,
      "grad_norm": 22.15743064880371,
      "learning_rate": 3.6500508646998984e-05,
      "loss": 2.6145,
      "step": 26540
    },
    {
      "epoch": 13.504577822990845,
      "grad_norm": 29.79359245300293,
      "learning_rate": 3.649542217700916e-05,
      "loss": 2.519,
      "step": 26550
    },
    {
      "epoch": 13.509664292980672,
      "grad_norm": 23.8304386138916,
      "learning_rate": 3.649033570701933e-05,
      "loss": 2.6222,
      "step": 26560
    },
    {
      "epoch": 13.514750762970499,
      "grad_norm": 21.09861946105957,
      "learning_rate": 3.64852492370295e-05,
      "loss": 2.5846,
      "step": 26570
    },
    {
      "epoch": 13.519837232960326,
      "grad_norm": 27.828697204589844,
      "learning_rate": 3.648016276703968e-05,
      "loss": 2.4558,
      "step": 26580
    },
    {
      "epoch": 13.524923702950153,
      "grad_norm": 23.382753372192383,
      "learning_rate": 3.647507629704985e-05,
      "loss": 2.5622,
      "step": 26590
    },
    {
      "epoch": 13.53001017293998,
      "grad_norm": 31.840574264526367,
      "learning_rate": 3.646998982706002e-05,
      "loss": 2.5174,
      "step": 26600
    },
    {
      "epoch": 13.535096642929807,
      "grad_norm": 26.356386184692383,
      "learning_rate": 3.6464903357070193e-05,
      "loss": 2.556,
      "step": 26610
    },
    {
      "epoch": 13.540183112919634,
      "grad_norm": 26.629438400268555,
      "learning_rate": 3.645981688708037e-05,
      "loss": 2.6898,
      "step": 26620
    },
    {
      "epoch": 13.54526958290946,
      "grad_norm": 26.500534057617188,
      "learning_rate": 3.645473041709055e-05,
      "loss": 2.5898,
      "step": 26630
    },
    {
      "epoch": 13.550356052899287,
      "grad_norm": 25.452009201049805,
      "learning_rate": 3.644964394710072e-05,
      "loss": 2.6162,
      "step": 26640
    },
    {
      "epoch": 13.555442522889114,
      "grad_norm": 24.05486297607422,
      "learning_rate": 3.6444557477110887e-05,
      "loss": 2.6035,
      "step": 26650
    },
    {
      "epoch": 13.560528992878941,
      "grad_norm": 21.55286979675293,
      "learning_rate": 3.643947100712106e-05,
      "loss": 2.5705,
      "step": 26660
    },
    {
      "epoch": 13.565615462868768,
      "grad_norm": 29.517589569091797,
      "learning_rate": 3.643438453713123e-05,
      "loss": 2.5773,
      "step": 26670
    },
    {
      "epoch": 13.570701932858595,
      "grad_norm": 30.016141891479492,
      "learning_rate": 3.64292980671414e-05,
      "loss": 2.5243,
      "step": 26680
    },
    {
      "epoch": 13.575788402848422,
      "grad_norm": 27.734861373901367,
      "learning_rate": 3.642421159715158e-05,
      "loss": 2.6681,
      "step": 26690
    },
    {
      "epoch": 13.580874872838251,
      "grad_norm": 20.793060302734375,
      "learning_rate": 3.641912512716175e-05,
      "loss": 2.6108,
      "step": 26700
    },
    {
      "epoch": 13.585961342828078,
      "grad_norm": 31.03541374206543,
      "learning_rate": 3.641403865717192e-05,
      "loss": 2.5377,
      "step": 26710
    },
    {
      "epoch": 13.591047812817905,
      "grad_norm": 23.065269470214844,
      "learning_rate": 3.6408952187182096e-05,
      "loss": 2.5317,
      "step": 26720
    },
    {
      "epoch": 13.596134282807732,
      "grad_norm": 27.813556671142578,
      "learning_rate": 3.640386571719227e-05,
      "loss": 2.6199,
      "step": 26730
    },
    {
      "epoch": 13.601220752797559,
      "grad_norm": 28.21671485900879,
      "learning_rate": 3.639877924720244e-05,
      "loss": 2.5226,
      "step": 26740
    },
    {
      "epoch": 13.606307222787386,
      "grad_norm": 25.3620662689209,
      "learning_rate": 3.639369277721262e-05,
      "loss": 2.5531,
      "step": 26750
    },
    {
      "epoch": 13.611393692777213,
      "grad_norm": 27.801469802856445,
      "learning_rate": 3.638860630722279e-05,
      "loss": 2.4945,
      "step": 26760
    },
    {
      "epoch": 13.61648016276704,
      "grad_norm": 20.666885375976562,
      "learning_rate": 3.638351983723296e-05,
      "loss": 2.4942,
      "step": 26770
    },
    {
      "epoch": 13.621566632756867,
      "grad_norm": 23.351789474487305,
      "learning_rate": 3.6378433367243136e-05,
      "loss": 2.5496,
      "step": 26780
    },
    {
      "epoch": 13.626653102746694,
      "grad_norm": 26.4359188079834,
      "learning_rate": 3.6373346897253306e-05,
      "loss": 2.6524,
      "step": 26790
    },
    {
      "epoch": 13.631739572736521,
      "grad_norm": 24.791418075561523,
      "learning_rate": 3.6368260427263476e-05,
      "loss": 2.5871,
      "step": 26800
    },
    {
      "epoch": 13.636826042726348,
      "grad_norm": 20.482624053955078,
      "learning_rate": 3.636317395727365e-05,
      "loss": 2.5798,
      "step": 26810
    },
    {
      "epoch": 13.641912512716175,
      "grad_norm": 29.322622299194336,
      "learning_rate": 3.635808748728383e-05,
      "loss": 2.5811,
      "step": 26820
    },
    {
      "epoch": 13.646998982706002,
      "grad_norm": 21.138526916503906,
      "learning_rate": 3.6353001017294e-05,
      "loss": 2.5801,
      "step": 26830
    },
    {
      "epoch": 13.652085452695829,
      "grad_norm": 31.20744514465332,
      "learning_rate": 3.6347914547304176e-05,
      "loss": 2.5022,
      "step": 26840
    },
    {
      "epoch": 13.657171922685656,
      "grad_norm": 20.94512176513672,
      "learning_rate": 3.6342828077314345e-05,
      "loss": 2.5375,
      "step": 26850
    },
    {
      "epoch": 13.662258392675483,
      "grad_norm": 24.29097557067871,
      "learning_rate": 3.6337741607324515e-05,
      "loss": 2.526,
      "step": 26860
    },
    {
      "epoch": 13.66734486266531,
      "grad_norm": 22.52212905883789,
      "learning_rate": 3.633265513733469e-05,
      "loss": 2.5789,
      "step": 26870
    },
    {
      "epoch": 13.672431332655137,
      "grad_norm": 22.996109008789062,
      "learning_rate": 3.632756866734486e-05,
      "loss": 2.5491,
      "step": 26880
    },
    {
      "epoch": 13.677517802644964,
      "grad_norm": 21.003026962280273,
      "learning_rate": 3.632248219735504e-05,
      "loss": 2.5973,
      "step": 26890
    },
    {
      "epoch": 13.68260427263479,
      "grad_norm": 21.454242706298828,
      "learning_rate": 3.631739572736521e-05,
      "loss": 2.4646,
      "step": 26900
    },
    {
      "epoch": 13.687690742624618,
      "grad_norm": 20.739931106567383,
      "learning_rate": 3.6312309257375385e-05,
      "loss": 2.6028,
      "step": 26910
    },
    {
      "epoch": 13.692777212614445,
      "grad_norm": 24.565847396850586,
      "learning_rate": 3.630722278738556e-05,
      "loss": 2.6089,
      "step": 26920
    },
    {
      "epoch": 13.697863682604273,
      "grad_norm": 23.056516647338867,
      "learning_rate": 3.630213631739573e-05,
      "loss": 2.5685,
      "step": 26930
    },
    {
      "epoch": 13.7029501525941,
      "grad_norm": 19.566570281982422,
      "learning_rate": 3.62970498474059e-05,
      "loss": 2.5894,
      "step": 26940
    },
    {
      "epoch": 13.708036622583927,
      "grad_norm": 30.533132553100586,
      "learning_rate": 3.629196337741608e-05,
      "loss": 2.6224,
      "step": 26950
    },
    {
      "epoch": 13.713123092573754,
      "grad_norm": 27.450294494628906,
      "learning_rate": 3.628687690742625e-05,
      "loss": 2.6718,
      "step": 26960
    },
    {
      "epoch": 13.718209562563581,
      "grad_norm": 23.47665786743164,
      "learning_rate": 3.628179043743642e-05,
      "loss": 2.5804,
      "step": 26970
    },
    {
      "epoch": 13.723296032553408,
      "grad_norm": 23.568050384521484,
      "learning_rate": 3.6276703967446595e-05,
      "loss": 2.5202,
      "step": 26980
    },
    {
      "epoch": 13.728382502543235,
      "grad_norm": 19.293088912963867,
      "learning_rate": 3.6271617497456765e-05,
      "loss": 2.7349,
      "step": 26990
    },
    {
      "epoch": 13.733468972533062,
      "grad_norm": 18.575265884399414,
      "learning_rate": 3.6266531027466935e-05,
      "loss": 2.5324,
      "step": 27000
    },
    {
      "epoch": 13.73855544252289,
      "grad_norm": 26.08985710144043,
      "learning_rate": 3.626144455747711e-05,
      "loss": 2.5345,
      "step": 27010
    },
    {
      "epoch": 13.743641912512716,
      "grad_norm": 20.41582489013672,
      "learning_rate": 3.625635808748729e-05,
      "loss": 2.6066,
      "step": 27020
    },
    {
      "epoch": 13.748728382502543,
      "grad_norm": 25.292238235473633,
      "learning_rate": 3.625127161749746e-05,
      "loss": 2.5338,
      "step": 27030
    },
    {
      "epoch": 13.75381485249237,
      "grad_norm": 26.00623893737793,
      "learning_rate": 3.6246185147507634e-05,
      "loss": 2.4899,
      "step": 27040
    },
    {
      "epoch": 13.758901322482197,
      "grad_norm": 24.827608108520508,
      "learning_rate": 3.6241098677517804e-05,
      "loss": 2.6023,
      "step": 27050
    },
    {
      "epoch": 13.763987792472024,
      "grad_norm": 25.76827621459961,
      "learning_rate": 3.6236012207527974e-05,
      "loss": 2.5551,
      "step": 27060
    },
    {
      "epoch": 13.769074262461851,
      "grad_norm": 22.17778778076172,
      "learning_rate": 3.623092573753815e-05,
      "loss": 2.5899,
      "step": 27070
    },
    {
      "epoch": 13.774160732451678,
      "grad_norm": 25.009193420410156,
      "learning_rate": 3.622583926754832e-05,
      "loss": 2.4894,
      "step": 27080
    },
    {
      "epoch": 13.779247202441505,
      "grad_norm": 28.718013763427734,
      "learning_rate": 3.622075279755849e-05,
      "loss": 2.5368,
      "step": 27090
    },
    {
      "epoch": 13.784333672431332,
      "grad_norm": 22.170061111450195,
      "learning_rate": 3.621566632756867e-05,
      "loss": 2.5819,
      "step": 27100
    },
    {
      "epoch": 13.789420142421159,
      "grad_norm": 24.222436904907227,
      "learning_rate": 3.6210579857578844e-05,
      "loss": 2.5739,
      "step": 27110
    },
    {
      "epoch": 13.794506612410986,
      "grad_norm": 30.870826721191406,
      "learning_rate": 3.6205493387589014e-05,
      "loss": 2.6144,
      "step": 27120
    },
    {
      "epoch": 13.799593082400813,
      "grad_norm": 27.82790184020996,
      "learning_rate": 3.620040691759919e-05,
      "loss": 2.511,
      "step": 27130
    },
    {
      "epoch": 13.804679552390642,
      "grad_norm": 28.235843658447266,
      "learning_rate": 3.619532044760936e-05,
      "loss": 2.5506,
      "step": 27140
    },
    {
      "epoch": 13.809766022380469,
      "grad_norm": 19.797042846679688,
      "learning_rate": 3.619023397761954e-05,
      "loss": 2.5871,
      "step": 27150
    },
    {
      "epoch": 13.814852492370296,
      "grad_norm": 22.468313217163086,
      "learning_rate": 3.618514750762971e-05,
      "loss": 2.5522,
      "step": 27160
    },
    {
      "epoch": 13.819938962360123,
      "grad_norm": 23.431007385253906,
      "learning_rate": 3.618006103763988e-05,
      "loss": 2.5117,
      "step": 27170
    },
    {
      "epoch": 13.82502543234995,
      "grad_norm": 25.181726455688477,
      "learning_rate": 3.6174974567650054e-05,
      "loss": 2.5181,
      "step": 27180
    },
    {
      "epoch": 13.830111902339777,
      "grad_norm": 29.0820255279541,
      "learning_rate": 3.6169888097660223e-05,
      "loss": 2.6052,
      "step": 27190
    },
    {
      "epoch": 13.835198372329604,
      "grad_norm": 22.278968811035156,
      "learning_rate": 3.61648016276704e-05,
      "loss": 2.6032,
      "step": 27200
    },
    {
      "epoch": 13.84028484231943,
      "grad_norm": 26.07713508605957,
      "learning_rate": 3.615971515768058e-05,
      "loss": 2.5334,
      "step": 27210
    },
    {
      "epoch": 13.845371312309258,
      "grad_norm": 23.795196533203125,
      "learning_rate": 3.615462868769075e-05,
      "loss": 2.6005,
      "step": 27220
    },
    {
      "epoch": 13.850457782299085,
      "grad_norm": 21.914491653442383,
      "learning_rate": 3.6149542217700917e-05,
      "loss": 2.5522,
      "step": 27230
    },
    {
      "epoch": 13.855544252288912,
      "grad_norm": 30.91019630432129,
      "learning_rate": 3.614445574771109e-05,
      "loss": 2.591,
      "step": 27240
    },
    {
      "epoch": 13.860630722278739,
      "grad_norm": 25.795534133911133,
      "learning_rate": 3.613936927772126e-05,
      "loss": 2.4599,
      "step": 27250
    },
    {
      "epoch": 13.865717192268566,
      "grad_norm": 25.56988525390625,
      "learning_rate": 3.613428280773143e-05,
      "loss": 2.5594,
      "step": 27260
    },
    {
      "epoch": 13.870803662258393,
      "grad_norm": 23.892433166503906,
      "learning_rate": 3.612919633774161e-05,
      "loss": 2.6197,
      "step": 27270
    },
    {
      "epoch": 13.87589013224822,
      "grad_norm": 20.363054275512695,
      "learning_rate": 3.612410986775178e-05,
      "loss": 2.6399,
      "step": 27280
    },
    {
      "epoch": 13.880976602238047,
      "grad_norm": 28.374893188476562,
      "learning_rate": 3.6119023397761956e-05,
      "loss": 2.5212,
      "step": 27290
    },
    {
      "epoch": 13.886063072227874,
      "grad_norm": 20.779987335205078,
      "learning_rate": 3.6113936927772126e-05,
      "loss": 2.5463,
      "step": 27300
    },
    {
      "epoch": 13.8911495422177,
      "grad_norm": 30.83829116821289,
      "learning_rate": 3.61088504577823e-05,
      "loss": 2.568,
      "step": 27310
    },
    {
      "epoch": 13.896236012207527,
      "grad_norm": 22.268905639648438,
      "learning_rate": 3.610376398779247e-05,
      "loss": 2.5678,
      "step": 27320
    },
    {
      "epoch": 13.901322482197354,
      "grad_norm": 20.479528427124023,
      "learning_rate": 3.609867751780265e-05,
      "loss": 2.5626,
      "step": 27330
    },
    {
      "epoch": 13.906408952187181,
      "grad_norm": 24.048852920532227,
      "learning_rate": 3.609359104781282e-05,
      "loss": 2.5513,
      "step": 27340
    },
    {
      "epoch": 13.911495422177008,
      "grad_norm": 21.214336395263672,
      "learning_rate": 3.608850457782299e-05,
      "loss": 2.4559,
      "step": 27350
    },
    {
      "epoch": 13.916581892166835,
      "grad_norm": 21.842130661010742,
      "learning_rate": 3.6083418107833166e-05,
      "loss": 2.66,
      "step": 27360
    },
    {
      "epoch": 13.921668362156662,
      "grad_norm": 40.9643440246582,
      "learning_rate": 3.6078331637843336e-05,
      "loss": 2.6749,
      "step": 27370
    },
    {
      "epoch": 13.926754832146491,
      "grad_norm": 23.278783798217773,
      "learning_rate": 3.6073245167853506e-05,
      "loss": 2.6717,
      "step": 27380
    },
    {
      "epoch": 13.931841302136318,
      "grad_norm": 23.418081283569336,
      "learning_rate": 3.606815869786368e-05,
      "loss": 2.4882,
      "step": 27390
    },
    {
      "epoch": 13.936927772126145,
      "grad_norm": 23.94286346435547,
      "learning_rate": 3.606307222787386e-05,
      "loss": 2.6292,
      "step": 27400
    },
    {
      "epoch": 13.942014242115972,
      "grad_norm": 22.67779541015625,
      "learning_rate": 3.605798575788403e-05,
      "loss": 2.6041,
      "step": 27410
    },
    {
      "epoch": 13.947100712105799,
      "grad_norm": 24.31475830078125,
      "learning_rate": 3.6052899287894206e-05,
      "loss": 2.5735,
      "step": 27420
    },
    {
      "epoch": 13.952187182095626,
      "grad_norm": 21.251192092895508,
      "learning_rate": 3.6047812817904375e-05,
      "loss": 2.5377,
      "step": 27430
    },
    {
      "epoch": 13.957273652085453,
      "grad_norm": 24.18180274963379,
      "learning_rate": 3.604272634791455e-05,
      "loss": 2.5313,
      "step": 27440
    },
    {
      "epoch": 13.96236012207528,
      "grad_norm": 25.26996421813965,
      "learning_rate": 3.603763987792472e-05,
      "loss": 2.5872,
      "step": 27450
    },
    {
      "epoch": 13.967446592065107,
      "grad_norm": 26.848100662231445,
      "learning_rate": 3.603255340793489e-05,
      "loss": 2.5687,
      "step": 27460
    },
    {
      "epoch": 13.972533062054934,
      "grad_norm": 23.497846603393555,
      "learning_rate": 3.602746693794507e-05,
      "loss": 2.5592,
      "step": 27470
    },
    {
      "epoch": 13.977619532044761,
      "grad_norm": 23.951282501220703,
      "learning_rate": 3.602238046795524e-05,
      "loss": 2.5408,
      "step": 27480
    },
    {
      "epoch": 13.982706002034588,
      "grad_norm": 23.46954345703125,
      "learning_rate": 3.6017293997965415e-05,
      "loss": 2.5545,
      "step": 27490
    },
    {
      "epoch": 13.987792472024415,
      "grad_norm": 20.639375686645508,
      "learning_rate": 3.601220752797559e-05,
      "loss": 2.6267,
      "step": 27500
    },
    {
      "epoch": 13.992878942014242,
      "grad_norm": 28.4224853515625,
      "learning_rate": 3.600712105798576e-05,
      "loss": 2.5845,
      "step": 27510
    },
    {
      "epoch": 13.997965412004069,
      "grad_norm": 24.17555809020996,
      "learning_rate": 3.600203458799593e-05,
      "loss": 2.5828,
      "step": 27520
    },
    {
      "epoch": 14.0,
      "eval_loss": 3.9522852897644043,
      "eval_runtime": 2.7133,
      "eval_samples_per_second": 1022.735,
      "eval_steps_per_second": 127.888,
      "step": 27524
    },
    {
      "epoch": 14.003051881993896,
      "grad_norm": 23.02442169189453,
      "learning_rate": 3.599694811800611e-05,
      "loss": 2.5278,
      "step": 27530
    },
    {
      "epoch": 14.008138351983723,
      "grad_norm": 34.341251373291016,
      "learning_rate": 3.599186164801628e-05,
      "loss": 2.565,
      "step": 27540
    },
    {
      "epoch": 14.01322482197355,
      "grad_norm": 24.42139434814453,
      "learning_rate": 3.598677517802645e-05,
      "loss": 2.5117,
      "step": 27550
    },
    {
      "epoch": 14.018311291963377,
      "grad_norm": 24.73546600341797,
      "learning_rate": 3.5981688708036625e-05,
      "loss": 2.5064,
      "step": 27560
    },
    {
      "epoch": 14.023397761953204,
      "grad_norm": 31.324190139770508,
      "learning_rate": 3.5976602238046795e-05,
      "loss": 2.5635,
      "step": 27570
    },
    {
      "epoch": 14.02848423194303,
      "grad_norm": 26.239206314086914,
      "learning_rate": 3.597151576805697e-05,
      "loss": 2.6754,
      "step": 27580
    },
    {
      "epoch": 14.033570701932858,
      "grad_norm": 26.381311416625977,
      "learning_rate": 3.596642929806715e-05,
      "loss": 2.5569,
      "step": 27590
    },
    {
      "epoch": 14.038657171922686,
      "grad_norm": 27.80300521850586,
      "learning_rate": 3.596134282807732e-05,
      "loss": 2.4857,
      "step": 27600
    },
    {
      "epoch": 14.043743641912513,
      "grad_norm": 23.336816787719727,
      "learning_rate": 3.595625635808749e-05,
      "loss": 2.5518,
      "step": 27610
    },
    {
      "epoch": 14.04883011190234,
      "grad_norm": 25.83771324157715,
      "learning_rate": 3.5951169888097664e-05,
      "loss": 2.56,
      "step": 27620
    },
    {
      "epoch": 14.053916581892167,
      "grad_norm": 25.179155349731445,
      "learning_rate": 3.5946083418107834e-05,
      "loss": 2.516,
      "step": 27630
    },
    {
      "epoch": 14.059003051881994,
      "grad_norm": 30.93147850036621,
      "learning_rate": 3.5940996948118004e-05,
      "loss": 2.5432,
      "step": 27640
    },
    {
      "epoch": 14.064089521871821,
      "grad_norm": 18.03292465209961,
      "learning_rate": 3.593591047812818e-05,
      "loss": 2.5669,
      "step": 27650
    },
    {
      "epoch": 14.069175991861648,
      "grad_norm": 27.481502532958984,
      "learning_rate": 3.593082400813835e-05,
      "loss": 2.5741,
      "step": 27660
    },
    {
      "epoch": 14.074262461851475,
      "grad_norm": 24.998573303222656,
      "learning_rate": 3.592573753814852e-05,
      "loss": 2.5851,
      "step": 27670
    },
    {
      "epoch": 14.079348931841302,
      "grad_norm": 29.91911506652832,
      "learning_rate": 3.59206510681587e-05,
      "loss": 2.5087,
      "step": 27680
    },
    {
      "epoch": 14.08443540183113,
      "grad_norm": 22.28266143798828,
      "learning_rate": 3.5915564598168874e-05,
      "loss": 2.5693,
      "step": 27690
    },
    {
      "epoch": 14.089521871820956,
      "grad_norm": 25.756250381469727,
      "learning_rate": 3.591047812817905e-05,
      "loss": 2.4505,
      "step": 27700
    },
    {
      "epoch": 14.094608341810783,
      "grad_norm": 22.13241958618164,
      "learning_rate": 3.590539165818922e-05,
      "loss": 2.5456,
      "step": 27710
    },
    {
      "epoch": 14.09969481180061,
      "grad_norm": 22.32196807861328,
      "learning_rate": 3.590030518819939e-05,
      "loss": 2.6063,
      "step": 27720
    },
    {
      "epoch": 14.104781281790437,
      "grad_norm": 26.844379425048828,
      "learning_rate": 3.589521871820957e-05,
      "loss": 2.471,
      "step": 27730
    },
    {
      "epoch": 14.109867751780264,
      "grad_norm": 33.23750305175781,
      "learning_rate": 3.589013224821974e-05,
      "loss": 2.4304,
      "step": 27740
    },
    {
      "epoch": 14.114954221770091,
      "grad_norm": 28.586637496948242,
      "learning_rate": 3.588504577822991e-05,
      "loss": 2.5157,
      "step": 27750
    },
    {
      "epoch": 14.120040691759918,
      "grad_norm": 25.272336959838867,
      "learning_rate": 3.5879959308240084e-05,
      "loss": 2.5411,
      "step": 27760
    },
    {
      "epoch": 14.125127161749745,
      "grad_norm": 21.634159088134766,
      "learning_rate": 3.5874872838250253e-05,
      "loss": 2.5746,
      "step": 27770
    },
    {
      "epoch": 14.130213631739572,
      "grad_norm": 25.416492462158203,
      "learning_rate": 3.586978636826043e-05,
      "loss": 2.4694,
      "step": 27780
    },
    {
      "epoch": 14.135300101729399,
      "grad_norm": 27.955917358398438,
      "learning_rate": 3.586469989827061e-05,
      "loss": 2.6244,
      "step": 27790
    },
    {
      "epoch": 14.140386571719226,
      "grad_norm": 25.07227897644043,
      "learning_rate": 3.585961342828078e-05,
      "loss": 2.4959,
      "step": 27800
    },
    {
      "epoch": 14.145473041709053,
      "grad_norm": 23.031604766845703,
      "learning_rate": 3.5854526958290947e-05,
      "loss": 2.6272,
      "step": 27810
    },
    {
      "epoch": 14.150559511698882,
      "grad_norm": 26.402864456176758,
      "learning_rate": 3.584944048830112e-05,
      "loss": 2.6377,
      "step": 27820
    },
    {
      "epoch": 14.155645981688709,
      "grad_norm": 21.3441219329834,
      "learning_rate": 3.584435401831129e-05,
      "loss": 2.5798,
      "step": 27830
    },
    {
      "epoch": 14.160732451678536,
      "grad_norm": 26.110092163085938,
      "learning_rate": 3.583926754832146e-05,
      "loss": 2.517,
      "step": 27840
    },
    {
      "epoch": 14.165818921668363,
      "grad_norm": 26.90934944152832,
      "learning_rate": 3.583418107833164e-05,
      "loss": 2.5587,
      "step": 27850
    },
    {
      "epoch": 14.17090539165819,
      "grad_norm": 24.770877838134766,
      "learning_rate": 3.582909460834181e-05,
      "loss": 2.6458,
      "step": 27860
    },
    {
      "epoch": 14.175991861648017,
      "grad_norm": 24.704038619995117,
      "learning_rate": 3.5824008138351986e-05,
      "loss": 2.5333,
      "step": 27870
    },
    {
      "epoch": 14.181078331637844,
      "grad_norm": 25.10818862915039,
      "learning_rate": 3.581892166836216e-05,
      "loss": 2.5596,
      "step": 27880
    },
    {
      "epoch": 14.18616480162767,
      "grad_norm": 21.685121536254883,
      "learning_rate": 3.581383519837233e-05,
      "loss": 2.5428,
      "step": 27890
    },
    {
      "epoch": 14.191251271617498,
      "grad_norm": 23.540090560913086,
      "learning_rate": 3.58087487283825e-05,
      "loss": 2.5406,
      "step": 27900
    },
    {
      "epoch": 14.196337741607325,
      "grad_norm": 26.335176467895508,
      "learning_rate": 3.580366225839268e-05,
      "loss": 2.5467,
      "step": 27910
    },
    {
      "epoch": 14.201424211597152,
      "grad_norm": 28.727397918701172,
      "learning_rate": 3.579857578840285e-05,
      "loss": 2.5397,
      "step": 27920
    },
    {
      "epoch": 14.206510681586979,
      "grad_norm": 26.07877540588379,
      "learning_rate": 3.579348931841302e-05,
      "loss": 2.5319,
      "step": 27930
    },
    {
      "epoch": 14.211597151576806,
      "grad_norm": 21.394180297851562,
      "learning_rate": 3.5788402848423196e-05,
      "loss": 2.5041,
      "step": 27940
    },
    {
      "epoch": 14.216683621566633,
      "grad_norm": 25.93737030029297,
      "learning_rate": 3.5783316378433366e-05,
      "loss": 2.4775,
      "step": 27950
    },
    {
      "epoch": 14.22177009155646,
      "grad_norm": 25.8689022064209,
      "learning_rate": 3.577822990844354e-05,
      "loss": 2.5432,
      "step": 27960
    },
    {
      "epoch": 14.226856561546287,
      "grad_norm": 24.87371063232422,
      "learning_rate": 3.577314343845371e-05,
      "loss": 2.5873,
      "step": 27970
    },
    {
      "epoch": 14.231943031536114,
      "grad_norm": 26.709238052368164,
      "learning_rate": 3.576805696846389e-05,
      "loss": 2.4492,
      "step": 27980
    },
    {
      "epoch": 14.23702950152594,
      "grad_norm": 28.631925582885742,
      "learning_rate": 3.5762970498474066e-05,
      "loss": 2.5491,
      "step": 27990
    },
    {
      "epoch": 14.242115971515767,
      "grad_norm": 29.03108024597168,
      "learning_rate": 3.5757884028484236e-05,
      "loss": 2.5821,
      "step": 28000
    },
    {
      "epoch": 14.247202441505594,
      "grad_norm": 29.169546127319336,
      "learning_rate": 3.5752797558494405e-05,
      "loss": 2.5953,
      "step": 28010
    },
    {
      "epoch": 14.252288911495421,
      "grad_norm": 22.64309310913086,
      "learning_rate": 3.574771108850458e-05,
      "loss": 2.4953,
      "step": 28020
    },
    {
      "epoch": 14.257375381485248,
      "grad_norm": 25.45767593383789,
      "learning_rate": 3.574262461851475e-05,
      "loss": 2.4692,
      "step": 28030
    },
    {
      "epoch": 14.262461851475077,
      "grad_norm": 27.585630416870117,
      "learning_rate": 3.573753814852492e-05,
      "loss": 2.5224,
      "step": 28040
    },
    {
      "epoch": 14.267548321464904,
      "grad_norm": 24.191198348999023,
      "learning_rate": 3.57324516785351e-05,
      "loss": 2.4913,
      "step": 28050
    },
    {
      "epoch": 14.272634791454731,
      "grad_norm": 23.633350372314453,
      "learning_rate": 3.572736520854527e-05,
      "loss": 2.6274,
      "step": 28060
    },
    {
      "epoch": 14.277721261444558,
      "grad_norm": 28.372421264648438,
      "learning_rate": 3.5722278738555445e-05,
      "loss": 2.4413,
      "step": 28070
    },
    {
      "epoch": 14.282807731434385,
      "grad_norm": 30.89458465576172,
      "learning_rate": 3.571719226856562e-05,
      "loss": 2.6182,
      "step": 28080
    },
    {
      "epoch": 14.287894201424212,
      "grad_norm": 29.709688186645508,
      "learning_rate": 3.571210579857579e-05,
      "loss": 2.5331,
      "step": 28090
    },
    {
      "epoch": 14.292980671414039,
      "grad_norm": 25.856197357177734,
      "learning_rate": 3.570701932858596e-05,
      "loss": 2.4867,
      "step": 28100
    },
    {
      "epoch": 14.298067141403866,
      "grad_norm": 25.454763412475586,
      "learning_rate": 3.570193285859614e-05,
      "loss": 2.4471,
      "step": 28110
    },
    {
      "epoch": 14.303153611393693,
      "grad_norm": 20.954425811767578,
      "learning_rate": 3.569684638860631e-05,
      "loss": 2.5837,
      "step": 28120
    },
    {
      "epoch": 14.30824008138352,
      "grad_norm": 27.942909240722656,
      "learning_rate": 3.569175991861648e-05,
      "loss": 2.5215,
      "step": 28130
    },
    {
      "epoch": 14.313326551373347,
      "grad_norm": 22.63932991027832,
      "learning_rate": 3.5686673448626655e-05,
      "loss": 2.4989,
      "step": 28140
    },
    {
      "epoch": 14.318413021363174,
      "grad_norm": 23.9727840423584,
      "learning_rate": 3.5681586978636825e-05,
      "loss": 2.5145,
      "step": 28150
    },
    {
      "epoch": 14.323499491353001,
      "grad_norm": 22.77756118774414,
      "learning_rate": 3.5676500508647e-05,
      "loss": 2.475,
      "step": 28160
    },
    {
      "epoch": 14.328585961342828,
      "grad_norm": 25.25762367248535,
      "learning_rate": 3.567141403865718e-05,
      "loss": 2.5328,
      "step": 28170
    },
    {
      "epoch": 14.333672431332655,
      "grad_norm": 23.550220489501953,
      "learning_rate": 3.566632756866735e-05,
      "loss": 2.5204,
      "step": 28180
    },
    {
      "epoch": 14.338758901322482,
      "grad_norm": 28.31683349609375,
      "learning_rate": 3.566124109867752e-05,
      "loss": 2.5394,
      "step": 28190
    },
    {
      "epoch": 14.343845371312309,
      "grad_norm": 34.73573303222656,
      "learning_rate": 3.5656154628687694e-05,
      "loss": 2.4886,
      "step": 28200
    },
    {
      "epoch": 14.348931841302136,
      "grad_norm": 28.326501846313477,
      "learning_rate": 3.5651068158697864e-05,
      "loss": 2.5404,
      "step": 28210
    },
    {
      "epoch": 14.354018311291963,
      "grad_norm": 21.73981285095215,
      "learning_rate": 3.5645981688708034e-05,
      "loss": 2.5041,
      "step": 28220
    },
    {
      "epoch": 14.35910478128179,
      "grad_norm": 24.812124252319336,
      "learning_rate": 3.564089521871821e-05,
      "loss": 2.5671,
      "step": 28230
    },
    {
      "epoch": 14.364191251271617,
      "grad_norm": 22.525175094604492,
      "learning_rate": 3.563580874872838e-05,
      "loss": 2.5281,
      "step": 28240
    },
    {
      "epoch": 14.369277721261444,
      "grad_norm": 22.007396697998047,
      "learning_rate": 3.563072227873856e-05,
      "loss": 2.4622,
      "step": 28250
    },
    {
      "epoch": 14.37436419125127,
      "grad_norm": 26.223844528198242,
      "learning_rate": 3.5625635808748734e-05,
      "loss": 2.4611,
      "step": 28260
    },
    {
      "epoch": 14.3794506612411,
      "grad_norm": 32.73186492919922,
      "learning_rate": 3.5620549338758904e-05,
      "loss": 2.5934,
      "step": 28270
    },
    {
      "epoch": 14.384537131230926,
      "grad_norm": 26.79529571533203,
      "learning_rate": 3.561546286876908e-05,
      "loss": 2.5714,
      "step": 28280
    },
    {
      "epoch": 14.389623601220753,
      "grad_norm": 30.037702560424805,
      "learning_rate": 3.561037639877925e-05,
      "loss": 2.596,
      "step": 28290
    },
    {
      "epoch": 14.39471007121058,
      "grad_norm": 24.428186416625977,
      "learning_rate": 3.560528992878942e-05,
      "loss": 2.6092,
      "step": 28300
    },
    {
      "epoch": 14.399796541200407,
      "grad_norm": 37.20284652709961,
      "learning_rate": 3.56002034587996e-05,
      "loss": 2.4768,
      "step": 28310
    },
    {
      "epoch": 14.404883011190234,
      "grad_norm": 25.815448760986328,
      "learning_rate": 3.559511698880977e-05,
      "loss": 2.5583,
      "step": 28320
    },
    {
      "epoch": 14.409969481180061,
      "grad_norm": 24.53287696838379,
      "learning_rate": 3.559003051881994e-05,
      "loss": 2.581,
      "step": 28330
    },
    {
      "epoch": 14.415055951169888,
      "grad_norm": 23.35264778137207,
      "learning_rate": 3.5584944048830114e-05,
      "loss": 2.5485,
      "step": 28340
    },
    {
      "epoch": 14.420142421159715,
      "grad_norm": 27.054325103759766,
      "learning_rate": 3.5579857578840283e-05,
      "loss": 2.3675,
      "step": 28350
    },
    {
      "epoch": 14.425228891149542,
      "grad_norm": 28.532840728759766,
      "learning_rate": 3.557477110885046e-05,
      "loss": 2.4951,
      "step": 28360
    },
    {
      "epoch": 14.43031536113937,
      "grad_norm": 23.943906784057617,
      "learning_rate": 3.556968463886064e-05,
      "loss": 2.5572,
      "step": 28370
    },
    {
      "epoch": 14.435401831129196,
      "grad_norm": 30.55764389038086,
      "learning_rate": 3.556459816887081e-05,
      "loss": 2.5243,
      "step": 28380
    },
    {
      "epoch": 14.440488301119023,
      "grad_norm": 19.370515823364258,
      "learning_rate": 3.5559511698880977e-05,
      "loss": 2.4833,
      "step": 28390
    },
    {
      "epoch": 14.44557477110885,
      "grad_norm": 28.160171508789062,
      "learning_rate": 3.555442522889115e-05,
      "loss": 2.4542,
      "step": 28400
    },
    {
      "epoch": 14.450661241098677,
      "grad_norm": 26.131431579589844,
      "learning_rate": 3.554933875890132e-05,
      "loss": 2.4223,
      "step": 28410
    },
    {
      "epoch": 14.455747711088504,
      "grad_norm": 19.305362701416016,
      "learning_rate": 3.554425228891149e-05,
      "loss": 2.5237,
      "step": 28420
    },
    {
      "epoch": 14.460834181078331,
      "grad_norm": 23.63187026977539,
      "learning_rate": 3.553916581892167e-05,
      "loss": 2.5425,
      "step": 28430
    },
    {
      "epoch": 14.465920651068158,
      "grad_norm": 26.095378875732422,
      "learning_rate": 3.553407934893184e-05,
      "loss": 2.5476,
      "step": 28440
    },
    {
      "epoch": 14.471007121057985,
      "grad_norm": 23.029281616210938,
      "learning_rate": 3.5528992878942016e-05,
      "loss": 2.5408,
      "step": 28450
    },
    {
      "epoch": 14.476093591047812,
      "grad_norm": 21.846572875976562,
      "learning_rate": 3.552390640895219e-05,
      "loss": 2.4467,
      "step": 28460
    },
    {
      "epoch": 14.481180061037639,
      "grad_norm": 29.034990310668945,
      "learning_rate": 3.551881993896236e-05,
      "loss": 2.4875,
      "step": 28470
    },
    {
      "epoch": 14.486266531027466,
      "grad_norm": 24.25367546081543,
      "learning_rate": 3.551373346897253e-05,
      "loss": 2.484,
      "step": 28480
    },
    {
      "epoch": 14.491353001017295,
      "grad_norm": 23.6737060546875,
      "learning_rate": 3.550864699898271e-05,
      "loss": 2.492,
      "step": 28490
    },
    {
      "epoch": 14.496439471007122,
      "grad_norm": 24.772872924804688,
      "learning_rate": 3.550356052899288e-05,
      "loss": 2.444,
      "step": 28500
    },
    {
      "epoch": 14.501525940996949,
      "grad_norm": 24.183679580688477,
      "learning_rate": 3.5498474059003056e-05,
      "loss": 2.5338,
      "step": 28510
    },
    {
      "epoch": 14.506612410986776,
      "grad_norm": 26.540552139282227,
      "learning_rate": 3.5493387589013226e-05,
      "loss": 2.4805,
      "step": 28520
    },
    {
      "epoch": 14.511698880976603,
      "grad_norm": 25.544658660888672,
      "learning_rate": 3.5488301119023396e-05,
      "loss": 2.4862,
      "step": 28530
    },
    {
      "epoch": 14.51678535096643,
      "grad_norm": 20.16910171508789,
      "learning_rate": 3.548321464903357e-05,
      "loss": 2.4742,
      "step": 28540
    },
    {
      "epoch": 14.521871820956257,
      "grad_norm": 26.655595779418945,
      "learning_rate": 3.547812817904375e-05,
      "loss": 2.5026,
      "step": 28550
    },
    {
      "epoch": 14.526958290946084,
      "grad_norm": 23.167383193969727,
      "learning_rate": 3.547304170905392e-05,
      "loss": 2.461,
      "step": 28560
    },
    {
      "epoch": 14.53204476093591,
      "grad_norm": 26.453502655029297,
      "learning_rate": 3.5467955239064096e-05,
      "loss": 2.4376,
      "step": 28570
    },
    {
      "epoch": 14.537131230925738,
      "grad_norm": 24.66230010986328,
      "learning_rate": 3.5462868769074266e-05,
      "loss": 2.4386,
      "step": 28580
    },
    {
      "epoch": 14.542217700915565,
      "grad_norm": 31.292688369750977,
      "learning_rate": 3.5457782299084435e-05,
      "loss": 2.4824,
      "step": 28590
    },
    {
      "epoch": 14.547304170905392,
      "grad_norm": 29.786876678466797,
      "learning_rate": 3.545269582909461e-05,
      "loss": 2.5379,
      "step": 28600
    },
    {
      "epoch": 14.552390640895219,
      "grad_norm": 23.36712646484375,
      "learning_rate": 3.544760935910478e-05,
      "loss": 2.4679,
      "step": 28610
    },
    {
      "epoch": 14.557477110885046,
      "grad_norm": 34.73958206176758,
      "learning_rate": 3.544252288911495e-05,
      "loss": 2.4661,
      "step": 28620
    },
    {
      "epoch": 14.562563580874873,
      "grad_norm": 25.540363311767578,
      "learning_rate": 3.543743641912513e-05,
      "loss": 2.4542,
      "step": 28630
    },
    {
      "epoch": 14.5676500508647,
      "grad_norm": 23.91717529296875,
      "learning_rate": 3.54323499491353e-05,
      "loss": 2.5426,
      "step": 28640
    },
    {
      "epoch": 14.572736520854527,
      "grad_norm": 26.966751098632812,
      "learning_rate": 3.5427263479145475e-05,
      "loss": 2.4597,
      "step": 28650
    },
    {
      "epoch": 14.577822990844354,
      "grad_norm": 26.640474319458008,
      "learning_rate": 3.542217700915565e-05,
      "loss": 2.4765,
      "step": 28660
    },
    {
      "epoch": 14.58290946083418,
      "grad_norm": 28.98847770690918,
      "learning_rate": 3.541709053916582e-05,
      "loss": 2.3943,
      "step": 28670
    },
    {
      "epoch": 14.587995930824007,
      "grad_norm": 31.63633155822754,
      "learning_rate": 3.541200406917599e-05,
      "loss": 2.5522,
      "step": 28680
    },
    {
      "epoch": 14.593082400813834,
      "grad_norm": 27.31325340270996,
      "learning_rate": 3.540691759918617e-05,
      "loss": 2.4771,
      "step": 28690
    },
    {
      "epoch": 14.598168870803661,
      "grad_norm": 26.284807205200195,
      "learning_rate": 3.540183112919634e-05,
      "loss": 2.4257,
      "step": 28700
    },
    {
      "epoch": 14.603255340793488,
      "grad_norm": 31.95053482055664,
      "learning_rate": 3.539674465920651e-05,
      "loss": 2.5042,
      "step": 28710
    },
    {
      "epoch": 14.608341810783317,
      "grad_norm": 28.223419189453125,
      "learning_rate": 3.5391658189216685e-05,
      "loss": 2.5009,
      "step": 28720
    },
    {
      "epoch": 14.613428280773144,
      "grad_norm": 29.337839126586914,
      "learning_rate": 3.5386571719226855e-05,
      "loss": 2.5385,
      "step": 28730
    },
    {
      "epoch": 14.618514750762971,
      "grad_norm": 31.141372680664062,
      "learning_rate": 3.538148524923703e-05,
      "loss": 2.5482,
      "step": 28740
    },
    {
      "epoch": 14.623601220752798,
      "grad_norm": 25.141212463378906,
      "learning_rate": 3.537639877924721e-05,
      "loss": 2.5089,
      "step": 28750
    },
    {
      "epoch": 14.628687690742625,
      "grad_norm": 24.528461456298828,
      "learning_rate": 3.537131230925738e-05,
      "loss": 2.4286,
      "step": 28760
    },
    {
      "epoch": 14.633774160732452,
      "grad_norm": 27.35757064819336,
      "learning_rate": 3.5366225839267555e-05,
      "loss": 2.51,
      "step": 28770
    },
    {
      "epoch": 14.638860630722279,
      "grad_norm": 27.91904067993164,
      "learning_rate": 3.5361139369277724e-05,
      "loss": 2.6221,
      "step": 28780
    },
    {
      "epoch": 14.643947100712106,
      "grad_norm": 25.266067504882812,
      "learning_rate": 3.5356052899287894e-05,
      "loss": 2.5233,
      "step": 28790
    },
    {
      "epoch": 14.649033570701933,
      "grad_norm": 27.3529052734375,
      "learning_rate": 3.535096642929807e-05,
      "loss": 2.5066,
      "step": 28800
    },
    {
      "epoch": 14.65412004069176,
      "grad_norm": 22.239643096923828,
      "learning_rate": 3.534587995930824e-05,
      "loss": 2.51,
      "step": 28810
    },
    {
      "epoch": 14.659206510681587,
      "grad_norm": 23.26616859436035,
      "learning_rate": 3.534079348931841e-05,
      "loss": 2.5235,
      "step": 28820
    },
    {
      "epoch": 14.664292980671414,
      "grad_norm": 25.3167724609375,
      "learning_rate": 3.533570701932859e-05,
      "loss": 2.4928,
      "step": 28830
    },
    {
      "epoch": 14.669379450661241,
      "grad_norm": 22.299755096435547,
      "learning_rate": 3.5330620549338764e-05,
      "loss": 2.4679,
      "step": 28840
    },
    {
      "epoch": 14.674465920651068,
      "grad_norm": 29.891761779785156,
      "learning_rate": 3.5325534079348934e-05,
      "loss": 2.5317,
      "step": 28850
    },
    {
      "epoch": 14.679552390640895,
      "grad_norm": 24.012733459472656,
      "learning_rate": 3.532044760935911e-05,
      "loss": 2.4065,
      "step": 28860
    },
    {
      "epoch": 14.684638860630722,
      "grad_norm": 24.013019561767578,
      "learning_rate": 3.531536113936928e-05,
      "loss": 2.6675,
      "step": 28870
    },
    {
      "epoch": 14.689725330620549,
      "grad_norm": 28.70829200744629,
      "learning_rate": 3.531027466937945e-05,
      "loss": 2.5734,
      "step": 28880
    },
    {
      "epoch": 14.694811800610376,
      "grad_norm": 35.9483528137207,
      "learning_rate": 3.530518819938963e-05,
      "loss": 2.5598,
      "step": 28890
    },
    {
      "epoch": 14.699898270600203,
      "grad_norm": 30.482097625732422,
      "learning_rate": 3.53001017293998e-05,
      "loss": 2.5082,
      "step": 28900
    },
    {
      "epoch": 14.70498474059003,
      "grad_norm": 25.364566802978516,
      "learning_rate": 3.529501525940997e-05,
      "loss": 2.5156,
      "step": 28910
    },
    {
      "epoch": 14.710071210579857,
      "grad_norm": 29.6782169342041,
      "learning_rate": 3.5289928789420144e-05,
      "loss": 2.4313,
      "step": 28920
    },
    {
      "epoch": 14.715157680569686,
      "grad_norm": 27.9703311920166,
      "learning_rate": 3.5284842319430313e-05,
      "loss": 2.5241,
      "step": 28930
    },
    {
      "epoch": 14.720244150559513,
      "grad_norm": 22.98571014404297,
      "learning_rate": 3.527975584944049e-05,
      "loss": 2.5438,
      "step": 28940
    },
    {
      "epoch": 14.72533062054934,
      "grad_norm": 24.145517349243164,
      "learning_rate": 3.527466937945067e-05,
      "loss": 2.5103,
      "step": 28950
    },
    {
      "epoch": 14.730417090539166,
      "grad_norm": 21.537866592407227,
      "learning_rate": 3.526958290946084e-05,
      "loss": 2.5445,
      "step": 28960
    },
    {
      "epoch": 14.735503560528993,
      "grad_norm": 23.345808029174805,
      "learning_rate": 3.526449643947101e-05,
      "loss": 2.4693,
      "step": 28970
    },
    {
      "epoch": 14.74059003051882,
      "grad_norm": 25.817493438720703,
      "learning_rate": 3.525940996948118e-05,
      "loss": 2.4631,
      "step": 28980
    },
    {
      "epoch": 14.745676500508647,
      "grad_norm": 23.663921356201172,
      "learning_rate": 3.525432349949135e-05,
      "loss": 2.491,
      "step": 28990
    },
    {
      "epoch": 14.750762970498474,
      "grad_norm": 24.414735794067383,
      "learning_rate": 3.524923702950152e-05,
      "loss": 2.4275,
      "step": 29000
    },
    {
      "epoch": 14.755849440488301,
      "grad_norm": 26.200517654418945,
      "learning_rate": 3.52441505595117e-05,
      "loss": 2.5277,
      "step": 29010
    },
    {
      "epoch": 14.760935910478128,
      "grad_norm": 21.63678550720215,
      "learning_rate": 3.523906408952187e-05,
      "loss": 2.4596,
      "step": 29020
    },
    {
      "epoch": 14.766022380467955,
      "grad_norm": 25.39794921875,
      "learning_rate": 3.5233977619532046e-05,
      "loss": 2.4751,
      "step": 29030
    },
    {
      "epoch": 14.771108850457782,
      "grad_norm": 28.736099243164062,
      "learning_rate": 3.522889114954222e-05,
      "loss": 2.5042,
      "step": 29040
    },
    {
      "epoch": 14.77619532044761,
      "grad_norm": 21.739641189575195,
      "learning_rate": 3.522380467955239e-05,
      "loss": 2.5677,
      "step": 29050
    },
    {
      "epoch": 14.781281790437436,
      "grad_norm": 24.9266300201416,
      "learning_rate": 3.521871820956257e-05,
      "loss": 2.4651,
      "step": 29060
    },
    {
      "epoch": 14.786368260427263,
      "grad_norm": 22.345304489135742,
      "learning_rate": 3.521363173957274e-05,
      "loss": 2.5416,
      "step": 29070
    },
    {
      "epoch": 14.79145473041709,
      "grad_norm": 22.66775131225586,
      "learning_rate": 3.520854526958291e-05,
      "loss": 2.4743,
      "step": 29080
    },
    {
      "epoch": 14.796541200406917,
      "grad_norm": 26.453853607177734,
      "learning_rate": 3.5203458799593086e-05,
      "loss": 2.5821,
      "step": 29090
    },
    {
      "epoch": 14.801627670396744,
      "grad_norm": 21.74451446533203,
      "learning_rate": 3.5198372329603256e-05,
      "loss": 2.4318,
      "step": 29100
    },
    {
      "epoch": 14.806714140386571,
      "grad_norm": 27.356189727783203,
      "learning_rate": 3.5193285859613426e-05,
      "loss": 2.4544,
      "step": 29110
    },
    {
      "epoch": 14.811800610376398,
      "grad_norm": 20.717330932617188,
      "learning_rate": 3.51881993896236e-05,
      "loss": 2.5243,
      "step": 29120
    },
    {
      "epoch": 14.816887080366225,
      "grad_norm": 25.744953155517578,
      "learning_rate": 3.518311291963378e-05,
      "loss": 2.4407,
      "step": 29130
    },
    {
      "epoch": 14.821973550356052,
      "grad_norm": 23.000253677368164,
      "learning_rate": 3.517802644964395e-05,
      "loss": 2.4617,
      "step": 29140
    },
    {
      "epoch": 14.827060020345879,
      "grad_norm": 26.02432632446289,
      "learning_rate": 3.5172939979654126e-05,
      "loss": 2.4576,
      "step": 29150
    },
    {
      "epoch": 14.832146490335706,
      "grad_norm": 25.4945011138916,
      "learning_rate": 3.5167853509664296e-05,
      "loss": 2.4912,
      "step": 29160
    },
    {
      "epoch": 14.837232960325535,
      "grad_norm": 27.810392379760742,
      "learning_rate": 3.5162767039674465e-05,
      "loss": 2.4399,
      "step": 29170
    },
    {
      "epoch": 14.842319430315362,
      "grad_norm": 27.91948127746582,
      "learning_rate": 3.515768056968464e-05,
      "loss": 2.4995,
      "step": 29180
    },
    {
      "epoch": 14.847405900305189,
      "grad_norm": 24.184398651123047,
      "learning_rate": 3.515259409969481e-05,
      "loss": 2.5247,
      "step": 29190
    },
    {
      "epoch": 14.852492370295016,
      "grad_norm": 26.13617515563965,
      "learning_rate": 3.514750762970498e-05,
      "loss": 2.5134,
      "step": 29200
    },
    {
      "epoch": 14.857578840284843,
      "grad_norm": 28.688982009887695,
      "learning_rate": 3.514242115971516e-05,
      "loss": 2.4275,
      "step": 29210
    },
    {
      "epoch": 14.86266531027467,
      "grad_norm": 28.453964233398438,
      "learning_rate": 3.5137334689725335e-05,
      "loss": 2.4554,
      "step": 29220
    },
    {
      "epoch": 14.867751780264497,
      "grad_norm": 25.891836166381836,
      "learning_rate": 3.5132248219735505e-05,
      "loss": 2.5236,
      "step": 29230
    },
    {
      "epoch": 14.872838250254324,
      "grad_norm": 27.20775604248047,
      "learning_rate": 3.512716174974568e-05,
      "loss": 2.482,
      "step": 29240
    },
    {
      "epoch": 14.87792472024415,
      "grad_norm": 28.596399307250977,
      "learning_rate": 3.512207527975585e-05,
      "loss": 2.5636,
      "step": 29250
    },
    {
      "epoch": 14.883011190233978,
      "grad_norm": 23.98911476135254,
      "learning_rate": 3.511698880976602e-05,
      "loss": 2.4699,
      "step": 29260
    },
    {
      "epoch": 14.888097660223805,
      "grad_norm": 25.52351188659668,
      "learning_rate": 3.51119023397762e-05,
      "loss": 2.4229,
      "step": 29270
    },
    {
      "epoch": 14.893184130213632,
      "grad_norm": 31.458269119262695,
      "learning_rate": 3.510681586978637e-05,
      "loss": 2.4614,
      "step": 29280
    },
    {
      "epoch": 14.898270600203459,
      "grad_norm": 21.40937614440918,
      "learning_rate": 3.510172939979654e-05,
      "loss": 2.499,
      "step": 29290
    },
    {
      "epoch": 14.903357070193286,
      "grad_norm": 23.453088760375977,
      "learning_rate": 3.5096642929806715e-05,
      "loss": 2.4633,
      "step": 29300
    },
    {
      "epoch": 14.908443540183113,
      "grad_norm": 20.384197235107422,
      "learning_rate": 3.5091556459816885e-05,
      "loss": 2.5911,
      "step": 29310
    },
    {
      "epoch": 14.91353001017294,
      "grad_norm": 32.28675842285156,
      "learning_rate": 3.508646998982706e-05,
      "loss": 2.4865,
      "step": 29320
    },
    {
      "epoch": 14.918616480162767,
      "grad_norm": 26.89551544189453,
      "learning_rate": 3.508138351983724e-05,
      "loss": 2.4703,
      "step": 29330
    },
    {
      "epoch": 14.923702950152594,
      "grad_norm": 22.40842056274414,
      "learning_rate": 3.507629704984741e-05,
      "loss": 2.4961,
      "step": 29340
    },
    {
      "epoch": 14.92878942014242,
      "grad_norm": 24.27043342590332,
      "learning_rate": 3.5071210579857585e-05,
      "loss": 2.4558,
      "step": 29350
    },
    {
      "epoch": 14.933875890132247,
      "grad_norm": 24.908851623535156,
      "learning_rate": 3.5066124109867754e-05,
      "loss": 2.4361,
      "step": 29360
    },
    {
      "epoch": 14.938962360122074,
      "grad_norm": 23.748600006103516,
      "learning_rate": 3.5061037639877924e-05,
      "loss": 2.5306,
      "step": 29370
    },
    {
      "epoch": 14.944048830111903,
      "grad_norm": 23.56559181213379,
      "learning_rate": 3.50559511698881e-05,
      "loss": 2.5158,
      "step": 29380
    },
    {
      "epoch": 14.94913530010173,
      "grad_norm": 32.367183685302734,
      "learning_rate": 3.505086469989827e-05,
      "loss": 2.5588,
      "step": 29390
    },
    {
      "epoch": 14.954221770091557,
      "grad_norm": 24.997194290161133,
      "learning_rate": 3.504577822990844e-05,
      "loss": 2.4523,
      "step": 29400
    },
    {
      "epoch": 14.959308240081384,
      "grad_norm": 28.942148208618164,
      "learning_rate": 3.504069175991862e-05,
      "loss": 2.5408,
      "step": 29410
    },
    {
      "epoch": 14.964394710071211,
      "grad_norm": 29.030820846557617,
      "learning_rate": 3.5035605289928794e-05,
      "loss": 2.4184,
      "step": 29420
    },
    {
      "epoch": 14.969481180061038,
      "grad_norm": 33.053192138671875,
      "learning_rate": 3.5030518819938964e-05,
      "loss": 2.5396,
      "step": 29430
    },
    {
      "epoch": 14.974567650050865,
      "grad_norm": 24.008058547973633,
      "learning_rate": 3.502543234994914e-05,
      "loss": 2.5176,
      "step": 29440
    },
    {
      "epoch": 14.979654120040692,
      "grad_norm": 27.737403869628906,
      "learning_rate": 3.502034587995931e-05,
      "loss": 2.5072,
      "step": 29450
    },
    {
      "epoch": 14.984740590030519,
      "grad_norm": 24.299449920654297,
      "learning_rate": 3.501525940996948e-05,
      "loss": 2.4927,
      "step": 29460
    },
    {
      "epoch": 14.989827060020346,
      "grad_norm": 25.68606948852539,
      "learning_rate": 3.501017293997966e-05,
      "loss": 2.4895,
      "step": 29470
    },
    {
      "epoch": 14.994913530010173,
      "grad_norm": 24.177410125732422,
      "learning_rate": 3.500508646998983e-05,
      "loss": 2.4582,
      "step": 29480
    },
    {
      "epoch": 15.0,
      "grad_norm": 30.78522300720215,
      "learning_rate": 3.5e-05,
      "loss": 2.51,
      "step": 29490
    },
    {
      "epoch": 15.0,
      "eval_loss": 4.0112128257751465,
      "eval_runtime": 2.8211,
      "eval_samples_per_second": 983.66,
      "eval_steps_per_second": 123.002,
      "step": 29490
    },
    {
      "epoch": 15.005086469989827,
      "grad_norm": 27.29754066467285,
      "learning_rate": 3.4994913530010174e-05,
      "loss": 2.4112,
      "step": 29500
    },
    {
      "epoch": 15.010172939979654,
      "grad_norm": 27.435441970825195,
      "learning_rate": 3.498982706002035e-05,
      "loss": 2.5097,
      "step": 29510
    },
    {
      "epoch": 15.015259409969481,
      "grad_norm": 24.39066505432129,
      "learning_rate": 3.498474059003052e-05,
      "loss": 2.3974,
      "step": 29520
    },
    {
      "epoch": 15.020345879959308,
      "grad_norm": 26.971895217895508,
      "learning_rate": 3.49796541200407e-05,
      "loss": 2.5463,
      "step": 29530
    },
    {
      "epoch": 15.025432349949135,
      "grad_norm": 25.674684524536133,
      "learning_rate": 3.497456765005087e-05,
      "loss": 2.5293,
      "step": 29540
    },
    {
      "epoch": 15.030518819938962,
      "grad_norm": 24.718965530395508,
      "learning_rate": 3.496948118006104e-05,
      "loss": 2.5035,
      "step": 29550
    },
    {
      "epoch": 15.035605289928789,
      "grad_norm": 23.71249008178711,
      "learning_rate": 3.496439471007121e-05,
      "loss": 2.5058,
      "step": 29560
    },
    {
      "epoch": 15.040691759918616,
      "grad_norm": 27.303424835205078,
      "learning_rate": 3.495930824008138e-05,
      "loss": 2.4795,
      "step": 29570
    },
    {
      "epoch": 15.045778229908443,
      "grad_norm": 25.054641723632812,
      "learning_rate": 3.495422177009156e-05,
      "loss": 2.3849,
      "step": 29580
    },
    {
      "epoch": 15.05086469989827,
      "grad_norm": 26.97388458251953,
      "learning_rate": 3.494913530010173e-05,
      "loss": 2.4576,
      "step": 29590
    },
    {
      "epoch": 15.055951169888097,
      "grad_norm": 26.868654251098633,
      "learning_rate": 3.49440488301119e-05,
      "loss": 2.4285,
      "step": 29600
    },
    {
      "epoch": 15.061037639877926,
      "grad_norm": 29.308818817138672,
      "learning_rate": 3.4938962360122076e-05,
      "loss": 2.4435,
      "step": 29610
    },
    {
      "epoch": 15.066124109867753,
      "grad_norm": 23.66623306274414,
      "learning_rate": 3.493387589013225e-05,
      "loss": 2.4432,
      "step": 29620
    },
    {
      "epoch": 15.07121057985758,
      "grad_norm": 24.886919021606445,
      "learning_rate": 3.492878942014242e-05,
      "loss": 2.5358,
      "step": 29630
    },
    {
      "epoch": 15.076297049847406,
      "grad_norm": 22.819576263427734,
      "learning_rate": 3.49237029501526e-05,
      "loss": 2.5237,
      "step": 29640
    },
    {
      "epoch": 15.081383519837233,
      "grad_norm": 30.659086227416992,
      "learning_rate": 3.491861648016277e-05,
      "loss": 2.5272,
      "step": 29650
    },
    {
      "epoch": 15.08646998982706,
      "grad_norm": 30.063364028930664,
      "learning_rate": 3.491353001017294e-05,
      "loss": 2.429,
      "step": 29660
    },
    {
      "epoch": 15.091556459816887,
      "grad_norm": 27.22756004333496,
      "learning_rate": 3.4908443540183116e-05,
      "loss": 2.3858,
      "step": 29670
    },
    {
      "epoch": 15.096642929806714,
      "grad_norm": 26.664756774902344,
      "learning_rate": 3.4903357070193286e-05,
      "loss": 2.4628,
      "step": 29680
    },
    {
      "epoch": 15.101729399796541,
      "grad_norm": 22.57786750793457,
      "learning_rate": 3.4898270600203456e-05,
      "loss": 2.4843,
      "step": 29690
    },
    {
      "epoch": 15.106815869786368,
      "grad_norm": 27.69800567626953,
      "learning_rate": 3.489318413021363e-05,
      "loss": 2.5226,
      "step": 29700
    },
    {
      "epoch": 15.111902339776195,
      "grad_norm": 26.178634643554688,
      "learning_rate": 3.488809766022381e-05,
      "loss": 2.448,
      "step": 29710
    },
    {
      "epoch": 15.116988809766022,
      "grad_norm": 25.596933364868164,
      "learning_rate": 3.488301119023398e-05,
      "loss": 2.533,
      "step": 29720
    },
    {
      "epoch": 15.12207527975585,
      "grad_norm": 23.90433692932129,
      "learning_rate": 3.4877924720244156e-05,
      "loss": 2.4263,
      "step": 29730
    },
    {
      "epoch": 15.127161749745676,
      "grad_norm": 25.963842391967773,
      "learning_rate": 3.4872838250254326e-05,
      "loss": 2.4215,
      "step": 29740
    },
    {
      "epoch": 15.132248219735503,
      "grad_norm": 28.55860710144043,
      "learning_rate": 3.4867751780264495e-05,
      "loss": 2.4305,
      "step": 29750
    },
    {
      "epoch": 15.13733468972533,
      "grad_norm": 26.66954231262207,
      "learning_rate": 3.486266531027467e-05,
      "loss": 2.4757,
      "step": 29760
    },
    {
      "epoch": 15.142421159715157,
      "grad_norm": 28.397432327270508,
      "learning_rate": 3.485757884028484e-05,
      "loss": 2.4749,
      "step": 29770
    },
    {
      "epoch": 15.147507629704984,
      "grad_norm": 23.881389617919922,
      "learning_rate": 3.485249237029501e-05,
      "loss": 2.363,
      "step": 29780
    },
    {
      "epoch": 15.152594099694811,
      "grad_norm": 25.560741424560547,
      "learning_rate": 3.484740590030519e-05,
      "loss": 2.4852,
      "step": 29790
    },
    {
      "epoch": 15.157680569684638,
      "grad_norm": 29.114633560180664,
      "learning_rate": 3.4842319430315365e-05,
      "loss": 2.427,
      "step": 29800
    },
    {
      "epoch": 15.162767039674465,
      "grad_norm": 26.618257522583008,
      "learning_rate": 3.4837232960325535e-05,
      "loss": 2.422,
      "step": 29810
    },
    {
      "epoch": 15.167853509664292,
      "grad_norm": 23.70066261291504,
      "learning_rate": 3.483214649033571e-05,
      "loss": 2.3892,
      "step": 29820
    },
    {
      "epoch": 15.172939979654121,
      "grad_norm": 34.40803146362305,
      "learning_rate": 3.482706002034588e-05,
      "loss": 2.4671,
      "step": 29830
    },
    {
      "epoch": 15.178026449643948,
      "grad_norm": 26.532711029052734,
      "learning_rate": 3.482197355035605e-05,
      "loss": 2.5759,
      "step": 29840
    },
    {
      "epoch": 15.183112919633775,
      "grad_norm": 27.369943618774414,
      "learning_rate": 3.481688708036623e-05,
      "loss": 2.482,
      "step": 29850
    },
    {
      "epoch": 15.188199389623602,
      "grad_norm": 28.74950408935547,
      "learning_rate": 3.48118006103764e-05,
      "loss": 2.4532,
      "step": 29860
    },
    {
      "epoch": 15.193285859613429,
      "grad_norm": 30.35732650756836,
      "learning_rate": 3.4806714140386575e-05,
      "loss": 2.4232,
      "step": 29870
    },
    {
      "epoch": 15.198372329603256,
      "grad_norm": 25.438533782958984,
      "learning_rate": 3.4801627670396745e-05,
      "loss": 2.4282,
      "step": 29880
    },
    {
      "epoch": 15.203458799593083,
      "grad_norm": 27.429790496826172,
      "learning_rate": 3.4796541200406915e-05,
      "loss": 2.5763,
      "step": 29890
    },
    {
      "epoch": 15.20854526958291,
      "grad_norm": 22.95433235168457,
      "learning_rate": 3.479145473041709e-05,
      "loss": 2.4685,
      "step": 29900
    },
    {
      "epoch": 15.213631739572737,
      "grad_norm": 29.485685348510742,
      "learning_rate": 3.478636826042727e-05,
      "loss": 2.471,
      "step": 29910
    },
    {
      "epoch": 15.218718209562564,
      "grad_norm": 25.924875259399414,
      "learning_rate": 3.478128179043744e-05,
      "loss": 2.5083,
      "step": 29920
    },
    {
      "epoch": 15.22380467955239,
      "grad_norm": 24.48922348022461,
      "learning_rate": 3.4776195320447615e-05,
      "loss": 2.3901,
      "step": 29930
    },
    {
      "epoch": 15.228891149542218,
      "grad_norm": 28.520631790161133,
      "learning_rate": 3.4771108850457784e-05,
      "loss": 2.4993,
      "step": 29940
    },
    {
      "epoch": 15.233977619532045,
      "grad_norm": 28.900846481323242,
      "learning_rate": 3.4766022380467954e-05,
      "loss": 2.426,
      "step": 29950
    },
    {
      "epoch": 15.239064089521872,
      "grad_norm": 26.890396118164062,
      "learning_rate": 3.476093591047813e-05,
      "loss": 2.5198,
      "step": 29960
    },
    {
      "epoch": 15.244150559511699,
      "grad_norm": 44.44890594482422,
      "learning_rate": 3.47558494404883e-05,
      "loss": 2.3945,
      "step": 29970
    },
    {
      "epoch": 15.249237029501526,
      "grad_norm": 28.41674041748047,
      "learning_rate": 3.475076297049847e-05,
      "loss": 2.4295,
      "step": 29980
    },
    {
      "epoch": 15.254323499491353,
      "grad_norm": 29.948774337768555,
      "learning_rate": 3.474567650050865e-05,
      "loss": 2.4134,
      "step": 29990
    },
    {
      "epoch": 15.25940996948118,
      "grad_norm": 24.974205017089844,
      "learning_rate": 3.4740590030518824e-05,
      "loss": 2.4731,
      "step": 30000
    },
    {
      "epoch": 15.264496439471007,
      "grad_norm": 25.636716842651367,
      "learning_rate": 3.4735503560528994e-05,
      "loss": 2.4335,
      "step": 30010
    },
    {
      "epoch": 15.269582909460834,
      "grad_norm": 27.06859016418457,
      "learning_rate": 3.473041709053917e-05,
      "loss": 2.5374,
      "step": 30020
    },
    {
      "epoch": 15.27466937945066,
      "grad_norm": 27.05963897705078,
      "learning_rate": 3.472533062054934e-05,
      "loss": 2.451,
      "step": 30030
    },
    {
      "epoch": 15.279755849440487,
      "grad_norm": 23.561065673828125,
      "learning_rate": 3.472024415055951e-05,
      "loss": 2.4369,
      "step": 30040
    },
    {
      "epoch": 15.284842319430314,
      "grad_norm": 25.32481575012207,
      "learning_rate": 3.471515768056969e-05,
      "loss": 2.3211,
      "step": 30050
    },
    {
      "epoch": 15.289928789420143,
      "grad_norm": 28.8862361907959,
      "learning_rate": 3.471007121057986e-05,
      "loss": 2.4638,
      "step": 30060
    },
    {
      "epoch": 15.29501525940997,
      "grad_norm": 25.573989868164062,
      "learning_rate": 3.470498474059003e-05,
      "loss": 2.5398,
      "step": 30070
    },
    {
      "epoch": 15.300101729399797,
      "grad_norm": 22.541162490844727,
      "learning_rate": 3.4699898270600204e-05,
      "loss": 2.4154,
      "step": 30080
    },
    {
      "epoch": 15.305188199389624,
      "grad_norm": 23.09764862060547,
      "learning_rate": 3.469481180061038e-05,
      "loss": 2.3878,
      "step": 30090
    },
    {
      "epoch": 15.310274669379451,
      "grad_norm": 27.758182525634766,
      "learning_rate": 3.468972533062055e-05,
      "loss": 2.486,
      "step": 30100
    },
    {
      "epoch": 15.315361139369278,
      "grad_norm": 22.49387550354004,
      "learning_rate": 3.468463886063073e-05,
      "loss": 2.4664,
      "step": 30110
    },
    {
      "epoch": 15.320447609359105,
      "grad_norm": 33.82982635498047,
      "learning_rate": 3.46795523906409e-05,
      "loss": 2.4716,
      "step": 30120
    },
    {
      "epoch": 15.325534079348932,
      "grad_norm": 23.16184425354004,
      "learning_rate": 3.4674465920651073e-05,
      "loss": 2.4202,
      "step": 30130
    },
    {
      "epoch": 15.330620549338759,
      "grad_norm": 25.05315589904785,
      "learning_rate": 3.466937945066124e-05,
      "loss": 2.4604,
      "step": 30140
    },
    {
      "epoch": 15.335707019328586,
      "grad_norm": 25.53561782836914,
      "learning_rate": 3.466429298067141e-05,
      "loss": 2.4919,
      "step": 30150
    },
    {
      "epoch": 15.340793489318413,
      "grad_norm": 29.063732147216797,
      "learning_rate": 3.465920651068159e-05,
      "loss": 2.3779,
      "step": 30160
    },
    {
      "epoch": 15.34587995930824,
      "grad_norm": 25.828813552856445,
      "learning_rate": 3.465412004069176e-05,
      "loss": 2.4604,
      "step": 30170
    },
    {
      "epoch": 15.350966429298067,
      "grad_norm": 24.7543888092041,
      "learning_rate": 3.4649033570701936e-05,
      "loss": 2.3846,
      "step": 30180
    },
    {
      "epoch": 15.356052899287894,
      "grad_norm": 24.26690673828125,
      "learning_rate": 3.4643947100712106e-05,
      "loss": 2.4165,
      "step": 30190
    },
    {
      "epoch": 15.361139369277721,
      "grad_norm": 31.61428451538086,
      "learning_rate": 3.463886063072228e-05,
      "loss": 2.4912,
      "step": 30200
    },
    {
      "epoch": 15.366225839267548,
      "grad_norm": 25.59087562561035,
      "learning_rate": 3.463377416073245e-05,
      "loss": 2.4255,
      "step": 30210
    },
    {
      "epoch": 15.371312309257375,
      "grad_norm": 34.20085525512695,
      "learning_rate": 3.462868769074263e-05,
      "loss": 2.4429,
      "step": 30220
    },
    {
      "epoch": 15.376398779247202,
      "grad_norm": 34.11115264892578,
      "learning_rate": 3.46236012207528e-05,
      "loss": 2.5079,
      "step": 30230
    },
    {
      "epoch": 15.381485249237029,
      "grad_norm": 26.117361068725586,
      "learning_rate": 3.461851475076297e-05,
      "loss": 2.4664,
      "step": 30240
    },
    {
      "epoch": 15.386571719226856,
      "grad_norm": 26.75281524658203,
      "learning_rate": 3.4613428280773146e-05,
      "loss": 2.4237,
      "step": 30250
    },
    {
      "epoch": 15.391658189216683,
      "grad_norm": 26.97981071472168,
      "learning_rate": 3.4608341810783316e-05,
      "loss": 2.4527,
      "step": 30260
    },
    {
      "epoch": 15.396744659206512,
      "grad_norm": 28.18638801574707,
      "learning_rate": 3.4603255340793486e-05,
      "loss": 2.4441,
      "step": 30270
    },
    {
      "epoch": 15.401831129196339,
      "grad_norm": 30.713186264038086,
      "learning_rate": 3.459816887080366e-05,
      "loss": 2.4517,
      "step": 30280
    },
    {
      "epoch": 15.406917599186166,
      "grad_norm": 22.94356918334961,
      "learning_rate": 3.459308240081384e-05,
      "loss": 2.5186,
      "step": 30290
    },
    {
      "epoch": 15.412004069175993,
      "grad_norm": 28.30901336669922,
      "learning_rate": 3.458799593082401e-05,
      "loss": 2.3955,
      "step": 30300
    },
    {
      "epoch": 15.41709053916582,
      "grad_norm": 23.26702117919922,
      "learning_rate": 3.4582909460834186e-05,
      "loss": 2.4953,
      "step": 30310
    },
    {
      "epoch": 15.422177009155646,
      "grad_norm": 24.888038635253906,
      "learning_rate": 3.4577822990844356e-05,
      "loss": 2.4414,
      "step": 30320
    },
    {
      "epoch": 15.427263479145473,
      "grad_norm": 24.365882873535156,
      "learning_rate": 3.4572736520854525e-05,
      "loss": 2.4176,
      "step": 30330
    },
    {
      "epoch": 15.4323499491353,
      "grad_norm": 27.271785736083984,
      "learning_rate": 3.45676500508647e-05,
      "loss": 2.3809,
      "step": 30340
    },
    {
      "epoch": 15.437436419125127,
      "grad_norm": 27.02661895751953,
      "learning_rate": 3.456256358087487e-05,
      "loss": 2.4529,
      "step": 30350
    },
    {
      "epoch": 15.442522889114954,
      "grad_norm": 28.889259338378906,
      "learning_rate": 3.455747711088504e-05,
      "loss": 2.4717,
      "step": 30360
    },
    {
      "epoch": 15.447609359104781,
      "grad_norm": 29.040782928466797,
      "learning_rate": 3.455239064089522e-05,
      "loss": 2.4549,
      "step": 30370
    },
    {
      "epoch": 15.452695829094608,
      "grad_norm": 30.334989547729492,
      "learning_rate": 3.4547304170905395e-05,
      "loss": 2.4257,
      "step": 30380
    },
    {
      "epoch": 15.457782299084435,
      "grad_norm": 29.516199111938477,
      "learning_rate": 3.454221770091557e-05,
      "loss": 2.4199,
      "step": 30390
    },
    {
      "epoch": 15.462868769074262,
      "grad_norm": 23.957183837890625,
      "learning_rate": 3.453713123092574e-05,
      "loss": 2.4569,
      "step": 30400
    },
    {
      "epoch": 15.46795523906409,
      "grad_norm": 23.180984497070312,
      "learning_rate": 3.453204476093591e-05,
      "loss": 2.4164,
      "step": 30410
    },
    {
      "epoch": 15.473041709053916,
      "grad_norm": 25.714187622070312,
      "learning_rate": 3.452695829094609e-05,
      "loss": 2.4917,
      "step": 30420
    },
    {
      "epoch": 15.478128179043743,
      "grad_norm": 24.582721710205078,
      "learning_rate": 3.452187182095626e-05,
      "loss": 2.371,
      "step": 30430
    },
    {
      "epoch": 15.48321464903357,
      "grad_norm": 31.900548934936523,
      "learning_rate": 3.451678535096643e-05,
      "loss": 2.4045,
      "step": 30440
    },
    {
      "epoch": 15.488301119023397,
      "grad_norm": 25.894012451171875,
      "learning_rate": 3.4511698880976605e-05,
      "loss": 2.428,
      "step": 30450
    },
    {
      "epoch": 15.493387589013224,
      "grad_norm": 25.42908477783203,
      "learning_rate": 3.4506612410986775e-05,
      "loss": 2.3939,
      "step": 30460
    },
    {
      "epoch": 15.498474059003051,
      "grad_norm": 26.878894805908203,
      "learning_rate": 3.450152594099695e-05,
      "loss": 2.4545,
      "step": 30470
    },
    {
      "epoch": 15.503560528992878,
      "grad_norm": 36.813716888427734,
      "learning_rate": 3.449643947100713e-05,
      "loss": 2.4845,
      "step": 30480
    },
    {
      "epoch": 15.508646998982705,
      "grad_norm": 27.977420806884766,
      "learning_rate": 3.44913530010173e-05,
      "loss": 2.5018,
      "step": 30490
    },
    {
      "epoch": 15.513733468972532,
      "grad_norm": 26.942340850830078,
      "learning_rate": 3.448626653102747e-05,
      "loss": 2.4617,
      "step": 30500
    },
    {
      "epoch": 15.518819938962361,
      "grad_norm": 27.658946990966797,
      "learning_rate": 3.4481180061037645e-05,
      "loss": 2.4505,
      "step": 30510
    },
    {
      "epoch": 15.523906408952188,
      "grad_norm": 27.28745460510254,
      "learning_rate": 3.4476093591047814e-05,
      "loss": 2.4569,
      "step": 30520
    },
    {
      "epoch": 15.528992878942015,
      "grad_norm": 24.387922286987305,
      "learning_rate": 3.4471007121057984e-05,
      "loss": 2.4062,
      "step": 30530
    },
    {
      "epoch": 15.534079348931842,
      "grad_norm": 25.979660034179688,
      "learning_rate": 3.446592065106816e-05,
      "loss": 2.5058,
      "step": 30540
    },
    {
      "epoch": 15.539165818921669,
      "grad_norm": 31.400449752807617,
      "learning_rate": 3.446083418107833e-05,
      "loss": 2.3781,
      "step": 30550
    },
    {
      "epoch": 15.544252288911496,
      "grad_norm": 29.141448974609375,
      "learning_rate": 3.44557477110885e-05,
      "loss": 2.4388,
      "step": 30560
    },
    {
      "epoch": 15.549338758901323,
      "grad_norm": 33.103397369384766,
      "learning_rate": 3.445066124109868e-05,
      "loss": 2.4279,
      "step": 30570
    },
    {
      "epoch": 15.55442522889115,
      "grad_norm": 26.3868465423584,
      "learning_rate": 3.4445574771108854e-05,
      "loss": 2.4517,
      "step": 30580
    },
    {
      "epoch": 15.559511698880977,
      "grad_norm": 26.327972412109375,
      "learning_rate": 3.4440488301119024e-05,
      "loss": 2.3938,
      "step": 30590
    },
    {
      "epoch": 15.564598168870804,
      "grad_norm": 27.900409698486328,
      "learning_rate": 3.44354018311292e-05,
      "loss": 2.4159,
      "step": 30600
    },
    {
      "epoch": 15.56968463886063,
      "grad_norm": 28.22157859802246,
      "learning_rate": 3.443031536113937e-05,
      "loss": 2.4312,
      "step": 30610
    },
    {
      "epoch": 15.574771108850458,
      "grad_norm": 35.54229736328125,
      "learning_rate": 3.442522889114954e-05,
      "loss": 2.4616,
      "step": 30620
    },
    {
      "epoch": 15.579857578840285,
      "grad_norm": 31.42286491394043,
      "learning_rate": 3.442014242115972e-05,
      "loss": 2.434,
      "step": 30630
    },
    {
      "epoch": 15.584944048830112,
      "grad_norm": 27.762874603271484,
      "learning_rate": 3.441505595116989e-05,
      "loss": 2.4028,
      "step": 30640
    },
    {
      "epoch": 15.590030518819939,
      "grad_norm": 23.536556243896484,
      "learning_rate": 3.440996948118006e-05,
      "loss": 2.5259,
      "step": 30650
    },
    {
      "epoch": 15.595116988809766,
      "grad_norm": 30.193201065063477,
      "learning_rate": 3.4404883011190234e-05,
      "loss": 2.4451,
      "step": 30660
    },
    {
      "epoch": 15.600203458799593,
      "grad_norm": 40.574371337890625,
      "learning_rate": 3.439979654120041e-05,
      "loss": 2.4376,
      "step": 30670
    },
    {
      "epoch": 15.60528992878942,
      "grad_norm": 26.30286407470703,
      "learning_rate": 3.439471007121059e-05,
      "loss": 2.3934,
      "step": 30680
    },
    {
      "epoch": 15.610376398779247,
      "grad_norm": 29.27397346496582,
      "learning_rate": 3.438962360122076e-05,
      "loss": 2.3495,
      "step": 30690
    },
    {
      "epoch": 15.615462868769074,
      "grad_norm": 30.767820358276367,
      "learning_rate": 3.438453713123093e-05,
      "loss": 2.4294,
      "step": 30700
    },
    {
      "epoch": 15.6205493387589,
      "grad_norm": 29.094865798950195,
      "learning_rate": 3.4379450661241103e-05,
      "loss": 2.3745,
      "step": 30710
    },
    {
      "epoch": 15.62563580874873,
      "grad_norm": 27.590404510498047,
      "learning_rate": 3.437436419125127e-05,
      "loss": 2.408,
      "step": 30720
    },
    {
      "epoch": 15.630722278738556,
      "grad_norm": 23.264991760253906,
      "learning_rate": 3.436927772126144e-05,
      "loss": 2.3801,
      "step": 30730
    },
    {
      "epoch": 15.635808748728383,
      "grad_norm": 27.155481338500977,
      "learning_rate": 3.436419125127162e-05,
      "loss": 2.5186,
      "step": 30740
    },
    {
      "epoch": 15.64089521871821,
      "grad_norm": 35.19974136352539,
      "learning_rate": 3.435910478128179e-05,
      "loss": 2.3632,
      "step": 30750
    },
    {
      "epoch": 15.645981688708037,
      "grad_norm": 22.659862518310547,
      "learning_rate": 3.4354018311291966e-05,
      "loss": 2.3744,
      "step": 30760
    },
    {
      "epoch": 15.651068158697864,
      "grad_norm": 29.904369354248047,
      "learning_rate": 3.434893184130214e-05,
      "loss": 2.3484,
      "step": 30770
    },
    {
      "epoch": 15.656154628687691,
      "grad_norm": 29.762720108032227,
      "learning_rate": 3.434384537131231e-05,
      "loss": 2.5045,
      "step": 30780
    },
    {
      "epoch": 15.661241098677518,
      "grad_norm": 30.839853286743164,
      "learning_rate": 3.433875890132248e-05,
      "loss": 2.422,
      "step": 30790
    },
    {
      "epoch": 15.666327568667345,
      "grad_norm": 25.843671798706055,
      "learning_rate": 3.433367243133266e-05,
      "loss": 2.4539,
      "step": 30800
    },
    {
      "epoch": 15.671414038657172,
      "grad_norm": 36.14833450317383,
      "learning_rate": 3.432858596134283e-05,
      "loss": 2.3468,
      "step": 30810
    },
    {
      "epoch": 15.676500508646999,
      "grad_norm": 24.150379180908203,
      "learning_rate": 3.4323499491353e-05,
      "loss": 2.4872,
      "step": 30820
    },
    {
      "epoch": 15.681586978636826,
      "grad_norm": 33.92300796508789,
      "learning_rate": 3.4318413021363176e-05,
      "loss": 2.5037,
      "step": 30830
    },
    {
      "epoch": 15.686673448626653,
      "grad_norm": 23.274089813232422,
      "learning_rate": 3.4313326551373346e-05,
      "loss": 2.5003,
      "step": 30840
    },
    {
      "epoch": 15.69175991861648,
      "grad_norm": 25.35913848876953,
      "learning_rate": 3.430824008138352e-05,
      "loss": 2.388,
      "step": 30850
    },
    {
      "epoch": 15.696846388606307,
      "grad_norm": 24.626197814941406,
      "learning_rate": 3.430315361139369e-05,
      "loss": 2.4249,
      "step": 30860
    },
    {
      "epoch": 15.701932858596134,
      "grad_norm": 32.429176330566406,
      "learning_rate": 3.429806714140387e-05,
      "loss": 2.3188,
      "step": 30870
    },
    {
      "epoch": 15.707019328585961,
      "grad_norm": 28.033281326293945,
      "learning_rate": 3.429298067141404e-05,
      "loss": 2.397,
      "step": 30880
    },
    {
      "epoch": 15.712105798575788,
      "grad_norm": 31.780044555664062,
      "learning_rate": 3.4287894201424216e-05,
      "loss": 2.3964,
      "step": 30890
    },
    {
      "epoch": 15.717192268565615,
      "grad_norm": 27.964563369750977,
      "learning_rate": 3.4282807731434386e-05,
      "loss": 2.4042,
      "step": 30900
    },
    {
      "epoch": 15.722278738555442,
      "grad_norm": 41.206180572509766,
      "learning_rate": 3.4277721261444555e-05,
      "loss": 2.3744,
      "step": 30910
    },
    {
      "epoch": 15.727365208545269,
      "grad_norm": 32.01177215576172,
      "learning_rate": 3.427263479145473e-05,
      "loss": 2.4423,
      "step": 30920
    },
    {
      "epoch": 15.732451678535096,
      "grad_norm": 25.75041961669922,
      "learning_rate": 3.42675483214649e-05,
      "loss": 2.3534,
      "step": 30930
    },
    {
      "epoch": 15.737538148524923,
      "grad_norm": 28.816925048828125,
      "learning_rate": 3.426246185147508e-05,
      "loss": 2.5039,
      "step": 30940
    },
    {
      "epoch": 15.742624618514752,
      "grad_norm": 24.00715446472168,
      "learning_rate": 3.425737538148525e-05,
      "loss": 2.4705,
      "step": 30950
    },
    {
      "epoch": 15.747711088504579,
      "grad_norm": 25.057228088378906,
      "learning_rate": 3.4252288911495425e-05,
      "loss": 2.4365,
      "step": 30960
    },
    {
      "epoch": 15.752797558494406,
      "grad_norm": 24.491151809692383,
      "learning_rate": 3.42472024415056e-05,
      "loss": 2.3566,
      "step": 30970
    },
    {
      "epoch": 15.757884028484233,
      "grad_norm": 30.626249313354492,
      "learning_rate": 3.424211597151577e-05,
      "loss": 2.5445,
      "step": 30980
    },
    {
      "epoch": 15.76297049847406,
      "grad_norm": 23.487882614135742,
      "learning_rate": 3.423702950152594e-05,
      "loss": 2.4786,
      "step": 30990
    },
    {
      "epoch": 15.768056968463886,
      "grad_norm": 29.194801330566406,
      "learning_rate": 3.423194303153612e-05,
      "loss": 2.4949,
      "step": 31000
    },
    {
      "epoch": 15.773143438453713,
      "grad_norm": 24.02753448486328,
      "learning_rate": 3.422685656154629e-05,
      "loss": 2.5473,
      "step": 31010
    },
    {
      "epoch": 15.77822990844354,
      "grad_norm": 27.501352310180664,
      "learning_rate": 3.422177009155646e-05,
      "loss": 2.3964,
      "step": 31020
    },
    {
      "epoch": 15.783316378433367,
      "grad_norm": 24.86678695678711,
      "learning_rate": 3.4216683621566635e-05,
      "loss": 2.4193,
      "step": 31030
    },
    {
      "epoch": 15.788402848423194,
      "grad_norm": 27.681217193603516,
      "learning_rate": 3.4211597151576805e-05,
      "loss": 2.368,
      "step": 31040
    },
    {
      "epoch": 15.793489318413021,
      "grad_norm": 22.971450805664062,
      "learning_rate": 3.420651068158698e-05,
      "loss": 2.4389,
      "step": 31050
    },
    {
      "epoch": 15.798575788402848,
      "grad_norm": 37.342567443847656,
      "learning_rate": 3.420142421159716e-05,
      "loss": 2.4267,
      "step": 31060
    },
    {
      "epoch": 15.803662258392675,
      "grad_norm": 25.65577507019043,
      "learning_rate": 3.419633774160733e-05,
      "loss": 2.5095,
      "step": 31070
    },
    {
      "epoch": 15.808748728382502,
      "grad_norm": 26.208852767944336,
      "learning_rate": 3.41912512716175e-05,
      "loss": 2.3707,
      "step": 31080
    },
    {
      "epoch": 15.81383519837233,
      "grad_norm": 30.873531341552734,
      "learning_rate": 3.4186164801627675e-05,
      "loss": 2.3452,
      "step": 31090
    },
    {
      "epoch": 15.818921668362156,
      "grad_norm": 26.187957763671875,
      "learning_rate": 3.4181078331637844e-05,
      "loss": 2.4838,
      "step": 31100
    },
    {
      "epoch": 15.824008138351983,
      "grad_norm": 30.684770584106445,
      "learning_rate": 3.4175991861648014e-05,
      "loss": 2.4387,
      "step": 31110
    },
    {
      "epoch": 15.82909460834181,
      "grad_norm": 28.170562744140625,
      "learning_rate": 3.417090539165819e-05,
      "loss": 2.4031,
      "step": 31120
    },
    {
      "epoch": 15.834181078331637,
      "grad_norm": 26.358642578125,
      "learning_rate": 3.416581892166836e-05,
      "loss": 2.4176,
      "step": 31130
    },
    {
      "epoch": 15.839267548321464,
      "grad_norm": 33.34544372558594,
      "learning_rate": 3.416073245167854e-05,
      "loss": 2.4964,
      "step": 31140
    },
    {
      "epoch": 15.844354018311291,
      "grad_norm": 24.260107040405273,
      "learning_rate": 3.4155645981688714e-05,
      "loss": 2.5352,
      "step": 31150
    },
    {
      "epoch": 15.84944048830112,
      "grad_norm": 26.156282424926758,
      "learning_rate": 3.4150559511698884e-05,
      "loss": 2.3976,
      "step": 31160
    },
    {
      "epoch": 15.854526958290947,
      "grad_norm": 27.343582153320312,
      "learning_rate": 3.4145473041709054e-05,
      "loss": 2.3644,
      "step": 31170
    },
    {
      "epoch": 15.859613428280774,
      "grad_norm": 28.750839233398438,
      "learning_rate": 3.414038657171923e-05,
      "loss": 2.4142,
      "step": 31180
    },
    {
      "epoch": 15.864699898270601,
      "grad_norm": 29.688825607299805,
      "learning_rate": 3.41353001017294e-05,
      "loss": 2.4207,
      "step": 31190
    },
    {
      "epoch": 15.869786368260428,
      "grad_norm": 32.52779006958008,
      "learning_rate": 3.413021363173958e-05,
      "loss": 2.3975,
      "step": 31200
    },
    {
      "epoch": 15.874872838250255,
      "grad_norm": 30.41461753845215,
      "learning_rate": 3.412512716174975e-05,
      "loss": 2.4428,
      "step": 31210
    },
    {
      "epoch": 15.879959308240082,
      "grad_norm": 30.226850509643555,
      "learning_rate": 3.412004069175992e-05,
      "loss": 2.4802,
      "step": 31220
    },
    {
      "epoch": 15.885045778229909,
      "grad_norm": 32.8388786315918,
      "learning_rate": 3.4114954221770094e-05,
      "loss": 2.4776,
      "step": 31230
    },
    {
      "epoch": 15.890132248219736,
      "grad_norm": 28.138702392578125,
      "learning_rate": 3.4109867751780264e-05,
      "loss": 2.3685,
      "step": 31240
    },
    {
      "epoch": 15.895218718209563,
      "grad_norm": 23.10917854309082,
      "learning_rate": 3.410478128179044e-05,
      "loss": 2.4455,
      "step": 31250
    },
    {
      "epoch": 15.90030518819939,
      "grad_norm": 38.625240325927734,
      "learning_rate": 3.409969481180062e-05,
      "loss": 2.3954,
      "step": 31260
    },
    {
      "epoch": 15.905391658189217,
      "grad_norm": 31.482194900512695,
      "learning_rate": 3.409460834181079e-05,
      "loss": 2.3948,
      "step": 31270
    },
    {
      "epoch": 15.910478128179044,
      "grad_norm": 27.710622787475586,
      "learning_rate": 3.408952187182096e-05,
      "loss": 2.4204,
      "step": 31280
    },
    {
      "epoch": 15.91556459816887,
      "grad_norm": 27.936086654663086,
      "learning_rate": 3.4084435401831133e-05,
      "loss": 2.3904,
      "step": 31290
    },
    {
      "epoch": 15.920651068158698,
      "grad_norm": 28.76117706298828,
      "learning_rate": 3.40793489318413e-05,
      "loss": 2.4029,
      "step": 31300
    },
    {
      "epoch": 15.925737538148525,
      "grad_norm": 29.965524673461914,
      "learning_rate": 3.407426246185147e-05,
      "loss": 2.5121,
      "step": 31310
    },
    {
      "epoch": 15.930824008138352,
      "grad_norm": 29.385791778564453,
      "learning_rate": 3.406917599186165e-05,
      "loss": 2.3896,
      "step": 31320
    },
    {
      "epoch": 15.935910478128179,
      "grad_norm": 27.126384735107422,
      "learning_rate": 3.406408952187182e-05,
      "loss": 2.4161,
      "step": 31330
    },
    {
      "epoch": 15.940996948118006,
      "grad_norm": 22.55360984802246,
      "learning_rate": 3.4059003051881996e-05,
      "loss": 2.4154,
      "step": 31340
    },
    {
      "epoch": 15.946083418107833,
      "grad_norm": 24.322446823120117,
      "learning_rate": 3.405391658189217e-05,
      "loss": 2.4617,
      "step": 31350
    },
    {
      "epoch": 15.95116988809766,
      "grad_norm": 32.34096145629883,
      "learning_rate": 3.404883011190234e-05,
      "loss": 2.4777,
      "step": 31360
    },
    {
      "epoch": 15.956256358087487,
      "grad_norm": 33.61653518676758,
      "learning_rate": 3.404374364191251e-05,
      "loss": 2.4838,
      "step": 31370
    },
    {
      "epoch": 15.961342828077314,
      "grad_norm": 24.700462341308594,
      "learning_rate": 3.403865717192269e-05,
      "loss": 2.4134,
      "step": 31380
    },
    {
      "epoch": 15.96642929806714,
      "grad_norm": 25.70876121520996,
      "learning_rate": 3.403357070193286e-05,
      "loss": 2.4385,
      "step": 31390
    },
    {
      "epoch": 15.97151576805697,
      "grad_norm": 27.329858779907227,
      "learning_rate": 3.402848423194303e-05,
      "loss": 2.435,
      "step": 31400
    },
    {
      "epoch": 15.976602238046796,
      "grad_norm": 25.063587188720703,
      "learning_rate": 3.4023397761953206e-05,
      "loss": 2.4368,
      "step": 31410
    },
    {
      "epoch": 15.981688708036623,
      "grad_norm": 27.25081443786621,
      "learning_rate": 3.4018311291963376e-05,
      "loss": 2.3725,
      "step": 31420
    },
    {
      "epoch": 15.98677517802645,
      "grad_norm": 28.20474624633789,
      "learning_rate": 3.401322482197355e-05,
      "loss": 2.4335,
      "step": 31430
    },
    {
      "epoch": 15.991861648016277,
      "grad_norm": 30.844465255737305,
      "learning_rate": 3.400813835198373e-05,
      "loss": 2.362,
      "step": 31440
    },
    {
      "epoch": 15.996948118006104,
      "grad_norm": 31.490415573120117,
      "learning_rate": 3.40030518819939e-05,
      "loss": 2.4303,
      "step": 31450
    },
    {
      "epoch": 16.0,
      "eval_loss": 4.073080539703369,
      "eval_runtime": 2.7748,
      "eval_samples_per_second": 1000.063,
      "eval_steps_per_second": 125.053,
      "step": 31456
    },
    {
      "epoch": 16.00203458799593,
      "grad_norm": 22.833518981933594,
      "learning_rate": 3.399796541200407e-05,
      "loss": 2.4113,
      "step": 31460
    },
    {
      "epoch": 16.007121057985756,
      "grad_norm": 26.804988861083984,
      "learning_rate": 3.3992878942014246e-05,
      "loss": 2.402,
      "step": 31470
    },
    {
      "epoch": 16.012207527975583,
      "grad_norm": 33.46712112426758,
      "learning_rate": 3.3987792472024416e-05,
      "loss": 2.4569,
      "step": 31480
    },
    {
      "epoch": 16.01729399796541,
      "grad_norm": 27.721670150756836,
      "learning_rate": 3.398270600203459e-05,
      "loss": 2.429,
      "step": 31490
    },
    {
      "epoch": 16.022380467955237,
      "grad_norm": 22.49195671081543,
      "learning_rate": 3.397761953204476e-05,
      "loss": 2.3751,
      "step": 31500
    },
    {
      "epoch": 16.027466937945068,
      "grad_norm": 25.660036087036133,
      "learning_rate": 3.397253306205493e-05,
      "loss": 2.4358,
      "step": 31510
    },
    {
      "epoch": 16.032553407934895,
      "grad_norm": 25.619409561157227,
      "learning_rate": 3.396744659206511e-05,
      "loss": 2.4163,
      "step": 31520
    },
    {
      "epoch": 16.037639877924722,
      "grad_norm": 28.32622718811035,
      "learning_rate": 3.396236012207528e-05,
      "loss": 2.4238,
      "step": 31530
    },
    {
      "epoch": 16.04272634791455,
      "grad_norm": 23.438743591308594,
      "learning_rate": 3.3957273652085455e-05,
      "loss": 2.3887,
      "step": 31540
    },
    {
      "epoch": 16.047812817904376,
      "grad_norm": 27.5078067779541,
      "learning_rate": 3.395218718209563e-05,
      "loss": 2.4368,
      "step": 31550
    },
    {
      "epoch": 16.052899287894203,
      "grad_norm": 25.3629093170166,
      "learning_rate": 3.39471007121058e-05,
      "loss": 2.4485,
      "step": 31560
    },
    {
      "epoch": 16.05798575788403,
      "grad_norm": 24.124204635620117,
      "learning_rate": 3.394201424211597e-05,
      "loss": 2.4323,
      "step": 31570
    },
    {
      "epoch": 16.063072227873857,
      "grad_norm": 27.231172561645508,
      "learning_rate": 3.393692777212615e-05,
      "loss": 2.3482,
      "step": 31580
    },
    {
      "epoch": 16.068158697863684,
      "grad_norm": 22.319507598876953,
      "learning_rate": 3.393184130213632e-05,
      "loss": 2.4013,
      "step": 31590
    },
    {
      "epoch": 16.07324516785351,
      "grad_norm": 32.11559295654297,
      "learning_rate": 3.392675483214649e-05,
      "loss": 2.3544,
      "step": 31600
    },
    {
      "epoch": 16.078331637843338,
      "grad_norm": 27.072124481201172,
      "learning_rate": 3.3921668362156665e-05,
      "loss": 2.4285,
      "step": 31610
    },
    {
      "epoch": 16.083418107833165,
      "grad_norm": 29.061067581176758,
      "learning_rate": 3.3916581892166835e-05,
      "loss": 2.4467,
      "step": 31620
    },
    {
      "epoch": 16.08850457782299,
      "grad_norm": 26.01592445373535,
      "learning_rate": 3.391149542217701e-05,
      "loss": 2.3817,
      "step": 31630
    },
    {
      "epoch": 16.09359104781282,
      "grad_norm": 35.020477294921875,
      "learning_rate": 3.390640895218719e-05,
      "loss": 2.3031,
      "step": 31640
    },
    {
      "epoch": 16.098677517802646,
      "grad_norm": 22.173580169677734,
      "learning_rate": 3.390132248219736e-05,
      "loss": 2.3525,
      "step": 31650
    },
    {
      "epoch": 16.103763987792473,
      "grad_norm": 36.788841247558594,
      "learning_rate": 3.389623601220753e-05,
      "loss": 2.3475,
      "step": 31660
    },
    {
      "epoch": 16.1088504577823,
      "grad_norm": 26.414979934692383,
      "learning_rate": 3.3891149542217705e-05,
      "loss": 2.4047,
      "step": 31670
    },
    {
      "epoch": 16.113936927772126,
      "grad_norm": 32.58243942260742,
      "learning_rate": 3.3886063072227874e-05,
      "loss": 2.4492,
      "step": 31680
    },
    {
      "epoch": 16.119023397761953,
      "grad_norm": 33.65373229980469,
      "learning_rate": 3.3880976602238044e-05,
      "loss": 2.2424,
      "step": 31690
    },
    {
      "epoch": 16.12410986775178,
      "grad_norm": 26.143369674682617,
      "learning_rate": 3.387589013224822e-05,
      "loss": 2.4691,
      "step": 31700
    },
    {
      "epoch": 16.129196337741607,
      "grad_norm": 24.00440216064453,
      "learning_rate": 3.387080366225839e-05,
      "loss": 2.3725,
      "step": 31710
    },
    {
      "epoch": 16.134282807731434,
      "grad_norm": 30.18073081970215,
      "learning_rate": 3.386571719226857e-05,
      "loss": 2.3133,
      "step": 31720
    },
    {
      "epoch": 16.13936927772126,
      "grad_norm": 27.48185920715332,
      "learning_rate": 3.3860630722278744e-05,
      "loss": 2.3379,
      "step": 31730
    },
    {
      "epoch": 16.14445574771109,
      "grad_norm": 27.546018600463867,
      "learning_rate": 3.3855544252288914e-05,
      "loss": 2.377,
      "step": 31740
    },
    {
      "epoch": 16.149542217700915,
      "grad_norm": 25.515274047851562,
      "learning_rate": 3.385045778229909e-05,
      "loss": 2.4352,
      "step": 31750
    },
    {
      "epoch": 16.154628687690742,
      "grad_norm": 27.810815811157227,
      "learning_rate": 3.384537131230926e-05,
      "loss": 2.3436,
      "step": 31760
    },
    {
      "epoch": 16.15971515768057,
      "grad_norm": 22.8195858001709,
      "learning_rate": 3.384028484231943e-05,
      "loss": 2.4537,
      "step": 31770
    },
    {
      "epoch": 16.164801627670396,
      "grad_norm": 21.261808395385742,
      "learning_rate": 3.383519837232961e-05,
      "loss": 2.4534,
      "step": 31780
    },
    {
      "epoch": 16.169888097660223,
      "grad_norm": 24.62705421447754,
      "learning_rate": 3.383011190233978e-05,
      "loss": 2.3517,
      "step": 31790
    },
    {
      "epoch": 16.17497456765005,
      "grad_norm": 24.377639770507812,
      "learning_rate": 3.382502543234995e-05,
      "loss": 2.4054,
      "step": 31800
    },
    {
      "epoch": 16.180061037639877,
      "grad_norm": 29.76148223876953,
      "learning_rate": 3.3819938962360124e-05,
      "loss": 2.4241,
      "step": 31810
    },
    {
      "epoch": 16.185147507629704,
      "grad_norm": 27.895456314086914,
      "learning_rate": 3.3814852492370294e-05,
      "loss": 2.3931,
      "step": 31820
    },
    {
      "epoch": 16.19023397761953,
      "grad_norm": 25.546977996826172,
      "learning_rate": 3.380976602238047e-05,
      "loss": 2.368,
      "step": 31830
    },
    {
      "epoch": 16.195320447609358,
      "grad_norm": 26.92864418029785,
      "learning_rate": 3.380467955239065e-05,
      "loss": 2.3075,
      "step": 31840
    },
    {
      "epoch": 16.200406917599185,
      "grad_norm": 29.903762817382812,
      "learning_rate": 3.379959308240082e-05,
      "loss": 2.3953,
      "step": 31850
    },
    {
      "epoch": 16.205493387589012,
      "grad_norm": 24.62114143371582,
      "learning_rate": 3.379450661241099e-05,
      "loss": 2.3985,
      "step": 31860
    },
    {
      "epoch": 16.21057985757884,
      "grad_norm": 24.74680519104004,
      "learning_rate": 3.3789420142421163e-05,
      "loss": 2.3646,
      "step": 31870
    },
    {
      "epoch": 16.215666327568666,
      "grad_norm": 31.274980545043945,
      "learning_rate": 3.378433367243133e-05,
      "loss": 2.3744,
      "step": 31880
    },
    {
      "epoch": 16.220752797558493,
      "grad_norm": 30.819978713989258,
      "learning_rate": 3.37792472024415e-05,
      "loss": 2.3981,
      "step": 31890
    },
    {
      "epoch": 16.22583926754832,
      "grad_norm": 28.404088973999023,
      "learning_rate": 3.377416073245168e-05,
      "loss": 2.4177,
      "step": 31900
    },
    {
      "epoch": 16.230925737538147,
      "grad_norm": 24.454917907714844,
      "learning_rate": 3.376907426246185e-05,
      "loss": 2.4331,
      "step": 31910
    },
    {
      "epoch": 16.236012207527974,
      "grad_norm": 30.806522369384766,
      "learning_rate": 3.3763987792472026e-05,
      "loss": 2.394,
      "step": 31920
    },
    {
      "epoch": 16.2410986775178,
      "grad_norm": 28.54370880126953,
      "learning_rate": 3.37589013224822e-05,
      "loss": 2.4326,
      "step": 31930
    },
    {
      "epoch": 16.246185147507628,
      "grad_norm": 29.043132781982422,
      "learning_rate": 3.375381485249237e-05,
      "loss": 2.3117,
      "step": 31940
    },
    {
      "epoch": 16.25127161749746,
      "grad_norm": 27.1964168548584,
      "learning_rate": 3.374872838250254e-05,
      "loss": 2.3407,
      "step": 31950
    },
    {
      "epoch": 16.256358087487286,
      "grad_norm": 31.488130569458008,
      "learning_rate": 3.374364191251272e-05,
      "loss": 2.3675,
      "step": 31960
    },
    {
      "epoch": 16.261444557477112,
      "grad_norm": 26.5798282623291,
      "learning_rate": 3.373855544252289e-05,
      "loss": 2.3213,
      "step": 31970
    },
    {
      "epoch": 16.26653102746694,
      "grad_norm": 25.274219512939453,
      "learning_rate": 3.373346897253306e-05,
      "loss": 2.3537,
      "step": 31980
    },
    {
      "epoch": 16.271617497456766,
      "grad_norm": 25.193275451660156,
      "learning_rate": 3.3728382502543236e-05,
      "loss": 2.3049,
      "step": 31990
    },
    {
      "epoch": 16.276703967446593,
      "grad_norm": 29.644447326660156,
      "learning_rate": 3.3723296032553406e-05,
      "loss": 2.395,
      "step": 32000
    },
    {
      "epoch": 16.28179043743642,
      "grad_norm": 28.373512268066406,
      "learning_rate": 3.371820956256358e-05,
      "loss": 2.3857,
      "step": 32010
    },
    {
      "epoch": 16.286876907426247,
      "grad_norm": 31.679370880126953,
      "learning_rate": 3.371312309257376e-05,
      "loss": 2.3603,
      "step": 32020
    },
    {
      "epoch": 16.291963377416074,
      "grad_norm": 23.230907440185547,
      "learning_rate": 3.370803662258393e-05,
      "loss": 2.2953,
      "step": 32030
    },
    {
      "epoch": 16.2970498474059,
      "grad_norm": 35.07829666137695,
      "learning_rate": 3.3702950152594106e-05,
      "loss": 2.3154,
      "step": 32040
    },
    {
      "epoch": 16.30213631739573,
      "grad_norm": 27.75473976135254,
      "learning_rate": 3.3697863682604276e-05,
      "loss": 2.3947,
      "step": 32050
    },
    {
      "epoch": 16.307222787385555,
      "grad_norm": 23.45212173461914,
      "learning_rate": 3.3692777212614446e-05,
      "loss": 2.4073,
      "step": 32060
    },
    {
      "epoch": 16.312309257375382,
      "grad_norm": 32.46105194091797,
      "learning_rate": 3.368769074262462e-05,
      "loss": 2.447,
      "step": 32070
    },
    {
      "epoch": 16.31739572736521,
      "grad_norm": 35.340694427490234,
      "learning_rate": 3.368260427263479e-05,
      "loss": 2.4103,
      "step": 32080
    },
    {
      "epoch": 16.322482197355036,
      "grad_norm": 26.667343139648438,
      "learning_rate": 3.367751780264496e-05,
      "loss": 2.3846,
      "step": 32090
    },
    {
      "epoch": 16.327568667344863,
      "grad_norm": 32.7632942199707,
      "learning_rate": 3.367243133265514e-05,
      "loss": 2.3531,
      "step": 32100
    },
    {
      "epoch": 16.33265513733469,
      "grad_norm": 34.411319732666016,
      "learning_rate": 3.3667344862665315e-05,
      "loss": 2.2662,
      "step": 32110
    },
    {
      "epoch": 16.337741607324517,
      "grad_norm": 24.23623275756836,
      "learning_rate": 3.3662258392675485e-05,
      "loss": 2.3222,
      "step": 32120
    },
    {
      "epoch": 16.342828077314344,
      "grad_norm": 24.725252151489258,
      "learning_rate": 3.365717192268566e-05,
      "loss": 2.3731,
      "step": 32130
    },
    {
      "epoch": 16.34791454730417,
      "grad_norm": 37.896297454833984,
      "learning_rate": 3.365208545269583e-05,
      "loss": 2.3587,
      "step": 32140
    },
    {
      "epoch": 16.353001017293998,
      "grad_norm": 28.445478439331055,
      "learning_rate": 3.3646998982706e-05,
      "loss": 2.4373,
      "step": 32150
    },
    {
      "epoch": 16.358087487283825,
      "grad_norm": 27.8233585357666,
      "learning_rate": 3.364191251271618e-05,
      "loss": 2.3415,
      "step": 32160
    },
    {
      "epoch": 16.363173957273652,
      "grad_norm": 25.4482364654541,
      "learning_rate": 3.363682604272635e-05,
      "loss": 2.5095,
      "step": 32170
    },
    {
      "epoch": 16.36826042726348,
      "grad_norm": 25.88382911682129,
      "learning_rate": 3.363173957273652e-05,
      "loss": 2.4318,
      "step": 32180
    },
    {
      "epoch": 16.373346897253306,
      "grad_norm": 26.143665313720703,
      "learning_rate": 3.3626653102746695e-05,
      "loss": 2.3021,
      "step": 32190
    },
    {
      "epoch": 16.378433367243133,
      "grad_norm": 26.991811752319336,
      "learning_rate": 3.3621566632756865e-05,
      "loss": 2.3816,
      "step": 32200
    },
    {
      "epoch": 16.38351983723296,
      "grad_norm": 29.968624114990234,
      "learning_rate": 3.361648016276704e-05,
      "loss": 2.3375,
      "step": 32210
    },
    {
      "epoch": 16.388606307222787,
      "grad_norm": 25.724397659301758,
      "learning_rate": 3.361139369277722e-05,
      "loss": 2.3811,
      "step": 32220
    },
    {
      "epoch": 16.393692777212614,
      "grad_norm": 39.274383544921875,
      "learning_rate": 3.360630722278739e-05,
      "loss": 2.3752,
      "step": 32230
    },
    {
      "epoch": 16.39877924720244,
      "grad_norm": 30.92546844482422,
      "learning_rate": 3.360122075279756e-05,
      "loss": 2.3553,
      "step": 32240
    },
    {
      "epoch": 16.403865717192268,
      "grad_norm": 24.863670349121094,
      "learning_rate": 3.3596134282807735e-05,
      "loss": 2.3647,
      "step": 32250
    },
    {
      "epoch": 16.408952187182095,
      "grad_norm": 27.268564224243164,
      "learning_rate": 3.3591047812817904e-05,
      "loss": 2.446,
      "step": 32260
    },
    {
      "epoch": 16.414038657171922,
      "grad_norm": 23.51303482055664,
      "learning_rate": 3.3585961342828074e-05,
      "loss": 2.4171,
      "step": 32270
    },
    {
      "epoch": 16.41912512716175,
      "grad_norm": 27.501426696777344,
      "learning_rate": 3.358087487283825e-05,
      "loss": 2.318,
      "step": 32280
    },
    {
      "epoch": 16.424211597151576,
      "grad_norm": 30.186071395874023,
      "learning_rate": 3.357578840284842e-05,
      "loss": 2.3016,
      "step": 32290
    },
    {
      "epoch": 16.429298067141403,
      "grad_norm": 29.59772491455078,
      "learning_rate": 3.35707019328586e-05,
      "loss": 2.3612,
      "step": 32300
    },
    {
      "epoch": 16.43438453713123,
      "grad_norm": 32.56248474121094,
      "learning_rate": 3.3565615462868774e-05,
      "loss": 2.3901,
      "step": 32310
    },
    {
      "epoch": 16.439471007121057,
      "grad_norm": 26.64494514465332,
      "learning_rate": 3.3560528992878944e-05,
      "loss": 2.3926,
      "step": 32320
    },
    {
      "epoch": 16.444557477110884,
      "grad_norm": 24.107072830200195,
      "learning_rate": 3.355544252288912e-05,
      "loss": 2.3773,
      "step": 32330
    },
    {
      "epoch": 16.44964394710071,
      "grad_norm": 30.328964233398438,
      "learning_rate": 3.355035605289929e-05,
      "loss": 2.4413,
      "step": 32340
    },
    {
      "epoch": 16.454730417090538,
      "grad_norm": 26.207660675048828,
      "learning_rate": 3.354526958290946e-05,
      "loss": 2.2769,
      "step": 32350
    },
    {
      "epoch": 16.459816887080365,
      "grad_norm": 26.00009536743164,
      "learning_rate": 3.354018311291964e-05,
      "loss": 2.3308,
      "step": 32360
    },
    {
      "epoch": 16.46490335707019,
      "grad_norm": 32.96688461303711,
      "learning_rate": 3.353509664292981e-05,
      "loss": 2.3491,
      "step": 32370
    },
    {
      "epoch": 16.46998982706002,
      "grad_norm": 35.501155853271484,
      "learning_rate": 3.353001017293998e-05,
      "loss": 2.5038,
      "step": 32380
    },
    {
      "epoch": 16.475076297049846,
      "grad_norm": 25.122556686401367,
      "learning_rate": 3.3524923702950154e-05,
      "loss": 2.4412,
      "step": 32390
    },
    {
      "epoch": 16.480162767039676,
      "grad_norm": 20.651371002197266,
      "learning_rate": 3.351983723296033e-05,
      "loss": 2.433,
      "step": 32400
    },
    {
      "epoch": 16.485249237029503,
      "grad_norm": 27.580808639526367,
      "learning_rate": 3.35147507629705e-05,
      "loss": 2.3491,
      "step": 32410
    },
    {
      "epoch": 16.49033570701933,
      "grad_norm": 26.889699935913086,
      "learning_rate": 3.350966429298068e-05,
      "loss": 2.3512,
      "step": 32420
    },
    {
      "epoch": 16.495422177009157,
      "grad_norm": 26.95343017578125,
      "learning_rate": 3.350457782299085e-05,
      "loss": 2.4158,
      "step": 32430
    },
    {
      "epoch": 16.500508646998984,
      "grad_norm": 25.51703643798828,
      "learning_rate": 3.349949135300102e-05,
      "loss": 2.453,
      "step": 32440
    },
    {
      "epoch": 16.50559511698881,
      "grad_norm": 30.109567642211914,
      "learning_rate": 3.3494404883011193e-05,
      "loss": 2.3363,
      "step": 32450
    },
    {
      "epoch": 16.510681586978638,
      "grad_norm": 25.92938804626465,
      "learning_rate": 3.348931841302136e-05,
      "loss": 2.3654,
      "step": 32460
    },
    {
      "epoch": 16.515768056968465,
      "grad_norm": 24.228649139404297,
      "learning_rate": 3.348423194303153e-05,
      "loss": 2.3983,
      "step": 32470
    },
    {
      "epoch": 16.520854526958292,
      "grad_norm": 29.11939811706543,
      "learning_rate": 3.347914547304171e-05,
      "loss": 2.33,
      "step": 32480
    },
    {
      "epoch": 16.52594099694812,
      "grad_norm": 30.700286865234375,
      "learning_rate": 3.347405900305188e-05,
      "loss": 2.3435,
      "step": 32490
    },
    {
      "epoch": 16.531027466937946,
      "grad_norm": 29.79747200012207,
      "learning_rate": 3.3468972533062056e-05,
      "loss": 2.3816,
      "step": 32500
    },
    {
      "epoch": 16.536113936927773,
      "grad_norm": 27.581396102905273,
      "learning_rate": 3.346388606307223e-05,
      "loss": 2.3991,
      "step": 32510
    },
    {
      "epoch": 16.5412004069176,
      "grad_norm": 21.856943130493164,
      "learning_rate": 3.34587995930824e-05,
      "loss": 2.3766,
      "step": 32520
    },
    {
      "epoch": 16.546286876907427,
      "grad_norm": 33.35432052612305,
      "learning_rate": 3.345371312309257e-05,
      "loss": 2.3623,
      "step": 32530
    },
    {
      "epoch": 16.551373346897254,
      "grad_norm": 26.545654296875,
      "learning_rate": 3.344862665310275e-05,
      "loss": 2.3338,
      "step": 32540
    },
    {
      "epoch": 16.55645981688708,
      "grad_norm": 28.542949676513672,
      "learning_rate": 3.344354018311292e-05,
      "loss": 2.338,
      "step": 32550
    },
    {
      "epoch": 16.561546286876908,
      "grad_norm": 34.843502044677734,
      "learning_rate": 3.3438453713123096e-05,
      "loss": 2.3534,
      "step": 32560
    },
    {
      "epoch": 16.566632756866735,
      "grad_norm": 25.007320404052734,
      "learning_rate": 3.3433367243133266e-05,
      "loss": 2.3782,
      "step": 32570
    },
    {
      "epoch": 16.571719226856562,
      "grad_norm": 26.005550384521484,
      "learning_rate": 3.3428280773143436e-05,
      "loss": 2.4344,
      "step": 32580
    },
    {
      "epoch": 16.57680569684639,
      "grad_norm": 23.79066276550293,
      "learning_rate": 3.342319430315361e-05,
      "loss": 2.31,
      "step": 32590
    },
    {
      "epoch": 16.581892166836216,
      "grad_norm": 29.89336585998535,
      "learning_rate": 3.341810783316379e-05,
      "loss": 2.4578,
      "step": 32600
    },
    {
      "epoch": 16.586978636826043,
      "grad_norm": 23.83551788330078,
      "learning_rate": 3.341302136317396e-05,
      "loss": 2.4551,
      "step": 32610
    },
    {
      "epoch": 16.59206510681587,
      "grad_norm": 30.713218688964844,
      "learning_rate": 3.3407934893184136e-05,
      "loss": 2.3568,
      "step": 32620
    },
    {
      "epoch": 16.597151576805697,
      "grad_norm": 28.606731414794922,
      "learning_rate": 3.3402848423194306e-05,
      "loss": 2.4482,
      "step": 32630
    },
    {
      "epoch": 16.602238046795524,
      "grad_norm": 28.250728607177734,
      "learning_rate": 3.3397761953204476e-05,
      "loss": 2.425,
      "step": 32640
    },
    {
      "epoch": 16.60732451678535,
      "grad_norm": 30.33182144165039,
      "learning_rate": 3.339267548321465e-05,
      "loss": 2.3675,
      "step": 32650
    },
    {
      "epoch": 16.612410986775178,
      "grad_norm": 31.93836784362793,
      "learning_rate": 3.338758901322482e-05,
      "loss": 2.4229,
      "step": 32660
    },
    {
      "epoch": 16.617497456765005,
      "grad_norm": 30.6135311126709,
      "learning_rate": 3.338250254323499e-05,
      "loss": 2.3022,
      "step": 32670
    },
    {
      "epoch": 16.62258392675483,
      "grad_norm": 32.32704162597656,
      "learning_rate": 3.337741607324517e-05,
      "loss": 2.4117,
      "step": 32680
    },
    {
      "epoch": 16.62767039674466,
      "grad_norm": 27.214250564575195,
      "learning_rate": 3.3372329603255345e-05,
      "loss": 2.3087,
      "step": 32690
    },
    {
      "epoch": 16.632756866734486,
      "grad_norm": 24.84925651550293,
      "learning_rate": 3.3367243133265515e-05,
      "loss": 2.4373,
      "step": 32700
    },
    {
      "epoch": 16.637843336724313,
      "grad_norm": 27.0092830657959,
      "learning_rate": 3.336215666327569e-05,
      "loss": 2.4579,
      "step": 32710
    },
    {
      "epoch": 16.64292980671414,
      "grad_norm": 34.220054626464844,
      "learning_rate": 3.335707019328586e-05,
      "loss": 2.3071,
      "step": 32720
    },
    {
      "epoch": 16.648016276703967,
      "grad_norm": 30.744565963745117,
      "learning_rate": 3.335198372329603e-05,
      "loss": 2.207,
      "step": 32730
    },
    {
      "epoch": 16.653102746693794,
      "grad_norm": 26.91023063659668,
      "learning_rate": 3.334689725330621e-05,
      "loss": 2.3468,
      "step": 32740
    },
    {
      "epoch": 16.65818921668362,
      "grad_norm": 36.84416580200195,
      "learning_rate": 3.334181078331638e-05,
      "loss": 2.3922,
      "step": 32750
    },
    {
      "epoch": 16.663275686673447,
      "grad_norm": 23.011215209960938,
      "learning_rate": 3.333672431332655e-05,
      "loss": 2.3373,
      "step": 32760
    },
    {
      "epoch": 16.668362156663274,
      "grad_norm": 30.314441680908203,
      "learning_rate": 3.3331637843336725e-05,
      "loss": 2.378,
      "step": 32770
    },
    {
      "epoch": 16.6734486266531,
      "grad_norm": 30.48365592956543,
      "learning_rate": 3.3326551373346895e-05,
      "loss": 2.4137,
      "step": 32780
    },
    {
      "epoch": 16.67853509664293,
      "grad_norm": 34.97801208496094,
      "learning_rate": 3.332146490335707e-05,
      "loss": 2.4164,
      "step": 32790
    },
    {
      "epoch": 16.683621566632755,
      "grad_norm": 35.554176330566406,
      "learning_rate": 3.331637843336725e-05,
      "loss": 2.3714,
      "step": 32800
    },
    {
      "epoch": 16.688708036622582,
      "grad_norm": 24.622983932495117,
      "learning_rate": 3.331129196337742e-05,
      "loss": 2.3539,
      "step": 32810
    },
    {
      "epoch": 16.69379450661241,
      "grad_norm": 24.740747451782227,
      "learning_rate": 3.3306205493387595e-05,
      "loss": 2.2565,
      "step": 32820
    },
    {
      "epoch": 16.698880976602236,
      "grad_norm": 28.084550857543945,
      "learning_rate": 3.3301119023397765e-05,
      "loss": 2.3525,
      "step": 32830
    },
    {
      "epoch": 16.703967446592067,
      "grad_norm": 23.502878189086914,
      "learning_rate": 3.3296032553407934e-05,
      "loss": 2.3196,
      "step": 32840
    },
    {
      "epoch": 16.709053916581894,
      "grad_norm": 25.7933406829834,
      "learning_rate": 3.329094608341811e-05,
      "loss": 2.4428,
      "step": 32850
    },
    {
      "epoch": 16.71414038657172,
      "grad_norm": 33.17264938354492,
      "learning_rate": 3.328585961342828e-05,
      "loss": 2.3905,
      "step": 32860
    },
    {
      "epoch": 16.719226856561548,
      "grad_norm": 32.34481430053711,
      "learning_rate": 3.328077314343845e-05,
      "loss": 2.2916,
      "step": 32870
    },
    {
      "epoch": 16.724313326551375,
      "grad_norm": 25.154983520507812,
      "learning_rate": 3.327568667344863e-05,
      "loss": 2.332,
      "step": 32880
    },
    {
      "epoch": 16.729399796541202,
      "grad_norm": 27.45275115966797,
      "learning_rate": 3.3270600203458804e-05,
      "loss": 2.3406,
      "step": 32890
    },
    {
      "epoch": 16.73448626653103,
      "grad_norm": 33.3878288269043,
      "learning_rate": 3.3265513733468974e-05,
      "loss": 2.3787,
      "step": 32900
    },
    {
      "epoch": 16.739572736520856,
      "grad_norm": 28.12668800354004,
      "learning_rate": 3.326042726347915e-05,
      "loss": 2.3534,
      "step": 32910
    },
    {
      "epoch": 16.744659206510683,
      "grad_norm": 32.58884048461914,
      "learning_rate": 3.325534079348932e-05,
      "loss": 2.3568,
      "step": 32920
    },
    {
      "epoch": 16.74974567650051,
      "grad_norm": 24.641244888305664,
      "learning_rate": 3.325025432349949e-05,
      "loss": 2.3685,
      "step": 32930
    },
    {
      "epoch": 16.754832146490337,
      "grad_norm": 28.978614807128906,
      "learning_rate": 3.324516785350967e-05,
      "loss": 2.4057,
      "step": 32940
    },
    {
      "epoch": 16.759918616480164,
      "grad_norm": 22.968448638916016,
      "learning_rate": 3.324008138351984e-05,
      "loss": 2.2827,
      "step": 32950
    },
    {
      "epoch": 16.76500508646999,
      "grad_norm": 26.65904998779297,
      "learning_rate": 3.323499491353001e-05,
      "loss": 2.4622,
      "step": 32960
    },
    {
      "epoch": 16.770091556459818,
      "grad_norm": 29.65627670288086,
      "learning_rate": 3.3229908443540184e-05,
      "loss": 2.4052,
      "step": 32970
    },
    {
      "epoch": 16.775178026449645,
      "grad_norm": 35.04934310913086,
      "learning_rate": 3.322482197355036e-05,
      "loss": 2.3618,
      "step": 32980
    },
    {
      "epoch": 16.78026449643947,
      "grad_norm": 29.330781936645508,
      "learning_rate": 3.321973550356053e-05,
      "loss": 2.3131,
      "step": 32990
    },
    {
      "epoch": 16.7853509664293,
      "grad_norm": 27.538137435913086,
      "learning_rate": 3.321464903357071e-05,
      "loss": 2.3123,
      "step": 33000
    },
    {
      "epoch": 16.790437436419126,
      "grad_norm": 33.16570281982422,
      "learning_rate": 3.320956256358088e-05,
      "loss": 2.3891,
      "step": 33010
    },
    {
      "epoch": 16.795523906408953,
      "grad_norm": 29.074005126953125,
      "learning_rate": 3.320447609359105e-05,
      "loss": 2.3637,
      "step": 33020
    },
    {
      "epoch": 16.80061037639878,
      "grad_norm": 26.83521270751953,
      "learning_rate": 3.3199389623601223e-05,
      "loss": 2.3476,
      "step": 33030
    },
    {
      "epoch": 16.805696846388607,
      "grad_norm": 26.238746643066406,
      "learning_rate": 3.319430315361139e-05,
      "loss": 2.3598,
      "step": 33040
    },
    {
      "epoch": 16.810783316378433,
      "grad_norm": 31.21303939819336,
      "learning_rate": 3.318921668362156e-05,
      "loss": 2.271,
      "step": 33050
    },
    {
      "epoch": 16.81586978636826,
      "grad_norm": 29.135398864746094,
      "learning_rate": 3.318413021363174e-05,
      "loss": 2.3303,
      "step": 33060
    },
    {
      "epoch": 16.820956256358087,
      "grad_norm": 25.19148826599121,
      "learning_rate": 3.3179043743641917e-05,
      "loss": 2.412,
      "step": 33070
    },
    {
      "epoch": 16.826042726347914,
      "grad_norm": 29.09592628479004,
      "learning_rate": 3.3173957273652086e-05,
      "loss": 2.3634,
      "step": 33080
    },
    {
      "epoch": 16.83112919633774,
      "grad_norm": 32.0401496887207,
      "learning_rate": 3.316887080366226e-05,
      "loss": 2.303,
      "step": 33090
    },
    {
      "epoch": 16.83621566632757,
      "grad_norm": 27.069013595581055,
      "learning_rate": 3.316378433367243e-05,
      "loss": 2.2782,
      "step": 33100
    },
    {
      "epoch": 16.841302136317395,
      "grad_norm": 30.938222885131836,
      "learning_rate": 3.315869786368261e-05,
      "loss": 2.4188,
      "step": 33110
    },
    {
      "epoch": 16.846388606307222,
      "grad_norm": 25.023963928222656,
      "learning_rate": 3.315361139369278e-05,
      "loss": 2.3924,
      "step": 33120
    },
    {
      "epoch": 16.85147507629705,
      "grad_norm": 37.872474670410156,
      "learning_rate": 3.314852492370295e-05,
      "loss": 2.3057,
      "step": 33130
    },
    {
      "epoch": 16.856561546286876,
      "grad_norm": 24.692012786865234,
      "learning_rate": 3.3143438453713126e-05,
      "loss": 2.4359,
      "step": 33140
    },
    {
      "epoch": 16.861648016276703,
      "grad_norm": 29.61428451538086,
      "learning_rate": 3.3138351983723296e-05,
      "loss": 2.3882,
      "step": 33150
    },
    {
      "epoch": 16.86673448626653,
      "grad_norm": 26.2061767578125,
      "learning_rate": 3.3133265513733466e-05,
      "loss": 2.4117,
      "step": 33160
    },
    {
      "epoch": 16.871820956256357,
      "grad_norm": 29.39246940612793,
      "learning_rate": 3.312817904374364e-05,
      "loss": 2.3511,
      "step": 33170
    },
    {
      "epoch": 16.876907426246184,
      "grad_norm": 30.232160568237305,
      "learning_rate": 3.312309257375382e-05,
      "loss": 2.3337,
      "step": 33180
    },
    {
      "epoch": 16.88199389623601,
      "grad_norm": 28.055818557739258,
      "learning_rate": 3.311800610376399e-05,
      "loss": 2.4025,
      "step": 33190
    },
    {
      "epoch": 16.887080366225838,
      "grad_norm": 35.12376403808594,
      "learning_rate": 3.3112919633774166e-05,
      "loss": 2.2811,
      "step": 33200
    },
    {
      "epoch": 16.892166836215665,
      "grad_norm": 29.077667236328125,
      "learning_rate": 3.3107833163784336e-05,
      "loss": 2.442,
      "step": 33210
    },
    {
      "epoch": 16.897253306205492,
      "grad_norm": 25.078577041625977,
      "learning_rate": 3.3102746693794506e-05,
      "loss": 2.38,
      "step": 33220
    },
    {
      "epoch": 16.90233977619532,
      "grad_norm": 23.802343368530273,
      "learning_rate": 3.309766022380468e-05,
      "loss": 2.4161,
      "step": 33230
    },
    {
      "epoch": 16.907426246185146,
      "grad_norm": 29.763290405273438,
      "learning_rate": 3.309257375381485e-05,
      "loss": 2.3449,
      "step": 33240
    },
    {
      "epoch": 16.912512716174973,
      "grad_norm": 25.955921173095703,
      "learning_rate": 3.308748728382502e-05,
      "loss": 2.4438,
      "step": 33250
    },
    {
      "epoch": 16.9175991861648,
      "grad_norm": 31.09093475341797,
      "learning_rate": 3.30824008138352e-05,
      "loss": 2.3331,
      "step": 33260
    },
    {
      "epoch": 16.922685656154627,
      "grad_norm": 26.92563247680664,
      "learning_rate": 3.3077314343845375e-05,
      "loss": 2.4273,
      "step": 33270
    },
    {
      "epoch": 16.927772126144454,
      "grad_norm": 28.210290908813477,
      "learning_rate": 3.3072227873855545e-05,
      "loss": 2.2995,
      "step": 33280
    },
    {
      "epoch": 16.93285859613428,
      "grad_norm": 30.954195022583008,
      "learning_rate": 3.306714140386572e-05,
      "loss": 2.3724,
      "step": 33290
    },
    {
      "epoch": 16.93794506612411,
      "grad_norm": 33.95633316040039,
      "learning_rate": 3.306205493387589e-05,
      "loss": 2.3686,
      "step": 33300
    },
    {
      "epoch": 16.94303153611394,
      "grad_norm": 35.265052795410156,
      "learning_rate": 3.305696846388606e-05,
      "loss": 2.3177,
      "step": 33310
    },
    {
      "epoch": 16.948118006103766,
      "grad_norm": 31.42696189880371,
      "learning_rate": 3.305188199389624e-05,
      "loss": 2.3493,
      "step": 33320
    },
    {
      "epoch": 16.953204476093592,
      "grad_norm": 29.62186622619629,
      "learning_rate": 3.304679552390641e-05,
      "loss": 2.4024,
      "step": 33330
    },
    {
      "epoch": 16.95829094608342,
      "grad_norm": 35.14799118041992,
      "learning_rate": 3.304170905391658e-05,
      "loss": 2.3508,
      "step": 33340
    },
    {
      "epoch": 16.963377416073246,
      "grad_norm": 26.006099700927734,
      "learning_rate": 3.3036622583926755e-05,
      "loss": 2.3919,
      "step": 33350
    },
    {
      "epoch": 16.968463886063073,
      "grad_norm": 27.49272346496582,
      "learning_rate": 3.303153611393693e-05,
      "loss": 2.388,
      "step": 33360
    },
    {
      "epoch": 16.9735503560529,
      "grad_norm": 30.36271095275879,
      "learning_rate": 3.302644964394711e-05,
      "loss": 2.3746,
      "step": 33370
    },
    {
      "epoch": 16.978636826042727,
      "grad_norm": 24.6755428314209,
      "learning_rate": 3.302136317395728e-05,
      "loss": 2.315,
      "step": 33380
    },
    {
      "epoch": 16.983723296032554,
      "grad_norm": 29.775470733642578,
      "learning_rate": 3.301627670396745e-05,
      "loss": 2.3564,
      "step": 33390
    },
    {
      "epoch": 16.98880976602238,
      "grad_norm": 26.98217010498047,
      "learning_rate": 3.3011190233977625e-05,
      "loss": 2.4661,
      "step": 33400
    },
    {
      "epoch": 16.99389623601221,
      "grad_norm": 30.666614532470703,
      "learning_rate": 3.3006103763987795e-05,
      "loss": 2.333,
      "step": 33410
    },
    {
      "epoch": 16.998982706002035,
      "grad_norm": 32.904170989990234,
      "learning_rate": 3.3001017293997964e-05,
      "loss": 2.3398,
      "step": 33420
    },
    {
      "epoch": 17.0,
      "eval_loss": 4.135850429534912,
      "eval_runtime": 2.7697,
      "eval_samples_per_second": 1001.921,
      "eval_steps_per_second": 125.285,
      "step": 33422
    },
    {
      "epoch": 17.004069175991862,
      "grad_norm": 29.667802810668945,
      "learning_rate": 3.299593082400814e-05,
      "loss": 2.3326,
      "step": 33430
    },
    {
      "epoch": 17.00915564598169,
      "grad_norm": 31.653202056884766,
      "learning_rate": 3.299084435401831e-05,
      "loss": 2.2331,
      "step": 33440
    },
    {
      "epoch": 17.014242115971516,
      "grad_norm": 34.73319625854492,
      "learning_rate": 3.298575788402848e-05,
      "loss": 2.219,
      "step": 33450
    },
    {
      "epoch": 17.019328585961343,
      "grad_norm": 25.65083885192871,
      "learning_rate": 3.298067141403866e-05,
      "loss": 2.3305,
      "step": 33460
    },
    {
      "epoch": 17.02441505595117,
      "grad_norm": 40.80827331542969,
      "learning_rate": 3.2975584944048834e-05,
      "loss": 2.2406,
      "step": 33470
    },
    {
      "epoch": 17.029501525940997,
      "grad_norm": 34.51754379272461,
      "learning_rate": 3.2970498474059004e-05,
      "loss": 2.2832,
      "step": 33480
    },
    {
      "epoch": 17.034587995930824,
      "grad_norm": 29.292221069335938,
      "learning_rate": 3.296541200406918e-05,
      "loss": 2.3711,
      "step": 33490
    },
    {
      "epoch": 17.03967446592065,
      "grad_norm": 29.756916046142578,
      "learning_rate": 3.296032553407935e-05,
      "loss": 2.3148,
      "step": 33500
    },
    {
      "epoch": 17.044760935910478,
      "grad_norm": 29.70798110961914,
      "learning_rate": 3.295523906408952e-05,
      "loss": 2.1856,
      "step": 33510
    },
    {
      "epoch": 17.049847405900305,
      "grad_norm": 31.232162475585938,
      "learning_rate": 3.29501525940997e-05,
      "loss": 2.2929,
      "step": 33520
    },
    {
      "epoch": 17.054933875890132,
      "grad_norm": 31.490217208862305,
      "learning_rate": 3.294506612410987e-05,
      "loss": 2.3918,
      "step": 33530
    },
    {
      "epoch": 17.06002034587996,
      "grad_norm": 33.35947036743164,
      "learning_rate": 3.293997965412004e-05,
      "loss": 2.3556,
      "step": 33540
    },
    {
      "epoch": 17.065106815869786,
      "grad_norm": 28.5191593170166,
      "learning_rate": 3.2934893184130214e-05,
      "loss": 2.2878,
      "step": 33550
    },
    {
      "epoch": 17.070193285859613,
      "grad_norm": 28.904739379882812,
      "learning_rate": 3.292980671414039e-05,
      "loss": 2.2856,
      "step": 33560
    },
    {
      "epoch": 17.07527975584944,
      "grad_norm": 45.79368209838867,
      "learning_rate": 3.292472024415056e-05,
      "loss": 2.2647,
      "step": 33570
    },
    {
      "epoch": 17.080366225839267,
      "grad_norm": 26.705524444580078,
      "learning_rate": 3.291963377416074e-05,
      "loss": 2.2761,
      "step": 33580
    },
    {
      "epoch": 17.085452695829094,
      "grad_norm": 28.897924423217773,
      "learning_rate": 3.291454730417091e-05,
      "loss": 2.3181,
      "step": 33590
    },
    {
      "epoch": 17.09053916581892,
      "grad_norm": 37.31569290161133,
      "learning_rate": 3.290946083418108e-05,
      "loss": 2.4427,
      "step": 33600
    },
    {
      "epoch": 17.095625635808748,
      "grad_norm": 26.154905319213867,
      "learning_rate": 3.2904374364191253e-05,
      "loss": 2.3623,
      "step": 33610
    },
    {
      "epoch": 17.100712105798575,
      "grad_norm": 25.17544937133789,
      "learning_rate": 3.289928789420142e-05,
      "loss": 2.2909,
      "step": 33620
    },
    {
      "epoch": 17.105798575788402,
      "grad_norm": 25.056047439575195,
      "learning_rate": 3.28942014242116e-05,
      "loss": 2.2695,
      "step": 33630
    },
    {
      "epoch": 17.11088504577823,
      "grad_norm": 28.401124954223633,
      "learning_rate": 3.288911495422177e-05,
      "loss": 2.3879,
      "step": 33640
    },
    {
      "epoch": 17.115971515768056,
      "grad_norm": 28.639015197753906,
      "learning_rate": 3.2884028484231947e-05,
      "loss": 2.3161,
      "step": 33650
    },
    {
      "epoch": 17.121057985757883,
      "grad_norm": 38.59193420410156,
      "learning_rate": 3.287894201424212e-05,
      "loss": 2.3175,
      "step": 33660
    },
    {
      "epoch": 17.12614445574771,
      "grad_norm": 25.18559455871582,
      "learning_rate": 3.287385554425229e-05,
      "loss": 2.3642,
      "step": 33670
    },
    {
      "epoch": 17.131230925737537,
      "grad_norm": 28.26870346069336,
      "learning_rate": 3.286876907426246e-05,
      "loss": 2.3283,
      "step": 33680
    },
    {
      "epoch": 17.136317395727364,
      "grad_norm": 35.8792724609375,
      "learning_rate": 3.286368260427264e-05,
      "loss": 2.3747,
      "step": 33690
    },
    {
      "epoch": 17.14140386571719,
      "grad_norm": 32.5461540222168,
      "learning_rate": 3.285859613428281e-05,
      "loss": 2.3897,
      "step": 33700
    },
    {
      "epoch": 17.146490335707018,
      "grad_norm": 28.48804473876953,
      "learning_rate": 3.285350966429298e-05,
      "loss": 2.2997,
      "step": 33710
    },
    {
      "epoch": 17.151576805696845,
      "grad_norm": 32.068931579589844,
      "learning_rate": 3.2848423194303156e-05,
      "loss": 2.4003,
      "step": 33720
    },
    {
      "epoch": 17.15666327568667,
      "grad_norm": 31.025449752807617,
      "learning_rate": 3.2843336724313326e-05,
      "loss": 2.3564,
      "step": 33730
    },
    {
      "epoch": 17.161749745676502,
      "grad_norm": 35.550228118896484,
      "learning_rate": 3.2838250254323496e-05,
      "loss": 2.2368,
      "step": 33740
    },
    {
      "epoch": 17.16683621566633,
      "grad_norm": 22.49515724182129,
      "learning_rate": 3.283316378433367e-05,
      "loss": 2.3849,
      "step": 33750
    },
    {
      "epoch": 17.171922685656156,
      "grad_norm": 34.46742630004883,
      "learning_rate": 3.282807731434385e-05,
      "loss": 2.3017,
      "step": 33760
    },
    {
      "epoch": 17.177009155645983,
      "grad_norm": 26.874162673950195,
      "learning_rate": 3.282299084435402e-05,
      "loss": 2.343,
      "step": 33770
    },
    {
      "epoch": 17.18209562563581,
      "grad_norm": 27.254470825195312,
      "learning_rate": 3.2817904374364196e-05,
      "loss": 2.3203,
      "step": 33780
    },
    {
      "epoch": 17.187182095625637,
      "grad_norm": 27.463134765625,
      "learning_rate": 3.2812817904374366e-05,
      "loss": 2.233,
      "step": 33790
    },
    {
      "epoch": 17.192268565615464,
      "grad_norm": 35.17401123046875,
      "learning_rate": 3.2807731434384536e-05,
      "loss": 2.4092,
      "step": 33800
    },
    {
      "epoch": 17.19735503560529,
      "grad_norm": 28.071868896484375,
      "learning_rate": 3.280264496439471e-05,
      "loss": 2.3915,
      "step": 33810
    },
    {
      "epoch": 17.202441505595118,
      "grad_norm": 28.732952117919922,
      "learning_rate": 3.279755849440488e-05,
      "loss": 2.2997,
      "step": 33820
    },
    {
      "epoch": 17.207527975584945,
      "grad_norm": 26.89963150024414,
      "learning_rate": 3.279247202441505e-05,
      "loss": 2.3766,
      "step": 33830
    },
    {
      "epoch": 17.212614445574772,
      "grad_norm": 31.549198150634766,
      "learning_rate": 3.278738555442523e-05,
      "loss": 2.3237,
      "step": 33840
    },
    {
      "epoch": 17.2177009155646,
      "grad_norm": 27.667499542236328,
      "learning_rate": 3.2782299084435405e-05,
      "loss": 2.3563,
      "step": 33850
    },
    {
      "epoch": 17.222787385554426,
      "grad_norm": 24.668725967407227,
      "learning_rate": 3.2777212614445575e-05,
      "loss": 2.3195,
      "step": 33860
    },
    {
      "epoch": 17.227873855544253,
      "grad_norm": 27.715456008911133,
      "learning_rate": 3.277212614445575e-05,
      "loss": 2.3963,
      "step": 33870
    },
    {
      "epoch": 17.23296032553408,
      "grad_norm": 32.86091613769531,
      "learning_rate": 3.276703967446592e-05,
      "loss": 2.3031,
      "step": 33880
    },
    {
      "epoch": 17.238046795523907,
      "grad_norm": 34.76339340209961,
      "learning_rate": 3.276195320447609e-05,
      "loss": 2.3714,
      "step": 33890
    },
    {
      "epoch": 17.243133265513734,
      "grad_norm": 33.2691764831543,
      "learning_rate": 3.275686673448627e-05,
      "loss": 2.4735,
      "step": 33900
    },
    {
      "epoch": 17.24821973550356,
      "grad_norm": 22.84650421142578,
      "learning_rate": 3.275178026449644e-05,
      "loss": 2.251,
      "step": 33910
    },
    {
      "epoch": 17.253306205493388,
      "grad_norm": 30.12354278564453,
      "learning_rate": 3.2746693794506615e-05,
      "loss": 2.3023,
      "step": 33920
    },
    {
      "epoch": 17.258392675483215,
      "grad_norm": 31.388891220092773,
      "learning_rate": 3.2741607324516785e-05,
      "loss": 2.2441,
      "step": 33930
    },
    {
      "epoch": 17.263479145473042,
      "grad_norm": 25.835865020751953,
      "learning_rate": 3.273652085452696e-05,
      "loss": 2.322,
      "step": 33940
    },
    {
      "epoch": 17.26856561546287,
      "grad_norm": 33.93967056274414,
      "learning_rate": 3.273143438453714e-05,
      "loss": 2.3121,
      "step": 33950
    },
    {
      "epoch": 17.273652085452696,
      "grad_norm": 27.33873176574707,
      "learning_rate": 3.272634791454731e-05,
      "loss": 2.2895,
      "step": 33960
    },
    {
      "epoch": 17.278738555442523,
      "grad_norm": 27.313312530517578,
      "learning_rate": 3.272126144455748e-05,
      "loss": 2.3044,
      "step": 33970
    },
    {
      "epoch": 17.28382502543235,
      "grad_norm": 29.268583297729492,
      "learning_rate": 3.2716174974567655e-05,
      "loss": 2.3844,
      "step": 33980
    },
    {
      "epoch": 17.288911495422177,
      "grad_norm": 27.89171028137207,
      "learning_rate": 3.2711088504577825e-05,
      "loss": 2.3878,
      "step": 33990
    },
    {
      "epoch": 17.293997965412004,
      "grad_norm": 30.02368927001953,
      "learning_rate": 3.2706002034587994e-05,
      "loss": 2.3388,
      "step": 34000
    },
    {
      "epoch": 17.29908443540183,
      "grad_norm": 31.756757736206055,
      "learning_rate": 3.270091556459817e-05,
      "loss": 2.2987,
      "step": 34010
    },
    {
      "epoch": 17.304170905391658,
      "grad_norm": 38.42860794067383,
      "learning_rate": 3.269582909460834e-05,
      "loss": 2.3816,
      "step": 34020
    },
    {
      "epoch": 17.309257375381485,
      "grad_norm": 30.301713943481445,
      "learning_rate": 3.269074262461852e-05,
      "loss": 2.251,
      "step": 34030
    },
    {
      "epoch": 17.31434384537131,
      "grad_norm": 26.91492462158203,
      "learning_rate": 3.2685656154628694e-05,
      "loss": 2.3201,
      "step": 34040
    },
    {
      "epoch": 17.31943031536114,
      "grad_norm": 26.814069747924805,
      "learning_rate": 3.2680569684638864e-05,
      "loss": 2.434,
      "step": 34050
    },
    {
      "epoch": 17.324516785350966,
      "grad_norm": 33.03962707519531,
      "learning_rate": 3.2675483214649034e-05,
      "loss": 2.3266,
      "step": 34060
    },
    {
      "epoch": 17.329603255340793,
      "grad_norm": 29.171253204345703,
      "learning_rate": 3.267039674465921e-05,
      "loss": 2.3376,
      "step": 34070
    },
    {
      "epoch": 17.33468972533062,
      "grad_norm": 31.570281982421875,
      "learning_rate": 3.266531027466938e-05,
      "loss": 2.3071,
      "step": 34080
    },
    {
      "epoch": 17.339776195320447,
      "grad_norm": 28.692359924316406,
      "learning_rate": 3.266022380467955e-05,
      "loss": 2.2955,
      "step": 34090
    },
    {
      "epoch": 17.344862665310274,
      "grad_norm": 27.799644470214844,
      "learning_rate": 3.265513733468973e-05,
      "loss": 2.3289,
      "step": 34100
    },
    {
      "epoch": 17.3499491353001,
      "grad_norm": 30.726659774780273,
      "learning_rate": 3.26500508646999e-05,
      "loss": 2.2761,
      "step": 34110
    },
    {
      "epoch": 17.355035605289928,
      "grad_norm": 25.663175582885742,
      "learning_rate": 3.264496439471007e-05,
      "loss": 2.4013,
      "step": 34120
    },
    {
      "epoch": 17.360122075279754,
      "grad_norm": 36.38714599609375,
      "learning_rate": 3.2639877924720244e-05,
      "loss": 2.3265,
      "step": 34130
    },
    {
      "epoch": 17.36520854526958,
      "grad_norm": 24.052120208740234,
      "learning_rate": 3.263479145473042e-05,
      "loss": 2.3888,
      "step": 34140
    },
    {
      "epoch": 17.37029501525941,
      "grad_norm": 30.440210342407227,
      "learning_rate": 3.262970498474059e-05,
      "loss": 2.2857,
      "step": 34150
    },
    {
      "epoch": 17.375381485249235,
      "grad_norm": 27.165239334106445,
      "learning_rate": 3.262461851475077e-05,
      "loss": 2.3025,
      "step": 34160
    },
    {
      "epoch": 17.380467955239062,
      "grad_norm": 28.879108428955078,
      "learning_rate": 3.261953204476094e-05,
      "loss": 2.3015,
      "step": 34170
    },
    {
      "epoch": 17.38555442522889,
      "grad_norm": 32.34967803955078,
      "learning_rate": 3.2614445574771114e-05,
      "loss": 2.2809,
      "step": 34180
    },
    {
      "epoch": 17.39064089521872,
      "grad_norm": 36.52687072753906,
      "learning_rate": 3.2609359104781283e-05,
      "loss": 2.2827,
      "step": 34190
    },
    {
      "epoch": 17.395727365208547,
      "grad_norm": 28.88563346862793,
      "learning_rate": 3.260427263479145e-05,
      "loss": 2.4122,
      "step": 34200
    },
    {
      "epoch": 17.400813835198374,
      "grad_norm": 32.71400833129883,
      "learning_rate": 3.259918616480163e-05,
      "loss": 2.2462,
      "step": 34210
    },
    {
      "epoch": 17.4059003051882,
      "grad_norm": 27.713747024536133,
      "learning_rate": 3.25940996948118e-05,
      "loss": 2.3265,
      "step": 34220
    },
    {
      "epoch": 17.410986775178028,
      "grad_norm": 24.76177406311035,
      "learning_rate": 3.2589013224821977e-05,
      "loss": 2.2998,
      "step": 34230
    },
    {
      "epoch": 17.416073245167855,
      "grad_norm": 27.339706420898438,
      "learning_rate": 3.258392675483215e-05,
      "loss": 2.2115,
      "step": 34240
    },
    {
      "epoch": 17.421159715157682,
      "grad_norm": 35.06245040893555,
      "learning_rate": 3.257884028484232e-05,
      "loss": 2.1463,
      "step": 34250
    },
    {
      "epoch": 17.42624618514751,
      "grad_norm": 30.248382568359375,
      "learning_rate": 3.257375381485249e-05,
      "loss": 2.2981,
      "step": 34260
    },
    {
      "epoch": 17.431332655137336,
      "grad_norm": 22.887704849243164,
      "learning_rate": 3.256866734486267e-05,
      "loss": 2.4063,
      "step": 34270
    },
    {
      "epoch": 17.436419125127163,
      "grad_norm": 30.359418869018555,
      "learning_rate": 3.256358087487284e-05,
      "loss": 2.3111,
      "step": 34280
    },
    {
      "epoch": 17.44150559511699,
      "grad_norm": 31.380807876586914,
      "learning_rate": 3.255849440488301e-05,
      "loss": 2.321,
      "step": 34290
    },
    {
      "epoch": 17.446592065106817,
      "grad_norm": 29.74623680114746,
      "learning_rate": 3.2553407934893186e-05,
      "loss": 2.3288,
      "step": 34300
    },
    {
      "epoch": 17.451678535096644,
      "grad_norm": 26.066614151000977,
      "learning_rate": 3.2548321464903356e-05,
      "loss": 2.222,
      "step": 34310
    },
    {
      "epoch": 17.45676500508647,
      "grad_norm": 30.064069747924805,
      "learning_rate": 3.254323499491353e-05,
      "loss": 2.2824,
      "step": 34320
    },
    {
      "epoch": 17.461851475076298,
      "grad_norm": 29.327680587768555,
      "learning_rate": 3.253814852492371e-05,
      "loss": 2.4055,
      "step": 34330
    },
    {
      "epoch": 17.466937945066125,
      "grad_norm": 25.550630569458008,
      "learning_rate": 3.253306205493388e-05,
      "loss": 2.2031,
      "step": 34340
    },
    {
      "epoch": 17.47202441505595,
      "grad_norm": 28.132776260375977,
      "learning_rate": 3.252797558494405e-05,
      "loss": 2.3381,
      "step": 34350
    },
    {
      "epoch": 17.47711088504578,
      "grad_norm": 27.771528244018555,
      "learning_rate": 3.2522889114954226e-05,
      "loss": 2.3119,
      "step": 34360
    },
    {
      "epoch": 17.482197355035606,
      "grad_norm": 31.02239418029785,
      "learning_rate": 3.2517802644964396e-05,
      "loss": 2.2347,
      "step": 34370
    },
    {
      "epoch": 17.487283825025433,
      "grad_norm": 28.913808822631836,
      "learning_rate": 3.2512716174974566e-05,
      "loss": 2.2617,
      "step": 34380
    },
    {
      "epoch": 17.49237029501526,
      "grad_norm": 31.53792953491211,
      "learning_rate": 3.250762970498474e-05,
      "loss": 2.2962,
      "step": 34390
    },
    {
      "epoch": 17.497456765005087,
      "grad_norm": 25.0299129486084,
      "learning_rate": 3.250254323499491e-05,
      "loss": 2.3644,
      "step": 34400
    },
    {
      "epoch": 17.502543234994913,
      "grad_norm": 39.915924072265625,
      "learning_rate": 3.249745676500508e-05,
      "loss": 2.2698,
      "step": 34410
    },
    {
      "epoch": 17.50762970498474,
      "grad_norm": 45.56943130493164,
      "learning_rate": 3.249237029501526e-05,
      "loss": 2.333,
      "step": 34420
    },
    {
      "epoch": 17.512716174974567,
      "grad_norm": 28.684616088867188,
      "learning_rate": 3.2487283825025435e-05,
      "loss": 2.3441,
      "step": 34430
    },
    {
      "epoch": 17.517802644964394,
      "grad_norm": 34.78679656982422,
      "learning_rate": 3.248219735503561e-05,
      "loss": 2.3907,
      "step": 34440
    },
    {
      "epoch": 17.52288911495422,
      "grad_norm": 32.91901397705078,
      "learning_rate": 3.247711088504578e-05,
      "loss": 2.2062,
      "step": 34450
    },
    {
      "epoch": 17.52797558494405,
      "grad_norm": 27.860292434692383,
      "learning_rate": 3.247202441505595e-05,
      "loss": 2.2485,
      "step": 34460
    },
    {
      "epoch": 17.533062054933875,
      "grad_norm": 27.338760375976562,
      "learning_rate": 3.246693794506613e-05,
      "loss": 2.3803,
      "step": 34470
    },
    {
      "epoch": 17.538148524923702,
      "grad_norm": 34.9934196472168,
      "learning_rate": 3.24618514750763e-05,
      "loss": 2.3463,
      "step": 34480
    },
    {
      "epoch": 17.54323499491353,
      "grad_norm": 30.292341232299805,
      "learning_rate": 3.245676500508647e-05,
      "loss": 2.4059,
      "step": 34490
    },
    {
      "epoch": 17.548321464903356,
      "grad_norm": 30.337188720703125,
      "learning_rate": 3.2451678535096645e-05,
      "loss": 2.3069,
      "step": 34500
    },
    {
      "epoch": 17.553407934893183,
      "grad_norm": 26.047452926635742,
      "learning_rate": 3.2446592065106815e-05,
      "loss": 2.2487,
      "step": 34510
    },
    {
      "epoch": 17.55849440488301,
      "grad_norm": 31.220006942749023,
      "learning_rate": 3.244150559511699e-05,
      "loss": 2.3129,
      "step": 34520
    },
    {
      "epoch": 17.563580874872837,
      "grad_norm": 24.093992233276367,
      "learning_rate": 3.243641912512717e-05,
      "loss": 2.3493,
      "step": 34530
    },
    {
      "epoch": 17.568667344862664,
      "grad_norm": 26.18400764465332,
      "learning_rate": 3.243133265513734e-05,
      "loss": 2.3569,
      "step": 34540
    },
    {
      "epoch": 17.57375381485249,
      "grad_norm": 29.888776779174805,
      "learning_rate": 3.242624618514751e-05,
      "loss": 2.252,
      "step": 34550
    },
    {
      "epoch": 17.578840284842318,
      "grad_norm": 29.03826904296875,
      "learning_rate": 3.2421159715157685e-05,
      "loss": 2.3349,
      "step": 34560
    },
    {
      "epoch": 17.583926754832145,
      "grad_norm": 33.106998443603516,
      "learning_rate": 3.2416073245167855e-05,
      "loss": 2.4031,
      "step": 34570
    },
    {
      "epoch": 17.589013224821972,
      "grad_norm": 27.158912658691406,
      "learning_rate": 3.2410986775178024e-05,
      "loss": 2.2397,
      "step": 34580
    },
    {
      "epoch": 17.5940996948118,
      "grad_norm": 31.782373428344727,
      "learning_rate": 3.24059003051882e-05,
      "loss": 2.4058,
      "step": 34590
    },
    {
      "epoch": 17.599186164801626,
      "grad_norm": 32.01353073120117,
      "learning_rate": 3.240081383519837e-05,
      "loss": 2.3187,
      "step": 34600
    },
    {
      "epoch": 17.604272634791453,
      "grad_norm": 39.186519622802734,
      "learning_rate": 3.239572736520855e-05,
      "loss": 2.2414,
      "step": 34610
    },
    {
      "epoch": 17.60935910478128,
      "grad_norm": 32.609535217285156,
      "learning_rate": 3.2390640895218724e-05,
      "loss": 2.3157,
      "step": 34620
    },
    {
      "epoch": 17.61444557477111,
      "grad_norm": 38.03860092163086,
      "learning_rate": 3.2385554425228894e-05,
      "loss": 2.3395,
      "step": 34630
    },
    {
      "epoch": 17.619532044760938,
      "grad_norm": 28.269145965576172,
      "learning_rate": 3.2380467955239064e-05,
      "loss": 2.2699,
      "step": 34640
    },
    {
      "epoch": 17.624618514750765,
      "grad_norm": 27.025226593017578,
      "learning_rate": 3.237538148524924e-05,
      "loss": 2.34,
      "step": 34650
    },
    {
      "epoch": 17.62970498474059,
      "grad_norm": 29.806976318359375,
      "learning_rate": 3.237029501525941e-05,
      "loss": 2.319,
      "step": 34660
    },
    {
      "epoch": 17.63479145473042,
      "grad_norm": 27.62681770324707,
      "learning_rate": 3.236520854526958e-05,
      "loss": 2.4054,
      "step": 34670
    },
    {
      "epoch": 17.639877924720246,
      "grad_norm": 26.626543045043945,
      "learning_rate": 3.236012207527976e-05,
      "loss": 2.3196,
      "step": 34680
    },
    {
      "epoch": 17.644964394710072,
      "grad_norm": 31.941730499267578,
      "learning_rate": 3.235503560528993e-05,
      "loss": 2.3119,
      "step": 34690
    },
    {
      "epoch": 17.6500508646999,
      "grad_norm": 28.890220642089844,
      "learning_rate": 3.2349949135300104e-05,
      "loss": 2.2454,
      "step": 34700
    },
    {
      "epoch": 17.655137334689726,
      "grad_norm": 43.289093017578125,
      "learning_rate": 3.2344862665310274e-05,
      "loss": 2.2498,
      "step": 34710
    },
    {
      "epoch": 17.660223804679553,
      "grad_norm": 35.469329833984375,
      "learning_rate": 3.233977619532045e-05,
      "loss": 2.3401,
      "step": 34720
    },
    {
      "epoch": 17.66531027466938,
      "grad_norm": 32.22002029418945,
      "learning_rate": 3.233468972533063e-05,
      "loss": 2.3649,
      "step": 34730
    },
    {
      "epoch": 17.670396744659207,
      "grad_norm": 31.376821517944336,
      "learning_rate": 3.23296032553408e-05,
      "loss": 2.2422,
      "step": 34740
    },
    {
      "epoch": 17.675483214649034,
      "grad_norm": 29.356456756591797,
      "learning_rate": 3.232451678535097e-05,
      "loss": 2.3259,
      "step": 34750
    },
    {
      "epoch": 17.68056968463886,
      "grad_norm": 28.30843734741211,
      "learning_rate": 3.2319430315361144e-05,
      "loss": 2.202,
      "step": 34760
    },
    {
      "epoch": 17.68565615462869,
      "grad_norm": 29.545339584350586,
      "learning_rate": 3.2314343845371313e-05,
      "loss": 2.3003,
      "step": 34770
    },
    {
      "epoch": 17.690742624618515,
      "grad_norm": 37.08954620361328,
      "learning_rate": 3.230925737538148e-05,
      "loss": 2.3355,
      "step": 34780
    },
    {
      "epoch": 17.695829094608342,
      "grad_norm": 29.445526123046875,
      "learning_rate": 3.230417090539166e-05,
      "loss": 2.342,
      "step": 34790
    },
    {
      "epoch": 17.70091556459817,
      "grad_norm": 27.890365600585938,
      "learning_rate": 3.229908443540183e-05,
      "loss": 2.2939,
      "step": 34800
    },
    {
      "epoch": 17.706002034587996,
      "grad_norm": 25.671274185180664,
      "learning_rate": 3.2293997965412007e-05,
      "loss": 2.3605,
      "step": 34810
    },
    {
      "epoch": 17.711088504577823,
      "grad_norm": 27.787189483642578,
      "learning_rate": 3.228891149542218e-05,
      "loss": 2.2863,
      "step": 34820
    },
    {
      "epoch": 17.71617497456765,
      "grad_norm": 27.995344161987305,
      "learning_rate": 3.228382502543235e-05,
      "loss": 2.2832,
      "step": 34830
    },
    {
      "epoch": 17.721261444557477,
      "grad_norm": 27.534469604492188,
      "learning_rate": 3.227873855544252e-05,
      "loss": 2.3333,
      "step": 34840
    },
    {
      "epoch": 17.726347914547304,
      "grad_norm": 31.839994430541992,
      "learning_rate": 3.22736520854527e-05,
      "loss": 2.3579,
      "step": 34850
    },
    {
      "epoch": 17.73143438453713,
      "grad_norm": 35.02449417114258,
      "learning_rate": 3.226856561546287e-05,
      "loss": 2.3128,
      "step": 34860
    },
    {
      "epoch": 17.736520854526958,
      "grad_norm": 24.72640037536621,
      "learning_rate": 3.226347914547304e-05,
      "loss": 2.3375,
      "step": 34870
    },
    {
      "epoch": 17.741607324516785,
      "grad_norm": 31.90007209777832,
      "learning_rate": 3.2258392675483216e-05,
      "loss": 2.3675,
      "step": 34880
    },
    {
      "epoch": 17.746693794506612,
      "grad_norm": 30.418241500854492,
      "learning_rate": 3.2253306205493386e-05,
      "loss": 2.2686,
      "step": 34890
    },
    {
      "epoch": 17.75178026449644,
      "grad_norm": 34.282466888427734,
      "learning_rate": 3.224821973550356e-05,
      "loss": 2.3036,
      "step": 34900
    },
    {
      "epoch": 17.756866734486266,
      "grad_norm": 36.15898132324219,
      "learning_rate": 3.224313326551374e-05,
      "loss": 2.2198,
      "step": 34910
    },
    {
      "epoch": 17.761953204476093,
      "grad_norm": 29.29920768737793,
      "learning_rate": 3.223804679552391e-05,
      "loss": 2.3223,
      "step": 34920
    },
    {
      "epoch": 17.76703967446592,
      "grad_norm": 39.16740036010742,
      "learning_rate": 3.223296032553408e-05,
      "loss": 2.3864,
      "step": 34930
    },
    {
      "epoch": 17.772126144455747,
      "grad_norm": 30.899702072143555,
      "learning_rate": 3.2227873855544256e-05,
      "loss": 2.2927,
      "step": 34940
    },
    {
      "epoch": 17.777212614445574,
      "grad_norm": 30.238414764404297,
      "learning_rate": 3.2222787385554426e-05,
      "loss": 2.2643,
      "step": 34950
    },
    {
      "epoch": 17.7822990844354,
      "grad_norm": 28.56869888305664,
      "learning_rate": 3.2217700915564596e-05,
      "loss": 2.3203,
      "step": 34960
    },
    {
      "epoch": 17.787385554425228,
      "grad_norm": 38.116050720214844,
      "learning_rate": 3.221261444557477e-05,
      "loss": 2.2781,
      "step": 34970
    },
    {
      "epoch": 17.792472024415055,
      "grad_norm": 32.64759063720703,
      "learning_rate": 3.220752797558494e-05,
      "loss": 2.2602,
      "step": 34980
    },
    {
      "epoch": 17.797558494404882,
      "grad_norm": 28.803186416625977,
      "learning_rate": 3.220244150559512e-05,
      "loss": 2.2878,
      "step": 34990
    },
    {
      "epoch": 17.80264496439471,
      "grad_norm": 27.741966247558594,
      "learning_rate": 3.2197355035605296e-05,
      "loss": 2.3497,
      "step": 35000
    },
    {
      "epoch": 17.807731434384536,
      "grad_norm": 30.364234924316406,
      "learning_rate": 3.2192268565615465e-05,
      "loss": 2.4021,
      "step": 35010
    },
    {
      "epoch": 17.812817904374363,
      "grad_norm": 32.235328674316406,
      "learning_rate": 3.218718209562564e-05,
      "loss": 2.2944,
      "step": 35020
    },
    {
      "epoch": 17.81790437436419,
      "grad_norm": 37.34453201293945,
      "learning_rate": 3.218209562563581e-05,
      "loss": 2.2149,
      "step": 35030
    },
    {
      "epoch": 17.822990844354017,
      "grad_norm": 36.734867095947266,
      "learning_rate": 3.217700915564598e-05,
      "loss": 2.2561,
      "step": 35040
    },
    {
      "epoch": 17.828077314343844,
      "grad_norm": 32.167640686035156,
      "learning_rate": 3.217192268565616e-05,
      "loss": 2.2375,
      "step": 35050
    },
    {
      "epoch": 17.83316378433367,
      "grad_norm": 31.739980697631836,
      "learning_rate": 3.216683621566633e-05,
      "loss": 2.2565,
      "step": 35060
    },
    {
      "epoch": 17.838250254323498,
      "grad_norm": 23.89754295349121,
      "learning_rate": 3.21617497456765e-05,
      "loss": 2.3359,
      "step": 35070
    },
    {
      "epoch": 17.843336724313325,
      "grad_norm": 23.97776222229004,
      "learning_rate": 3.2156663275686675e-05,
      "loss": 2.2617,
      "step": 35080
    },
    {
      "epoch": 17.848423194303155,
      "grad_norm": 39.469696044921875,
      "learning_rate": 3.2151576805696845e-05,
      "loss": 2.3361,
      "step": 35090
    },
    {
      "epoch": 17.853509664292982,
      "grad_norm": 32.34641647338867,
      "learning_rate": 3.214649033570702e-05,
      "loss": 2.2668,
      "step": 35100
    },
    {
      "epoch": 17.85859613428281,
      "grad_norm": 31.53340721130371,
      "learning_rate": 3.21414038657172e-05,
      "loss": 2.3606,
      "step": 35110
    },
    {
      "epoch": 17.863682604272636,
      "grad_norm": 37.93163299560547,
      "learning_rate": 3.213631739572737e-05,
      "loss": 2.2267,
      "step": 35120
    },
    {
      "epoch": 17.868769074262463,
      "grad_norm": 30.49757957458496,
      "learning_rate": 3.213123092573754e-05,
      "loss": 2.3133,
      "step": 35130
    },
    {
      "epoch": 17.87385554425229,
      "grad_norm": 29.53685760498047,
      "learning_rate": 3.2126144455747715e-05,
      "loss": 2.247,
      "step": 35140
    },
    {
      "epoch": 17.878942014242117,
      "grad_norm": 26.72383689880371,
      "learning_rate": 3.2121057985757885e-05,
      "loss": 2.1554,
      "step": 35150
    },
    {
      "epoch": 17.884028484231944,
      "grad_norm": 32.303470611572266,
      "learning_rate": 3.2115971515768055e-05,
      "loss": 2.2874,
      "step": 35160
    },
    {
      "epoch": 17.88911495422177,
      "grad_norm": 32.475242614746094,
      "learning_rate": 3.211088504577823e-05,
      "loss": 2.3243,
      "step": 35170
    },
    {
      "epoch": 17.894201424211598,
      "grad_norm": 34.801025390625,
      "learning_rate": 3.21057985757884e-05,
      "loss": 2.3569,
      "step": 35180
    },
    {
      "epoch": 17.899287894201425,
      "grad_norm": 25.180339813232422,
      "learning_rate": 3.210071210579858e-05,
      "loss": 2.3634,
      "step": 35190
    },
    {
      "epoch": 17.904374364191252,
      "grad_norm": 25.20892906188965,
      "learning_rate": 3.2095625635808754e-05,
      "loss": 2.2677,
      "step": 35200
    },
    {
      "epoch": 17.90946083418108,
      "grad_norm": 29.626846313476562,
      "learning_rate": 3.2090539165818924e-05,
      "loss": 2.316,
      "step": 35210
    },
    {
      "epoch": 17.914547304170906,
      "grad_norm": 39.696372985839844,
      "learning_rate": 3.2085452695829094e-05,
      "loss": 2.3542,
      "step": 35220
    },
    {
      "epoch": 17.919633774160733,
      "grad_norm": 25.407657623291016,
      "learning_rate": 3.208036622583927e-05,
      "loss": 2.2608,
      "step": 35230
    },
    {
      "epoch": 17.92472024415056,
      "grad_norm": 23.494409561157227,
      "learning_rate": 3.207527975584944e-05,
      "loss": 2.3523,
      "step": 35240
    },
    {
      "epoch": 17.929806714140387,
      "grad_norm": 29.38047218322754,
      "learning_rate": 3.207019328585962e-05,
      "loss": 2.3558,
      "step": 35250
    },
    {
      "epoch": 17.934893184130214,
      "grad_norm": 31.1119441986084,
      "learning_rate": 3.206510681586979e-05,
      "loss": 2.2333,
      "step": 35260
    },
    {
      "epoch": 17.93997965412004,
      "grad_norm": 27.087495803833008,
      "learning_rate": 3.206002034587996e-05,
      "loss": 2.2892,
      "step": 35270
    },
    {
      "epoch": 17.945066124109868,
      "grad_norm": 30.141328811645508,
      "learning_rate": 3.2054933875890134e-05,
      "loss": 2.3147,
      "step": 35280
    },
    {
      "epoch": 17.950152594099695,
      "grad_norm": 44.01845169067383,
      "learning_rate": 3.204984740590031e-05,
      "loss": 2.3074,
      "step": 35290
    },
    {
      "epoch": 17.955239064089522,
      "grad_norm": 27.452186584472656,
      "learning_rate": 3.204476093591048e-05,
      "loss": 2.3236,
      "step": 35300
    },
    {
      "epoch": 17.96032553407935,
      "grad_norm": 30.010839462280273,
      "learning_rate": 3.203967446592066e-05,
      "loss": 2.2449,
      "step": 35310
    },
    {
      "epoch": 17.965412004069176,
      "grad_norm": 31.306678771972656,
      "learning_rate": 3.203458799593083e-05,
      "loss": 2.314,
      "step": 35320
    },
    {
      "epoch": 17.970498474059003,
      "grad_norm": 28.76388168334961,
      "learning_rate": 3.2029501525941e-05,
      "loss": 2.2935,
      "step": 35330
    },
    {
      "epoch": 17.97558494404883,
      "grad_norm": 32.759559631347656,
      "learning_rate": 3.2024415055951174e-05,
      "loss": 2.2657,
      "step": 35340
    },
    {
      "epoch": 17.980671414038657,
      "grad_norm": 38.745086669921875,
      "learning_rate": 3.2019328585961343e-05,
      "loss": 2.2997,
      "step": 35350
    },
    {
      "epoch": 17.985757884028484,
      "grad_norm": 27.865943908691406,
      "learning_rate": 3.201424211597151e-05,
      "loss": 2.3795,
      "step": 35360
    },
    {
      "epoch": 17.99084435401831,
      "grad_norm": 30.472034454345703,
      "learning_rate": 3.200915564598169e-05,
      "loss": 2.2874,
      "step": 35370
    },
    {
      "epoch": 17.995930824008138,
      "grad_norm": 35.434329986572266,
      "learning_rate": 3.200406917599186e-05,
      "loss": 2.3614,
      "step": 35380
    },
    {
      "epoch": 18.0,
      "eval_loss": 4.190389156341553,
      "eval_runtime": 2.7848,
      "eval_samples_per_second": 996.482,
      "eval_steps_per_second": 124.605,
      "step": 35388
    },
    {
      "epoch": 18.001017293997965,
      "grad_norm": 29.849224090576172,
      "learning_rate": 3.1998982706002037e-05,
      "loss": 2.2971,
      "step": 35390
    },
    {
      "epoch": 18.00610376398779,
      "grad_norm": 34.691349029541016,
      "learning_rate": 3.199389623601221e-05,
      "loss": 2.3082,
      "step": 35400
    },
    {
      "epoch": 18.01119023397762,
      "grad_norm": 30.487619400024414,
      "learning_rate": 3.198880976602238e-05,
      "loss": 2.2249,
      "step": 35410
    },
    {
      "epoch": 18.016276703967446,
      "grad_norm": 32.45216369628906,
      "learning_rate": 3.198372329603255e-05,
      "loss": 2.2283,
      "step": 35420
    },
    {
      "epoch": 18.021363173957273,
      "grad_norm": 26.5003719329834,
      "learning_rate": 3.197863682604273e-05,
      "loss": 2.1897,
      "step": 35430
    },
    {
      "epoch": 18.0264496439471,
      "grad_norm": 28.840530395507812,
      "learning_rate": 3.19735503560529e-05,
      "loss": 2.2133,
      "step": 35440
    },
    {
      "epoch": 18.031536113936927,
      "grad_norm": 24.953136444091797,
      "learning_rate": 3.196846388606307e-05,
      "loss": 2.2059,
      "step": 35450
    },
    {
      "epoch": 18.036622583926754,
      "grad_norm": 29.555315017700195,
      "learning_rate": 3.1963377416073246e-05,
      "loss": 2.3231,
      "step": 35460
    },
    {
      "epoch": 18.04170905391658,
      "grad_norm": 26.714736938476562,
      "learning_rate": 3.1958290946083416e-05,
      "loss": 2.3073,
      "step": 35470
    },
    {
      "epoch": 18.046795523906408,
      "grad_norm": 32.79590606689453,
      "learning_rate": 3.195320447609359e-05,
      "loss": 2.2734,
      "step": 35480
    },
    {
      "epoch": 18.051881993896234,
      "grad_norm": 37.93794631958008,
      "learning_rate": 3.194811800610377e-05,
      "loss": 2.2833,
      "step": 35490
    },
    {
      "epoch": 18.05696846388606,
      "grad_norm": 29.516965866088867,
      "learning_rate": 3.194303153611394e-05,
      "loss": 2.3158,
      "step": 35500
    },
    {
      "epoch": 18.06205493387589,
      "grad_norm": 49.635108947753906,
      "learning_rate": 3.193794506612411e-05,
      "loss": 2.2536,
      "step": 35510
    },
    {
      "epoch": 18.067141403865715,
      "grad_norm": 40.01588439941406,
      "learning_rate": 3.1932858596134286e-05,
      "loss": 2.3454,
      "step": 35520
    },
    {
      "epoch": 18.072227873855546,
      "grad_norm": 27.59291648864746,
      "learning_rate": 3.1927772126144456e-05,
      "loss": 2.2514,
      "step": 35530
    },
    {
      "epoch": 18.077314343845373,
      "grad_norm": 28.075138092041016,
      "learning_rate": 3.192268565615463e-05,
      "loss": 2.35,
      "step": 35540
    },
    {
      "epoch": 18.0824008138352,
      "grad_norm": 23.978546142578125,
      "learning_rate": 3.19175991861648e-05,
      "loss": 2.3579,
      "step": 35550
    },
    {
      "epoch": 18.087487283825027,
      "grad_norm": 26.18883514404297,
      "learning_rate": 3.191251271617497e-05,
      "loss": 2.2861,
      "step": 35560
    },
    {
      "epoch": 18.092573753814854,
      "grad_norm": 30.081340789794922,
      "learning_rate": 3.190742624618515e-05,
      "loss": 2.2097,
      "step": 35570
    },
    {
      "epoch": 18.09766022380468,
      "grad_norm": 28.328330993652344,
      "learning_rate": 3.1902339776195326e-05,
      "loss": 2.2004,
      "step": 35580
    },
    {
      "epoch": 18.102746693794508,
      "grad_norm": 43.10849380493164,
      "learning_rate": 3.1897253306205495e-05,
      "loss": 2.304,
      "step": 35590
    },
    {
      "epoch": 18.107833163784335,
      "grad_norm": 31.675260543823242,
      "learning_rate": 3.189216683621567e-05,
      "loss": 2.2187,
      "step": 35600
    },
    {
      "epoch": 18.112919633774162,
      "grad_norm": 30.9468994140625,
      "learning_rate": 3.188708036622584e-05,
      "loss": 2.2845,
      "step": 35610
    },
    {
      "epoch": 18.11800610376399,
      "grad_norm": 35.63999557495117,
      "learning_rate": 3.188199389623601e-05,
      "loss": 2.4052,
      "step": 35620
    },
    {
      "epoch": 18.123092573753816,
      "grad_norm": 32.046939849853516,
      "learning_rate": 3.187690742624619e-05,
      "loss": 2.2042,
      "step": 35630
    },
    {
      "epoch": 18.128179043743643,
      "grad_norm": 29.190265655517578,
      "learning_rate": 3.187182095625636e-05,
      "loss": 2.2884,
      "step": 35640
    },
    {
      "epoch": 18.13326551373347,
      "grad_norm": 30.791580200195312,
      "learning_rate": 3.186673448626653e-05,
      "loss": 2.255,
      "step": 35650
    },
    {
      "epoch": 18.138351983723297,
      "grad_norm": 27.504926681518555,
      "learning_rate": 3.1861648016276705e-05,
      "loss": 2.2635,
      "step": 35660
    },
    {
      "epoch": 18.143438453713124,
      "grad_norm": 24.966928482055664,
      "learning_rate": 3.1856561546286875e-05,
      "loss": 2.3004,
      "step": 35670
    },
    {
      "epoch": 18.14852492370295,
      "grad_norm": 32.194881439208984,
      "learning_rate": 3.185147507629705e-05,
      "loss": 2.1866,
      "step": 35680
    },
    {
      "epoch": 18.153611393692778,
      "grad_norm": 28.636051177978516,
      "learning_rate": 3.184638860630723e-05,
      "loss": 2.1621,
      "step": 35690
    },
    {
      "epoch": 18.158697863682605,
      "grad_norm": 23.679569244384766,
      "learning_rate": 3.18413021363174e-05,
      "loss": 2.2865,
      "step": 35700
    },
    {
      "epoch": 18.16378433367243,
      "grad_norm": 34.972900390625,
      "learning_rate": 3.183621566632757e-05,
      "loss": 2.1884,
      "step": 35710
    },
    {
      "epoch": 18.16887080366226,
      "grad_norm": 32.11781692504883,
      "learning_rate": 3.1831129196337745e-05,
      "loss": 2.1999,
      "step": 35720
    },
    {
      "epoch": 18.173957273652086,
      "grad_norm": 36.95368576049805,
      "learning_rate": 3.1826042726347915e-05,
      "loss": 2.28,
      "step": 35730
    },
    {
      "epoch": 18.179043743641913,
      "grad_norm": 26.064498901367188,
      "learning_rate": 3.1820956256358085e-05,
      "loss": 2.3063,
      "step": 35740
    },
    {
      "epoch": 18.18413021363174,
      "grad_norm": 27.750362396240234,
      "learning_rate": 3.181586978636826e-05,
      "loss": 2.2048,
      "step": 35750
    },
    {
      "epoch": 18.189216683621567,
      "grad_norm": 35.24855422973633,
      "learning_rate": 3.181078331637843e-05,
      "loss": 2.3167,
      "step": 35760
    },
    {
      "epoch": 18.194303153611393,
      "grad_norm": 32.71845245361328,
      "learning_rate": 3.180569684638861e-05,
      "loss": 2.2904,
      "step": 35770
    },
    {
      "epoch": 18.19938962360122,
      "grad_norm": 29.252206802368164,
      "learning_rate": 3.1800610376398784e-05,
      "loss": 2.225,
      "step": 35780
    },
    {
      "epoch": 18.204476093591047,
      "grad_norm": 29.78980255126953,
      "learning_rate": 3.1795523906408954e-05,
      "loss": 2.1927,
      "step": 35790
    },
    {
      "epoch": 18.209562563580874,
      "grad_norm": 30.368770599365234,
      "learning_rate": 3.179043743641913e-05,
      "loss": 2.303,
      "step": 35800
    },
    {
      "epoch": 18.2146490335707,
      "grad_norm": 26.170879364013672,
      "learning_rate": 3.17853509664293e-05,
      "loss": 2.2872,
      "step": 35810
    },
    {
      "epoch": 18.21973550356053,
      "grad_norm": 32.92437744140625,
      "learning_rate": 3.178026449643947e-05,
      "loss": 2.2423,
      "step": 35820
    },
    {
      "epoch": 18.224821973550355,
      "grad_norm": 26.526199340820312,
      "learning_rate": 3.177517802644965e-05,
      "loss": 2.1888,
      "step": 35830
    },
    {
      "epoch": 18.229908443540182,
      "grad_norm": 43.934326171875,
      "learning_rate": 3.177009155645982e-05,
      "loss": 2.1681,
      "step": 35840
    },
    {
      "epoch": 18.23499491353001,
      "grad_norm": 30.86100196838379,
      "learning_rate": 3.176500508646999e-05,
      "loss": 2.2228,
      "step": 35850
    },
    {
      "epoch": 18.240081383519836,
      "grad_norm": 24.891193389892578,
      "learning_rate": 3.1759918616480164e-05,
      "loss": 2.3604,
      "step": 35860
    },
    {
      "epoch": 18.245167853509663,
      "grad_norm": 22.44628143310547,
      "learning_rate": 3.175483214649034e-05,
      "loss": 2.2853,
      "step": 35870
    },
    {
      "epoch": 18.25025432349949,
      "grad_norm": 26.095495223999023,
      "learning_rate": 3.174974567650051e-05,
      "loss": 2.2598,
      "step": 35880
    },
    {
      "epoch": 18.255340793489317,
      "grad_norm": 32.51009750366211,
      "learning_rate": 3.174465920651069e-05,
      "loss": 2.2585,
      "step": 35890
    },
    {
      "epoch": 18.260427263479144,
      "grad_norm": 35.858707427978516,
      "learning_rate": 3.173957273652086e-05,
      "loss": 2.2485,
      "step": 35900
    },
    {
      "epoch": 18.26551373346897,
      "grad_norm": 31.111873626708984,
      "learning_rate": 3.173448626653103e-05,
      "loss": 2.2167,
      "step": 35910
    },
    {
      "epoch": 18.270600203458798,
      "grad_norm": 32.71385192871094,
      "learning_rate": 3.1729399796541204e-05,
      "loss": 2.1502,
      "step": 35920
    },
    {
      "epoch": 18.275686673448625,
      "grad_norm": 28.262657165527344,
      "learning_rate": 3.1724313326551373e-05,
      "loss": 2.2183,
      "step": 35930
    },
    {
      "epoch": 18.280773143438452,
      "grad_norm": 29.257919311523438,
      "learning_rate": 3.171922685656154e-05,
      "loss": 2.2643,
      "step": 35940
    },
    {
      "epoch": 18.28585961342828,
      "grad_norm": 35.101871490478516,
      "learning_rate": 3.171414038657172e-05,
      "loss": 2.2967,
      "step": 35950
    },
    {
      "epoch": 18.290946083418106,
      "grad_norm": 36.60392761230469,
      "learning_rate": 3.17090539165819e-05,
      "loss": 2.3058,
      "step": 35960
    },
    {
      "epoch": 18.296032553407933,
      "grad_norm": 28.157686233520508,
      "learning_rate": 3.1703967446592067e-05,
      "loss": 2.2639,
      "step": 35970
    },
    {
      "epoch": 18.301119023397764,
      "grad_norm": 32.767967224121094,
      "learning_rate": 3.169888097660224e-05,
      "loss": 2.2842,
      "step": 35980
    },
    {
      "epoch": 18.30620549338759,
      "grad_norm": 24.8610782623291,
      "learning_rate": 3.169379450661241e-05,
      "loss": 2.2434,
      "step": 35990
    },
    {
      "epoch": 18.311291963377418,
      "grad_norm": 25.210153579711914,
      "learning_rate": 3.168870803662258e-05,
      "loss": 2.3099,
      "step": 36000
    },
    {
      "epoch": 18.316378433367245,
      "grad_norm": 27.52773094177246,
      "learning_rate": 3.168362156663276e-05,
      "loss": 2.2406,
      "step": 36010
    },
    {
      "epoch": 18.32146490335707,
      "grad_norm": 33.01058578491211,
      "learning_rate": 3.167853509664293e-05,
      "loss": 2.2547,
      "step": 36020
    },
    {
      "epoch": 18.3265513733469,
      "grad_norm": 38.23484420776367,
      "learning_rate": 3.16734486266531e-05,
      "loss": 2.2722,
      "step": 36030
    },
    {
      "epoch": 18.331637843336726,
      "grad_norm": 35.525630950927734,
      "learning_rate": 3.1668362156663276e-05,
      "loss": 2.3417,
      "step": 36040
    },
    {
      "epoch": 18.336724313326553,
      "grad_norm": 39.10609436035156,
      "learning_rate": 3.1663275686673446e-05,
      "loss": 2.2344,
      "step": 36050
    },
    {
      "epoch": 18.34181078331638,
      "grad_norm": 25.116222381591797,
      "learning_rate": 3.165818921668362e-05,
      "loss": 2.2997,
      "step": 36060
    },
    {
      "epoch": 18.346897253306206,
      "grad_norm": 38.62306213378906,
      "learning_rate": 3.16531027466938e-05,
      "loss": 2.2187,
      "step": 36070
    },
    {
      "epoch": 18.351983723296033,
      "grad_norm": 35.12945556640625,
      "learning_rate": 3.164801627670397e-05,
      "loss": 2.2599,
      "step": 36080
    },
    {
      "epoch": 18.35707019328586,
      "grad_norm": 23.230937957763672,
      "learning_rate": 3.1642929806714146e-05,
      "loss": 2.3124,
      "step": 36090
    },
    {
      "epoch": 18.362156663275687,
      "grad_norm": 30.25659942626953,
      "learning_rate": 3.1637843336724316e-05,
      "loss": 2.2072,
      "step": 36100
    },
    {
      "epoch": 18.367243133265514,
      "grad_norm": 38.407859802246094,
      "learning_rate": 3.1632756866734486e-05,
      "loss": 2.2336,
      "step": 36110
    },
    {
      "epoch": 18.37232960325534,
      "grad_norm": 27.058443069458008,
      "learning_rate": 3.162767039674466e-05,
      "loss": 2.2488,
      "step": 36120
    },
    {
      "epoch": 18.37741607324517,
      "grad_norm": 29.791406631469727,
      "learning_rate": 3.162258392675483e-05,
      "loss": 2.2793,
      "step": 36130
    },
    {
      "epoch": 18.382502543234995,
      "grad_norm": 32.4749755859375,
      "learning_rate": 3.1617497456765e-05,
      "loss": 2.337,
      "step": 36140
    },
    {
      "epoch": 18.387589013224822,
      "grad_norm": 31.202463150024414,
      "learning_rate": 3.161241098677518e-05,
      "loss": 2.3127,
      "step": 36150
    },
    {
      "epoch": 18.39267548321465,
      "grad_norm": 28.341352462768555,
      "learning_rate": 3.1607324516785356e-05,
      "loss": 2.2321,
      "step": 36160
    },
    {
      "epoch": 18.397761953204476,
      "grad_norm": 27.17453956604004,
      "learning_rate": 3.1602238046795525e-05,
      "loss": 2.3314,
      "step": 36170
    },
    {
      "epoch": 18.402848423194303,
      "grad_norm": 31.45462989807129,
      "learning_rate": 3.15971515768057e-05,
      "loss": 2.2505,
      "step": 36180
    },
    {
      "epoch": 18.40793489318413,
      "grad_norm": 28.792930603027344,
      "learning_rate": 3.159206510681587e-05,
      "loss": 2.2298,
      "step": 36190
    },
    {
      "epoch": 18.413021363173957,
      "grad_norm": 29.35610580444336,
      "learning_rate": 3.158697863682604e-05,
      "loss": 2.1392,
      "step": 36200
    },
    {
      "epoch": 18.418107833163784,
      "grad_norm": 32.299686431884766,
      "learning_rate": 3.158189216683622e-05,
      "loss": 2.1356,
      "step": 36210
    },
    {
      "epoch": 18.42319430315361,
      "grad_norm": 25.887842178344727,
      "learning_rate": 3.157680569684639e-05,
      "loss": 2.274,
      "step": 36220
    },
    {
      "epoch": 18.428280773143438,
      "grad_norm": 39.076419830322266,
      "learning_rate": 3.157171922685656e-05,
      "loss": 2.2242,
      "step": 36230
    },
    {
      "epoch": 18.433367243133265,
      "grad_norm": 28.91887664794922,
      "learning_rate": 3.1566632756866735e-05,
      "loss": 2.2674,
      "step": 36240
    },
    {
      "epoch": 18.438453713123092,
      "grad_norm": 27.595335006713867,
      "learning_rate": 3.156154628687691e-05,
      "loss": 2.265,
      "step": 36250
    },
    {
      "epoch": 18.44354018311292,
      "grad_norm": 28.10325813293457,
      "learning_rate": 3.155645981688708e-05,
      "loss": 2.293,
      "step": 36260
    },
    {
      "epoch": 18.448626653102746,
      "grad_norm": 26.652130126953125,
      "learning_rate": 3.155137334689726e-05,
      "loss": 2.2373,
      "step": 36270
    },
    {
      "epoch": 18.453713123092573,
      "grad_norm": 29.5018253326416,
      "learning_rate": 3.154628687690743e-05,
      "loss": 2.2474,
      "step": 36280
    },
    {
      "epoch": 18.4587995930824,
      "grad_norm": 35.08016586303711,
      "learning_rate": 3.15412004069176e-05,
      "loss": 2.201,
      "step": 36290
    },
    {
      "epoch": 18.463886063072227,
      "grad_norm": 33.84527587890625,
      "learning_rate": 3.1536113936927775e-05,
      "loss": 2.314,
      "step": 36300
    },
    {
      "epoch": 18.468972533062054,
      "grad_norm": 29.980520248413086,
      "learning_rate": 3.1531027466937945e-05,
      "loss": 2.3802,
      "step": 36310
    },
    {
      "epoch": 18.47405900305188,
      "grad_norm": 29.013113021850586,
      "learning_rate": 3.1525940996948115e-05,
      "loss": 2.2526,
      "step": 36320
    },
    {
      "epoch": 18.479145473041708,
      "grad_norm": 30.57776641845703,
      "learning_rate": 3.152085452695829e-05,
      "loss": 2.3151,
      "step": 36330
    },
    {
      "epoch": 18.484231943031535,
      "grad_norm": 26.04262351989746,
      "learning_rate": 3.151576805696846e-05,
      "loss": 2.3226,
      "step": 36340
    },
    {
      "epoch": 18.489318413021362,
      "grad_norm": 32.16754913330078,
      "learning_rate": 3.151068158697864e-05,
      "loss": 2.2397,
      "step": 36350
    },
    {
      "epoch": 18.49440488301119,
      "grad_norm": 32.02252960205078,
      "learning_rate": 3.1505595116988814e-05,
      "loss": 2.2491,
      "step": 36360
    },
    {
      "epoch": 18.499491353001016,
      "grad_norm": 33.631744384765625,
      "learning_rate": 3.1500508646998984e-05,
      "loss": 2.3351,
      "step": 36370
    },
    {
      "epoch": 18.504577822990843,
      "grad_norm": 38.38560104370117,
      "learning_rate": 3.149542217700916e-05,
      "loss": 2.3204,
      "step": 36380
    },
    {
      "epoch": 18.50966429298067,
      "grad_norm": 27.888986587524414,
      "learning_rate": 3.149033570701933e-05,
      "loss": 2.2154,
      "step": 36390
    },
    {
      "epoch": 18.514750762970497,
      "grad_norm": 27.36655044555664,
      "learning_rate": 3.14852492370295e-05,
      "loss": 2.2981,
      "step": 36400
    },
    {
      "epoch": 18.519837232960327,
      "grad_norm": 33.155128479003906,
      "learning_rate": 3.148016276703968e-05,
      "loss": 2.1831,
      "step": 36410
    },
    {
      "epoch": 18.524923702950154,
      "grad_norm": 26.594928741455078,
      "learning_rate": 3.147507629704985e-05,
      "loss": 2.2311,
      "step": 36420
    },
    {
      "epoch": 18.53001017293998,
      "grad_norm": 26.189634323120117,
      "learning_rate": 3.146998982706002e-05,
      "loss": 2.2235,
      "step": 36430
    },
    {
      "epoch": 18.53509664292981,
      "grad_norm": 29.840160369873047,
      "learning_rate": 3.1464903357070194e-05,
      "loss": 2.2189,
      "step": 36440
    },
    {
      "epoch": 18.540183112919635,
      "grad_norm": 30.78571128845215,
      "learning_rate": 3.145981688708037e-05,
      "loss": 2.2206,
      "step": 36450
    },
    {
      "epoch": 18.545269582909462,
      "grad_norm": 26.777084350585938,
      "learning_rate": 3.145473041709054e-05,
      "loss": 2.3532,
      "step": 36460
    },
    {
      "epoch": 18.55035605289929,
      "grad_norm": 39.11761474609375,
      "learning_rate": 3.144964394710072e-05,
      "loss": 2.2967,
      "step": 36470
    },
    {
      "epoch": 18.555442522889116,
      "grad_norm": 27.97113609313965,
      "learning_rate": 3.144455747711089e-05,
      "loss": 2.2298,
      "step": 36480
    },
    {
      "epoch": 18.560528992878943,
      "grad_norm": 33.591400146484375,
      "learning_rate": 3.143947100712106e-05,
      "loss": 2.2988,
      "step": 36490
    },
    {
      "epoch": 18.56561546286877,
      "grad_norm": 32.82482147216797,
      "learning_rate": 3.1434384537131234e-05,
      "loss": 2.2674,
      "step": 36500
    },
    {
      "epoch": 18.570701932858597,
      "grad_norm": 27.374094009399414,
      "learning_rate": 3.1429298067141403e-05,
      "loss": 2.2428,
      "step": 36510
    },
    {
      "epoch": 18.575788402848424,
      "grad_norm": 29.354461669921875,
      "learning_rate": 3.142421159715157e-05,
      "loss": 2.1943,
      "step": 36520
    },
    {
      "epoch": 18.58087487283825,
      "grad_norm": 33.290283203125,
      "learning_rate": 3.141912512716175e-05,
      "loss": 2.3039,
      "step": 36530
    },
    {
      "epoch": 18.585961342828078,
      "grad_norm": 26.056232452392578,
      "learning_rate": 3.141403865717193e-05,
      "loss": 2.2427,
      "step": 36540
    },
    {
      "epoch": 18.591047812817905,
      "grad_norm": 35.30779266357422,
      "learning_rate": 3.1408952187182097e-05,
      "loss": 2.2658,
      "step": 36550
    },
    {
      "epoch": 18.596134282807732,
      "grad_norm": 29.51120948791504,
      "learning_rate": 3.140386571719227e-05,
      "loss": 2.3437,
      "step": 36560
    },
    {
      "epoch": 18.60122075279756,
      "grad_norm": 27.115612030029297,
      "learning_rate": 3.139877924720244e-05,
      "loss": 2.2434,
      "step": 36570
    },
    {
      "epoch": 18.606307222787386,
      "grad_norm": 26.701705932617188,
      "learning_rate": 3.139369277721261e-05,
      "loss": 2.2289,
      "step": 36580
    },
    {
      "epoch": 18.611393692777213,
      "grad_norm": 33.924644470214844,
      "learning_rate": 3.138860630722279e-05,
      "loss": 2.3022,
      "step": 36590
    },
    {
      "epoch": 18.61648016276704,
      "grad_norm": 25.801532745361328,
      "learning_rate": 3.138351983723296e-05,
      "loss": 2.2509,
      "step": 36600
    },
    {
      "epoch": 18.621566632756867,
      "grad_norm": 29.738309860229492,
      "learning_rate": 3.1378433367243136e-05,
      "loss": 2.1378,
      "step": 36610
    },
    {
      "epoch": 18.626653102746694,
      "grad_norm": 28.07364845275879,
      "learning_rate": 3.1373346897253306e-05,
      "loss": 2.3031,
      "step": 36620
    },
    {
      "epoch": 18.63173957273652,
      "grad_norm": 35.14064407348633,
      "learning_rate": 3.1368260427263476e-05,
      "loss": 2.1769,
      "step": 36630
    },
    {
      "epoch": 18.636826042726348,
      "grad_norm": 30.57267189025879,
      "learning_rate": 3.136317395727365e-05,
      "loss": 2.2033,
      "step": 36640
    },
    {
      "epoch": 18.641912512716175,
      "grad_norm": 39.79961395263672,
      "learning_rate": 3.135808748728383e-05,
      "loss": 2.1718,
      "step": 36650
    },
    {
      "epoch": 18.646998982706002,
      "grad_norm": 31.79742431640625,
      "learning_rate": 3.1353001017294e-05,
      "loss": 2.3576,
      "step": 36660
    },
    {
      "epoch": 18.65208545269583,
      "grad_norm": 32.078670501708984,
      "learning_rate": 3.1347914547304176e-05,
      "loss": 2.2929,
      "step": 36670
    },
    {
      "epoch": 18.657171922685656,
      "grad_norm": 39.47534942626953,
      "learning_rate": 3.1342828077314346e-05,
      "loss": 2.21,
      "step": 36680
    },
    {
      "epoch": 18.662258392675483,
      "grad_norm": 30.628955841064453,
      "learning_rate": 3.1337741607324516e-05,
      "loss": 2.2306,
      "step": 36690
    },
    {
      "epoch": 18.66734486266531,
      "grad_norm": 32.972469329833984,
      "learning_rate": 3.133265513733469e-05,
      "loss": 2.2308,
      "step": 36700
    },
    {
      "epoch": 18.672431332655137,
      "grad_norm": 33.7562370300293,
      "learning_rate": 3.132756866734486e-05,
      "loss": 2.2906,
      "step": 36710
    },
    {
      "epoch": 18.677517802644964,
      "grad_norm": 39.74196243286133,
      "learning_rate": 3.132248219735503e-05,
      "loss": 2.229,
      "step": 36720
    },
    {
      "epoch": 18.68260427263479,
      "grad_norm": 26.242692947387695,
      "learning_rate": 3.131739572736521e-05,
      "loss": 2.2719,
      "step": 36730
    },
    {
      "epoch": 18.687690742624618,
      "grad_norm": 29.29735565185547,
      "learning_rate": 3.1312309257375386e-05,
      "loss": 2.2651,
      "step": 36740
    },
    {
      "epoch": 18.692777212614445,
      "grad_norm": 36.87482452392578,
      "learning_rate": 3.1307222787385555e-05,
      "loss": 2.2107,
      "step": 36750
    },
    {
      "epoch": 18.69786368260427,
      "grad_norm": 36.29994201660156,
      "learning_rate": 3.130213631739573e-05,
      "loss": 2.2005,
      "step": 36760
    },
    {
      "epoch": 18.7029501525941,
      "grad_norm": 39.183956146240234,
      "learning_rate": 3.12970498474059e-05,
      "loss": 2.2673,
      "step": 36770
    },
    {
      "epoch": 18.708036622583926,
      "grad_norm": 30.952625274658203,
      "learning_rate": 3.129196337741607e-05,
      "loss": 2.1946,
      "step": 36780
    },
    {
      "epoch": 18.713123092573753,
      "grad_norm": 25.929353713989258,
      "learning_rate": 3.128687690742625e-05,
      "loss": 2.2249,
      "step": 36790
    },
    {
      "epoch": 18.71820956256358,
      "grad_norm": 37.43108367919922,
      "learning_rate": 3.128179043743642e-05,
      "loss": 2.2455,
      "step": 36800
    },
    {
      "epoch": 18.723296032553407,
      "grad_norm": 31.33913803100586,
      "learning_rate": 3.127670396744659e-05,
      "loss": 2.1548,
      "step": 36810
    },
    {
      "epoch": 18.728382502543234,
      "grad_norm": 32.73765563964844,
      "learning_rate": 3.1271617497456765e-05,
      "loss": 2.2134,
      "step": 36820
    },
    {
      "epoch": 18.73346897253306,
      "grad_norm": 23.11245346069336,
      "learning_rate": 3.126653102746694e-05,
      "loss": 2.2681,
      "step": 36830
    },
    {
      "epoch": 18.738555442522888,
      "grad_norm": 36.2789306640625,
      "learning_rate": 3.126144455747711e-05,
      "loss": 2.2331,
      "step": 36840
    },
    {
      "epoch": 18.743641912512714,
      "grad_norm": 24.90999984741211,
      "learning_rate": 3.125635808748729e-05,
      "loss": 2.1905,
      "step": 36850
    },
    {
      "epoch": 18.74872838250254,
      "grad_norm": 35.49935531616211,
      "learning_rate": 3.125127161749746e-05,
      "loss": 2.2649,
      "step": 36860
    },
    {
      "epoch": 18.753814852492372,
      "grad_norm": 26.347383499145508,
      "learning_rate": 3.1246185147507635e-05,
      "loss": 2.276,
      "step": 36870
    },
    {
      "epoch": 18.7589013224822,
      "grad_norm": 26.926631927490234,
      "learning_rate": 3.1241098677517805e-05,
      "loss": 2.2753,
      "step": 36880
    },
    {
      "epoch": 18.763987792472026,
      "grad_norm": 30.824928283691406,
      "learning_rate": 3.1236012207527975e-05,
      "loss": 2.2863,
      "step": 36890
    },
    {
      "epoch": 18.769074262461853,
      "grad_norm": 36.14662170410156,
      "learning_rate": 3.123092573753815e-05,
      "loss": 2.2832,
      "step": 36900
    },
    {
      "epoch": 18.77416073245168,
      "grad_norm": 25.732093811035156,
      "learning_rate": 3.122583926754832e-05,
      "loss": 2.3187,
      "step": 36910
    },
    {
      "epoch": 18.779247202441507,
      "grad_norm": 26.19925880432129,
      "learning_rate": 3.12207527975585e-05,
      "loss": 2.2536,
      "step": 36920
    },
    {
      "epoch": 18.784333672431334,
      "grad_norm": 33.81591033935547,
      "learning_rate": 3.1215666327568675e-05,
      "loss": 2.2034,
      "step": 36930
    },
    {
      "epoch": 18.78942014242116,
      "grad_norm": 28.33380126953125,
      "learning_rate": 3.1210579857578844e-05,
      "loss": 2.1856,
      "step": 36940
    },
    {
      "epoch": 18.794506612410988,
      "grad_norm": 28.40678596496582,
      "learning_rate": 3.1205493387589014e-05,
      "loss": 2.2336,
      "step": 36950
    },
    {
      "epoch": 18.799593082400815,
      "grad_norm": 30.368555068969727,
      "learning_rate": 3.120040691759919e-05,
      "loss": 2.2137,
      "step": 36960
    },
    {
      "epoch": 18.804679552390642,
      "grad_norm": 29.56035804748535,
      "learning_rate": 3.119532044760936e-05,
      "loss": 2.1971,
      "step": 36970
    },
    {
      "epoch": 18.80976602238047,
      "grad_norm": 37.08443069458008,
      "learning_rate": 3.119023397761953e-05,
      "loss": 2.1892,
      "step": 36980
    },
    {
      "epoch": 18.814852492370296,
      "grad_norm": 36.546531677246094,
      "learning_rate": 3.118514750762971e-05,
      "loss": 2.2957,
      "step": 36990
    },
    {
      "epoch": 18.819938962360123,
      "grad_norm": 27.84378433227539,
      "learning_rate": 3.118006103763988e-05,
      "loss": 2.1947,
      "step": 37000
    },
    {
      "epoch": 18.82502543234995,
      "grad_norm": 27.81289291381836,
      "learning_rate": 3.117497456765005e-05,
      "loss": 2.2649,
      "step": 37010
    },
    {
      "epoch": 18.830111902339777,
      "grad_norm": 27.497940063476562,
      "learning_rate": 3.1169888097660224e-05,
      "loss": 2.2561,
      "step": 37020
    },
    {
      "epoch": 18.835198372329604,
      "grad_norm": 36.64883041381836,
      "learning_rate": 3.11648016276704e-05,
      "loss": 2.2946,
      "step": 37030
    },
    {
      "epoch": 18.84028484231943,
      "grad_norm": 26.114713668823242,
      "learning_rate": 3.115971515768057e-05,
      "loss": 2.2865,
      "step": 37040
    },
    {
      "epoch": 18.845371312309258,
      "grad_norm": 29.94086456298828,
      "learning_rate": 3.115462868769075e-05,
      "loss": 2.2525,
      "step": 37050
    },
    {
      "epoch": 18.850457782299085,
      "grad_norm": 23.86642074584961,
      "learning_rate": 3.114954221770092e-05,
      "loss": 2.2501,
      "step": 37060
    },
    {
      "epoch": 18.85554425228891,
      "grad_norm": 38.39973449707031,
      "learning_rate": 3.114445574771109e-05,
      "loss": 2.2941,
      "step": 37070
    },
    {
      "epoch": 18.86063072227874,
      "grad_norm": 30.670745849609375,
      "learning_rate": 3.1139369277721264e-05,
      "loss": 2.2024,
      "step": 37080
    },
    {
      "epoch": 18.865717192268566,
      "grad_norm": 28.985307693481445,
      "learning_rate": 3.1134282807731433e-05,
      "loss": 2.3017,
      "step": 37090
    },
    {
      "epoch": 18.870803662258393,
      "grad_norm": 35.40129470825195,
      "learning_rate": 3.11291963377416e-05,
      "loss": 2.2231,
      "step": 37100
    },
    {
      "epoch": 18.87589013224822,
      "grad_norm": 28.63679313659668,
      "learning_rate": 3.112410986775178e-05,
      "loss": 2.2366,
      "step": 37110
    },
    {
      "epoch": 18.880976602238047,
      "grad_norm": 31.491132736206055,
      "learning_rate": 3.111902339776196e-05,
      "loss": 2.2687,
      "step": 37120
    },
    {
      "epoch": 18.886063072227874,
      "grad_norm": 35.645599365234375,
      "learning_rate": 3.111393692777213e-05,
      "loss": 2.2584,
      "step": 37130
    },
    {
      "epoch": 18.8911495422177,
      "grad_norm": 29.797685623168945,
      "learning_rate": 3.11088504577823e-05,
      "loss": 2.255,
      "step": 37140
    },
    {
      "epoch": 18.896236012207527,
      "grad_norm": 23.857271194458008,
      "learning_rate": 3.110376398779247e-05,
      "loss": 2.2896,
      "step": 37150
    },
    {
      "epoch": 18.901322482197354,
      "grad_norm": 33.819908142089844,
      "learning_rate": 3.109867751780265e-05,
      "loss": 2.1683,
      "step": 37160
    },
    {
      "epoch": 18.90640895218718,
      "grad_norm": 27.428293228149414,
      "learning_rate": 3.109359104781282e-05,
      "loss": 2.1767,
      "step": 37170
    },
    {
      "epoch": 18.91149542217701,
      "grad_norm": 23.40215301513672,
      "learning_rate": 3.108850457782299e-05,
      "loss": 2.2266,
      "step": 37180
    },
    {
      "epoch": 18.916581892166835,
      "grad_norm": 31.376686096191406,
      "learning_rate": 3.1083418107833166e-05,
      "loss": 2.2395,
      "step": 37190
    },
    {
      "epoch": 18.921668362156662,
      "grad_norm": 27.512489318847656,
      "learning_rate": 3.1078331637843336e-05,
      "loss": 2.2738,
      "step": 37200
    },
    {
      "epoch": 18.92675483214649,
      "grad_norm": 31.954402923583984,
      "learning_rate": 3.107324516785351e-05,
      "loss": 2.1911,
      "step": 37210
    },
    {
      "epoch": 18.931841302136316,
      "grad_norm": 36.25025939941406,
      "learning_rate": 3.106815869786369e-05,
      "loss": 2.2249,
      "step": 37220
    },
    {
      "epoch": 18.936927772126143,
      "grad_norm": 34.92180252075195,
      "learning_rate": 3.106307222787386e-05,
      "loss": 2.1436,
      "step": 37230
    },
    {
      "epoch": 18.94201424211597,
      "grad_norm": 33.17228698730469,
      "learning_rate": 3.105798575788403e-05,
      "loss": 2.2683,
      "step": 37240
    },
    {
      "epoch": 18.947100712105797,
      "grad_norm": 28.237407684326172,
      "learning_rate": 3.1052899287894206e-05,
      "loss": 2.3215,
      "step": 37250
    },
    {
      "epoch": 18.952187182095624,
      "grad_norm": 32.21083068847656,
      "learning_rate": 3.1047812817904376e-05,
      "loss": 2.1946,
      "step": 37260
    },
    {
      "epoch": 18.95727365208545,
      "grad_norm": 35.601234436035156,
      "learning_rate": 3.1042726347914546e-05,
      "loss": 2.2113,
      "step": 37270
    },
    {
      "epoch": 18.962360122075278,
      "grad_norm": 30.172578811645508,
      "learning_rate": 3.103763987792472e-05,
      "loss": 2.1762,
      "step": 37280
    },
    {
      "epoch": 18.967446592065105,
      "grad_norm": 35.74785232543945,
      "learning_rate": 3.103255340793489e-05,
      "loss": 2.308,
      "step": 37290
    },
    {
      "epoch": 18.972533062054932,
      "grad_norm": 29.72243881225586,
      "learning_rate": 3.102746693794506e-05,
      "loss": 2.2826,
      "step": 37300
    },
    {
      "epoch": 18.977619532044763,
      "grad_norm": 28.470169067382812,
      "learning_rate": 3.102238046795524e-05,
      "loss": 2.1799,
      "step": 37310
    },
    {
      "epoch": 18.98270600203459,
      "grad_norm": 29.03660011291504,
      "learning_rate": 3.1017293997965416e-05,
      "loss": 2.3434,
      "step": 37320
    },
    {
      "epoch": 18.987792472024417,
      "grad_norm": 28.516637802124023,
      "learning_rate": 3.1012207527975585e-05,
      "loss": 2.3156,
      "step": 37330
    },
    {
      "epoch": 18.992878942014244,
      "grad_norm": 34.77800369262695,
      "learning_rate": 3.100712105798576e-05,
      "loss": 2.1799,
      "step": 37340
    },
    {
      "epoch": 18.99796541200407,
      "grad_norm": 30.61395835876465,
      "learning_rate": 3.100203458799593e-05,
      "loss": 2.2023,
      "step": 37350
    },
    {
      "epoch": 19.0,
      "eval_loss": 4.242403030395508,
      "eval_runtime": 2.7451,
      "eval_samples_per_second": 1010.899,
      "eval_steps_per_second": 126.408,
      "step": 37354
    },
    {
      "epoch": 19.003051881993898,
      "grad_norm": 35.67583465576172,
      "learning_rate": 3.09969481180061e-05,
      "loss": 2.2308,
      "step": 37360
    },
    {
      "epoch": 19.008138351983725,
      "grad_norm": 28.979883193969727,
      "learning_rate": 3.099186164801628e-05,
      "loss": 2.1818,
      "step": 37370
    },
    {
      "epoch": 19.01322482197355,
      "grad_norm": 35.83147048950195,
      "learning_rate": 3.098677517802645e-05,
      "loss": 2.1835,
      "step": 37380
    },
    {
      "epoch": 19.01831129196338,
      "grad_norm": 32.00633239746094,
      "learning_rate": 3.098168870803662e-05,
      "loss": 2.1721,
      "step": 37390
    },
    {
      "epoch": 19.023397761953206,
      "grad_norm": 32.8237190246582,
      "learning_rate": 3.0976602238046795e-05,
      "loss": 2.1206,
      "step": 37400
    },
    {
      "epoch": 19.028484231943033,
      "grad_norm": 28.02911949157715,
      "learning_rate": 3.097151576805697e-05,
      "loss": 2.1781,
      "step": 37410
    },
    {
      "epoch": 19.03357070193286,
      "grad_norm": 27.021116256713867,
      "learning_rate": 3.096642929806715e-05,
      "loss": 2.1817,
      "step": 37420
    },
    {
      "epoch": 19.038657171922686,
      "grad_norm": 31.30787467956543,
      "learning_rate": 3.096134282807732e-05,
      "loss": 2.2106,
      "step": 37430
    },
    {
      "epoch": 19.043743641912513,
      "grad_norm": 27.252164840698242,
      "learning_rate": 3.095625635808749e-05,
      "loss": 2.14,
      "step": 37440
    },
    {
      "epoch": 19.04883011190234,
      "grad_norm": 44.76591491699219,
      "learning_rate": 3.0951169888097665e-05,
      "loss": 2.2558,
      "step": 37450
    },
    {
      "epoch": 19.053916581892167,
      "grad_norm": 32.158172607421875,
      "learning_rate": 3.0946083418107835e-05,
      "loss": 2.1832,
      "step": 37460
    },
    {
      "epoch": 19.059003051881994,
      "grad_norm": 34.15754699707031,
      "learning_rate": 3.0940996948118005e-05,
      "loss": 2.0792,
      "step": 37470
    },
    {
      "epoch": 19.06408952187182,
      "grad_norm": 28.95398712158203,
      "learning_rate": 3.093591047812818e-05,
      "loss": 2.1745,
      "step": 37480
    },
    {
      "epoch": 19.06917599186165,
      "grad_norm": 35.88385772705078,
      "learning_rate": 3.093082400813835e-05,
      "loss": 2.2249,
      "step": 37490
    },
    {
      "epoch": 19.074262461851475,
      "grad_norm": 34.98846435546875,
      "learning_rate": 3.092573753814853e-05,
      "loss": 2.2171,
      "step": 37500
    },
    {
      "epoch": 19.079348931841302,
      "grad_norm": 26.908845901489258,
      "learning_rate": 3.0920651068158705e-05,
      "loss": 2.2209,
      "step": 37510
    },
    {
      "epoch": 19.08443540183113,
      "grad_norm": 42.89575958251953,
      "learning_rate": 3.0915564598168874e-05,
      "loss": 2.1507,
      "step": 37520
    },
    {
      "epoch": 19.089521871820956,
      "grad_norm": 31.789403915405273,
      "learning_rate": 3.0910478128179044e-05,
      "loss": 2.1634,
      "step": 37530
    },
    {
      "epoch": 19.094608341810783,
      "grad_norm": 29.981700897216797,
      "learning_rate": 3.090539165818922e-05,
      "loss": 2.1808,
      "step": 37540
    },
    {
      "epoch": 19.09969481180061,
      "grad_norm": 32.64442825317383,
      "learning_rate": 3.090030518819939e-05,
      "loss": 2.1313,
      "step": 37550
    },
    {
      "epoch": 19.104781281790437,
      "grad_norm": 26.882749557495117,
      "learning_rate": 3.089521871820956e-05,
      "loss": 2.2503,
      "step": 37560
    },
    {
      "epoch": 19.109867751780264,
      "grad_norm": 33.59244918823242,
      "learning_rate": 3.089013224821974e-05,
      "loss": 2.1853,
      "step": 37570
    },
    {
      "epoch": 19.11495422177009,
      "grad_norm": 30.723108291625977,
      "learning_rate": 3.088504577822991e-05,
      "loss": 2.1983,
      "step": 37580
    },
    {
      "epoch": 19.120040691759918,
      "grad_norm": 32.39717483520508,
      "learning_rate": 3.0879959308240084e-05,
      "loss": 2.1742,
      "step": 37590
    },
    {
      "epoch": 19.125127161749745,
      "grad_norm": 29.310964584350586,
      "learning_rate": 3.0874872838250254e-05,
      "loss": 2.1887,
      "step": 37600
    },
    {
      "epoch": 19.130213631739572,
      "grad_norm": 25.835020065307617,
      "learning_rate": 3.086978636826043e-05,
      "loss": 2.2209,
      "step": 37610
    },
    {
      "epoch": 19.1353001017294,
      "grad_norm": 38.28053283691406,
      "learning_rate": 3.08646998982706e-05,
      "loss": 2.1962,
      "step": 37620
    },
    {
      "epoch": 19.140386571719226,
      "grad_norm": 31.057437896728516,
      "learning_rate": 3.085961342828078e-05,
      "loss": 2.291,
      "step": 37630
    },
    {
      "epoch": 19.145473041709053,
      "grad_norm": 28.887786865234375,
      "learning_rate": 3.085452695829095e-05,
      "loss": 2.1581,
      "step": 37640
    },
    {
      "epoch": 19.15055951169888,
      "grad_norm": 31.106731414794922,
      "learning_rate": 3.084944048830112e-05,
      "loss": 2.0995,
      "step": 37650
    },
    {
      "epoch": 19.155645981688707,
      "grad_norm": 36.78444290161133,
      "learning_rate": 3.0844354018311294e-05,
      "loss": 2.2303,
      "step": 37660
    },
    {
      "epoch": 19.160732451678534,
      "grad_norm": 35.388511657714844,
      "learning_rate": 3.0839267548321463e-05,
      "loss": 2.2031,
      "step": 37670
    },
    {
      "epoch": 19.16581892166836,
      "grad_norm": 35.08185958862305,
      "learning_rate": 3.083418107833164e-05,
      "loss": 2.1965,
      "step": 37680
    },
    {
      "epoch": 19.170905391658188,
      "grad_norm": 27.86770248413086,
      "learning_rate": 3.082909460834181e-05,
      "loss": 2.2096,
      "step": 37690
    },
    {
      "epoch": 19.175991861648015,
      "grad_norm": 29.26350212097168,
      "learning_rate": 3.082400813835199e-05,
      "loss": 2.2615,
      "step": 37700
    },
    {
      "epoch": 19.181078331637842,
      "grad_norm": 32.16578674316406,
      "learning_rate": 3.0818921668362163e-05,
      "loss": 2.1802,
      "step": 37710
    },
    {
      "epoch": 19.18616480162767,
      "grad_norm": 36.95943069458008,
      "learning_rate": 3.081383519837233e-05,
      "loss": 2.2416,
      "step": 37720
    },
    {
      "epoch": 19.191251271617496,
      "grad_norm": 34.00532150268555,
      "learning_rate": 3.08087487283825e-05,
      "loss": 2.3408,
      "step": 37730
    },
    {
      "epoch": 19.196337741607323,
      "grad_norm": 29.5921688079834,
      "learning_rate": 3.080366225839268e-05,
      "loss": 2.2215,
      "step": 37740
    },
    {
      "epoch": 19.20142421159715,
      "grad_norm": 35.49702072143555,
      "learning_rate": 3.079857578840285e-05,
      "loss": 2.2207,
      "step": 37750
    },
    {
      "epoch": 19.20651068158698,
      "grad_norm": 29.17945098876953,
      "learning_rate": 3.079348931841302e-05,
      "loss": 2.2618,
      "step": 37760
    },
    {
      "epoch": 19.211597151576807,
      "grad_norm": 42.46745681762695,
      "learning_rate": 3.0788402848423196e-05,
      "loss": 2.2034,
      "step": 37770
    },
    {
      "epoch": 19.216683621566634,
      "grad_norm": 28.9754638671875,
      "learning_rate": 3.0783316378433366e-05,
      "loss": 2.2129,
      "step": 37780
    },
    {
      "epoch": 19.22177009155646,
      "grad_norm": 28.70093536376953,
      "learning_rate": 3.077822990844354e-05,
      "loss": 2.3584,
      "step": 37790
    },
    {
      "epoch": 19.22685656154629,
      "grad_norm": 31.910964965820312,
      "learning_rate": 3.077314343845372e-05,
      "loss": 2.2485,
      "step": 37800
    },
    {
      "epoch": 19.231943031536115,
      "grad_norm": 31.366539001464844,
      "learning_rate": 3.076805696846389e-05,
      "loss": 2.1673,
      "step": 37810
    },
    {
      "epoch": 19.237029501525942,
      "grad_norm": 26.175928115844727,
      "learning_rate": 3.076297049847406e-05,
      "loss": 2.2192,
      "step": 37820
    },
    {
      "epoch": 19.24211597151577,
      "grad_norm": 32.18185043334961,
      "learning_rate": 3.0757884028484236e-05,
      "loss": 2.1729,
      "step": 37830
    },
    {
      "epoch": 19.247202441505596,
      "grad_norm": 35.83628463745117,
      "learning_rate": 3.0752797558494406e-05,
      "loss": 2.2391,
      "step": 37840
    },
    {
      "epoch": 19.252288911495423,
      "grad_norm": 24.00373649597168,
      "learning_rate": 3.0747711088504576e-05,
      "loss": 2.2431,
      "step": 37850
    },
    {
      "epoch": 19.25737538148525,
      "grad_norm": 30.142683029174805,
      "learning_rate": 3.074262461851475e-05,
      "loss": 2.2212,
      "step": 37860
    },
    {
      "epoch": 19.262461851475077,
      "grad_norm": 39.097900390625,
      "learning_rate": 3.073753814852492e-05,
      "loss": 2.1213,
      "step": 37870
    },
    {
      "epoch": 19.267548321464904,
      "grad_norm": 32.30960464477539,
      "learning_rate": 3.07324516785351e-05,
      "loss": 2.2536,
      "step": 37880
    },
    {
      "epoch": 19.27263479145473,
      "grad_norm": 26.296300888061523,
      "learning_rate": 3.0727365208545276e-05,
      "loss": 2.1894,
      "step": 37890
    },
    {
      "epoch": 19.277721261444558,
      "grad_norm": 35.521297454833984,
      "learning_rate": 3.0722278738555446e-05,
      "loss": 2.1986,
      "step": 37900
    },
    {
      "epoch": 19.282807731434385,
      "grad_norm": 33.45161056518555,
      "learning_rate": 3.0717192268565615e-05,
      "loss": 2.1654,
      "step": 37910
    },
    {
      "epoch": 19.287894201424212,
      "grad_norm": 31.217912673950195,
      "learning_rate": 3.071210579857579e-05,
      "loss": 2.1591,
      "step": 37920
    },
    {
      "epoch": 19.29298067141404,
      "grad_norm": 24.418535232543945,
      "learning_rate": 3.070701932858596e-05,
      "loss": 2.227,
      "step": 37930
    },
    {
      "epoch": 19.298067141403866,
      "grad_norm": 26.24611473083496,
      "learning_rate": 3.070193285859613e-05,
      "loss": 2.2322,
      "step": 37940
    },
    {
      "epoch": 19.303153611393693,
      "grad_norm": 29.173503875732422,
      "learning_rate": 3.069684638860631e-05,
      "loss": 2.1529,
      "step": 37950
    },
    {
      "epoch": 19.30824008138352,
      "grad_norm": 33.56642532348633,
      "learning_rate": 3.069175991861648e-05,
      "loss": 2.09,
      "step": 37960
    },
    {
      "epoch": 19.313326551373347,
      "grad_norm": 25.670087814331055,
      "learning_rate": 3.0686673448626655e-05,
      "loss": 2.2975,
      "step": 37970
    },
    {
      "epoch": 19.318413021363174,
      "grad_norm": 37.880035400390625,
      "learning_rate": 3.0681586978636825e-05,
      "loss": 2.2724,
      "step": 37980
    },
    {
      "epoch": 19.323499491353,
      "grad_norm": 33.548866271972656,
      "learning_rate": 3.0676500508647e-05,
      "loss": 2.2185,
      "step": 37990
    },
    {
      "epoch": 19.328585961342828,
      "grad_norm": 28.694347381591797,
      "learning_rate": 3.067141403865718e-05,
      "loss": 2.1763,
      "step": 38000
    },
    {
      "epoch": 19.333672431332655,
      "grad_norm": 38.54804992675781,
      "learning_rate": 3.066632756866735e-05,
      "loss": 2.1841,
      "step": 38010
    },
    {
      "epoch": 19.338758901322482,
      "grad_norm": 27.434673309326172,
      "learning_rate": 3.066124109867752e-05,
      "loss": 2.2862,
      "step": 38020
    },
    {
      "epoch": 19.34384537131231,
      "grad_norm": 27.94388771057129,
      "learning_rate": 3.0656154628687695e-05,
      "loss": 2.2671,
      "step": 38030
    },
    {
      "epoch": 19.348931841302136,
      "grad_norm": 39.9749870300293,
      "learning_rate": 3.0651068158697865e-05,
      "loss": 2.1288,
      "step": 38040
    },
    {
      "epoch": 19.354018311291963,
      "grad_norm": 35.588558197021484,
      "learning_rate": 3.0645981688708035e-05,
      "loss": 2.2405,
      "step": 38050
    },
    {
      "epoch": 19.35910478128179,
      "grad_norm": 29.614788055419922,
      "learning_rate": 3.064089521871821e-05,
      "loss": 2.2393,
      "step": 38060
    },
    {
      "epoch": 19.364191251271617,
      "grad_norm": 31.190589904785156,
      "learning_rate": 3.063580874872838e-05,
      "loss": 2.22,
      "step": 38070
    },
    {
      "epoch": 19.369277721261444,
      "grad_norm": 26.80434226989746,
      "learning_rate": 3.063072227873856e-05,
      "loss": 2.2524,
      "step": 38080
    },
    {
      "epoch": 19.37436419125127,
      "grad_norm": 32.600563049316406,
      "learning_rate": 3.0625635808748735e-05,
      "loss": 2.2297,
      "step": 38090
    },
    {
      "epoch": 19.379450661241098,
      "grad_norm": 26.04241371154785,
      "learning_rate": 3.0620549338758904e-05,
      "loss": 2.3185,
      "step": 38100
    },
    {
      "epoch": 19.384537131230925,
      "grad_norm": 28.402158737182617,
      "learning_rate": 3.0615462868769074e-05,
      "loss": 2.2332,
      "step": 38110
    },
    {
      "epoch": 19.38962360122075,
      "grad_norm": 37.06746292114258,
      "learning_rate": 3.061037639877925e-05,
      "loss": 2.1617,
      "step": 38120
    },
    {
      "epoch": 19.39471007121058,
      "grad_norm": 34.19157409667969,
      "learning_rate": 3.060528992878942e-05,
      "loss": 2.1471,
      "step": 38130
    },
    {
      "epoch": 19.399796541200406,
      "grad_norm": 36.192626953125,
      "learning_rate": 3.060020345879959e-05,
      "loss": 2.231,
      "step": 38140
    },
    {
      "epoch": 19.404883011190233,
      "grad_norm": 38.561668395996094,
      "learning_rate": 3.059511698880977e-05,
      "loss": 2.2378,
      "step": 38150
    },
    {
      "epoch": 19.40996948118006,
      "grad_norm": 30.842641830444336,
      "learning_rate": 3.059003051881994e-05,
      "loss": 2.2186,
      "step": 38160
    },
    {
      "epoch": 19.415055951169887,
      "grad_norm": 29.413509368896484,
      "learning_rate": 3.0584944048830114e-05,
      "loss": 2.2371,
      "step": 38170
    },
    {
      "epoch": 19.420142421159714,
      "grad_norm": 34.998504638671875,
      "learning_rate": 3.057985757884029e-05,
      "loss": 2.188,
      "step": 38180
    },
    {
      "epoch": 19.42522889114954,
      "grad_norm": 33.314544677734375,
      "learning_rate": 3.057477110885046e-05,
      "loss": 2.1453,
      "step": 38190
    },
    {
      "epoch": 19.43031536113937,
      "grad_norm": 38.879764556884766,
      "learning_rate": 3.056968463886063e-05,
      "loss": 2.1696,
      "step": 38200
    },
    {
      "epoch": 19.435401831129198,
      "grad_norm": 30.693544387817383,
      "learning_rate": 3.056459816887081e-05,
      "loss": 2.2076,
      "step": 38210
    },
    {
      "epoch": 19.440488301119025,
      "grad_norm": 34.06538009643555,
      "learning_rate": 3.055951169888098e-05,
      "loss": 2.2427,
      "step": 38220
    },
    {
      "epoch": 19.445574771108852,
      "grad_norm": 35.116817474365234,
      "learning_rate": 3.0554425228891154e-05,
      "loss": 2.1383,
      "step": 38230
    },
    {
      "epoch": 19.45066124109868,
      "grad_norm": 25.440584182739258,
      "learning_rate": 3.0549338758901324e-05,
      "loss": 2.2376,
      "step": 38240
    },
    {
      "epoch": 19.455747711088506,
      "grad_norm": 34.55621337890625,
      "learning_rate": 3.0544252288911494e-05,
      "loss": 2.1924,
      "step": 38250
    },
    {
      "epoch": 19.460834181078333,
      "grad_norm": 32.271549224853516,
      "learning_rate": 3.053916581892167e-05,
      "loss": 2.2313,
      "step": 38260
    },
    {
      "epoch": 19.46592065106816,
      "grad_norm": 39.12033462524414,
      "learning_rate": 3.053407934893184e-05,
      "loss": 2.2347,
      "step": 38270
    },
    {
      "epoch": 19.471007121057987,
      "grad_norm": 27.127809524536133,
      "learning_rate": 3.052899287894202e-05,
      "loss": 2.1842,
      "step": 38280
    },
    {
      "epoch": 19.476093591047814,
      "grad_norm": 29.618675231933594,
      "learning_rate": 3.0523906408952193e-05,
      "loss": 2.1804,
      "step": 38290
    },
    {
      "epoch": 19.48118006103764,
      "grad_norm": 35.65815353393555,
      "learning_rate": 3.051881993896236e-05,
      "loss": 2.2163,
      "step": 38300
    },
    {
      "epoch": 19.486266531027468,
      "grad_norm": 34.30446243286133,
      "learning_rate": 3.0513733468972533e-05,
      "loss": 2.2091,
      "step": 38310
    },
    {
      "epoch": 19.491353001017295,
      "grad_norm": 36.3190803527832,
      "learning_rate": 3.050864699898271e-05,
      "loss": 2.1873,
      "step": 38320
    },
    {
      "epoch": 19.496439471007122,
      "grad_norm": 39.269405364990234,
      "learning_rate": 3.050356052899288e-05,
      "loss": 2.1816,
      "step": 38330
    },
    {
      "epoch": 19.50152594099695,
      "grad_norm": 29.51306915283203,
      "learning_rate": 3.0498474059003053e-05,
      "loss": 2.1452,
      "step": 38340
    },
    {
      "epoch": 19.506612410986776,
      "grad_norm": 31.851490020751953,
      "learning_rate": 3.049338758901323e-05,
      "loss": 2.1497,
      "step": 38350
    },
    {
      "epoch": 19.511698880976603,
      "grad_norm": 33.5982780456543,
      "learning_rate": 3.04883011190234e-05,
      "loss": 2.2599,
      "step": 38360
    },
    {
      "epoch": 19.51678535096643,
      "grad_norm": 39.45866012573242,
      "learning_rate": 3.048321464903357e-05,
      "loss": 2.2157,
      "step": 38370
    },
    {
      "epoch": 19.521871820956257,
      "grad_norm": 27.28255844116211,
      "learning_rate": 3.0478128179043746e-05,
      "loss": 2.2524,
      "step": 38380
    },
    {
      "epoch": 19.526958290946084,
      "grad_norm": 25.055862426757812,
      "learning_rate": 3.0473041709053916e-05,
      "loss": 2.097,
      "step": 38390
    },
    {
      "epoch": 19.53204476093591,
      "grad_norm": 21.840456008911133,
      "learning_rate": 3.046795523906409e-05,
      "loss": 2.1877,
      "step": 38400
    },
    {
      "epoch": 19.537131230925738,
      "grad_norm": 28.878978729248047,
      "learning_rate": 3.0462868769074266e-05,
      "loss": 2.2285,
      "step": 38410
    },
    {
      "epoch": 19.542217700915565,
      "grad_norm": 30.55763053894043,
      "learning_rate": 3.0457782299084436e-05,
      "loss": 2.2524,
      "step": 38420
    },
    {
      "epoch": 19.54730417090539,
      "grad_norm": 39.74020004272461,
      "learning_rate": 3.0452695829094606e-05,
      "loss": 2.142,
      "step": 38430
    },
    {
      "epoch": 19.55239064089522,
      "grad_norm": 33.32891845703125,
      "learning_rate": 3.0447609359104782e-05,
      "loss": 2.2175,
      "step": 38440
    },
    {
      "epoch": 19.557477110885046,
      "grad_norm": 32.65008544921875,
      "learning_rate": 3.0442522889114956e-05,
      "loss": 2.2257,
      "step": 38450
    },
    {
      "epoch": 19.562563580874873,
      "grad_norm": 31.3526668548584,
      "learning_rate": 3.0437436419125126e-05,
      "loss": 2.2217,
      "step": 38460
    },
    {
      "epoch": 19.5676500508647,
      "grad_norm": 29.81949806213379,
      "learning_rate": 3.0432349949135302e-05,
      "loss": 2.1862,
      "step": 38470
    },
    {
      "epoch": 19.572736520854527,
      "grad_norm": 32.31312942504883,
      "learning_rate": 3.0427263479145472e-05,
      "loss": 2.2298,
      "step": 38480
    },
    {
      "epoch": 19.577822990844354,
      "grad_norm": 31.289016723632812,
      "learning_rate": 3.042217700915565e-05,
      "loss": 2.1493,
      "step": 38490
    },
    {
      "epoch": 19.58290946083418,
      "grad_norm": 35.45064926147461,
      "learning_rate": 3.0417090539165822e-05,
      "loss": 2.1868,
      "step": 38500
    },
    {
      "epoch": 19.587995930824007,
      "grad_norm": 27.878067016601562,
      "learning_rate": 3.0412004069175992e-05,
      "loss": 2.2188,
      "step": 38510
    },
    {
      "epoch": 19.593082400813834,
      "grad_norm": 35.985294342041016,
      "learning_rate": 3.040691759918617e-05,
      "loss": 2.1251,
      "step": 38520
    },
    {
      "epoch": 19.59816887080366,
      "grad_norm": 30.36426544189453,
      "learning_rate": 3.040183112919634e-05,
      "loss": 2.1436,
      "step": 38530
    },
    {
      "epoch": 19.60325534079349,
      "grad_norm": 32.24137496948242,
      "learning_rate": 3.0396744659206512e-05,
      "loss": 2.2042,
      "step": 38540
    },
    {
      "epoch": 19.608341810783315,
      "grad_norm": 34.25721740722656,
      "learning_rate": 3.039165818921669e-05,
      "loss": 2.1742,
      "step": 38550
    },
    {
      "epoch": 19.613428280773142,
      "grad_norm": 29.407081604003906,
      "learning_rate": 3.038657171922686e-05,
      "loss": 2.2006,
      "step": 38560
    },
    {
      "epoch": 19.61851475076297,
      "grad_norm": 35.74137878417969,
      "learning_rate": 3.038148524923703e-05,
      "loss": 2.2028,
      "step": 38570
    },
    {
      "epoch": 19.623601220752796,
      "grad_norm": 33.44921875,
      "learning_rate": 3.0376398779247205e-05,
      "loss": 2.1838,
      "step": 38580
    },
    {
      "epoch": 19.628687690742623,
      "grad_norm": 35.002227783203125,
      "learning_rate": 3.0371312309257378e-05,
      "loss": 2.2971,
      "step": 38590
    },
    {
      "epoch": 19.63377416073245,
      "grad_norm": 39.34737777709961,
      "learning_rate": 3.0366225839267548e-05,
      "loss": 2.2922,
      "step": 38600
    },
    {
      "epoch": 19.638860630722277,
      "grad_norm": 32.46263885498047,
      "learning_rate": 3.0361139369277725e-05,
      "loss": 2.2452,
      "step": 38610
    },
    {
      "epoch": 19.643947100712104,
      "grad_norm": 35.23108673095703,
      "learning_rate": 3.0356052899287895e-05,
      "loss": 2.1957,
      "step": 38620
    },
    {
      "epoch": 19.64903357070193,
      "grad_norm": 32.24105453491211,
      "learning_rate": 3.0350966429298068e-05,
      "loss": 2.1908,
      "step": 38630
    },
    {
      "epoch": 19.654120040691758,
      "grad_norm": 41.617958068847656,
      "learning_rate": 3.0345879959308245e-05,
      "loss": 2.2214,
      "step": 38640
    },
    {
      "epoch": 19.659206510681585,
      "grad_norm": 36.429473876953125,
      "learning_rate": 3.0340793489318415e-05,
      "loss": 2.1635,
      "step": 38650
    },
    {
      "epoch": 19.664292980671416,
      "grad_norm": 27.507333755493164,
      "learning_rate": 3.0335707019328584e-05,
      "loss": 2.1972,
      "step": 38660
    },
    {
      "epoch": 19.669379450661243,
      "grad_norm": 30.346784591674805,
      "learning_rate": 3.033062054933876e-05,
      "loss": 2.187,
      "step": 38670
    },
    {
      "epoch": 19.67446592065107,
      "grad_norm": 31.098060607910156,
      "learning_rate": 3.0325534079348934e-05,
      "loss": 2.2148,
      "step": 38680
    },
    {
      "epoch": 19.679552390640897,
      "grad_norm": 29.055049896240234,
      "learning_rate": 3.0320447609359104e-05,
      "loss": 2.1885,
      "step": 38690
    },
    {
      "epoch": 19.684638860630724,
      "grad_norm": 26.88352394104004,
      "learning_rate": 3.031536113936928e-05,
      "loss": 2.268,
      "step": 38700
    },
    {
      "epoch": 19.68972533062055,
      "grad_norm": 33.76780700683594,
      "learning_rate": 3.031027466937945e-05,
      "loss": 2.2298,
      "step": 38710
    },
    {
      "epoch": 19.694811800610378,
      "grad_norm": 33.0426139831543,
      "learning_rate": 3.0305188199389624e-05,
      "loss": 2.1856,
      "step": 38720
    },
    {
      "epoch": 19.699898270600205,
      "grad_norm": 33.746089935302734,
      "learning_rate": 3.0300101729399797e-05,
      "loss": 2.2058,
      "step": 38730
    },
    {
      "epoch": 19.70498474059003,
      "grad_norm": 35.64166259765625,
      "learning_rate": 3.029501525940997e-05,
      "loss": 2.1898,
      "step": 38740
    },
    {
      "epoch": 19.71007121057986,
      "grad_norm": 28.62327003479004,
      "learning_rate": 3.028992878942014e-05,
      "loss": 2.231,
      "step": 38750
    },
    {
      "epoch": 19.715157680569686,
      "grad_norm": 38.787628173828125,
      "learning_rate": 3.0284842319430317e-05,
      "loss": 2.2423,
      "step": 38760
    },
    {
      "epoch": 19.720244150559513,
      "grad_norm": 35.791542053222656,
      "learning_rate": 3.0279755849440487e-05,
      "loss": 2.2185,
      "step": 38770
    },
    {
      "epoch": 19.72533062054934,
      "grad_norm": 25.054058074951172,
      "learning_rate": 3.0274669379450664e-05,
      "loss": 2.3132,
      "step": 38780
    },
    {
      "epoch": 19.730417090539166,
      "grad_norm": 35.68856430053711,
      "learning_rate": 3.0269582909460837e-05,
      "loss": 2.1693,
      "step": 38790
    },
    {
      "epoch": 19.735503560528993,
      "grad_norm": 28.68145751953125,
      "learning_rate": 3.0264496439471007e-05,
      "loss": 2.3435,
      "step": 38800
    },
    {
      "epoch": 19.74059003051882,
      "grad_norm": 33.56260299682617,
      "learning_rate": 3.0259409969481184e-05,
      "loss": 2.1517,
      "step": 38810
    },
    {
      "epoch": 19.745676500508647,
      "grad_norm": 31.213045120239258,
      "learning_rate": 3.0254323499491354e-05,
      "loss": 2.1721,
      "step": 38820
    },
    {
      "epoch": 19.750762970498474,
      "grad_norm": 29.86954689025879,
      "learning_rate": 3.0249237029501527e-05,
      "loss": 2.2336,
      "step": 38830
    },
    {
      "epoch": 19.7558494404883,
      "grad_norm": 33.431304931640625,
      "learning_rate": 3.0244150559511704e-05,
      "loss": 2.1233,
      "step": 38840
    },
    {
      "epoch": 19.76093591047813,
      "grad_norm": 28.294769287109375,
      "learning_rate": 3.0239064089521873e-05,
      "loss": 2.1306,
      "step": 38850
    },
    {
      "epoch": 19.766022380467955,
      "grad_norm": 39.11708068847656,
      "learning_rate": 3.0233977619532043e-05,
      "loss": 2.1735,
      "step": 38860
    },
    {
      "epoch": 19.771108850457782,
      "grad_norm": 33.34193420410156,
      "learning_rate": 3.022889114954222e-05,
      "loss": 2.1514,
      "step": 38870
    },
    {
      "epoch": 19.77619532044761,
      "grad_norm": 33.3855094909668,
      "learning_rate": 3.0223804679552393e-05,
      "loss": 2.1631,
      "step": 38880
    },
    {
      "epoch": 19.781281790437436,
      "grad_norm": 37.8277473449707,
      "learning_rate": 3.0218718209562563e-05,
      "loss": 2.1,
      "step": 38890
    },
    {
      "epoch": 19.786368260427263,
      "grad_norm": 33.07523727416992,
      "learning_rate": 3.021363173957274e-05,
      "loss": 2.1547,
      "step": 38900
    },
    {
      "epoch": 19.79145473041709,
      "grad_norm": 31.73186492919922,
      "learning_rate": 3.020854526958291e-05,
      "loss": 2.1888,
      "step": 38910
    },
    {
      "epoch": 19.796541200406917,
      "grad_norm": 34.145912170410156,
      "learning_rate": 3.0203458799593083e-05,
      "loss": 2.2091,
      "step": 38920
    },
    {
      "epoch": 19.801627670396744,
      "grad_norm": 29.746864318847656,
      "learning_rate": 3.019837232960326e-05,
      "loss": 2.2483,
      "step": 38930
    },
    {
      "epoch": 19.80671414038657,
      "grad_norm": 37.200626373291016,
      "learning_rate": 3.019328585961343e-05,
      "loss": 2.2033,
      "step": 38940
    },
    {
      "epoch": 19.811800610376398,
      "grad_norm": 33.620155334472656,
      "learning_rate": 3.01881993896236e-05,
      "loss": 2.2595,
      "step": 38950
    },
    {
      "epoch": 19.816887080366225,
      "grad_norm": 35.559146881103516,
      "learning_rate": 3.0183112919633776e-05,
      "loss": 2.1407,
      "step": 38960
    },
    {
      "epoch": 19.821973550356052,
      "grad_norm": 32.58271408081055,
      "learning_rate": 3.017802644964395e-05,
      "loss": 2.2368,
      "step": 38970
    },
    {
      "epoch": 19.82706002034588,
      "grad_norm": 28.3833065032959,
      "learning_rate": 3.017293997965412e-05,
      "loss": 2.1902,
      "step": 38980
    },
    {
      "epoch": 19.832146490335706,
      "grad_norm": 31.70928192138672,
      "learning_rate": 3.0167853509664296e-05,
      "loss": 2.1619,
      "step": 38990
    },
    {
      "epoch": 19.837232960325533,
      "grad_norm": 46.09169387817383,
      "learning_rate": 3.0162767039674466e-05,
      "loss": 2.1176,
      "step": 39000
    },
    {
      "epoch": 19.84231943031536,
      "grad_norm": 30.013532638549805,
      "learning_rate": 3.015768056968464e-05,
      "loss": 2.0904,
      "step": 39010
    },
    {
      "epoch": 19.847405900305187,
      "grad_norm": 32.99589920043945,
      "learning_rate": 3.0152594099694816e-05,
      "loss": 2.096,
      "step": 39020
    },
    {
      "epoch": 19.852492370295014,
      "grad_norm": 28.899417877197266,
      "learning_rate": 3.0147507629704986e-05,
      "loss": 2.0494,
      "step": 39030
    },
    {
      "epoch": 19.85757884028484,
      "grad_norm": 36.870086669921875,
      "learning_rate": 3.0142421159715162e-05,
      "loss": 2.1924,
      "step": 39040
    },
    {
      "epoch": 19.862665310274668,
      "grad_norm": 38.81496810913086,
      "learning_rate": 3.0137334689725332e-05,
      "loss": 2.1146,
      "step": 39050
    },
    {
      "epoch": 19.867751780264495,
      "grad_norm": 52.16096496582031,
      "learning_rate": 3.0132248219735502e-05,
      "loss": 2.1518,
      "step": 39060
    },
    {
      "epoch": 19.872838250254322,
      "grad_norm": 29.5759334564209,
      "learning_rate": 3.012716174974568e-05,
      "loss": 2.2509,
      "step": 39070
    },
    {
      "epoch": 19.87792472024415,
      "grad_norm": 29.668794631958008,
      "learning_rate": 3.0122075279755852e-05,
      "loss": 2.2351,
      "step": 39080
    },
    {
      "epoch": 19.88301119023398,
      "grad_norm": 48.02509307861328,
      "learning_rate": 3.0116988809766022e-05,
      "loss": 2.2757,
      "step": 39090
    },
    {
      "epoch": 19.888097660223806,
      "grad_norm": 30.60219383239746,
      "learning_rate": 3.01119023397762e-05,
      "loss": 2.1839,
      "step": 39100
    },
    {
      "epoch": 19.893184130213633,
      "grad_norm": 29.018259048461914,
      "learning_rate": 3.010681586978637e-05,
      "loss": 2.0672,
      "step": 39110
    },
    {
      "epoch": 19.89827060020346,
      "grad_norm": 30.940095901489258,
      "learning_rate": 3.0101729399796542e-05,
      "loss": 2.1945,
      "step": 39120
    },
    {
      "epoch": 19.903357070193287,
      "grad_norm": 42.20103454589844,
      "learning_rate": 3.009664292980672e-05,
      "loss": 2.1515,
      "step": 39130
    },
    {
      "epoch": 19.908443540183114,
      "grad_norm": 36.65021514892578,
      "learning_rate": 3.009155645981689e-05,
      "loss": 2.2846,
      "step": 39140
    },
    {
      "epoch": 19.91353001017294,
      "grad_norm": 26.050905227661133,
      "learning_rate": 3.008646998982706e-05,
      "loss": 2.2489,
      "step": 39150
    },
    {
      "epoch": 19.91861648016277,
      "grad_norm": 34.499691009521484,
      "learning_rate": 3.0081383519837235e-05,
      "loss": 2.2017,
      "step": 39160
    },
    {
      "epoch": 19.923702950152595,
      "grad_norm": 30.589807510375977,
      "learning_rate": 3.007629704984741e-05,
      "loss": 2.1693,
      "step": 39170
    },
    {
      "epoch": 19.928789420142422,
      "grad_norm": 29.77852439880371,
      "learning_rate": 3.0071210579857578e-05,
      "loss": 2.1822,
      "step": 39180
    },
    {
      "epoch": 19.93387589013225,
      "grad_norm": 27.693687438964844,
      "learning_rate": 3.0066124109867755e-05,
      "loss": 2.0469,
      "step": 39190
    },
    {
      "epoch": 19.938962360122076,
      "grad_norm": 46.8339958190918,
      "learning_rate": 3.0061037639877925e-05,
      "loss": 2.2236,
      "step": 39200
    },
    {
      "epoch": 19.944048830111903,
      "grad_norm": 34.97294616699219,
      "learning_rate": 3.0055951169888098e-05,
      "loss": 2.1685,
      "step": 39210
    },
    {
      "epoch": 19.94913530010173,
      "grad_norm": 30.678844451904297,
      "learning_rate": 3.0050864699898275e-05,
      "loss": 2.2072,
      "step": 39220
    },
    {
      "epoch": 19.954221770091557,
      "grad_norm": 32.53857421875,
      "learning_rate": 3.0045778229908445e-05,
      "loss": 2.1672,
      "step": 39230
    },
    {
      "epoch": 19.959308240081384,
      "grad_norm": 28.520238876342773,
      "learning_rate": 3.0040691759918615e-05,
      "loss": 2.1979,
      "step": 39240
    },
    {
      "epoch": 19.96439471007121,
      "grad_norm": 31.87737274169922,
      "learning_rate": 3.003560528992879e-05,
      "loss": 2.1447,
      "step": 39250
    },
    {
      "epoch": 19.969481180061038,
      "grad_norm": 28.26153564453125,
      "learning_rate": 3.0030518819938964e-05,
      "loss": 2.1574,
      "step": 39260
    },
    {
      "epoch": 19.974567650050865,
      "grad_norm": 30.11377716064453,
      "learning_rate": 3.0025432349949134e-05,
      "loss": 2.157,
      "step": 39270
    },
    {
      "epoch": 19.979654120040692,
      "grad_norm": 30.294828414916992,
      "learning_rate": 3.002034587995931e-05,
      "loss": 2.2214,
      "step": 39280
    },
    {
      "epoch": 19.98474059003052,
      "grad_norm": 26.346521377563477,
      "learning_rate": 3.001525940996948e-05,
      "loss": 2.1637,
      "step": 39290
    },
    {
      "epoch": 19.989827060020346,
      "grad_norm": 36.636962890625,
      "learning_rate": 3.0010172939979658e-05,
      "loss": 2.23,
      "step": 39300
    },
    {
      "epoch": 19.994913530010173,
      "grad_norm": 28.721548080444336,
      "learning_rate": 3.000508646998983e-05,
      "loss": 2.233,
      "step": 39310
    },
    {
      "epoch": 20.0,
      "grad_norm": 41.27714538574219,
      "learning_rate": 3e-05,
      "loss": 2.2201,
      "step": 39320
    },
    {
      "epoch": 20.0,
      "eval_loss": 4.310016632080078,
      "eval_runtime": 2.7134,
      "eval_samples_per_second": 1022.712,
      "eval_steps_per_second": 127.885,
      "step": 39320
    },
    {
      "epoch": 20.005086469989827,
      "grad_norm": 28.997726440429688,
      "learning_rate": 2.9994913530010177e-05,
      "loss": 2.1421,
      "step": 39330
    },
    {
      "epoch": 20.010172939979654,
      "grad_norm": 30.19815444946289,
      "learning_rate": 2.9989827060020347e-05,
      "loss": 2.2044,
      "step": 39340
    },
    {
      "epoch": 20.01525940996948,
      "grad_norm": 32.2803955078125,
      "learning_rate": 2.998474059003052e-05,
      "loss": 2.1536,
      "step": 39350
    },
    {
      "epoch": 20.020345879959308,
      "grad_norm": 27.340646743774414,
      "learning_rate": 2.9979654120040694e-05,
      "loss": 2.1161,
      "step": 39360
    },
    {
      "epoch": 20.025432349949135,
      "grad_norm": 33.884063720703125,
      "learning_rate": 2.9974567650050867e-05,
      "loss": 2.1383,
      "step": 39370
    },
    {
      "epoch": 20.030518819938962,
      "grad_norm": 31.174955368041992,
      "learning_rate": 2.9969481180061037e-05,
      "loss": 2.2237,
      "step": 39380
    },
    {
      "epoch": 20.03560528992879,
      "grad_norm": 32.9777717590332,
      "learning_rate": 2.9964394710071214e-05,
      "loss": 2.25,
      "step": 39390
    },
    {
      "epoch": 20.040691759918616,
      "grad_norm": 38.772830963134766,
      "learning_rate": 2.9959308240081384e-05,
      "loss": 2.2308,
      "step": 39400
    },
    {
      "epoch": 20.045778229908443,
      "grad_norm": 41.72845458984375,
      "learning_rate": 2.9954221770091557e-05,
      "loss": 2.1903,
      "step": 39410
    },
    {
      "epoch": 20.05086469989827,
      "grad_norm": 38.91363525390625,
      "learning_rate": 2.9949135300101734e-05,
      "loss": 2.1863,
      "step": 39420
    },
    {
      "epoch": 20.055951169888097,
      "grad_norm": 33.08799362182617,
      "learning_rate": 2.9944048830111903e-05,
      "loss": 2.2393,
      "step": 39430
    },
    {
      "epoch": 20.061037639877924,
      "grad_norm": 41.023807525634766,
      "learning_rate": 2.9938962360122073e-05,
      "loss": 2.158,
      "step": 39440
    },
    {
      "epoch": 20.06612410986775,
      "grad_norm": 31.22499656677246,
      "learning_rate": 2.993387589013225e-05,
      "loss": 2.0586,
      "step": 39450
    },
    {
      "epoch": 20.071210579857578,
      "grad_norm": 29.05055046081543,
      "learning_rate": 2.9928789420142423e-05,
      "loss": 2.1301,
      "step": 39460
    },
    {
      "epoch": 20.076297049847405,
      "grad_norm": 28.022342681884766,
      "learning_rate": 2.9923702950152593e-05,
      "loss": 2.1633,
      "step": 39470
    },
    {
      "epoch": 20.08138351983723,
      "grad_norm": 28.160503387451172,
      "learning_rate": 2.991861648016277e-05,
      "loss": 2.1683,
      "step": 39480
    },
    {
      "epoch": 20.08646998982706,
      "grad_norm": 30.285484313964844,
      "learning_rate": 2.991353001017294e-05,
      "loss": 2.1728,
      "step": 39490
    },
    {
      "epoch": 20.091556459816886,
      "grad_norm": 38.7761116027832,
      "learning_rate": 2.9908443540183113e-05,
      "loss": 2.2167,
      "step": 39500
    },
    {
      "epoch": 20.096642929806713,
      "grad_norm": 35.228092193603516,
      "learning_rate": 2.990335707019329e-05,
      "loss": 2.156,
      "step": 39510
    },
    {
      "epoch": 20.10172939979654,
      "grad_norm": 32.82669448852539,
      "learning_rate": 2.989827060020346e-05,
      "loss": 2.0837,
      "step": 39520
    },
    {
      "epoch": 20.106815869786367,
      "grad_norm": 31.454931259155273,
      "learning_rate": 2.989318413021363e-05,
      "loss": 2.1581,
      "step": 39530
    },
    {
      "epoch": 20.111902339776194,
      "grad_norm": 32.51541519165039,
      "learning_rate": 2.9888097660223806e-05,
      "loss": 2.2258,
      "step": 39540
    },
    {
      "epoch": 20.116988809766024,
      "grad_norm": 36.30036163330078,
      "learning_rate": 2.988301119023398e-05,
      "loss": 2.173,
      "step": 39550
    },
    {
      "epoch": 20.12207527975585,
      "grad_norm": 30.35230827331543,
      "learning_rate": 2.987792472024415e-05,
      "loss": 1.988,
      "step": 39560
    },
    {
      "epoch": 20.127161749745678,
      "grad_norm": 31.230363845825195,
      "learning_rate": 2.9872838250254326e-05,
      "loss": 2.1093,
      "step": 39570
    },
    {
      "epoch": 20.132248219735505,
      "grad_norm": 39.27633285522461,
      "learning_rate": 2.9867751780264496e-05,
      "loss": 2.1908,
      "step": 39580
    },
    {
      "epoch": 20.137334689725332,
      "grad_norm": 40.50791931152344,
      "learning_rate": 2.9862665310274673e-05,
      "loss": 2.1896,
      "step": 39590
    },
    {
      "epoch": 20.14242115971516,
      "grad_norm": 40.870323181152344,
      "learning_rate": 2.9857578840284846e-05,
      "loss": 2.1649,
      "step": 39600
    },
    {
      "epoch": 20.147507629704986,
      "grad_norm": 31.428241729736328,
      "learning_rate": 2.9852492370295016e-05,
      "loss": 2.1787,
      "step": 39610
    },
    {
      "epoch": 20.152594099694813,
      "grad_norm": 32.44557189941406,
      "learning_rate": 2.9847405900305192e-05,
      "loss": 2.0805,
      "step": 39620
    },
    {
      "epoch": 20.15768056968464,
      "grad_norm": 33.573272705078125,
      "learning_rate": 2.9842319430315362e-05,
      "loss": 2.1049,
      "step": 39630
    },
    {
      "epoch": 20.162767039674467,
      "grad_norm": 33.26456069946289,
      "learning_rate": 2.9837232960325536e-05,
      "loss": 2.1431,
      "step": 39640
    },
    {
      "epoch": 20.167853509664294,
      "grad_norm": 33.44924545288086,
      "learning_rate": 2.9832146490335712e-05,
      "loss": 2.1406,
      "step": 39650
    },
    {
      "epoch": 20.17293997965412,
      "grad_norm": 32.16496658325195,
      "learning_rate": 2.9827060020345882e-05,
      "loss": 2.1427,
      "step": 39660
    },
    {
      "epoch": 20.178026449643948,
      "grad_norm": 31.33477783203125,
      "learning_rate": 2.9821973550356052e-05,
      "loss": 2.2132,
      "step": 39670
    },
    {
      "epoch": 20.183112919633775,
      "grad_norm": 28.264909744262695,
      "learning_rate": 2.981688708036623e-05,
      "loss": 2.1415,
      "step": 39680
    },
    {
      "epoch": 20.188199389623602,
      "grad_norm": 29.47553253173828,
      "learning_rate": 2.98118006103764e-05,
      "loss": 2.1842,
      "step": 39690
    },
    {
      "epoch": 20.19328585961343,
      "grad_norm": 24.628774642944336,
      "learning_rate": 2.9806714140386572e-05,
      "loss": 2.127,
      "step": 39700
    },
    {
      "epoch": 20.198372329603256,
      "grad_norm": 37.15328598022461,
      "learning_rate": 2.980162767039675e-05,
      "loss": 2.2454,
      "step": 39710
    },
    {
      "epoch": 20.203458799593083,
      "grad_norm": 34.57984161376953,
      "learning_rate": 2.979654120040692e-05,
      "loss": 2.1065,
      "step": 39720
    },
    {
      "epoch": 20.20854526958291,
      "grad_norm": 29.82302474975586,
      "learning_rate": 2.979145473041709e-05,
      "loss": 2.2083,
      "step": 39730
    },
    {
      "epoch": 20.213631739572737,
      "grad_norm": 29.707969665527344,
      "learning_rate": 2.9786368260427265e-05,
      "loss": 2.2399,
      "step": 39740
    },
    {
      "epoch": 20.218718209562564,
      "grad_norm": 30.615808486938477,
      "learning_rate": 2.978128179043744e-05,
      "loss": 2.1782,
      "step": 39750
    },
    {
      "epoch": 20.22380467955239,
      "grad_norm": 40.19190216064453,
      "learning_rate": 2.9776195320447608e-05,
      "loss": 2.2554,
      "step": 39760
    },
    {
      "epoch": 20.228891149542218,
      "grad_norm": 34.11509323120117,
      "learning_rate": 2.9771108850457785e-05,
      "loss": 2.199,
      "step": 39770
    },
    {
      "epoch": 20.233977619532045,
      "grad_norm": 35.17177963256836,
      "learning_rate": 2.9766022380467955e-05,
      "loss": 2.112,
      "step": 39780
    },
    {
      "epoch": 20.23906408952187,
      "grad_norm": 30.880977630615234,
      "learning_rate": 2.9760935910478128e-05,
      "loss": 2.1499,
      "step": 39790
    },
    {
      "epoch": 20.2441505595117,
      "grad_norm": 32.56714630126953,
      "learning_rate": 2.9755849440488305e-05,
      "loss": 2.2107,
      "step": 39800
    },
    {
      "epoch": 20.249237029501526,
      "grad_norm": 39.87312316894531,
      "learning_rate": 2.9750762970498475e-05,
      "loss": 2.1478,
      "step": 39810
    },
    {
      "epoch": 20.254323499491353,
      "grad_norm": 39.79527282714844,
      "learning_rate": 2.9745676500508645e-05,
      "loss": 2.1641,
      "step": 39820
    },
    {
      "epoch": 20.25940996948118,
      "grad_norm": 37.313316345214844,
      "learning_rate": 2.974059003051882e-05,
      "loss": 2.1139,
      "step": 39830
    },
    {
      "epoch": 20.264496439471007,
      "grad_norm": 31.207773208618164,
      "learning_rate": 2.9735503560528994e-05,
      "loss": 2.223,
      "step": 39840
    },
    {
      "epoch": 20.269582909460834,
      "grad_norm": 29.627071380615234,
      "learning_rate": 2.973041709053917e-05,
      "loss": 2.1949,
      "step": 39850
    },
    {
      "epoch": 20.27466937945066,
      "grad_norm": 36.437252044677734,
      "learning_rate": 2.972533062054934e-05,
      "loss": 2.1902,
      "step": 39860
    },
    {
      "epoch": 20.279755849440487,
      "grad_norm": 33.09086608886719,
      "learning_rate": 2.972024415055951e-05,
      "loss": 2.0908,
      "step": 39870
    },
    {
      "epoch": 20.284842319430314,
      "grad_norm": 41.13917541503906,
      "learning_rate": 2.9715157680569688e-05,
      "loss": 2.0065,
      "step": 39880
    },
    {
      "epoch": 20.28992878942014,
      "grad_norm": 35.0848274230957,
      "learning_rate": 2.971007121057986e-05,
      "loss": 2.0934,
      "step": 39890
    },
    {
      "epoch": 20.29501525940997,
      "grad_norm": 27.276643753051758,
      "learning_rate": 2.970498474059003e-05,
      "loss": 2.0269,
      "step": 39900
    },
    {
      "epoch": 20.300101729399795,
      "grad_norm": 37.5555534362793,
      "learning_rate": 2.9699898270600207e-05,
      "loss": 2.2351,
      "step": 39910
    },
    {
      "epoch": 20.305188199389622,
      "grad_norm": 37.919185638427734,
      "learning_rate": 2.9694811800610377e-05,
      "loss": 2.1456,
      "step": 39920
    },
    {
      "epoch": 20.31027466937945,
      "grad_norm": 33.53733444213867,
      "learning_rate": 2.968972533062055e-05,
      "loss": 2.1632,
      "step": 39930
    },
    {
      "epoch": 20.315361139369276,
      "grad_norm": 33.54396057128906,
      "learning_rate": 2.9684638860630727e-05,
      "loss": 2.1602,
      "step": 39940
    },
    {
      "epoch": 20.320447609359103,
      "grad_norm": 29.811189651489258,
      "learning_rate": 2.9679552390640897e-05,
      "loss": 2.1327,
      "step": 39950
    },
    {
      "epoch": 20.32553407934893,
      "grad_norm": 32.91393280029297,
      "learning_rate": 2.9674465920651067e-05,
      "loss": 2.1286,
      "step": 39960
    },
    {
      "epoch": 20.330620549338757,
      "grad_norm": 39.61063003540039,
      "learning_rate": 2.9669379450661244e-05,
      "loss": 2.1957,
      "step": 39970
    },
    {
      "epoch": 20.335707019328584,
      "grad_norm": 27.453506469726562,
      "learning_rate": 2.9664292980671417e-05,
      "loss": 2.0867,
      "step": 39980
    },
    {
      "epoch": 20.340793489318415,
      "grad_norm": 34.96903610229492,
      "learning_rate": 2.9659206510681587e-05,
      "loss": 2.0431,
      "step": 39990
    },
    {
      "epoch": 20.345879959308242,
      "grad_norm": 41.30762481689453,
      "learning_rate": 2.9654120040691764e-05,
      "loss": 2.1728,
      "step": 40000
    },
    {
      "epoch": 20.35096642929807,
      "grad_norm": 31.75385856628418,
      "learning_rate": 2.9649033570701933e-05,
      "loss": 2.2132,
      "step": 40010
    },
    {
      "epoch": 20.356052899287896,
      "grad_norm": 29.841373443603516,
      "learning_rate": 2.9643947100712103e-05,
      "loss": 2.1988,
      "step": 40020
    },
    {
      "epoch": 20.361139369277723,
      "grad_norm": 29.037033081054688,
      "learning_rate": 2.963886063072228e-05,
      "loss": 2.1182,
      "step": 40030
    },
    {
      "epoch": 20.36622583926755,
      "grad_norm": 30.69029998779297,
      "learning_rate": 2.9633774160732453e-05,
      "loss": 2.2465,
      "step": 40040
    },
    {
      "epoch": 20.371312309257377,
      "grad_norm": 34.02972412109375,
      "learning_rate": 2.9628687690742623e-05,
      "loss": 2.1456,
      "step": 40050
    },
    {
      "epoch": 20.376398779247204,
      "grad_norm": 36.24125289916992,
      "learning_rate": 2.96236012207528e-05,
      "loss": 2.1411,
      "step": 40060
    },
    {
      "epoch": 20.38148524923703,
      "grad_norm": 28.352537155151367,
      "learning_rate": 2.961851475076297e-05,
      "loss": 2.207,
      "step": 40070
    },
    {
      "epoch": 20.386571719226858,
      "grad_norm": 33.37364959716797,
      "learning_rate": 2.9613428280773143e-05,
      "loss": 2.0371,
      "step": 40080
    },
    {
      "epoch": 20.391658189216685,
      "grad_norm": 29.503522872924805,
      "learning_rate": 2.960834181078332e-05,
      "loss": 2.2578,
      "step": 40090
    },
    {
      "epoch": 20.39674465920651,
      "grad_norm": 29.637889862060547,
      "learning_rate": 2.960325534079349e-05,
      "loss": 2.1487,
      "step": 40100
    },
    {
      "epoch": 20.40183112919634,
      "grad_norm": 26.889108657836914,
      "learning_rate": 2.9598168870803666e-05,
      "loss": 2.1321,
      "step": 40110
    },
    {
      "epoch": 20.406917599186166,
      "grad_norm": 32.128177642822266,
      "learning_rate": 2.9593082400813836e-05,
      "loss": 2.1398,
      "step": 40120
    },
    {
      "epoch": 20.412004069175993,
      "grad_norm": 34.27824020385742,
      "learning_rate": 2.958799593082401e-05,
      "loss": 2.191,
      "step": 40130
    },
    {
      "epoch": 20.41709053916582,
      "grad_norm": 34.559226989746094,
      "learning_rate": 2.9582909460834186e-05,
      "loss": 2.1806,
      "step": 40140
    },
    {
      "epoch": 20.422177009155646,
      "grad_norm": 34.671897888183594,
      "learning_rate": 2.9577822990844356e-05,
      "loss": 2.1422,
      "step": 40150
    },
    {
      "epoch": 20.427263479145473,
      "grad_norm": 30.98681640625,
      "learning_rate": 2.9572736520854526e-05,
      "loss": 2.153,
      "step": 40160
    },
    {
      "epoch": 20.4323499491353,
      "grad_norm": 26.205123901367188,
      "learning_rate": 2.9567650050864703e-05,
      "loss": 2.094,
      "step": 40170
    },
    {
      "epoch": 20.437436419125127,
      "grad_norm": 35.364471435546875,
      "learning_rate": 2.9562563580874876e-05,
      "loss": 2.1534,
      "step": 40180
    },
    {
      "epoch": 20.442522889114954,
      "grad_norm": 31.83049201965332,
      "learning_rate": 2.9557477110885046e-05,
      "loss": 2.1014,
      "step": 40190
    },
    {
      "epoch": 20.44760935910478,
      "grad_norm": 28.740339279174805,
      "learning_rate": 2.9552390640895222e-05,
      "loss": 2.1611,
      "step": 40200
    },
    {
      "epoch": 20.45269582909461,
      "grad_norm": 32.30047607421875,
      "learning_rate": 2.9547304170905392e-05,
      "loss": 2.1433,
      "step": 40210
    },
    {
      "epoch": 20.457782299084435,
      "grad_norm": 34.057861328125,
      "learning_rate": 2.9542217700915566e-05,
      "loss": 2.1385,
      "step": 40220
    },
    {
      "epoch": 20.462868769074262,
      "grad_norm": 28.754791259765625,
      "learning_rate": 2.9537131230925742e-05,
      "loss": 2.1099,
      "step": 40230
    },
    {
      "epoch": 20.46795523906409,
      "grad_norm": 32.44017028808594,
      "learning_rate": 2.9532044760935912e-05,
      "loss": 2.0466,
      "step": 40240
    },
    {
      "epoch": 20.473041709053916,
      "grad_norm": 30.249666213989258,
      "learning_rate": 2.9526958290946082e-05,
      "loss": 2.1291,
      "step": 40250
    },
    {
      "epoch": 20.478128179043743,
      "grad_norm": 32.31593322753906,
      "learning_rate": 2.952187182095626e-05,
      "loss": 2.1813,
      "step": 40260
    },
    {
      "epoch": 20.48321464903357,
      "grad_norm": 37.39329528808594,
      "learning_rate": 2.9516785350966432e-05,
      "loss": 2.2149,
      "step": 40270
    },
    {
      "epoch": 20.488301119023397,
      "grad_norm": 27.41156578063965,
      "learning_rate": 2.9511698880976602e-05,
      "loss": 2.1712,
      "step": 40280
    },
    {
      "epoch": 20.493387589013224,
      "grad_norm": 34.789546966552734,
      "learning_rate": 2.950661241098678e-05,
      "loss": 2.1729,
      "step": 40290
    },
    {
      "epoch": 20.49847405900305,
      "grad_norm": 24.8428955078125,
      "learning_rate": 2.950152594099695e-05,
      "loss": 2.1269,
      "step": 40300
    },
    {
      "epoch": 20.503560528992878,
      "grad_norm": 26.934335708618164,
      "learning_rate": 2.9496439471007122e-05,
      "loss": 2.1867,
      "step": 40310
    },
    {
      "epoch": 20.508646998982705,
      "grad_norm": 34.30465316772461,
      "learning_rate": 2.9491353001017295e-05,
      "loss": 2.2309,
      "step": 40320
    },
    {
      "epoch": 20.513733468972532,
      "grad_norm": 39.90480041503906,
      "learning_rate": 2.948626653102747e-05,
      "loss": 2.2519,
      "step": 40330
    },
    {
      "epoch": 20.51881993896236,
      "grad_norm": 32.46590805053711,
      "learning_rate": 2.9481180061037638e-05,
      "loss": 2.1847,
      "step": 40340
    },
    {
      "epoch": 20.523906408952186,
      "grad_norm": 33.25416946411133,
      "learning_rate": 2.9476093591047815e-05,
      "loss": 2.1295,
      "step": 40350
    },
    {
      "epoch": 20.528992878942013,
      "grad_norm": 30.32557487487793,
      "learning_rate": 2.9471007121057985e-05,
      "loss": 2.1586,
      "step": 40360
    },
    {
      "epoch": 20.53407934893184,
      "grad_norm": 36.1660041809082,
      "learning_rate": 2.9465920651068158e-05,
      "loss": 2.1101,
      "step": 40370
    },
    {
      "epoch": 20.539165818921667,
      "grad_norm": 29.296689987182617,
      "learning_rate": 2.9460834181078335e-05,
      "loss": 2.0303,
      "step": 40380
    },
    {
      "epoch": 20.544252288911494,
      "grad_norm": 28.58147430419922,
      "learning_rate": 2.9455747711088505e-05,
      "loss": 2.1308,
      "step": 40390
    },
    {
      "epoch": 20.54933875890132,
      "grad_norm": 38.96611404418945,
      "learning_rate": 2.945066124109868e-05,
      "loss": 2.1344,
      "step": 40400
    },
    {
      "epoch": 20.554425228891148,
      "grad_norm": 31.887182235717773,
      "learning_rate": 2.944557477110885e-05,
      "loss": 2.1519,
      "step": 40410
    },
    {
      "epoch": 20.559511698880975,
      "grad_norm": 28.477989196777344,
      "learning_rate": 2.9440488301119024e-05,
      "loss": 2.1017,
      "step": 40420
    },
    {
      "epoch": 20.564598168870802,
      "grad_norm": 31.25484848022461,
      "learning_rate": 2.94354018311292e-05,
      "loss": 2.1442,
      "step": 40430
    },
    {
      "epoch": 20.56968463886063,
      "grad_norm": 33.02587890625,
      "learning_rate": 2.943031536113937e-05,
      "loss": 2.0923,
      "step": 40440
    },
    {
      "epoch": 20.57477110885046,
      "grad_norm": 38.245121002197266,
      "learning_rate": 2.942522889114954e-05,
      "loss": 2.1103,
      "step": 40450
    },
    {
      "epoch": 20.579857578840286,
      "grad_norm": 36.87946701049805,
      "learning_rate": 2.9420142421159718e-05,
      "loss": 2.1843,
      "step": 40460
    },
    {
      "epoch": 20.584944048830113,
      "grad_norm": 39.63914108276367,
      "learning_rate": 2.941505595116989e-05,
      "loss": 2.1508,
      "step": 40470
    },
    {
      "epoch": 20.59003051881994,
      "grad_norm": 27.980226516723633,
      "learning_rate": 2.940996948118006e-05,
      "loss": 2.1534,
      "step": 40480
    },
    {
      "epoch": 20.595116988809767,
      "grad_norm": 34.23249816894531,
      "learning_rate": 2.9404883011190237e-05,
      "loss": 2.2002,
      "step": 40490
    },
    {
      "epoch": 20.600203458799594,
      "grad_norm": 32.9755744934082,
      "learning_rate": 2.9399796541200407e-05,
      "loss": 2.0909,
      "step": 40500
    },
    {
      "epoch": 20.60528992878942,
      "grad_norm": 30.792619705200195,
      "learning_rate": 2.939471007121058e-05,
      "loss": 2.0863,
      "step": 40510
    },
    {
      "epoch": 20.61037639877925,
      "grad_norm": 32.81019592285156,
      "learning_rate": 2.9389623601220757e-05,
      "loss": 2.14,
      "step": 40520
    },
    {
      "epoch": 20.615462868769075,
      "grad_norm": 30.12408447265625,
      "learning_rate": 2.9384537131230927e-05,
      "loss": 2.1341,
      "step": 40530
    },
    {
      "epoch": 20.620549338758902,
      "grad_norm": 32.06267166137695,
      "learning_rate": 2.9379450661241097e-05,
      "loss": 2.1658,
      "step": 40540
    },
    {
      "epoch": 20.62563580874873,
      "grad_norm": 38.184051513671875,
      "learning_rate": 2.9374364191251274e-05,
      "loss": 2.2453,
      "step": 40550
    },
    {
      "epoch": 20.630722278738556,
      "grad_norm": 31.170970916748047,
      "learning_rate": 2.9369277721261447e-05,
      "loss": 2.1548,
      "step": 40560
    },
    {
      "epoch": 20.635808748728383,
      "grad_norm": 32.79682159423828,
      "learning_rate": 2.9364191251271617e-05,
      "loss": 2.1838,
      "step": 40570
    },
    {
      "epoch": 20.64089521871821,
      "grad_norm": 35.46763610839844,
      "learning_rate": 2.9359104781281794e-05,
      "loss": 2.1976,
      "step": 40580
    },
    {
      "epoch": 20.645981688708037,
      "grad_norm": 29.650182723999023,
      "learning_rate": 2.9354018311291963e-05,
      "loss": 2.0977,
      "step": 40590
    },
    {
      "epoch": 20.651068158697864,
      "grad_norm": 30.88252067565918,
      "learning_rate": 2.9348931841302137e-05,
      "loss": 2.1632,
      "step": 40600
    },
    {
      "epoch": 20.65615462868769,
      "grad_norm": 38.62568283081055,
      "learning_rate": 2.9343845371312313e-05,
      "loss": 2.1776,
      "step": 40610
    },
    {
      "epoch": 20.661241098677518,
      "grad_norm": 24.346691131591797,
      "learning_rate": 2.9338758901322483e-05,
      "loss": 2.1186,
      "step": 40620
    },
    {
      "epoch": 20.666327568667345,
      "grad_norm": 38.808162689208984,
      "learning_rate": 2.9333672431332653e-05,
      "loss": 2.0843,
      "step": 40630
    },
    {
      "epoch": 20.671414038657172,
      "grad_norm": 27.531478881835938,
      "learning_rate": 2.932858596134283e-05,
      "loss": 2.0407,
      "step": 40640
    },
    {
      "epoch": 20.676500508647,
      "grad_norm": 33.142215728759766,
      "learning_rate": 2.9323499491353e-05,
      "loss": 2.1889,
      "step": 40650
    },
    {
      "epoch": 20.681586978636826,
      "grad_norm": 29.755596160888672,
      "learning_rate": 2.9318413021363176e-05,
      "loss": 2.071,
      "step": 40660
    },
    {
      "epoch": 20.686673448626653,
      "grad_norm": 27.631820678710938,
      "learning_rate": 2.931332655137335e-05,
      "loss": 2.1137,
      "step": 40670
    },
    {
      "epoch": 20.69175991861648,
      "grad_norm": 36.117061614990234,
      "learning_rate": 2.930824008138352e-05,
      "loss": 2.0883,
      "step": 40680
    },
    {
      "epoch": 20.696846388606307,
      "grad_norm": 29.074203491210938,
      "learning_rate": 2.9303153611393696e-05,
      "loss": 2.0846,
      "step": 40690
    },
    {
      "epoch": 20.701932858596134,
      "grad_norm": 32.9822998046875,
      "learning_rate": 2.9298067141403866e-05,
      "loss": 2.0705,
      "step": 40700
    },
    {
      "epoch": 20.70701932858596,
      "grad_norm": 37.1926155090332,
      "learning_rate": 2.929298067141404e-05,
      "loss": 2.2252,
      "step": 40710
    },
    {
      "epoch": 20.712105798575788,
      "grad_norm": 32.426170349121094,
      "learning_rate": 2.9287894201424216e-05,
      "loss": 2.0942,
      "step": 40720
    },
    {
      "epoch": 20.717192268565615,
      "grad_norm": 32.367774963378906,
      "learning_rate": 2.9282807731434386e-05,
      "loss": 2.1381,
      "step": 40730
    },
    {
      "epoch": 20.722278738555442,
      "grad_norm": 41.275821685791016,
      "learning_rate": 2.9277721261444556e-05,
      "loss": 2.2348,
      "step": 40740
    },
    {
      "epoch": 20.72736520854527,
      "grad_norm": 34.63500213623047,
      "learning_rate": 2.9272634791454733e-05,
      "loss": 2.1365,
      "step": 40750
    },
    {
      "epoch": 20.732451678535096,
      "grad_norm": 25.26398277282715,
      "learning_rate": 2.9267548321464906e-05,
      "loss": 2.151,
      "step": 40760
    },
    {
      "epoch": 20.737538148524923,
      "grad_norm": 27.356327056884766,
      "learning_rate": 2.9262461851475076e-05,
      "loss": 2.0665,
      "step": 40770
    },
    {
      "epoch": 20.74262461851475,
      "grad_norm": 28.34131622314453,
      "learning_rate": 2.9257375381485252e-05,
      "loss": 2.1578,
      "step": 40780
    },
    {
      "epoch": 20.747711088504577,
      "grad_norm": 28.946922302246094,
      "learning_rate": 2.9252288911495422e-05,
      "loss": 2.1315,
      "step": 40790
    },
    {
      "epoch": 20.752797558494404,
      "grad_norm": 34.432281494140625,
      "learning_rate": 2.9247202441505596e-05,
      "loss": 2.1589,
      "step": 40800
    },
    {
      "epoch": 20.75788402848423,
      "grad_norm": 34.66006851196289,
      "learning_rate": 2.9242115971515772e-05,
      "loss": 2.1581,
      "step": 40810
    },
    {
      "epoch": 20.762970498474058,
      "grad_norm": 29.18134117126465,
      "learning_rate": 2.9237029501525942e-05,
      "loss": 2.162,
      "step": 40820
    },
    {
      "epoch": 20.768056968463885,
      "grad_norm": 37.68328857421875,
      "learning_rate": 2.9231943031536112e-05,
      "loss": 2.1875,
      "step": 40830
    },
    {
      "epoch": 20.77314343845371,
      "grad_norm": 33.79582595825195,
      "learning_rate": 2.922685656154629e-05,
      "loss": 2.1225,
      "step": 40840
    },
    {
      "epoch": 20.77822990844354,
      "grad_norm": 29.374588012695312,
      "learning_rate": 2.9221770091556462e-05,
      "loss": 2.2006,
      "step": 40850
    },
    {
      "epoch": 20.783316378433366,
      "grad_norm": 33.419071197509766,
      "learning_rate": 2.9216683621566632e-05,
      "loss": 2.169,
      "step": 40860
    },
    {
      "epoch": 20.788402848423193,
      "grad_norm": 34.50758743286133,
      "learning_rate": 2.921159715157681e-05,
      "loss": 2.1871,
      "step": 40870
    },
    {
      "epoch": 20.793489318413023,
      "grad_norm": 29.878210067749023,
      "learning_rate": 2.920651068158698e-05,
      "loss": 2.1217,
      "step": 40880
    },
    {
      "epoch": 20.79857578840285,
      "grad_norm": 36.46143341064453,
      "learning_rate": 2.9201424211597152e-05,
      "loss": 2.1641,
      "step": 40890
    },
    {
      "epoch": 20.803662258392677,
      "grad_norm": 31.91267204284668,
      "learning_rate": 2.919633774160733e-05,
      "loss": 2.1958,
      "step": 40900
    },
    {
      "epoch": 20.808748728382504,
      "grad_norm": 30.92219352722168,
      "learning_rate": 2.91912512716175e-05,
      "loss": 2.0954,
      "step": 40910
    },
    {
      "epoch": 20.81383519837233,
      "grad_norm": 28.524551391601562,
      "learning_rate": 2.9186164801627675e-05,
      "loss": 2.1532,
      "step": 40920
    },
    {
      "epoch": 20.818921668362158,
      "grad_norm": 37.42904281616211,
      "learning_rate": 2.9181078331637845e-05,
      "loss": 2.0728,
      "step": 40930
    },
    {
      "epoch": 20.824008138351985,
      "grad_norm": 30.145782470703125,
      "learning_rate": 2.9175991861648018e-05,
      "loss": 2.1238,
      "step": 40940
    },
    {
      "epoch": 20.829094608341812,
      "grad_norm": 34.95808792114258,
      "learning_rate": 2.917090539165819e-05,
      "loss": 2.205,
      "step": 40950
    },
    {
      "epoch": 20.83418107833164,
      "grad_norm": 37.196842193603516,
      "learning_rate": 2.9165818921668365e-05,
      "loss": 2.189,
      "step": 40960
    },
    {
      "epoch": 20.839267548321466,
      "grad_norm": 40.570457458496094,
      "learning_rate": 2.9160732451678535e-05,
      "loss": 2.088,
      "step": 40970
    },
    {
      "epoch": 20.844354018311293,
      "grad_norm": 34.76288604736328,
      "learning_rate": 2.915564598168871e-05,
      "loss": 2.2096,
      "step": 40980
    },
    {
      "epoch": 20.84944048830112,
      "grad_norm": 33.23433303833008,
      "learning_rate": 2.915055951169888e-05,
      "loss": 2.1771,
      "step": 40990
    },
    {
      "epoch": 20.854526958290947,
      "grad_norm": 31.84105682373047,
      "learning_rate": 2.9145473041709054e-05,
      "loss": 2.1229,
      "step": 41000
    },
    {
      "epoch": 20.859613428280774,
      "grad_norm": 37.88972091674805,
      "learning_rate": 2.914038657171923e-05,
      "loss": 2.1718,
      "step": 41010
    },
    {
      "epoch": 20.8646998982706,
      "grad_norm": 32.225982666015625,
      "learning_rate": 2.91353001017294e-05,
      "loss": 2.2254,
      "step": 41020
    },
    {
      "epoch": 20.869786368260428,
      "grad_norm": 35.21931838989258,
      "learning_rate": 2.913021363173957e-05,
      "loss": 2.1773,
      "step": 41030
    },
    {
      "epoch": 20.874872838250255,
      "grad_norm": 37.68464279174805,
      "learning_rate": 2.9125127161749748e-05,
      "loss": 2.1551,
      "step": 41040
    },
    {
      "epoch": 20.879959308240082,
      "grad_norm": 37.180564880371094,
      "learning_rate": 2.912004069175992e-05,
      "loss": 2.0454,
      "step": 41050
    },
    {
      "epoch": 20.88504577822991,
      "grad_norm": 26.32599639892578,
      "learning_rate": 2.911495422177009e-05,
      "loss": 2.091,
      "step": 41060
    },
    {
      "epoch": 20.890132248219736,
      "grad_norm": 26.306243896484375,
      "learning_rate": 2.9109867751780267e-05,
      "loss": 2.0754,
      "step": 41070
    },
    {
      "epoch": 20.895218718209563,
      "grad_norm": 31.110254287719727,
      "learning_rate": 2.9104781281790437e-05,
      "loss": 2.1749,
      "step": 41080
    },
    {
      "epoch": 20.90030518819939,
      "grad_norm": 38.95341873168945,
      "learning_rate": 2.909969481180061e-05,
      "loss": 2.1776,
      "step": 41090
    },
    {
      "epoch": 20.905391658189217,
      "grad_norm": 38.70188522338867,
      "learning_rate": 2.9094608341810787e-05,
      "loss": 2.0317,
      "step": 41100
    },
    {
      "epoch": 20.910478128179044,
      "grad_norm": 32.900909423828125,
      "learning_rate": 2.9089521871820957e-05,
      "loss": 2.0325,
      "step": 41110
    },
    {
      "epoch": 20.91556459816887,
      "grad_norm": 35.36144256591797,
      "learning_rate": 2.9084435401831127e-05,
      "loss": 2.0616,
      "step": 41120
    },
    {
      "epoch": 20.920651068158698,
      "grad_norm": 31.336713790893555,
      "learning_rate": 2.9079348931841304e-05,
      "loss": 2.1623,
      "step": 41130
    },
    {
      "epoch": 20.925737538148525,
      "grad_norm": 36.105125427246094,
      "learning_rate": 2.9074262461851477e-05,
      "loss": 2.1104,
      "step": 41140
    },
    {
      "epoch": 20.93082400813835,
      "grad_norm": 31.571365356445312,
      "learning_rate": 2.9069175991861647e-05,
      "loss": 2.1263,
      "step": 41150
    },
    {
      "epoch": 20.93591047812818,
      "grad_norm": 27.534900665283203,
      "learning_rate": 2.9064089521871824e-05,
      "loss": 2.1077,
      "step": 41160
    },
    {
      "epoch": 20.940996948118006,
      "grad_norm": 28.141695022583008,
      "learning_rate": 2.9059003051881993e-05,
      "loss": 2.1798,
      "step": 41170
    },
    {
      "epoch": 20.946083418107833,
      "grad_norm": 32.173316955566406,
      "learning_rate": 2.9053916581892167e-05,
      "loss": 2.1727,
      "step": 41180
    },
    {
      "epoch": 20.95116988809766,
      "grad_norm": 36.71086502075195,
      "learning_rate": 2.9048830111902343e-05,
      "loss": 2.1656,
      "step": 41190
    },
    {
      "epoch": 20.956256358087487,
      "grad_norm": 32.06315612792969,
      "learning_rate": 2.9043743641912513e-05,
      "loss": 2.1809,
      "step": 41200
    },
    {
      "epoch": 20.961342828077314,
      "grad_norm": 35.5992317199707,
      "learning_rate": 2.903865717192269e-05,
      "loss": 2.1666,
      "step": 41210
    },
    {
      "epoch": 20.96642929806714,
      "grad_norm": 30.374374389648438,
      "learning_rate": 2.903357070193286e-05,
      "loss": 2.2174,
      "step": 41220
    },
    {
      "epoch": 20.971515768056967,
      "grad_norm": 33.04757308959961,
      "learning_rate": 2.9028484231943033e-05,
      "loss": 2.1343,
      "step": 41230
    },
    {
      "epoch": 20.976602238046794,
      "grad_norm": 28.283506393432617,
      "learning_rate": 2.902339776195321e-05,
      "loss": 2.2257,
      "step": 41240
    },
    {
      "epoch": 20.98168870803662,
      "grad_norm": 30.239625930786133,
      "learning_rate": 2.901831129196338e-05,
      "loss": 2.0749,
      "step": 41250
    },
    {
      "epoch": 20.98677517802645,
      "grad_norm": 28.295198440551758,
      "learning_rate": 2.901322482197355e-05,
      "loss": 2.1087,
      "step": 41260
    },
    {
      "epoch": 20.991861648016275,
      "grad_norm": 31.46535873413086,
      "learning_rate": 2.9008138351983726e-05,
      "loss": 2.197,
      "step": 41270
    },
    {
      "epoch": 20.996948118006102,
      "grad_norm": 30.653635025024414,
      "learning_rate": 2.9003051881993896e-05,
      "loss": 2.0244,
      "step": 41280
    },
    {
      "epoch": 21.0,
      "eval_loss": 4.373236179351807,
      "eval_runtime": 2.6943,
      "eval_samples_per_second": 1029.959,
      "eval_steps_per_second": 128.791,
      "step": 41286
    },
    {
      "epoch": 21.00203458799593,
      "grad_norm": 37.18890380859375,
      "learning_rate": 2.899796541200407e-05,
      "loss": 2.0862,
      "step": 41290
    },
    {
      "epoch": 21.007121057985756,
      "grad_norm": 35.683143615722656,
      "learning_rate": 2.8992878942014246e-05,
      "loss": 2.0549,
      "step": 41300
    },
    {
      "epoch": 21.012207527975583,
      "grad_norm": 30.7244930267334,
      "learning_rate": 2.8987792472024416e-05,
      "loss": 2.022,
      "step": 41310
    },
    {
      "epoch": 21.01729399796541,
      "grad_norm": 35.04450988769531,
      "learning_rate": 2.8982706002034586e-05,
      "loss": 2.1669,
      "step": 41320
    },
    {
      "epoch": 21.022380467955237,
      "grad_norm": 31.0410213470459,
      "learning_rate": 2.8977619532044763e-05,
      "loss": 2.065,
      "step": 41330
    },
    {
      "epoch": 21.027466937945068,
      "grad_norm": 29.419601440429688,
      "learning_rate": 2.8972533062054936e-05,
      "loss": 2.0929,
      "step": 41340
    },
    {
      "epoch": 21.032553407934895,
      "grad_norm": 35.991703033447266,
      "learning_rate": 2.8967446592065106e-05,
      "loss": 2.051,
      "step": 41350
    },
    {
      "epoch": 21.037639877924722,
      "grad_norm": 33.05012512207031,
      "learning_rate": 2.8962360122075282e-05,
      "loss": 2.1607,
      "step": 41360
    },
    {
      "epoch": 21.04272634791455,
      "grad_norm": 32.053955078125,
      "learning_rate": 2.8957273652085452e-05,
      "loss": 2.0576,
      "step": 41370
    },
    {
      "epoch": 21.047812817904376,
      "grad_norm": 29.835336685180664,
      "learning_rate": 2.8952187182095626e-05,
      "loss": 2.1118,
      "step": 41380
    },
    {
      "epoch": 21.052899287894203,
      "grad_norm": 30.811969757080078,
      "learning_rate": 2.8947100712105802e-05,
      "loss": 2.1631,
      "step": 41390
    },
    {
      "epoch": 21.05798575788403,
      "grad_norm": 32.022159576416016,
      "learning_rate": 2.8942014242115972e-05,
      "loss": 2.1212,
      "step": 41400
    },
    {
      "epoch": 21.063072227873857,
      "grad_norm": 30.490062713623047,
      "learning_rate": 2.8936927772126142e-05,
      "loss": 2.0395,
      "step": 41410
    },
    {
      "epoch": 21.068158697863684,
      "grad_norm": 39.13295364379883,
      "learning_rate": 2.893184130213632e-05,
      "loss": 2.1004,
      "step": 41420
    },
    {
      "epoch": 21.07324516785351,
      "grad_norm": 34.389827728271484,
      "learning_rate": 2.8926754832146492e-05,
      "loss": 2.0866,
      "step": 41430
    },
    {
      "epoch": 21.078331637843338,
      "grad_norm": 38.34764099121094,
      "learning_rate": 2.8921668362156662e-05,
      "loss": 2.1268,
      "step": 41440
    },
    {
      "epoch": 21.083418107833165,
      "grad_norm": 40.33451843261719,
      "learning_rate": 2.891658189216684e-05,
      "loss": 2.1028,
      "step": 41450
    },
    {
      "epoch": 21.08850457782299,
      "grad_norm": 30.391542434692383,
      "learning_rate": 2.891149542217701e-05,
      "loss": 2.1493,
      "step": 41460
    },
    {
      "epoch": 21.09359104781282,
      "grad_norm": 31.15532112121582,
      "learning_rate": 2.8906408952187185e-05,
      "loss": 2.2722,
      "step": 41470
    },
    {
      "epoch": 21.098677517802646,
      "grad_norm": 35.59874725341797,
      "learning_rate": 2.890132248219736e-05,
      "loss": 2.1686,
      "step": 41480
    },
    {
      "epoch": 21.103763987792473,
      "grad_norm": 38.36699676513672,
      "learning_rate": 2.889623601220753e-05,
      "loss": 2.0684,
      "step": 41490
    },
    {
      "epoch": 21.1088504577823,
      "grad_norm": 33.906578063964844,
      "learning_rate": 2.8891149542217705e-05,
      "loss": 2.1442,
      "step": 41500
    },
    {
      "epoch": 21.113936927772126,
      "grad_norm": 29.787540435791016,
      "learning_rate": 2.8886063072227875e-05,
      "loss": 2.0969,
      "step": 41510
    },
    {
      "epoch": 21.119023397761953,
      "grad_norm": 41.269554138183594,
      "learning_rate": 2.8880976602238048e-05,
      "loss": 2.1052,
      "step": 41520
    },
    {
      "epoch": 21.12410986775178,
      "grad_norm": 25.260465621948242,
      "learning_rate": 2.8875890132248225e-05,
      "loss": 2.1517,
      "step": 41530
    },
    {
      "epoch": 21.129196337741607,
      "grad_norm": 35.654972076416016,
      "learning_rate": 2.8870803662258395e-05,
      "loss": 1.9708,
      "step": 41540
    },
    {
      "epoch": 21.134282807731434,
      "grad_norm": 39.98212814331055,
      "learning_rate": 2.8865717192268565e-05,
      "loss": 2.185,
      "step": 41550
    },
    {
      "epoch": 21.13936927772126,
      "grad_norm": 28.688308715820312,
      "learning_rate": 2.886063072227874e-05,
      "loss": 2.1291,
      "step": 41560
    },
    {
      "epoch": 21.14445574771109,
      "grad_norm": 34.02149200439453,
      "learning_rate": 2.8855544252288915e-05,
      "loss": 2.016,
      "step": 41570
    },
    {
      "epoch": 21.149542217700915,
      "grad_norm": 34.646915435791016,
      "learning_rate": 2.8850457782299084e-05,
      "loss": 2.08,
      "step": 41580
    },
    {
      "epoch": 21.154628687690742,
      "grad_norm": 26.714157104492188,
      "learning_rate": 2.884537131230926e-05,
      "loss": 2.2015,
      "step": 41590
    },
    {
      "epoch": 21.15971515768057,
      "grad_norm": 32.423828125,
      "learning_rate": 2.884028484231943e-05,
      "loss": 2.168,
      "step": 41600
    },
    {
      "epoch": 21.164801627670396,
      "grad_norm": 36.242225646972656,
      "learning_rate": 2.8835198372329604e-05,
      "loss": 2.0259,
      "step": 41610
    },
    {
      "epoch": 21.169888097660223,
      "grad_norm": 25.21797752380371,
      "learning_rate": 2.8830111902339778e-05,
      "loss": 2.1552,
      "step": 41620
    },
    {
      "epoch": 21.17497456765005,
      "grad_norm": 27.21285057067871,
      "learning_rate": 2.882502543234995e-05,
      "loss": 2.1215,
      "step": 41630
    },
    {
      "epoch": 21.180061037639877,
      "grad_norm": 35.590126037597656,
      "learning_rate": 2.881993896236012e-05,
      "loss": 2.1079,
      "step": 41640
    },
    {
      "epoch": 21.185147507629704,
      "grad_norm": 29.01654624938965,
      "learning_rate": 2.8814852492370297e-05,
      "loss": 2.1544,
      "step": 41650
    },
    {
      "epoch": 21.19023397761953,
      "grad_norm": 28.61025047302246,
      "learning_rate": 2.8809766022380467e-05,
      "loss": 2.1008,
      "step": 41660
    },
    {
      "epoch": 21.195320447609358,
      "grad_norm": 26.335994720458984,
      "learning_rate": 2.880467955239064e-05,
      "loss": 2.0903,
      "step": 41670
    },
    {
      "epoch": 21.200406917599185,
      "grad_norm": 32.0352668762207,
      "learning_rate": 2.8799593082400817e-05,
      "loss": 2.062,
      "step": 41680
    },
    {
      "epoch": 21.205493387589012,
      "grad_norm": 32.772743225097656,
      "learning_rate": 2.8794506612410987e-05,
      "loss": 2.1424,
      "step": 41690
    },
    {
      "epoch": 21.21057985757884,
      "grad_norm": 37.40601348876953,
      "learning_rate": 2.8789420142421157e-05,
      "loss": 2.1379,
      "step": 41700
    },
    {
      "epoch": 21.215666327568666,
      "grad_norm": 31.50815200805664,
      "learning_rate": 2.8784333672431334e-05,
      "loss": 2.0163,
      "step": 41710
    },
    {
      "epoch": 21.220752797558493,
      "grad_norm": 25.934343338012695,
      "learning_rate": 2.8779247202441507e-05,
      "loss": 2.1538,
      "step": 41720
    },
    {
      "epoch": 21.22583926754832,
      "grad_norm": 31.805110931396484,
      "learning_rate": 2.8774160732451684e-05,
      "loss": 2.0717,
      "step": 41730
    },
    {
      "epoch": 21.230925737538147,
      "grad_norm": 36.6323127746582,
      "learning_rate": 2.8769074262461854e-05,
      "loss": 2.1522,
      "step": 41740
    },
    {
      "epoch": 21.236012207527974,
      "grad_norm": 35.08153533935547,
      "learning_rate": 2.8763987792472023e-05,
      "loss": 2.0827,
      "step": 41750
    },
    {
      "epoch": 21.2410986775178,
      "grad_norm": 27.984920501708984,
      "learning_rate": 2.87589013224822e-05,
      "loss": 2.07,
      "step": 41760
    },
    {
      "epoch": 21.246185147507628,
      "grad_norm": 31.30099868774414,
      "learning_rate": 2.8753814852492373e-05,
      "loss": 2.0885,
      "step": 41770
    },
    {
      "epoch": 21.25127161749746,
      "grad_norm": 33.542869567871094,
      "learning_rate": 2.8748728382502543e-05,
      "loss": 2.1519,
      "step": 41780
    },
    {
      "epoch": 21.256358087487286,
      "grad_norm": 34.803157806396484,
      "learning_rate": 2.874364191251272e-05,
      "loss": 2.1211,
      "step": 41790
    },
    {
      "epoch": 21.261444557477112,
      "grad_norm": 27.063724517822266,
      "learning_rate": 2.873855544252289e-05,
      "loss": 2.1605,
      "step": 41800
    },
    {
      "epoch": 21.26653102746694,
      "grad_norm": 33.23997116088867,
      "learning_rate": 2.8733468972533063e-05,
      "loss": 2.0924,
      "step": 41810
    },
    {
      "epoch": 21.271617497456766,
      "grad_norm": 32.966217041015625,
      "learning_rate": 2.872838250254324e-05,
      "loss": 2.1559,
      "step": 41820
    },
    {
      "epoch": 21.276703967446593,
      "grad_norm": 31.792543411254883,
      "learning_rate": 2.872329603255341e-05,
      "loss": 2.109,
      "step": 41830
    },
    {
      "epoch": 21.28179043743642,
      "grad_norm": 37.714759826660156,
      "learning_rate": 2.871820956256358e-05,
      "loss": 2.1755,
      "step": 41840
    },
    {
      "epoch": 21.286876907426247,
      "grad_norm": 35.721797943115234,
      "learning_rate": 2.8713123092573756e-05,
      "loss": 2.086,
      "step": 41850
    },
    {
      "epoch": 21.291963377416074,
      "grad_norm": 28.814485549926758,
      "learning_rate": 2.870803662258393e-05,
      "loss": 2.0822,
      "step": 41860
    },
    {
      "epoch": 21.2970498474059,
      "grad_norm": 39.03535842895508,
      "learning_rate": 2.87029501525941e-05,
      "loss": 2.1081,
      "step": 41870
    },
    {
      "epoch": 21.30213631739573,
      "grad_norm": 41.31708526611328,
      "learning_rate": 2.8697863682604276e-05,
      "loss": 2.1024,
      "step": 41880
    },
    {
      "epoch": 21.307222787385555,
      "grad_norm": 43.301353454589844,
      "learning_rate": 2.8692777212614446e-05,
      "loss": 2.0401,
      "step": 41890
    },
    {
      "epoch": 21.312309257375382,
      "grad_norm": 32.05913543701172,
      "learning_rate": 2.868769074262462e-05,
      "loss": 2.1189,
      "step": 41900
    },
    {
      "epoch": 21.31739572736521,
      "grad_norm": 32.616493225097656,
      "learning_rate": 2.8682604272634796e-05,
      "loss": 2.1569,
      "step": 41910
    },
    {
      "epoch": 21.322482197355036,
      "grad_norm": 38.32603454589844,
      "learning_rate": 2.8677517802644966e-05,
      "loss": 2.0909,
      "step": 41920
    },
    {
      "epoch": 21.327568667344863,
      "grad_norm": 31.814851760864258,
      "learning_rate": 2.8672431332655136e-05,
      "loss": 2.1211,
      "step": 41930
    },
    {
      "epoch": 21.33265513733469,
      "grad_norm": 29.684412002563477,
      "learning_rate": 2.8667344862665312e-05,
      "loss": 2.065,
      "step": 41940
    },
    {
      "epoch": 21.337741607324517,
      "grad_norm": 35.61882019042969,
      "learning_rate": 2.8662258392675482e-05,
      "loss": 2.1931,
      "step": 41950
    },
    {
      "epoch": 21.342828077314344,
      "grad_norm": 41.86014938354492,
      "learning_rate": 2.8657171922685656e-05,
      "loss": 2.1008,
      "step": 41960
    },
    {
      "epoch": 21.34791454730417,
      "grad_norm": 34.65428161621094,
      "learning_rate": 2.8652085452695832e-05,
      "loss": 2.1378,
      "step": 41970
    },
    {
      "epoch": 21.353001017293998,
      "grad_norm": 31.889995574951172,
      "learning_rate": 2.8646998982706002e-05,
      "loss": 2.172,
      "step": 41980
    },
    {
      "epoch": 21.358087487283825,
      "grad_norm": 39.384910583496094,
      "learning_rate": 2.8641912512716172e-05,
      "loss": 2.2285,
      "step": 41990
    },
    {
      "epoch": 21.363173957273652,
      "grad_norm": 38.14363479614258,
      "learning_rate": 2.863682604272635e-05,
      "loss": 2.1399,
      "step": 42000
    },
    {
      "epoch": 21.36826042726348,
      "grad_norm": 32.03318786621094,
      "learning_rate": 2.8631739572736522e-05,
      "loss": 2.135,
      "step": 42010
    },
    {
      "epoch": 21.373346897253306,
      "grad_norm": 38.732818603515625,
      "learning_rate": 2.86266531027467e-05,
      "loss": 2.0896,
      "step": 42020
    },
    {
      "epoch": 21.378433367243133,
      "grad_norm": 29.90719223022461,
      "learning_rate": 2.862156663275687e-05,
      "loss": 2.0202,
      "step": 42030
    },
    {
      "epoch": 21.38351983723296,
      "grad_norm": 33.028316497802734,
      "learning_rate": 2.861648016276704e-05,
      "loss": 2.0445,
      "step": 42040
    },
    {
      "epoch": 21.388606307222787,
      "grad_norm": 34.1246337890625,
      "learning_rate": 2.8611393692777215e-05,
      "loss": 2.1495,
      "step": 42050
    },
    {
      "epoch": 21.393692777212614,
      "grad_norm": 29.383159637451172,
      "learning_rate": 2.860630722278739e-05,
      "loss": 2.0602,
      "step": 42060
    },
    {
      "epoch": 21.39877924720244,
      "grad_norm": 27.305641174316406,
      "learning_rate": 2.860122075279756e-05,
      "loss": 2.0529,
      "step": 42070
    },
    {
      "epoch": 21.403865717192268,
      "grad_norm": 35.05324935913086,
      "learning_rate": 2.8596134282807735e-05,
      "loss": 2.1533,
      "step": 42080
    },
    {
      "epoch": 21.408952187182095,
      "grad_norm": 34.861812591552734,
      "learning_rate": 2.8591047812817905e-05,
      "loss": 2.0888,
      "step": 42090
    },
    {
      "epoch": 21.414038657171922,
      "grad_norm": 46.08061218261719,
      "learning_rate": 2.8585961342828078e-05,
      "loss": 2.0899,
      "step": 42100
    },
    {
      "epoch": 21.41912512716175,
      "grad_norm": 39.104248046875,
      "learning_rate": 2.8580874872838255e-05,
      "loss": 2.1068,
      "step": 42110
    },
    {
      "epoch": 21.424211597151576,
      "grad_norm": 36.241310119628906,
      "learning_rate": 2.8575788402848425e-05,
      "loss": 2.0607,
      "step": 42120
    },
    {
      "epoch": 21.429298067141403,
      "grad_norm": 26.589101791381836,
      "learning_rate": 2.8570701932858595e-05,
      "loss": 2.2464,
      "step": 42130
    },
    {
      "epoch": 21.43438453713123,
      "grad_norm": 38.17255783081055,
      "learning_rate": 2.856561546286877e-05,
      "loss": 2.1286,
      "step": 42140
    },
    {
      "epoch": 21.439471007121057,
      "grad_norm": 30.66083526611328,
      "learning_rate": 2.8560528992878945e-05,
      "loss": 2.0888,
      "step": 42150
    },
    {
      "epoch": 21.444557477110884,
      "grad_norm": 35.57216262817383,
      "learning_rate": 2.8555442522889114e-05,
      "loss": 2.1084,
      "step": 42160
    },
    {
      "epoch": 21.44964394710071,
      "grad_norm": 37.05813217163086,
      "learning_rate": 2.855035605289929e-05,
      "loss": 2.0942,
      "step": 42170
    },
    {
      "epoch": 21.454730417090538,
      "grad_norm": 29.888282775878906,
      "learning_rate": 2.854526958290946e-05,
      "loss": 2.1027,
      "step": 42180
    },
    {
      "epoch": 21.459816887080365,
      "grad_norm": 39.314109802246094,
      "learning_rate": 2.8540183112919634e-05,
      "loss": 2.0872,
      "step": 42190
    },
    {
      "epoch": 21.46490335707019,
      "grad_norm": 26.073637008666992,
      "learning_rate": 2.853509664292981e-05,
      "loss": 2.1048,
      "step": 42200
    },
    {
      "epoch": 21.46998982706002,
      "grad_norm": 31.762868881225586,
      "learning_rate": 2.853001017293998e-05,
      "loss": 2.15,
      "step": 42210
    },
    {
      "epoch": 21.475076297049846,
      "grad_norm": 37.637874603271484,
      "learning_rate": 2.852492370295015e-05,
      "loss": 2.068,
      "step": 42220
    },
    {
      "epoch": 21.480162767039676,
      "grad_norm": 30.68409538269043,
      "learning_rate": 2.8519837232960327e-05,
      "loss": 2.0846,
      "step": 42230
    },
    {
      "epoch": 21.485249237029503,
      "grad_norm": 29.460412979125977,
      "learning_rate": 2.85147507629705e-05,
      "loss": 2.1081,
      "step": 42240
    },
    {
      "epoch": 21.49033570701933,
      "grad_norm": 37.64036560058594,
      "learning_rate": 2.850966429298067e-05,
      "loss": 2.0821,
      "step": 42250
    },
    {
      "epoch": 21.495422177009157,
      "grad_norm": 36.49904251098633,
      "learning_rate": 2.8504577822990847e-05,
      "loss": 2.1056,
      "step": 42260
    },
    {
      "epoch": 21.500508646998984,
      "grad_norm": 31.46434783935547,
      "learning_rate": 2.8499491353001017e-05,
      "loss": 2.0947,
      "step": 42270
    },
    {
      "epoch": 21.50559511698881,
      "grad_norm": 29.021596908569336,
      "learning_rate": 2.8494404883011194e-05,
      "loss": 2.0394,
      "step": 42280
    },
    {
      "epoch": 21.510681586978638,
      "grad_norm": 30.6632137298584,
      "learning_rate": 2.8489318413021364e-05,
      "loss": 2.0442,
      "step": 42290
    },
    {
      "epoch": 21.515768056968465,
      "grad_norm": 38.072818756103516,
      "learning_rate": 2.8484231943031537e-05,
      "loss": 2.1014,
      "step": 42300
    },
    {
      "epoch": 21.520854526958292,
      "grad_norm": 34.23765563964844,
      "learning_rate": 2.8479145473041714e-05,
      "loss": 2.1394,
      "step": 42310
    },
    {
      "epoch": 21.52594099694812,
      "grad_norm": 39.21687316894531,
      "learning_rate": 2.8474059003051884e-05,
      "loss": 2.0937,
      "step": 42320
    },
    {
      "epoch": 21.531027466937946,
      "grad_norm": 31.278749465942383,
      "learning_rate": 2.8468972533062054e-05,
      "loss": 2.1262,
      "step": 42330
    },
    {
      "epoch": 21.536113936927773,
      "grad_norm": 36.7343864440918,
      "learning_rate": 2.846388606307223e-05,
      "loss": 2.114,
      "step": 42340
    },
    {
      "epoch": 21.5412004069176,
      "grad_norm": 31.19441032409668,
      "learning_rate": 2.8458799593082403e-05,
      "loss": 2.141,
      "step": 42350
    },
    {
      "epoch": 21.546286876907427,
      "grad_norm": 40.35824966430664,
      "learning_rate": 2.8453713123092573e-05,
      "loss": 2.021,
      "step": 42360
    },
    {
      "epoch": 21.551373346897254,
      "grad_norm": 33.490211486816406,
      "learning_rate": 2.844862665310275e-05,
      "loss": 2.1148,
      "step": 42370
    },
    {
      "epoch": 21.55645981688708,
      "grad_norm": 36.965572357177734,
      "learning_rate": 2.844354018311292e-05,
      "loss": 2.1604,
      "step": 42380
    },
    {
      "epoch": 21.561546286876908,
      "grad_norm": 36.7941780090332,
      "learning_rate": 2.8438453713123093e-05,
      "loss": 2.1439,
      "step": 42390
    },
    {
      "epoch": 21.566632756866735,
      "grad_norm": 25.273496627807617,
      "learning_rate": 2.843336724313327e-05,
      "loss": 1.9933,
      "step": 42400
    },
    {
      "epoch": 21.571719226856562,
      "grad_norm": 30.771154403686523,
      "learning_rate": 2.842828077314344e-05,
      "loss": 2.078,
      "step": 42410
    },
    {
      "epoch": 21.57680569684639,
      "grad_norm": 35.32637023925781,
      "learning_rate": 2.842319430315361e-05,
      "loss": 2.1307,
      "step": 42420
    },
    {
      "epoch": 21.581892166836216,
      "grad_norm": 30.063016891479492,
      "learning_rate": 2.8418107833163786e-05,
      "loss": 2.0574,
      "step": 42430
    },
    {
      "epoch": 21.586978636826043,
      "grad_norm": 42.687408447265625,
      "learning_rate": 2.841302136317396e-05,
      "loss": 2.1164,
      "step": 42440
    },
    {
      "epoch": 21.59206510681587,
      "grad_norm": 29.593677520751953,
      "learning_rate": 2.840793489318413e-05,
      "loss": 2.057,
      "step": 42450
    },
    {
      "epoch": 21.597151576805697,
      "grad_norm": 33.958404541015625,
      "learning_rate": 2.8402848423194306e-05,
      "loss": 2.0935,
      "step": 42460
    },
    {
      "epoch": 21.602238046795524,
      "grad_norm": 24.40697479248047,
      "learning_rate": 2.8397761953204476e-05,
      "loss": 2.1014,
      "step": 42470
    },
    {
      "epoch": 21.60732451678535,
      "grad_norm": 31.931697845458984,
      "learning_rate": 2.839267548321465e-05,
      "loss": 2.0842,
      "step": 42480
    },
    {
      "epoch": 21.612410986775178,
      "grad_norm": 44.59122848510742,
      "learning_rate": 2.8387589013224826e-05,
      "loss": 2.0713,
      "step": 42490
    },
    {
      "epoch": 21.617497456765005,
      "grad_norm": 29.504108428955078,
      "learning_rate": 2.8382502543234996e-05,
      "loss": 2.0678,
      "step": 42500
    },
    {
      "epoch": 21.62258392675483,
      "grad_norm": 28.388078689575195,
      "learning_rate": 2.8377416073245166e-05,
      "loss": 2.109,
      "step": 42510
    },
    {
      "epoch": 21.62767039674466,
      "grad_norm": 33.0999870300293,
      "learning_rate": 2.8372329603255342e-05,
      "loss": 2.1552,
      "step": 42520
    },
    {
      "epoch": 21.632756866734486,
      "grad_norm": 38.0040168762207,
      "learning_rate": 2.8367243133265516e-05,
      "loss": 2.0797,
      "step": 42530
    },
    {
      "epoch": 21.637843336724313,
      "grad_norm": 29.35052490234375,
      "learning_rate": 2.8362156663275692e-05,
      "loss": 2.137,
      "step": 42540
    },
    {
      "epoch": 21.64292980671414,
      "grad_norm": 34.04643630981445,
      "learning_rate": 2.8357070193285862e-05,
      "loss": 2.084,
      "step": 42550
    },
    {
      "epoch": 21.648016276703967,
      "grad_norm": 34.07992172241211,
      "learning_rate": 2.8351983723296032e-05,
      "loss": 2.078,
      "step": 42560
    },
    {
      "epoch": 21.653102746693794,
      "grad_norm": 36.642333984375,
      "learning_rate": 2.834689725330621e-05,
      "loss": 2.1342,
      "step": 42570
    },
    {
      "epoch": 21.65818921668362,
      "grad_norm": 36.12307357788086,
      "learning_rate": 2.834181078331638e-05,
      "loss": 2.0346,
      "step": 42580
    },
    {
      "epoch": 21.663275686673447,
      "grad_norm": 36.3634033203125,
      "learning_rate": 2.8336724313326552e-05,
      "loss": 2.134,
      "step": 42590
    },
    {
      "epoch": 21.668362156663274,
      "grad_norm": 30.883995056152344,
      "learning_rate": 2.833163784333673e-05,
      "loss": 2.132,
      "step": 42600
    },
    {
      "epoch": 21.6734486266531,
      "grad_norm": 33.743412017822266,
      "learning_rate": 2.83265513733469e-05,
      "loss": 2.0237,
      "step": 42610
    },
    {
      "epoch": 21.67853509664293,
      "grad_norm": 35.796363830566406,
      "learning_rate": 2.832146490335707e-05,
      "loss": 2.0472,
      "step": 42620
    },
    {
      "epoch": 21.683621566632755,
      "grad_norm": 33.426326751708984,
      "learning_rate": 2.8316378433367245e-05,
      "loss": 2.0931,
      "step": 42630
    },
    {
      "epoch": 21.688708036622582,
      "grad_norm": 36.38954544067383,
      "learning_rate": 2.831129196337742e-05,
      "loss": 2.1063,
      "step": 42640
    },
    {
      "epoch": 21.69379450661241,
      "grad_norm": 31.414173126220703,
      "learning_rate": 2.830620549338759e-05,
      "loss": 2.0091,
      "step": 42650
    },
    {
      "epoch": 21.698880976602236,
      "grad_norm": 30.052217483520508,
      "learning_rate": 2.8301119023397765e-05,
      "loss": 2.031,
      "step": 42660
    },
    {
      "epoch": 21.703967446592067,
      "grad_norm": 34.96775436401367,
      "learning_rate": 2.8296032553407935e-05,
      "loss": 2.042,
      "step": 42670
    },
    {
      "epoch": 21.709053916581894,
      "grad_norm": 42.40078353881836,
      "learning_rate": 2.8290946083418108e-05,
      "loss": 2.0355,
      "step": 42680
    },
    {
      "epoch": 21.71414038657172,
      "grad_norm": 35.957115173339844,
      "learning_rate": 2.8285859613428285e-05,
      "loss": 2.0482,
      "step": 42690
    },
    {
      "epoch": 21.719226856561548,
      "grad_norm": 39.07231521606445,
      "learning_rate": 2.8280773143438455e-05,
      "loss": 2.116,
      "step": 42700
    },
    {
      "epoch": 21.724313326551375,
      "grad_norm": 30.08587074279785,
      "learning_rate": 2.8275686673448625e-05,
      "loss": 2.0958,
      "step": 42710
    },
    {
      "epoch": 21.729399796541202,
      "grad_norm": 32.90644454956055,
      "learning_rate": 2.82706002034588e-05,
      "loss": 2.1054,
      "step": 42720
    },
    {
      "epoch": 21.73448626653103,
      "grad_norm": 40.02156066894531,
      "learning_rate": 2.8265513733468975e-05,
      "loss": 2.1759,
      "step": 42730
    },
    {
      "epoch": 21.739572736520856,
      "grad_norm": 42.33570861816406,
      "learning_rate": 2.8260427263479145e-05,
      "loss": 2.0387,
      "step": 42740
    },
    {
      "epoch": 21.744659206510683,
      "grad_norm": 32.50557327270508,
      "learning_rate": 2.825534079348932e-05,
      "loss": 2.1002,
      "step": 42750
    },
    {
      "epoch": 21.74974567650051,
      "grad_norm": 33.61836624145508,
      "learning_rate": 2.825025432349949e-05,
      "loss": 2.1103,
      "step": 42760
    },
    {
      "epoch": 21.754832146490337,
      "grad_norm": 36.510406494140625,
      "learning_rate": 2.8245167853509664e-05,
      "loss": 1.9938,
      "step": 42770
    },
    {
      "epoch": 21.759918616480164,
      "grad_norm": 38.12616729736328,
      "learning_rate": 2.824008138351984e-05,
      "loss": 2.1112,
      "step": 42780
    },
    {
      "epoch": 21.76500508646999,
      "grad_norm": 34.67367935180664,
      "learning_rate": 2.823499491353001e-05,
      "loss": 2.1453,
      "step": 42790
    },
    {
      "epoch": 21.770091556459818,
      "grad_norm": 31.996374130249023,
      "learning_rate": 2.822990844354018e-05,
      "loss": 2.1639,
      "step": 42800
    },
    {
      "epoch": 21.775178026449645,
      "grad_norm": 42.646827697753906,
      "learning_rate": 2.8224821973550357e-05,
      "loss": 2.0537,
      "step": 42810
    },
    {
      "epoch": 21.78026449643947,
      "grad_norm": 31.646263122558594,
      "learning_rate": 2.821973550356053e-05,
      "loss": 2.0524,
      "step": 42820
    },
    {
      "epoch": 21.7853509664293,
      "grad_norm": 36.974849700927734,
      "learning_rate": 2.8214649033570707e-05,
      "loss": 2.0231,
      "step": 42830
    },
    {
      "epoch": 21.790437436419126,
      "grad_norm": 34.93040084838867,
      "learning_rate": 2.8209562563580877e-05,
      "loss": 2.0567,
      "step": 42840
    },
    {
      "epoch": 21.795523906408953,
      "grad_norm": 39.01923370361328,
      "learning_rate": 2.8204476093591047e-05,
      "loss": 2.1802,
      "step": 42850
    },
    {
      "epoch": 21.80061037639878,
      "grad_norm": 40.32093811035156,
      "learning_rate": 2.8199389623601224e-05,
      "loss": 2.0402,
      "step": 42860
    },
    {
      "epoch": 21.805696846388607,
      "grad_norm": 33.056434631347656,
      "learning_rate": 2.8194303153611397e-05,
      "loss": 2.0184,
      "step": 42870
    },
    {
      "epoch": 21.810783316378433,
      "grad_norm": 37.472843170166016,
      "learning_rate": 2.8189216683621567e-05,
      "loss": 2.1127,
      "step": 42880
    },
    {
      "epoch": 21.81586978636826,
      "grad_norm": 32.05426788330078,
      "learning_rate": 2.8184130213631744e-05,
      "loss": 2.079,
      "step": 42890
    },
    {
      "epoch": 21.820956256358087,
      "grad_norm": 29.39789581298828,
      "learning_rate": 2.8179043743641914e-05,
      "loss": 2.0213,
      "step": 42900
    },
    {
      "epoch": 21.826042726347914,
      "grad_norm": 31.48784637451172,
      "learning_rate": 2.8173957273652084e-05,
      "loss": 2.052,
      "step": 42910
    },
    {
      "epoch": 21.83112919633774,
      "grad_norm": 41.0211296081543,
      "learning_rate": 2.816887080366226e-05,
      "loss": 2.0606,
      "step": 42920
    },
    {
      "epoch": 21.83621566632757,
      "grad_norm": 32.22871398925781,
      "learning_rate": 2.8163784333672433e-05,
      "loss": 2.1197,
      "step": 42930
    },
    {
      "epoch": 21.841302136317395,
      "grad_norm": 35.023834228515625,
      "learning_rate": 2.8158697863682603e-05,
      "loss": 2.0034,
      "step": 42940
    },
    {
      "epoch": 21.846388606307222,
      "grad_norm": 33.66178512573242,
      "learning_rate": 2.815361139369278e-05,
      "loss": 2.1531,
      "step": 42950
    },
    {
      "epoch": 21.85147507629705,
      "grad_norm": 35.3668327331543,
      "learning_rate": 2.814852492370295e-05,
      "loss": 2.0733,
      "step": 42960
    },
    {
      "epoch": 21.856561546286876,
      "grad_norm": 31.92698860168457,
      "learning_rate": 2.8143438453713123e-05,
      "loss": 2.0837,
      "step": 42970
    },
    {
      "epoch": 21.861648016276703,
      "grad_norm": 24.132593154907227,
      "learning_rate": 2.81383519837233e-05,
      "loss": 2.059,
      "step": 42980
    },
    {
      "epoch": 21.86673448626653,
      "grad_norm": 40.199745178222656,
      "learning_rate": 2.813326551373347e-05,
      "loss": 2.0401,
      "step": 42990
    },
    {
      "epoch": 21.871820956256357,
      "grad_norm": 34.01411056518555,
      "learning_rate": 2.812817904374364e-05,
      "loss": 2.0561,
      "step": 43000
    },
    {
      "epoch": 21.876907426246184,
      "grad_norm": 36.12664031982422,
      "learning_rate": 2.8123092573753816e-05,
      "loss": 2.1012,
      "step": 43010
    },
    {
      "epoch": 21.88199389623601,
      "grad_norm": 27.943195343017578,
      "learning_rate": 2.811800610376399e-05,
      "loss": 2.0248,
      "step": 43020
    },
    {
      "epoch": 21.887080366225838,
      "grad_norm": 43.20275115966797,
      "learning_rate": 2.811291963377416e-05,
      "loss": 2.0616,
      "step": 43030
    },
    {
      "epoch": 21.892166836215665,
      "grad_norm": 29.54177474975586,
      "learning_rate": 2.8107833163784336e-05,
      "loss": 2.0571,
      "step": 43040
    },
    {
      "epoch": 21.897253306205492,
      "grad_norm": 31.824565887451172,
      "learning_rate": 2.8102746693794506e-05,
      "loss": 2.1573,
      "step": 43050
    },
    {
      "epoch": 21.90233977619532,
      "grad_norm": 34.36501693725586,
      "learning_rate": 2.809766022380468e-05,
      "loss": 2.0128,
      "step": 43060
    },
    {
      "epoch": 21.907426246185146,
      "grad_norm": 31.417686462402344,
      "learning_rate": 2.8092573753814856e-05,
      "loss": 2.1362,
      "step": 43070
    },
    {
      "epoch": 21.912512716174973,
      "grad_norm": 43.29077911376953,
      "learning_rate": 2.8087487283825026e-05,
      "loss": 2.1087,
      "step": 43080
    },
    {
      "epoch": 21.9175991861648,
      "grad_norm": 34.48372268676758,
      "learning_rate": 2.8082400813835203e-05,
      "loss": 2.1066,
      "step": 43090
    },
    {
      "epoch": 21.922685656154627,
      "grad_norm": 26.48891830444336,
      "learning_rate": 2.8077314343845372e-05,
      "loss": 2.088,
      "step": 43100
    },
    {
      "epoch": 21.927772126144454,
      "grad_norm": 30.307607650756836,
      "learning_rate": 2.8072227873855546e-05,
      "loss": 2.1459,
      "step": 43110
    },
    {
      "epoch": 21.93285859613428,
      "grad_norm": 35.47817611694336,
      "learning_rate": 2.8067141403865722e-05,
      "loss": 2.0481,
      "step": 43120
    },
    {
      "epoch": 21.93794506612411,
      "grad_norm": 28.661161422729492,
      "learning_rate": 2.8062054933875892e-05,
      "loss": 2.0101,
      "step": 43130
    },
    {
      "epoch": 21.94303153611394,
      "grad_norm": 39.24298095703125,
      "learning_rate": 2.8056968463886062e-05,
      "loss": 2.0563,
      "step": 43140
    },
    {
      "epoch": 21.948118006103766,
      "grad_norm": 35.070655822753906,
      "learning_rate": 2.805188199389624e-05,
      "loss": 2.0342,
      "step": 43150
    },
    {
      "epoch": 21.953204476093592,
      "grad_norm": 35.64886474609375,
      "learning_rate": 2.8046795523906412e-05,
      "loss": 2.1393,
      "step": 43160
    },
    {
      "epoch": 21.95829094608342,
      "grad_norm": 36.87492752075195,
      "learning_rate": 2.8041709053916582e-05,
      "loss": 2.1179,
      "step": 43170
    },
    {
      "epoch": 21.963377416073246,
      "grad_norm": 31.6040096282959,
      "learning_rate": 2.803662258392676e-05,
      "loss": 2.1253,
      "step": 43180
    },
    {
      "epoch": 21.968463886063073,
      "grad_norm": 39.17481231689453,
      "learning_rate": 2.803153611393693e-05,
      "loss": 2.1198,
      "step": 43190
    },
    {
      "epoch": 21.9735503560529,
      "grad_norm": 26.617427825927734,
      "learning_rate": 2.8026449643947102e-05,
      "loss": 2.121,
      "step": 43200
    },
    {
      "epoch": 21.978636826042727,
      "grad_norm": 34.77021026611328,
      "learning_rate": 2.8021363173957275e-05,
      "loss": 2.1169,
      "step": 43210
    },
    {
      "epoch": 21.983723296032554,
      "grad_norm": 33.46255111694336,
      "learning_rate": 2.801627670396745e-05,
      "loss": 2.0367,
      "step": 43220
    },
    {
      "epoch": 21.98880976602238,
      "grad_norm": 32.03183364868164,
      "learning_rate": 2.801119023397762e-05,
      "loss": 2.0255,
      "step": 43230
    },
    {
      "epoch": 21.99389623601221,
      "grad_norm": 32.5904541015625,
      "learning_rate": 2.8006103763987795e-05,
      "loss": 2.1678,
      "step": 43240
    },
    {
      "epoch": 21.998982706002035,
      "grad_norm": 31.818166732788086,
      "learning_rate": 2.8001017293997965e-05,
      "loss": 2.1176,
      "step": 43250
    },
    {
      "epoch": 22.0,
      "eval_loss": 4.424034118652344,
      "eval_runtime": 2.7503,
      "eval_samples_per_second": 1008.967,
      "eval_steps_per_second": 126.166,
      "step": 43252
    },
    {
      "epoch": 22.004069175991862,
      "grad_norm": 38.193695068359375,
      "learning_rate": 2.7995930824008138e-05,
      "loss": 1.9968,
      "step": 43260
    },
    {
      "epoch": 22.00915564598169,
      "grad_norm": 34.08876037597656,
      "learning_rate": 2.7990844354018315e-05,
      "loss": 2.0771,
      "step": 43270
    },
    {
      "epoch": 22.014242115971516,
      "grad_norm": 43.262821197509766,
      "learning_rate": 2.7985757884028485e-05,
      "loss": 2.071,
      "step": 43280
    },
    {
      "epoch": 22.019328585961343,
      "grad_norm": 33.76662826538086,
      "learning_rate": 2.7980671414038655e-05,
      "loss": 2.0429,
      "step": 43290
    },
    {
      "epoch": 22.02441505595117,
      "grad_norm": 43.37928771972656,
      "learning_rate": 2.797558494404883e-05,
      "loss": 2.034,
      "step": 43300
    },
    {
      "epoch": 22.029501525940997,
      "grad_norm": 29.89325714111328,
      "learning_rate": 2.7970498474059005e-05,
      "loss": 2.1731,
      "step": 43310
    },
    {
      "epoch": 22.034587995930824,
      "grad_norm": 31.94928741455078,
      "learning_rate": 2.7965412004069175e-05,
      "loss": 2.0778,
      "step": 43320
    },
    {
      "epoch": 22.03967446592065,
      "grad_norm": 40.75657272338867,
      "learning_rate": 2.796032553407935e-05,
      "loss": 2.1932,
      "step": 43330
    },
    {
      "epoch": 22.044760935910478,
      "grad_norm": 36.8969612121582,
      "learning_rate": 2.795523906408952e-05,
      "loss": 2.0936,
      "step": 43340
    },
    {
      "epoch": 22.049847405900305,
      "grad_norm": 29.648731231689453,
      "learning_rate": 2.7950152594099698e-05,
      "loss": 2.0946,
      "step": 43350
    },
    {
      "epoch": 22.054933875890132,
      "grad_norm": 39.02933120727539,
      "learning_rate": 2.794506612410987e-05,
      "loss": 2.0275,
      "step": 43360
    },
    {
      "epoch": 22.06002034587996,
      "grad_norm": 35.404022216796875,
      "learning_rate": 2.793997965412004e-05,
      "loss": 2.1323,
      "step": 43370
    },
    {
      "epoch": 22.065106815869786,
      "grad_norm": 31.024192810058594,
      "learning_rate": 2.7934893184130218e-05,
      "loss": 2.0299,
      "step": 43380
    },
    {
      "epoch": 22.070193285859613,
      "grad_norm": 32.24729537963867,
      "learning_rate": 2.7929806714140387e-05,
      "loss": 1.9627,
      "step": 43390
    },
    {
      "epoch": 22.07527975584944,
      "grad_norm": 33.91964340209961,
      "learning_rate": 2.792472024415056e-05,
      "loss": 2.0702,
      "step": 43400
    },
    {
      "epoch": 22.080366225839267,
      "grad_norm": 26.785202026367188,
      "learning_rate": 2.7919633774160737e-05,
      "loss": 2.1268,
      "step": 43410
    },
    {
      "epoch": 22.085452695829094,
      "grad_norm": 44.98666763305664,
      "learning_rate": 2.7914547304170907e-05,
      "loss": 2.0268,
      "step": 43420
    },
    {
      "epoch": 22.09053916581892,
      "grad_norm": 36.761287689208984,
      "learning_rate": 2.7909460834181077e-05,
      "loss": 1.964,
      "step": 43430
    },
    {
      "epoch": 22.095625635808748,
      "grad_norm": 37.580631256103516,
      "learning_rate": 2.7904374364191254e-05,
      "loss": 2.0583,
      "step": 43440
    },
    {
      "epoch": 22.100712105798575,
      "grad_norm": 36.11151885986328,
      "learning_rate": 2.7899287894201427e-05,
      "loss": 2.0431,
      "step": 43450
    },
    {
      "epoch": 22.105798575788402,
      "grad_norm": 29.50057601928711,
      "learning_rate": 2.7894201424211597e-05,
      "loss": 2.0381,
      "step": 43460
    },
    {
      "epoch": 22.11088504577823,
      "grad_norm": 31.11164093017578,
      "learning_rate": 2.7889114954221774e-05,
      "loss": 1.9575,
      "step": 43470
    },
    {
      "epoch": 22.115971515768056,
      "grad_norm": 27.910266876220703,
      "learning_rate": 2.7884028484231944e-05,
      "loss": 2.1368,
      "step": 43480
    },
    {
      "epoch": 22.121057985757883,
      "grad_norm": 34.06142807006836,
      "learning_rate": 2.7878942014242117e-05,
      "loss": 2.0935,
      "step": 43490
    },
    {
      "epoch": 22.12614445574771,
      "grad_norm": 34.23297882080078,
      "learning_rate": 2.7873855544252294e-05,
      "loss": 2.0651,
      "step": 43500
    },
    {
      "epoch": 22.131230925737537,
      "grad_norm": 35.56618881225586,
      "learning_rate": 2.7868769074262463e-05,
      "loss": 2.1451,
      "step": 43510
    },
    {
      "epoch": 22.136317395727364,
      "grad_norm": 27.548009872436523,
      "learning_rate": 2.7863682604272633e-05,
      "loss": 2.0739,
      "step": 43520
    },
    {
      "epoch": 22.14140386571719,
      "grad_norm": 25.574121475219727,
      "learning_rate": 2.785859613428281e-05,
      "loss": 2.051,
      "step": 43530
    },
    {
      "epoch": 22.146490335707018,
      "grad_norm": 34.28104019165039,
      "learning_rate": 2.785350966429298e-05,
      "loss": 2.0092,
      "step": 43540
    },
    {
      "epoch": 22.151576805696845,
      "grad_norm": 46.743892669677734,
      "learning_rate": 2.7848423194303153e-05,
      "loss": 1.9571,
      "step": 43550
    },
    {
      "epoch": 22.15666327568667,
      "grad_norm": 45.101104736328125,
      "learning_rate": 2.784333672431333e-05,
      "loss": 2.0812,
      "step": 43560
    },
    {
      "epoch": 22.161749745676502,
      "grad_norm": 33.42441940307617,
      "learning_rate": 2.78382502543235e-05,
      "loss": 1.9978,
      "step": 43570
    },
    {
      "epoch": 22.16683621566633,
      "grad_norm": 29.900005340576172,
      "learning_rate": 2.783316378433367e-05,
      "loss": 2.0883,
      "step": 43580
    },
    {
      "epoch": 22.171922685656156,
      "grad_norm": 31.559226989746094,
      "learning_rate": 2.7828077314343846e-05,
      "loss": 2.0428,
      "step": 43590
    },
    {
      "epoch": 22.177009155645983,
      "grad_norm": 27.707242965698242,
      "learning_rate": 2.782299084435402e-05,
      "loss": 2.0227,
      "step": 43600
    },
    {
      "epoch": 22.18209562563581,
      "grad_norm": 31.922616958618164,
      "learning_rate": 2.781790437436419e-05,
      "loss": 2.0226,
      "step": 43610
    },
    {
      "epoch": 22.187182095625637,
      "grad_norm": 37.45664596557617,
      "learning_rate": 2.7812817904374366e-05,
      "loss": 2.0089,
      "step": 43620
    },
    {
      "epoch": 22.192268565615464,
      "grad_norm": 29.462961196899414,
      "learning_rate": 2.7807731434384536e-05,
      "loss": 2.0608,
      "step": 43630
    },
    {
      "epoch": 22.19735503560529,
      "grad_norm": 34.206119537353516,
      "learning_rate": 2.7802644964394713e-05,
      "loss": 2.0325,
      "step": 43640
    },
    {
      "epoch": 22.202441505595118,
      "grad_norm": 26.356752395629883,
      "learning_rate": 2.7797558494404886e-05,
      "loss": 2.017,
      "step": 43650
    },
    {
      "epoch": 22.207527975584945,
      "grad_norm": 39.01683807373047,
      "learning_rate": 2.7792472024415056e-05,
      "loss": 2.1456,
      "step": 43660
    },
    {
      "epoch": 22.212614445574772,
      "grad_norm": 36.973758697509766,
      "learning_rate": 2.7787385554425233e-05,
      "loss": 2.0378,
      "step": 43670
    },
    {
      "epoch": 22.2177009155646,
      "grad_norm": 38.10862731933594,
      "learning_rate": 2.7782299084435402e-05,
      "loss": 1.9926,
      "step": 43680
    },
    {
      "epoch": 22.222787385554426,
      "grad_norm": 37.313636779785156,
      "learning_rate": 2.7777212614445576e-05,
      "loss": 1.9831,
      "step": 43690
    },
    {
      "epoch": 22.227873855544253,
      "grad_norm": 40.0440673828125,
      "learning_rate": 2.7772126144455752e-05,
      "loss": 2.1322,
      "step": 43700
    },
    {
      "epoch": 22.23296032553408,
      "grad_norm": 35.28525924682617,
      "learning_rate": 2.7767039674465922e-05,
      "loss": 2.0319,
      "step": 43710
    },
    {
      "epoch": 22.238046795523907,
      "grad_norm": 36.38277053833008,
      "learning_rate": 2.7761953204476092e-05,
      "loss": 2.1449,
      "step": 43720
    },
    {
      "epoch": 22.243133265513734,
      "grad_norm": 30.309173583984375,
      "learning_rate": 2.775686673448627e-05,
      "loss": 2.1236,
      "step": 43730
    },
    {
      "epoch": 22.24821973550356,
      "grad_norm": 38.79725646972656,
      "learning_rate": 2.7751780264496442e-05,
      "loss": 2.1326,
      "step": 43740
    },
    {
      "epoch": 22.253306205493388,
      "grad_norm": 35.383609771728516,
      "learning_rate": 2.7746693794506612e-05,
      "loss": 2.0077,
      "step": 43750
    },
    {
      "epoch": 22.258392675483215,
      "grad_norm": 31.227310180664062,
      "learning_rate": 2.774160732451679e-05,
      "loss": 1.9939,
      "step": 43760
    },
    {
      "epoch": 22.263479145473042,
      "grad_norm": 30.10059356689453,
      "learning_rate": 2.773652085452696e-05,
      "loss": 1.984,
      "step": 43770
    },
    {
      "epoch": 22.26856561546287,
      "grad_norm": 34.46501541137695,
      "learning_rate": 2.7731434384537132e-05,
      "loss": 1.9758,
      "step": 43780
    },
    {
      "epoch": 22.273652085452696,
      "grad_norm": 25.470455169677734,
      "learning_rate": 2.772634791454731e-05,
      "loss": 2.1018,
      "step": 43790
    },
    {
      "epoch": 22.278738555442523,
      "grad_norm": 34.9564323425293,
      "learning_rate": 2.772126144455748e-05,
      "loss": 2.0825,
      "step": 43800
    },
    {
      "epoch": 22.28382502543235,
      "grad_norm": 35.70981216430664,
      "learning_rate": 2.771617497456765e-05,
      "loss": 2.0912,
      "step": 43810
    },
    {
      "epoch": 22.288911495422177,
      "grad_norm": 41.296592712402344,
      "learning_rate": 2.7711088504577825e-05,
      "loss": 2.0406,
      "step": 43820
    },
    {
      "epoch": 22.293997965412004,
      "grad_norm": 32.03762435913086,
      "learning_rate": 2.7706002034588e-05,
      "loss": 2.0638,
      "step": 43830
    },
    {
      "epoch": 22.29908443540183,
      "grad_norm": 37.358612060546875,
      "learning_rate": 2.7700915564598168e-05,
      "loss": 2.1478,
      "step": 43840
    },
    {
      "epoch": 22.304170905391658,
      "grad_norm": 39.98018264770508,
      "learning_rate": 2.7695829094608345e-05,
      "loss": 2.0296,
      "step": 43850
    },
    {
      "epoch": 22.309257375381485,
      "grad_norm": 43.68971252441406,
      "learning_rate": 2.7690742624618515e-05,
      "loss": 2.0076,
      "step": 43860
    },
    {
      "epoch": 22.31434384537131,
      "grad_norm": 34.09195327758789,
      "learning_rate": 2.7685656154628685e-05,
      "loss": 1.9564,
      "step": 43870
    },
    {
      "epoch": 22.31943031536114,
      "grad_norm": 33.99861145019531,
      "learning_rate": 2.768056968463886e-05,
      "loss": 2.0404,
      "step": 43880
    },
    {
      "epoch": 22.324516785350966,
      "grad_norm": 33.0114631652832,
      "learning_rate": 2.7675483214649035e-05,
      "loss": 2.0572,
      "step": 43890
    },
    {
      "epoch": 22.329603255340793,
      "grad_norm": 30.882722854614258,
      "learning_rate": 2.767039674465921e-05,
      "loss": 2.0443,
      "step": 43900
    },
    {
      "epoch": 22.33468972533062,
      "grad_norm": 46.064605712890625,
      "learning_rate": 2.766531027466938e-05,
      "loss": 2.033,
      "step": 43910
    },
    {
      "epoch": 22.339776195320447,
      "grad_norm": 31.89069175720215,
      "learning_rate": 2.766022380467955e-05,
      "loss": 2.0529,
      "step": 43920
    },
    {
      "epoch": 22.344862665310274,
      "grad_norm": 34.49905014038086,
      "learning_rate": 2.7655137334689728e-05,
      "loss": 2.0227,
      "step": 43930
    },
    {
      "epoch": 22.3499491353001,
      "grad_norm": 28.986059188842773,
      "learning_rate": 2.76500508646999e-05,
      "loss": 2.0709,
      "step": 43940
    },
    {
      "epoch": 22.355035605289928,
      "grad_norm": 29.10320281982422,
      "learning_rate": 2.764496439471007e-05,
      "loss": 2.0779,
      "step": 43950
    },
    {
      "epoch": 22.360122075279754,
      "grad_norm": 27.16347312927246,
      "learning_rate": 2.7639877924720248e-05,
      "loss": 2.1084,
      "step": 43960
    },
    {
      "epoch": 22.36520854526958,
      "grad_norm": 30.344934463500977,
      "learning_rate": 2.7634791454730417e-05,
      "loss": 2.1098,
      "step": 43970
    },
    {
      "epoch": 22.37029501525941,
      "grad_norm": 32.726558685302734,
      "learning_rate": 2.762970498474059e-05,
      "loss": 2.102,
      "step": 43980
    },
    {
      "epoch": 22.375381485249235,
      "grad_norm": 35.36732482910156,
      "learning_rate": 2.7624618514750767e-05,
      "loss": 2.0435,
      "step": 43990
    },
    {
      "epoch": 22.380467955239062,
      "grad_norm": 40.79416275024414,
      "learning_rate": 2.7619532044760937e-05,
      "loss": 2.0278,
      "step": 44000
    },
    {
      "epoch": 22.38555442522889,
      "grad_norm": 32.54851531982422,
      "learning_rate": 2.7614445574771107e-05,
      "loss": 1.9754,
      "step": 44010
    },
    {
      "epoch": 22.39064089521872,
      "grad_norm": 31.196117401123047,
      "learning_rate": 2.7609359104781284e-05,
      "loss": 2.0936,
      "step": 44020
    },
    {
      "epoch": 22.395727365208547,
      "grad_norm": 33.87307357788086,
      "learning_rate": 2.7604272634791457e-05,
      "loss": 1.9618,
      "step": 44030
    },
    {
      "epoch": 22.400813835198374,
      "grad_norm": 36.71186828613281,
      "learning_rate": 2.7599186164801627e-05,
      "loss": 2.0546,
      "step": 44040
    },
    {
      "epoch": 22.4059003051882,
      "grad_norm": 34.1236572265625,
      "learning_rate": 2.7594099694811804e-05,
      "loss": 2.0561,
      "step": 44050
    },
    {
      "epoch": 22.410986775178028,
      "grad_norm": 26.13008689880371,
      "learning_rate": 2.7589013224821974e-05,
      "loss": 2.1406,
      "step": 44060
    },
    {
      "epoch": 22.416073245167855,
      "grad_norm": 32.27595520019531,
      "learning_rate": 2.7583926754832147e-05,
      "loss": 2.1536,
      "step": 44070
    },
    {
      "epoch": 22.421159715157682,
      "grad_norm": 34.86764144897461,
      "learning_rate": 2.7578840284842324e-05,
      "loss": 2.1397,
      "step": 44080
    },
    {
      "epoch": 22.42624618514751,
      "grad_norm": 28.404216766357422,
      "learning_rate": 2.7573753814852493e-05,
      "loss": 2.022,
      "step": 44090
    },
    {
      "epoch": 22.431332655137336,
      "grad_norm": 36.359683990478516,
      "learning_rate": 2.7568667344862663e-05,
      "loss": 2.0324,
      "step": 44100
    },
    {
      "epoch": 22.436419125127163,
      "grad_norm": 32.11434555053711,
      "learning_rate": 2.756358087487284e-05,
      "loss": 2.0082,
      "step": 44110
    },
    {
      "epoch": 22.44150559511699,
      "grad_norm": 27.18741226196289,
      "learning_rate": 2.7558494404883013e-05,
      "loss": 2.035,
      "step": 44120
    },
    {
      "epoch": 22.446592065106817,
      "grad_norm": 31.858898162841797,
      "learning_rate": 2.7553407934893183e-05,
      "loss": 2.0437,
      "step": 44130
    },
    {
      "epoch": 22.451678535096644,
      "grad_norm": 28.447214126586914,
      "learning_rate": 2.754832146490336e-05,
      "loss": 2.0107,
      "step": 44140
    },
    {
      "epoch": 22.45676500508647,
      "grad_norm": 35.08102035522461,
      "learning_rate": 2.754323499491353e-05,
      "loss": 2.0381,
      "step": 44150
    },
    {
      "epoch": 22.461851475076298,
      "grad_norm": 32.762027740478516,
      "learning_rate": 2.7538148524923706e-05,
      "loss": 2.097,
      "step": 44160
    },
    {
      "epoch": 22.466937945066125,
      "grad_norm": 32.65575408935547,
      "learning_rate": 2.7533062054933876e-05,
      "loss": 2.0756,
      "step": 44170
    },
    {
      "epoch": 22.47202441505595,
      "grad_norm": 34.868011474609375,
      "learning_rate": 2.752797558494405e-05,
      "loss": 2.0594,
      "step": 44180
    },
    {
      "epoch": 22.47711088504578,
      "grad_norm": 38.78969192504883,
      "learning_rate": 2.7522889114954226e-05,
      "loss": 1.9741,
      "step": 44190
    },
    {
      "epoch": 22.482197355035606,
      "grad_norm": 31.304895401000977,
      "learning_rate": 2.7517802644964396e-05,
      "loss": 2.1202,
      "step": 44200
    },
    {
      "epoch": 22.487283825025433,
      "grad_norm": 45.64719772338867,
      "learning_rate": 2.7512716174974566e-05,
      "loss": 2.0868,
      "step": 44210
    },
    {
      "epoch": 22.49237029501526,
      "grad_norm": 35.7495002746582,
      "learning_rate": 2.7507629704984743e-05,
      "loss": 2.0924,
      "step": 44220
    },
    {
      "epoch": 22.497456765005087,
      "grad_norm": 33.04206848144531,
      "learning_rate": 2.7502543234994916e-05,
      "loss": 1.9673,
      "step": 44230
    },
    {
      "epoch": 22.502543234994913,
      "grad_norm": 34.83177185058594,
      "learning_rate": 2.7497456765005086e-05,
      "loss": 2.1169,
      "step": 44240
    },
    {
      "epoch": 22.50762970498474,
      "grad_norm": 30.788114547729492,
      "learning_rate": 2.7492370295015263e-05,
      "loss": 2.026,
      "step": 44250
    },
    {
      "epoch": 22.512716174974567,
      "grad_norm": 38.34397888183594,
      "learning_rate": 2.7487283825025432e-05,
      "loss": 2.0073,
      "step": 44260
    },
    {
      "epoch": 22.517802644964394,
      "grad_norm": 37.95893478393555,
      "learning_rate": 2.7482197355035606e-05,
      "loss": 2.0971,
      "step": 44270
    },
    {
      "epoch": 22.52288911495422,
      "grad_norm": 26.46036720275879,
      "learning_rate": 2.7477110885045782e-05,
      "loss": 2.0264,
      "step": 44280
    },
    {
      "epoch": 22.52797558494405,
      "grad_norm": 31.820632934570312,
      "learning_rate": 2.7472024415055952e-05,
      "loss": 2.023,
      "step": 44290
    },
    {
      "epoch": 22.533062054933875,
      "grad_norm": 29.342514038085938,
      "learning_rate": 2.7466937945066122e-05,
      "loss": 2.1583,
      "step": 44300
    },
    {
      "epoch": 22.538148524923702,
      "grad_norm": 37.61686706542969,
      "learning_rate": 2.74618514750763e-05,
      "loss": 2.0881,
      "step": 44310
    },
    {
      "epoch": 22.54323499491353,
      "grad_norm": 41.43098831176758,
      "learning_rate": 2.7456765005086472e-05,
      "loss": 1.9916,
      "step": 44320
    },
    {
      "epoch": 22.548321464903356,
      "grad_norm": 32.1905517578125,
      "learning_rate": 2.7451678535096642e-05,
      "loss": 2.0698,
      "step": 44330
    },
    {
      "epoch": 22.553407934893183,
      "grad_norm": 33.88386154174805,
      "learning_rate": 2.744659206510682e-05,
      "loss": 2.0864,
      "step": 44340
    },
    {
      "epoch": 22.55849440488301,
      "grad_norm": 31.67683982849121,
      "learning_rate": 2.744150559511699e-05,
      "loss": 2.0055,
      "step": 44350
    },
    {
      "epoch": 22.563580874872837,
      "grad_norm": 32.8506965637207,
      "learning_rate": 2.7436419125127162e-05,
      "loss": 1.9184,
      "step": 44360
    },
    {
      "epoch": 22.568667344862664,
      "grad_norm": 40.59299087524414,
      "learning_rate": 2.743133265513734e-05,
      "loss": 2.0501,
      "step": 44370
    },
    {
      "epoch": 22.57375381485249,
      "grad_norm": 40.34793472290039,
      "learning_rate": 2.742624618514751e-05,
      "loss": 2.088,
      "step": 44380
    },
    {
      "epoch": 22.578840284842318,
      "grad_norm": 40.37788772583008,
      "learning_rate": 2.742115971515768e-05,
      "loss": 2.0002,
      "step": 44390
    },
    {
      "epoch": 22.583926754832145,
      "grad_norm": 34.453609466552734,
      "learning_rate": 2.7416073245167855e-05,
      "loss": 2.0643,
      "step": 44400
    },
    {
      "epoch": 22.589013224821972,
      "grad_norm": 44.26478576660156,
      "learning_rate": 2.741098677517803e-05,
      "loss": 2.1006,
      "step": 44410
    },
    {
      "epoch": 22.5940996948118,
      "grad_norm": 34.158668518066406,
      "learning_rate": 2.7405900305188198e-05,
      "loss": 2.0364,
      "step": 44420
    },
    {
      "epoch": 22.599186164801626,
      "grad_norm": 35.746097564697266,
      "learning_rate": 2.7400813835198375e-05,
      "loss": 2.0335,
      "step": 44430
    },
    {
      "epoch": 22.604272634791453,
      "grad_norm": 32.080963134765625,
      "learning_rate": 2.7395727365208545e-05,
      "loss": 2.124,
      "step": 44440
    },
    {
      "epoch": 22.60935910478128,
      "grad_norm": 36.86631774902344,
      "learning_rate": 2.739064089521872e-05,
      "loss": 2.0705,
      "step": 44450
    },
    {
      "epoch": 22.61444557477111,
      "grad_norm": 37.531341552734375,
      "learning_rate": 2.7385554425228895e-05,
      "loss": 2.1025,
      "step": 44460
    },
    {
      "epoch": 22.619532044760938,
      "grad_norm": 28.78119659423828,
      "learning_rate": 2.7380467955239065e-05,
      "loss": 1.9499,
      "step": 44470
    },
    {
      "epoch": 22.624618514750765,
      "grad_norm": 34.30078887939453,
      "learning_rate": 2.737538148524924e-05,
      "loss": 2.0097,
      "step": 44480
    },
    {
      "epoch": 22.62970498474059,
      "grad_norm": 35.32513427734375,
      "learning_rate": 2.737029501525941e-05,
      "loss": 2.1786,
      "step": 44490
    },
    {
      "epoch": 22.63479145473042,
      "grad_norm": 34.56674575805664,
      "learning_rate": 2.736520854526958e-05,
      "loss": 2.076,
      "step": 44500
    },
    {
      "epoch": 22.639877924720246,
      "grad_norm": 48.324642181396484,
      "learning_rate": 2.7360122075279758e-05,
      "loss": 2.0373,
      "step": 44510
    },
    {
      "epoch": 22.644964394710072,
      "grad_norm": 36.89558792114258,
      "learning_rate": 2.735503560528993e-05,
      "loss": 2.0703,
      "step": 44520
    },
    {
      "epoch": 22.6500508646999,
      "grad_norm": 34.36024856567383,
      "learning_rate": 2.73499491353001e-05,
      "loss": 2.0661,
      "step": 44530
    },
    {
      "epoch": 22.655137334689726,
      "grad_norm": 40.828369140625,
      "learning_rate": 2.7344862665310278e-05,
      "loss": 2.0183,
      "step": 44540
    },
    {
      "epoch": 22.660223804679553,
      "grad_norm": 33.44073486328125,
      "learning_rate": 2.7339776195320447e-05,
      "loss": 2.0666,
      "step": 44550
    },
    {
      "epoch": 22.66531027466938,
      "grad_norm": 33.576072692871094,
      "learning_rate": 2.733468972533062e-05,
      "loss": 2.0213,
      "step": 44560
    },
    {
      "epoch": 22.670396744659207,
      "grad_norm": 35.65469741821289,
      "learning_rate": 2.7329603255340797e-05,
      "loss": 2.1893,
      "step": 44570
    },
    {
      "epoch": 22.675483214649034,
      "grad_norm": 39.613975524902344,
      "learning_rate": 2.7324516785350967e-05,
      "loss": 2.0781,
      "step": 44580
    },
    {
      "epoch": 22.68056968463886,
      "grad_norm": 37.78632736206055,
      "learning_rate": 2.7319430315361137e-05,
      "loss": 2.0116,
      "step": 44590
    },
    {
      "epoch": 22.68565615462869,
      "grad_norm": 29.318750381469727,
      "learning_rate": 2.7314343845371314e-05,
      "loss": 2.0026,
      "step": 44600
    },
    {
      "epoch": 22.690742624618515,
      "grad_norm": 35.03548049926758,
      "learning_rate": 2.7309257375381487e-05,
      "loss": 2.0143,
      "step": 44610
    },
    {
      "epoch": 22.695829094608342,
      "grad_norm": 38.37934494018555,
      "learning_rate": 2.7304170905391657e-05,
      "loss": 2.0486,
      "step": 44620
    },
    {
      "epoch": 22.70091556459817,
      "grad_norm": 30.183191299438477,
      "learning_rate": 2.7299084435401834e-05,
      "loss": 2.0387,
      "step": 44630
    },
    {
      "epoch": 22.706002034587996,
      "grad_norm": 39.59845733642578,
      "learning_rate": 2.7293997965412004e-05,
      "loss": 2.0591,
      "step": 44640
    },
    {
      "epoch": 22.711088504577823,
      "grad_norm": 46.019813537597656,
      "learning_rate": 2.7288911495422177e-05,
      "loss": 2.1623,
      "step": 44650
    },
    {
      "epoch": 22.71617497456765,
      "grad_norm": 31.145858764648438,
      "learning_rate": 2.7283825025432354e-05,
      "loss": 2.0182,
      "step": 44660
    },
    {
      "epoch": 22.721261444557477,
      "grad_norm": 32.52781677246094,
      "learning_rate": 2.7278738555442523e-05,
      "loss": 2.1316,
      "step": 44670
    },
    {
      "epoch": 22.726347914547304,
      "grad_norm": 41.11311340332031,
      "learning_rate": 2.7273652085452693e-05,
      "loss": 2.0509,
      "step": 44680
    },
    {
      "epoch": 22.73143438453713,
      "grad_norm": 33.07559585571289,
      "learning_rate": 2.726856561546287e-05,
      "loss": 2.0622,
      "step": 44690
    },
    {
      "epoch": 22.736520854526958,
      "grad_norm": 32.305030822753906,
      "learning_rate": 2.7263479145473043e-05,
      "loss": 2.0458,
      "step": 44700
    },
    {
      "epoch": 22.741607324516785,
      "grad_norm": 38.30922317504883,
      "learning_rate": 2.725839267548322e-05,
      "loss": 2.052,
      "step": 44710
    },
    {
      "epoch": 22.746693794506612,
      "grad_norm": 42.55848693847656,
      "learning_rate": 2.725330620549339e-05,
      "loss": 2.0702,
      "step": 44720
    },
    {
      "epoch": 22.75178026449644,
      "grad_norm": 35.31770706176758,
      "learning_rate": 2.724821973550356e-05,
      "loss": 2.0056,
      "step": 44730
    },
    {
      "epoch": 22.756866734486266,
      "grad_norm": 35.91688919067383,
      "learning_rate": 2.7243133265513736e-05,
      "loss": 2.0941,
      "step": 44740
    },
    {
      "epoch": 22.761953204476093,
      "grad_norm": 44.667816162109375,
      "learning_rate": 2.723804679552391e-05,
      "loss": 2.0811,
      "step": 44750
    },
    {
      "epoch": 22.76703967446592,
      "grad_norm": 36.79338455200195,
      "learning_rate": 2.723296032553408e-05,
      "loss": 2.037,
      "step": 44760
    },
    {
      "epoch": 22.772126144455747,
      "grad_norm": 34.67884826660156,
      "learning_rate": 2.7227873855544256e-05,
      "loss": 1.9478,
      "step": 44770
    },
    {
      "epoch": 22.777212614445574,
      "grad_norm": 41.53953170776367,
      "learning_rate": 2.7222787385554426e-05,
      "loss": 2.0273,
      "step": 44780
    },
    {
      "epoch": 22.7822990844354,
      "grad_norm": 38.88218307495117,
      "learning_rate": 2.72177009155646e-05,
      "loss": 2.0427,
      "step": 44790
    },
    {
      "epoch": 22.787385554425228,
      "grad_norm": 45.39846420288086,
      "learning_rate": 2.7212614445574776e-05,
      "loss": 2.0937,
      "step": 44800
    },
    {
      "epoch": 22.792472024415055,
      "grad_norm": 33.188724517822266,
      "learning_rate": 2.7207527975584946e-05,
      "loss": 2.0422,
      "step": 44810
    },
    {
      "epoch": 22.797558494404882,
      "grad_norm": 33.962764739990234,
      "learning_rate": 2.7202441505595116e-05,
      "loss": 2.0791,
      "step": 44820
    },
    {
      "epoch": 22.80264496439471,
      "grad_norm": 36.9113883972168,
      "learning_rate": 2.7197355035605293e-05,
      "loss": 1.9774,
      "step": 44830
    },
    {
      "epoch": 22.807731434384536,
      "grad_norm": 36.66278839111328,
      "learning_rate": 2.7192268565615462e-05,
      "loss": 1.9891,
      "step": 44840
    },
    {
      "epoch": 22.812817904374363,
      "grad_norm": 38.31208038330078,
      "learning_rate": 2.7187182095625636e-05,
      "loss": 2.0365,
      "step": 44850
    },
    {
      "epoch": 22.81790437436419,
      "grad_norm": 34.702667236328125,
      "learning_rate": 2.7182095625635812e-05,
      "loss": 2.0302,
      "step": 44860
    },
    {
      "epoch": 22.822990844354017,
      "grad_norm": 39.49755859375,
      "learning_rate": 2.7177009155645982e-05,
      "loss": 2.116,
      "step": 44870
    },
    {
      "epoch": 22.828077314343844,
      "grad_norm": 32.803165435791016,
      "learning_rate": 2.7171922685656152e-05,
      "loss": 2.0853,
      "step": 44880
    },
    {
      "epoch": 22.83316378433367,
      "grad_norm": 35.181949615478516,
      "learning_rate": 2.716683621566633e-05,
      "loss": 2.065,
      "step": 44890
    },
    {
      "epoch": 22.838250254323498,
      "grad_norm": 31.040929794311523,
      "learning_rate": 2.7161749745676502e-05,
      "loss": 1.9916,
      "step": 44900
    },
    {
      "epoch": 22.843336724313325,
      "grad_norm": 32.142276763916016,
      "learning_rate": 2.7156663275686672e-05,
      "loss": 2.0087,
      "step": 44910
    },
    {
      "epoch": 22.848423194303155,
      "grad_norm": 38.539730072021484,
      "learning_rate": 2.715157680569685e-05,
      "loss": 1.9552,
      "step": 44920
    },
    {
      "epoch": 22.853509664292982,
      "grad_norm": 33.428104400634766,
      "learning_rate": 2.714649033570702e-05,
      "loss": 1.9546,
      "step": 44930
    },
    {
      "epoch": 22.85859613428281,
      "grad_norm": 40.235015869140625,
      "learning_rate": 2.7141403865717192e-05,
      "loss": 2.0363,
      "step": 44940
    },
    {
      "epoch": 22.863682604272636,
      "grad_norm": 37.81403350830078,
      "learning_rate": 2.713631739572737e-05,
      "loss": 2.0516,
      "step": 44950
    },
    {
      "epoch": 22.868769074262463,
      "grad_norm": 39.7965087890625,
      "learning_rate": 2.713123092573754e-05,
      "loss": 1.9762,
      "step": 44960
    },
    {
      "epoch": 22.87385554425229,
      "grad_norm": 30.055994033813477,
      "learning_rate": 2.7126144455747715e-05,
      "loss": 2.0253,
      "step": 44970
    },
    {
      "epoch": 22.878942014242117,
      "grad_norm": 31.388643264770508,
      "learning_rate": 2.7121057985757885e-05,
      "loss": 2.058,
      "step": 44980
    },
    {
      "epoch": 22.884028484231944,
      "grad_norm": 35.20084762573242,
      "learning_rate": 2.711597151576806e-05,
      "loss": 2.0915,
      "step": 44990
    },
    {
      "epoch": 22.88911495422177,
      "grad_norm": 34.23679733276367,
      "learning_rate": 2.7110885045778235e-05,
      "loss": 2.043,
      "step": 45000
    },
    {
      "epoch": 22.894201424211598,
      "grad_norm": 31.216798782348633,
      "learning_rate": 2.7105798575788405e-05,
      "loss": 2.0271,
      "step": 45010
    },
    {
      "epoch": 22.899287894201425,
      "grad_norm": 36.404666900634766,
      "learning_rate": 2.7100712105798575e-05,
      "loss": 2.0565,
      "step": 45020
    },
    {
      "epoch": 22.904374364191252,
      "grad_norm": 39.04670715332031,
      "learning_rate": 2.709562563580875e-05,
      "loss": 2.0323,
      "step": 45030
    },
    {
      "epoch": 22.90946083418108,
      "grad_norm": 29.977949142456055,
      "learning_rate": 2.7090539165818925e-05,
      "loss": 2.0497,
      "step": 45040
    },
    {
      "epoch": 22.914547304170906,
      "grad_norm": 29.67259407043457,
      "learning_rate": 2.7085452695829095e-05,
      "loss": 2.0095,
      "step": 45050
    },
    {
      "epoch": 22.919633774160733,
      "grad_norm": 40.85349655151367,
      "learning_rate": 2.708036622583927e-05,
      "loss": 2.1243,
      "step": 45060
    },
    {
      "epoch": 22.92472024415056,
      "grad_norm": 30.297069549560547,
      "learning_rate": 2.707527975584944e-05,
      "loss": 2.0782,
      "step": 45070
    },
    {
      "epoch": 22.929806714140387,
      "grad_norm": 43.73365020751953,
      "learning_rate": 2.7070193285859614e-05,
      "loss": 2.0843,
      "step": 45080
    },
    {
      "epoch": 22.934893184130214,
      "grad_norm": 33.18136215209961,
      "learning_rate": 2.706510681586979e-05,
      "loss": 2.0756,
      "step": 45090
    },
    {
      "epoch": 22.93997965412004,
      "grad_norm": 32.778785705566406,
      "learning_rate": 2.706002034587996e-05,
      "loss": 2.1184,
      "step": 45100
    },
    {
      "epoch": 22.945066124109868,
      "grad_norm": 33.396942138671875,
      "learning_rate": 2.705493387589013e-05,
      "loss": 2.0657,
      "step": 45110
    },
    {
      "epoch": 22.950152594099695,
      "grad_norm": 25.85683822631836,
      "learning_rate": 2.7049847405900308e-05,
      "loss": 1.9801,
      "step": 45120
    },
    {
      "epoch": 22.955239064089522,
      "grad_norm": 33.016536712646484,
      "learning_rate": 2.704476093591048e-05,
      "loss": 2.0201,
      "step": 45130
    },
    {
      "epoch": 22.96032553407935,
      "grad_norm": 35.47407531738281,
      "learning_rate": 2.703967446592065e-05,
      "loss": 2.0726,
      "step": 45140
    },
    {
      "epoch": 22.965412004069176,
      "grad_norm": 36.094112396240234,
      "learning_rate": 2.7034587995930827e-05,
      "loss": 1.9966,
      "step": 45150
    },
    {
      "epoch": 22.970498474059003,
      "grad_norm": 35.72618865966797,
      "learning_rate": 2.7029501525940997e-05,
      "loss": 2.046,
      "step": 45160
    },
    {
      "epoch": 22.97558494404883,
      "grad_norm": 37.698036193847656,
      "learning_rate": 2.7024415055951167e-05,
      "loss": 2.026,
      "step": 45170
    },
    {
      "epoch": 22.980671414038657,
      "grad_norm": 41.112979888916016,
      "learning_rate": 2.7019328585961344e-05,
      "loss": 2.013,
      "step": 45180
    },
    {
      "epoch": 22.985757884028484,
      "grad_norm": 27.608243942260742,
      "learning_rate": 2.7014242115971517e-05,
      "loss": 2.0371,
      "step": 45190
    },
    {
      "epoch": 22.99084435401831,
      "grad_norm": 30.352872848510742,
      "learning_rate": 2.7009155645981687e-05,
      "loss": 2.0427,
      "step": 45200
    },
    {
      "epoch": 22.995930824008138,
      "grad_norm": 32.91804122924805,
      "learning_rate": 2.7004069175991864e-05,
      "loss": 2.0814,
      "step": 45210
    },
    {
      "epoch": 23.0,
      "eval_loss": 4.473212242126465,
      "eval_runtime": 3.2793,
      "eval_samples_per_second": 846.228,
      "eval_steps_per_second": 105.817,
      "step": 45218
    },
    {
      "epoch": 23.001017293997965,
      "grad_norm": 35.85939025878906,
      "learning_rate": 2.6998982706002034e-05,
      "loss": 1.9965,
      "step": 45220
    },
    {
      "epoch": 23.00610376398779,
      "grad_norm": 34.30508804321289,
      "learning_rate": 2.6993896236012207e-05,
      "loss": 2.0749,
      "step": 45230
    },
    {
      "epoch": 23.01119023397762,
      "grad_norm": 40.285709381103516,
      "learning_rate": 2.6988809766022384e-05,
      "loss": 2.0953,
      "step": 45240
    },
    {
      "epoch": 23.016276703967446,
      "grad_norm": 30.1495304107666,
      "learning_rate": 2.6983723296032553e-05,
      "loss": 2.0646,
      "step": 45250
    },
    {
      "epoch": 23.021363173957273,
      "grad_norm": 38.53070068359375,
      "learning_rate": 2.697863682604273e-05,
      "loss": 2.0812,
      "step": 45260
    },
    {
      "epoch": 23.0264496439471,
      "grad_norm": 30.337646484375,
      "learning_rate": 2.69735503560529e-05,
      "loss": 2.0655,
      "step": 45270
    },
    {
      "epoch": 23.031536113936927,
      "grad_norm": 34.231056213378906,
      "learning_rate": 2.6968463886063073e-05,
      "loss": 1.9212,
      "step": 45280
    },
    {
      "epoch": 23.036622583926754,
      "grad_norm": 29.854442596435547,
      "learning_rate": 2.696337741607325e-05,
      "loss": 2.0455,
      "step": 45290
    },
    {
      "epoch": 23.04170905391658,
      "grad_norm": 42.02571105957031,
      "learning_rate": 2.695829094608342e-05,
      "loss": 1.9875,
      "step": 45300
    },
    {
      "epoch": 23.046795523906408,
      "grad_norm": 34.11883544921875,
      "learning_rate": 2.695320447609359e-05,
      "loss": 1.9861,
      "step": 45310
    },
    {
      "epoch": 23.051881993896234,
      "grad_norm": 34.78714370727539,
      "learning_rate": 2.6948118006103766e-05,
      "loss": 2.0949,
      "step": 45320
    },
    {
      "epoch": 23.05696846388606,
      "grad_norm": 44.61876678466797,
      "learning_rate": 2.694303153611394e-05,
      "loss": 2.0541,
      "step": 45330
    },
    {
      "epoch": 23.06205493387589,
      "grad_norm": 31.708406448364258,
      "learning_rate": 2.693794506612411e-05,
      "loss": 1.9951,
      "step": 45340
    },
    {
      "epoch": 23.067141403865715,
      "grad_norm": 33.50127410888672,
      "learning_rate": 2.6932858596134286e-05,
      "loss": 2.0551,
      "step": 45350
    },
    {
      "epoch": 23.072227873855546,
      "grad_norm": 37.87872314453125,
      "learning_rate": 2.6927772126144456e-05,
      "loss": 2.0622,
      "step": 45360
    },
    {
      "epoch": 23.077314343845373,
      "grad_norm": 36.36692428588867,
      "learning_rate": 2.692268565615463e-05,
      "loss": 2.0405,
      "step": 45370
    },
    {
      "epoch": 23.0824008138352,
      "grad_norm": 39.704952239990234,
      "learning_rate": 2.6917599186164806e-05,
      "loss": 1.9993,
      "step": 45380
    },
    {
      "epoch": 23.087487283825027,
      "grad_norm": 28.125717163085938,
      "learning_rate": 2.6912512716174976e-05,
      "loss": 2.0412,
      "step": 45390
    },
    {
      "epoch": 23.092573753814854,
      "grad_norm": 36.13299560546875,
      "learning_rate": 2.6907426246185146e-05,
      "loss": 2.0516,
      "step": 45400
    },
    {
      "epoch": 23.09766022380468,
      "grad_norm": 36.291194915771484,
      "learning_rate": 2.6902339776195323e-05,
      "loss": 1.9746,
      "step": 45410
    },
    {
      "epoch": 23.102746693794508,
      "grad_norm": 35.89059829711914,
      "learning_rate": 2.6897253306205496e-05,
      "loss": 1.948,
      "step": 45420
    },
    {
      "epoch": 23.107833163784335,
      "grad_norm": 32.008949279785156,
      "learning_rate": 2.6892166836215666e-05,
      "loss": 1.8905,
      "step": 45430
    },
    {
      "epoch": 23.112919633774162,
      "grad_norm": 31.6400203704834,
      "learning_rate": 2.6887080366225842e-05,
      "loss": 1.993,
      "step": 45440
    },
    {
      "epoch": 23.11800610376399,
      "grad_norm": 34.08035659790039,
      "learning_rate": 2.6881993896236012e-05,
      "loss": 1.9856,
      "step": 45450
    },
    {
      "epoch": 23.123092573753816,
      "grad_norm": 35.425392150878906,
      "learning_rate": 2.6876907426246186e-05,
      "loss": 2.0843,
      "step": 45460
    },
    {
      "epoch": 23.128179043743643,
      "grad_norm": 31.665361404418945,
      "learning_rate": 2.687182095625636e-05,
      "loss": 2.0583,
      "step": 45470
    },
    {
      "epoch": 23.13326551373347,
      "grad_norm": 34.599151611328125,
      "learning_rate": 2.6866734486266532e-05,
      "loss": 2.0315,
      "step": 45480
    },
    {
      "epoch": 23.138351983723297,
      "grad_norm": 40.11334228515625,
      "learning_rate": 2.6861648016276702e-05,
      "loss": 2.0266,
      "step": 45490
    },
    {
      "epoch": 23.143438453713124,
      "grad_norm": 37.36092758178711,
      "learning_rate": 2.685656154628688e-05,
      "loss": 2.028,
      "step": 45500
    },
    {
      "epoch": 23.14852492370295,
      "grad_norm": 36.883506774902344,
      "learning_rate": 2.685147507629705e-05,
      "loss": 2.0123,
      "step": 45510
    },
    {
      "epoch": 23.153611393692778,
      "grad_norm": 34.42063522338867,
      "learning_rate": 2.6846388606307225e-05,
      "loss": 2.0244,
      "step": 45520
    },
    {
      "epoch": 23.158697863682605,
      "grad_norm": 39.69327926635742,
      "learning_rate": 2.68413021363174e-05,
      "loss": 2.0687,
      "step": 45530
    },
    {
      "epoch": 23.16378433367243,
      "grad_norm": 34.29756546020508,
      "learning_rate": 2.683621566632757e-05,
      "loss": 2.0163,
      "step": 45540
    },
    {
      "epoch": 23.16887080366226,
      "grad_norm": 38.425254821777344,
      "learning_rate": 2.6831129196337745e-05,
      "loss": 2.0883,
      "step": 45550
    },
    {
      "epoch": 23.173957273652086,
      "grad_norm": 35.54791259765625,
      "learning_rate": 2.6826042726347915e-05,
      "loss": 2.059,
      "step": 45560
    },
    {
      "epoch": 23.179043743641913,
      "grad_norm": 31.87934112548828,
      "learning_rate": 2.682095625635809e-05,
      "loss": 2.0986,
      "step": 45570
    },
    {
      "epoch": 23.18413021363174,
      "grad_norm": 42.22715759277344,
      "learning_rate": 2.6815869786368265e-05,
      "loss": 2.1101,
      "step": 45580
    },
    {
      "epoch": 23.189216683621567,
      "grad_norm": 33.00725555419922,
      "learning_rate": 2.6810783316378435e-05,
      "loss": 1.9646,
      "step": 45590
    },
    {
      "epoch": 23.194303153611393,
      "grad_norm": 34.691551208496094,
      "learning_rate": 2.6805696846388605e-05,
      "loss": 2.0038,
      "step": 45600
    },
    {
      "epoch": 23.19938962360122,
      "grad_norm": 35.36833953857422,
      "learning_rate": 2.680061037639878e-05,
      "loss": 1.992,
      "step": 45610
    },
    {
      "epoch": 23.204476093591047,
      "grad_norm": 36.9212760925293,
      "learning_rate": 2.6795523906408955e-05,
      "loss": 2.0407,
      "step": 45620
    },
    {
      "epoch": 23.209562563580874,
      "grad_norm": 32.20200729370117,
      "learning_rate": 2.6790437436419125e-05,
      "loss": 2.0463,
      "step": 45630
    },
    {
      "epoch": 23.2146490335707,
      "grad_norm": 30.062667846679688,
      "learning_rate": 2.67853509664293e-05,
      "loss": 1.956,
      "step": 45640
    },
    {
      "epoch": 23.21973550356053,
      "grad_norm": 32.731998443603516,
      "learning_rate": 2.678026449643947e-05,
      "loss": 2.0693,
      "step": 45650
    },
    {
      "epoch": 23.224821973550355,
      "grad_norm": 34.0152587890625,
      "learning_rate": 2.6775178026449644e-05,
      "loss": 2.0054,
      "step": 45660
    },
    {
      "epoch": 23.229908443540182,
      "grad_norm": 27.747297286987305,
      "learning_rate": 2.677009155645982e-05,
      "loss": 1.9648,
      "step": 45670
    },
    {
      "epoch": 23.23499491353001,
      "grad_norm": 31.8016357421875,
      "learning_rate": 2.676500508646999e-05,
      "loss": 2.0039,
      "step": 45680
    },
    {
      "epoch": 23.240081383519836,
      "grad_norm": 44.33107376098633,
      "learning_rate": 2.675991861648016e-05,
      "loss": 2.026,
      "step": 45690
    },
    {
      "epoch": 23.245167853509663,
      "grad_norm": 30.1732177734375,
      "learning_rate": 2.6754832146490338e-05,
      "loss": 1.9822,
      "step": 45700
    },
    {
      "epoch": 23.25025432349949,
      "grad_norm": 35.80350875854492,
      "learning_rate": 2.674974567650051e-05,
      "loss": 1.9951,
      "step": 45710
    },
    {
      "epoch": 23.255340793489317,
      "grad_norm": 41.92422866821289,
      "learning_rate": 2.674465920651068e-05,
      "loss": 2.0533,
      "step": 45720
    },
    {
      "epoch": 23.260427263479144,
      "grad_norm": 30.66621208190918,
      "learning_rate": 2.6739572736520857e-05,
      "loss": 2.0661,
      "step": 45730
    },
    {
      "epoch": 23.26551373346897,
      "grad_norm": 42.72330093383789,
      "learning_rate": 2.6734486266531027e-05,
      "loss": 2.0268,
      "step": 45740
    },
    {
      "epoch": 23.270600203458798,
      "grad_norm": 33.754634857177734,
      "learning_rate": 2.67293997965412e-05,
      "loss": 2.0782,
      "step": 45750
    },
    {
      "epoch": 23.275686673448625,
      "grad_norm": 49.7157096862793,
      "learning_rate": 2.6724313326551377e-05,
      "loss": 2.0118,
      "step": 45760
    },
    {
      "epoch": 23.280773143438452,
      "grad_norm": 32.77764892578125,
      "learning_rate": 2.6719226856561547e-05,
      "loss": 2.049,
      "step": 45770
    },
    {
      "epoch": 23.28585961342828,
      "grad_norm": 35.620426177978516,
      "learning_rate": 2.6714140386571724e-05,
      "loss": 1.9572,
      "step": 45780
    },
    {
      "epoch": 23.290946083418106,
      "grad_norm": 41.49407196044922,
      "learning_rate": 2.6709053916581894e-05,
      "loss": 2.012,
      "step": 45790
    },
    {
      "epoch": 23.296032553407933,
      "grad_norm": 34.57987976074219,
      "learning_rate": 2.6703967446592064e-05,
      "loss": 2.0574,
      "step": 45800
    },
    {
      "epoch": 23.301119023397764,
      "grad_norm": 31.722932815551758,
      "learning_rate": 2.669888097660224e-05,
      "loss": 1.9825,
      "step": 45810
    },
    {
      "epoch": 23.30620549338759,
      "grad_norm": 31.9809513092041,
      "learning_rate": 2.6693794506612414e-05,
      "loss": 2.0695,
      "step": 45820
    },
    {
      "epoch": 23.311291963377418,
      "grad_norm": 33.70310592651367,
      "learning_rate": 2.6688708036622583e-05,
      "loss": 2.0919,
      "step": 45830
    },
    {
      "epoch": 23.316378433367245,
      "grad_norm": 32.96799850463867,
      "learning_rate": 2.668362156663276e-05,
      "loss": 2.1345,
      "step": 45840
    },
    {
      "epoch": 23.32146490335707,
      "grad_norm": 39.789302825927734,
      "learning_rate": 2.667853509664293e-05,
      "loss": 2.1105,
      "step": 45850
    },
    {
      "epoch": 23.3265513733469,
      "grad_norm": 34.32497024536133,
      "learning_rate": 2.6673448626653103e-05,
      "loss": 2.0119,
      "step": 45860
    },
    {
      "epoch": 23.331637843336726,
      "grad_norm": 29.243295669555664,
      "learning_rate": 2.666836215666328e-05,
      "loss": 2.0501,
      "step": 45870
    },
    {
      "epoch": 23.336724313326553,
      "grad_norm": 30.8122501373291,
      "learning_rate": 2.666327568667345e-05,
      "loss": 2.0186,
      "step": 45880
    },
    {
      "epoch": 23.34181078331638,
      "grad_norm": 34.17912292480469,
      "learning_rate": 2.665818921668362e-05,
      "loss": 1.9772,
      "step": 45890
    },
    {
      "epoch": 23.346897253306206,
      "grad_norm": 31.576271057128906,
      "learning_rate": 2.6653102746693796e-05,
      "loss": 2.0733,
      "step": 45900
    },
    {
      "epoch": 23.351983723296033,
      "grad_norm": 31.33445167541504,
      "learning_rate": 2.664801627670397e-05,
      "loss": 2.0812,
      "step": 45910
    },
    {
      "epoch": 23.35707019328586,
      "grad_norm": 33.60533142089844,
      "learning_rate": 2.664292980671414e-05,
      "loss": 2.02,
      "step": 45920
    },
    {
      "epoch": 23.362156663275687,
      "grad_norm": 28.052663803100586,
      "learning_rate": 2.6637843336724316e-05,
      "loss": 2.0384,
      "step": 45930
    },
    {
      "epoch": 23.367243133265514,
      "grad_norm": 32.14857864379883,
      "learning_rate": 2.6632756866734486e-05,
      "loss": 2.017,
      "step": 45940
    },
    {
      "epoch": 23.37232960325534,
      "grad_norm": 32.43165588378906,
      "learning_rate": 2.662767039674466e-05,
      "loss": 2.0163,
      "step": 45950
    },
    {
      "epoch": 23.37741607324517,
      "grad_norm": 28.83259391784668,
      "learning_rate": 2.6622583926754836e-05,
      "loss": 2.0343,
      "step": 45960
    },
    {
      "epoch": 23.382502543234995,
      "grad_norm": 34.95424270629883,
      "learning_rate": 2.6617497456765006e-05,
      "loss": 2.0643,
      "step": 45970
    },
    {
      "epoch": 23.387589013224822,
      "grad_norm": 28.920568466186523,
      "learning_rate": 2.6612410986775176e-05,
      "loss": 1.9698,
      "step": 45980
    },
    {
      "epoch": 23.39267548321465,
      "grad_norm": 33.783878326416016,
      "learning_rate": 2.6607324516785353e-05,
      "loss": 2.0489,
      "step": 45990
    },
    {
      "epoch": 23.397761953204476,
      "grad_norm": 34.92576217651367,
      "learning_rate": 2.6602238046795526e-05,
      "loss": 1.9847,
      "step": 46000
    },
    {
      "epoch": 23.402848423194303,
      "grad_norm": 34.58087158203125,
      "learning_rate": 2.6597151576805696e-05,
      "loss": 2.0093,
      "step": 46010
    },
    {
      "epoch": 23.40793489318413,
      "grad_norm": 36.91884994506836,
      "learning_rate": 2.6592065106815872e-05,
      "loss": 2.0432,
      "step": 46020
    },
    {
      "epoch": 23.413021363173957,
      "grad_norm": 30.081523895263672,
      "learning_rate": 2.6586978636826042e-05,
      "loss": 1.9718,
      "step": 46030
    },
    {
      "epoch": 23.418107833163784,
      "grad_norm": 29.25042152404785,
      "learning_rate": 2.6581892166836216e-05,
      "loss": 2.0036,
      "step": 46040
    },
    {
      "epoch": 23.42319430315361,
      "grad_norm": 44.44661331176758,
      "learning_rate": 2.6576805696846392e-05,
      "loss": 1.9681,
      "step": 46050
    },
    {
      "epoch": 23.428280773143438,
      "grad_norm": 35.44632339477539,
      "learning_rate": 2.6571719226856562e-05,
      "loss": 2.0291,
      "step": 46060
    },
    {
      "epoch": 23.433367243133265,
      "grad_norm": 38.825645446777344,
      "learning_rate": 2.656663275686674e-05,
      "loss": 2.0117,
      "step": 46070
    },
    {
      "epoch": 23.438453713123092,
      "grad_norm": 40.603904724121094,
      "learning_rate": 2.656154628687691e-05,
      "loss": 1.8643,
      "step": 46080
    },
    {
      "epoch": 23.44354018311292,
      "grad_norm": 30.287914276123047,
      "learning_rate": 2.6556459816887082e-05,
      "loss": 1.9454,
      "step": 46090
    },
    {
      "epoch": 23.448626653102746,
      "grad_norm": 38.257320404052734,
      "learning_rate": 2.6551373346897255e-05,
      "loss": 2.026,
      "step": 46100
    },
    {
      "epoch": 23.453713123092573,
      "grad_norm": 30.806957244873047,
      "learning_rate": 2.654628687690743e-05,
      "loss": 2.0262,
      "step": 46110
    },
    {
      "epoch": 23.4587995930824,
      "grad_norm": 31.355287551879883,
      "learning_rate": 2.65412004069176e-05,
      "loss": 1.9804,
      "step": 46120
    },
    {
      "epoch": 23.463886063072227,
      "grad_norm": 32.19845199584961,
      "learning_rate": 2.6536113936927775e-05,
      "loss": 2.0611,
      "step": 46130
    },
    {
      "epoch": 23.468972533062054,
      "grad_norm": 39.51015853881836,
      "learning_rate": 2.6531027466937945e-05,
      "loss": 2.0554,
      "step": 46140
    },
    {
      "epoch": 23.47405900305188,
      "grad_norm": 38.206207275390625,
      "learning_rate": 2.652594099694812e-05,
      "loss": 2.0262,
      "step": 46150
    },
    {
      "epoch": 23.479145473041708,
      "grad_norm": 29.88613510131836,
      "learning_rate": 2.6520854526958295e-05,
      "loss": 2.1266,
      "step": 46160
    },
    {
      "epoch": 23.484231943031535,
      "grad_norm": 43.180233001708984,
      "learning_rate": 2.6515768056968465e-05,
      "loss": 1.9805,
      "step": 46170
    },
    {
      "epoch": 23.489318413021362,
      "grad_norm": 36.25638961791992,
      "learning_rate": 2.6510681586978635e-05,
      "loss": 1.9192,
      "step": 46180
    },
    {
      "epoch": 23.49440488301119,
      "grad_norm": 34.100196838378906,
      "learning_rate": 2.650559511698881e-05,
      "loss": 2.0677,
      "step": 46190
    },
    {
      "epoch": 23.499491353001016,
      "grad_norm": 34.66936111450195,
      "learning_rate": 2.6500508646998985e-05,
      "loss": 1.9587,
      "step": 46200
    },
    {
      "epoch": 23.504577822990843,
      "grad_norm": 35.93681335449219,
      "learning_rate": 2.6495422177009155e-05,
      "loss": 2.0298,
      "step": 46210
    },
    {
      "epoch": 23.50966429298067,
      "grad_norm": 35.7129020690918,
      "learning_rate": 2.649033570701933e-05,
      "loss": 1.9241,
      "step": 46220
    },
    {
      "epoch": 23.514750762970497,
      "grad_norm": 41.64299392700195,
      "learning_rate": 2.64852492370295e-05,
      "loss": 2.0716,
      "step": 46230
    },
    {
      "epoch": 23.519837232960327,
      "grad_norm": 36.00650405883789,
      "learning_rate": 2.6480162767039674e-05,
      "loss": 1.9265,
      "step": 46240
    },
    {
      "epoch": 23.524923702950154,
      "grad_norm": 33.00190353393555,
      "learning_rate": 2.647507629704985e-05,
      "loss": 1.9793,
      "step": 46250
    },
    {
      "epoch": 23.53001017293998,
      "grad_norm": 36.385623931884766,
      "learning_rate": 2.646998982706002e-05,
      "loss": 1.9993,
      "step": 46260
    },
    {
      "epoch": 23.53509664292981,
      "grad_norm": 27.564903259277344,
      "learning_rate": 2.646490335707019e-05,
      "loss": 2.0338,
      "step": 46270
    },
    {
      "epoch": 23.540183112919635,
      "grad_norm": 43.16294479370117,
      "learning_rate": 2.6459816887080368e-05,
      "loss": 2.0723,
      "step": 46280
    },
    {
      "epoch": 23.545269582909462,
      "grad_norm": 50.195831298828125,
      "learning_rate": 2.645473041709054e-05,
      "loss": 2.005,
      "step": 46290
    },
    {
      "epoch": 23.55035605289929,
      "grad_norm": 38.399253845214844,
      "learning_rate": 2.644964394710071e-05,
      "loss": 1.9921,
      "step": 46300
    },
    {
      "epoch": 23.555442522889116,
      "grad_norm": 31.575681686401367,
      "learning_rate": 2.6444557477110887e-05,
      "loss": 1.9975,
      "step": 46310
    },
    {
      "epoch": 23.560528992878943,
      "grad_norm": 34.51800537109375,
      "learning_rate": 2.6439471007121057e-05,
      "loss": 1.9543,
      "step": 46320
    },
    {
      "epoch": 23.56561546286877,
      "grad_norm": 42.20923614501953,
      "learning_rate": 2.6434384537131234e-05,
      "loss": 2.0607,
      "step": 46330
    },
    {
      "epoch": 23.570701932858597,
      "grad_norm": 49.41324234008789,
      "learning_rate": 2.6429298067141407e-05,
      "loss": 1.9186,
      "step": 46340
    },
    {
      "epoch": 23.575788402848424,
      "grad_norm": 37.792259216308594,
      "learning_rate": 2.6424211597151577e-05,
      "loss": 1.9832,
      "step": 46350
    },
    {
      "epoch": 23.58087487283825,
      "grad_norm": 31.793045043945312,
      "learning_rate": 2.6419125127161754e-05,
      "loss": 2.0003,
      "step": 46360
    },
    {
      "epoch": 23.585961342828078,
      "grad_norm": 33.707515716552734,
      "learning_rate": 2.6414038657171924e-05,
      "loss": 2.0305,
      "step": 46370
    },
    {
      "epoch": 23.591047812817905,
      "grad_norm": 34.11482238769531,
      "learning_rate": 2.6408952187182097e-05,
      "loss": 2.1333,
      "step": 46380
    },
    {
      "epoch": 23.596134282807732,
      "grad_norm": 33.97135543823242,
      "learning_rate": 2.6403865717192274e-05,
      "loss": 1.964,
      "step": 46390
    },
    {
      "epoch": 23.60122075279756,
      "grad_norm": 35.80574035644531,
      "learning_rate": 2.6398779247202444e-05,
      "loss": 1.9056,
      "step": 46400
    },
    {
      "epoch": 23.606307222787386,
      "grad_norm": 29.587417602539062,
      "learning_rate": 2.6393692777212614e-05,
      "loss": 2.0061,
      "step": 46410
    },
    {
      "epoch": 23.611393692777213,
      "grad_norm": 46.87436294555664,
      "learning_rate": 2.638860630722279e-05,
      "loss": 2.0272,
      "step": 46420
    },
    {
      "epoch": 23.61648016276704,
      "grad_norm": 44.35063552856445,
      "learning_rate": 2.638351983723296e-05,
      "loss": 1.9181,
      "step": 46430
    },
    {
      "epoch": 23.621566632756867,
      "grad_norm": 37.593597412109375,
      "learning_rate": 2.6378433367243133e-05,
      "loss": 1.9855,
      "step": 46440
    },
    {
      "epoch": 23.626653102746694,
      "grad_norm": 32.12263107299805,
      "learning_rate": 2.637334689725331e-05,
      "loss": 2.0055,
      "step": 46450
    },
    {
      "epoch": 23.63173957273652,
      "grad_norm": 37.046546936035156,
      "learning_rate": 2.636826042726348e-05,
      "loss": 2.01,
      "step": 46460
    },
    {
      "epoch": 23.636826042726348,
      "grad_norm": 33.416404724121094,
      "learning_rate": 2.636317395727365e-05,
      "loss": 2.0385,
      "step": 46470
    },
    {
      "epoch": 23.641912512716175,
      "grad_norm": 46.653564453125,
      "learning_rate": 2.6358087487283826e-05,
      "loss": 2.0889,
      "step": 46480
    },
    {
      "epoch": 23.646998982706002,
      "grad_norm": 42.10287857055664,
      "learning_rate": 2.6353001017294e-05,
      "loss": 2.0646,
      "step": 46490
    },
    {
      "epoch": 23.65208545269583,
      "grad_norm": 40.40158462524414,
      "learning_rate": 2.634791454730417e-05,
      "loss": 2.0396,
      "step": 46500
    },
    {
      "epoch": 23.657171922685656,
      "grad_norm": 36.849586486816406,
      "learning_rate": 2.6342828077314346e-05,
      "loss": 2.0533,
      "step": 46510
    },
    {
      "epoch": 23.662258392675483,
      "grad_norm": 36.30702209472656,
      "learning_rate": 2.6337741607324516e-05,
      "loss": 2.0231,
      "step": 46520
    },
    {
      "epoch": 23.66734486266531,
      "grad_norm": 36.819114685058594,
      "learning_rate": 2.633265513733469e-05,
      "loss": 2.0311,
      "step": 46530
    },
    {
      "epoch": 23.672431332655137,
      "grad_norm": 44.726802825927734,
      "learning_rate": 2.6327568667344866e-05,
      "loss": 1.885,
      "step": 46540
    },
    {
      "epoch": 23.677517802644964,
      "grad_norm": 40.802467346191406,
      "learning_rate": 2.6322482197355036e-05,
      "loss": 2.0111,
      "step": 46550
    },
    {
      "epoch": 23.68260427263479,
      "grad_norm": 36.684661865234375,
      "learning_rate": 2.6317395727365206e-05,
      "loss": 1.9461,
      "step": 46560
    },
    {
      "epoch": 23.687690742624618,
      "grad_norm": 32.15298080444336,
      "learning_rate": 2.6312309257375383e-05,
      "loss": 2.0109,
      "step": 46570
    },
    {
      "epoch": 23.692777212614445,
      "grad_norm": 34.676937103271484,
      "learning_rate": 2.6307222787385556e-05,
      "loss": 1.9785,
      "step": 46580
    },
    {
      "epoch": 23.69786368260427,
      "grad_norm": 38.874637603759766,
      "learning_rate": 2.6302136317395733e-05,
      "loss": 2.0917,
      "step": 46590
    },
    {
      "epoch": 23.7029501525941,
      "grad_norm": 36.829891204833984,
      "learning_rate": 2.6297049847405902e-05,
      "loss": 1.9936,
      "step": 46600
    },
    {
      "epoch": 23.708036622583926,
      "grad_norm": 28.0228328704834,
      "learning_rate": 2.6291963377416072e-05,
      "loss": 2.0193,
      "step": 46610
    },
    {
      "epoch": 23.713123092573753,
      "grad_norm": 32.103153228759766,
      "learning_rate": 2.628687690742625e-05,
      "loss": 1.9424,
      "step": 46620
    },
    {
      "epoch": 23.71820956256358,
      "grad_norm": 36.64055252075195,
      "learning_rate": 2.6281790437436422e-05,
      "loss": 2.0015,
      "step": 46630
    },
    {
      "epoch": 23.723296032553407,
      "grad_norm": 46.8184814453125,
      "learning_rate": 2.6276703967446592e-05,
      "loss": 1.9946,
      "step": 46640
    },
    {
      "epoch": 23.728382502543234,
      "grad_norm": 42.22957229614258,
      "learning_rate": 2.627161749745677e-05,
      "loss": 1.9544,
      "step": 46650
    },
    {
      "epoch": 23.73346897253306,
      "grad_norm": 36.46739959716797,
      "learning_rate": 2.626653102746694e-05,
      "loss": 2.0012,
      "step": 46660
    },
    {
      "epoch": 23.738555442522888,
      "grad_norm": 31.263561248779297,
      "learning_rate": 2.6261444557477112e-05,
      "loss": 1.9964,
      "step": 46670
    },
    {
      "epoch": 23.743641912512714,
      "grad_norm": 33.98554611206055,
      "learning_rate": 2.625635808748729e-05,
      "loss": 2.0782,
      "step": 46680
    },
    {
      "epoch": 23.74872838250254,
      "grad_norm": 37.7291259765625,
      "learning_rate": 2.625127161749746e-05,
      "loss": 1.9348,
      "step": 46690
    },
    {
      "epoch": 23.753814852492372,
      "grad_norm": 30.655488967895508,
      "learning_rate": 2.624618514750763e-05,
      "loss": 1.9922,
      "step": 46700
    },
    {
      "epoch": 23.7589013224822,
      "grad_norm": 30.579381942749023,
      "learning_rate": 2.6241098677517805e-05,
      "loss": 1.9846,
      "step": 46710
    },
    {
      "epoch": 23.763987792472026,
      "grad_norm": 35.63555908203125,
      "learning_rate": 2.623601220752798e-05,
      "loss": 2.0801,
      "step": 46720
    },
    {
      "epoch": 23.769074262461853,
      "grad_norm": 38.500518798828125,
      "learning_rate": 2.623092573753815e-05,
      "loss": 1.9604,
      "step": 46730
    },
    {
      "epoch": 23.77416073245168,
      "grad_norm": 34.755958557128906,
      "learning_rate": 2.6225839267548325e-05,
      "loss": 2.0132,
      "step": 46740
    },
    {
      "epoch": 23.779247202441507,
      "grad_norm": 32.61661148071289,
      "learning_rate": 2.6220752797558495e-05,
      "loss": 1.9878,
      "step": 46750
    },
    {
      "epoch": 23.784333672431334,
      "grad_norm": 40.19924545288086,
      "learning_rate": 2.6215666327568665e-05,
      "loss": 1.9881,
      "step": 46760
    },
    {
      "epoch": 23.78942014242116,
      "grad_norm": 38.93104553222656,
      "learning_rate": 2.621057985757884e-05,
      "loss": 2.0319,
      "step": 46770
    },
    {
      "epoch": 23.794506612410988,
      "grad_norm": 40.06427764892578,
      "learning_rate": 2.6205493387589015e-05,
      "loss": 1.9825,
      "step": 46780
    },
    {
      "epoch": 23.799593082400815,
      "grad_norm": 34.89834213256836,
      "learning_rate": 2.6200406917599185e-05,
      "loss": 2.0415,
      "step": 46790
    },
    {
      "epoch": 23.804679552390642,
      "grad_norm": 33.8733024597168,
      "learning_rate": 2.619532044760936e-05,
      "loss": 1.9925,
      "step": 46800
    },
    {
      "epoch": 23.80976602238047,
      "grad_norm": 35.956302642822266,
      "learning_rate": 2.619023397761953e-05,
      "loss": 1.9494,
      "step": 46810
    },
    {
      "epoch": 23.814852492370296,
      "grad_norm": 36.25482177734375,
      "learning_rate": 2.6185147507629705e-05,
      "loss": 2.0171,
      "step": 46820
    },
    {
      "epoch": 23.819938962360123,
      "grad_norm": 26.919979095458984,
      "learning_rate": 2.618006103763988e-05,
      "loss": 2.0112,
      "step": 46830
    },
    {
      "epoch": 23.82502543234995,
      "grad_norm": 32.62968063354492,
      "learning_rate": 2.617497456765005e-05,
      "loss": 1.9499,
      "step": 46840
    },
    {
      "epoch": 23.830111902339777,
      "grad_norm": 29.219329833984375,
      "learning_rate": 2.616988809766022e-05,
      "loss": 1.9929,
      "step": 46850
    },
    {
      "epoch": 23.835198372329604,
      "grad_norm": 30.118680953979492,
      "learning_rate": 2.6164801627670398e-05,
      "loss": 2.0058,
      "step": 46860
    },
    {
      "epoch": 23.84028484231943,
      "grad_norm": 30.721384048461914,
      "learning_rate": 2.615971515768057e-05,
      "loss": 1.9539,
      "step": 46870
    },
    {
      "epoch": 23.845371312309258,
      "grad_norm": 35.91325759887695,
      "learning_rate": 2.6154628687690748e-05,
      "loss": 1.8953,
      "step": 46880
    },
    {
      "epoch": 23.850457782299085,
      "grad_norm": 28.077468872070312,
      "learning_rate": 2.6149542217700917e-05,
      "loss": 1.9737,
      "step": 46890
    },
    {
      "epoch": 23.85554425228891,
      "grad_norm": 31.894899368286133,
      "learning_rate": 2.6144455747711087e-05,
      "loss": 2.0805,
      "step": 46900
    },
    {
      "epoch": 23.86063072227874,
      "grad_norm": 33.46664810180664,
      "learning_rate": 2.6139369277721264e-05,
      "loss": 1.9482,
      "step": 46910
    },
    {
      "epoch": 23.865717192268566,
      "grad_norm": 34.810176849365234,
      "learning_rate": 2.6134282807731437e-05,
      "loss": 2.0047,
      "step": 46920
    },
    {
      "epoch": 23.870803662258393,
      "grad_norm": 32.965667724609375,
      "learning_rate": 2.6129196337741607e-05,
      "loss": 2.0181,
      "step": 46930
    },
    {
      "epoch": 23.87589013224822,
      "grad_norm": 35.32907485961914,
      "learning_rate": 2.6124109867751784e-05,
      "loss": 1.8869,
      "step": 46940
    },
    {
      "epoch": 23.880976602238047,
      "grad_norm": 37.28916931152344,
      "learning_rate": 2.6119023397761954e-05,
      "loss": 1.9897,
      "step": 46950
    },
    {
      "epoch": 23.886063072227874,
      "grad_norm": 38.59625244140625,
      "learning_rate": 2.6113936927772127e-05,
      "loss": 1.9165,
      "step": 46960
    },
    {
      "epoch": 23.8911495422177,
      "grad_norm": 58.34353256225586,
      "learning_rate": 2.6108850457782304e-05,
      "loss": 1.9064,
      "step": 46970
    },
    {
      "epoch": 23.896236012207527,
      "grad_norm": 30.660724639892578,
      "learning_rate": 2.6103763987792474e-05,
      "loss": 1.8983,
      "step": 46980
    },
    {
      "epoch": 23.901322482197354,
      "grad_norm": 33.37619400024414,
      "learning_rate": 2.6098677517802644e-05,
      "loss": 2.0726,
      "step": 46990
    },
    {
      "epoch": 23.90640895218718,
      "grad_norm": 33.512908935546875,
      "learning_rate": 2.609359104781282e-05,
      "loss": 1.9379,
      "step": 47000
    },
    {
      "epoch": 23.91149542217701,
      "grad_norm": 26.978551864624023,
      "learning_rate": 2.6088504577822993e-05,
      "loss": 2.0035,
      "step": 47010
    },
    {
      "epoch": 23.916581892166835,
      "grad_norm": 33.535606384277344,
      "learning_rate": 2.6083418107833163e-05,
      "loss": 2.0323,
      "step": 47020
    },
    {
      "epoch": 23.921668362156662,
      "grad_norm": 27.759553909301758,
      "learning_rate": 2.607833163784334e-05,
      "loss": 1.9092,
      "step": 47030
    },
    {
      "epoch": 23.92675483214649,
      "grad_norm": 34.98681640625,
      "learning_rate": 2.607324516785351e-05,
      "loss": 1.9532,
      "step": 47040
    },
    {
      "epoch": 23.931841302136316,
      "grad_norm": 31.15569305419922,
      "learning_rate": 2.6068158697863683e-05,
      "loss": 2.0037,
      "step": 47050
    },
    {
      "epoch": 23.936927772126143,
      "grad_norm": 34.136993408203125,
      "learning_rate": 2.6063072227873856e-05,
      "loss": 1.9788,
      "step": 47060
    },
    {
      "epoch": 23.94201424211597,
      "grad_norm": 37.42873764038086,
      "learning_rate": 2.605798575788403e-05,
      "loss": 1.9947,
      "step": 47070
    },
    {
      "epoch": 23.947100712105797,
      "grad_norm": 36.36067199707031,
      "learning_rate": 2.60528992878942e-05,
      "loss": 1.9716,
      "step": 47080
    },
    {
      "epoch": 23.952187182095624,
      "grad_norm": 31.101455688476562,
      "learning_rate": 2.6047812817904376e-05,
      "loss": 2.0341,
      "step": 47090
    },
    {
      "epoch": 23.95727365208545,
      "grad_norm": 36.33583450317383,
      "learning_rate": 2.6042726347914546e-05,
      "loss": 1.9017,
      "step": 47100
    },
    {
      "epoch": 23.962360122075278,
      "grad_norm": 36.05696105957031,
      "learning_rate": 2.603763987792472e-05,
      "loss": 2.0398,
      "step": 47110
    },
    {
      "epoch": 23.967446592065105,
      "grad_norm": 33.07676315307617,
      "learning_rate": 2.6032553407934896e-05,
      "loss": 1.9807,
      "step": 47120
    },
    {
      "epoch": 23.972533062054932,
      "grad_norm": 29.47878646850586,
      "learning_rate": 2.6027466937945066e-05,
      "loss": 1.9327,
      "step": 47130
    },
    {
      "epoch": 23.977619532044763,
      "grad_norm": 29.930566787719727,
      "learning_rate": 2.6022380467955243e-05,
      "loss": 2.0623,
      "step": 47140
    },
    {
      "epoch": 23.98270600203459,
      "grad_norm": 38.41767501831055,
      "learning_rate": 2.6017293997965413e-05,
      "loss": 1.9729,
      "step": 47150
    },
    {
      "epoch": 23.987792472024417,
      "grad_norm": 39.41700744628906,
      "learning_rate": 2.6012207527975586e-05,
      "loss": 1.9498,
      "step": 47160
    },
    {
      "epoch": 23.992878942014244,
      "grad_norm": 27.98204803466797,
      "learning_rate": 2.6007121057985763e-05,
      "loss": 1.9752,
      "step": 47170
    },
    {
      "epoch": 23.99796541200407,
      "grad_norm": 34.083518981933594,
      "learning_rate": 2.6002034587995932e-05,
      "loss": 1.984,
      "step": 47180
    },
    {
      "epoch": 24.0,
      "eval_loss": 4.479389190673828,
      "eval_runtime": 2.8722,
      "eval_samples_per_second": 966.172,
      "eval_steps_per_second": 120.815,
      "step": 47184
    },
    {
      "epoch": 24.003051881993898,
      "grad_norm": 35.25590133666992,
      "learning_rate": 2.5996948118006102e-05,
      "loss": 1.9091,
      "step": 47190
    },
    {
      "epoch": 24.008138351983725,
      "grad_norm": 43.89562225341797,
      "learning_rate": 2.599186164801628e-05,
      "loss": 1.9356,
      "step": 47200
    },
    {
      "epoch": 24.01322482197355,
      "grad_norm": 33.81155014038086,
      "learning_rate": 2.5986775178026452e-05,
      "loss": 2.0725,
      "step": 47210
    },
    {
      "epoch": 24.01831129196338,
      "grad_norm": 28.5559024810791,
      "learning_rate": 2.5981688708036622e-05,
      "loss": 1.9402,
      "step": 47220
    },
    {
      "epoch": 24.023397761953206,
      "grad_norm": 31.847972869873047,
      "learning_rate": 2.59766022380468e-05,
      "loss": 1.9466,
      "step": 47230
    },
    {
      "epoch": 24.028484231943033,
      "grad_norm": 35.522850036621094,
      "learning_rate": 2.597151576805697e-05,
      "loss": 1.9269,
      "step": 47240
    },
    {
      "epoch": 24.03357070193286,
      "grad_norm": 38.440895080566406,
      "learning_rate": 2.5966429298067142e-05,
      "loss": 2.0019,
      "step": 47250
    },
    {
      "epoch": 24.038657171922686,
      "grad_norm": 40.73480224609375,
      "learning_rate": 2.596134282807732e-05,
      "loss": 2.0334,
      "step": 47260
    },
    {
      "epoch": 24.043743641912513,
      "grad_norm": 33.7872200012207,
      "learning_rate": 2.595625635808749e-05,
      "loss": 2.0197,
      "step": 47270
    },
    {
      "epoch": 24.04883011190234,
      "grad_norm": 38.47068405151367,
      "learning_rate": 2.595116988809766e-05,
      "loss": 1.9585,
      "step": 47280
    },
    {
      "epoch": 24.053916581892167,
      "grad_norm": 37.620460510253906,
      "learning_rate": 2.5946083418107835e-05,
      "loss": 1.9842,
      "step": 47290
    },
    {
      "epoch": 24.059003051881994,
      "grad_norm": 38.92677307128906,
      "learning_rate": 2.594099694811801e-05,
      "loss": 1.8272,
      "step": 47300
    },
    {
      "epoch": 24.06408952187182,
      "grad_norm": 37.34513854980469,
      "learning_rate": 2.593591047812818e-05,
      "loss": 1.9237,
      "step": 47310
    },
    {
      "epoch": 24.06917599186165,
      "grad_norm": 48.534324645996094,
      "learning_rate": 2.5930824008138355e-05,
      "loss": 1.992,
      "step": 47320
    },
    {
      "epoch": 24.074262461851475,
      "grad_norm": 39.75569152832031,
      "learning_rate": 2.5925737538148525e-05,
      "loss": 1.9428,
      "step": 47330
    },
    {
      "epoch": 24.079348931841302,
      "grad_norm": 28.48988914489746,
      "learning_rate": 2.5920651068158698e-05,
      "loss": 1.978,
      "step": 47340
    },
    {
      "epoch": 24.08443540183113,
      "grad_norm": 33.068153381347656,
      "learning_rate": 2.5915564598168875e-05,
      "loss": 1.9679,
      "step": 47350
    },
    {
      "epoch": 24.089521871820956,
      "grad_norm": 38.47710037231445,
      "learning_rate": 2.5910478128179045e-05,
      "loss": 2.0285,
      "step": 47360
    },
    {
      "epoch": 24.094608341810783,
      "grad_norm": 38.707923889160156,
      "learning_rate": 2.5905391658189215e-05,
      "loss": 1.9457,
      "step": 47370
    },
    {
      "epoch": 24.09969481180061,
      "grad_norm": 37.35641098022461,
      "learning_rate": 2.590030518819939e-05,
      "loss": 1.8747,
      "step": 47380
    },
    {
      "epoch": 24.104781281790437,
      "grad_norm": 28.627656936645508,
      "learning_rate": 2.589521871820956e-05,
      "loss": 2.0334,
      "step": 47390
    },
    {
      "epoch": 24.109867751780264,
      "grad_norm": 32.27869415283203,
      "learning_rate": 2.5890132248219738e-05,
      "loss": 1.9679,
      "step": 47400
    },
    {
      "epoch": 24.11495422177009,
      "grad_norm": 35.590675354003906,
      "learning_rate": 2.588504577822991e-05,
      "loss": 1.9853,
      "step": 47410
    },
    {
      "epoch": 24.120040691759918,
      "grad_norm": 39.34186935424805,
      "learning_rate": 2.587995930824008e-05,
      "loss": 1.9156,
      "step": 47420
    },
    {
      "epoch": 24.125127161749745,
      "grad_norm": 42.02450180053711,
      "learning_rate": 2.5874872838250258e-05,
      "loss": 1.976,
      "step": 47430
    },
    {
      "epoch": 24.130213631739572,
      "grad_norm": 29.313583374023438,
      "learning_rate": 2.5869786368260428e-05,
      "loss": 2.0032,
      "step": 47440
    },
    {
      "epoch": 24.1353001017294,
      "grad_norm": 41.77195739746094,
      "learning_rate": 2.58646998982706e-05,
      "loss": 1.9162,
      "step": 47450
    },
    {
      "epoch": 24.140386571719226,
      "grad_norm": 30.68418312072754,
      "learning_rate": 2.5859613428280778e-05,
      "loss": 1.9773,
      "step": 47460
    },
    {
      "epoch": 24.145473041709053,
      "grad_norm": 42.14760208129883,
      "learning_rate": 2.5854526958290947e-05,
      "loss": 2.0101,
      "step": 47470
    },
    {
      "epoch": 24.15055951169888,
      "grad_norm": 36.13754653930664,
      "learning_rate": 2.5849440488301117e-05,
      "loss": 1.9482,
      "step": 47480
    },
    {
      "epoch": 24.155645981688707,
      "grad_norm": 31.301464080810547,
      "learning_rate": 2.5844354018311294e-05,
      "loss": 1.9571,
      "step": 47490
    },
    {
      "epoch": 24.160732451678534,
      "grad_norm": 43.498619079589844,
      "learning_rate": 2.5839267548321467e-05,
      "loss": 1.9948,
      "step": 47500
    },
    {
      "epoch": 24.16581892166836,
      "grad_norm": 32.657283782958984,
      "learning_rate": 2.5834181078331637e-05,
      "loss": 1.9742,
      "step": 47510
    },
    {
      "epoch": 24.170905391658188,
      "grad_norm": 33.405548095703125,
      "learning_rate": 2.5829094608341814e-05,
      "loss": 2.0717,
      "step": 47520
    },
    {
      "epoch": 24.175991861648015,
      "grad_norm": 32.814537048339844,
      "learning_rate": 2.5824008138351984e-05,
      "loss": 2.0191,
      "step": 47530
    },
    {
      "epoch": 24.181078331637842,
      "grad_norm": 30.53484535217285,
      "learning_rate": 2.5818921668362157e-05,
      "loss": 2.0155,
      "step": 47540
    },
    {
      "epoch": 24.18616480162767,
      "grad_norm": 34.53741455078125,
      "learning_rate": 2.5813835198372334e-05,
      "loss": 2.0267,
      "step": 47550
    },
    {
      "epoch": 24.191251271617496,
      "grad_norm": 32.99121856689453,
      "learning_rate": 2.5808748728382504e-05,
      "loss": 1.9988,
      "step": 47560
    },
    {
      "epoch": 24.196337741607323,
      "grad_norm": 32.450382232666016,
      "learning_rate": 2.5803662258392674e-05,
      "loss": 2.0153,
      "step": 47570
    },
    {
      "epoch": 24.20142421159715,
      "grad_norm": 35.87202453613281,
      "learning_rate": 2.579857578840285e-05,
      "loss": 1.9947,
      "step": 47580
    },
    {
      "epoch": 24.20651068158698,
      "grad_norm": 47.65556716918945,
      "learning_rate": 2.5793489318413023e-05,
      "loss": 1.9848,
      "step": 47590
    },
    {
      "epoch": 24.211597151576807,
      "grad_norm": 30.257455825805664,
      "learning_rate": 2.5788402848423193e-05,
      "loss": 1.8225,
      "step": 47600
    },
    {
      "epoch": 24.216683621566634,
      "grad_norm": 48.02427673339844,
      "learning_rate": 2.578331637843337e-05,
      "loss": 2.0006,
      "step": 47610
    },
    {
      "epoch": 24.22177009155646,
      "grad_norm": 28.924610137939453,
      "learning_rate": 2.577822990844354e-05,
      "loss": 2.0013,
      "step": 47620
    },
    {
      "epoch": 24.22685656154629,
      "grad_norm": 31.657785415649414,
      "learning_rate": 2.5773143438453713e-05,
      "loss": 2.0189,
      "step": 47630
    },
    {
      "epoch": 24.231943031536115,
      "grad_norm": 36.563262939453125,
      "learning_rate": 2.576805696846389e-05,
      "loss": 1.9753,
      "step": 47640
    },
    {
      "epoch": 24.237029501525942,
      "grad_norm": 37.68893814086914,
      "learning_rate": 2.576297049847406e-05,
      "loss": 1.8562,
      "step": 47650
    },
    {
      "epoch": 24.24211597151577,
      "grad_norm": 39.30939865112305,
      "learning_rate": 2.575788402848423e-05,
      "loss": 1.9507,
      "step": 47660
    },
    {
      "epoch": 24.247202441505596,
      "grad_norm": 27.352453231811523,
      "learning_rate": 2.5752797558494406e-05,
      "loss": 1.9217,
      "step": 47670
    },
    {
      "epoch": 24.252288911495423,
      "grad_norm": 38.361820220947266,
      "learning_rate": 2.574771108850458e-05,
      "loss": 1.9741,
      "step": 47680
    },
    {
      "epoch": 24.25737538148525,
      "grad_norm": 36.27880859375,
      "learning_rate": 2.5742624618514756e-05,
      "loss": 1.8685,
      "step": 47690
    },
    {
      "epoch": 24.262461851475077,
      "grad_norm": 47.147544860839844,
      "learning_rate": 2.5737538148524926e-05,
      "loss": 1.9559,
      "step": 47700
    },
    {
      "epoch": 24.267548321464904,
      "grad_norm": 47.84363555908203,
      "learning_rate": 2.5732451678535096e-05,
      "loss": 1.9926,
      "step": 47710
    },
    {
      "epoch": 24.27263479145473,
      "grad_norm": 38.28353500366211,
      "learning_rate": 2.5727365208545273e-05,
      "loss": 1.9769,
      "step": 47720
    },
    {
      "epoch": 24.277721261444558,
      "grad_norm": 30.83321762084961,
      "learning_rate": 2.5722278738555443e-05,
      "loss": 1.9959,
      "step": 47730
    },
    {
      "epoch": 24.282807731434385,
      "grad_norm": 41.73593521118164,
      "learning_rate": 2.5717192268565616e-05,
      "loss": 1.9864,
      "step": 47740
    },
    {
      "epoch": 24.287894201424212,
      "grad_norm": 30.004764556884766,
      "learning_rate": 2.5712105798575793e-05,
      "loss": 2.037,
      "step": 47750
    },
    {
      "epoch": 24.29298067141404,
      "grad_norm": 47.552284240722656,
      "learning_rate": 2.5707019328585962e-05,
      "loss": 1.9701,
      "step": 47760
    },
    {
      "epoch": 24.298067141403866,
      "grad_norm": 35.14980697631836,
      "learning_rate": 2.5701932858596132e-05,
      "loss": 2.0399,
      "step": 47770
    },
    {
      "epoch": 24.303153611393693,
      "grad_norm": 39.175140380859375,
      "learning_rate": 2.569684638860631e-05,
      "loss": 1.9226,
      "step": 47780
    },
    {
      "epoch": 24.30824008138352,
      "grad_norm": 41.91194152832031,
      "learning_rate": 2.5691759918616482e-05,
      "loss": 2.0167,
      "step": 47790
    },
    {
      "epoch": 24.313326551373347,
      "grad_norm": 34.96483612060547,
      "learning_rate": 2.5686673448626652e-05,
      "loss": 1.9475,
      "step": 47800
    },
    {
      "epoch": 24.318413021363174,
      "grad_norm": 27.93353843688965,
      "learning_rate": 2.568158697863683e-05,
      "loss": 1.9693,
      "step": 47810
    },
    {
      "epoch": 24.323499491353,
      "grad_norm": 33.16935348510742,
      "learning_rate": 2.5676500508647e-05,
      "loss": 1.9735,
      "step": 47820
    },
    {
      "epoch": 24.328585961342828,
      "grad_norm": 43.49806213378906,
      "learning_rate": 2.5671414038657172e-05,
      "loss": 2.0678,
      "step": 47830
    },
    {
      "epoch": 24.333672431332655,
      "grad_norm": 45.21903991699219,
      "learning_rate": 2.566632756866735e-05,
      "loss": 1.9805,
      "step": 47840
    },
    {
      "epoch": 24.338758901322482,
      "grad_norm": 38.095855712890625,
      "learning_rate": 2.566124109867752e-05,
      "loss": 1.9817,
      "step": 47850
    },
    {
      "epoch": 24.34384537131231,
      "grad_norm": 37.136985778808594,
      "learning_rate": 2.565615462868769e-05,
      "loss": 1.921,
      "step": 47860
    },
    {
      "epoch": 24.348931841302136,
      "grad_norm": 41.18095016479492,
      "learning_rate": 2.5651068158697865e-05,
      "loss": 1.9207,
      "step": 47870
    },
    {
      "epoch": 24.354018311291963,
      "grad_norm": 38.043540954589844,
      "learning_rate": 2.564598168870804e-05,
      "loss": 2.0752,
      "step": 47880
    },
    {
      "epoch": 24.35910478128179,
      "grad_norm": 41.01583480834961,
      "learning_rate": 2.564089521871821e-05,
      "loss": 1.9695,
      "step": 47890
    },
    {
      "epoch": 24.364191251271617,
      "grad_norm": 29.047603607177734,
      "learning_rate": 2.5635808748728385e-05,
      "loss": 1.9595,
      "step": 47900
    },
    {
      "epoch": 24.369277721261444,
      "grad_norm": 36.379730224609375,
      "learning_rate": 2.5630722278738555e-05,
      "loss": 1.9568,
      "step": 47910
    },
    {
      "epoch": 24.37436419125127,
      "grad_norm": 37.44243621826172,
      "learning_rate": 2.5625635808748728e-05,
      "loss": 1.9123,
      "step": 47920
    },
    {
      "epoch": 24.379450661241098,
      "grad_norm": 45.539371490478516,
      "learning_rate": 2.5620549338758905e-05,
      "loss": 2.0194,
      "step": 47930
    },
    {
      "epoch": 24.384537131230925,
      "grad_norm": 27.532255172729492,
      "learning_rate": 2.5615462868769075e-05,
      "loss": 1.9587,
      "step": 47940
    },
    {
      "epoch": 24.38962360122075,
      "grad_norm": 34.87377166748047,
      "learning_rate": 2.561037639877925e-05,
      "loss": 1.9461,
      "step": 47950
    },
    {
      "epoch": 24.39471007121058,
      "grad_norm": 42.89693069458008,
      "learning_rate": 2.560528992878942e-05,
      "loss": 1.8754,
      "step": 47960
    },
    {
      "epoch": 24.399796541200406,
      "grad_norm": 40.285892486572266,
      "learning_rate": 2.5600203458799595e-05,
      "loss": 1.9777,
      "step": 47970
    },
    {
      "epoch": 24.404883011190233,
      "grad_norm": 34.67412185668945,
      "learning_rate": 2.559511698880977e-05,
      "loss": 2.0019,
      "step": 47980
    },
    {
      "epoch": 24.40996948118006,
      "grad_norm": 35.767452239990234,
      "learning_rate": 2.559003051881994e-05,
      "loss": 1.9385,
      "step": 47990
    },
    {
      "epoch": 24.415055951169887,
      "grad_norm": 30.837514877319336,
      "learning_rate": 2.558494404883011e-05,
      "loss": 1.9023,
      "step": 48000
    },
    {
      "epoch": 24.420142421159714,
      "grad_norm": 29.541202545166016,
      "learning_rate": 2.5579857578840288e-05,
      "loss": 1.9861,
      "step": 48010
    },
    {
      "epoch": 24.42522889114954,
      "grad_norm": 33.44878387451172,
      "learning_rate": 2.557477110885046e-05,
      "loss": 1.9675,
      "step": 48020
    },
    {
      "epoch": 24.43031536113937,
      "grad_norm": 32.17876052856445,
      "learning_rate": 2.556968463886063e-05,
      "loss": 1.9055,
      "step": 48030
    },
    {
      "epoch": 24.435401831129198,
      "grad_norm": 35.68975067138672,
      "learning_rate": 2.5564598168870808e-05,
      "loss": 1.949,
      "step": 48040
    },
    {
      "epoch": 24.440488301119025,
      "grad_norm": 38.389434814453125,
      "learning_rate": 2.5559511698880977e-05,
      "loss": 2.0044,
      "step": 48050
    },
    {
      "epoch": 24.445574771108852,
      "grad_norm": 38.71593475341797,
      "learning_rate": 2.5554425228891147e-05,
      "loss": 2.1361,
      "step": 48060
    },
    {
      "epoch": 24.45066124109868,
      "grad_norm": 31.105756759643555,
      "learning_rate": 2.5549338758901324e-05,
      "loss": 2.0367,
      "step": 48070
    },
    {
      "epoch": 24.455747711088506,
      "grad_norm": 34.35033416748047,
      "learning_rate": 2.5544252288911497e-05,
      "loss": 1.9937,
      "step": 48080
    },
    {
      "epoch": 24.460834181078333,
      "grad_norm": 33.74874496459961,
      "learning_rate": 2.5539165818921667e-05,
      "loss": 1.9552,
      "step": 48090
    },
    {
      "epoch": 24.46592065106816,
      "grad_norm": 30.15139389038086,
      "learning_rate": 2.5534079348931844e-05,
      "loss": 1.906,
      "step": 48100
    },
    {
      "epoch": 24.471007121057987,
      "grad_norm": 30.37560272216797,
      "learning_rate": 2.5528992878942014e-05,
      "loss": 1.9495,
      "step": 48110
    },
    {
      "epoch": 24.476093591047814,
      "grad_norm": 32.10414123535156,
      "learning_rate": 2.5523906408952187e-05,
      "loss": 1.9596,
      "step": 48120
    },
    {
      "epoch": 24.48118006103764,
      "grad_norm": 39.353824615478516,
      "learning_rate": 2.5518819938962364e-05,
      "loss": 2.0094,
      "step": 48130
    },
    {
      "epoch": 24.486266531027468,
      "grad_norm": 42.53944396972656,
      "learning_rate": 2.5513733468972534e-05,
      "loss": 1.8902,
      "step": 48140
    },
    {
      "epoch": 24.491353001017295,
      "grad_norm": 51.17795181274414,
      "learning_rate": 2.5508646998982704e-05,
      "loss": 2.0018,
      "step": 48150
    },
    {
      "epoch": 24.496439471007122,
      "grad_norm": 43.35330581665039,
      "learning_rate": 2.550356052899288e-05,
      "loss": 1.9651,
      "step": 48160
    },
    {
      "epoch": 24.50152594099695,
      "grad_norm": 32.07320022583008,
      "learning_rate": 2.5498474059003053e-05,
      "loss": 1.9709,
      "step": 48170
    },
    {
      "epoch": 24.506612410986776,
      "grad_norm": 30.1441707611084,
      "learning_rate": 2.5493387589013223e-05,
      "loss": 1.9902,
      "step": 48180
    },
    {
      "epoch": 24.511698880976603,
      "grad_norm": 33.197452545166016,
      "learning_rate": 2.54883011190234e-05,
      "loss": 1.9936,
      "step": 48190
    },
    {
      "epoch": 24.51678535096643,
      "grad_norm": 34.01964569091797,
      "learning_rate": 2.548321464903357e-05,
      "loss": 1.948,
      "step": 48200
    },
    {
      "epoch": 24.521871820956257,
      "grad_norm": 37.53329086303711,
      "learning_rate": 2.5478128179043747e-05,
      "loss": 2.0174,
      "step": 48210
    },
    {
      "epoch": 24.526958290946084,
      "grad_norm": 29.729656219482422,
      "learning_rate": 2.547304170905392e-05,
      "loss": 1.9784,
      "step": 48220
    },
    {
      "epoch": 24.53204476093591,
      "grad_norm": 35.57394790649414,
      "learning_rate": 2.546795523906409e-05,
      "loss": 1.9832,
      "step": 48230
    },
    {
      "epoch": 24.537131230925738,
      "grad_norm": 29.75118637084961,
      "learning_rate": 2.5462868769074266e-05,
      "loss": 2.0095,
      "step": 48240
    },
    {
      "epoch": 24.542217700915565,
      "grad_norm": 32.31577682495117,
      "learning_rate": 2.5457782299084436e-05,
      "loss": 1.8935,
      "step": 48250
    },
    {
      "epoch": 24.54730417090539,
      "grad_norm": 42.21127700805664,
      "learning_rate": 2.545269582909461e-05,
      "loss": 2.0684,
      "step": 48260
    },
    {
      "epoch": 24.55239064089522,
      "grad_norm": 38.11647415161133,
      "learning_rate": 2.5447609359104786e-05,
      "loss": 1.9531,
      "step": 48270
    },
    {
      "epoch": 24.557477110885046,
      "grad_norm": 33.81925582885742,
      "learning_rate": 2.5442522889114956e-05,
      "loss": 2.0039,
      "step": 48280
    },
    {
      "epoch": 24.562563580874873,
      "grad_norm": 34.77541732788086,
      "learning_rate": 2.5437436419125126e-05,
      "loss": 2.064,
      "step": 48290
    },
    {
      "epoch": 24.5676500508647,
      "grad_norm": 41.811866760253906,
      "learning_rate": 2.5432349949135303e-05,
      "loss": 1.9688,
      "step": 48300
    },
    {
      "epoch": 24.572736520854527,
      "grad_norm": 40.324546813964844,
      "learning_rate": 2.5427263479145476e-05,
      "loss": 2.0342,
      "step": 48310
    },
    {
      "epoch": 24.577822990844354,
      "grad_norm": 40.899166107177734,
      "learning_rate": 2.5422177009155646e-05,
      "loss": 1.9769,
      "step": 48320
    },
    {
      "epoch": 24.58290946083418,
      "grad_norm": 41.49272918701172,
      "learning_rate": 2.5417090539165823e-05,
      "loss": 1.9742,
      "step": 48330
    },
    {
      "epoch": 24.587995930824007,
      "grad_norm": 37.91895294189453,
      "learning_rate": 2.5412004069175992e-05,
      "loss": 1.9502,
      "step": 48340
    },
    {
      "epoch": 24.593082400813834,
      "grad_norm": 33.0387077331543,
      "learning_rate": 2.5406917599186166e-05,
      "loss": 1.9213,
      "step": 48350
    },
    {
      "epoch": 24.59816887080366,
      "grad_norm": 39.88418960571289,
      "learning_rate": 2.540183112919634e-05,
      "loss": 1.9747,
      "step": 48360
    },
    {
      "epoch": 24.60325534079349,
      "grad_norm": 42.61697769165039,
      "learning_rate": 2.5396744659206512e-05,
      "loss": 1.9637,
      "step": 48370
    },
    {
      "epoch": 24.608341810783315,
      "grad_norm": 35.80010223388672,
      "learning_rate": 2.5391658189216682e-05,
      "loss": 1.9178,
      "step": 48380
    },
    {
      "epoch": 24.613428280773142,
      "grad_norm": 33.13066864013672,
      "learning_rate": 2.538657171922686e-05,
      "loss": 1.9656,
      "step": 48390
    },
    {
      "epoch": 24.61851475076297,
      "grad_norm": 28.279956817626953,
      "learning_rate": 2.538148524923703e-05,
      "loss": 2.0648,
      "step": 48400
    },
    {
      "epoch": 24.623601220752796,
      "grad_norm": 33.63958740234375,
      "learning_rate": 2.5376398779247202e-05,
      "loss": 2.0199,
      "step": 48410
    },
    {
      "epoch": 24.628687690742623,
      "grad_norm": 30.07362937927246,
      "learning_rate": 2.537131230925738e-05,
      "loss": 1.9671,
      "step": 48420
    },
    {
      "epoch": 24.63377416073245,
      "grad_norm": 38.60784149169922,
      "learning_rate": 2.536622583926755e-05,
      "loss": 1.9574,
      "step": 48430
    },
    {
      "epoch": 24.638860630722277,
      "grad_norm": 40.622432708740234,
      "learning_rate": 2.536113936927772e-05,
      "loss": 2.0237,
      "step": 48440
    },
    {
      "epoch": 24.643947100712104,
      "grad_norm": 41.612396240234375,
      "learning_rate": 2.5356052899287895e-05,
      "loss": 1.9539,
      "step": 48450
    },
    {
      "epoch": 24.64903357070193,
      "grad_norm": 32.43288040161133,
      "learning_rate": 2.535096642929807e-05,
      "loss": 1.9292,
      "step": 48460
    },
    {
      "epoch": 24.654120040691758,
      "grad_norm": 42.80030059814453,
      "learning_rate": 2.534587995930824e-05,
      "loss": 1.9817,
      "step": 48470
    },
    {
      "epoch": 24.659206510681585,
      "grad_norm": 37.02116012573242,
      "learning_rate": 2.5340793489318415e-05,
      "loss": 2.0195,
      "step": 48480
    },
    {
      "epoch": 24.664292980671416,
      "grad_norm": 39.433677673339844,
      "learning_rate": 2.5335707019328585e-05,
      "loss": 1.8922,
      "step": 48490
    },
    {
      "epoch": 24.669379450661243,
      "grad_norm": 28.7502498626709,
      "learning_rate": 2.533062054933876e-05,
      "loss": 1.9456,
      "step": 48500
    },
    {
      "epoch": 24.67446592065107,
      "grad_norm": 30.10186195373535,
      "learning_rate": 2.5325534079348935e-05,
      "loss": 1.9602,
      "step": 48510
    },
    {
      "epoch": 24.679552390640897,
      "grad_norm": 42.22883224487305,
      "learning_rate": 2.5320447609359105e-05,
      "loss": 1.9825,
      "step": 48520
    },
    {
      "epoch": 24.684638860630724,
      "grad_norm": 34.797080993652344,
      "learning_rate": 2.531536113936928e-05,
      "loss": 1.9851,
      "step": 48530
    },
    {
      "epoch": 24.68972533062055,
      "grad_norm": 29.372493743896484,
      "learning_rate": 2.531027466937945e-05,
      "loss": 1.9935,
      "step": 48540
    },
    {
      "epoch": 24.694811800610378,
      "grad_norm": 38.620182037353516,
      "learning_rate": 2.5305188199389625e-05,
      "loss": 2.024,
      "step": 48550
    },
    {
      "epoch": 24.699898270600205,
      "grad_norm": 34.38361740112305,
      "learning_rate": 2.53001017293998e-05,
      "loss": 2.0037,
      "step": 48560
    },
    {
      "epoch": 24.70498474059003,
      "grad_norm": 35.651241302490234,
      "learning_rate": 2.529501525940997e-05,
      "loss": 1.9377,
      "step": 48570
    },
    {
      "epoch": 24.71007121057986,
      "grad_norm": 41.20127487182617,
      "learning_rate": 2.528992878942014e-05,
      "loss": 2.0916,
      "step": 48580
    },
    {
      "epoch": 24.715157680569686,
      "grad_norm": 28.076934814453125,
      "learning_rate": 2.5284842319430318e-05,
      "loss": 1.9569,
      "step": 48590
    },
    {
      "epoch": 24.720244150559513,
      "grad_norm": 33.146583557128906,
      "learning_rate": 2.527975584944049e-05,
      "loss": 1.8993,
      "step": 48600
    },
    {
      "epoch": 24.72533062054934,
      "grad_norm": 36.312232971191406,
      "learning_rate": 2.527466937945066e-05,
      "loss": 1.955,
      "step": 48610
    },
    {
      "epoch": 24.730417090539166,
      "grad_norm": 38.63947677612305,
      "learning_rate": 2.5269582909460838e-05,
      "loss": 1.9798,
      "step": 48620
    },
    {
      "epoch": 24.735503560528993,
      "grad_norm": 35.22476577758789,
      "learning_rate": 2.5264496439471007e-05,
      "loss": 2.0582,
      "step": 48630
    },
    {
      "epoch": 24.74059003051882,
      "grad_norm": 46.5095329284668,
      "learning_rate": 2.525940996948118e-05,
      "loss": 2.0631,
      "step": 48640
    },
    {
      "epoch": 24.745676500508647,
      "grad_norm": 29.06787109375,
      "learning_rate": 2.5254323499491357e-05,
      "loss": 1.9138,
      "step": 48650
    },
    {
      "epoch": 24.750762970498474,
      "grad_norm": 29.862192153930664,
      "learning_rate": 2.5249237029501527e-05,
      "loss": 2.0269,
      "step": 48660
    },
    {
      "epoch": 24.7558494404883,
      "grad_norm": 37.22433853149414,
      "learning_rate": 2.5244150559511697e-05,
      "loss": 1.9386,
      "step": 48670
    },
    {
      "epoch": 24.76093591047813,
      "grad_norm": 37.97491455078125,
      "learning_rate": 2.5239064089521874e-05,
      "loss": 1.961,
      "step": 48680
    },
    {
      "epoch": 24.766022380467955,
      "grad_norm": 36.027740478515625,
      "learning_rate": 2.5233977619532044e-05,
      "loss": 1.9922,
      "step": 48690
    },
    {
      "epoch": 24.771108850457782,
      "grad_norm": 32.01231002807617,
      "learning_rate": 2.5228891149542217e-05,
      "loss": 1.9412,
      "step": 48700
    },
    {
      "epoch": 24.77619532044761,
      "grad_norm": 32.181427001953125,
      "learning_rate": 2.5223804679552394e-05,
      "loss": 2.0819,
      "step": 48710
    },
    {
      "epoch": 24.781281790437436,
      "grad_norm": 40.240882873535156,
      "learning_rate": 2.5218718209562564e-05,
      "loss": 1.9084,
      "step": 48720
    },
    {
      "epoch": 24.786368260427263,
      "grad_norm": 38.67054748535156,
      "learning_rate": 2.5213631739572734e-05,
      "loss": 1.9174,
      "step": 48730
    },
    {
      "epoch": 24.79145473041709,
      "grad_norm": 37.02390670776367,
      "learning_rate": 2.520854526958291e-05,
      "loss": 1.9141,
      "step": 48740
    },
    {
      "epoch": 24.796541200406917,
      "grad_norm": 40.20021057128906,
      "learning_rate": 2.5203458799593083e-05,
      "loss": 2.0446,
      "step": 48750
    },
    {
      "epoch": 24.801627670396744,
      "grad_norm": 30.729419708251953,
      "learning_rate": 2.519837232960326e-05,
      "loss": 1.9777,
      "step": 48760
    },
    {
      "epoch": 24.80671414038657,
      "grad_norm": 41.02753829956055,
      "learning_rate": 2.519328585961343e-05,
      "loss": 1.9634,
      "step": 48770
    },
    {
      "epoch": 24.811800610376398,
      "grad_norm": 40.70108413696289,
      "learning_rate": 2.51881993896236e-05,
      "loss": 1.9442,
      "step": 48780
    },
    {
      "epoch": 24.816887080366225,
      "grad_norm": 35.25626754760742,
      "learning_rate": 2.5183112919633777e-05,
      "loss": 2.043,
      "step": 48790
    },
    {
      "epoch": 24.821973550356052,
      "grad_norm": 31.999038696289062,
      "learning_rate": 2.517802644964395e-05,
      "loss": 1.9738,
      "step": 48800
    },
    {
      "epoch": 24.82706002034588,
      "grad_norm": 36.48850631713867,
      "learning_rate": 2.517293997965412e-05,
      "loss": 1.9723,
      "step": 48810
    },
    {
      "epoch": 24.832146490335706,
      "grad_norm": 31.629119873046875,
      "learning_rate": 2.5167853509664296e-05,
      "loss": 1.9075,
      "step": 48820
    },
    {
      "epoch": 24.837232960325533,
      "grad_norm": 34.646278381347656,
      "learning_rate": 2.5162767039674466e-05,
      "loss": 1.9238,
      "step": 48830
    },
    {
      "epoch": 24.84231943031536,
      "grad_norm": 33.98160934448242,
      "learning_rate": 2.515768056968464e-05,
      "loss": 1.874,
      "step": 48840
    },
    {
      "epoch": 24.847405900305187,
      "grad_norm": 36.362152099609375,
      "learning_rate": 2.5152594099694816e-05,
      "loss": 1.8382,
      "step": 48850
    },
    {
      "epoch": 24.852492370295014,
      "grad_norm": 33.63570785522461,
      "learning_rate": 2.5147507629704986e-05,
      "loss": 1.9823,
      "step": 48860
    },
    {
      "epoch": 24.85757884028484,
      "grad_norm": 32.080135345458984,
      "learning_rate": 2.5142421159715156e-05,
      "loss": 2.0177,
      "step": 48870
    },
    {
      "epoch": 24.862665310274668,
      "grad_norm": 37.6434326171875,
      "learning_rate": 2.5137334689725333e-05,
      "loss": 1.941,
      "step": 48880
    },
    {
      "epoch": 24.867751780264495,
      "grad_norm": 41.67818832397461,
      "learning_rate": 2.5132248219735506e-05,
      "loss": 1.9539,
      "step": 48890
    },
    {
      "epoch": 24.872838250254322,
      "grad_norm": 32.797245025634766,
      "learning_rate": 2.5127161749745676e-05,
      "loss": 1.9689,
      "step": 48900
    },
    {
      "epoch": 24.87792472024415,
      "grad_norm": 34.39550018310547,
      "learning_rate": 2.5122075279755853e-05,
      "loss": 1.9287,
      "step": 48910
    },
    {
      "epoch": 24.88301119023398,
      "grad_norm": 35.55121612548828,
      "learning_rate": 2.5116988809766022e-05,
      "loss": 1.9656,
      "step": 48920
    },
    {
      "epoch": 24.888097660223806,
      "grad_norm": 38.06582260131836,
      "learning_rate": 2.5111902339776196e-05,
      "loss": 1.9567,
      "step": 48930
    },
    {
      "epoch": 24.893184130213633,
      "grad_norm": 32.386207580566406,
      "learning_rate": 2.5106815869786372e-05,
      "loss": 2.0182,
      "step": 48940
    },
    {
      "epoch": 24.89827060020346,
      "grad_norm": 33.094940185546875,
      "learning_rate": 2.5101729399796542e-05,
      "loss": 1.945,
      "step": 48950
    },
    {
      "epoch": 24.903357070193287,
      "grad_norm": 36.458133697509766,
      "learning_rate": 2.5096642929806712e-05,
      "loss": 2.0095,
      "step": 48960
    },
    {
      "epoch": 24.908443540183114,
      "grad_norm": 37.57600021362305,
      "learning_rate": 2.509155645981689e-05,
      "loss": 1.8795,
      "step": 48970
    },
    {
      "epoch": 24.91353001017294,
      "grad_norm": 36.58060073852539,
      "learning_rate": 2.5086469989827062e-05,
      "loss": 1.8469,
      "step": 48980
    },
    {
      "epoch": 24.91861648016277,
      "grad_norm": 35.65409851074219,
      "learning_rate": 2.5081383519837232e-05,
      "loss": 1.9512,
      "step": 48990
    },
    {
      "epoch": 24.923702950152595,
      "grad_norm": 40.44188690185547,
      "learning_rate": 2.507629704984741e-05,
      "loss": 1.9141,
      "step": 49000
    },
    {
      "epoch": 24.928789420142422,
      "grad_norm": 32.88553237915039,
      "learning_rate": 2.507121057985758e-05,
      "loss": 1.9504,
      "step": 49010
    },
    {
      "epoch": 24.93387589013225,
      "grad_norm": 32.514495849609375,
      "learning_rate": 2.5066124109867755e-05,
      "loss": 1.98,
      "step": 49020
    },
    {
      "epoch": 24.938962360122076,
      "grad_norm": 38.25628662109375,
      "learning_rate": 2.5061037639877925e-05,
      "loss": 1.9267,
      "step": 49030
    },
    {
      "epoch": 24.944048830111903,
      "grad_norm": 40.20993423461914,
      "learning_rate": 2.50559511698881e-05,
      "loss": 1.9611,
      "step": 49040
    },
    {
      "epoch": 24.94913530010173,
      "grad_norm": 31.803787231445312,
      "learning_rate": 2.5050864699898275e-05,
      "loss": 1.9341,
      "step": 49050
    },
    {
      "epoch": 24.954221770091557,
      "grad_norm": 33.550453186035156,
      "learning_rate": 2.5045778229908445e-05,
      "loss": 1.9833,
      "step": 49060
    },
    {
      "epoch": 24.959308240081384,
      "grad_norm": 32.87727355957031,
      "learning_rate": 2.5040691759918615e-05,
      "loss": 2.0051,
      "step": 49070
    },
    {
      "epoch": 24.96439471007121,
      "grad_norm": 34.616939544677734,
      "learning_rate": 2.503560528992879e-05,
      "loss": 1.9523,
      "step": 49080
    },
    {
      "epoch": 24.969481180061038,
      "grad_norm": 36.540706634521484,
      "learning_rate": 2.5030518819938965e-05,
      "loss": 1.9703,
      "step": 49090
    },
    {
      "epoch": 24.974567650050865,
      "grad_norm": 41.30297088623047,
      "learning_rate": 2.5025432349949135e-05,
      "loss": 1.9713,
      "step": 49100
    },
    {
      "epoch": 24.979654120040692,
      "grad_norm": 42.073734283447266,
      "learning_rate": 2.502034587995931e-05,
      "loss": 1.9743,
      "step": 49110
    },
    {
      "epoch": 24.98474059003052,
      "grad_norm": 35.122413635253906,
      "learning_rate": 2.501525940996948e-05,
      "loss": 1.9552,
      "step": 49120
    },
    {
      "epoch": 24.989827060020346,
      "grad_norm": 33.99144744873047,
      "learning_rate": 2.5010172939979655e-05,
      "loss": 1.972,
      "step": 49130
    },
    {
      "epoch": 24.994913530010173,
      "grad_norm": 31.8408260345459,
      "learning_rate": 2.500508646998983e-05,
      "loss": 1.9675,
      "step": 49140
    },
    {
      "epoch": 25.0,
      "grad_norm": 44.16516876220703,
      "learning_rate": 2.5e-05,
      "loss": 1.9521,
      "step": 49150
    },
    {
      "epoch": 25.0,
      "eval_loss": 4.544183731079102,
      "eval_runtime": 2.7483,
      "eval_samples_per_second": 1009.7,
      "eval_steps_per_second": 126.258,
      "step": 49150
    },
    {
      "epoch": 25.005086469989827,
      "grad_norm": 25.59243392944336,
      "learning_rate": 2.4994913530010174e-05,
      "loss": 1.9115,
      "step": 49160
    },
    {
      "epoch": 25.010172939979654,
      "grad_norm": 45.42445373535156,
      "learning_rate": 2.4989827060020344e-05,
      "loss": 1.9103,
      "step": 49170
    },
    {
      "epoch": 25.01525940996948,
      "grad_norm": 33.70905685424805,
      "learning_rate": 2.498474059003052e-05,
      "loss": 1.9265,
      "step": 49180
    },
    {
      "epoch": 25.020345879959308,
      "grad_norm": 29.74462890625,
      "learning_rate": 2.4979654120040694e-05,
      "loss": 1.9227,
      "step": 49190
    },
    {
      "epoch": 25.025432349949135,
      "grad_norm": 41.805267333984375,
      "learning_rate": 2.4974567650050864e-05,
      "loss": 1.8316,
      "step": 49200
    },
    {
      "epoch": 25.030518819938962,
      "grad_norm": 37.46194076538086,
      "learning_rate": 2.4969481180061037e-05,
      "loss": 1.9187,
      "step": 49210
    },
    {
      "epoch": 25.03560528992879,
      "grad_norm": 33.49860382080078,
      "learning_rate": 2.496439471007121e-05,
      "loss": 1.9688,
      "step": 49220
    },
    {
      "epoch": 25.040691759918616,
      "grad_norm": 43.734352111816406,
      "learning_rate": 2.4959308240081387e-05,
      "loss": 1.9273,
      "step": 49230
    },
    {
      "epoch": 25.045778229908443,
      "grad_norm": 32.74810028076172,
      "learning_rate": 2.4954221770091557e-05,
      "loss": 1.9688,
      "step": 49240
    },
    {
      "epoch": 25.05086469989827,
      "grad_norm": 44.6898078918457,
      "learning_rate": 2.494913530010173e-05,
      "loss": 1.9665,
      "step": 49250
    },
    {
      "epoch": 25.055951169888097,
      "grad_norm": 48.01955032348633,
      "learning_rate": 2.4944048830111904e-05,
      "loss": 1.9362,
      "step": 49260
    },
    {
      "epoch": 25.061037639877924,
      "grad_norm": 33.933982849121094,
      "learning_rate": 2.4938962360122077e-05,
      "loss": 1.9488,
      "step": 49270
    },
    {
      "epoch": 25.06612410986775,
      "grad_norm": 33.17481231689453,
      "learning_rate": 2.493387589013225e-05,
      "loss": 1.9355,
      "step": 49280
    },
    {
      "epoch": 25.071210579857578,
      "grad_norm": 33.560333251953125,
      "learning_rate": 2.4928789420142424e-05,
      "loss": 1.9324,
      "step": 49290
    },
    {
      "epoch": 25.076297049847405,
      "grad_norm": 34.374534606933594,
      "learning_rate": 2.4923702950152594e-05,
      "loss": 1.8738,
      "step": 49300
    },
    {
      "epoch": 25.08138351983723,
      "grad_norm": 30.87063980102539,
      "learning_rate": 2.4918616480162767e-05,
      "loss": 2.0905,
      "step": 49310
    },
    {
      "epoch": 25.08646998982706,
      "grad_norm": 41.51753234863281,
      "learning_rate": 2.491353001017294e-05,
      "loss": 1.9908,
      "step": 49320
    },
    {
      "epoch": 25.091556459816886,
      "grad_norm": 30.584171295166016,
      "learning_rate": 2.4908443540183113e-05,
      "loss": 1.974,
      "step": 49330
    },
    {
      "epoch": 25.096642929806713,
      "grad_norm": 40.0252799987793,
      "learning_rate": 2.4903357070193287e-05,
      "loss": 2.0296,
      "step": 49340
    },
    {
      "epoch": 25.10172939979654,
      "grad_norm": 38.89438247680664,
      "learning_rate": 2.489827060020346e-05,
      "loss": 1.9278,
      "step": 49350
    },
    {
      "epoch": 25.106815869786367,
      "grad_norm": 42.1064338684082,
      "learning_rate": 2.4893184130213633e-05,
      "loss": 1.8704,
      "step": 49360
    },
    {
      "epoch": 25.111902339776194,
      "grad_norm": 38.50651168823242,
      "learning_rate": 2.4888097660223807e-05,
      "loss": 1.9653,
      "step": 49370
    },
    {
      "epoch": 25.116988809766024,
      "grad_norm": 44.533607482910156,
      "learning_rate": 2.488301119023398e-05,
      "loss": 1.93,
      "step": 49380
    },
    {
      "epoch": 25.12207527975585,
      "grad_norm": 30.793766021728516,
      "learning_rate": 2.4877924720244153e-05,
      "loss": 1.9208,
      "step": 49390
    },
    {
      "epoch": 25.127161749745678,
      "grad_norm": 39.28757095336914,
      "learning_rate": 2.4872838250254323e-05,
      "loss": 1.9935,
      "step": 49400
    },
    {
      "epoch": 25.132248219735505,
      "grad_norm": 36.74407196044922,
      "learning_rate": 2.4867751780264496e-05,
      "loss": 1.8941,
      "step": 49410
    },
    {
      "epoch": 25.137334689725332,
      "grad_norm": 39.343345642089844,
      "learning_rate": 2.4862665310274673e-05,
      "loss": 1.896,
      "step": 49420
    },
    {
      "epoch": 25.14242115971516,
      "grad_norm": 36.10470962524414,
      "learning_rate": 2.4857578840284843e-05,
      "loss": 1.9933,
      "step": 49430
    },
    {
      "epoch": 25.147507629704986,
      "grad_norm": 30.8111515045166,
      "learning_rate": 2.4852492370295016e-05,
      "loss": 1.9139,
      "step": 49440
    },
    {
      "epoch": 25.152594099694813,
      "grad_norm": 35.94441223144531,
      "learning_rate": 2.484740590030519e-05,
      "loss": 1.9415,
      "step": 49450
    },
    {
      "epoch": 25.15768056968464,
      "grad_norm": 41.309844970703125,
      "learning_rate": 2.4842319430315363e-05,
      "loss": 1.8298,
      "step": 49460
    },
    {
      "epoch": 25.162767039674467,
      "grad_norm": 30.836055755615234,
      "learning_rate": 2.4837232960325536e-05,
      "loss": 1.9259,
      "step": 49470
    },
    {
      "epoch": 25.167853509664294,
      "grad_norm": 33.262535095214844,
      "learning_rate": 2.483214649033571e-05,
      "loss": 1.9517,
      "step": 49480
    },
    {
      "epoch": 25.17293997965412,
      "grad_norm": 35.13552474975586,
      "learning_rate": 2.4827060020345883e-05,
      "loss": 1.8882,
      "step": 49490
    },
    {
      "epoch": 25.178026449643948,
      "grad_norm": 36.821990966796875,
      "learning_rate": 2.4821973550356053e-05,
      "loss": 2.0064,
      "step": 49500
    },
    {
      "epoch": 25.183112919633775,
      "grad_norm": 39.132720947265625,
      "learning_rate": 2.4816887080366226e-05,
      "loss": 1.8138,
      "step": 49510
    },
    {
      "epoch": 25.188199389623602,
      "grad_norm": 33.852569580078125,
      "learning_rate": 2.4811800610376402e-05,
      "loss": 2.0319,
      "step": 49520
    },
    {
      "epoch": 25.19328585961343,
      "grad_norm": 41.914512634277344,
      "learning_rate": 2.4806714140386572e-05,
      "loss": 1.9187,
      "step": 49530
    },
    {
      "epoch": 25.198372329603256,
      "grad_norm": 30.124094009399414,
      "learning_rate": 2.4801627670396746e-05,
      "loss": 1.8614,
      "step": 49540
    },
    {
      "epoch": 25.203458799593083,
      "grad_norm": 32.050941467285156,
      "learning_rate": 2.479654120040692e-05,
      "loss": 1.9116,
      "step": 49550
    },
    {
      "epoch": 25.20854526958291,
      "grad_norm": 39.37516403198242,
      "learning_rate": 2.4791454730417092e-05,
      "loss": 2.0183,
      "step": 49560
    },
    {
      "epoch": 25.213631739572737,
      "grad_norm": 32.01845169067383,
      "learning_rate": 2.4786368260427265e-05,
      "loss": 1.939,
      "step": 49570
    },
    {
      "epoch": 25.218718209562564,
      "grad_norm": 31.752771377563477,
      "learning_rate": 2.478128179043744e-05,
      "loss": 2.0328,
      "step": 49580
    },
    {
      "epoch": 25.22380467955239,
      "grad_norm": 37.79056167602539,
      "learning_rate": 2.477619532044761e-05,
      "loss": 1.9601,
      "step": 49590
    },
    {
      "epoch": 25.228891149542218,
      "grad_norm": 31.86996841430664,
      "learning_rate": 2.4771108850457782e-05,
      "loss": 1.8729,
      "step": 49600
    },
    {
      "epoch": 25.233977619532045,
      "grad_norm": 41.29650115966797,
      "learning_rate": 2.476602238046796e-05,
      "loss": 1.9516,
      "step": 49610
    },
    {
      "epoch": 25.23906408952187,
      "grad_norm": 29.309993743896484,
      "learning_rate": 2.476093591047813e-05,
      "loss": 2.0019,
      "step": 49620
    },
    {
      "epoch": 25.2441505595117,
      "grad_norm": 33.34682083129883,
      "learning_rate": 2.4755849440488302e-05,
      "loss": 1.9847,
      "step": 49630
    },
    {
      "epoch": 25.249237029501526,
      "grad_norm": 34.780174255371094,
      "learning_rate": 2.4750762970498475e-05,
      "loss": 1.9365,
      "step": 49640
    },
    {
      "epoch": 25.254323499491353,
      "grad_norm": 30.661182403564453,
      "learning_rate": 2.474567650050865e-05,
      "loss": 2.0029,
      "step": 49650
    },
    {
      "epoch": 25.25940996948118,
      "grad_norm": 37.985721588134766,
      "learning_rate": 2.474059003051882e-05,
      "loss": 1.9096,
      "step": 49660
    },
    {
      "epoch": 25.264496439471007,
      "grad_norm": 39.49757385253906,
      "learning_rate": 2.4735503560528995e-05,
      "loss": 1.9898,
      "step": 49670
    },
    {
      "epoch": 25.269582909460834,
      "grad_norm": 30.920719146728516,
      "learning_rate": 2.4730417090539168e-05,
      "loss": 1.8943,
      "step": 49680
    },
    {
      "epoch": 25.27466937945066,
      "grad_norm": 35.36216354370117,
      "learning_rate": 2.4725330620549338e-05,
      "loss": 1.8943,
      "step": 49690
    },
    {
      "epoch": 25.279755849440487,
      "grad_norm": 36.94884490966797,
      "learning_rate": 2.472024415055951e-05,
      "loss": 1.879,
      "step": 49700
    },
    {
      "epoch": 25.284842319430314,
      "grad_norm": 36.44988250732422,
      "learning_rate": 2.4715157680569688e-05,
      "loss": 1.9663,
      "step": 49710
    },
    {
      "epoch": 25.28992878942014,
      "grad_norm": 31.70140838623047,
      "learning_rate": 2.4710071210579858e-05,
      "loss": 1.8788,
      "step": 49720
    },
    {
      "epoch": 25.29501525940997,
      "grad_norm": 42.03323745727539,
      "learning_rate": 2.470498474059003e-05,
      "loss": 1.872,
      "step": 49730
    },
    {
      "epoch": 25.300101729399795,
      "grad_norm": 29.273052215576172,
      "learning_rate": 2.4699898270600204e-05,
      "loss": 1.959,
      "step": 49740
    },
    {
      "epoch": 25.305188199389622,
      "grad_norm": 43.98262405395508,
      "learning_rate": 2.4694811800610378e-05,
      "loss": 1.9832,
      "step": 49750
    },
    {
      "epoch": 25.31027466937945,
      "grad_norm": 34.4832763671875,
      "learning_rate": 2.468972533062055e-05,
      "loss": 1.916,
      "step": 49760
    },
    {
      "epoch": 25.315361139369276,
      "grad_norm": 39.536651611328125,
      "learning_rate": 2.4684638860630724e-05,
      "loss": 1.8841,
      "step": 49770
    },
    {
      "epoch": 25.320447609359103,
      "grad_norm": 38.70446014404297,
      "learning_rate": 2.4679552390640898e-05,
      "loss": 1.9292,
      "step": 49780
    },
    {
      "epoch": 25.32553407934893,
      "grad_norm": 34.905517578125,
      "learning_rate": 2.4674465920651068e-05,
      "loss": 1.9301,
      "step": 49790
    },
    {
      "epoch": 25.330620549338757,
      "grad_norm": 43.30043029785156,
      "learning_rate": 2.466937945066124e-05,
      "loss": 1.9336,
      "step": 49800
    },
    {
      "epoch": 25.335707019328584,
      "grad_norm": 35.278255462646484,
      "learning_rate": 2.4664292980671417e-05,
      "loss": 1.8898,
      "step": 49810
    },
    {
      "epoch": 25.340793489318415,
      "grad_norm": 40.191261291503906,
      "learning_rate": 2.4659206510681587e-05,
      "loss": 1.9856,
      "step": 49820
    },
    {
      "epoch": 25.345879959308242,
      "grad_norm": 34.28895568847656,
      "learning_rate": 2.465412004069176e-05,
      "loss": 1.9553,
      "step": 49830
    },
    {
      "epoch": 25.35096642929807,
      "grad_norm": 39.558197021484375,
      "learning_rate": 2.4649033570701934e-05,
      "loss": 1.9888,
      "step": 49840
    },
    {
      "epoch": 25.356052899287896,
      "grad_norm": 32.82614517211914,
      "learning_rate": 2.4643947100712107e-05,
      "loss": 1.9478,
      "step": 49850
    },
    {
      "epoch": 25.361139369277723,
      "grad_norm": 34.019927978515625,
      "learning_rate": 2.463886063072228e-05,
      "loss": 2.0692,
      "step": 49860
    },
    {
      "epoch": 25.36622583926755,
      "grad_norm": 38.21299743652344,
      "learning_rate": 2.4633774160732454e-05,
      "loss": 1.9802,
      "step": 49870
    },
    {
      "epoch": 25.371312309257377,
      "grad_norm": 39.895172119140625,
      "learning_rate": 2.4628687690742624e-05,
      "loss": 1.9643,
      "step": 49880
    },
    {
      "epoch": 25.376398779247204,
      "grad_norm": 30.991365432739258,
      "learning_rate": 2.4623601220752797e-05,
      "loss": 1.9394,
      "step": 49890
    },
    {
      "epoch": 25.38148524923703,
      "grad_norm": 33.902687072753906,
      "learning_rate": 2.4618514750762974e-05,
      "loss": 1.9993,
      "step": 49900
    },
    {
      "epoch": 25.386571719226858,
      "grad_norm": 34.59510803222656,
      "learning_rate": 2.4613428280773147e-05,
      "loss": 1.979,
      "step": 49910
    },
    {
      "epoch": 25.391658189216685,
      "grad_norm": 34.74424743652344,
      "learning_rate": 2.4608341810783317e-05,
      "loss": 1.9338,
      "step": 49920
    },
    {
      "epoch": 25.39674465920651,
      "grad_norm": 44.08173751831055,
      "learning_rate": 2.460325534079349e-05,
      "loss": 1.987,
      "step": 49930
    },
    {
      "epoch": 25.40183112919634,
      "grad_norm": 45.72809600830078,
      "learning_rate": 2.4598168870803663e-05,
      "loss": 1.9882,
      "step": 49940
    },
    {
      "epoch": 25.406917599186166,
      "grad_norm": 41.72338104248047,
      "learning_rate": 2.4593082400813837e-05,
      "loss": 1.9418,
      "step": 49950
    },
    {
      "epoch": 25.412004069175993,
      "grad_norm": 40.06916809082031,
      "learning_rate": 2.458799593082401e-05,
      "loss": 1.8872,
      "step": 49960
    },
    {
      "epoch": 25.41709053916582,
      "grad_norm": 35.032779693603516,
      "learning_rate": 2.4582909460834183e-05,
      "loss": 1.9603,
      "step": 49970
    },
    {
      "epoch": 25.422177009155646,
      "grad_norm": 61.520565032958984,
      "learning_rate": 2.4577822990844353e-05,
      "loss": 1.9055,
      "step": 49980
    },
    {
      "epoch": 25.427263479145473,
      "grad_norm": 29.13100814819336,
      "learning_rate": 2.4572736520854526e-05,
      "loss": 1.9969,
      "step": 49990
    },
    {
      "epoch": 25.4323499491353,
      "grad_norm": 42.72353744506836,
      "learning_rate": 2.4567650050864703e-05,
      "loss": 1.9054,
      "step": 50000
    },
    {
      "epoch": 25.437436419125127,
      "grad_norm": 41.11893844604492,
      "learning_rate": 2.4562563580874873e-05,
      "loss": 1.9101,
      "step": 50010
    },
    {
      "epoch": 25.442522889114954,
      "grad_norm": 35.6888313293457,
      "learning_rate": 2.4557477110885046e-05,
      "loss": 1.9739,
      "step": 50020
    },
    {
      "epoch": 25.44760935910478,
      "grad_norm": 31.962587356567383,
      "learning_rate": 2.455239064089522e-05,
      "loss": 1.978,
      "step": 50030
    },
    {
      "epoch": 25.45269582909461,
      "grad_norm": 36.98756790161133,
      "learning_rate": 2.4547304170905393e-05,
      "loss": 1.9699,
      "step": 50040
    },
    {
      "epoch": 25.457782299084435,
      "grad_norm": 39.61176681518555,
      "learning_rate": 2.4542217700915566e-05,
      "loss": 1.8159,
      "step": 50050
    },
    {
      "epoch": 25.462868769074262,
      "grad_norm": 34.62461471557617,
      "learning_rate": 2.453713123092574e-05,
      "loss": 1.9051,
      "step": 50060
    },
    {
      "epoch": 25.46795523906409,
      "grad_norm": 33.53685760498047,
      "learning_rate": 2.4532044760935913e-05,
      "loss": 1.9322,
      "step": 50070
    },
    {
      "epoch": 25.473041709053916,
      "grad_norm": 40.39431381225586,
      "learning_rate": 2.4526958290946083e-05,
      "loss": 1.9027,
      "step": 50080
    },
    {
      "epoch": 25.478128179043743,
      "grad_norm": 30.83631134033203,
      "learning_rate": 2.452187182095626e-05,
      "loss": 1.9741,
      "step": 50090
    },
    {
      "epoch": 25.48321464903357,
      "grad_norm": 33.98126220703125,
      "learning_rate": 2.4516785350966432e-05,
      "loss": 1.9759,
      "step": 50100
    },
    {
      "epoch": 25.488301119023397,
      "grad_norm": 37.235538482666016,
      "learning_rate": 2.4511698880976602e-05,
      "loss": 1.8896,
      "step": 50110
    },
    {
      "epoch": 25.493387589013224,
      "grad_norm": 37.95184326171875,
      "learning_rate": 2.4506612410986776e-05,
      "loss": 1.8517,
      "step": 50120
    },
    {
      "epoch": 25.49847405900305,
      "grad_norm": 44.55205535888672,
      "learning_rate": 2.450152594099695e-05,
      "loss": 1.9152,
      "step": 50130
    },
    {
      "epoch": 25.503560528992878,
      "grad_norm": 31.762243270874023,
      "learning_rate": 2.4496439471007122e-05,
      "loss": 1.9305,
      "step": 50140
    },
    {
      "epoch": 25.508646998982705,
      "grad_norm": 55.48313903808594,
      "learning_rate": 2.4491353001017295e-05,
      "loss": 1.8961,
      "step": 50150
    },
    {
      "epoch": 25.513733468972532,
      "grad_norm": 33.07308578491211,
      "learning_rate": 2.448626653102747e-05,
      "loss": 2.0082,
      "step": 50160
    },
    {
      "epoch": 25.51881993896236,
      "grad_norm": 38.79300308227539,
      "learning_rate": 2.4481180061037642e-05,
      "loss": 1.9138,
      "step": 50170
    },
    {
      "epoch": 25.523906408952186,
      "grad_norm": 30.553874969482422,
      "learning_rate": 2.4476093591047812e-05,
      "loss": 1.8194,
      "step": 50180
    },
    {
      "epoch": 25.528992878942013,
      "grad_norm": 35.27605438232422,
      "learning_rate": 2.447100712105799e-05,
      "loss": 1.9598,
      "step": 50190
    },
    {
      "epoch": 25.53407934893184,
      "grad_norm": 38.89934539794922,
      "learning_rate": 2.4465920651068162e-05,
      "loss": 1.9569,
      "step": 50200
    },
    {
      "epoch": 25.539165818921667,
      "grad_norm": 37.9526252746582,
      "learning_rate": 2.4460834181078332e-05,
      "loss": 1.9555,
      "step": 50210
    },
    {
      "epoch": 25.544252288911494,
      "grad_norm": 30.08629608154297,
      "learning_rate": 2.4455747711088505e-05,
      "loss": 1.9751,
      "step": 50220
    },
    {
      "epoch": 25.54933875890132,
      "grad_norm": 30.957555770874023,
      "learning_rate": 2.445066124109868e-05,
      "loss": 1.9549,
      "step": 50230
    },
    {
      "epoch": 25.554425228891148,
      "grad_norm": 31.203533172607422,
      "learning_rate": 2.444557477110885e-05,
      "loss": 1.8752,
      "step": 50240
    },
    {
      "epoch": 25.559511698880975,
      "grad_norm": 46.007667541503906,
      "learning_rate": 2.4440488301119025e-05,
      "loss": 1.9404,
      "step": 50250
    },
    {
      "epoch": 25.564598168870802,
      "grad_norm": 38.583656311035156,
      "learning_rate": 2.4435401831129198e-05,
      "loss": 1.8264,
      "step": 50260
    },
    {
      "epoch": 25.56968463886063,
      "grad_norm": 30.51549530029297,
      "learning_rate": 2.4430315361139368e-05,
      "loss": 1.9226,
      "step": 50270
    },
    {
      "epoch": 25.57477110885046,
      "grad_norm": 27.3862247467041,
      "learning_rate": 2.442522889114954e-05,
      "loss": 1.9274,
      "step": 50280
    },
    {
      "epoch": 25.579857578840286,
      "grad_norm": 39.0006217956543,
      "learning_rate": 2.4420142421159718e-05,
      "loss": 1.8945,
      "step": 50290
    },
    {
      "epoch": 25.584944048830113,
      "grad_norm": 33.958984375,
      "learning_rate": 2.441505595116989e-05,
      "loss": 1.8657,
      "step": 50300
    },
    {
      "epoch": 25.59003051881994,
      "grad_norm": 46.41764450073242,
      "learning_rate": 2.440996948118006e-05,
      "loss": 1.8413,
      "step": 50310
    },
    {
      "epoch": 25.595116988809767,
      "grad_norm": 42.40257263183594,
      "learning_rate": 2.4404883011190234e-05,
      "loss": 2.0031,
      "step": 50320
    },
    {
      "epoch": 25.600203458799594,
      "grad_norm": 35.32602310180664,
      "learning_rate": 2.4399796541200408e-05,
      "loss": 1.8915,
      "step": 50330
    },
    {
      "epoch": 25.60528992878942,
      "grad_norm": 35.55802536010742,
      "learning_rate": 2.439471007121058e-05,
      "loss": 1.8621,
      "step": 50340
    },
    {
      "epoch": 25.61037639877925,
      "grad_norm": 35.07682800292969,
      "learning_rate": 2.4389623601220754e-05,
      "loss": 1.8899,
      "step": 50350
    },
    {
      "epoch": 25.615462868769075,
      "grad_norm": 38.54634475708008,
      "learning_rate": 2.4384537131230928e-05,
      "loss": 1.9113,
      "step": 50360
    },
    {
      "epoch": 25.620549338758902,
      "grad_norm": 37.631935119628906,
      "learning_rate": 2.4379450661241098e-05,
      "loss": 1.9051,
      "step": 50370
    },
    {
      "epoch": 25.62563580874873,
      "grad_norm": 36.83076477050781,
      "learning_rate": 2.4374364191251274e-05,
      "loss": 1.8914,
      "step": 50380
    },
    {
      "epoch": 25.630722278738556,
      "grad_norm": 35.1046028137207,
      "learning_rate": 2.4369277721261447e-05,
      "loss": 1.9415,
      "step": 50390
    },
    {
      "epoch": 25.635808748728383,
      "grad_norm": 32.40800857543945,
      "learning_rate": 2.4364191251271617e-05,
      "loss": 1.9841,
      "step": 50400
    },
    {
      "epoch": 25.64089521871821,
      "grad_norm": 33.174747467041016,
      "learning_rate": 2.435910478128179e-05,
      "loss": 1.9148,
      "step": 50410
    },
    {
      "epoch": 25.645981688708037,
      "grad_norm": 33.794677734375,
      "learning_rate": 2.4354018311291964e-05,
      "loss": 1.9751,
      "step": 50420
    },
    {
      "epoch": 25.651068158697864,
      "grad_norm": 39.456214904785156,
      "learning_rate": 2.4348931841302137e-05,
      "loss": 1.9535,
      "step": 50430
    },
    {
      "epoch": 25.65615462868769,
      "grad_norm": 36.02193069458008,
      "learning_rate": 2.434384537131231e-05,
      "loss": 1.8916,
      "step": 50440
    },
    {
      "epoch": 25.661241098677518,
      "grad_norm": 33.290809631347656,
      "learning_rate": 2.4338758901322484e-05,
      "loss": 1.9245,
      "step": 50450
    },
    {
      "epoch": 25.666327568667345,
      "grad_norm": 29.09331703186035,
      "learning_rate": 2.4333672431332657e-05,
      "loss": 1.8821,
      "step": 50460
    },
    {
      "epoch": 25.671414038657172,
      "grad_norm": 34.44880676269531,
      "learning_rate": 2.4328585961342827e-05,
      "loss": 2.0054,
      "step": 50470
    },
    {
      "epoch": 25.676500508647,
      "grad_norm": 32.36385726928711,
      "learning_rate": 2.4323499491353004e-05,
      "loss": 1.8896,
      "step": 50480
    },
    {
      "epoch": 25.681586978636826,
      "grad_norm": 31.73781967163086,
      "learning_rate": 2.4318413021363177e-05,
      "loss": 1.9574,
      "step": 50490
    },
    {
      "epoch": 25.686673448626653,
      "grad_norm": 38.79484176635742,
      "learning_rate": 2.4313326551373347e-05,
      "loss": 1.9914,
      "step": 50500
    },
    {
      "epoch": 25.69175991861648,
      "grad_norm": 35.41246795654297,
      "learning_rate": 2.430824008138352e-05,
      "loss": 1.9752,
      "step": 50510
    },
    {
      "epoch": 25.696846388606307,
      "grad_norm": 35.41465377807617,
      "learning_rate": 2.4303153611393693e-05,
      "loss": 1.9319,
      "step": 50520
    },
    {
      "epoch": 25.701932858596134,
      "grad_norm": 33.62130355834961,
      "learning_rate": 2.4298067141403867e-05,
      "loss": 1.8929,
      "step": 50530
    },
    {
      "epoch": 25.70701932858596,
      "grad_norm": 32.62223434448242,
      "learning_rate": 2.429298067141404e-05,
      "loss": 2.027,
      "step": 50540
    },
    {
      "epoch": 25.712105798575788,
      "grad_norm": 40.35304641723633,
      "learning_rate": 2.4287894201424213e-05,
      "loss": 1.9085,
      "step": 50550
    },
    {
      "epoch": 25.717192268565615,
      "grad_norm": 35.430599212646484,
      "learning_rate": 2.4282807731434383e-05,
      "loss": 1.9307,
      "step": 50560
    },
    {
      "epoch": 25.722278738555442,
      "grad_norm": 42.087886810302734,
      "learning_rate": 2.427772126144456e-05,
      "loss": 1.855,
      "step": 50570
    },
    {
      "epoch": 25.72736520854527,
      "grad_norm": 35.77888488769531,
      "learning_rate": 2.4272634791454733e-05,
      "loss": 1.9274,
      "step": 50580
    },
    {
      "epoch": 25.732451678535096,
      "grad_norm": 35.8232536315918,
      "learning_rate": 2.4267548321464906e-05,
      "loss": 1.9469,
      "step": 50590
    },
    {
      "epoch": 25.737538148524923,
      "grad_norm": 36.2990608215332,
      "learning_rate": 2.4262461851475076e-05,
      "loss": 1.9311,
      "step": 50600
    },
    {
      "epoch": 25.74262461851475,
      "grad_norm": 33.32126998901367,
      "learning_rate": 2.425737538148525e-05,
      "loss": 1.9976,
      "step": 50610
    },
    {
      "epoch": 25.747711088504577,
      "grad_norm": 38.35270309448242,
      "learning_rate": 2.4252288911495423e-05,
      "loss": 1.9579,
      "step": 50620
    },
    {
      "epoch": 25.752797558494404,
      "grad_norm": 36.594966888427734,
      "learning_rate": 2.4247202441505596e-05,
      "loss": 1.9771,
      "step": 50630
    },
    {
      "epoch": 25.75788402848423,
      "grad_norm": 30.343435287475586,
      "learning_rate": 2.424211597151577e-05,
      "loss": 1.9301,
      "step": 50640
    },
    {
      "epoch": 25.762970498474058,
      "grad_norm": 41.28019714355469,
      "learning_rate": 2.4237029501525943e-05,
      "loss": 1.913,
      "step": 50650
    },
    {
      "epoch": 25.768056968463885,
      "grad_norm": 35.490074157714844,
      "learning_rate": 2.4231943031536113e-05,
      "loss": 1.8856,
      "step": 50660
    },
    {
      "epoch": 25.77314343845371,
      "grad_norm": 31.568891525268555,
      "learning_rate": 2.422685656154629e-05,
      "loss": 1.7978,
      "step": 50670
    },
    {
      "epoch": 25.77822990844354,
      "grad_norm": 30.816761016845703,
      "learning_rate": 2.4221770091556462e-05,
      "loss": 1.925,
      "step": 50680
    },
    {
      "epoch": 25.783316378433366,
      "grad_norm": 41.8367805480957,
      "learning_rate": 2.4216683621566632e-05,
      "loss": 1.8951,
      "step": 50690
    },
    {
      "epoch": 25.788402848423193,
      "grad_norm": 37.55289840698242,
      "learning_rate": 2.4211597151576806e-05,
      "loss": 1.9651,
      "step": 50700
    },
    {
      "epoch": 25.793489318413023,
      "grad_norm": 35.42148971557617,
      "learning_rate": 2.420651068158698e-05,
      "loss": 1.904,
      "step": 50710
    },
    {
      "epoch": 25.79857578840285,
      "grad_norm": 34.94953155517578,
      "learning_rate": 2.4201424211597156e-05,
      "loss": 1.8973,
      "step": 50720
    },
    {
      "epoch": 25.803662258392677,
      "grad_norm": 36.412208557128906,
      "learning_rate": 2.4196337741607325e-05,
      "loss": 1.9117,
      "step": 50730
    },
    {
      "epoch": 25.808748728382504,
      "grad_norm": 44.15196990966797,
      "learning_rate": 2.41912512716175e-05,
      "loss": 1.9485,
      "step": 50740
    },
    {
      "epoch": 25.81383519837233,
      "grad_norm": 28.27474594116211,
      "learning_rate": 2.4186164801627672e-05,
      "loss": 1.9353,
      "step": 50750
    },
    {
      "epoch": 25.818921668362158,
      "grad_norm": 40.21878433227539,
      "learning_rate": 2.4181078331637845e-05,
      "loss": 1.9044,
      "step": 50760
    },
    {
      "epoch": 25.824008138351985,
      "grad_norm": 34.04240417480469,
      "learning_rate": 2.417599186164802e-05,
      "loss": 1.899,
      "step": 50770
    },
    {
      "epoch": 25.829094608341812,
      "grad_norm": 37.57704162597656,
      "learning_rate": 2.4170905391658192e-05,
      "loss": 1.9183,
      "step": 50780
    },
    {
      "epoch": 25.83418107833164,
      "grad_norm": 38.79201889038086,
      "learning_rate": 2.4165818921668362e-05,
      "loss": 1.8684,
      "step": 50790
    },
    {
      "epoch": 25.839267548321466,
      "grad_norm": 37.863853454589844,
      "learning_rate": 2.4160732451678535e-05,
      "loss": 1.9128,
      "step": 50800
    },
    {
      "epoch": 25.844354018311293,
      "grad_norm": 36.71749496459961,
      "learning_rate": 2.415564598168871e-05,
      "loss": 1.8744,
      "step": 50810
    },
    {
      "epoch": 25.84944048830112,
      "grad_norm": 46.48759078979492,
      "learning_rate": 2.415055951169888e-05,
      "loss": 1.9792,
      "step": 50820
    },
    {
      "epoch": 25.854526958290947,
      "grad_norm": 30.58659553527832,
      "learning_rate": 2.4145473041709055e-05,
      "loss": 1.9456,
      "step": 50830
    },
    {
      "epoch": 25.859613428280774,
      "grad_norm": 36.116703033447266,
      "learning_rate": 2.4140386571719228e-05,
      "loss": 1.9534,
      "step": 50840
    },
    {
      "epoch": 25.8646998982706,
      "grad_norm": 38.82765197753906,
      "learning_rate": 2.41353001017294e-05,
      "loss": 1.8774,
      "step": 50850
    },
    {
      "epoch": 25.869786368260428,
      "grad_norm": 32.15995788574219,
      "learning_rate": 2.4130213631739575e-05,
      "loss": 1.9829,
      "step": 50860
    },
    {
      "epoch": 25.874872838250255,
      "grad_norm": 33.18433380126953,
      "learning_rate": 2.4125127161749748e-05,
      "loss": 1.8353,
      "step": 50870
    },
    {
      "epoch": 25.879959308240082,
      "grad_norm": 32.9324951171875,
      "learning_rate": 2.412004069175992e-05,
      "loss": 1.911,
      "step": 50880
    },
    {
      "epoch": 25.88504577822991,
      "grad_norm": 35.15220260620117,
      "learning_rate": 2.411495422177009e-05,
      "loss": 1.9549,
      "step": 50890
    },
    {
      "epoch": 25.890132248219736,
      "grad_norm": 32.22139358520508,
      "learning_rate": 2.4109867751780265e-05,
      "loss": 1.9629,
      "step": 50900
    },
    {
      "epoch": 25.895218718209563,
      "grad_norm": 30.953845977783203,
      "learning_rate": 2.410478128179044e-05,
      "loss": 1.9801,
      "step": 50910
    },
    {
      "epoch": 25.90030518819939,
      "grad_norm": 36.882755279541016,
      "learning_rate": 2.409969481180061e-05,
      "loss": 1.9618,
      "step": 50920
    },
    {
      "epoch": 25.905391658189217,
      "grad_norm": 44.09806442260742,
      "learning_rate": 2.4094608341810784e-05,
      "loss": 1.8438,
      "step": 50930
    },
    {
      "epoch": 25.910478128179044,
      "grad_norm": 37.00766372680664,
      "learning_rate": 2.4089521871820958e-05,
      "loss": 1.8942,
      "step": 50940
    },
    {
      "epoch": 25.91556459816887,
      "grad_norm": 34.16830062866211,
      "learning_rate": 2.4084435401831128e-05,
      "loss": 1.936,
      "step": 50950
    },
    {
      "epoch": 25.920651068158698,
      "grad_norm": 44.878875732421875,
      "learning_rate": 2.4079348931841304e-05,
      "loss": 1.9213,
      "step": 50960
    },
    {
      "epoch": 25.925737538148525,
      "grad_norm": 36.90739440917969,
      "learning_rate": 2.4074262461851477e-05,
      "loss": 1.9703,
      "step": 50970
    },
    {
      "epoch": 25.93082400813835,
      "grad_norm": 36.248680114746094,
      "learning_rate": 2.406917599186165e-05,
      "loss": 1.8237,
      "step": 50980
    },
    {
      "epoch": 25.93591047812818,
      "grad_norm": 32.99855422973633,
      "learning_rate": 2.406408952187182e-05,
      "loss": 1.8926,
      "step": 50990
    },
    {
      "epoch": 25.940996948118006,
      "grad_norm": 31.741771697998047,
      "learning_rate": 2.4059003051881994e-05,
      "loss": 1.9866,
      "step": 51000
    },
    {
      "epoch": 25.946083418107833,
      "grad_norm": 30.545251846313477,
      "learning_rate": 2.405391658189217e-05,
      "loss": 2.0258,
      "step": 51010
    },
    {
      "epoch": 25.95116988809766,
      "grad_norm": 33.69670104980469,
      "learning_rate": 2.404883011190234e-05,
      "loss": 1.9194,
      "step": 51020
    },
    {
      "epoch": 25.956256358087487,
      "grad_norm": 45.4490966796875,
      "learning_rate": 2.4043743641912514e-05,
      "loss": 1.936,
      "step": 51030
    },
    {
      "epoch": 25.961342828077314,
      "grad_norm": 44.375186920166016,
      "learning_rate": 2.4038657171922687e-05,
      "loss": 1.8677,
      "step": 51040
    },
    {
      "epoch": 25.96642929806714,
      "grad_norm": 36.23652267456055,
      "learning_rate": 2.403357070193286e-05,
      "loss": 1.9886,
      "step": 51050
    },
    {
      "epoch": 25.971515768056967,
      "grad_norm": 36.90753936767578,
      "learning_rate": 2.4028484231943034e-05,
      "loss": 1.9106,
      "step": 51060
    },
    {
      "epoch": 25.976602238046794,
      "grad_norm": 32.5518913269043,
      "learning_rate": 2.4023397761953207e-05,
      "loss": 1.9869,
      "step": 51070
    },
    {
      "epoch": 25.98168870803662,
      "grad_norm": 37.938941955566406,
      "learning_rate": 2.4018311291963377e-05,
      "loss": 1.8081,
      "step": 51080
    },
    {
      "epoch": 25.98677517802645,
      "grad_norm": 40.83217239379883,
      "learning_rate": 2.401322482197355e-05,
      "loss": 1.9583,
      "step": 51090
    },
    {
      "epoch": 25.991861648016275,
      "grad_norm": 46.37499237060547,
      "learning_rate": 2.4008138351983723e-05,
      "loss": 1.9076,
      "step": 51100
    },
    {
      "epoch": 25.996948118006102,
      "grad_norm": 33.582149505615234,
      "learning_rate": 2.40030518819939e-05,
      "loss": 1.8995,
      "step": 51110
    },
    {
      "epoch": 26.0,
      "eval_loss": 4.588091850280762,
      "eval_runtime": 2.7572,
      "eval_samples_per_second": 1006.458,
      "eval_steps_per_second": 125.853,
      "step": 51116
    },
    {
      "epoch": 26.00203458799593,
      "grad_norm": 35.924400329589844,
      "learning_rate": 2.399796541200407e-05,
      "loss": 1.8328,
      "step": 51120
    },
    {
      "epoch": 26.007121057985756,
      "grad_norm": 38.94694519042969,
      "learning_rate": 2.3992878942014243e-05,
      "loss": 1.8603,
      "step": 51130
    },
    {
      "epoch": 26.012207527975583,
      "grad_norm": 34.649803161621094,
      "learning_rate": 2.3987792472024416e-05,
      "loss": 1.9205,
      "step": 51140
    },
    {
      "epoch": 26.01729399796541,
      "grad_norm": 35.46893310546875,
      "learning_rate": 2.398270600203459e-05,
      "loss": 1.9209,
      "step": 51150
    },
    {
      "epoch": 26.022380467955237,
      "grad_norm": 39.28309631347656,
      "learning_rate": 2.3977619532044763e-05,
      "loss": 1.893,
      "step": 51160
    },
    {
      "epoch": 26.027466937945068,
      "grad_norm": 29.81507110595703,
      "learning_rate": 2.3972533062054936e-05,
      "loss": 1.8618,
      "step": 51170
    },
    {
      "epoch": 26.032553407934895,
      "grad_norm": 34.672454833984375,
      "learning_rate": 2.3967446592065106e-05,
      "loss": 1.8852,
      "step": 51180
    },
    {
      "epoch": 26.037639877924722,
      "grad_norm": 37.76008605957031,
      "learning_rate": 2.396236012207528e-05,
      "loss": 2.0167,
      "step": 51190
    },
    {
      "epoch": 26.04272634791455,
      "grad_norm": 33.867828369140625,
      "learning_rate": 2.3957273652085456e-05,
      "loss": 1.9462,
      "step": 51200
    },
    {
      "epoch": 26.047812817904376,
      "grad_norm": 29.235565185546875,
      "learning_rate": 2.3952187182095626e-05,
      "loss": 1.9175,
      "step": 51210
    },
    {
      "epoch": 26.052899287894203,
      "grad_norm": 37.99305725097656,
      "learning_rate": 2.39471007121058e-05,
      "loss": 1.8839,
      "step": 51220
    },
    {
      "epoch": 26.05798575788403,
      "grad_norm": 38.602943420410156,
      "learning_rate": 2.3942014242115973e-05,
      "loss": 1.8478,
      "step": 51230
    },
    {
      "epoch": 26.063072227873857,
      "grad_norm": 29.184368133544922,
      "learning_rate": 2.3936927772126146e-05,
      "loss": 1.8702,
      "step": 51240
    },
    {
      "epoch": 26.068158697863684,
      "grad_norm": 45.68467330932617,
      "learning_rate": 2.393184130213632e-05,
      "loss": 1.8824,
      "step": 51250
    },
    {
      "epoch": 26.07324516785351,
      "grad_norm": 34.59672164916992,
      "learning_rate": 2.3926754832146492e-05,
      "loss": 1.9316,
      "step": 51260
    },
    {
      "epoch": 26.078331637843338,
      "grad_norm": 48.608280181884766,
      "learning_rate": 2.3921668362156666e-05,
      "loss": 2.0112,
      "step": 51270
    },
    {
      "epoch": 26.083418107833165,
      "grad_norm": 43.3566780090332,
      "learning_rate": 2.3916581892166836e-05,
      "loss": 1.981,
      "step": 51280
    },
    {
      "epoch": 26.08850457782299,
      "grad_norm": 33.823543548583984,
      "learning_rate": 2.391149542217701e-05,
      "loss": 1.9141,
      "step": 51290
    },
    {
      "epoch": 26.09359104781282,
      "grad_norm": 41.26799011230469,
      "learning_rate": 2.3906408952187186e-05,
      "loss": 1.9495,
      "step": 51300
    },
    {
      "epoch": 26.098677517802646,
      "grad_norm": 43.29554748535156,
      "learning_rate": 2.3901322482197355e-05,
      "loss": 1.9173,
      "step": 51310
    },
    {
      "epoch": 26.103763987792473,
      "grad_norm": 47.067203521728516,
      "learning_rate": 2.389623601220753e-05,
      "loss": 1.8554,
      "step": 51320
    },
    {
      "epoch": 26.1088504577823,
      "grad_norm": 40.879608154296875,
      "learning_rate": 2.3891149542217702e-05,
      "loss": 1.9387,
      "step": 51330
    },
    {
      "epoch": 26.113936927772126,
      "grad_norm": 27.043710708618164,
      "learning_rate": 2.3886063072227875e-05,
      "loss": 1.9907,
      "step": 51340
    },
    {
      "epoch": 26.119023397761953,
      "grad_norm": 32.01223373413086,
      "learning_rate": 2.388097660223805e-05,
      "loss": 1.9217,
      "step": 51350
    },
    {
      "epoch": 26.12410986775178,
      "grad_norm": 38.68889617919922,
      "learning_rate": 2.3875890132248222e-05,
      "loss": 1.8597,
      "step": 51360
    },
    {
      "epoch": 26.129196337741607,
      "grad_norm": 32.11515426635742,
      "learning_rate": 2.3870803662258392e-05,
      "loss": 1.8684,
      "step": 51370
    },
    {
      "epoch": 26.134282807731434,
      "grad_norm": 36.22804641723633,
      "learning_rate": 2.3865717192268565e-05,
      "loss": 1.955,
      "step": 51380
    },
    {
      "epoch": 26.13936927772126,
      "grad_norm": 37.630252838134766,
      "learning_rate": 2.3860630722278742e-05,
      "loss": 1.8836,
      "step": 51390
    },
    {
      "epoch": 26.14445574771109,
      "grad_norm": 44.67724609375,
      "learning_rate": 2.3855544252288915e-05,
      "loss": 1.8494,
      "step": 51400
    },
    {
      "epoch": 26.149542217700915,
      "grad_norm": 32.2613410949707,
      "learning_rate": 2.3850457782299085e-05,
      "loss": 1.8637,
      "step": 51410
    },
    {
      "epoch": 26.154628687690742,
      "grad_norm": 34.86315155029297,
      "learning_rate": 2.3845371312309258e-05,
      "loss": 1.9561,
      "step": 51420
    },
    {
      "epoch": 26.15971515768057,
      "grad_norm": 39.426605224609375,
      "learning_rate": 2.384028484231943e-05,
      "loss": 1.8669,
      "step": 51430
    },
    {
      "epoch": 26.164801627670396,
      "grad_norm": 38.68331527709961,
      "learning_rate": 2.3835198372329605e-05,
      "loss": 1.8563,
      "step": 51440
    },
    {
      "epoch": 26.169888097660223,
      "grad_norm": 32.52793502807617,
      "learning_rate": 2.3830111902339778e-05,
      "loss": 1.8874,
      "step": 51450
    },
    {
      "epoch": 26.17497456765005,
      "grad_norm": 37.318214416503906,
      "learning_rate": 2.382502543234995e-05,
      "loss": 1.8545,
      "step": 51460
    },
    {
      "epoch": 26.180061037639877,
      "grad_norm": 40.45199966430664,
      "learning_rate": 2.381993896236012e-05,
      "loss": 1.8992,
      "step": 51470
    },
    {
      "epoch": 26.185147507629704,
      "grad_norm": 32.8386116027832,
      "learning_rate": 2.3814852492370295e-05,
      "loss": 1.7782,
      "step": 51480
    },
    {
      "epoch": 26.19023397761953,
      "grad_norm": 34.86037826538086,
      "learning_rate": 2.380976602238047e-05,
      "loss": 1.866,
      "step": 51490
    },
    {
      "epoch": 26.195320447609358,
      "grad_norm": 32.56147384643555,
      "learning_rate": 2.380467955239064e-05,
      "loss": 1.9353,
      "step": 51500
    },
    {
      "epoch": 26.200406917599185,
      "grad_norm": 32.70528030395508,
      "learning_rate": 2.3799593082400814e-05,
      "loss": 1.7955,
      "step": 51510
    },
    {
      "epoch": 26.205493387589012,
      "grad_norm": 30.53534698486328,
      "learning_rate": 2.3794506612410988e-05,
      "loss": 1.8541,
      "step": 51520
    },
    {
      "epoch": 26.21057985757884,
      "grad_norm": 33.96843338012695,
      "learning_rate": 2.378942014242116e-05,
      "loss": 1.9556,
      "step": 51530
    },
    {
      "epoch": 26.215666327568666,
      "grad_norm": 44.150299072265625,
      "learning_rate": 2.3784333672431334e-05,
      "loss": 2.002,
      "step": 51540
    },
    {
      "epoch": 26.220752797558493,
      "grad_norm": 39.408607482910156,
      "learning_rate": 2.3779247202441507e-05,
      "loss": 1.7752,
      "step": 51550
    },
    {
      "epoch": 26.22583926754832,
      "grad_norm": 37.28819274902344,
      "learning_rate": 2.377416073245168e-05,
      "loss": 1.9209,
      "step": 51560
    },
    {
      "epoch": 26.230925737538147,
      "grad_norm": 30.578697204589844,
      "learning_rate": 2.376907426246185e-05,
      "loss": 1.8665,
      "step": 51570
    },
    {
      "epoch": 26.236012207527974,
      "grad_norm": 43.535308837890625,
      "learning_rate": 2.3763987792472024e-05,
      "loss": 1.8712,
      "step": 51580
    },
    {
      "epoch": 26.2410986775178,
      "grad_norm": 42.75648880004883,
      "learning_rate": 2.37589013224822e-05,
      "loss": 1.94,
      "step": 51590
    },
    {
      "epoch": 26.246185147507628,
      "grad_norm": 39.02756118774414,
      "learning_rate": 2.375381485249237e-05,
      "loss": 1.8514,
      "step": 51600
    },
    {
      "epoch": 26.25127161749746,
      "grad_norm": 33.48963165283203,
      "learning_rate": 2.3748728382502544e-05,
      "loss": 1.8371,
      "step": 51610
    },
    {
      "epoch": 26.256358087487286,
      "grad_norm": 37.235469818115234,
      "learning_rate": 2.3743641912512717e-05,
      "loss": 1.9014,
      "step": 51620
    },
    {
      "epoch": 26.261444557477112,
      "grad_norm": 39.927860260009766,
      "learning_rate": 2.373855544252289e-05,
      "loss": 1.9013,
      "step": 51630
    },
    {
      "epoch": 26.26653102746694,
      "grad_norm": 27.538978576660156,
      "learning_rate": 2.3733468972533064e-05,
      "loss": 1.9249,
      "step": 51640
    },
    {
      "epoch": 26.271617497456766,
      "grad_norm": 39.095680236816406,
      "learning_rate": 2.3728382502543237e-05,
      "loss": 1.9421,
      "step": 51650
    },
    {
      "epoch": 26.276703967446593,
      "grad_norm": 33.23809051513672,
      "learning_rate": 2.372329603255341e-05,
      "loss": 1.8271,
      "step": 51660
    },
    {
      "epoch": 26.28179043743642,
      "grad_norm": 41.47551727294922,
      "learning_rate": 2.371820956256358e-05,
      "loss": 1.8185,
      "step": 51670
    },
    {
      "epoch": 26.286876907426247,
      "grad_norm": 41.21820068359375,
      "learning_rate": 2.3713123092573757e-05,
      "loss": 1.7513,
      "step": 51680
    },
    {
      "epoch": 26.291963377416074,
      "grad_norm": 37.07841110229492,
      "learning_rate": 2.370803662258393e-05,
      "loss": 1.8666,
      "step": 51690
    },
    {
      "epoch": 26.2970498474059,
      "grad_norm": 37.98819351196289,
      "learning_rate": 2.37029501525941e-05,
      "loss": 1.9067,
      "step": 51700
    },
    {
      "epoch": 26.30213631739573,
      "grad_norm": 34.59904098510742,
      "learning_rate": 2.3697863682604273e-05,
      "loss": 1.8713,
      "step": 51710
    },
    {
      "epoch": 26.307222787385555,
      "grad_norm": 32.83185577392578,
      "learning_rate": 2.3692777212614446e-05,
      "loss": 1.8956,
      "step": 51720
    },
    {
      "epoch": 26.312309257375382,
      "grad_norm": 33.446533203125,
      "learning_rate": 2.368769074262462e-05,
      "loss": 1.9159,
      "step": 51730
    },
    {
      "epoch": 26.31739572736521,
      "grad_norm": 39.701839447021484,
      "learning_rate": 2.3682604272634793e-05,
      "loss": 1.9096,
      "step": 51740
    },
    {
      "epoch": 26.322482197355036,
      "grad_norm": 34.45585250854492,
      "learning_rate": 2.3677517802644966e-05,
      "loss": 1.9066,
      "step": 51750
    },
    {
      "epoch": 26.327568667344863,
      "grad_norm": 36.06045150756836,
      "learning_rate": 2.3672431332655136e-05,
      "loss": 1.895,
      "step": 51760
    },
    {
      "epoch": 26.33265513733469,
      "grad_norm": 39.32196807861328,
      "learning_rate": 2.366734486266531e-05,
      "loss": 1.8789,
      "step": 51770
    },
    {
      "epoch": 26.337741607324517,
      "grad_norm": 36.90146255493164,
      "learning_rate": 2.3662258392675486e-05,
      "loss": 1.868,
      "step": 51780
    },
    {
      "epoch": 26.342828077314344,
      "grad_norm": 36.006248474121094,
      "learning_rate": 2.365717192268566e-05,
      "loss": 1.9162,
      "step": 51790
    },
    {
      "epoch": 26.34791454730417,
      "grad_norm": 43.41082000732422,
      "learning_rate": 2.365208545269583e-05,
      "loss": 1.8738,
      "step": 51800
    },
    {
      "epoch": 26.353001017293998,
      "grad_norm": 31.301286697387695,
      "learning_rate": 2.3646998982706003e-05,
      "loss": 1.925,
      "step": 51810
    },
    {
      "epoch": 26.358087487283825,
      "grad_norm": 29.932668685913086,
      "learning_rate": 2.3641912512716176e-05,
      "loss": 1.814,
      "step": 51820
    },
    {
      "epoch": 26.363173957273652,
      "grad_norm": 41.13313674926758,
      "learning_rate": 2.363682604272635e-05,
      "loss": 1.8087,
      "step": 51830
    },
    {
      "epoch": 26.36826042726348,
      "grad_norm": 52.89539337158203,
      "learning_rate": 2.3631739572736522e-05,
      "loss": 1.892,
      "step": 51840
    },
    {
      "epoch": 26.373346897253306,
      "grad_norm": 41.87567901611328,
      "learning_rate": 2.3626653102746696e-05,
      "loss": 1.9493,
      "step": 51850
    },
    {
      "epoch": 26.378433367243133,
      "grad_norm": 34.51865768432617,
      "learning_rate": 2.3621566632756866e-05,
      "loss": 1.9097,
      "step": 51860
    },
    {
      "epoch": 26.38351983723296,
      "grad_norm": 38.80131912231445,
      "learning_rate": 2.3616480162767042e-05,
      "loss": 1.9152,
      "step": 51870
    },
    {
      "epoch": 26.388606307222787,
      "grad_norm": 34.48135757446289,
      "learning_rate": 2.3611393692777216e-05,
      "loss": 1.848,
      "step": 51880
    },
    {
      "epoch": 26.393692777212614,
      "grad_norm": 36.89737319946289,
      "learning_rate": 2.3606307222787386e-05,
      "loss": 1.8847,
      "step": 51890
    },
    {
      "epoch": 26.39877924720244,
      "grad_norm": 42.124488830566406,
      "learning_rate": 2.360122075279756e-05,
      "loss": 1.8545,
      "step": 51900
    },
    {
      "epoch": 26.403865717192268,
      "grad_norm": 42.09123611450195,
      "learning_rate": 2.3596134282807732e-05,
      "loss": 1.8364,
      "step": 51910
    },
    {
      "epoch": 26.408952187182095,
      "grad_norm": 36.56629180908203,
      "learning_rate": 2.3591047812817905e-05,
      "loss": 1.9107,
      "step": 51920
    },
    {
      "epoch": 26.414038657171922,
      "grad_norm": 34.99872589111328,
      "learning_rate": 2.358596134282808e-05,
      "loss": 1.8975,
      "step": 51930
    },
    {
      "epoch": 26.41912512716175,
      "grad_norm": 36.90654754638672,
      "learning_rate": 2.3580874872838252e-05,
      "loss": 1.9119,
      "step": 51940
    },
    {
      "epoch": 26.424211597151576,
      "grad_norm": 44.84592056274414,
      "learning_rate": 2.3575788402848425e-05,
      "loss": 1.9508,
      "step": 51950
    },
    {
      "epoch": 26.429298067141403,
      "grad_norm": 43.381568908691406,
      "learning_rate": 2.3570701932858595e-05,
      "loss": 1.8605,
      "step": 51960
    },
    {
      "epoch": 26.43438453713123,
      "grad_norm": 37.501956939697266,
      "learning_rate": 2.3565615462868772e-05,
      "loss": 1.9255,
      "step": 51970
    },
    {
      "epoch": 26.439471007121057,
      "grad_norm": 37.388641357421875,
      "learning_rate": 2.3560528992878945e-05,
      "loss": 1.9036,
      "step": 51980
    },
    {
      "epoch": 26.444557477110884,
      "grad_norm": 37.025394439697266,
      "learning_rate": 2.3555442522889115e-05,
      "loss": 1.9146,
      "step": 51990
    },
    {
      "epoch": 26.44964394710071,
      "grad_norm": 38.40141296386719,
      "learning_rate": 2.3550356052899288e-05,
      "loss": 1.9398,
      "step": 52000
    },
    {
      "epoch": 26.454730417090538,
      "grad_norm": 32.79649353027344,
      "learning_rate": 2.354526958290946e-05,
      "loss": 1.8457,
      "step": 52010
    },
    {
      "epoch": 26.459816887080365,
      "grad_norm": 32.30529022216797,
      "learning_rate": 2.3540183112919635e-05,
      "loss": 1.8817,
      "step": 52020
    },
    {
      "epoch": 26.46490335707019,
      "grad_norm": 34.707733154296875,
      "learning_rate": 2.3535096642929808e-05,
      "loss": 1.8979,
      "step": 52030
    },
    {
      "epoch": 26.46998982706002,
      "grad_norm": 37.85392379760742,
      "learning_rate": 2.353001017293998e-05,
      "loss": 1.9731,
      "step": 52040
    },
    {
      "epoch": 26.475076297049846,
      "grad_norm": 40.85184860229492,
      "learning_rate": 2.352492370295015e-05,
      "loss": 1.9884,
      "step": 52050
    },
    {
      "epoch": 26.480162767039676,
      "grad_norm": 40.74282455444336,
      "learning_rate": 2.3519837232960325e-05,
      "loss": 1.8944,
      "step": 52060
    },
    {
      "epoch": 26.485249237029503,
      "grad_norm": 36.35807418823242,
      "learning_rate": 2.35147507629705e-05,
      "loss": 1.8509,
      "step": 52070
    },
    {
      "epoch": 26.49033570701933,
      "grad_norm": 34.446922302246094,
      "learning_rate": 2.3509664292980674e-05,
      "loss": 1.9256,
      "step": 52080
    },
    {
      "epoch": 26.495422177009157,
      "grad_norm": 31.282474517822266,
      "learning_rate": 2.3504577822990844e-05,
      "loss": 1.7842,
      "step": 52090
    },
    {
      "epoch": 26.500508646998984,
      "grad_norm": 37.619728088378906,
      "learning_rate": 2.3499491353001018e-05,
      "loss": 1.7797,
      "step": 52100
    },
    {
      "epoch": 26.50559511698881,
      "grad_norm": 31.617935180664062,
      "learning_rate": 2.349440488301119e-05,
      "loss": 1.8965,
      "step": 52110
    },
    {
      "epoch": 26.510681586978638,
      "grad_norm": 39.523223876953125,
      "learning_rate": 2.3489318413021364e-05,
      "loss": 1.887,
      "step": 52120
    },
    {
      "epoch": 26.515768056968465,
      "grad_norm": 38.222991943359375,
      "learning_rate": 2.3484231943031537e-05,
      "loss": 1.8724,
      "step": 52130
    },
    {
      "epoch": 26.520854526958292,
      "grad_norm": 38.03753662109375,
      "learning_rate": 2.347914547304171e-05,
      "loss": 1.8691,
      "step": 52140
    },
    {
      "epoch": 26.52594099694812,
      "grad_norm": 31.464807510375977,
      "learning_rate": 2.347405900305188e-05,
      "loss": 1.898,
      "step": 52150
    },
    {
      "epoch": 26.531027466937946,
      "grad_norm": 38.79180908203125,
      "learning_rate": 2.3468972533062057e-05,
      "loss": 1.8895,
      "step": 52160
    },
    {
      "epoch": 26.536113936927773,
      "grad_norm": 34.54254913330078,
      "learning_rate": 2.346388606307223e-05,
      "loss": 1.8276,
      "step": 52170
    },
    {
      "epoch": 26.5412004069176,
      "grad_norm": 33.11532974243164,
      "learning_rate": 2.34587995930824e-05,
      "loss": 1.935,
      "step": 52180
    },
    {
      "epoch": 26.546286876907427,
      "grad_norm": 34.21809387207031,
      "learning_rate": 2.3453713123092574e-05,
      "loss": 1.9832,
      "step": 52190
    },
    {
      "epoch": 26.551373346897254,
      "grad_norm": 41.60951614379883,
      "learning_rate": 2.3448626653102747e-05,
      "loss": 1.8415,
      "step": 52200
    },
    {
      "epoch": 26.55645981688708,
      "grad_norm": 35.66988754272461,
      "learning_rate": 2.344354018311292e-05,
      "loss": 1.9711,
      "step": 52210
    },
    {
      "epoch": 26.561546286876908,
      "grad_norm": 32.384368896484375,
      "learning_rate": 2.3438453713123094e-05,
      "loss": 1.889,
      "step": 52220
    },
    {
      "epoch": 26.566632756866735,
      "grad_norm": 35.467124938964844,
      "learning_rate": 2.3433367243133267e-05,
      "loss": 1.947,
      "step": 52230
    },
    {
      "epoch": 26.571719226856562,
      "grad_norm": 32.9979248046875,
      "learning_rate": 2.342828077314344e-05,
      "loss": 1.8967,
      "step": 52240
    },
    {
      "epoch": 26.57680569684639,
      "grad_norm": 37.562442779541016,
      "learning_rate": 2.342319430315361e-05,
      "loss": 1.9449,
      "step": 52250
    },
    {
      "epoch": 26.581892166836216,
      "grad_norm": 38.08262252807617,
      "learning_rate": 2.3418107833163787e-05,
      "loss": 1.9819,
      "step": 52260
    },
    {
      "epoch": 26.586978636826043,
      "grad_norm": 29.883941650390625,
      "learning_rate": 2.341302136317396e-05,
      "loss": 1.8941,
      "step": 52270
    },
    {
      "epoch": 26.59206510681587,
      "grad_norm": 33.551658630371094,
      "learning_rate": 2.340793489318413e-05,
      "loss": 1.8875,
      "step": 52280
    },
    {
      "epoch": 26.597151576805697,
      "grad_norm": 37.96044158935547,
      "learning_rate": 2.3402848423194303e-05,
      "loss": 1.9212,
      "step": 52290
    },
    {
      "epoch": 26.602238046795524,
      "grad_norm": 41.14561080932617,
      "learning_rate": 2.3397761953204476e-05,
      "loss": 1.8275,
      "step": 52300
    },
    {
      "epoch": 26.60732451678535,
      "grad_norm": 43.81417465209961,
      "learning_rate": 2.339267548321465e-05,
      "loss": 1.9319,
      "step": 52310
    },
    {
      "epoch": 26.612410986775178,
      "grad_norm": 44.96173858642578,
      "learning_rate": 2.3387589013224823e-05,
      "loss": 1.9491,
      "step": 52320
    },
    {
      "epoch": 26.617497456765005,
      "grad_norm": 39.634674072265625,
      "learning_rate": 2.3382502543234996e-05,
      "loss": 1.8615,
      "step": 52330
    },
    {
      "epoch": 26.62258392675483,
      "grad_norm": 39.548954010009766,
      "learning_rate": 2.337741607324517e-05,
      "loss": 1.9756,
      "step": 52340
    },
    {
      "epoch": 26.62767039674466,
      "grad_norm": 31.217313766479492,
      "learning_rate": 2.3372329603255343e-05,
      "loss": 1.8742,
      "step": 52350
    },
    {
      "epoch": 26.632756866734486,
      "grad_norm": 38.393489837646484,
      "learning_rate": 2.3367243133265516e-05,
      "loss": 1.8842,
      "step": 52360
    },
    {
      "epoch": 26.637843336724313,
      "grad_norm": 35.95132827758789,
      "learning_rate": 2.336215666327569e-05,
      "loss": 1.8929,
      "step": 52370
    },
    {
      "epoch": 26.64292980671414,
      "grad_norm": 38.06645584106445,
      "learning_rate": 2.335707019328586e-05,
      "loss": 1.8498,
      "step": 52380
    },
    {
      "epoch": 26.648016276703967,
      "grad_norm": 31.27138328552246,
      "learning_rate": 2.3351983723296033e-05,
      "loss": 1.8743,
      "step": 52390
    },
    {
      "epoch": 26.653102746693794,
      "grad_norm": 37.69335174560547,
      "learning_rate": 2.3346897253306206e-05,
      "loss": 1.9095,
      "step": 52400
    },
    {
      "epoch": 26.65818921668362,
      "grad_norm": 33.17023468017578,
      "learning_rate": 2.334181078331638e-05,
      "loss": 1.9023,
      "step": 52410
    },
    {
      "epoch": 26.663275686673447,
      "grad_norm": 39.10249328613281,
      "learning_rate": 2.3336724313326552e-05,
      "loss": 1.9549,
      "step": 52420
    },
    {
      "epoch": 26.668362156663274,
      "grad_norm": 32.246002197265625,
      "learning_rate": 2.3331637843336726e-05,
      "loss": 1.923,
      "step": 52430
    },
    {
      "epoch": 26.6734486266531,
      "grad_norm": 44.62007141113281,
      "learning_rate": 2.3326551373346896e-05,
      "loss": 1.8939,
      "step": 52440
    },
    {
      "epoch": 26.67853509664293,
      "grad_norm": 35.64411544799805,
      "learning_rate": 2.3321464903357072e-05,
      "loss": 1.8482,
      "step": 52450
    },
    {
      "epoch": 26.683621566632755,
      "grad_norm": 35.90365982055664,
      "learning_rate": 2.3316378433367246e-05,
      "loss": 1.8739,
      "step": 52460
    },
    {
      "epoch": 26.688708036622582,
      "grad_norm": 41.26519775390625,
      "learning_rate": 2.331129196337742e-05,
      "loss": 1.8653,
      "step": 52470
    },
    {
      "epoch": 26.69379450661241,
      "grad_norm": 42.42811965942383,
      "learning_rate": 2.330620549338759e-05,
      "loss": 1.903,
      "step": 52480
    },
    {
      "epoch": 26.698880976602236,
      "grad_norm": 39.59384536743164,
      "learning_rate": 2.3301119023397762e-05,
      "loss": 1.8196,
      "step": 52490
    },
    {
      "epoch": 26.703967446592067,
      "grad_norm": 28.65833854675293,
      "learning_rate": 2.329603255340794e-05,
      "loss": 1.7996,
      "step": 52500
    },
    {
      "epoch": 26.709053916581894,
      "grad_norm": 38.777652740478516,
      "learning_rate": 2.329094608341811e-05,
      "loss": 1.9266,
      "step": 52510
    },
    {
      "epoch": 26.71414038657172,
      "grad_norm": 37.79423904418945,
      "learning_rate": 2.3285859613428282e-05,
      "loss": 1.9156,
      "step": 52520
    },
    {
      "epoch": 26.719226856561548,
      "grad_norm": 25.65719985961914,
      "learning_rate": 2.3280773143438455e-05,
      "loss": 1.9888,
      "step": 52530
    },
    {
      "epoch": 26.724313326551375,
      "grad_norm": 40.48673629760742,
      "learning_rate": 2.3275686673448625e-05,
      "loss": 1.8608,
      "step": 52540
    },
    {
      "epoch": 26.729399796541202,
      "grad_norm": 39.205772399902344,
      "learning_rate": 2.3270600203458802e-05,
      "loss": 1.8659,
      "step": 52550
    },
    {
      "epoch": 26.73448626653103,
      "grad_norm": 40.28451156616211,
      "learning_rate": 2.3265513733468975e-05,
      "loss": 1.8071,
      "step": 52560
    },
    {
      "epoch": 26.739572736520856,
      "grad_norm": 36.756996154785156,
      "learning_rate": 2.3260427263479145e-05,
      "loss": 1.8833,
      "step": 52570
    },
    {
      "epoch": 26.744659206510683,
      "grad_norm": 39.50895309448242,
      "learning_rate": 2.3255340793489318e-05,
      "loss": 1.9108,
      "step": 52580
    },
    {
      "epoch": 26.74974567650051,
      "grad_norm": 39.17048645019531,
      "learning_rate": 2.325025432349949e-05,
      "loss": 1.7818,
      "step": 52590
    },
    {
      "epoch": 26.754832146490337,
      "grad_norm": 33.90900421142578,
      "learning_rate": 2.3245167853509668e-05,
      "loss": 1.9087,
      "step": 52600
    },
    {
      "epoch": 26.759918616480164,
      "grad_norm": 36.79829406738281,
      "learning_rate": 2.3240081383519838e-05,
      "loss": 1.9299,
      "step": 52610
    },
    {
      "epoch": 26.76500508646999,
      "grad_norm": 37.69731521606445,
      "learning_rate": 2.323499491353001e-05,
      "loss": 1.8382,
      "step": 52620
    },
    {
      "epoch": 26.770091556459818,
      "grad_norm": 37.467613220214844,
      "learning_rate": 2.3229908443540185e-05,
      "loss": 1.8569,
      "step": 52630
    },
    {
      "epoch": 26.775178026449645,
      "grad_norm": 30.231647491455078,
      "learning_rate": 2.3224821973550358e-05,
      "loss": 1.9529,
      "step": 52640
    },
    {
      "epoch": 26.78026449643947,
      "grad_norm": 37.45956802368164,
      "learning_rate": 2.321973550356053e-05,
      "loss": 1.8182,
      "step": 52650
    },
    {
      "epoch": 26.7853509664293,
      "grad_norm": 47.53627014160156,
      "learning_rate": 2.3214649033570704e-05,
      "loss": 1.9765,
      "step": 52660
    },
    {
      "epoch": 26.790437436419126,
      "grad_norm": 40.345706939697266,
      "learning_rate": 2.3209562563580874e-05,
      "loss": 2.0182,
      "step": 52670
    },
    {
      "epoch": 26.795523906408953,
      "grad_norm": 46.84233093261719,
      "learning_rate": 2.3204476093591048e-05,
      "loss": 1.8723,
      "step": 52680
    },
    {
      "epoch": 26.80061037639878,
      "grad_norm": 41.619659423828125,
      "learning_rate": 2.319938962360122e-05,
      "loss": 1.8918,
      "step": 52690
    },
    {
      "epoch": 26.805696846388607,
      "grad_norm": 30.79087257385254,
      "learning_rate": 2.3194303153611394e-05,
      "loss": 1.8693,
      "step": 52700
    },
    {
      "epoch": 26.810783316378433,
      "grad_norm": 34.175941467285156,
      "learning_rate": 2.3189216683621567e-05,
      "loss": 1.9307,
      "step": 52710
    },
    {
      "epoch": 26.81586978636826,
      "grad_norm": 27.837646484375,
      "learning_rate": 2.318413021363174e-05,
      "loss": 1.8497,
      "step": 52720
    },
    {
      "epoch": 26.820956256358087,
      "grad_norm": 42.81328582763672,
      "learning_rate": 2.3179043743641914e-05,
      "loss": 1.874,
      "step": 52730
    },
    {
      "epoch": 26.826042726347914,
      "grad_norm": 33.4134635925293,
      "learning_rate": 2.3173957273652087e-05,
      "loss": 1.802,
      "step": 52740
    },
    {
      "epoch": 26.83112919633774,
      "grad_norm": 35.29811096191406,
      "learning_rate": 2.316887080366226e-05,
      "loss": 1.8985,
      "step": 52750
    },
    {
      "epoch": 26.83621566632757,
      "grad_norm": 46.479122161865234,
      "learning_rate": 2.3163784333672434e-05,
      "loss": 1.9431,
      "step": 52760
    },
    {
      "epoch": 26.841302136317395,
      "grad_norm": 31.761428833007812,
      "learning_rate": 2.3158697863682604e-05,
      "loss": 1.8112,
      "step": 52770
    },
    {
      "epoch": 26.846388606307222,
      "grad_norm": 40.66055679321289,
      "learning_rate": 2.3153611393692777e-05,
      "loss": 2.0026,
      "step": 52780
    },
    {
      "epoch": 26.85147507629705,
      "grad_norm": 31.258869171142578,
      "learning_rate": 2.3148524923702954e-05,
      "loss": 1.9063,
      "step": 52790
    },
    {
      "epoch": 26.856561546286876,
      "grad_norm": 46.000511169433594,
      "learning_rate": 2.3143438453713124e-05,
      "loss": 1.8233,
      "step": 52800
    },
    {
      "epoch": 26.861648016276703,
      "grad_norm": 26.46453857421875,
      "learning_rate": 2.3138351983723297e-05,
      "loss": 1.8745,
      "step": 52810
    },
    {
      "epoch": 26.86673448626653,
      "grad_norm": 40.80030822753906,
      "learning_rate": 2.313326551373347e-05,
      "loss": 1.7926,
      "step": 52820
    },
    {
      "epoch": 26.871820956256357,
      "grad_norm": 35.54518127441406,
      "learning_rate": 2.3128179043743643e-05,
      "loss": 1.9192,
      "step": 52830
    },
    {
      "epoch": 26.876907426246184,
      "grad_norm": 40.97871398925781,
      "learning_rate": 2.3123092573753817e-05,
      "loss": 1.8964,
      "step": 52840
    },
    {
      "epoch": 26.88199389623601,
      "grad_norm": 34.60577392578125,
      "learning_rate": 2.311800610376399e-05,
      "loss": 1.8923,
      "step": 52850
    },
    {
      "epoch": 26.887080366225838,
      "grad_norm": 37.93825149536133,
      "learning_rate": 2.311291963377416e-05,
      "loss": 1.9232,
      "step": 52860
    },
    {
      "epoch": 26.892166836215665,
      "grad_norm": 35.90005111694336,
      "learning_rate": 2.3107833163784333e-05,
      "loss": 1.8534,
      "step": 52870
    },
    {
      "epoch": 26.897253306205492,
      "grad_norm": 47.66752624511719,
      "learning_rate": 2.3102746693794507e-05,
      "loss": 1.9308,
      "step": 52880
    },
    {
      "epoch": 26.90233977619532,
      "grad_norm": 35.24140167236328,
      "learning_rate": 2.3097660223804683e-05,
      "loss": 1.865,
      "step": 52890
    },
    {
      "epoch": 26.907426246185146,
      "grad_norm": 47.13160705566406,
      "learning_rate": 2.3092573753814853e-05,
      "loss": 1.8813,
      "step": 52900
    },
    {
      "epoch": 26.912512716174973,
      "grad_norm": 34.547969818115234,
      "learning_rate": 2.3087487283825026e-05,
      "loss": 1.8663,
      "step": 52910
    },
    {
      "epoch": 26.9175991861648,
      "grad_norm": 35.292633056640625,
      "learning_rate": 2.30824008138352e-05,
      "loss": 1.9147,
      "step": 52920
    },
    {
      "epoch": 26.922685656154627,
      "grad_norm": 37.028743743896484,
      "learning_rate": 2.3077314343845373e-05,
      "loss": 1.7882,
      "step": 52930
    },
    {
      "epoch": 26.927772126144454,
      "grad_norm": 32.567115783691406,
      "learning_rate": 2.3072227873855546e-05,
      "loss": 1.9185,
      "step": 52940
    },
    {
      "epoch": 26.93285859613428,
      "grad_norm": 44.26395797729492,
      "learning_rate": 2.306714140386572e-05,
      "loss": 1.8968,
      "step": 52950
    },
    {
      "epoch": 26.93794506612411,
      "grad_norm": 43.078819274902344,
      "learning_rate": 2.306205493387589e-05,
      "loss": 1.7928,
      "step": 52960
    },
    {
      "epoch": 26.94303153611394,
      "grad_norm": 34.22793197631836,
      "learning_rate": 2.3056968463886063e-05,
      "loss": 1.8621,
      "step": 52970
    },
    {
      "epoch": 26.948118006103766,
      "grad_norm": 31.15629768371582,
      "learning_rate": 2.305188199389624e-05,
      "loss": 1.7662,
      "step": 52980
    },
    {
      "epoch": 26.953204476093592,
      "grad_norm": 35.50762939453125,
      "learning_rate": 2.304679552390641e-05,
      "loss": 1.9005,
      "step": 52990
    },
    {
      "epoch": 26.95829094608342,
      "grad_norm": 36.690223693847656,
      "learning_rate": 2.3041709053916582e-05,
      "loss": 1.9189,
      "step": 53000
    },
    {
      "epoch": 26.963377416073246,
      "grad_norm": 40.84468460083008,
      "learning_rate": 2.3036622583926756e-05,
      "loss": 1.932,
      "step": 53010
    },
    {
      "epoch": 26.968463886063073,
      "grad_norm": 50.96818923950195,
      "learning_rate": 2.303153611393693e-05,
      "loss": 1.7969,
      "step": 53020
    },
    {
      "epoch": 26.9735503560529,
      "grad_norm": 32.239532470703125,
      "learning_rate": 2.3026449643947102e-05,
      "loss": 1.8881,
      "step": 53030
    },
    {
      "epoch": 26.978636826042727,
      "grad_norm": 30.993623733520508,
      "learning_rate": 2.3021363173957276e-05,
      "loss": 1.873,
      "step": 53040
    },
    {
      "epoch": 26.983723296032554,
      "grad_norm": 38.09880828857422,
      "learning_rate": 2.301627670396745e-05,
      "loss": 1.8588,
      "step": 53050
    },
    {
      "epoch": 26.98880976602238,
      "grad_norm": 41.88258361816406,
      "learning_rate": 2.301119023397762e-05,
      "loss": 1.9,
      "step": 53060
    },
    {
      "epoch": 26.99389623601221,
      "grad_norm": 34.534034729003906,
      "learning_rate": 2.3006103763987792e-05,
      "loss": 1.9634,
      "step": 53070
    },
    {
      "epoch": 26.998982706002035,
      "grad_norm": 38.36847686767578,
      "learning_rate": 2.300101729399797e-05,
      "loss": 1.9796,
      "step": 53080
    },
    {
      "epoch": 27.0,
      "eval_loss": 4.639437675476074,
      "eval_runtime": 2.7919,
      "eval_samples_per_second": 993.958,
      "eval_steps_per_second": 124.29,
      "step": 53082
    },
    {
      "epoch": 27.004069175991862,
      "grad_norm": 37.865108489990234,
      "learning_rate": 2.299593082400814e-05,
      "loss": 1.8092,
      "step": 53090
    },
    {
      "epoch": 27.00915564598169,
      "grad_norm": 40.526432037353516,
      "learning_rate": 2.2990844354018312e-05,
      "loss": 1.8072,
      "step": 53100
    },
    {
      "epoch": 27.014242115971516,
      "grad_norm": 41.784812927246094,
      "learning_rate": 2.2985757884028485e-05,
      "loss": 1.8673,
      "step": 53110
    },
    {
      "epoch": 27.019328585961343,
      "grad_norm": 54.68363952636719,
      "learning_rate": 2.298067141403866e-05,
      "loss": 1.876,
      "step": 53120
    },
    {
      "epoch": 27.02441505595117,
      "grad_norm": 37.10456085205078,
      "learning_rate": 2.2975584944048832e-05,
      "loss": 1.9219,
      "step": 53130
    },
    {
      "epoch": 27.029501525940997,
      "grad_norm": 42.415225982666016,
      "learning_rate": 2.2970498474059005e-05,
      "loss": 1.8687,
      "step": 53140
    },
    {
      "epoch": 27.034587995930824,
      "grad_norm": 32.6048583984375,
      "learning_rate": 2.296541200406918e-05,
      "loss": 1.8405,
      "step": 53150
    },
    {
      "epoch": 27.03967446592065,
      "grad_norm": 36.96748733520508,
      "learning_rate": 2.2960325534079348e-05,
      "loss": 1.7644,
      "step": 53160
    },
    {
      "epoch": 27.044760935910478,
      "grad_norm": 33.467018127441406,
      "learning_rate": 2.295523906408952e-05,
      "loss": 1.9346,
      "step": 53170
    },
    {
      "epoch": 27.049847405900305,
      "grad_norm": 38.00342559814453,
      "learning_rate": 2.2950152594099698e-05,
      "loss": 1.6722,
      "step": 53180
    },
    {
      "epoch": 27.054933875890132,
      "grad_norm": 41.023658752441406,
      "learning_rate": 2.2945066124109868e-05,
      "loss": 1.8106,
      "step": 53190
    },
    {
      "epoch": 27.06002034587996,
      "grad_norm": 30.714641571044922,
      "learning_rate": 2.293997965412004e-05,
      "loss": 1.8683,
      "step": 53200
    },
    {
      "epoch": 27.065106815869786,
      "grad_norm": 38.897396087646484,
      "learning_rate": 2.2934893184130215e-05,
      "loss": 1.7797,
      "step": 53210
    },
    {
      "epoch": 27.070193285859613,
      "grad_norm": 31.565383911132812,
      "learning_rate": 2.2929806714140388e-05,
      "loss": 1.8936,
      "step": 53220
    },
    {
      "epoch": 27.07527975584944,
      "grad_norm": 34.172264099121094,
      "learning_rate": 2.292472024415056e-05,
      "loss": 1.9,
      "step": 53230
    },
    {
      "epoch": 27.080366225839267,
      "grad_norm": 35.28185272216797,
      "learning_rate": 2.2919633774160734e-05,
      "loss": 1.8694,
      "step": 53240
    },
    {
      "epoch": 27.085452695829094,
      "grad_norm": 34.861602783203125,
      "learning_rate": 2.2914547304170904e-05,
      "loss": 1.8671,
      "step": 53250
    },
    {
      "epoch": 27.09053916581892,
      "grad_norm": 38.84874725341797,
      "learning_rate": 2.2909460834181078e-05,
      "loss": 1.834,
      "step": 53260
    },
    {
      "epoch": 27.095625635808748,
      "grad_norm": 38.22065353393555,
      "learning_rate": 2.2904374364191254e-05,
      "loss": 1.8446,
      "step": 53270
    },
    {
      "epoch": 27.100712105798575,
      "grad_norm": 36.99679946899414,
      "learning_rate": 2.2899287894201428e-05,
      "loss": 1.9086,
      "step": 53280
    },
    {
      "epoch": 27.105798575788402,
      "grad_norm": 39.61203384399414,
      "learning_rate": 2.2894201424211598e-05,
      "loss": 1.8972,
      "step": 53290
    },
    {
      "epoch": 27.11088504577823,
      "grad_norm": 29.2473087310791,
      "learning_rate": 2.288911495422177e-05,
      "loss": 1.7981,
      "step": 53300
    },
    {
      "epoch": 27.115971515768056,
      "grad_norm": 34.826969146728516,
      "learning_rate": 2.2884028484231944e-05,
      "loss": 1.8909,
      "step": 53310
    },
    {
      "epoch": 27.121057985757883,
      "grad_norm": 35.58644485473633,
      "learning_rate": 2.2878942014242117e-05,
      "loss": 1.869,
      "step": 53320
    },
    {
      "epoch": 27.12614445574771,
      "grad_norm": 40.1932487487793,
      "learning_rate": 2.287385554425229e-05,
      "loss": 1.8243,
      "step": 53330
    },
    {
      "epoch": 27.131230925737537,
      "grad_norm": 35.198974609375,
      "learning_rate": 2.2868769074262464e-05,
      "loss": 1.854,
      "step": 53340
    },
    {
      "epoch": 27.136317395727364,
      "grad_norm": 36.357330322265625,
      "learning_rate": 2.2863682604272634e-05,
      "loss": 1.8245,
      "step": 53350
    },
    {
      "epoch": 27.14140386571719,
      "grad_norm": 47.076271057128906,
      "learning_rate": 2.2858596134282807e-05,
      "loss": 1.8439,
      "step": 53360
    },
    {
      "epoch": 27.146490335707018,
      "grad_norm": 39.484413146972656,
      "learning_rate": 2.2853509664292984e-05,
      "loss": 1.9149,
      "step": 53370
    },
    {
      "epoch": 27.151576805696845,
      "grad_norm": 31.797260284423828,
      "learning_rate": 2.2848423194303154e-05,
      "loss": 1.8168,
      "step": 53380
    },
    {
      "epoch": 27.15666327568667,
      "grad_norm": 35.84524154663086,
      "learning_rate": 2.2843336724313327e-05,
      "loss": 1.8612,
      "step": 53390
    },
    {
      "epoch": 27.161749745676502,
      "grad_norm": 36.307987213134766,
      "learning_rate": 2.28382502543235e-05,
      "loss": 1.8342,
      "step": 53400
    },
    {
      "epoch": 27.16683621566633,
      "grad_norm": 37.524688720703125,
      "learning_rate": 2.2833163784333673e-05,
      "loss": 1.8706,
      "step": 53410
    },
    {
      "epoch": 27.171922685656156,
      "grad_norm": 32.18286895751953,
      "learning_rate": 2.2828077314343847e-05,
      "loss": 1.9162,
      "step": 53420
    },
    {
      "epoch": 27.177009155645983,
      "grad_norm": 40.6359977722168,
      "learning_rate": 2.282299084435402e-05,
      "loss": 1.899,
      "step": 53430
    },
    {
      "epoch": 27.18209562563581,
      "grad_norm": 35.10564422607422,
      "learning_rate": 2.2817904374364193e-05,
      "loss": 1.8987,
      "step": 53440
    },
    {
      "epoch": 27.187182095625637,
      "grad_norm": 41.682762145996094,
      "learning_rate": 2.2812817904374363e-05,
      "loss": 1.9302,
      "step": 53450
    },
    {
      "epoch": 27.192268565615464,
      "grad_norm": 36.494632720947266,
      "learning_rate": 2.280773143438454e-05,
      "loss": 1.953,
      "step": 53460
    },
    {
      "epoch": 27.19735503560529,
      "grad_norm": 34.669864654541016,
      "learning_rate": 2.2802644964394713e-05,
      "loss": 1.846,
      "step": 53470
    },
    {
      "epoch": 27.202441505595118,
      "grad_norm": 33.41643524169922,
      "learning_rate": 2.2797558494404883e-05,
      "loss": 1.8391,
      "step": 53480
    },
    {
      "epoch": 27.207527975584945,
      "grad_norm": 39.541542053222656,
      "learning_rate": 2.2792472024415056e-05,
      "loss": 1.9698,
      "step": 53490
    },
    {
      "epoch": 27.212614445574772,
      "grad_norm": 38.19635772705078,
      "learning_rate": 2.278738555442523e-05,
      "loss": 1.9104,
      "step": 53500
    },
    {
      "epoch": 27.2177009155646,
      "grad_norm": 36.11312484741211,
      "learning_rate": 2.2782299084435403e-05,
      "loss": 1.9756,
      "step": 53510
    },
    {
      "epoch": 27.222787385554426,
      "grad_norm": 42.065452575683594,
      "learning_rate": 2.2777212614445576e-05,
      "loss": 1.8797,
      "step": 53520
    },
    {
      "epoch": 27.227873855544253,
      "grad_norm": 43.71677017211914,
      "learning_rate": 2.277212614445575e-05,
      "loss": 1.8425,
      "step": 53530
    },
    {
      "epoch": 27.23296032553408,
      "grad_norm": 31.992549896240234,
      "learning_rate": 2.2767039674465923e-05,
      "loss": 1.9346,
      "step": 53540
    },
    {
      "epoch": 27.238046795523907,
      "grad_norm": 38.27919006347656,
      "learning_rate": 2.2761953204476093e-05,
      "loss": 1.9371,
      "step": 53550
    },
    {
      "epoch": 27.243133265513734,
      "grad_norm": 40.38703155517578,
      "learning_rate": 2.275686673448627e-05,
      "loss": 1.9227,
      "step": 53560
    },
    {
      "epoch": 27.24821973550356,
      "grad_norm": 34.625186920166016,
      "learning_rate": 2.2751780264496443e-05,
      "loss": 1.9126,
      "step": 53570
    },
    {
      "epoch": 27.253306205493388,
      "grad_norm": 33.284244537353516,
      "learning_rate": 2.2746693794506613e-05,
      "loss": 1.8863,
      "step": 53580
    },
    {
      "epoch": 27.258392675483215,
      "grad_norm": 37.664939880371094,
      "learning_rate": 2.2741607324516786e-05,
      "loss": 1.8787,
      "step": 53590
    },
    {
      "epoch": 27.263479145473042,
      "grad_norm": 37.156227111816406,
      "learning_rate": 2.273652085452696e-05,
      "loss": 1.9034,
      "step": 53600
    },
    {
      "epoch": 27.26856561546287,
      "grad_norm": 43.33607864379883,
      "learning_rate": 2.2731434384537132e-05,
      "loss": 1.8589,
      "step": 53610
    },
    {
      "epoch": 27.273652085452696,
      "grad_norm": 39.44927215576172,
      "learning_rate": 2.2726347914547306e-05,
      "loss": 1.9028,
      "step": 53620
    },
    {
      "epoch": 27.278738555442523,
      "grad_norm": 39.5096549987793,
      "learning_rate": 2.272126144455748e-05,
      "loss": 1.78,
      "step": 53630
    },
    {
      "epoch": 27.28382502543235,
      "grad_norm": 36.19486999511719,
      "learning_rate": 2.271617497456765e-05,
      "loss": 1.9351,
      "step": 53640
    },
    {
      "epoch": 27.288911495422177,
      "grad_norm": 36.91262435913086,
      "learning_rate": 2.2711088504577822e-05,
      "loss": 1.8132,
      "step": 53650
    },
    {
      "epoch": 27.293997965412004,
      "grad_norm": 38.27655792236328,
      "learning_rate": 2.2706002034588e-05,
      "loss": 1.9042,
      "step": 53660
    },
    {
      "epoch": 27.29908443540183,
      "grad_norm": 38.19184875488281,
      "learning_rate": 2.270091556459817e-05,
      "loss": 1.8437,
      "step": 53670
    },
    {
      "epoch": 27.304170905391658,
      "grad_norm": 34.920711517333984,
      "learning_rate": 2.2695829094608342e-05,
      "loss": 1.9117,
      "step": 53680
    },
    {
      "epoch": 27.309257375381485,
      "grad_norm": 31.81175422668457,
      "learning_rate": 2.2690742624618515e-05,
      "loss": 1.8234,
      "step": 53690
    },
    {
      "epoch": 27.31434384537131,
      "grad_norm": 34.67573165893555,
      "learning_rate": 2.268565615462869e-05,
      "loss": 1.8054,
      "step": 53700
    },
    {
      "epoch": 27.31943031536114,
      "grad_norm": 49.37334442138672,
      "learning_rate": 2.2680569684638862e-05,
      "loss": 1.8797,
      "step": 53710
    },
    {
      "epoch": 27.324516785350966,
      "grad_norm": 36.55179214477539,
      "learning_rate": 2.2675483214649035e-05,
      "loss": 1.9188,
      "step": 53720
    },
    {
      "epoch": 27.329603255340793,
      "grad_norm": 52.15943908691406,
      "learning_rate": 2.267039674465921e-05,
      "loss": 1.8954,
      "step": 53730
    },
    {
      "epoch": 27.33468972533062,
      "grad_norm": 38.0450325012207,
      "learning_rate": 2.2665310274669378e-05,
      "loss": 1.8608,
      "step": 53740
    },
    {
      "epoch": 27.339776195320447,
      "grad_norm": 35.262699127197266,
      "learning_rate": 2.2660223804679555e-05,
      "loss": 1.8818,
      "step": 53750
    },
    {
      "epoch": 27.344862665310274,
      "grad_norm": 35.17316818237305,
      "learning_rate": 2.2655137334689728e-05,
      "loss": 1.7508,
      "step": 53760
    },
    {
      "epoch": 27.3499491353001,
      "grad_norm": 33.51792907714844,
      "learning_rate": 2.2650050864699898e-05,
      "loss": 1.8461,
      "step": 53770
    },
    {
      "epoch": 27.355035605289928,
      "grad_norm": 42.173946380615234,
      "learning_rate": 2.264496439471007e-05,
      "loss": 1.9334,
      "step": 53780
    },
    {
      "epoch": 27.360122075279754,
      "grad_norm": 40.68714904785156,
      "learning_rate": 2.2639877924720245e-05,
      "loss": 1.9478,
      "step": 53790
    },
    {
      "epoch": 27.36520854526958,
      "grad_norm": 35.242977142333984,
      "learning_rate": 2.2634791454730418e-05,
      "loss": 1.9254,
      "step": 53800
    },
    {
      "epoch": 27.37029501525941,
      "grad_norm": 37.231971740722656,
      "learning_rate": 2.262970498474059e-05,
      "loss": 1.8845,
      "step": 53810
    },
    {
      "epoch": 27.375381485249235,
      "grad_norm": 36.31602096557617,
      "learning_rate": 2.2624618514750764e-05,
      "loss": 1.8099,
      "step": 53820
    },
    {
      "epoch": 27.380467955239062,
      "grad_norm": 30.6256046295166,
      "learning_rate": 2.2619532044760938e-05,
      "loss": 1.8734,
      "step": 53830
    },
    {
      "epoch": 27.38555442522889,
      "grad_norm": 36.686031341552734,
      "learning_rate": 2.2614445574771108e-05,
      "loss": 1.8745,
      "step": 53840
    },
    {
      "epoch": 27.39064089521872,
      "grad_norm": 35.67613220214844,
      "learning_rate": 2.2609359104781284e-05,
      "loss": 1.9465,
      "step": 53850
    },
    {
      "epoch": 27.395727365208547,
      "grad_norm": 33.4703483581543,
      "learning_rate": 2.2604272634791458e-05,
      "loss": 1.8388,
      "step": 53860
    },
    {
      "epoch": 27.400813835198374,
      "grad_norm": 39.126380920410156,
      "learning_rate": 2.2599186164801628e-05,
      "loss": 1.8749,
      "step": 53870
    },
    {
      "epoch": 27.4059003051882,
      "grad_norm": 42.563140869140625,
      "learning_rate": 2.25940996948118e-05,
      "loss": 1.9001,
      "step": 53880
    },
    {
      "epoch": 27.410986775178028,
      "grad_norm": 32.44158935546875,
      "learning_rate": 2.2589013224821974e-05,
      "loss": 1.8406,
      "step": 53890
    },
    {
      "epoch": 27.416073245167855,
      "grad_norm": 35.97068786621094,
      "learning_rate": 2.2583926754832147e-05,
      "loss": 1.8847,
      "step": 53900
    },
    {
      "epoch": 27.421159715157682,
      "grad_norm": 30.12428855895996,
      "learning_rate": 2.257884028484232e-05,
      "loss": 1.7925,
      "step": 53910
    },
    {
      "epoch": 27.42624618514751,
      "grad_norm": 36.56829071044922,
      "learning_rate": 2.2573753814852494e-05,
      "loss": 1.8235,
      "step": 53920
    },
    {
      "epoch": 27.431332655137336,
      "grad_norm": 39.50154113769531,
      "learning_rate": 2.2568667344862664e-05,
      "loss": 1.9379,
      "step": 53930
    },
    {
      "epoch": 27.436419125127163,
      "grad_norm": 39.6364631652832,
      "learning_rate": 2.256358087487284e-05,
      "loss": 1.8842,
      "step": 53940
    },
    {
      "epoch": 27.44150559511699,
      "grad_norm": 32.14957809448242,
      "learning_rate": 2.2558494404883014e-05,
      "loss": 1.8739,
      "step": 53950
    },
    {
      "epoch": 27.446592065106817,
      "grad_norm": 26.733423233032227,
      "learning_rate": 2.2553407934893187e-05,
      "loss": 1.9438,
      "step": 53960
    },
    {
      "epoch": 27.451678535096644,
      "grad_norm": 43.655418395996094,
      "learning_rate": 2.2548321464903357e-05,
      "loss": 1.8233,
      "step": 53970
    },
    {
      "epoch": 27.45676500508647,
      "grad_norm": 41.45753479003906,
      "learning_rate": 2.254323499491353e-05,
      "loss": 1.8833,
      "step": 53980
    },
    {
      "epoch": 27.461851475076298,
      "grad_norm": 34.82609176635742,
      "learning_rate": 2.2538148524923703e-05,
      "loss": 1.8817,
      "step": 53990
    },
    {
      "epoch": 27.466937945066125,
      "grad_norm": 37.285640716552734,
      "learning_rate": 2.2533062054933877e-05,
      "loss": 1.8209,
      "step": 54000
    },
    {
      "epoch": 27.47202441505595,
      "grad_norm": 36.994564056396484,
      "learning_rate": 2.252797558494405e-05,
      "loss": 1.7846,
      "step": 54010
    },
    {
      "epoch": 27.47711088504578,
      "grad_norm": 27.926048278808594,
      "learning_rate": 2.2522889114954223e-05,
      "loss": 1.8173,
      "step": 54020
    },
    {
      "epoch": 27.482197355035606,
      "grad_norm": 38.07267379760742,
      "learning_rate": 2.2517802644964393e-05,
      "loss": 1.9277,
      "step": 54030
    },
    {
      "epoch": 27.487283825025433,
      "grad_norm": 34.26470947265625,
      "learning_rate": 2.251271617497457e-05,
      "loss": 1.8478,
      "step": 54040
    },
    {
      "epoch": 27.49237029501526,
      "grad_norm": 33.70304489135742,
      "learning_rate": 2.2507629704984743e-05,
      "loss": 1.827,
      "step": 54050
    },
    {
      "epoch": 27.497456765005087,
      "grad_norm": 44.15065383911133,
      "learning_rate": 2.2502543234994913e-05,
      "loss": 1.8504,
      "step": 54060
    },
    {
      "epoch": 27.502543234994913,
      "grad_norm": 30.74545669555664,
      "learning_rate": 2.2497456765005086e-05,
      "loss": 1.8116,
      "step": 54070
    },
    {
      "epoch": 27.50762970498474,
      "grad_norm": 36.49958801269531,
      "learning_rate": 2.249237029501526e-05,
      "loss": 1.9732,
      "step": 54080
    },
    {
      "epoch": 27.512716174974567,
      "grad_norm": 30.25004005432129,
      "learning_rate": 2.2487283825025436e-05,
      "loss": 1.8014,
      "step": 54090
    },
    {
      "epoch": 27.517802644964394,
      "grad_norm": 35.979793548583984,
      "learning_rate": 2.2482197355035606e-05,
      "loss": 1.8783,
      "step": 54100
    },
    {
      "epoch": 27.52288911495422,
      "grad_norm": 36.135005950927734,
      "learning_rate": 2.247711088504578e-05,
      "loss": 1.8463,
      "step": 54110
    },
    {
      "epoch": 27.52797558494405,
      "grad_norm": 37.299720764160156,
      "learning_rate": 2.2472024415055953e-05,
      "loss": 1.8537,
      "step": 54120
    },
    {
      "epoch": 27.533062054933875,
      "grad_norm": 42.95977783203125,
      "learning_rate": 2.2466937945066126e-05,
      "loss": 1.8355,
      "step": 54130
    },
    {
      "epoch": 27.538148524923702,
      "grad_norm": 32.065589904785156,
      "learning_rate": 2.24618514750763e-05,
      "loss": 1.8486,
      "step": 54140
    },
    {
      "epoch": 27.54323499491353,
      "grad_norm": 34.2982292175293,
      "learning_rate": 2.2456765005086473e-05,
      "loss": 1.8819,
      "step": 54150
    },
    {
      "epoch": 27.548321464903356,
      "grad_norm": 40.18988800048828,
      "learning_rate": 2.2451678535096643e-05,
      "loss": 1.8745,
      "step": 54160
    },
    {
      "epoch": 27.553407934893183,
      "grad_norm": 35.25639343261719,
      "learning_rate": 2.2446592065106816e-05,
      "loss": 1.8452,
      "step": 54170
    },
    {
      "epoch": 27.55849440488301,
      "grad_norm": 34.92845153808594,
      "learning_rate": 2.244150559511699e-05,
      "loss": 1.8728,
      "step": 54180
    },
    {
      "epoch": 27.563580874872837,
      "grad_norm": 39.43309020996094,
      "learning_rate": 2.2436419125127162e-05,
      "loss": 1.9751,
      "step": 54190
    },
    {
      "epoch": 27.568667344862664,
      "grad_norm": 45.167877197265625,
      "learning_rate": 2.2431332655137336e-05,
      "loss": 1.9088,
      "step": 54200
    },
    {
      "epoch": 27.57375381485249,
      "grad_norm": 46.561038970947266,
      "learning_rate": 2.242624618514751e-05,
      "loss": 1.9488,
      "step": 54210
    },
    {
      "epoch": 27.578840284842318,
      "grad_norm": 38.67793655395508,
      "learning_rate": 2.2421159715157682e-05,
      "loss": 1.9226,
      "step": 54220
    },
    {
      "epoch": 27.583926754832145,
      "grad_norm": 35.671791076660156,
      "learning_rate": 2.2416073245167855e-05,
      "loss": 1.8777,
      "step": 54230
    },
    {
      "epoch": 27.589013224821972,
      "grad_norm": 31.843456268310547,
      "learning_rate": 2.241098677517803e-05,
      "loss": 1.8988,
      "step": 54240
    },
    {
      "epoch": 27.5940996948118,
      "grad_norm": 38.43707275390625,
      "learning_rate": 2.2405900305188202e-05,
      "loss": 1.8935,
      "step": 54250
    },
    {
      "epoch": 27.599186164801626,
      "grad_norm": 37.71229934692383,
      "learning_rate": 2.2400813835198372e-05,
      "loss": 1.8861,
      "step": 54260
    },
    {
      "epoch": 27.604272634791453,
      "grad_norm": 44.44675827026367,
      "learning_rate": 2.2395727365208545e-05,
      "loss": 1.8353,
      "step": 54270
    },
    {
      "epoch": 27.60935910478128,
      "grad_norm": 30.548717498779297,
      "learning_rate": 2.2390640895218722e-05,
      "loss": 1.8887,
      "step": 54280
    },
    {
      "epoch": 27.61444557477111,
      "grad_norm": 39.13442611694336,
      "learning_rate": 2.2385554425228892e-05,
      "loss": 1.8599,
      "step": 54290
    },
    {
      "epoch": 27.619532044760938,
      "grad_norm": 38.51585006713867,
      "learning_rate": 2.2380467955239065e-05,
      "loss": 1.837,
      "step": 54300
    },
    {
      "epoch": 27.624618514750765,
      "grad_norm": 33.46529006958008,
      "learning_rate": 2.237538148524924e-05,
      "loss": 1.838,
      "step": 54310
    },
    {
      "epoch": 27.62970498474059,
      "grad_norm": 36.050228118896484,
      "learning_rate": 2.2370295015259408e-05,
      "loss": 1.9212,
      "step": 54320
    },
    {
      "epoch": 27.63479145473042,
      "grad_norm": 38.46817398071289,
      "learning_rate": 2.2365208545269585e-05,
      "loss": 1.829,
      "step": 54330
    },
    {
      "epoch": 27.639877924720246,
      "grad_norm": 39.147239685058594,
      "learning_rate": 2.2360122075279758e-05,
      "loss": 1.9102,
      "step": 54340
    },
    {
      "epoch": 27.644964394710072,
      "grad_norm": 33.381351470947266,
      "learning_rate": 2.235503560528993e-05,
      "loss": 1.8131,
      "step": 54350
    },
    {
      "epoch": 27.6500508646999,
      "grad_norm": 37.18912124633789,
      "learning_rate": 2.23499491353001e-05,
      "loss": 1.8086,
      "step": 54360
    },
    {
      "epoch": 27.655137334689726,
      "grad_norm": 33.27641677856445,
      "learning_rate": 2.2344862665310275e-05,
      "loss": 1.9284,
      "step": 54370
    },
    {
      "epoch": 27.660223804679553,
      "grad_norm": 33.96046829223633,
      "learning_rate": 2.233977619532045e-05,
      "loss": 1.8575,
      "step": 54380
    },
    {
      "epoch": 27.66531027466938,
      "grad_norm": 40.13359069824219,
      "learning_rate": 2.233468972533062e-05,
      "loss": 1.8461,
      "step": 54390
    },
    {
      "epoch": 27.670396744659207,
      "grad_norm": 34.50345993041992,
      "learning_rate": 2.2329603255340794e-05,
      "loss": 1.9251,
      "step": 54400
    },
    {
      "epoch": 27.675483214649034,
      "grad_norm": 37.4985237121582,
      "learning_rate": 2.2324516785350968e-05,
      "loss": 1.8289,
      "step": 54410
    },
    {
      "epoch": 27.68056968463886,
      "grad_norm": 36.98738479614258,
      "learning_rate": 2.231943031536114e-05,
      "loss": 1.8888,
      "step": 54420
    },
    {
      "epoch": 27.68565615462869,
      "grad_norm": 44.886131286621094,
      "learning_rate": 2.2314343845371314e-05,
      "loss": 1.8542,
      "step": 54430
    },
    {
      "epoch": 27.690742624618515,
      "grad_norm": 39.83378601074219,
      "learning_rate": 2.2309257375381488e-05,
      "loss": 1.8857,
      "step": 54440
    },
    {
      "epoch": 27.695829094608342,
      "grad_norm": 36.072750091552734,
      "learning_rate": 2.2304170905391658e-05,
      "loss": 1.8911,
      "step": 54450
    },
    {
      "epoch": 27.70091556459817,
      "grad_norm": 30.959423065185547,
      "learning_rate": 2.229908443540183e-05,
      "loss": 1.8601,
      "step": 54460
    },
    {
      "epoch": 27.706002034587996,
      "grad_norm": 45.30659484863281,
      "learning_rate": 2.2293997965412004e-05,
      "loss": 1.8444,
      "step": 54470
    },
    {
      "epoch": 27.711088504577823,
      "grad_norm": 29.532217025756836,
      "learning_rate": 2.2288911495422177e-05,
      "loss": 1.8887,
      "step": 54480
    },
    {
      "epoch": 27.71617497456765,
      "grad_norm": 32.534175872802734,
      "learning_rate": 2.228382502543235e-05,
      "loss": 1.9437,
      "step": 54490
    },
    {
      "epoch": 27.721261444557477,
      "grad_norm": 46.83637619018555,
      "learning_rate": 2.2278738555442524e-05,
      "loss": 1.8725,
      "step": 54500
    },
    {
      "epoch": 27.726347914547304,
      "grad_norm": 34.12397766113281,
      "learning_rate": 2.2273652085452697e-05,
      "loss": 1.8109,
      "step": 54510
    },
    {
      "epoch": 27.73143438453713,
      "grad_norm": 31.051525115966797,
      "learning_rate": 2.226856561546287e-05,
      "loss": 1.8739,
      "step": 54520
    },
    {
      "epoch": 27.736520854526958,
      "grad_norm": 35.23031997680664,
      "learning_rate": 2.2263479145473044e-05,
      "loss": 1.8833,
      "step": 54530
    },
    {
      "epoch": 27.741607324516785,
      "grad_norm": 30.18960952758789,
      "learning_rate": 2.2258392675483217e-05,
      "loss": 1.8095,
      "step": 54540
    },
    {
      "epoch": 27.746693794506612,
      "grad_norm": 36.96628952026367,
      "learning_rate": 2.2253306205493387e-05,
      "loss": 1.8743,
      "step": 54550
    },
    {
      "epoch": 27.75178026449644,
      "grad_norm": 33.03968048095703,
      "learning_rate": 2.224821973550356e-05,
      "loss": 1.8345,
      "step": 54560
    },
    {
      "epoch": 27.756866734486266,
      "grad_norm": 35.0732536315918,
      "learning_rate": 2.2243133265513737e-05,
      "loss": 1.7606,
      "step": 54570
    },
    {
      "epoch": 27.761953204476093,
      "grad_norm": 40.927860260009766,
      "learning_rate": 2.2238046795523907e-05,
      "loss": 1.8642,
      "step": 54580
    },
    {
      "epoch": 27.76703967446592,
      "grad_norm": 34.62605667114258,
      "learning_rate": 2.223296032553408e-05,
      "loss": 1.8544,
      "step": 54590
    },
    {
      "epoch": 27.772126144455747,
      "grad_norm": 29.866090774536133,
      "learning_rate": 2.2227873855544253e-05,
      "loss": 1.867,
      "step": 54600
    },
    {
      "epoch": 27.777212614445574,
      "grad_norm": 47.387245178222656,
      "learning_rate": 2.2222787385554427e-05,
      "loss": 1.836,
      "step": 54610
    },
    {
      "epoch": 27.7822990844354,
      "grad_norm": 36.64976501464844,
      "learning_rate": 2.22177009155646e-05,
      "loss": 1.8271,
      "step": 54620
    },
    {
      "epoch": 27.787385554425228,
      "grad_norm": 38.53630447387695,
      "learning_rate": 2.2212614445574773e-05,
      "loss": 1.8699,
      "step": 54630
    },
    {
      "epoch": 27.792472024415055,
      "grad_norm": 29.930923461914062,
      "learning_rate": 2.2207527975584946e-05,
      "loss": 1.795,
      "step": 54640
    },
    {
      "epoch": 27.797558494404882,
      "grad_norm": 29.50096321105957,
      "learning_rate": 2.2202441505595116e-05,
      "loss": 1.8661,
      "step": 54650
    },
    {
      "epoch": 27.80264496439471,
      "grad_norm": 35.752891540527344,
      "learning_rate": 2.219735503560529e-05,
      "loss": 1.8147,
      "step": 54660
    },
    {
      "epoch": 27.807731434384536,
      "grad_norm": 33.54902267456055,
      "learning_rate": 2.2192268565615466e-05,
      "loss": 1.8719,
      "step": 54670
    },
    {
      "epoch": 27.812817904374363,
      "grad_norm": 45.76530075073242,
      "learning_rate": 2.2187182095625636e-05,
      "loss": 1.8213,
      "step": 54680
    },
    {
      "epoch": 27.81790437436419,
      "grad_norm": 36.316253662109375,
      "learning_rate": 2.218209562563581e-05,
      "loss": 1.8593,
      "step": 54690
    },
    {
      "epoch": 27.822990844354017,
      "grad_norm": 34.097877502441406,
      "learning_rate": 2.2177009155645983e-05,
      "loss": 1.7996,
      "step": 54700
    },
    {
      "epoch": 27.828077314343844,
      "grad_norm": 39.65650939941406,
      "learning_rate": 2.2171922685656156e-05,
      "loss": 1.9171,
      "step": 54710
    },
    {
      "epoch": 27.83316378433367,
      "grad_norm": 35.729000091552734,
      "learning_rate": 2.216683621566633e-05,
      "loss": 1.8226,
      "step": 54720
    },
    {
      "epoch": 27.838250254323498,
      "grad_norm": 30.50887107849121,
      "learning_rate": 2.2161749745676503e-05,
      "loss": 1.8206,
      "step": 54730
    },
    {
      "epoch": 27.843336724313325,
      "grad_norm": 39.365875244140625,
      "learning_rate": 2.2156663275686673e-05,
      "loss": 1.8645,
      "step": 54740
    },
    {
      "epoch": 27.848423194303155,
      "grad_norm": 47.04718780517578,
      "learning_rate": 2.2151576805696846e-05,
      "loss": 1.9204,
      "step": 54750
    },
    {
      "epoch": 27.853509664292982,
      "grad_norm": 28.262672424316406,
      "learning_rate": 2.2146490335707022e-05,
      "loss": 1.8458,
      "step": 54760
    },
    {
      "epoch": 27.85859613428281,
      "grad_norm": 36.28224182128906,
      "learning_rate": 2.2141403865717196e-05,
      "loss": 1.8702,
      "step": 54770
    },
    {
      "epoch": 27.863682604272636,
      "grad_norm": 47.590511322021484,
      "learning_rate": 2.2136317395727366e-05,
      "loss": 1.9737,
      "step": 54780
    },
    {
      "epoch": 27.868769074262463,
      "grad_norm": 40.959293365478516,
      "learning_rate": 2.213123092573754e-05,
      "loss": 1.8784,
      "step": 54790
    },
    {
      "epoch": 27.87385554425229,
      "grad_norm": 30.98870277404785,
      "learning_rate": 2.2126144455747712e-05,
      "loss": 1.8743,
      "step": 54800
    },
    {
      "epoch": 27.878942014242117,
      "grad_norm": 31.521177291870117,
      "learning_rate": 2.2121057985757885e-05,
      "loss": 1.843,
      "step": 54810
    },
    {
      "epoch": 27.884028484231944,
      "grad_norm": 36.3825569152832,
      "learning_rate": 2.211597151576806e-05,
      "loss": 1.8736,
      "step": 54820
    },
    {
      "epoch": 27.88911495422177,
      "grad_norm": 31.384065628051758,
      "learning_rate": 2.2110885045778232e-05,
      "loss": 1.9963,
      "step": 54830
    },
    {
      "epoch": 27.894201424211598,
      "grad_norm": 37.32671356201172,
      "learning_rate": 2.2105798575788402e-05,
      "loss": 1.8883,
      "step": 54840
    },
    {
      "epoch": 27.899287894201425,
      "grad_norm": 44.20631790161133,
      "learning_rate": 2.2100712105798575e-05,
      "loss": 1.8971,
      "step": 54850
    },
    {
      "epoch": 27.904374364191252,
      "grad_norm": 35.22734451293945,
      "learning_rate": 2.2095625635808752e-05,
      "loss": 1.8344,
      "step": 54860
    },
    {
      "epoch": 27.90946083418108,
      "grad_norm": 39.979496002197266,
      "learning_rate": 2.2090539165818922e-05,
      "loss": 1.894,
      "step": 54870
    },
    {
      "epoch": 27.914547304170906,
      "grad_norm": 39.42411804199219,
      "learning_rate": 2.2085452695829095e-05,
      "loss": 1.8499,
      "step": 54880
    },
    {
      "epoch": 27.919633774160733,
      "grad_norm": 36.9084358215332,
      "learning_rate": 2.208036622583927e-05,
      "loss": 1.8491,
      "step": 54890
    },
    {
      "epoch": 27.92472024415056,
      "grad_norm": 32.188636779785156,
      "learning_rate": 2.207527975584944e-05,
      "loss": 1.9076,
      "step": 54900
    },
    {
      "epoch": 27.929806714140387,
      "grad_norm": 37.95615005493164,
      "learning_rate": 2.2070193285859615e-05,
      "loss": 1.8415,
      "step": 54910
    },
    {
      "epoch": 27.934893184130214,
      "grad_norm": 35.828041076660156,
      "learning_rate": 2.2065106815869788e-05,
      "loss": 1.8114,
      "step": 54920
    },
    {
      "epoch": 27.93997965412004,
      "grad_norm": 47.91267776489258,
      "learning_rate": 2.206002034587996e-05,
      "loss": 1.8563,
      "step": 54930
    },
    {
      "epoch": 27.945066124109868,
      "grad_norm": 29.76188087463379,
      "learning_rate": 2.205493387589013e-05,
      "loss": 1.9853,
      "step": 54940
    },
    {
      "epoch": 27.950152594099695,
      "grad_norm": 42.89669418334961,
      "learning_rate": 2.2049847405900305e-05,
      "loss": 1.7687,
      "step": 54950
    },
    {
      "epoch": 27.955239064089522,
      "grad_norm": 41.17207717895508,
      "learning_rate": 2.204476093591048e-05,
      "loss": 1.76,
      "step": 54960
    },
    {
      "epoch": 27.96032553407935,
      "grad_norm": 36.5020751953125,
      "learning_rate": 2.203967446592065e-05,
      "loss": 1.8044,
      "step": 54970
    },
    {
      "epoch": 27.965412004069176,
      "grad_norm": 36.546085357666016,
      "learning_rate": 2.2034587995930825e-05,
      "loss": 1.9134,
      "step": 54980
    },
    {
      "epoch": 27.970498474059003,
      "grad_norm": 43.98332977294922,
      "learning_rate": 2.2029501525940998e-05,
      "loss": 1.823,
      "step": 54990
    },
    {
      "epoch": 27.97558494404883,
      "grad_norm": 44.09556198120117,
      "learning_rate": 2.202441505595117e-05,
      "loss": 1.8036,
      "step": 55000
    },
    {
      "epoch": 27.980671414038657,
      "grad_norm": 34.783206939697266,
      "learning_rate": 2.2019328585961344e-05,
      "loss": 1.8512,
      "step": 55010
    },
    {
      "epoch": 27.985757884028484,
      "grad_norm": 45.4379997253418,
      "learning_rate": 2.2014242115971518e-05,
      "loss": 1.8885,
      "step": 55020
    },
    {
      "epoch": 27.99084435401831,
      "grad_norm": 43.30485534667969,
      "learning_rate": 2.200915564598169e-05,
      "loss": 1.9173,
      "step": 55030
    },
    {
      "epoch": 27.995930824008138,
      "grad_norm": 37.91819381713867,
      "learning_rate": 2.200406917599186e-05,
      "loss": 1.8522,
      "step": 55040
    },
    {
      "epoch": 28.0,
      "eval_loss": 4.681943893432617,
      "eval_runtime": 2.8181,
      "eval_samples_per_second": 984.695,
      "eval_steps_per_second": 123.131,
      "step": 55048
    },
    {
      "epoch": 28.001017293997965,
      "grad_norm": 45.51271057128906,
      "learning_rate": 2.1998982706002037e-05,
      "loss": 1.8268,
      "step": 55050
    },
    {
      "epoch": 28.00610376398779,
      "grad_norm": 45.56169128417969,
      "learning_rate": 2.199389623601221e-05,
      "loss": 1.8148,
      "step": 55060
    },
    {
      "epoch": 28.01119023397762,
      "grad_norm": 27.176454544067383,
      "learning_rate": 2.198880976602238e-05,
      "loss": 1.8854,
      "step": 55070
    },
    {
      "epoch": 28.016276703967446,
      "grad_norm": 40.44842529296875,
      "learning_rate": 2.1983723296032554e-05,
      "loss": 1.7979,
      "step": 55080
    },
    {
      "epoch": 28.021363173957273,
      "grad_norm": 50.348690032958984,
      "learning_rate": 2.1978636826042727e-05,
      "loss": 1.8625,
      "step": 55090
    },
    {
      "epoch": 28.0264496439471,
      "grad_norm": 31.60828971862793,
      "learning_rate": 2.19735503560529e-05,
      "loss": 1.9277,
      "step": 55100
    },
    {
      "epoch": 28.031536113936927,
      "grad_norm": 36.167396545410156,
      "learning_rate": 2.1968463886063074e-05,
      "loss": 1.8018,
      "step": 55110
    },
    {
      "epoch": 28.036622583926754,
      "grad_norm": 39.00973892211914,
      "learning_rate": 2.1963377416073247e-05,
      "loss": 1.8188,
      "step": 55120
    },
    {
      "epoch": 28.04170905391658,
      "grad_norm": 42.97662353515625,
      "learning_rate": 2.1958290946083417e-05,
      "loss": 1.8844,
      "step": 55130
    },
    {
      "epoch": 28.046795523906408,
      "grad_norm": 34.1744384765625,
      "learning_rate": 2.195320447609359e-05,
      "loss": 1.7727,
      "step": 55140
    },
    {
      "epoch": 28.051881993896234,
      "grad_norm": 30.680946350097656,
      "learning_rate": 2.1948118006103767e-05,
      "loss": 1.9023,
      "step": 55150
    },
    {
      "epoch": 28.05696846388606,
      "grad_norm": 30.9516658782959,
      "learning_rate": 2.194303153611394e-05,
      "loss": 1.8518,
      "step": 55160
    },
    {
      "epoch": 28.06205493387589,
      "grad_norm": 34.24361801147461,
      "learning_rate": 2.193794506612411e-05,
      "loss": 1.8729,
      "step": 55170
    },
    {
      "epoch": 28.067141403865715,
      "grad_norm": 32.087581634521484,
      "learning_rate": 2.1932858596134283e-05,
      "loss": 1.7908,
      "step": 55180
    },
    {
      "epoch": 28.072227873855546,
      "grad_norm": 44.4073600769043,
      "learning_rate": 2.1927772126144457e-05,
      "loss": 1.9,
      "step": 55190
    },
    {
      "epoch": 28.077314343845373,
      "grad_norm": 32.11861038208008,
      "learning_rate": 2.192268565615463e-05,
      "loss": 1.7576,
      "step": 55200
    },
    {
      "epoch": 28.0824008138352,
      "grad_norm": 31.664026260375977,
      "learning_rate": 2.1917599186164803e-05,
      "loss": 1.9029,
      "step": 55210
    },
    {
      "epoch": 28.087487283825027,
      "grad_norm": 46.47614288330078,
      "learning_rate": 2.1912512716174976e-05,
      "loss": 1.9173,
      "step": 55220
    },
    {
      "epoch": 28.092573753814854,
      "grad_norm": 41.08613967895508,
      "learning_rate": 2.1907426246185146e-05,
      "loss": 1.8086,
      "step": 55230
    },
    {
      "epoch": 28.09766022380468,
      "grad_norm": 42.23514938354492,
      "learning_rate": 2.1902339776195323e-05,
      "loss": 1.7692,
      "step": 55240
    },
    {
      "epoch": 28.102746693794508,
      "grad_norm": 26.983135223388672,
      "learning_rate": 2.1897253306205496e-05,
      "loss": 1.8641,
      "step": 55250
    },
    {
      "epoch": 28.107833163784335,
      "grad_norm": 32.10609436035156,
      "learning_rate": 2.1892166836215666e-05,
      "loss": 1.8165,
      "step": 55260
    },
    {
      "epoch": 28.112919633774162,
      "grad_norm": 35.65093994140625,
      "learning_rate": 2.188708036622584e-05,
      "loss": 1.8493,
      "step": 55270
    },
    {
      "epoch": 28.11800610376399,
      "grad_norm": 51.58157730102539,
      "learning_rate": 2.1881993896236013e-05,
      "loss": 1.8436,
      "step": 55280
    },
    {
      "epoch": 28.123092573753816,
      "grad_norm": 49.69462966918945,
      "learning_rate": 2.1876907426246186e-05,
      "loss": 1.7848,
      "step": 55290
    },
    {
      "epoch": 28.128179043743643,
      "grad_norm": 37.4539909362793,
      "learning_rate": 2.187182095625636e-05,
      "loss": 1.798,
      "step": 55300
    },
    {
      "epoch": 28.13326551373347,
      "grad_norm": 38.63105773925781,
      "learning_rate": 2.1866734486266533e-05,
      "loss": 1.7658,
      "step": 55310
    },
    {
      "epoch": 28.138351983723297,
      "grad_norm": 34.9138069152832,
      "learning_rate": 2.1861648016276706e-05,
      "loss": 1.8338,
      "step": 55320
    },
    {
      "epoch": 28.143438453713124,
      "grad_norm": 38.578739166259766,
      "learning_rate": 2.1856561546286876e-05,
      "loss": 1.9252,
      "step": 55330
    },
    {
      "epoch": 28.14852492370295,
      "grad_norm": 35.21156692504883,
      "learning_rate": 2.1851475076297052e-05,
      "loss": 1.8307,
      "step": 55340
    },
    {
      "epoch": 28.153611393692778,
      "grad_norm": 30.83718490600586,
      "learning_rate": 2.1846388606307226e-05,
      "loss": 1.8704,
      "step": 55350
    },
    {
      "epoch": 28.158697863682605,
      "grad_norm": 38.604698181152344,
      "learning_rate": 2.1841302136317396e-05,
      "loss": 1.8638,
      "step": 55360
    },
    {
      "epoch": 28.16378433367243,
      "grad_norm": 45.5654296875,
      "learning_rate": 2.183621566632757e-05,
      "loss": 1.7717,
      "step": 55370
    },
    {
      "epoch": 28.16887080366226,
      "grad_norm": 30.577539443969727,
      "learning_rate": 2.1831129196337742e-05,
      "loss": 1.8486,
      "step": 55380
    },
    {
      "epoch": 28.173957273652086,
      "grad_norm": 29.416662216186523,
      "learning_rate": 2.1826042726347915e-05,
      "loss": 1.8669,
      "step": 55390
    },
    {
      "epoch": 28.179043743641913,
      "grad_norm": 40.56189727783203,
      "learning_rate": 2.182095625635809e-05,
      "loss": 1.8955,
      "step": 55400
    },
    {
      "epoch": 28.18413021363174,
      "grad_norm": 32.731285095214844,
      "learning_rate": 2.1815869786368262e-05,
      "loss": 1.8348,
      "step": 55410
    },
    {
      "epoch": 28.189216683621567,
      "grad_norm": 39.639892578125,
      "learning_rate": 2.1810783316378432e-05,
      "loss": 1.8651,
      "step": 55420
    },
    {
      "epoch": 28.194303153611393,
      "grad_norm": 35.615684509277344,
      "learning_rate": 2.1805696846388605e-05,
      "loss": 1.8388,
      "step": 55430
    },
    {
      "epoch": 28.19938962360122,
      "grad_norm": 38.820655822753906,
      "learning_rate": 2.1800610376398782e-05,
      "loss": 1.8722,
      "step": 55440
    },
    {
      "epoch": 28.204476093591047,
      "grad_norm": 39.157291412353516,
      "learning_rate": 2.1795523906408955e-05,
      "loss": 1.7916,
      "step": 55450
    },
    {
      "epoch": 28.209562563580874,
      "grad_norm": 30.101490020751953,
      "learning_rate": 2.1790437436419125e-05,
      "loss": 1.7661,
      "step": 55460
    },
    {
      "epoch": 28.2146490335707,
      "grad_norm": 45.217018127441406,
      "learning_rate": 2.17853509664293e-05,
      "loss": 1.8009,
      "step": 55470
    },
    {
      "epoch": 28.21973550356053,
      "grad_norm": 41.72254943847656,
      "learning_rate": 2.178026449643947e-05,
      "loss": 1.8626,
      "step": 55480
    },
    {
      "epoch": 28.224821973550355,
      "grad_norm": 41.99797058105469,
      "learning_rate": 2.1775178026449645e-05,
      "loss": 1.8301,
      "step": 55490
    },
    {
      "epoch": 28.229908443540182,
      "grad_norm": 42.707515716552734,
      "learning_rate": 2.1770091556459818e-05,
      "loss": 1.8255,
      "step": 55500
    },
    {
      "epoch": 28.23499491353001,
      "grad_norm": 35.292423248291016,
      "learning_rate": 2.176500508646999e-05,
      "loss": 1.8046,
      "step": 55510
    },
    {
      "epoch": 28.240081383519836,
      "grad_norm": 35.509029388427734,
      "learning_rate": 2.175991861648016e-05,
      "loss": 1.9074,
      "step": 55520
    },
    {
      "epoch": 28.245167853509663,
      "grad_norm": 39.97136688232422,
      "learning_rate": 2.1754832146490338e-05,
      "loss": 1.9431,
      "step": 55530
    },
    {
      "epoch": 28.25025432349949,
      "grad_norm": 36.0041389465332,
      "learning_rate": 2.174974567650051e-05,
      "loss": 1.9261,
      "step": 55540
    },
    {
      "epoch": 28.255340793489317,
      "grad_norm": 40.674530029296875,
      "learning_rate": 2.174465920651068e-05,
      "loss": 1.861,
      "step": 55550
    },
    {
      "epoch": 28.260427263479144,
      "grad_norm": 33.978031158447266,
      "learning_rate": 2.1739572736520855e-05,
      "loss": 1.8059,
      "step": 55560
    },
    {
      "epoch": 28.26551373346897,
      "grad_norm": 30.768774032592773,
      "learning_rate": 2.1734486266531028e-05,
      "loss": 1.8654,
      "step": 55570
    },
    {
      "epoch": 28.270600203458798,
      "grad_norm": 34.19971466064453,
      "learning_rate": 2.17293997965412e-05,
      "loss": 1.7597,
      "step": 55580
    },
    {
      "epoch": 28.275686673448625,
      "grad_norm": 36.21770477294922,
      "learning_rate": 2.1724313326551374e-05,
      "loss": 1.9183,
      "step": 55590
    },
    {
      "epoch": 28.280773143438452,
      "grad_norm": 31.955101013183594,
      "learning_rate": 2.1719226856561548e-05,
      "loss": 1.8947,
      "step": 55600
    },
    {
      "epoch": 28.28585961342828,
      "grad_norm": 48.470523834228516,
      "learning_rate": 2.171414038657172e-05,
      "loss": 1.7036,
      "step": 55610
    },
    {
      "epoch": 28.290946083418106,
      "grad_norm": 35.971412658691406,
      "learning_rate": 2.170905391658189e-05,
      "loss": 1.7807,
      "step": 55620
    },
    {
      "epoch": 28.296032553407933,
      "grad_norm": 45.28403854370117,
      "learning_rate": 2.1703967446592067e-05,
      "loss": 1.8792,
      "step": 55630
    },
    {
      "epoch": 28.301119023397764,
      "grad_norm": 35.370601654052734,
      "learning_rate": 2.169888097660224e-05,
      "loss": 1.7361,
      "step": 55640
    },
    {
      "epoch": 28.30620549338759,
      "grad_norm": 41.683929443359375,
      "learning_rate": 2.169379450661241e-05,
      "loss": 1.8796,
      "step": 55650
    },
    {
      "epoch": 28.311291963377418,
      "grad_norm": 32.45576477050781,
      "learning_rate": 2.1688708036622584e-05,
      "loss": 1.7532,
      "step": 55660
    },
    {
      "epoch": 28.316378433367245,
      "grad_norm": 32.526763916015625,
      "learning_rate": 2.1683621566632757e-05,
      "loss": 1.8074,
      "step": 55670
    },
    {
      "epoch": 28.32146490335707,
      "grad_norm": 37.78499984741211,
      "learning_rate": 2.167853509664293e-05,
      "loss": 1.853,
      "step": 55680
    },
    {
      "epoch": 28.3265513733469,
      "grad_norm": 35.99647903442383,
      "learning_rate": 2.1673448626653104e-05,
      "loss": 1.8573,
      "step": 55690
    },
    {
      "epoch": 28.331637843336726,
      "grad_norm": 31.976850509643555,
      "learning_rate": 2.1668362156663277e-05,
      "loss": 1.8758,
      "step": 55700
    },
    {
      "epoch": 28.336724313326553,
      "grad_norm": 44.36299133300781,
      "learning_rate": 2.166327568667345e-05,
      "loss": 1.7573,
      "step": 55710
    },
    {
      "epoch": 28.34181078331638,
      "grad_norm": 27.997392654418945,
      "learning_rate": 2.1658189216683624e-05,
      "loss": 1.8933,
      "step": 55720
    },
    {
      "epoch": 28.346897253306206,
      "grad_norm": 32.68901824951172,
      "learning_rate": 2.1653102746693797e-05,
      "loss": 1.8768,
      "step": 55730
    },
    {
      "epoch": 28.351983723296033,
      "grad_norm": 37.754981994628906,
      "learning_rate": 2.164801627670397e-05,
      "loss": 1.8177,
      "step": 55740
    },
    {
      "epoch": 28.35707019328586,
      "grad_norm": 37.11946105957031,
      "learning_rate": 2.164292980671414e-05,
      "loss": 1.8772,
      "step": 55750
    },
    {
      "epoch": 28.362156663275687,
      "grad_norm": 29.886844635009766,
      "learning_rate": 2.1637843336724313e-05,
      "loss": 1.7543,
      "step": 55760
    },
    {
      "epoch": 28.367243133265514,
      "grad_norm": 39.984771728515625,
      "learning_rate": 2.1632756866734487e-05,
      "loss": 1.836,
      "step": 55770
    },
    {
      "epoch": 28.37232960325534,
      "grad_norm": 38.64105224609375,
      "learning_rate": 2.162767039674466e-05,
      "loss": 1.8526,
      "step": 55780
    },
    {
      "epoch": 28.37741607324517,
      "grad_norm": 35.65595245361328,
      "learning_rate": 2.1622583926754833e-05,
      "loss": 1.7982,
      "step": 55790
    },
    {
      "epoch": 28.382502543234995,
      "grad_norm": 32.626930236816406,
      "learning_rate": 2.1617497456765006e-05,
      "loss": 1.8354,
      "step": 55800
    },
    {
      "epoch": 28.387589013224822,
      "grad_norm": 41.16682434082031,
      "learning_rate": 2.1612410986775176e-05,
      "loss": 1.9428,
      "step": 55810
    },
    {
      "epoch": 28.39267548321465,
      "grad_norm": 38.996551513671875,
      "learning_rate": 2.1607324516785353e-05,
      "loss": 1.7482,
      "step": 55820
    },
    {
      "epoch": 28.397761953204476,
      "grad_norm": 43.76711654663086,
      "learning_rate": 2.1602238046795526e-05,
      "loss": 1.9021,
      "step": 55830
    },
    {
      "epoch": 28.402848423194303,
      "grad_norm": 34.45464324951172,
      "learning_rate": 2.15971515768057e-05,
      "loss": 1.8333,
      "step": 55840
    },
    {
      "epoch": 28.40793489318413,
      "grad_norm": 36.002586364746094,
      "learning_rate": 2.159206510681587e-05,
      "loss": 1.7969,
      "step": 55850
    },
    {
      "epoch": 28.413021363173957,
      "grad_norm": 36.85551452636719,
      "learning_rate": 2.1586978636826043e-05,
      "loss": 1.9466,
      "step": 55860
    },
    {
      "epoch": 28.418107833163784,
      "grad_norm": 33.2947883605957,
      "learning_rate": 2.158189216683622e-05,
      "loss": 1.7959,
      "step": 55870
    },
    {
      "epoch": 28.42319430315361,
      "grad_norm": 46.864933013916016,
      "learning_rate": 2.157680569684639e-05,
      "loss": 1.8109,
      "step": 55880
    },
    {
      "epoch": 28.428280773143438,
      "grad_norm": 40.341732025146484,
      "learning_rate": 2.1571719226856563e-05,
      "loss": 1.7891,
      "step": 55890
    },
    {
      "epoch": 28.433367243133265,
      "grad_norm": 37.72450637817383,
      "learning_rate": 2.1566632756866736e-05,
      "loss": 1.7315,
      "step": 55900
    },
    {
      "epoch": 28.438453713123092,
      "grad_norm": 35.793792724609375,
      "learning_rate": 2.1561546286876906e-05,
      "loss": 1.7241,
      "step": 55910
    },
    {
      "epoch": 28.44354018311292,
      "grad_norm": 39.632415771484375,
      "learning_rate": 2.1556459816887082e-05,
      "loss": 1.9458,
      "step": 55920
    },
    {
      "epoch": 28.448626653102746,
      "grad_norm": 37.191734313964844,
      "learning_rate": 2.1551373346897256e-05,
      "loss": 1.8374,
      "step": 55930
    },
    {
      "epoch": 28.453713123092573,
      "grad_norm": 37.23485565185547,
      "learning_rate": 2.1546286876907426e-05,
      "loss": 1.9095,
      "step": 55940
    },
    {
      "epoch": 28.4587995930824,
      "grad_norm": 40.139488220214844,
      "learning_rate": 2.15412004069176e-05,
      "loss": 1.7966,
      "step": 55950
    },
    {
      "epoch": 28.463886063072227,
      "grad_norm": 37.45564651489258,
      "learning_rate": 2.1536113936927772e-05,
      "loss": 1.8861,
      "step": 55960
    },
    {
      "epoch": 28.468972533062054,
      "grad_norm": 32.95078659057617,
      "learning_rate": 2.153102746693795e-05,
      "loss": 1.7829,
      "step": 55970
    },
    {
      "epoch": 28.47405900305188,
      "grad_norm": 38.59175109863281,
      "learning_rate": 2.152594099694812e-05,
      "loss": 1.8399,
      "step": 55980
    },
    {
      "epoch": 28.479145473041708,
      "grad_norm": 34.06034851074219,
      "learning_rate": 2.1520854526958292e-05,
      "loss": 1.8583,
      "step": 55990
    },
    {
      "epoch": 28.484231943031535,
      "grad_norm": 39.063682556152344,
      "learning_rate": 2.1515768056968465e-05,
      "loss": 1.8297,
      "step": 56000
    },
    {
      "epoch": 28.489318413021362,
      "grad_norm": 34.017051696777344,
      "learning_rate": 2.151068158697864e-05,
      "loss": 1.7575,
      "step": 56010
    },
    {
      "epoch": 28.49440488301119,
      "grad_norm": 32.843318939208984,
      "learning_rate": 2.1505595116988812e-05,
      "loss": 1.8599,
      "step": 56020
    },
    {
      "epoch": 28.499491353001016,
      "grad_norm": 51.927734375,
      "learning_rate": 2.1500508646998985e-05,
      "loss": 1.8469,
      "step": 56030
    },
    {
      "epoch": 28.504577822990843,
      "grad_norm": 44.39371871948242,
      "learning_rate": 2.1495422177009155e-05,
      "loss": 1.8289,
      "step": 56040
    },
    {
      "epoch": 28.50966429298067,
      "grad_norm": 42.667144775390625,
      "learning_rate": 2.149033570701933e-05,
      "loss": 1.8128,
      "step": 56050
    },
    {
      "epoch": 28.514750762970497,
      "grad_norm": 36.392791748046875,
      "learning_rate": 2.14852492370295e-05,
      "loss": 1.8019,
      "step": 56060
    },
    {
      "epoch": 28.519837232960327,
      "grad_norm": 37.896785736083984,
      "learning_rate": 2.1480162767039675e-05,
      "loss": 1.9582,
      "step": 56070
    },
    {
      "epoch": 28.524923702950154,
      "grad_norm": 31.162752151489258,
      "learning_rate": 2.1475076297049848e-05,
      "loss": 1.7803,
      "step": 56080
    },
    {
      "epoch": 28.53001017293998,
      "grad_norm": 33.089691162109375,
      "learning_rate": 2.146998982706002e-05,
      "loss": 1.7169,
      "step": 56090
    },
    {
      "epoch": 28.53509664292981,
      "grad_norm": 38.17915344238281,
      "learning_rate": 2.146490335707019e-05,
      "loss": 1.8897,
      "step": 56100
    },
    {
      "epoch": 28.540183112919635,
      "grad_norm": 43.160308837890625,
      "learning_rate": 2.1459816887080368e-05,
      "loss": 1.8871,
      "step": 56110
    },
    {
      "epoch": 28.545269582909462,
      "grad_norm": 34.17209243774414,
      "learning_rate": 2.145473041709054e-05,
      "loss": 1.8181,
      "step": 56120
    },
    {
      "epoch": 28.55035605289929,
      "grad_norm": 31.559635162353516,
      "learning_rate": 2.1449643947100715e-05,
      "loss": 1.7972,
      "step": 56130
    },
    {
      "epoch": 28.555442522889116,
      "grad_norm": 34.245765686035156,
      "learning_rate": 2.1444557477110885e-05,
      "loss": 1.7603,
      "step": 56140
    },
    {
      "epoch": 28.560528992878943,
      "grad_norm": 37.45424270629883,
      "learning_rate": 2.1439471007121058e-05,
      "loss": 1.8342,
      "step": 56150
    },
    {
      "epoch": 28.56561546286877,
      "grad_norm": 31.851818084716797,
      "learning_rate": 2.1434384537131234e-05,
      "loss": 1.824,
      "step": 56160
    },
    {
      "epoch": 28.570701932858597,
      "grad_norm": 36.819557189941406,
      "learning_rate": 2.1429298067141404e-05,
      "loss": 1.8335,
      "step": 56170
    },
    {
      "epoch": 28.575788402848424,
      "grad_norm": 42.878196716308594,
      "learning_rate": 2.1424211597151578e-05,
      "loss": 1.818,
      "step": 56180
    },
    {
      "epoch": 28.58087487283825,
      "grad_norm": 38.95814514160156,
      "learning_rate": 2.141912512716175e-05,
      "loss": 1.8318,
      "step": 56190
    },
    {
      "epoch": 28.585961342828078,
      "grad_norm": 43.92539596557617,
      "learning_rate": 2.1414038657171924e-05,
      "loss": 1.7844,
      "step": 56200
    },
    {
      "epoch": 28.591047812817905,
      "grad_norm": 36.55918502807617,
      "learning_rate": 2.1408952187182097e-05,
      "loss": 1.8438,
      "step": 56210
    },
    {
      "epoch": 28.596134282807732,
      "grad_norm": 42.26865768432617,
      "learning_rate": 2.140386571719227e-05,
      "loss": 1.8093,
      "step": 56220
    },
    {
      "epoch": 28.60122075279756,
      "grad_norm": 45.56405258178711,
      "learning_rate": 2.139877924720244e-05,
      "loss": 1.7704,
      "step": 56230
    },
    {
      "epoch": 28.606307222787386,
      "grad_norm": 39.701385498046875,
      "learning_rate": 2.1393692777212614e-05,
      "loss": 1.8331,
      "step": 56240
    },
    {
      "epoch": 28.611393692777213,
      "grad_norm": 36.830963134765625,
      "learning_rate": 2.1388606307222787e-05,
      "loss": 1.8032,
      "step": 56250
    },
    {
      "epoch": 28.61648016276704,
      "grad_norm": 32.01597595214844,
      "learning_rate": 2.1383519837232964e-05,
      "loss": 1.7613,
      "step": 56260
    },
    {
      "epoch": 28.621566632756867,
      "grad_norm": 39.62264633178711,
      "learning_rate": 2.1378433367243134e-05,
      "loss": 1.8674,
      "step": 56270
    },
    {
      "epoch": 28.626653102746694,
      "grad_norm": 33.61018371582031,
      "learning_rate": 2.1373346897253307e-05,
      "loss": 1.8174,
      "step": 56280
    },
    {
      "epoch": 28.63173957273652,
      "grad_norm": 56.11789321899414,
      "learning_rate": 2.136826042726348e-05,
      "loss": 1.8439,
      "step": 56290
    },
    {
      "epoch": 28.636826042726348,
      "grad_norm": 37.574039459228516,
      "learning_rate": 2.1363173957273654e-05,
      "loss": 1.8251,
      "step": 56300
    },
    {
      "epoch": 28.641912512716175,
      "grad_norm": 42.093414306640625,
      "learning_rate": 2.1358087487283827e-05,
      "loss": 1.9034,
      "step": 56310
    },
    {
      "epoch": 28.646998982706002,
      "grad_norm": 36.206905364990234,
      "learning_rate": 2.1353001017294e-05,
      "loss": 1.8556,
      "step": 56320
    },
    {
      "epoch": 28.65208545269583,
      "grad_norm": 41.49635696411133,
      "learning_rate": 2.134791454730417e-05,
      "loss": 1.7446,
      "step": 56330
    },
    {
      "epoch": 28.657171922685656,
      "grad_norm": 32.193233489990234,
      "learning_rate": 2.1342828077314343e-05,
      "loss": 1.8933,
      "step": 56340
    },
    {
      "epoch": 28.662258392675483,
      "grad_norm": 49.96056365966797,
      "learning_rate": 2.133774160732452e-05,
      "loss": 1.7526,
      "step": 56350
    },
    {
      "epoch": 28.66734486266531,
      "grad_norm": 35.16084671020508,
      "learning_rate": 2.133265513733469e-05,
      "loss": 1.8315,
      "step": 56360
    },
    {
      "epoch": 28.672431332655137,
      "grad_norm": 39.57767868041992,
      "learning_rate": 2.1327568667344863e-05,
      "loss": 1.8046,
      "step": 56370
    },
    {
      "epoch": 28.677517802644964,
      "grad_norm": 33.43346405029297,
      "learning_rate": 2.1322482197355036e-05,
      "loss": 1.8325,
      "step": 56380
    },
    {
      "epoch": 28.68260427263479,
      "grad_norm": 49.867584228515625,
      "learning_rate": 2.131739572736521e-05,
      "loss": 1.7886,
      "step": 56390
    },
    {
      "epoch": 28.687690742624618,
      "grad_norm": 31.841991424560547,
      "learning_rate": 2.1312309257375383e-05,
      "loss": 1.8057,
      "step": 56400
    },
    {
      "epoch": 28.692777212614445,
      "grad_norm": 35.60723114013672,
      "learning_rate": 2.1307222787385556e-05,
      "loss": 1.8279,
      "step": 56410
    },
    {
      "epoch": 28.69786368260427,
      "grad_norm": 39.28297805786133,
      "learning_rate": 2.130213631739573e-05,
      "loss": 1.7697,
      "step": 56420
    },
    {
      "epoch": 28.7029501525941,
      "grad_norm": 34.59308624267578,
      "learning_rate": 2.12970498474059e-05,
      "loss": 1.8353,
      "step": 56430
    },
    {
      "epoch": 28.708036622583926,
      "grad_norm": 41.92103576660156,
      "learning_rate": 2.1291963377416073e-05,
      "loss": 1.8461,
      "step": 56440
    },
    {
      "epoch": 28.713123092573753,
      "grad_norm": 38.43385696411133,
      "learning_rate": 2.128687690742625e-05,
      "loss": 1.832,
      "step": 56450
    },
    {
      "epoch": 28.71820956256358,
      "grad_norm": 33.53049087524414,
      "learning_rate": 2.128179043743642e-05,
      "loss": 1.7471,
      "step": 56460
    },
    {
      "epoch": 28.723296032553407,
      "grad_norm": 36.47651290893555,
      "learning_rate": 2.1276703967446593e-05,
      "loss": 1.7814,
      "step": 56470
    },
    {
      "epoch": 28.728382502543234,
      "grad_norm": 38.35385513305664,
      "learning_rate": 2.1271617497456766e-05,
      "loss": 1.8683,
      "step": 56480
    },
    {
      "epoch": 28.73346897253306,
      "grad_norm": 43.58327102661133,
      "learning_rate": 2.126653102746694e-05,
      "loss": 1.8951,
      "step": 56490
    },
    {
      "epoch": 28.738555442522888,
      "grad_norm": 43.17529296875,
      "learning_rate": 2.1261444557477112e-05,
      "loss": 1.8978,
      "step": 56500
    },
    {
      "epoch": 28.743641912512714,
      "grad_norm": 36.76128005981445,
      "learning_rate": 2.1256358087487286e-05,
      "loss": 1.8102,
      "step": 56510
    },
    {
      "epoch": 28.74872838250254,
      "grad_norm": 38.00861740112305,
      "learning_rate": 2.125127161749746e-05,
      "loss": 1.9148,
      "step": 56520
    },
    {
      "epoch": 28.753814852492372,
      "grad_norm": 38.91468811035156,
      "learning_rate": 2.124618514750763e-05,
      "loss": 1.8691,
      "step": 56530
    },
    {
      "epoch": 28.7589013224822,
      "grad_norm": 39.836875915527344,
      "learning_rate": 2.1241098677517802e-05,
      "loss": 1.7571,
      "step": 56540
    },
    {
      "epoch": 28.763987792472026,
      "grad_norm": 42.78831100463867,
      "learning_rate": 2.123601220752798e-05,
      "loss": 1.8784,
      "step": 56550
    },
    {
      "epoch": 28.769074262461853,
      "grad_norm": 31.340715408325195,
      "learning_rate": 2.123092573753815e-05,
      "loss": 1.8493,
      "step": 56560
    },
    {
      "epoch": 28.77416073245168,
      "grad_norm": 32.98045349121094,
      "learning_rate": 2.1225839267548322e-05,
      "loss": 1.8416,
      "step": 56570
    },
    {
      "epoch": 28.779247202441507,
      "grad_norm": 40.42180252075195,
      "learning_rate": 2.1220752797558495e-05,
      "loss": 1.7923,
      "step": 56580
    },
    {
      "epoch": 28.784333672431334,
      "grad_norm": 28.493343353271484,
      "learning_rate": 2.121566632756867e-05,
      "loss": 1.8199,
      "step": 56590
    },
    {
      "epoch": 28.78942014242116,
      "grad_norm": 37.198368072509766,
      "learning_rate": 2.1210579857578842e-05,
      "loss": 1.8519,
      "step": 56600
    },
    {
      "epoch": 28.794506612410988,
      "grad_norm": 42.91858673095703,
      "learning_rate": 2.1205493387589015e-05,
      "loss": 1.8095,
      "step": 56610
    },
    {
      "epoch": 28.799593082400815,
      "grad_norm": 33.35054016113281,
      "learning_rate": 2.1200406917599185e-05,
      "loss": 1.8279,
      "step": 56620
    },
    {
      "epoch": 28.804679552390642,
      "grad_norm": 35.06664276123047,
      "learning_rate": 2.119532044760936e-05,
      "loss": 1.9234,
      "step": 56630
    },
    {
      "epoch": 28.80976602238047,
      "grad_norm": 34.8661994934082,
      "learning_rate": 2.1190233977619535e-05,
      "loss": 1.8253,
      "step": 56640
    },
    {
      "epoch": 28.814852492370296,
      "grad_norm": 30.07627296447754,
      "learning_rate": 2.118514750762971e-05,
      "loss": 1.7999,
      "step": 56650
    },
    {
      "epoch": 28.819938962360123,
      "grad_norm": 41.46201705932617,
      "learning_rate": 2.1180061037639878e-05,
      "loss": 1.8434,
      "step": 56660
    },
    {
      "epoch": 28.82502543234995,
      "grad_norm": 34.305381774902344,
      "learning_rate": 2.117497456765005e-05,
      "loss": 1.7599,
      "step": 56670
    },
    {
      "epoch": 28.830111902339777,
      "grad_norm": 33.508522033691406,
      "learning_rate": 2.1169888097660225e-05,
      "loss": 1.8431,
      "step": 56680
    },
    {
      "epoch": 28.835198372329604,
      "grad_norm": 33.64722442626953,
      "learning_rate": 2.1164801627670398e-05,
      "loss": 1.8583,
      "step": 56690
    },
    {
      "epoch": 28.84028484231943,
      "grad_norm": 30.14426040649414,
      "learning_rate": 2.115971515768057e-05,
      "loss": 1.7953,
      "step": 56700
    },
    {
      "epoch": 28.845371312309258,
      "grad_norm": 29.807926177978516,
      "learning_rate": 2.1154628687690745e-05,
      "loss": 1.8049,
      "step": 56710
    },
    {
      "epoch": 28.850457782299085,
      "grad_norm": 42.638423919677734,
      "learning_rate": 2.1149542217700915e-05,
      "loss": 1.8824,
      "step": 56720
    },
    {
      "epoch": 28.85554425228891,
      "grad_norm": 30.063817977905273,
      "learning_rate": 2.1144455747711088e-05,
      "loss": 1.786,
      "step": 56730
    },
    {
      "epoch": 28.86063072227874,
      "grad_norm": 39.76115036010742,
      "learning_rate": 2.1139369277721264e-05,
      "loss": 1.8273,
      "step": 56740
    },
    {
      "epoch": 28.865717192268566,
      "grad_norm": 34.95760726928711,
      "learning_rate": 2.1134282807731434e-05,
      "loss": 1.8945,
      "step": 56750
    },
    {
      "epoch": 28.870803662258393,
      "grad_norm": 49.958152770996094,
      "learning_rate": 2.1129196337741608e-05,
      "loss": 1.7996,
      "step": 56760
    },
    {
      "epoch": 28.87589013224822,
      "grad_norm": 40.89891815185547,
      "learning_rate": 2.112410986775178e-05,
      "loss": 1.8133,
      "step": 56770
    },
    {
      "epoch": 28.880976602238047,
      "grad_norm": 44.752891540527344,
      "learning_rate": 2.1119023397761954e-05,
      "loss": 1.8119,
      "step": 56780
    },
    {
      "epoch": 28.886063072227874,
      "grad_norm": 31.192155838012695,
      "learning_rate": 2.1113936927772127e-05,
      "loss": 1.8292,
      "step": 56790
    },
    {
      "epoch": 28.8911495422177,
      "grad_norm": 34.890560150146484,
      "learning_rate": 2.11088504577823e-05,
      "loss": 1.8467,
      "step": 56800
    },
    {
      "epoch": 28.896236012207527,
      "grad_norm": 37.13525390625,
      "learning_rate": 2.1103763987792474e-05,
      "loss": 1.8623,
      "step": 56810
    },
    {
      "epoch": 28.901322482197354,
      "grad_norm": 43.62290573120117,
      "learning_rate": 2.1098677517802644e-05,
      "loss": 1.7509,
      "step": 56820
    },
    {
      "epoch": 28.90640895218718,
      "grad_norm": 49.02640151977539,
      "learning_rate": 2.109359104781282e-05,
      "loss": 1.8716,
      "step": 56830
    },
    {
      "epoch": 28.91149542217701,
      "grad_norm": 37.087188720703125,
      "learning_rate": 2.1088504577822994e-05,
      "loss": 1.8314,
      "step": 56840
    },
    {
      "epoch": 28.916581892166835,
      "grad_norm": 37.79404067993164,
      "learning_rate": 2.1083418107833164e-05,
      "loss": 1.8364,
      "step": 56850
    },
    {
      "epoch": 28.921668362156662,
      "grad_norm": 30.941078186035156,
      "learning_rate": 2.1078331637843337e-05,
      "loss": 1.8927,
      "step": 56860
    },
    {
      "epoch": 28.92675483214649,
      "grad_norm": 43.138492584228516,
      "learning_rate": 2.107324516785351e-05,
      "loss": 1.9086,
      "step": 56870
    },
    {
      "epoch": 28.931841302136316,
      "grad_norm": 41.04108428955078,
      "learning_rate": 2.1068158697863684e-05,
      "loss": 1.7397,
      "step": 56880
    },
    {
      "epoch": 28.936927772126143,
      "grad_norm": 34.08507537841797,
      "learning_rate": 2.1063072227873857e-05,
      "loss": 1.7774,
      "step": 56890
    },
    {
      "epoch": 28.94201424211597,
      "grad_norm": 42.14320755004883,
      "learning_rate": 2.105798575788403e-05,
      "loss": 1.8843,
      "step": 56900
    },
    {
      "epoch": 28.947100712105797,
      "grad_norm": 46.78276443481445,
      "learning_rate": 2.10528992878942e-05,
      "loss": 1.8272,
      "step": 56910
    },
    {
      "epoch": 28.952187182095624,
      "grad_norm": 38.17200469970703,
      "learning_rate": 2.1047812817904373e-05,
      "loss": 1.7019,
      "step": 56920
    },
    {
      "epoch": 28.95727365208545,
      "grad_norm": 35.37496566772461,
      "learning_rate": 2.104272634791455e-05,
      "loss": 1.8733,
      "step": 56930
    },
    {
      "epoch": 28.962360122075278,
      "grad_norm": 32.1593017578125,
      "learning_rate": 2.1037639877924723e-05,
      "loss": 1.924,
      "step": 56940
    },
    {
      "epoch": 28.967446592065105,
      "grad_norm": 38.064579010009766,
      "learning_rate": 2.1032553407934893e-05,
      "loss": 1.792,
      "step": 56950
    },
    {
      "epoch": 28.972533062054932,
      "grad_norm": 31.30706787109375,
      "learning_rate": 2.1027466937945067e-05,
      "loss": 1.8423,
      "step": 56960
    },
    {
      "epoch": 28.977619532044763,
      "grad_norm": 43.60382843017578,
      "learning_rate": 2.102238046795524e-05,
      "loss": 1.8291,
      "step": 56970
    },
    {
      "epoch": 28.98270600203459,
      "grad_norm": 36.400474548339844,
      "learning_rate": 2.1017293997965413e-05,
      "loss": 1.8219,
      "step": 56980
    },
    {
      "epoch": 28.987792472024417,
      "grad_norm": 34.25310134887695,
      "learning_rate": 2.1012207527975586e-05,
      "loss": 1.8285,
      "step": 56990
    },
    {
      "epoch": 28.992878942014244,
      "grad_norm": 36.26434326171875,
      "learning_rate": 2.100712105798576e-05,
      "loss": 1.8539,
      "step": 57000
    },
    {
      "epoch": 28.99796541200407,
      "grad_norm": 38.86045837402344,
      "learning_rate": 2.100203458799593e-05,
      "loss": 1.8602,
      "step": 57010
    },
    {
      "epoch": 29.0,
      "eval_loss": 4.702866554260254,
      "eval_runtime": 2.8678,
      "eval_samples_per_second": 967.638,
      "eval_steps_per_second": 120.998,
      "step": 57014
    },
    {
      "epoch": 29.003051881993898,
      "grad_norm": 31.58915901184082,
      "learning_rate": 2.0996948118006106e-05,
      "loss": 1.855,
      "step": 57020
    },
    {
      "epoch": 29.008138351983725,
      "grad_norm": 35.93832015991211,
      "learning_rate": 2.099186164801628e-05,
      "loss": 1.7987,
      "step": 57030
    },
    {
      "epoch": 29.01322482197355,
      "grad_norm": 33.937713623046875,
      "learning_rate": 2.098677517802645e-05,
      "loss": 1.8444,
      "step": 57040
    },
    {
      "epoch": 29.01831129196338,
      "grad_norm": 36.226768493652344,
      "learning_rate": 2.0981688708036623e-05,
      "loss": 1.7678,
      "step": 57050
    },
    {
      "epoch": 29.023397761953206,
      "grad_norm": 36.992034912109375,
      "learning_rate": 2.0976602238046796e-05,
      "loss": 1.702,
      "step": 57060
    },
    {
      "epoch": 29.028484231943033,
      "grad_norm": 34.06267547607422,
      "learning_rate": 2.097151576805697e-05,
      "loss": 1.7479,
      "step": 57070
    },
    {
      "epoch": 29.03357070193286,
      "grad_norm": 36.00074768066406,
      "learning_rate": 2.0966429298067142e-05,
      "loss": 1.7283,
      "step": 57080
    },
    {
      "epoch": 29.038657171922686,
      "grad_norm": 40.162986755371094,
      "learning_rate": 2.0961342828077316e-05,
      "loss": 1.8879,
      "step": 57090
    },
    {
      "epoch": 29.043743641912513,
      "grad_norm": 37.60031509399414,
      "learning_rate": 2.095625635808749e-05,
      "loss": 1.8307,
      "step": 57100
    },
    {
      "epoch": 29.04883011190234,
      "grad_norm": 35.021785736083984,
      "learning_rate": 2.095116988809766e-05,
      "loss": 1.7034,
      "step": 57110
    },
    {
      "epoch": 29.053916581892167,
      "grad_norm": 34.73843765258789,
      "learning_rate": 2.0946083418107836e-05,
      "loss": 1.9099,
      "step": 57120
    },
    {
      "epoch": 29.059003051881994,
      "grad_norm": 39.23413848876953,
      "learning_rate": 2.094099694811801e-05,
      "loss": 1.7241,
      "step": 57130
    },
    {
      "epoch": 29.06408952187182,
      "grad_norm": 37.2475700378418,
      "learning_rate": 2.093591047812818e-05,
      "loss": 1.8453,
      "step": 57140
    },
    {
      "epoch": 29.06917599186165,
      "grad_norm": 33.74615478515625,
      "learning_rate": 2.0930824008138352e-05,
      "loss": 1.7685,
      "step": 57150
    },
    {
      "epoch": 29.074262461851475,
      "grad_norm": 39.72920608520508,
      "learning_rate": 2.0925737538148525e-05,
      "loss": 1.8165,
      "step": 57160
    },
    {
      "epoch": 29.079348931841302,
      "grad_norm": 44.59523391723633,
      "learning_rate": 2.09206510681587e-05,
      "loss": 1.8417,
      "step": 57170
    },
    {
      "epoch": 29.08443540183113,
      "grad_norm": 39.61349868774414,
      "learning_rate": 2.0915564598168872e-05,
      "loss": 1.7722,
      "step": 57180
    },
    {
      "epoch": 29.089521871820956,
      "grad_norm": 38.716766357421875,
      "learning_rate": 2.0910478128179045e-05,
      "loss": 1.8731,
      "step": 57190
    },
    {
      "epoch": 29.094608341810783,
      "grad_norm": 38.273353576660156,
      "learning_rate": 2.090539165818922e-05,
      "loss": 1.8017,
      "step": 57200
    },
    {
      "epoch": 29.09969481180061,
      "grad_norm": 35.96748733520508,
      "learning_rate": 2.090030518819939e-05,
      "loss": 1.7764,
      "step": 57210
    },
    {
      "epoch": 29.104781281790437,
      "grad_norm": 33.25873947143555,
      "learning_rate": 2.0895218718209565e-05,
      "loss": 1.8453,
      "step": 57220
    },
    {
      "epoch": 29.109867751780264,
      "grad_norm": 37.82709884643555,
      "learning_rate": 2.089013224821974e-05,
      "loss": 1.7874,
      "step": 57230
    },
    {
      "epoch": 29.11495422177009,
      "grad_norm": 28.941802978515625,
      "learning_rate": 2.0885045778229908e-05,
      "loss": 1.8541,
      "step": 57240
    },
    {
      "epoch": 29.120040691759918,
      "grad_norm": 37.90678024291992,
      "learning_rate": 2.087995930824008e-05,
      "loss": 1.7979,
      "step": 57250
    },
    {
      "epoch": 29.125127161749745,
      "grad_norm": 46.19567108154297,
      "learning_rate": 2.0874872838250255e-05,
      "loss": 1.7443,
      "step": 57260
    },
    {
      "epoch": 29.130213631739572,
      "grad_norm": 40.52621841430664,
      "learning_rate": 2.0869786368260428e-05,
      "loss": 1.8375,
      "step": 57270
    },
    {
      "epoch": 29.1353001017294,
      "grad_norm": 30.762548446655273,
      "learning_rate": 2.08646998982706e-05,
      "loss": 1.7865,
      "step": 57280
    },
    {
      "epoch": 29.140386571719226,
      "grad_norm": 52.43812942504883,
      "learning_rate": 2.0859613428280775e-05,
      "loss": 1.8074,
      "step": 57290
    },
    {
      "epoch": 29.145473041709053,
      "grad_norm": 32.6961669921875,
      "learning_rate": 2.0854526958290945e-05,
      "loss": 1.7277,
      "step": 57300
    },
    {
      "epoch": 29.15055951169888,
      "grad_norm": 32.004356384277344,
      "learning_rate": 2.084944048830112e-05,
      "loss": 1.7637,
      "step": 57310
    },
    {
      "epoch": 29.155645981688707,
      "grad_norm": 35.12717056274414,
      "learning_rate": 2.0844354018311294e-05,
      "loss": 1.7962,
      "step": 57320
    },
    {
      "epoch": 29.160732451678534,
      "grad_norm": 34.72059631347656,
      "learning_rate": 2.0839267548321468e-05,
      "loss": 1.868,
      "step": 57330
    },
    {
      "epoch": 29.16581892166836,
      "grad_norm": 42.69084930419922,
      "learning_rate": 2.0834181078331638e-05,
      "loss": 1.7966,
      "step": 57340
    },
    {
      "epoch": 29.170905391658188,
      "grad_norm": 47.802093505859375,
      "learning_rate": 2.082909460834181e-05,
      "loss": 1.9017,
      "step": 57350
    },
    {
      "epoch": 29.175991861648015,
      "grad_norm": 39.45486068725586,
      "learning_rate": 2.0824008138351984e-05,
      "loss": 1.8496,
      "step": 57360
    },
    {
      "epoch": 29.181078331637842,
      "grad_norm": 43.39232635498047,
      "learning_rate": 2.0818921668362158e-05,
      "loss": 1.7854,
      "step": 57370
    },
    {
      "epoch": 29.18616480162767,
      "grad_norm": 39.81327438354492,
      "learning_rate": 2.081383519837233e-05,
      "loss": 1.7523,
      "step": 57380
    },
    {
      "epoch": 29.191251271617496,
      "grad_norm": 31.587961196899414,
      "learning_rate": 2.0808748728382504e-05,
      "loss": 1.8779,
      "step": 57390
    },
    {
      "epoch": 29.196337741607323,
      "grad_norm": 35.11244583129883,
      "learning_rate": 2.0803662258392674e-05,
      "loss": 1.8644,
      "step": 57400
    },
    {
      "epoch": 29.20142421159715,
      "grad_norm": 39.35976791381836,
      "learning_rate": 2.079857578840285e-05,
      "loss": 1.7875,
      "step": 57410
    },
    {
      "epoch": 29.20651068158698,
      "grad_norm": 41.66611862182617,
      "learning_rate": 2.0793489318413024e-05,
      "loss": 1.7889,
      "step": 57420
    },
    {
      "epoch": 29.211597151576807,
      "grad_norm": 40.81087875366211,
      "learning_rate": 2.0788402848423194e-05,
      "loss": 1.8091,
      "step": 57430
    },
    {
      "epoch": 29.216683621566634,
      "grad_norm": 42.64695739746094,
      "learning_rate": 2.0783316378433367e-05,
      "loss": 1.7173,
      "step": 57440
    },
    {
      "epoch": 29.22177009155646,
      "grad_norm": 38.080081939697266,
      "learning_rate": 2.077822990844354e-05,
      "loss": 1.8294,
      "step": 57450
    },
    {
      "epoch": 29.22685656154629,
      "grad_norm": 37.47248458862305,
      "learning_rate": 2.0773143438453717e-05,
      "loss": 1.7818,
      "step": 57460
    },
    {
      "epoch": 29.231943031536115,
      "grad_norm": 40.812049865722656,
      "learning_rate": 2.0768056968463887e-05,
      "loss": 1.8434,
      "step": 57470
    },
    {
      "epoch": 29.237029501525942,
      "grad_norm": 54.224578857421875,
      "learning_rate": 2.076297049847406e-05,
      "loss": 1.8531,
      "step": 57480
    },
    {
      "epoch": 29.24211597151577,
      "grad_norm": 43.18060302734375,
      "learning_rate": 2.0757884028484233e-05,
      "loss": 1.7986,
      "step": 57490
    },
    {
      "epoch": 29.247202441505596,
      "grad_norm": 36.30809020996094,
      "learning_rate": 2.0752797558494407e-05,
      "loss": 1.8104,
      "step": 57500
    },
    {
      "epoch": 29.252288911495423,
      "grad_norm": 38.01708984375,
      "learning_rate": 2.074771108850458e-05,
      "loss": 1.7346,
      "step": 57510
    },
    {
      "epoch": 29.25737538148525,
      "grad_norm": 26.646757125854492,
      "learning_rate": 2.0742624618514753e-05,
      "loss": 1.8385,
      "step": 57520
    },
    {
      "epoch": 29.262461851475077,
      "grad_norm": 34.99927520751953,
      "learning_rate": 2.0737538148524923e-05,
      "loss": 1.7988,
      "step": 57530
    },
    {
      "epoch": 29.267548321464904,
      "grad_norm": 31.88682746887207,
      "learning_rate": 2.0732451678535097e-05,
      "loss": 1.8195,
      "step": 57540
    },
    {
      "epoch": 29.27263479145473,
      "grad_norm": 35.505958557128906,
      "learning_rate": 2.072736520854527e-05,
      "loss": 1.8951,
      "step": 57550
    },
    {
      "epoch": 29.277721261444558,
      "grad_norm": 38.2768669128418,
      "learning_rate": 2.0722278738555443e-05,
      "loss": 1.7704,
      "step": 57560
    },
    {
      "epoch": 29.282807731434385,
      "grad_norm": 45.682430267333984,
      "learning_rate": 2.0717192268565616e-05,
      "loss": 1.7674,
      "step": 57570
    },
    {
      "epoch": 29.287894201424212,
      "grad_norm": 34.42539596557617,
      "learning_rate": 2.071210579857579e-05,
      "loss": 1.7801,
      "step": 57580
    },
    {
      "epoch": 29.29298067141404,
      "grad_norm": 31.081226348876953,
      "learning_rate": 2.0707019328585963e-05,
      "loss": 1.8056,
      "step": 57590
    },
    {
      "epoch": 29.298067141403866,
      "grad_norm": 31.515453338623047,
      "learning_rate": 2.0701932858596136e-05,
      "loss": 1.863,
      "step": 57600
    },
    {
      "epoch": 29.303153611393693,
      "grad_norm": 34.6521110534668,
      "learning_rate": 2.069684638860631e-05,
      "loss": 1.7896,
      "step": 57610
    },
    {
      "epoch": 29.30824008138352,
      "grad_norm": 29.26007652282715,
      "learning_rate": 2.0691759918616483e-05,
      "loss": 1.7778,
      "step": 57620
    },
    {
      "epoch": 29.313326551373347,
      "grad_norm": 40.33757781982422,
      "learning_rate": 2.0686673448626653e-05,
      "loss": 1.798,
      "step": 57630
    },
    {
      "epoch": 29.318413021363174,
      "grad_norm": 44.521759033203125,
      "learning_rate": 2.0681586978636826e-05,
      "loss": 1.8119,
      "step": 57640
    },
    {
      "epoch": 29.323499491353,
      "grad_norm": 31.80255699157715,
      "learning_rate": 2.0676500508647003e-05,
      "loss": 1.8108,
      "step": 57650
    },
    {
      "epoch": 29.328585961342828,
      "grad_norm": 34.84675979614258,
      "learning_rate": 2.0671414038657173e-05,
      "loss": 1.8465,
      "step": 57660
    },
    {
      "epoch": 29.333672431332655,
      "grad_norm": 41.788551330566406,
      "learning_rate": 2.0666327568667346e-05,
      "loss": 1.8259,
      "step": 57670
    },
    {
      "epoch": 29.338758901322482,
      "grad_norm": 36.9884033203125,
      "learning_rate": 2.066124109867752e-05,
      "loss": 1.8244,
      "step": 57680
    },
    {
      "epoch": 29.34384537131231,
      "grad_norm": 43.13072204589844,
      "learning_rate": 2.065615462868769e-05,
      "loss": 1.8037,
      "step": 57690
    },
    {
      "epoch": 29.348931841302136,
      "grad_norm": 34.56854248046875,
      "learning_rate": 2.0651068158697866e-05,
      "loss": 1.7058,
      "step": 57700
    },
    {
      "epoch": 29.354018311291963,
      "grad_norm": 34.023094177246094,
      "learning_rate": 2.064598168870804e-05,
      "loss": 1.7999,
      "step": 57710
    },
    {
      "epoch": 29.35910478128179,
      "grad_norm": 31.67388916015625,
      "learning_rate": 2.064089521871821e-05,
      "loss": 1.8337,
      "step": 57720
    },
    {
      "epoch": 29.364191251271617,
      "grad_norm": 39.91859817504883,
      "learning_rate": 2.0635808748728382e-05,
      "loss": 1.8067,
      "step": 57730
    },
    {
      "epoch": 29.369277721261444,
      "grad_norm": 49.822574615478516,
      "learning_rate": 2.0630722278738555e-05,
      "loss": 1.9033,
      "step": 57740
    },
    {
      "epoch": 29.37436419125127,
      "grad_norm": 43.74064254760742,
      "learning_rate": 2.0625635808748732e-05,
      "loss": 1.8148,
      "step": 57750
    },
    {
      "epoch": 29.379450661241098,
      "grad_norm": 35.746925354003906,
      "learning_rate": 2.0620549338758902e-05,
      "loss": 1.8085,
      "step": 57760
    },
    {
      "epoch": 29.384537131230925,
      "grad_norm": 49.87977981567383,
      "learning_rate": 2.0615462868769075e-05,
      "loss": 1.7654,
      "step": 57770
    },
    {
      "epoch": 29.38962360122075,
      "grad_norm": 33.66054916381836,
      "learning_rate": 2.061037639877925e-05,
      "loss": 1.8857,
      "step": 57780
    },
    {
      "epoch": 29.39471007121058,
      "grad_norm": 47.751136779785156,
      "learning_rate": 2.0605289928789422e-05,
      "loss": 1.7725,
      "step": 57790
    },
    {
      "epoch": 29.399796541200406,
      "grad_norm": 36.61275863647461,
      "learning_rate": 2.0600203458799595e-05,
      "loss": 1.7708,
      "step": 57800
    },
    {
      "epoch": 29.404883011190233,
      "grad_norm": 36.313045501708984,
      "learning_rate": 2.059511698880977e-05,
      "loss": 1.8536,
      "step": 57810
    },
    {
      "epoch": 29.40996948118006,
      "grad_norm": 45.786163330078125,
      "learning_rate": 2.0590030518819938e-05,
      "loss": 1.8033,
      "step": 57820
    },
    {
      "epoch": 29.415055951169887,
      "grad_norm": 37.28147506713867,
      "learning_rate": 2.058494404883011e-05,
      "loss": 1.8065,
      "step": 57830
    },
    {
      "epoch": 29.420142421159714,
      "grad_norm": 31.14695167541504,
      "learning_rate": 2.0579857578840285e-05,
      "loss": 1.7945,
      "step": 57840
    },
    {
      "epoch": 29.42522889114954,
      "grad_norm": 32.73432540893555,
      "learning_rate": 2.0574771108850458e-05,
      "loss": 1.8004,
      "step": 57850
    },
    {
      "epoch": 29.43031536113937,
      "grad_norm": 34.592689514160156,
      "learning_rate": 2.056968463886063e-05,
      "loss": 1.8436,
      "step": 57860
    },
    {
      "epoch": 29.435401831129198,
      "grad_norm": 36.2112922668457,
      "learning_rate": 2.0564598168870805e-05,
      "loss": 1.8457,
      "step": 57870
    },
    {
      "epoch": 29.440488301119025,
      "grad_norm": 43.2689094543457,
      "learning_rate": 2.0559511698880978e-05,
      "loss": 1.7984,
      "step": 57880
    },
    {
      "epoch": 29.445574771108852,
      "grad_norm": 33.59125900268555,
      "learning_rate": 2.055442522889115e-05,
      "loss": 1.7849,
      "step": 57890
    },
    {
      "epoch": 29.45066124109868,
      "grad_norm": 35.85712432861328,
      "learning_rate": 2.0549338758901324e-05,
      "loss": 1.751,
      "step": 57900
    },
    {
      "epoch": 29.455747711088506,
      "grad_norm": 49.52427291870117,
      "learning_rate": 2.0544252288911498e-05,
      "loss": 1.7356,
      "step": 57910
    },
    {
      "epoch": 29.460834181078333,
      "grad_norm": 33.23781204223633,
      "learning_rate": 2.0539165818921668e-05,
      "loss": 1.7582,
      "step": 57920
    },
    {
      "epoch": 29.46592065106816,
      "grad_norm": 44.82572555541992,
      "learning_rate": 2.053407934893184e-05,
      "loss": 1.7944,
      "step": 57930
    },
    {
      "epoch": 29.471007121057987,
      "grad_norm": 39.69868469238281,
      "learning_rate": 2.0528992878942018e-05,
      "loss": 1.7881,
      "step": 57940
    },
    {
      "epoch": 29.476093591047814,
      "grad_norm": 33.6584587097168,
      "learning_rate": 2.0523906408952188e-05,
      "loss": 1.8386,
      "step": 57950
    },
    {
      "epoch": 29.48118006103764,
      "grad_norm": 32.011863708496094,
      "learning_rate": 2.051881993896236e-05,
      "loss": 1.8867,
      "step": 57960
    },
    {
      "epoch": 29.486266531027468,
      "grad_norm": 38.40258026123047,
      "learning_rate": 2.0513733468972534e-05,
      "loss": 1.7581,
      "step": 57970
    },
    {
      "epoch": 29.491353001017295,
      "grad_norm": 49.98612594604492,
      "learning_rate": 2.0508646998982707e-05,
      "loss": 1.7534,
      "step": 57980
    },
    {
      "epoch": 29.496439471007122,
      "grad_norm": 36.839111328125,
      "learning_rate": 2.050356052899288e-05,
      "loss": 1.7862,
      "step": 57990
    },
    {
      "epoch": 29.50152594099695,
      "grad_norm": 39.63227844238281,
      "learning_rate": 2.0498474059003054e-05,
      "loss": 1.8358,
      "step": 58000
    },
    {
      "epoch": 29.506612410986776,
      "grad_norm": 36.774375915527344,
      "learning_rate": 2.0493387589013227e-05,
      "loss": 1.7576,
      "step": 58010
    },
    {
      "epoch": 29.511698880976603,
      "grad_norm": 41.28483200073242,
      "learning_rate": 2.0488301119023397e-05,
      "loss": 1.8235,
      "step": 58020
    },
    {
      "epoch": 29.51678535096643,
      "grad_norm": 38.237239837646484,
      "learning_rate": 2.048321464903357e-05,
      "loss": 1.8701,
      "step": 58030
    },
    {
      "epoch": 29.521871820956257,
      "grad_norm": 34.350337982177734,
      "learning_rate": 2.0478128179043747e-05,
      "loss": 1.76,
      "step": 58040
    },
    {
      "epoch": 29.526958290946084,
      "grad_norm": 42.050052642822266,
      "learning_rate": 2.0473041709053917e-05,
      "loss": 1.7934,
      "step": 58050
    },
    {
      "epoch": 29.53204476093591,
      "grad_norm": 47.9708251953125,
      "learning_rate": 2.046795523906409e-05,
      "loss": 1.8455,
      "step": 58060
    },
    {
      "epoch": 29.537131230925738,
      "grad_norm": 54.76093673706055,
      "learning_rate": 2.0462868769074263e-05,
      "loss": 1.8303,
      "step": 58070
    },
    {
      "epoch": 29.542217700915565,
      "grad_norm": 37.80995559692383,
      "learning_rate": 2.0457782299084437e-05,
      "loss": 1.8187,
      "step": 58080
    },
    {
      "epoch": 29.54730417090539,
      "grad_norm": 37.6259765625,
      "learning_rate": 2.045269582909461e-05,
      "loss": 1.815,
      "step": 58090
    },
    {
      "epoch": 29.55239064089522,
      "grad_norm": 32.132999420166016,
      "learning_rate": 2.0447609359104783e-05,
      "loss": 1.7989,
      "step": 58100
    },
    {
      "epoch": 29.557477110885046,
      "grad_norm": 30.99059295654297,
      "learning_rate": 2.0442522889114953e-05,
      "loss": 1.8439,
      "step": 58110
    },
    {
      "epoch": 29.562563580874873,
      "grad_norm": 32.52766799926758,
      "learning_rate": 2.0437436419125127e-05,
      "loss": 1.847,
      "step": 58120
    },
    {
      "epoch": 29.5676500508647,
      "grad_norm": 38.58678436279297,
      "learning_rate": 2.0432349949135303e-05,
      "loss": 1.8282,
      "step": 58130
    },
    {
      "epoch": 29.572736520854527,
      "grad_norm": 34.954654693603516,
      "learning_rate": 2.0427263479145476e-05,
      "loss": 1.8627,
      "step": 58140
    },
    {
      "epoch": 29.577822990844354,
      "grad_norm": 31.922935485839844,
      "learning_rate": 2.0422177009155646e-05,
      "loss": 1.7436,
      "step": 58150
    },
    {
      "epoch": 29.58290946083418,
      "grad_norm": 34.01737594604492,
      "learning_rate": 2.041709053916582e-05,
      "loss": 1.7989,
      "step": 58160
    },
    {
      "epoch": 29.587995930824007,
      "grad_norm": 47.39046096801758,
      "learning_rate": 2.0412004069175993e-05,
      "loss": 1.8856,
      "step": 58170
    },
    {
      "epoch": 29.593082400813834,
      "grad_norm": 41.85794448852539,
      "learning_rate": 2.0406917599186166e-05,
      "loss": 1.792,
      "step": 58180
    },
    {
      "epoch": 29.59816887080366,
      "grad_norm": 39.402259826660156,
      "learning_rate": 2.040183112919634e-05,
      "loss": 1.8483,
      "step": 58190
    },
    {
      "epoch": 29.60325534079349,
      "grad_norm": 36.093162536621094,
      "learning_rate": 2.0396744659206513e-05,
      "loss": 1.802,
      "step": 58200
    },
    {
      "epoch": 29.608341810783315,
      "grad_norm": 45.67518615722656,
      "learning_rate": 2.0391658189216683e-05,
      "loss": 1.7986,
      "step": 58210
    },
    {
      "epoch": 29.613428280773142,
      "grad_norm": 32.445377349853516,
      "learning_rate": 2.0386571719226856e-05,
      "loss": 1.7272,
      "step": 58220
    },
    {
      "epoch": 29.61851475076297,
      "grad_norm": 34.54600524902344,
      "learning_rate": 2.0381485249237033e-05,
      "loss": 1.7823,
      "step": 58230
    },
    {
      "epoch": 29.623601220752796,
      "grad_norm": 33.260257720947266,
      "learning_rate": 2.0376398779247203e-05,
      "loss": 1.7862,
      "step": 58240
    },
    {
      "epoch": 29.628687690742623,
      "grad_norm": 39.502967834472656,
      "learning_rate": 2.0371312309257376e-05,
      "loss": 1.8045,
      "step": 58250
    },
    {
      "epoch": 29.63377416073245,
      "grad_norm": 35.581790924072266,
      "learning_rate": 2.036622583926755e-05,
      "loss": 1.7637,
      "step": 58260
    },
    {
      "epoch": 29.638860630722277,
      "grad_norm": 30.778568267822266,
      "learning_rate": 2.0361139369277722e-05,
      "loss": 1.856,
      "step": 58270
    },
    {
      "epoch": 29.643947100712104,
      "grad_norm": 45.11316680908203,
      "learning_rate": 2.0356052899287896e-05,
      "loss": 1.7843,
      "step": 58280
    },
    {
      "epoch": 29.64903357070193,
      "grad_norm": 42.376346588134766,
      "learning_rate": 2.035096642929807e-05,
      "loss": 1.8453,
      "step": 58290
    },
    {
      "epoch": 29.654120040691758,
      "grad_norm": 35.74779510498047,
      "learning_rate": 2.0345879959308242e-05,
      "loss": 1.7973,
      "step": 58300
    },
    {
      "epoch": 29.659206510681585,
      "grad_norm": 41.062015533447266,
      "learning_rate": 2.0340793489318412e-05,
      "loss": 1.7999,
      "step": 58310
    },
    {
      "epoch": 29.664292980671416,
      "grad_norm": 44.32179260253906,
      "learning_rate": 2.0335707019328585e-05,
      "loss": 1.6686,
      "step": 58320
    },
    {
      "epoch": 29.669379450661243,
      "grad_norm": 41.49177169799805,
      "learning_rate": 2.0330620549338762e-05,
      "loss": 1.8553,
      "step": 58330
    },
    {
      "epoch": 29.67446592065107,
      "grad_norm": 35.093238830566406,
      "learning_rate": 2.0325534079348932e-05,
      "loss": 1.7654,
      "step": 58340
    },
    {
      "epoch": 29.679552390640897,
      "grad_norm": 38.33704376220703,
      "learning_rate": 2.0320447609359105e-05,
      "loss": 1.812,
      "step": 58350
    },
    {
      "epoch": 29.684638860630724,
      "grad_norm": 33.81504821777344,
      "learning_rate": 2.031536113936928e-05,
      "loss": 1.8011,
      "step": 58360
    },
    {
      "epoch": 29.68972533062055,
      "grad_norm": 35.98757553100586,
      "learning_rate": 2.0310274669379452e-05,
      "loss": 1.8085,
      "step": 58370
    },
    {
      "epoch": 29.694811800610378,
      "grad_norm": 35.35273742675781,
      "learning_rate": 2.0305188199389625e-05,
      "loss": 1.7249,
      "step": 58380
    },
    {
      "epoch": 29.699898270600205,
      "grad_norm": 43.54792022705078,
      "learning_rate": 2.03001017293998e-05,
      "loss": 1.7923,
      "step": 58390
    },
    {
      "epoch": 29.70498474059003,
      "grad_norm": 38.158042907714844,
      "learning_rate": 2.029501525940997e-05,
      "loss": 1.7675,
      "step": 58400
    },
    {
      "epoch": 29.71007121057986,
      "grad_norm": 31.8210391998291,
      "learning_rate": 2.028992878942014e-05,
      "loss": 1.823,
      "step": 58410
    },
    {
      "epoch": 29.715157680569686,
      "grad_norm": 39.39955520629883,
      "learning_rate": 2.0284842319430318e-05,
      "loss": 1.7799,
      "step": 58420
    },
    {
      "epoch": 29.720244150559513,
      "grad_norm": 30.279817581176758,
      "learning_rate": 2.027975584944049e-05,
      "loss": 1.8624,
      "step": 58430
    },
    {
      "epoch": 29.72533062054934,
      "grad_norm": 39.095703125,
      "learning_rate": 2.027466937945066e-05,
      "loss": 1.8639,
      "step": 58440
    },
    {
      "epoch": 29.730417090539166,
      "grad_norm": 36.36486053466797,
      "learning_rate": 2.0269582909460835e-05,
      "loss": 1.851,
      "step": 58450
    },
    {
      "epoch": 29.735503560528993,
      "grad_norm": 41.18498611450195,
      "learning_rate": 2.0264496439471008e-05,
      "loss": 1.8673,
      "step": 58460
    },
    {
      "epoch": 29.74059003051882,
      "grad_norm": 62.104068756103516,
      "learning_rate": 2.025940996948118e-05,
      "loss": 1.8022,
      "step": 58470
    },
    {
      "epoch": 29.745676500508647,
      "grad_norm": 30.119260787963867,
      "learning_rate": 2.0254323499491354e-05,
      "loss": 1.7449,
      "step": 58480
    },
    {
      "epoch": 29.750762970498474,
      "grad_norm": 32.39310073852539,
      "learning_rate": 2.0249237029501528e-05,
      "loss": 1.7189,
      "step": 58490
    },
    {
      "epoch": 29.7558494404883,
      "grad_norm": 48.07068634033203,
      "learning_rate": 2.0244150559511698e-05,
      "loss": 1.7902,
      "step": 58500
    },
    {
      "epoch": 29.76093591047813,
      "grad_norm": 45.60800552368164,
      "learning_rate": 2.023906408952187e-05,
      "loss": 1.907,
      "step": 58510
    },
    {
      "epoch": 29.766022380467955,
      "grad_norm": 42.90004348754883,
      "learning_rate": 2.0233977619532048e-05,
      "loss": 1.7981,
      "step": 58520
    },
    {
      "epoch": 29.771108850457782,
      "grad_norm": 43.868804931640625,
      "learning_rate": 2.0228891149542218e-05,
      "loss": 1.8591,
      "step": 58530
    },
    {
      "epoch": 29.77619532044761,
      "grad_norm": 34.99403762817383,
      "learning_rate": 2.022380467955239e-05,
      "loss": 1.7954,
      "step": 58540
    },
    {
      "epoch": 29.781281790437436,
      "grad_norm": 41.133975982666016,
      "learning_rate": 2.0218718209562564e-05,
      "loss": 1.7953,
      "step": 58550
    },
    {
      "epoch": 29.786368260427263,
      "grad_norm": 39.66263961791992,
      "learning_rate": 2.0213631739572737e-05,
      "loss": 1.7538,
      "step": 58560
    },
    {
      "epoch": 29.79145473041709,
      "grad_norm": 37.99345016479492,
      "learning_rate": 2.020854526958291e-05,
      "loss": 1.7618,
      "step": 58570
    },
    {
      "epoch": 29.796541200406917,
      "grad_norm": 40.278236389160156,
      "learning_rate": 2.0203458799593084e-05,
      "loss": 1.7559,
      "step": 58580
    },
    {
      "epoch": 29.801627670396744,
      "grad_norm": 41.482948303222656,
      "learning_rate": 2.0198372329603257e-05,
      "loss": 1.7621,
      "step": 58590
    },
    {
      "epoch": 29.80671414038657,
      "grad_norm": 31.067649841308594,
      "learning_rate": 2.0193285859613427e-05,
      "loss": 1.8713,
      "step": 58600
    },
    {
      "epoch": 29.811800610376398,
      "grad_norm": 51.49238967895508,
      "learning_rate": 2.0188199389623604e-05,
      "loss": 1.7703,
      "step": 58610
    },
    {
      "epoch": 29.816887080366225,
      "grad_norm": 42.731544494628906,
      "learning_rate": 2.0183112919633777e-05,
      "loss": 1.8317,
      "step": 58620
    },
    {
      "epoch": 29.821973550356052,
      "grad_norm": 41.3768196105957,
      "learning_rate": 2.0178026449643947e-05,
      "loss": 1.918,
      "step": 58630
    },
    {
      "epoch": 29.82706002034588,
      "grad_norm": 36.196250915527344,
      "learning_rate": 2.017293997965412e-05,
      "loss": 1.7894,
      "step": 58640
    },
    {
      "epoch": 29.832146490335706,
      "grad_norm": 35.46602249145508,
      "learning_rate": 2.0167853509664294e-05,
      "loss": 1.7996,
      "step": 58650
    },
    {
      "epoch": 29.837232960325533,
      "grad_norm": 38.051658630371094,
      "learning_rate": 2.0162767039674467e-05,
      "loss": 1.8386,
      "step": 58660
    },
    {
      "epoch": 29.84231943031536,
      "grad_norm": 33.56793212890625,
      "learning_rate": 2.015768056968464e-05,
      "loss": 1.7419,
      "step": 58670
    },
    {
      "epoch": 29.847405900305187,
      "grad_norm": 36.908607482910156,
      "learning_rate": 2.0152594099694813e-05,
      "loss": 1.8177,
      "step": 58680
    },
    {
      "epoch": 29.852492370295014,
      "grad_norm": 43.02933883666992,
      "learning_rate": 2.0147507629704987e-05,
      "loss": 1.8239,
      "step": 58690
    },
    {
      "epoch": 29.85757884028484,
      "grad_norm": 34.874210357666016,
      "learning_rate": 2.0142421159715157e-05,
      "loss": 1.7507,
      "step": 58700
    },
    {
      "epoch": 29.862665310274668,
      "grad_norm": 38.01945114135742,
      "learning_rate": 2.0137334689725333e-05,
      "loss": 1.7344,
      "step": 58710
    },
    {
      "epoch": 29.867751780264495,
      "grad_norm": 35.17869567871094,
      "learning_rate": 2.0132248219735506e-05,
      "loss": 1.7471,
      "step": 58720
    },
    {
      "epoch": 29.872838250254322,
      "grad_norm": 38.899112701416016,
      "learning_rate": 2.0127161749745676e-05,
      "loss": 1.8347,
      "step": 58730
    },
    {
      "epoch": 29.87792472024415,
      "grad_norm": 33.612213134765625,
      "learning_rate": 2.012207527975585e-05,
      "loss": 1.753,
      "step": 58740
    },
    {
      "epoch": 29.88301119023398,
      "grad_norm": 38.84490203857422,
      "learning_rate": 2.0116988809766023e-05,
      "loss": 1.7894,
      "step": 58750
    },
    {
      "epoch": 29.888097660223806,
      "grad_norm": 30.032442092895508,
      "learning_rate": 2.0111902339776196e-05,
      "loss": 1.8007,
      "step": 58760
    },
    {
      "epoch": 29.893184130213633,
      "grad_norm": 38.090538024902344,
      "learning_rate": 2.010681586978637e-05,
      "loss": 1.826,
      "step": 58770
    },
    {
      "epoch": 29.89827060020346,
      "grad_norm": 34.20924377441406,
      "learning_rate": 2.0101729399796543e-05,
      "loss": 1.7808,
      "step": 58780
    },
    {
      "epoch": 29.903357070193287,
      "grad_norm": 29.28729248046875,
      "learning_rate": 2.0096642929806713e-05,
      "loss": 1.8222,
      "step": 58790
    },
    {
      "epoch": 29.908443540183114,
      "grad_norm": 36.40299606323242,
      "learning_rate": 2.0091556459816886e-05,
      "loss": 1.8911,
      "step": 58800
    },
    {
      "epoch": 29.91353001017294,
      "grad_norm": 37.55208206176758,
      "learning_rate": 2.0086469989827063e-05,
      "loss": 1.8042,
      "step": 58810
    },
    {
      "epoch": 29.91861648016277,
      "grad_norm": 42.1579475402832,
      "learning_rate": 2.0081383519837236e-05,
      "loss": 1.7624,
      "step": 58820
    },
    {
      "epoch": 29.923702950152595,
      "grad_norm": 40.45604705810547,
      "learning_rate": 2.0076297049847406e-05,
      "loss": 1.8239,
      "step": 58830
    },
    {
      "epoch": 29.928789420142422,
      "grad_norm": 46.691688537597656,
      "learning_rate": 2.007121057985758e-05,
      "loss": 1.7646,
      "step": 58840
    },
    {
      "epoch": 29.93387589013225,
      "grad_norm": 35.29044723510742,
      "learning_rate": 2.0066124109867752e-05,
      "loss": 1.8013,
      "step": 58850
    },
    {
      "epoch": 29.938962360122076,
      "grad_norm": 33.39830780029297,
      "learning_rate": 2.0061037639877926e-05,
      "loss": 1.8073,
      "step": 58860
    },
    {
      "epoch": 29.944048830111903,
      "grad_norm": 33.22600555419922,
      "learning_rate": 2.00559511698881e-05,
      "loss": 1.7706,
      "step": 58870
    },
    {
      "epoch": 29.94913530010173,
      "grad_norm": 47.53446960449219,
      "learning_rate": 2.0050864699898272e-05,
      "loss": 1.6872,
      "step": 58880
    },
    {
      "epoch": 29.954221770091557,
      "grad_norm": 35.66378402709961,
      "learning_rate": 2.0045778229908442e-05,
      "loss": 1.7514,
      "step": 58890
    },
    {
      "epoch": 29.959308240081384,
      "grad_norm": 44.61838150024414,
      "learning_rate": 2.004069175991862e-05,
      "loss": 1.7867,
      "step": 58900
    },
    {
      "epoch": 29.96439471007121,
      "grad_norm": 39.32313919067383,
      "learning_rate": 2.0035605289928792e-05,
      "loss": 1.7442,
      "step": 58910
    },
    {
      "epoch": 29.969481180061038,
      "grad_norm": 34.61909484863281,
      "learning_rate": 2.0030518819938962e-05,
      "loss": 1.8349,
      "step": 58920
    },
    {
      "epoch": 29.974567650050865,
      "grad_norm": 39.0494270324707,
      "learning_rate": 2.0025432349949135e-05,
      "loss": 1.8489,
      "step": 58930
    },
    {
      "epoch": 29.979654120040692,
      "grad_norm": 43.89737319946289,
      "learning_rate": 2.002034587995931e-05,
      "loss": 1.8703,
      "step": 58940
    },
    {
      "epoch": 29.98474059003052,
      "grad_norm": 33.60262680053711,
      "learning_rate": 2.0015259409969482e-05,
      "loss": 1.7906,
      "step": 58950
    },
    {
      "epoch": 29.989827060020346,
      "grad_norm": 49.9954719543457,
      "learning_rate": 2.0010172939979655e-05,
      "loss": 1.7837,
      "step": 58960
    },
    {
      "epoch": 29.994913530010173,
      "grad_norm": 38.152305603027344,
      "learning_rate": 2.000508646998983e-05,
      "loss": 1.8115,
      "step": 58970
    },
    {
      "epoch": 30.0,
      "grad_norm": 37.45637130737305,
      "learning_rate": 2e-05,
      "loss": 1.7417,
      "step": 58980
    },
    {
      "epoch": 30.0,
      "eval_loss": 4.737858295440674,
      "eval_runtime": 2.9666,
      "eval_samples_per_second": 935.423,
      "eval_steps_per_second": 116.97,
      "step": 58980
    },
    {
      "epoch": 30.005086469989827,
      "grad_norm": 40.96864700317383,
      "learning_rate": 1.999491353001017e-05,
      "loss": 1.72,
      "step": 58990
    },
    {
      "epoch": 30.010172939979654,
      "grad_norm": 35.13817596435547,
      "learning_rate": 1.9989827060020348e-05,
      "loss": 1.7552,
      "step": 59000
    },
    {
      "epoch": 30.01525940996948,
      "grad_norm": 39.68697738647461,
      "learning_rate": 1.998474059003052e-05,
      "loss": 1.7797,
      "step": 59010
    },
    {
      "epoch": 30.020345879959308,
      "grad_norm": 37.79356384277344,
      "learning_rate": 1.997965412004069e-05,
      "loss": 1.859,
      "step": 59020
    },
    {
      "epoch": 30.025432349949135,
      "grad_norm": 45.057167053222656,
      "learning_rate": 1.9974567650050865e-05,
      "loss": 1.7873,
      "step": 59030
    },
    {
      "epoch": 30.030518819938962,
      "grad_norm": 33.08047103881836,
      "learning_rate": 1.9969481180061038e-05,
      "loss": 1.7889,
      "step": 59040
    },
    {
      "epoch": 30.03560528992879,
      "grad_norm": 35.18910598754883,
      "learning_rate": 1.996439471007121e-05,
      "loss": 1.7958,
      "step": 59050
    },
    {
      "epoch": 30.040691759918616,
      "grad_norm": 32.086036682128906,
      "learning_rate": 1.9959308240081385e-05,
      "loss": 1.8001,
      "step": 59060
    },
    {
      "epoch": 30.045778229908443,
      "grad_norm": 31.344343185424805,
      "learning_rate": 1.9954221770091558e-05,
      "loss": 1.7778,
      "step": 59070
    },
    {
      "epoch": 30.05086469989827,
      "grad_norm": 32.777931213378906,
      "learning_rate": 1.994913530010173e-05,
      "loss": 1.7,
      "step": 59080
    },
    {
      "epoch": 30.055951169888097,
      "grad_norm": 39.463253021240234,
      "learning_rate": 1.9944048830111904e-05,
      "loss": 1.7347,
      "step": 59090
    },
    {
      "epoch": 30.061037639877924,
      "grad_norm": 38.040767669677734,
      "learning_rate": 1.9938962360122078e-05,
      "loss": 1.703,
      "step": 59100
    },
    {
      "epoch": 30.06612410986775,
      "grad_norm": 52.16684341430664,
      "learning_rate": 1.993387589013225e-05,
      "loss": 1.838,
      "step": 59110
    },
    {
      "epoch": 30.071210579857578,
      "grad_norm": 31.832054138183594,
      "learning_rate": 1.992878942014242e-05,
      "loss": 1.8867,
      "step": 59120
    },
    {
      "epoch": 30.076297049847405,
      "grad_norm": 30.65567398071289,
      "learning_rate": 1.9923702950152594e-05,
      "loss": 1.82,
      "step": 59130
    },
    {
      "epoch": 30.08138351983723,
      "grad_norm": 45.711822509765625,
      "learning_rate": 1.9918616480162767e-05,
      "loss": 1.7575,
      "step": 59140
    },
    {
      "epoch": 30.08646998982706,
      "grad_norm": 46.32084274291992,
      "learning_rate": 1.991353001017294e-05,
      "loss": 1.8078,
      "step": 59150
    },
    {
      "epoch": 30.091556459816886,
      "grad_norm": 33.79012680053711,
      "learning_rate": 1.9908443540183114e-05,
      "loss": 1.7514,
      "step": 59160
    },
    {
      "epoch": 30.096642929806713,
      "grad_norm": 35.66333770751953,
      "learning_rate": 1.9903357070193287e-05,
      "loss": 1.8455,
      "step": 59170
    },
    {
      "epoch": 30.10172939979654,
      "grad_norm": 35.13335037231445,
      "learning_rate": 1.9898270600203457e-05,
      "loss": 1.7329,
      "step": 59180
    },
    {
      "epoch": 30.106815869786367,
      "grad_norm": 40.24960708618164,
      "learning_rate": 1.9893184130213634e-05,
      "loss": 1.7769,
      "step": 59190
    },
    {
      "epoch": 30.111902339776194,
      "grad_norm": 41.67108154296875,
      "learning_rate": 1.9888097660223807e-05,
      "loss": 1.7766,
      "step": 59200
    },
    {
      "epoch": 30.116988809766024,
      "grad_norm": 39.99887466430664,
      "learning_rate": 1.988301119023398e-05,
      "loss": 1.6996,
      "step": 59210
    },
    {
      "epoch": 30.12207527975585,
      "grad_norm": 36.75379943847656,
      "learning_rate": 1.987792472024415e-05,
      "loss": 1.7872,
      "step": 59220
    },
    {
      "epoch": 30.127161749745678,
      "grad_norm": 38.785369873046875,
      "learning_rate": 1.9872838250254324e-05,
      "loss": 1.8069,
      "step": 59230
    },
    {
      "epoch": 30.132248219735505,
      "grad_norm": 43.075599670410156,
      "learning_rate": 1.98677517802645e-05,
      "loss": 1.7284,
      "step": 59240
    },
    {
      "epoch": 30.137334689725332,
      "grad_norm": 34.653194427490234,
      "learning_rate": 1.986266531027467e-05,
      "loss": 1.7829,
      "step": 59250
    },
    {
      "epoch": 30.14242115971516,
      "grad_norm": 30.8913516998291,
      "learning_rate": 1.9857578840284843e-05,
      "loss": 1.8235,
      "step": 59260
    },
    {
      "epoch": 30.147507629704986,
      "grad_norm": 32.88140869140625,
      "learning_rate": 1.9852492370295017e-05,
      "loss": 1.7884,
      "step": 59270
    },
    {
      "epoch": 30.152594099694813,
      "grad_norm": 37.04851150512695,
      "learning_rate": 1.9847405900305187e-05,
      "loss": 1.7962,
      "step": 59280
    },
    {
      "epoch": 30.15768056968464,
      "grad_norm": 35.36301803588867,
      "learning_rate": 1.9842319430315363e-05,
      "loss": 1.683,
      "step": 59290
    },
    {
      "epoch": 30.162767039674467,
      "grad_norm": 29.206789016723633,
      "learning_rate": 1.9837232960325536e-05,
      "loss": 1.745,
      "step": 59300
    },
    {
      "epoch": 30.167853509664294,
      "grad_norm": 46.77309036254883,
      "learning_rate": 1.9832146490335706e-05,
      "loss": 1.7839,
      "step": 59310
    },
    {
      "epoch": 30.17293997965412,
      "grad_norm": 37.93008041381836,
      "learning_rate": 1.982706002034588e-05,
      "loss": 1.723,
      "step": 59320
    },
    {
      "epoch": 30.178026449643948,
      "grad_norm": 36.71625900268555,
      "learning_rate": 1.9821973550356053e-05,
      "loss": 1.8613,
      "step": 59330
    },
    {
      "epoch": 30.183112919633775,
      "grad_norm": 43.949920654296875,
      "learning_rate": 1.9816887080366226e-05,
      "loss": 1.7636,
      "step": 59340
    },
    {
      "epoch": 30.188199389623602,
      "grad_norm": 38.42070770263672,
      "learning_rate": 1.98118006103764e-05,
      "loss": 1.7931,
      "step": 59350
    },
    {
      "epoch": 30.19328585961343,
      "grad_norm": 32.406227111816406,
      "learning_rate": 1.9806714140386573e-05,
      "loss": 1.7566,
      "step": 59360
    },
    {
      "epoch": 30.198372329603256,
      "grad_norm": 38.3922119140625,
      "learning_rate": 1.9801627670396746e-05,
      "loss": 1.7665,
      "step": 59370
    },
    {
      "epoch": 30.203458799593083,
      "grad_norm": 36.11747741699219,
      "learning_rate": 1.979654120040692e-05,
      "loss": 1.7653,
      "step": 59380
    },
    {
      "epoch": 30.20854526958291,
      "grad_norm": 47.125003814697266,
      "learning_rate": 1.9791454730417093e-05,
      "loss": 1.727,
      "step": 59390
    },
    {
      "epoch": 30.213631739572737,
      "grad_norm": 33.673240661621094,
      "learning_rate": 1.9786368260427266e-05,
      "loss": 1.7996,
      "step": 59400
    },
    {
      "epoch": 30.218718209562564,
      "grad_norm": 35.677852630615234,
      "learning_rate": 1.9781281790437436e-05,
      "loss": 1.8707,
      "step": 59410
    },
    {
      "epoch": 30.22380467955239,
      "grad_norm": 40.30497360229492,
      "learning_rate": 1.977619532044761e-05,
      "loss": 1.7934,
      "step": 59420
    },
    {
      "epoch": 30.228891149542218,
      "grad_norm": 39.482810974121094,
      "learning_rate": 1.9771108850457782e-05,
      "loss": 1.7073,
      "step": 59430
    },
    {
      "epoch": 30.233977619532045,
      "grad_norm": 42.936378479003906,
      "learning_rate": 1.9766022380467956e-05,
      "loss": 1.745,
      "step": 59440
    },
    {
      "epoch": 30.23906408952187,
      "grad_norm": 37.073001861572266,
      "learning_rate": 1.976093591047813e-05,
      "loss": 1.8287,
      "step": 59450
    },
    {
      "epoch": 30.2441505595117,
      "grad_norm": 34.67564392089844,
      "learning_rate": 1.9755849440488302e-05,
      "loss": 1.758,
      "step": 59460
    },
    {
      "epoch": 30.249237029501526,
      "grad_norm": 35.444122314453125,
      "learning_rate": 1.9750762970498472e-05,
      "loss": 1.8271,
      "step": 59470
    },
    {
      "epoch": 30.254323499491353,
      "grad_norm": 32.33348083496094,
      "learning_rate": 1.974567650050865e-05,
      "loss": 1.82,
      "step": 59480
    },
    {
      "epoch": 30.25940996948118,
      "grad_norm": 38.71125030517578,
      "learning_rate": 1.9740590030518822e-05,
      "loss": 1.8132,
      "step": 59490
    },
    {
      "epoch": 30.264496439471007,
      "grad_norm": 36.34281921386719,
      "learning_rate": 1.9735503560528995e-05,
      "loss": 1.828,
      "step": 59500
    },
    {
      "epoch": 30.269582909460834,
      "grad_norm": 38.890987396240234,
      "learning_rate": 1.9730417090539165e-05,
      "loss": 1.7456,
      "step": 59510
    },
    {
      "epoch": 30.27466937945066,
      "grad_norm": 41.26475524902344,
      "learning_rate": 1.972533062054934e-05,
      "loss": 1.7338,
      "step": 59520
    },
    {
      "epoch": 30.279755849440487,
      "grad_norm": 42.43794631958008,
      "learning_rate": 1.9720244150559515e-05,
      "loss": 1.8093,
      "step": 59530
    },
    {
      "epoch": 30.284842319430314,
      "grad_norm": 43.355506896972656,
      "learning_rate": 1.9715157680569685e-05,
      "loss": 1.8418,
      "step": 59540
    },
    {
      "epoch": 30.28992878942014,
      "grad_norm": 32.552127838134766,
      "learning_rate": 1.971007121057986e-05,
      "loss": 1.7894,
      "step": 59550
    },
    {
      "epoch": 30.29501525940997,
      "grad_norm": 41.40239715576172,
      "learning_rate": 1.970498474059003e-05,
      "loss": 1.7449,
      "step": 59560
    },
    {
      "epoch": 30.300101729399795,
      "grad_norm": 40.26655578613281,
      "learning_rate": 1.9699898270600205e-05,
      "loss": 1.7845,
      "step": 59570
    },
    {
      "epoch": 30.305188199389622,
      "grad_norm": 37.200897216796875,
      "learning_rate": 1.9694811800610378e-05,
      "loss": 1.8186,
      "step": 59580
    },
    {
      "epoch": 30.31027466937945,
      "grad_norm": 63.28391647338867,
      "learning_rate": 1.968972533062055e-05,
      "loss": 1.6673,
      "step": 59590
    },
    {
      "epoch": 30.315361139369276,
      "grad_norm": 35.980865478515625,
      "learning_rate": 1.968463886063072e-05,
      "loss": 1.8662,
      "step": 59600
    },
    {
      "epoch": 30.320447609359103,
      "grad_norm": 38.68909454345703,
      "learning_rate": 1.9679552390640895e-05,
      "loss": 1.7265,
      "step": 59610
    },
    {
      "epoch": 30.32553407934893,
      "grad_norm": 38.657588958740234,
      "learning_rate": 1.9674465920651068e-05,
      "loss": 1.819,
      "step": 59620
    },
    {
      "epoch": 30.330620549338757,
      "grad_norm": 38.34850311279297,
      "learning_rate": 1.9669379450661245e-05,
      "loss": 1.8293,
      "step": 59630
    },
    {
      "epoch": 30.335707019328584,
      "grad_norm": 42.53168487548828,
      "learning_rate": 1.9664292980671415e-05,
      "loss": 1.8391,
      "step": 59640
    },
    {
      "epoch": 30.340793489318415,
      "grad_norm": 52.28428649902344,
      "learning_rate": 1.9659206510681588e-05,
      "loss": 1.8792,
      "step": 59650
    },
    {
      "epoch": 30.345879959308242,
      "grad_norm": 40.7051887512207,
      "learning_rate": 1.965412004069176e-05,
      "loss": 1.7875,
      "step": 59660
    },
    {
      "epoch": 30.35096642929807,
      "grad_norm": 33.75398254394531,
      "learning_rate": 1.9649033570701934e-05,
      "loss": 1.7726,
      "step": 59670
    },
    {
      "epoch": 30.356052899287896,
      "grad_norm": 50.19467544555664,
      "learning_rate": 1.9643947100712108e-05,
      "loss": 1.787,
      "step": 59680
    },
    {
      "epoch": 30.361139369277723,
      "grad_norm": 37.07197189331055,
      "learning_rate": 1.963886063072228e-05,
      "loss": 1.7405,
      "step": 59690
    },
    {
      "epoch": 30.36622583926755,
      "grad_norm": 36.19366455078125,
      "learning_rate": 1.963377416073245e-05,
      "loss": 1.8592,
      "step": 59700
    },
    {
      "epoch": 30.371312309257377,
      "grad_norm": 42.01773452758789,
      "learning_rate": 1.9628687690742624e-05,
      "loss": 1.761,
      "step": 59710
    },
    {
      "epoch": 30.376398779247204,
      "grad_norm": 44.74338150024414,
      "learning_rate": 1.96236012207528e-05,
      "loss": 1.7,
      "step": 59720
    },
    {
      "epoch": 30.38148524923703,
      "grad_norm": 36.148677825927734,
      "learning_rate": 1.961851475076297e-05,
      "loss": 1.9205,
      "step": 59730
    },
    {
      "epoch": 30.386571719226858,
      "grad_norm": 44.6973991394043,
      "learning_rate": 1.9613428280773144e-05,
      "loss": 1.878,
      "step": 59740
    },
    {
      "epoch": 30.391658189216685,
      "grad_norm": 35.22710037231445,
      "learning_rate": 1.9608341810783317e-05,
      "loss": 1.7472,
      "step": 59750
    },
    {
      "epoch": 30.39674465920651,
      "grad_norm": 38.2351188659668,
      "learning_rate": 1.960325534079349e-05,
      "loss": 1.8102,
      "step": 59760
    },
    {
      "epoch": 30.40183112919634,
      "grad_norm": 33.542259216308594,
      "learning_rate": 1.9598168870803664e-05,
      "loss": 1.8201,
      "step": 59770
    },
    {
      "epoch": 30.406917599186166,
      "grad_norm": 33.579689025878906,
      "learning_rate": 1.9593082400813837e-05,
      "loss": 1.7179,
      "step": 59780
    },
    {
      "epoch": 30.412004069175993,
      "grad_norm": 33.05765151977539,
      "learning_rate": 1.958799593082401e-05,
      "loss": 1.722,
      "step": 59790
    },
    {
      "epoch": 30.41709053916582,
      "grad_norm": 39.71815872192383,
      "learning_rate": 1.958290946083418e-05,
      "loss": 1.7377,
      "step": 59800
    },
    {
      "epoch": 30.422177009155646,
      "grad_norm": 37.037261962890625,
      "learning_rate": 1.9577822990844354e-05,
      "loss": 1.8048,
      "step": 59810
    },
    {
      "epoch": 30.427263479145473,
      "grad_norm": 31.1098575592041,
      "learning_rate": 1.957273652085453e-05,
      "loss": 1.8167,
      "step": 59820
    },
    {
      "epoch": 30.4323499491353,
      "grad_norm": 35.14183044433594,
      "learning_rate": 1.95676500508647e-05,
      "loss": 1.8166,
      "step": 59830
    },
    {
      "epoch": 30.437436419125127,
      "grad_norm": 36.0909538269043,
      "learning_rate": 1.9562563580874873e-05,
      "loss": 1.7899,
      "step": 59840
    },
    {
      "epoch": 30.442522889114954,
      "grad_norm": 47.263916015625,
      "learning_rate": 1.9557477110885047e-05,
      "loss": 1.787,
      "step": 59850
    },
    {
      "epoch": 30.44760935910478,
      "grad_norm": 46.02203369140625,
      "learning_rate": 1.955239064089522e-05,
      "loss": 1.7126,
      "step": 59860
    },
    {
      "epoch": 30.45269582909461,
      "grad_norm": 41.82244873046875,
      "learning_rate": 1.9547304170905393e-05,
      "loss": 1.8033,
      "step": 59870
    },
    {
      "epoch": 30.457782299084435,
      "grad_norm": 37.96708297729492,
      "learning_rate": 1.9542217700915566e-05,
      "loss": 1.764,
      "step": 59880
    },
    {
      "epoch": 30.462868769074262,
      "grad_norm": 32.146629333496094,
      "learning_rate": 1.953713123092574e-05,
      "loss": 1.7426,
      "step": 59890
    },
    {
      "epoch": 30.46795523906409,
      "grad_norm": 43.45884704589844,
      "learning_rate": 1.953204476093591e-05,
      "loss": 1.73,
      "step": 59900
    },
    {
      "epoch": 30.473041709053916,
      "grad_norm": 36.12677001953125,
      "learning_rate": 1.9526958290946086e-05,
      "loss": 1.7943,
      "step": 59910
    },
    {
      "epoch": 30.478128179043743,
      "grad_norm": 34.376461029052734,
      "learning_rate": 1.952187182095626e-05,
      "loss": 1.7853,
      "step": 59920
    },
    {
      "epoch": 30.48321464903357,
      "grad_norm": 47.26095962524414,
      "learning_rate": 1.951678535096643e-05,
      "loss": 1.791,
      "step": 59930
    },
    {
      "epoch": 30.488301119023397,
      "grad_norm": 42.53904342651367,
      "learning_rate": 1.9511698880976603e-05,
      "loss": 1.8134,
      "step": 59940
    },
    {
      "epoch": 30.493387589013224,
      "grad_norm": 39.56831741333008,
      "learning_rate": 1.9506612410986776e-05,
      "loss": 1.7353,
      "step": 59950
    },
    {
      "epoch": 30.49847405900305,
      "grad_norm": 41.60362243652344,
      "learning_rate": 1.950152594099695e-05,
      "loss": 1.8182,
      "step": 59960
    },
    {
      "epoch": 30.503560528992878,
      "grad_norm": 32.8188362121582,
      "learning_rate": 1.9496439471007123e-05,
      "loss": 1.7463,
      "step": 59970
    },
    {
      "epoch": 30.508646998982705,
      "grad_norm": 36.4378547668457,
      "learning_rate": 1.9491353001017296e-05,
      "loss": 1.7025,
      "step": 59980
    },
    {
      "epoch": 30.513733468972532,
      "grad_norm": 42.11046600341797,
      "learning_rate": 1.9486266531027466e-05,
      "loss": 1.7268,
      "step": 59990
    },
    {
      "epoch": 30.51881993896236,
      "grad_norm": 38.744110107421875,
      "learning_rate": 1.948118006103764e-05,
      "loss": 1.7028,
      "step": 60000
    },
    {
      "epoch": 30.523906408952186,
      "grad_norm": 39.21582794189453,
      "learning_rate": 1.9476093591047816e-05,
      "loss": 1.709,
      "step": 60010
    },
    {
      "epoch": 30.528992878942013,
      "grad_norm": 33.7628173828125,
      "learning_rate": 1.947100712105799e-05,
      "loss": 1.8223,
      "step": 60020
    },
    {
      "epoch": 30.53407934893184,
      "grad_norm": 34.181182861328125,
      "learning_rate": 1.946592065106816e-05,
      "loss": 1.7229,
      "step": 60030
    },
    {
      "epoch": 30.539165818921667,
      "grad_norm": 32.135704040527344,
      "learning_rate": 1.9460834181078332e-05,
      "loss": 1.7269,
      "step": 60040
    },
    {
      "epoch": 30.544252288911494,
      "grad_norm": 36.59146499633789,
      "learning_rate": 1.9455747711088506e-05,
      "loss": 1.7479,
      "step": 60050
    },
    {
      "epoch": 30.54933875890132,
      "grad_norm": 41.82394790649414,
      "learning_rate": 1.945066124109868e-05,
      "loss": 1.7713,
      "step": 60060
    },
    {
      "epoch": 30.554425228891148,
      "grad_norm": 43.64938735961914,
      "learning_rate": 1.9445574771108852e-05,
      "loss": 1.8676,
      "step": 60070
    },
    {
      "epoch": 30.559511698880975,
      "grad_norm": 37.555320739746094,
      "learning_rate": 1.9440488301119025e-05,
      "loss": 1.7868,
      "step": 60080
    },
    {
      "epoch": 30.564598168870802,
      "grad_norm": 35.06143569946289,
      "learning_rate": 1.9435401831129195e-05,
      "loss": 1.766,
      "step": 60090
    },
    {
      "epoch": 30.56968463886063,
      "grad_norm": 35.94019317626953,
      "learning_rate": 1.943031536113937e-05,
      "loss": 1.7336,
      "step": 60100
    },
    {
      "epoch": 30.57477110885046,
      "grad_norm": 42.091392517089844,
      "learning_rate": 1.9425228891149545e-05,
      "loss": 1.7774,
      "step": 60110
    },
    {
      "epoch": 30.579857578840286,
      "grad_norm": 41.18055725097656,
      "learning_rate": 1.9420142421159715e-05,
      "loss": 1.7746,
      "step": 60120
    },
    {
      "epoch": 30.584944048830113,
      "grad_norm": 44.5437126159668,
      "learning_rate": 1.941505595116989e-05,
      "loss": 1.7482,
      "step": 60130
    },
    {
      "epoch": 30.59003051881994,
      "grad_norm": 33.01371765136719,
      "learning_rate": 1.940996948118006e-05,
      "loss": 1.7135,
      "step": 60140
    },
    {
      "epoch": 30.595116988809767,
      "grad_norm": 44.9420166015625,
      "learning_rate": 1.9404883011190235e-05,
      "loss": 1.7589,
      "step": 60150
    },
    {
      "epoch": 30.600203458799594,
      "grad_norm": 48.52125549316406,
      "learning_rate": 1.9399796541200408e-05,
      "loss": 1.7505,
      "step": 60160
    },
    {
      "epoch": 30.60528992878942,
      "grad_norm": 32.89174270629883,
      "learning_rate": 1.939471007121058e-05,
      "loss": 1.7469,
      "step": 60170
    },
    {
      "epoch": 30.61037639877925,
      "grad_norm": 43.45581817626953,
      "learning_rate": 1.9389623601220755e-05,
      "loss": 1.863,
      "step": 60180
    },
    {
      "epoch": 30.615462868769075,
      "grad_norm": 42.41881561279297,
      "learning_rate": 1.9384537131230925e-05,
      "loss": 1.8305,
      "step": 60190
    },
    {
      "epoch": 30.620549338758902,
      "grad_norm": 32.89965057373047,
      "learning_rate": 1.93794506612411e-05,
      "loss": 1.7932,
      "step": 60200
    },
    {
      "epoch": 30.62563580874873,
      "grad_norm": 39.770912170410156,
      "learning_rate": 1.9374364191251275e-05,
      "loss": 1.8342,
      "step": 60210
    },
    {
      "epoch": 30.630722278738556,
      "grad_norm": 34.23653030395508,
      "learning_rate": 1.9369277721261445e-05,
      "loss": 1.84,
      "step": 60220
    },
    {
      "epoch": 30.635808748728383,
      "grad_norm": 37.49974822998047,
      "learning_rate": 1.9364191251271618e-05,
      "loss": 1.7593,
      "step": 60230
    },
    {
      "epoch": 30.64089521871821,
      "grad_norm": 34.905723571777344,
      "learning_rate": 1.935910478128179e-05,
      "loss": 1.7724,
      "step": 60240
    },
    {
      "epoch": 30.645981688708037,
      "grad_norm": 34.085121154785156,
      "learning_rate": 1.9354018311291964e-05,
      "loss": 1.7864,
      "step": 60250
    },
    {
      "epoch": 30.651068158697864,
      "grad_norm": 40.69936752319336,
      "learning_rate": 1.9348931841302138e-05,
      "loss": 1.7143,
      "step": 60260
    },
    {
      "epoch": 30.65615462868769,
      "grad_norm": 48.67129135131836,
      "learning_rate": 1.934384537131231e-05,
      "loss": 1.7381,
      "step": 60270
    },
    {
      "epoch": 30.661241098677518,
      "grad_norm": 47.175750732421875,
      "learning_rate": 1.933875890132248e-05,
      "loss": 1.8204,
      "step": 60280
    },
    {
      "epoch": 30.666327568667345,
      "grad_norm": 35.29282760620117,
      "learning_rate": 1.9333672431332654e-05,
      "loss": 1.724,
      "step": 60290
    },
    {
      "epoch": 30.671414038657172,
      "grad_norm": 34.61269760131836,
      "learning_rate": 1.932858596134283e-05,
      "loss": 1.7307,
      "step": 60300
    },
    {
      "epoch": 30.676500508647,
      "grad_norm": 39.56231689453125,
      "learning_rate": 1.9323499491353004e-05,
      "loss": 1.7795,
      "step": 60310
    },
    {
      "epoch": 30.681586978636826,
      "grad_norm": 38.78199768066406,
      "learning_rate": 1.9318413021363174e-05,
      "loss": 1.8248,
      "step": 60320
    },
    {
      "epoch": 30.686673448626653,
      "grad_norm": 44.754722595214844,
      "learning_rate": 1.9313326551373347e-05,
      "loss": 1.8005,
      "step": 60330
    },
    {
      "epoch": 30.69175991861648,
      "grad_norm": 32.47549819946289,
      "learning_rate": 1.930824008138352e-05,
      "loss": 1.7712,
      "step": 60340
    },
    {
      "epoch": 30.696846388606307,
      "grad_norm": 35.343074798583984,
      "learning_rate": 1.9303153611393694e-05,
      "loss": 1.7556,
      "step": 60350
    },
    {
      "epoch": 30.701932858596134,
      "grad_norm": 35.70362091064453,
      "learning_rate": 1.9298067141403867e-05,
      "loss": 1.6364,
      "step": 60360
    },
    {
      "epoch": 30.70701932858596,
      "grad_norm": 42.053096771240234,
      "learning_rate": 1.929298067141404e-05,
      "loss": 1.7274,
      "step": 60370
    },
    {
      "epoch": 30.712105798575788,
      "grad_norm": 38.95115280151367,
      "learning_rate": 1.928789420142421e-05,
      "loss": 1.6932,
      "step": 60380
    },
    {
      "epoch": 30.717192268565615,
      "grad_norm": 51.900001525878906,
      "learning_rate": 1.9282807731434387e-05,
      "loss": 1.8706,
      "step": 60390
    },
    {
      "epoch": 30.722278738555442,
      "grad_norm": 42.5277214050293,
      "learning_rate": 1.927772126144456e-05,
      "loss": 1.7651,
      "step": 60400
    },
    {
      "epoch": 30.72736520854527,
      "grad_norm": 25.283329010009766,
      "learning_rate": 1.927263479145473e-05,
      "loss": 1.8307,
      "step": 60410
    },
    {
      "epoch": 30.732451678535096,
      "grad_norm": 37.250911712646484,
      "learning_rate": 1.9267548321464903e-05,
      "loss": 1.7519,
      "step": 60420
    },
    {
      "epoch": 30.737538148524923,
      "grad_norm": 55.05501174926758,
      "learning_rate": 1.9262461851475077e-05,
      "loss": 1.7446,
      "step": 60430
    },
    {
      "epoch": 30.74262461851475,
      "grad_norm": 38.7212028503418,
      "learning_rate": 1.925737538148525e-05,
      "loss": 1.7729,
      "step": 60440
    },
    {
      "epoch": 30.747711088504577,
      "grad_norm": 37.462547302246094,
      "learning_rate": 1.9252288911495423e-05,
      "loss": 1.749,
      "step": 60450
    },
    {
      "epoch": 30.752797558494404,
      "grad_norm": 45.189208984375,
      "learning_rate": 1.9247202441505596e-05,
      "loss": 1.7428,
      "step": 60460
    },
    {
      "epoch": 30.75788402848423,
      "grad_norm": 38.67606735229492,
      "learning_rate": 1.924211597151577e-05,
      "loss": 1.7688,
      "step": 60470
    },
    {
      "epoch": 30.762970498474058,
      "grad_norm": 40.909786224365234,
      "learning_rate": 1.923702950152594e-05,
      "loss": 1.7738,
      "step": 60480
    },
    {
      "epoch": 30.768056968463885,
      "grad_norm": 34.05377197265625,
      "learning_rate": 1.9231943031536116e-05,
      "loss": 1.732,
      "step": 60490
    },
    {
      "epoch": 30.77314343845371,
      "grad_norm": 34.8040885925293,
      "learning_rate": 1.922685656154629e-05,
      "loss": 1.7646,
      "step": 60500
    },
    {
      "epoch": 30.77822990844354,
      "grad_norm": 41.94587326049805,
      "learning_rate": 1.922177009155646e-05,
      "loss": 1.7757,
      "step": 60510
    },
    {
      "epoch": 30.783316378433366,
      "grad_norm": 39.24787521362305,
      "learning_rate": 1.9216683621566633e-05,
      "loss": 1.7954,
      "step": 60520
    },
    {
      "epoch": 30.788402848423193,
      "grad_norm": 33.6678581237793,
      "learning_rate": 1.9211597151576806e-05,
      "loss": 1.7549,
      "step": 60530
    },
    {
      "epoch": 30.793489318413023,
      "grad_norm": 41.78901290893555,
      "learning_rate": 1.920651068158698e-05,
      "loss": 1.7354,
      "step": 60540
    },
    {
      "epoch": 30.79857578840285,
      "grad_norm": 34.14809799194336,
      "learning_rate": 1.9201424211597153e-05,
      "loss": 1.7605,
      "step": 60550
    },
    {
      "epoch": 30.803662258392677,
      "grad_norm": 34.30134963989258,
      "learning_rate": 1.9196337741607326e-05,
      "loss": 1.7845,
      "step": 60560
    },
    {
      "epoch": 30.808748728382504,
      "grad_norm": 37.93326187133789,
      "learning_rate": 1.91912512716175e-05,
      "loss": 1.7056,
      "step": 60570
    },
    {
      "epoch": 30.81383519837233,
      "grad_norm": 40.53284454345703,
      "learning_rate": 1.918616480162767e-05,
      "loss": 1.7241,
      "step": 60580
    },
    {
      "epoch": 30.818921668362158,
      "grad_norm": 38.5233268737793,
      "learning_rate": 1.9181078331637846e-05,
      "loss": 1.7907,
      "step": 60590
    },
    {
      "epoch": 30.824008138351985,
      "grad_norm": 37.993045806884766,
      "learning_rate": 1.917599186164802e-05,
      "loss": 1.7235,
      "step": 60600
    },
    {
      "epoch": 30.829094608341812,
      "grad_norm": 46.514015197753906,
      "learning_rate": 1.917090539165819e-05,
      "loss": 1.7828,
      "step": 60610
    },
    {
      "epoch": 30.83418107833164,
      "grad_norm": 41.948890686035156,
      "learning_rate": 1.9165818921668362e-05,
      "loss": 1.7888,
      "step": 60620
    },
    {
      "epoch": 30.839267548321466,
      "grad_norm": 42.881534576416016,
      "learning_rate": 1.9160732451678536e-05,
      "loss": 1.799,
      "step": 60630
    },
    {
      "epoch": 30.844354018311293,
      "grad_norm": 35.17768478393555,
      "learning_rate": 1.915564598168871e-05,
      "loss": 1.7992,
      "step": 60640
    },
    {
      "epoch": 30.84944048830112,
      "grad_norm": 37.50973129272461,
      "learning_rate": 1.9150559511698882e-05,
      "loss": 1.7609,
      "step": 60650
    },
    {
      "epoch": 30.854526958290947,
      "grad_norm": 32.36540985107422,
      "learning_rate": 1.9145473041709055e-05,
      "loss": 1.8212,
      "step": 60660
    },
    {
      "epoch": 30.859613428280774,
      "grad_norm": 35.95819854736328,
      "learning_rate": 1.9140386571719225e-05,
      "loss": 1.853,
      "step": 60670
    },
    {
      "epoch": 30.8646998982706,
      "grad_norm": 41.5563850402832,
      "learning_rate": 1.9135300101729402e-05,
      "loss": 1.8802,
      "step": 60680
    },
    {
      "epoch": 30.869786368260428,
      "grad_norm": 41.09843444824219,
      "learning_rate": 1.9130213631739575e-05,
      "loss": 1.7985,
      "step": 60690
    },
    {
      "epoch": 30.874872838250255,
      "grad_norm": 32.75691223144531,
      "learning_rate": 1.912512716174975e-05,
      "loss": 1.7122,
      "step": 60700
    },
    {
      "epoch": 30.879959308240082,
      "grad_norm": 42.99653244018555,
      "learning_rate": 1.912004069175992e-05,
      "loss": 1.8257,
      "step": 60710
    },
    {
      "epoch": 30.88504577822991,
      "grad_norm": 42.375919342041016,
      "learning_rate": 1.911495422177009e-05,
      "loss": 1.7889,
      "step": 60720
    },
    {
      "epoch": 30.890132248219736,
      "grad_norm": 38.55585861206055,
      "learning_rate": 1.9109867751780265e-05,
      "loss": 1.7522,
      "step": 60730
    },
    {
      "epoch": 30.895218718209563,
      "grad_norm": 30.68817901611328,
      "learning_rate": 1.9104781281790438e-05,
      "loss": 1.8039,
      "step": 60740
    },
    {
      "epoch": 30.90030518819939,
      "grad_norm": 38.58570861816406,
      "learning_rate": 1.909969481180061e-05,
      "loss": 1.7206,
      "step": 60750
    },
    {
      "epoch": 30.905391658189217,
      "grad_norm": 35.69130325317383,
      "learning_rate": 1.9094608341810785e-05,
      "loss": 1.8814,
      "step": 60760
    },
    {
      "epoch": 30.910478128179044,
      "grad_norm": 37.29463195800781,
      "learning_rate": 1.9089521871820955e-05,
      "loss": 1.7425,
      "step": 60770
    },
    {
      "epoch": 30.91556459816887,
      "grad_norm": 34.7082633972168,
      "learning_rate": 1.908443540183113e-05,
      "loss": 1.8194,
      "step": 60780
    },
    {
      "epoch": 30.920651068158698,
      "grad_norm": 37.089412689208984,
      "learning_rate": 1.9079348931841305e-05,
      "loss": 1.7499,
      "step": 60790
    },
    {
      "epoch": 30.925737538148525,
      "grad_norm": 45.847740173339844,
      "learning_rate": 1.9074262461851475e-05,
      "loss": 1.7306,
      "step": 60800
    },
    {
      "epoch": 30.93082400813835,
      "grad_norm": 41.66584777832031,
      "learning_rate": 1.9069175991861648e-05,
      "loss": 1.7254,
      "step": 60810
    },
    {
      "epoch": 30.93591047812818,
      "grad_norm": 34.1158447265625,
      "learning_rate": 1.906408952187182e-05,
      "loss": 1.7501,
      "step": 60820
    },
    {
      "epoch": 30.940996948118006,
      "grad_norm": 46.675350189208984,
      "learning_rate": 1.9059003051881998e-05,
      "loss": 1.7534,
      "step": 60830
    },
    {
      "epoch": 30.946083418107833,
      "grad_norm": 30.950607299804688,
      "learning_rate": 1.9053916581892168e-05,
      "loss": 1.7833,
      "step": 60840
    },
    {
      "epoch": 30.95116988809766,
      "grad_norm": 32.479652404785156,
      "learning_rate": 1.904883011190234e-05,
      "loss": 1.755,
      "step": 60850
    },
    {
      "epoch": 30.956256358087487,
      "grad_norm": 28.703880310058594,
      "learning_rate": 1.9043743641912514e-05,
      "loss": 1.7619,
      "step": 60860
    },
    {
      "epoch": 30.961342828077314,
      "grad_norm": 48.49419021606445,
      "learning_rate": 1.9038657171922687e-05,
      "loss": 1.7663,
      "step": 60870
    },
    {
      "epoch": 30.96642929806714,
      "grad_norm": 51.10711669921875,
      "learning_rate": 1.903357070193286e-05,
      "loss": 1.7343,
      "step": 60880
    },
    {
      "epoch": 30.971515768056967,
      "grad_norm": 56.49894714355469,
      "learning_rate": 1.9028484231943034e-05,
      "loss": 1.7584,
      "step": 60890
    },
    {
      "epoch": 30.976602238046794,
      "grad_norm": 38.768638610839844,
      "learning_rate": 1.9023397761953204e-05,
      "loss": 1.8423,
      "step": 60900
    },
    {
      "epoch": 30.98168870803662,
      "grad_norm": 33.56474685668945,
      "learning_rate": 1.9018311291963377e-05,
      "loss": 1.849,
      "step": 60910
    },
    {
      "epoch": 30.98677517802645,
      "grad_norm": 36.42937088012695,
      "learning_rate": 1.901322482197355e-05,
      "loss": 1.7522,
      "step": 60920
    },
    {
      "epoch": 30.991861648016275,
      "grad_norm": 36.107032775878906,
      "learning_rate": 1.9008138351983724e-05,
      "loss": 1.7696,
      "step": 60930
    },
    {
      "epoch": 30.996948118006102,
      "grad_norm": 28.472816467285156,
      "learning_rate": 1.9003051881993897e-05,
      "loss": 1.772,
      "step": 60940
    },
    {
      "epoch": 31.0,
      "eval_loss": 4.760265350341797,
      "eval_runtime": 2.8107,
      "eval_samples_per_second": 987.286,
      "eval_steps_per_second": 123.455,
      "step": 60946
    },
    {
      "epoch": 31.00203458799593,
      "grad_norm": 40.27886962890625,
      "learning_rate": 1.899796541200407e-05,
      "loss": 1.7537,
      "step": 60950
    },
    {
      "epoch": 31.007121057985756,
      "grad_norm": 39.36678695678711,
      "learning_rate": 1.899287894201424e-05,
      "loss": 1.8005,
      "step": 60960
    },
    {
      "epoch": 31.012207527975583,
      "grad_norm": 52.15998077392578,
      "learning_rate": 1.8987792472024417e-05,
      "loss": 1.7796,
      "step": 60970
    },
    {
      "epoch": 31.01729399796541,
      "grad_norm": 41.145957946777344,
      "learning_rate": 1.898270600203459e-05,
      "loss": 1.7528,
      "step": 60980
    },
    {
      "epoch": 31.022380467955237,
      "grad_norm": 39.94182586669922,
      "learning_rate": 1.8977619532044763e-05,
      "loss": 1.781,
      "step": 60990
    },
    {
      "epoch": 31.027466937945068,
      "grad_norm": 55.79063034057617,
      "learning_rate": 1.8972533062054933e-05,
      "loss": 1.7847,
      "step": 61000
    },
    {
      "epoch": 31.032553407934895,
      "grad_norm": 35.6830940246582,
      "learning_rate": 1.8967446592065107e-05,
      "loss": 1.7556,
      "step": 61010
    },
    {
      "epoch": 31.037639877924722,
      "grad_norm": 44.669246673583984,
      "learning_rate": 1.8962360122075283e-05,
      "loss": 1.8237,
      "step": 61020
    },
    {
      "epoch": 31.04272634791455,
      "grad_norm": 41.495338439941406,
      "learning_rate": 1.8957273652085453e-05,
      "loss": 1.7138,
      "step": 61030
    },
    {
      "epoch": 31.047812817904376,
      "grad_norm": 46.683265686035156,
      "learning_rate": 1.8952187182095627e-05,
      "loss": 1.8224,
      "step": 61040
    },
    {
      "epoch": 31.052899287894203,
      "grad_norm": 37.718318939208984,
      "learning_rate": 1.89471007121058e-05,
      "loss": 1.7131,
      "step": 61050
    },
    {
      "epoch": 31.05798575788403,
      "grad_norm": 35.99467849731445,
      "learning_rate": 1.894201424211597e-05,
      "loss": 1.8266,
      "step": 61060
    },
    {
      "epoch": 31.063072227873857,
      "grad_norm": 34.8369140625,
      "learning_rate": 1.8936927772126146e-05,
      "loss": 1.7539,
      "step": 61070
    },
    {
      "epoch": 31.068158697863684,
      "grad_norm": 41.93960952758789,
      "learning_rate": 1.893184130213632e-05,
      "loss": 1.8031,
      "step": 61080
    },
    {
      "epoch": 31.07324516785351,
      "grad_norm": 44.92223358154297,
      "learning_rate": 1.892675483214649e-05,
      "loss": 1.8138,
      "step": 61090
    },
    {
      "epoch": 31.078331637843338,
      "grad_norm": 48.33223342895508,
      "learning_rate": 1.8921668362156663e-05,
      "loss": 1.8156,
      "step": 61100
    },
    {
      "epoch": 31.083418107833165,
      "grad_norm": 40.867008209228516,
      "learning_rate": 1.8916581892166836e-05,
      "loss": 1.7735,
      "step": 61110
    },
    {
      "epoch": 31.08850457782299,
      "grad_norm": 37.685672760009766,
      "learning_rate": 1.8911495422177013e-05,
      "loss": 1.653,
      "step": 61120
    },
    {
      "epoch": 31.09359104781282,
      "grad_norm": 40.55849075317383,
      "learning_rate": 1.8906408952187183e-05,
      "loss": 1.7184,
      "step": 61130
    },
    {
      "epoch": 31.098677517802646,
      "grad_norm": 34.0133171081543,
      "learning_rate": 1.8901322482197356e-05,
      "loss": 1.7315,
      "step": 61140
    },
    {
      "epoch": 31.103763987792473,
      "grad_norm": 41.967525482177734,
      "learning_rate": 1.889623601220753e-05,
      "loss": 1.733,
      "step": 61150
    },
    {
      "epoch": 31.1088504577823,
      "grad_norm": 40.41710662841797,
      "learning_rate": 1.8891149542217702e-05,
      "loss": 1.8699,
      "step": 61160
    },
    {
      "epoch": 31.113936927772126,
      "grad_norm": 40.03151321411133,
      "learning_rate": 1.8886063072227876e-05,
      "loss": 1.733,
      "step": 61170
    },
    {
      "epoch": 31.119023397761953,
      "grad_norm": 45.29899215698242,
      "learning_rate": 1.888097660223805e-05,
      "loss": 1.744,
      "step": 61180
    },
    {
      "epoch": 31.12410986775178,
      "grad_norm": 42.207252502441406,
      "learning_rate": 1.887589013224822e-05,
      "loss": 1.7346,
      "step": 61190
    },
    {
      "epoch": 31.129196337741607,
      "grad_norm": 31.278179168701172,
      "learning_rate": 1.8870803662258392e-05,
      "loss": 1.6975,
      "step": 61200
    },
    {
      "epoch": 31.134282807731434,
      "grad_norm": 31.744342803955078,
      "learning_rate": 1.8865717192268566e-05,
      "loss": 1.7557,
      "step": 61210
    },
    {
      "epoch": 31.13936927772126,
      "grad_norm": 38.66166687011719,
      "learning_rate": 1.886063072227874e-05,
      "loss": 1.6695,
      "step": 61220
    },
    {
      "epoch": 31.14445574771109,
      "grad_norm": 39.38412857055664,
      "learning_rate": 1.8855544252288912e-05,
      "loss": 1.7757,
      "step": 61230
    },
    {
      "epoch": 31.149542217700915,
      "grad_norm": 36.82821273803711,
      "learning_rate": 1.8850457782299085e-05,
      "loss": 1.7261,
      "step": 61240
    },
    {
      "epoch": 31.154628687690742,
      "grad_norm": 39.193275451660156,
      "learning_rate": 1.884537131230926e-05,
      "loss": 1.7773,
      "step": 61250
    },
    {
      "epoch": 31.15971515768057,
      "grad_norm": 38.040279388427734,
      "learning_rate": 1.8840284842319432e-05,
      "loss": 1.7797,
      "step": 61260
    },
    {
      "epoch": 31.164801627670396,
      "grad_norm": 35.76008605957031,
      "learning_rate": 1.8835198372329605e-05,
      "loss": 1.6552,
      "step": 61270
    },
    {
      "epoch": 31.169888097660223,
      "grad_norm": 31.11970329284668,
      "learning_rate": 1.883011190233978e-05,
      "loss": 1.8418,
      "step": 61280
    },
    {
      "epoch": 31.17497456765005,
      "grad_norm": 43.89621353149414,
      "learning_rate": 1.882502543234995e-05,
      "loss": 1.777,
      "step": 61290
    },
    {
      "epoch": 31.180061037639877,
      "grad_norm": 34.86985778808594,
      "learning_rate": 1.881993896236012e-05,
      "loss": 1.7525,
      "step": 61300
    },
    {
      "epoch": 31.185147507629704,
      "grad_norm": 39.107967376708984,
      "learning_rate": 1.88148524923703e-05,
      "loss": 1.7087,
      "step": 61310
    },
    {
      "epoch": 31.19023397761953,
      "grad_norm": 44.29792404174805,
      "learning_rate": 1.8809766022380468e-05,
      "loss": 1.7678,
      "step": 61320
    },
    {
      "epoch": 31.195320447609358,
      "grad_norm": 38.01856994628906,
      "learning_rate": 1.880467955239064e-05,
      "loss": 1.7754,
      "step": 61330
    },
    {
      "epoch": 31.200406917599185,
      "grad_norm": 36.67540740966797,
      "learning_rate": 1.8799593082400815e-05,
      "loss": 1.7083,
      "step": 61340
    },
    {
      "epoch": 31.205493387589012,
      "grad_norm": 41.94051742553711,
      "learning_rate": 1.8794506612410988e-05,
      "loss": 1.7509,
      "step": 61350
    },
    {
      "epoch": 31.21057985757884,
      "grad_norm": 34.6425895690918,
      "learning_rate": 1.878942014242116e-05,
      "loss": 1.8686,
      "step": 61360
    },
    {
      "epoch": 31.215666327568666,
      "grad_norm": 41.69541931152344,
      "learning_rate": 1.8784333672431335e-05,
      "loss": 1.7315,
      "step": 61370
    },
    {
      "epoch": 31.220752797558493,
      "grad_norm": 40.17936706542969,
      "learning_rate": 1.8779247202441508e-05,
      "loss": 1.7475,
      "step": 61380
    },
    {
      "epoch": 31.22583926754832,
      "grad_norm": 39.5249137878418,
      "learning_rate": 1.8774160732451678e-05,
      "loss": 1.7137,
      "step": 61390
    },
    {
      "epoch": 31.230925737538147,
      "grad_norm": 38.53757095336914,
      "learning_rate": 1.876907426246185e-05,
      "loss": 1.7646,
      "step": 61400
    },
    {
      "epoch": 31.236012207527974,
      "grad_norm": 44.385196685791016,
      "learning_rate": 1.8763987792472028e-05,
      "loss": 1.7313,
      "step": 61410
    },
    {
      "epoch": 31.2410986775178,
      "grad_norm": 37.60477828979492,
      "learning_rate": 1.8758901322482198e-05,
      "loss": 1.7191,
      "step": 61420
    },
    {
      "epoch": 31.246185147507628,
      "grad_norm": 40.587547302246094,
      "learning_rate": 1.875381485249237e-05,
      "loss": 1.72,
      "step": 61430
    },
    {
      "epoch": 31.25127161749746,
      "grad_norm": 37.237091064453125,
      "learning_rate": 1.8748728382502544e-05,
      "loss": 1.7726,
      "step": 61440
    },
    {
      "epoch": 31.256358087487286,
      "grad_norm": 36.61970520019531,
      "learning_rate": 1.8743641912512718e-05,
      "loss": 1.7635,
      "step": 61450
    },
    {
      "epoch": 31.261444557477112,
      "grad_norm": 41.212764739990234,
      "learning_rate": 1.873855544252289e-05,
      "loss": 1.7796,
      "step": 61460
    },
    {
      "epoch": 31.26653102746694,
      "grad_norm": 37.59980392456055,
      "learning_rate": 1.8733468972533064e-05,
      "loss": 1.7266,
      "step": 61470
    },
    {
      "epoch": 31.271617497456766,
      "grad_norm": 35.289756774902344,
      "learning_rate": 1.8728382502543234e-05,
      "loss": 1.7951,
      "step": 61480
    },
    {
      "epoch": 31.276703967446593,
      "grad_norm": 34.716094970703125,
      "learning_rate": 1.8723296032553407e-05,
      "loss": 1.6288,
      "step": 61490
    },
    {
      "epoch": 31.28179043743642,
      "grad_norm": 49.47138977050781,
      "learning_rate": 1.8718209562563584e-05,
      "loss": 1.6887,
      "step": 61500
    },
    {
      "epoch": 31.286876907426247,
      "grad_norm": 32.055091857910156,
      "learning_rate": 1.8713123092573757e-05,
      "loss": 1.7706,
      "step": 61510
    },
    {
      "epoch": 31.291963377416074,
      "grad_norm": 48.08970260620117,
      "learning_rate": 1.8708036622583927e-05,
      "loss": 1.7974,
      "step": 61520
    },
    {
      "epoch": 31.2970498474059,
      "grad_norm": 36.5704345703125,
      "learning_rate": 1.87029501525941e-05,
      "loss": 1.6548,
      "step": 61530
    },
    {
      "epoch": 31.30213631739573,
      "grad_norm": 34.10719680786133,
      "learning_rate": 1.8697863682604274e-05,
      "loss": 1.6826,
      "step": 61540
    },
    {
      "epoch": 31.307222787385555,
      "grad_norm": 35.4705696105957,
      "learning_rate": 1.8692777212614447e-05,
      "loss": 1.8048,
      "step": 61550
    },
    {
      "epoch": 31.312309257375382,
      "grad_norm": 39.98064041137695,
      "learning_rate": 1.868769074262462e-05,
      "loss": 1.8499,
      "step": 61560
    },
    {
      "epoch": 31.31739572736521,
      "grad_norm": 38.647789001464844,
      "learning_rate": 1.8682604272634793e-05,
      "loss": 1.699,
      "step": 61570
    },
    {
      "epoch": 31.322482197355036,
      "grad_norm": 35.83157730102539,
      "learning_rate": 1.8677517802644963e-05,
      "loss": 1.6734,
      "step": 61580
    },
    {
      "epoch": 31.327568667344863,
      "grad_norm": 35.250335693359375,
      "learning_rate": 1.8672431332655137e-05,
      "loss": 1.7442,
      "step": 61590
    },
    {
      "epoch": 31.33265513733469,
      "grad_norm": 38.6570930480957,
      "learning_rate": 1.8667344862665313e-05,
      "loss": 1.752,
      "step": 61600
    },
    {
      "epoch": 31.337741607324517,
      "grad_norm": 39.39155197143555,
      "learning_rate": 1.8662258392675483e-05,
      "loss": 1.7797,
      "step": 61610
    },
    {
      "epoch": 31.342828077314344,
      "grad_norm": 41.93170166015625,
      "learning_rate": 1.8657171922685657e-05,
      "loss": 1.77,
      "step": 61620
    },
    {
      "epoch": 31.34791454730417,
      "grad_norm": 30.606945037841797,
      "learning_rate": 1.865208545269583e-05,
      "loss": 1.7922,
      "step": 61630
    },
    {
      "epoch": 31.353001017293998,
      "grad_norm": 29.545297622680664,
      "learning_rate": 1.8646998982706003e-05,
      "loss": 1.7281,
      "step": 61640
    },
    {
      "epoch": 31.358087487283825,
      "grad_norm": 36.63359069824219,
      "learning_rate": 1.8641912512716176e-05,
      "loss": 1.6841,
      "step": 61650
    },
    {
      "epoch": 31.363173957273652,
      "grad_norm": 39.37704849243164,
      "learning_rate": 1.863682604272635e-05,
      "loss": 1.7893,
      "step": 61660
    },
    {
      "epoch": 31.36826042726348,
      "grad_norm": 38.861324310302734,
      "learning_rate": 1.8631739572736523e-05,
      "loss": 1.6466,
      "step": 61670
    },
    {
      "epoch": 31.373346897253306,
      "grad_norm": 31.966459274291992,
      "learning_rate": 1.8626653102746693e-05,
      "loss": 1.7716,
      "step": 61680
    },
    {
      "epoch": 31.378433367243133,
      "grad_norm": 38.38363265991211,
      "learning_rate": 1.8621566632756866e-05,
      "loss": 1.7857,
      "step": 61690
    },
    {
      "epoch": 31.38351983723296,
      "grad_norm": 39.81328201293945,
      "learning_rate": 1.8616480162767043e-05,
      "loss": 1.7559,
      "step": 61700
    },
    {
      "epoch": 31.388606307222787,
      "grad_norm": 38.846004486083984,
      "learning_rate": 1.8611393692777213e-05,
      "loss": 1.7095,
      "step": 61710
    },
    {
      "epoch": 31.393692777212614,
      "grad_norm": 31.094053268432617,
      "learning_rate": 1.8606307222787386e-05,
      "loss": 1.7889,
      "step": 61720
    },
    {
      "epoch": 31.39877924720244,
      "grad_norm": 36.135921478271484,
      "learning_rate": 1.860122075279756e-05,
      "loss": 1.7301,
      "step": 61730
    },
    {
      "epoch": 31.403865717192268,
      "grad_norm": 39.55466842651367,
      "learning_rate": 1.8596134282807733e-05,
      "loss": 1.7616,
      "step": 61740
    },
    {
      "epoch": 31.408952187182095,
      "grad_norm": 36.48881530761719,
      "learning_rate": 1.8591047812817906e-05,
      "loss": 1.681,
      "step": 61750
    },
    {
      "epoch": 31.414038657171922,
      "grad_norm": 46.577491760253906,
      "learning_rate": 1.858596134282808e-05,
      "loss": 1.7447,
      "step": 61760
    },
    {
      "epoch": 31.41912512716175,
      "grad_norm": 43.04072570800781,
      "learning_rate": 1.858087487283825e-05,
      "loss": 1.6879,
      "step": 61770
    },
    {
      "epoch": 31.424211597151576,
      "grad_norm": 31.314199447631836,
      "learning_rate": 1.8575788402848422e-05,
      "loss": 1.7787,
      "step": 61780
    },
    {
      "epoch": 31.429298067141403,
      "grad_norm": 43.07817077636719,
      "learning_rate": 1.85707019328586e-05,
      "loss": 1.8628,
      "step": 61790
    },
    {
      "epoch": 31.43438453713123,
      "grad_norm": 39.603309631347656,
      "learning_rate": 1.8565615462868772e-05,
      "loss": 1.6346,
      "step": 61800
    },
    {
      "epoch": 31.439471007121057,
      "grad_norm": 39.89573669433594,
      "learning_rate": 1.8560528992878942e-05,
      "loss": 1.7766,
      "step": 61810
    },
    {
      "epoch": 31.444557477110884,
      "grad_norm": 30.127155303955078,
      "learning_rate": 1.8555442522889115e-05,
      "loss": 1.7499,
      "step": 61820
    },
    {
      "epoch": 31.44964394710071,
      "grad_norm": 41.634788513183594,
      "learning_rate": 1.855035605289929e-05,
      "loss": 1.7259,
      "step": 61830
    },
    {
      "epoch": 31.454730417090538,
      "grad_norm": 43.15324783325195,
      "learning_rate": 1.8545269582909462e-05,
      "loss": 1.6872,
      "step": 61840
    },
    {
      "epoch": 31.459816887080365,
      "grad_norm": 35.802459716796875,
      "learning_rate": 1.8540183112919635e-05,
      "loss": 1.7137,
      "step": 61850
    },
    {
      "epoch": 31.46490335707019,
      "grad_norm": 34.046173095703125,
      "learning_rate": 1.853509664292981e-05,
      "loss": 1.7079,
      "step": 61860
    },
    {
      "epoch": 31.46998982706002,
      "grad_norm": 39.546119689941406,
      "learning_rate": 1.853001017293998e-05,
      "loss": 1.6896,
      "step": 61870
    },
    {
      "epoch": 31.475076297049846,
      "grad_norm": 36.56583786010742,
      "learning_rate": 1.852492370295015e-05,
      "loss": 1.6742,
      "step": 61880
    },
    {
      "epoch": 31.480162767039676,
      "grad_norm": 46.46760559082031,
      "learning_rate": 1.851983723296033e-05,
      "loss": 1.7545,
      "step": 61890
    },
    {
      "epoch": 31.485249237029503,
      "grad_norm": 33.27689743041992,
      "learning_rate": 1.8514750762970498e-05,
      "loss": 1.7634,
      "step": 61900
    },
    {
      "epoch": 31.49033570701933,
      "grad_norm": 34.552486419677734,
      "learning_rate": 1.850966429298067e-05,
      "loss": 1.839,
      "step": 61910
    },
    {
      "epoch": 31.495422177009157,
      "grad_norm": 38.1478271484375,
      "learning_rate": 1.8504577822990845e-05,
      "loss": 1.6556,
      "step": 61920
    },
    {
      "epoch": 31.500508646998984,
      "grad_norm": 39.50722122192383,
      "learning_rate": 1.8499491353001018e-05,
      "loss": 1.6576,
      "step": 61930
    },
    {
      "epoch": 31.50559511698881,
      "grad_norm": 39.654151916503906,
      "learning_rate": 1.849440488301119e-05,
      "loss": 1.7547,
      "step": 61940
    },
    {
      "epoch": 31.510681586978638,
      "grad_norm": 38.35356140136719,
      "learning_rate": 1.8489318413021365e-05,
      "loss": 1.7881,
      "step": 61950
    },
    {
      "epoch": 31.515768056968465,
      "grad_norm": 39.98825454711914,
      "learning_rate": 1.8484231943031538e-05,
      "loss": 1.7795,
      "step": 61960
    },
    {
      "epoch": 31.520854526958292,
      "grad_norm": 54.36723709106445,
      "learning_rate": 1.8479145473041708e-05,
      "loss": 1.7657,
      "step": 61970
    },
    {
      "epoch": 31.52594099694812,
      "grad_norm": 50.441993713378906,
      "learning_rate": 1.8474059003051884e-05,
      "loss": 1.7549,
      "step": 61980
    },
    {
      "epoch": 31.531027466937946,
      "grad_norm": 35.985904693603516,
      "learning_rate": 1.8468972533062058e-05,
      "loss": 1.7312,
      "step": 61990
    },
    {
      "epoch": 31.536113936927773,
      "grad_norm": 39.59580612182617,
      "learning_rate": 1.8463886063072228e-05,
      "loss": 1.7491,
      "step": 62000
    },
    {
      "epoch": 31.5412004069176,
      "grad_norm": 33.857139587402344,
      "learning_rate": 1.84587995930824e-05,
      "loss": 1.7143,
      "step": 62010
    },
    {
      "epoch": 31.546286876907427,
      "grad_norm": 33.4304084777832,
      "learning_rate": 1.8453713123092574e-05,
      "loss": 1.6714,
      "step": 62020
    },
    {
      "epoch": 31.551373346897254,
      "grad_norm": 47.17689895629883,
      "learning_rate": 1.8448626653102748e-05,
      "loss": 1.7058,
      "step": 62030
    },
    {
      "epoch": 31.55645981688708,
      "grad_norm": 37.212196350097656,
      "learning_rate": 1.844354018311292e-05,
      "loss": 1.7169,
      "step": 62040
    },
    {
      "epoch": 31.561546286876908,
      "grad_norm": 42.561649322509766,
      "learning_rate": 1.8438453713123094e-05,
      "loss": 1.7394,
      "step": 62050
    },
    {
      "epoch": 31.566632756866735,
      "grad_norm": 36.52809143066406,
      "learning_rate": 1.8433367243133267e-05,
      "loss": 1.6617,
      "step": 62060
    },
    {
      "epoch": 31.571719226856562,
      "grad_norm": 32.196372985839844,
      "learning_rate": 1.8428280773143437e-05,
      "loss": 1.7692,
      "step": 62070
    },
    {
      "epoch": 31.57680569684639,
      "grad_norm": 42.55571365356445,
      "learning_rate": 1.8423194303153614e-05,
      "loss": 1.7684,
      "step": 62080
    },
    {
      "epoch": 31.581892166836216,
      "grad_norm": 38.52266311645508,
      "learning_rate": 1.8418107833163787e-05,
      "loss": 1.7494,
      "step": 62090
    },
    {
      "epoch": 31.586978636826043,
      "grad_norm": 39.050567626953125,
      "learning_rate": 1.8413021363173957e-05,
      "loss": 1.8146,
      "step": 62100
    },
    {
      "epoch": 31.59206510681587,
      "grad_norm": 36.437538146972656,
      "learning_rate": 1.840793489318413e-05,
      "loss": 1.7282,
      "step": 62110
    },
    {
      "epoch": 31.597151576805697,
      "grad_norm": 41.680328369140625,
      "learning_rate": 1.8402848423194304e-05,
      "loss": 1.724,
      "step": 62120
    },
    {
      "epoch": 31.602238046795524,
      "grad_norm": 41.36949157714844,
      "learning_rate": 1.8397761953204477e-05,
      "loss": 1.7669,
      "step": 62130
    },
    {
      "epoch": 31.60732451678535,
      "grad_norm": 37.1373176574707,
      "learning_rate": 1.839267548321465e-05,
      "loss": 1.6986,
      "step": 62140
    },
    {
      "epoch": 31.612410986775178,
      "grad_norm": 38.4561767578125,
      "learning_rate": 1.8387589013224823e-05,
      "loss": 1.7372,
      "step": 62150
    },
    {
      "epoch": 31.617497456765005,
      "grad_norm": 36.888916015625,
      "learning_rate": 1.8382502543234993e-05,
      "loss": 1.7714,
      "step": 62160
    },
    {
      "epoch": 31.62258392675483,
      "grad_norm": 38.41234588623047,
      "learning_rate": 1.8377416073245167e-05,
      "loss": 1.7218,
      "step": 62170
    },
    {
      "epoch": 31.62767039674466,
      "grad_norm": 35.389591217041016,
      "learning_rate": 1.8372329603255343e-05,
      "loss": 1.744,
      "step": 62180
    },
    {
      "epoch": 31.632756866734486,
      "grad_norm": 47.89485168457031,
      "learning_rate": 1.8367243133265517e-05,
      "loss": 1.759,
      "step": 62190
    },
    {
      "epoch": 31.637843336724313,
      "grad_norm": 32.90937423706055,
      "learning_rate": 1.8362156663275687e-05,
      "loss": 1.8051,
      "step": 62200
    },
    {
      "epoch": 31.64292980671414,
      "grad_norm": 43.24626922607422,
      "learning_rate": 1.835707019328586e-05,
      "loss": 1.7491,
      "step": 62210
    },
    {
      "epoch": 31.648016276703967,
      "grad_norm": 44.74668884277344,
      "learning_rate": 1.8351983723296033e-05,
      "loss": 1.7701,
      "step": 62220
    },
    {
      "epoch": 31.653102746693794,
      "grad_norm": 32.38775634765625,
      "learning_rate": 1.8346897253306206e-05,
      "loss": 1.8729,
      "step": 62230
    },
    {
      "epoch": 31.65818921668362,
      "grad_norm": 38.386348724365234,
      "learning_rate": 1.834181078331638e-05,
      "loss": 1.7503,
      "step": 62240
    },
    {
      "epoch": 31.663275686673447,
      "grad_norm": 33.7255859375,
      "learning_rate": 1.8336724313326553e-05,
      "loss": 1.7868,
      "step": 62250
    },
    {
      "epoch": 31.668362156663274,
      "grad_norm": 37.00188064575195,
      "learning_rate": 1.8331637843336723e-05,
      "loss": 1.7514,
      "step": 62260
    },
    {
      "epoch": 31.6734486266531,
      "grad_norm": 48.777339935302734,
      "learning_rate": 1.83265513733469e-05,
      "loss": 1.6692,
      "step": 62270
    },
    {
      "epoch": 31.67853509664293,
      "grad_norm": 36.320587158203125,
      "learning_rate": 1.8321464903357073e-05,
      "loss": 1.7235,
      "step": 62280
    },
    {
      "epoch": 31.683621566632755,
      "grad_norm": 39.38495635986328,
      "learning_rate": 1.8316378433367243e-05,
      "loss": 1.7927,
      "step": 62290
    },
    {
      "epoch": 31.688708036622582,
      "grad_norm": 31.970129013061523,
      "learning_rate": 1.8311291963377416e-05,
      "loss": 1.7822,
      "step": 62300
    },
    {
      "epoch": 31.69379450661241,
      "grad_norm": 53.922096252441406,
      "learning_rate": 1.830620549338759e-05,
      "loss": 1.7426,
      "step": 62310
    },
    {
      "epoch": 31.698880976602236,
      "grad_norm": 45.22300338745117,
      "learning_rate": 1.8301119023397763e-05,
      "loss": 1.6793,
      "step": 62320
    },
    {
      "epoch": 31.703967446592067,
      "grad_norm": 35.369510650634766,
      "learning_rate": 1.8296032553407936e-05,
      "loss": 1.7974,
      "step": 62330
    },
    {
      "epoch": 31.709053916581894,
      "grad_norm": 32.12101745605469,
      "learning_rate": 1.829094608341811e-05,
      "loss": 1.7284,
      "step": 62340
    },
    {
      "epoch": 31.71414038657172,
      "grad_norm": 41.17545700073242,
      "learning_rate": 1.8285859613428282e-05,
      "loss": 1.7884,
      "step": 62350
    },
    {
      "epoch": 31.719226856561548,
      "grad_norm": 37.1142463684082,
      "learning_rate": 1.8280773143438452e-05,
      "loss": 1.7272,
      "step": 62360
    },
    {
      "epoch": 31.724313326551375,
      "grad_norm": 43.83570861816406,
      "learning_rate": 1.827568667344863e-05,
      "loss": 1.7399,
      "step": 62370
    },
    {
      "epoch": 31.729399796541202,
      "grad_norm": 38.882755279541016,
      "learning_rate": 1.8270600203458802e-05,
      "loss": 1.7043,
      "step": 62380
    },
    {
      "epoch": 31.73448626653103,
      "grad_norm": 36.37739562988281,
      "learning_rate": 1.8265513733468972e-05,
      "loss": 1.7493,
      "step": 62390
    },
    {
      "epoch": 31.739572736520856,
      "grad_norm": 38.615745544433594,
      "learning_rate": 1.8260427263479145e-05,
      "loss": 1.8233,
      "step": 62400
    },
    {
      "epoch": 31.744659206510683,
      "grad_norm": 32.81993865966797,
      "learning_rate": 1.825534079348932e-05,
      "loss": 1.8455,
      "step": 62410
    },
    {
      "epoch": 31.74974567650051,
      "grad_norm": 32.04153823852539,
      "learning_rate": 1.8250254323499492e-05,
      "loss": 1.7631,
      "step": 62420
    },
    {
      "epoch": 31.754832146490337,
      "grad_norm": 39.577789306640625,
      "learning_rate": 1.8245167853509665e-05,
      "loss": 1.7488,
      "step": 62430
    },
    {
      "epoch": 31.759918616480164,
      "grad_norm": 34.942081451416016,
      "learning_rate": 1.824008138351984e-05,
      "loss": 1.7759,
      "step": 62440
    },
    {
      "epoch": 31.76500508646999,
      "grad_norm": 36.99660110473633,
      "learning_rate": 1.823499491353001e-05,
      "loss": 1.7648,
      "step": 62450
    },
    {
      "epoch": 31.770091556459818,
      "grad_norm": 42.378150939941406,
      "learning_rate": 1.8229908443540185e-05,
      "loss": 1.7835,
      "step": 62460
    },
    {
      "epoch": 31.775178026449645,
      "grad_norm": 32.105491638183594,
      "learning_rate": 1.822482197355036e-05,
      "loss": 1.7964,
      "step": 62470
    },
    {
      "epoch": 31.78026449643947,
      "grad_norm": 41.39695739746094,
      "learning_rate": 1.821973550356053e-05,
      "loss": 1.7781,
      "step": 62480
    },
    {
      "epoch": 31.7853509664293,
      "grad_norm": 35.4168586730957,
      "learning_rate": 1.82146490335707e-05,
      "loss": 1.7775,
      "step": 62490
    },
    {
      "epoch": 31.790437436419126,
      "grad_norm": 51.81678009033203,
      "learning_rate": 1.8209562563580875e-05,
      "loss": 1.7913,
      "step": 62500
    },
    {
      "epoch": 31.795523906408953,
      "grad_norm": 36.94265365600586,
      "learning_rate": 1.8204476093591048e-05,
      "loss": 1.8084,
      "step": 62510
    },
    {
      "epoch": 31.80061037639878,
      "grad_norm": 34.12557601928711,
      "learning_rate": 1.819938962360122e-05,
      "loss": 1.7439,
      "step": 62520
    },
    {
      "epoch": 31.805696846388607,
      "grad_norm": 34.830474853515625,
      "learning_rate": 1.8194303153611395e-05,
      "loss": 1.7329,
      "step": 62530
    },
    {
      "epoch": 31.810783316378433,
      "grad_norm": 39.098567962646484,
      "learning_rate": 1.8189216683621568e-05,
      "loss": 1.6623,
      "step": 62540
    },
    {
      "epoch": 31.81586978636826,
      "grad_norm": 33.35914611816406,
      "learning_rate": 1.8184130213631738e-05,
      "loss": 1.6833,
      "step": 62550
    },
    {
      "epoch": 31.820956256358087,
      "grad_norm": 36.60926818847656,
      "learning_rate": 1.8179043743641914e-05,
      "loss": 1.609,
      "step": 62560
    },
    {
      "epoch": 31.826042726347914,
      "grad_norm": 33.91197204589844,
      "learning_rate": 1.8173957273652088e-05,
      "loss": 1.6337,
      "step": 62570
    },
    {
      "epoch": 31.83112919633774,
      "grad_norm": 48.2971076965332,
      "learning_rate": 1.8168870803662258e-05,
      "loss": 1.7463,
      "step": 62580
    },
    {
      "epoch": 31.83621566632757,
      "grad_norm": 38.911705017089844,
      "learning_rate": 1.816378433367243e-05,
      "loss": 1.7713,
      "step": 62590
    },
    {
      "epoch": 31.841302136317395,
      "grad_norm": 37.892215728759766,
      "learning_rate": 1.8158697863682604e-05,
      "loss": 1.6999,
      "step": 62600
    },
    {
      "epoch": 31.846388606307222,
      "grad_norm": 36.375579833984375,
      "learning_rate": 1.815361139369278e-05,
      "loss": 1.7809,
      "step": 62610
    },
    {
      "epoch": 31.85147507629705,
      "grad_norm": 35.060035705566406,
      "learning_rate": 1.814852492370295e-05,
      "loss": 1.7076,
      "step": 62620
    },
    {
      "epoch": 31.856561546286876,
      "grad_norm": 39.55412673950195,
      "learning_rate": 1.8143438453713124e-05,
      "loss": 1.6603,
      "step": 62630
    },
    {
      "epoch": 31.861648016276703,
      "grad_norm": 38.22513961791992,
      "learning_rate": 1.8138351983723297e-05,
      "loss": 1.6988,
      "step": 62640
    },
    {
      "epoch": 31.86673448626653,
      "grad_norm": 35.427215576171875,
      "learning_rate": 1.8133265513733467e-05,
      "loss": 1.7017,
      "step": 62650
    },
    {
      "epoch": 31.871820956256357,
      "grad_norm": 36.147945404052734,
      "learning_rate": 1.8128179043743644e-05,
      "loss": 1.7898,
      "step": 62660
    },
    {
      "epoch": 31.876907426246184,
      "grad_norm": 49.411407470703125,
      "learning_rate": 1.8123092573753817e-05,
      "loss": 1.7596,
      "step": 62670
    },
    {
      "epoch": 31.88199389623601,
      "grad_norm": 38.34050750732422,
      "learning_rate": 1.8118006103763987e-05,
      "loss": 1.7817,
      "step": 62680
    },
    {
      "epoch": 31.887080366225838,
      "grad_norm": 39.14099884033203,
      "learning_rate": 1.811291963377416e-05,
      "loss": 1.7844,
      "step": 62690
    },
    {
      "epoch": 31.892166836215665,
      "grad_norm": 48.33980941772461,
      "learning_rate": 1.8107833163784334e-05,
      "loss": 1.7198,
      "step": 62700
    },
    {
      "epoch": 31.897253306205492,
      "grad_norm": 40.11444854736328,
      "learning_rate": 1.8102746693794507e-05,
      "loss": 1.6713,
      "step": 62710
    },
    {
      "epoch": 31.90233977619532,
      "grad_norm": 41.40735626220703,
      "learning_rate": 1.809766022380468e-05,
      "loss": 1.7264,
      "step": 62720
    },
    {
      "epoch": 31.907426246185146,
      "grad_norm": 38.922306060791016,
      "learning_rate": 1.8092573753814854e-05,
      "loss": 1.7587,
      "step": 62730
    },
    {
      "epoch": 31.912512716174973,
      "grad_norm": 39.380226135253906,
      "learning_rate": 1.8087487283825027e-05,
      "loss": 1.7035,
      "step": 62740
    },
    {
      "epoch": 31.9175991861648,
      "grad_norm": 40.18115234375,
      "learning_rate": 1.80824008138352e-05,
      "loss": 1.826,
      "step": 62750
    },
    {
      "epoch": 31.922685656154627,
      "grad_norm": 36.15968322753906,
      "learning_rate": 1.8077314343845373e-05,
      "loss": 1.758,
      "step": 62760
    },
    {
      "epoch": 31.927772126144454,
      "grad_norm": 38.29911422729492,
      "learning_rate": 1.8072227873855547e-05,
      "loss": 1.7611,
      "step": 62770
    },
    {
      "epoch": 31.93285859613428,
      "grad_norm": 40.08191680908203,
      "learning_rate": 1.8067141403865717e-05,
      "loss": 1.7517,
      "step": 62780
    },
    {
      "epoch": 31.93794506612411,
      "grad_norm": 34.09846496582031,
      "learning_rate": 1.806205493387589e-05,
      "loss": 1.7621,
      "step": 62790
    },
    {
      "epoch": 31.94303153611394,
      "grad_norm": 33.47877883911133,
      "learning_rate": 1.8056968463886063e-05,
      "loss": 1.797,
      "step": 62800
    },
    {
      "epoch": 31.948118006103766,
      "grad_norm": 35.861534118652344,
      "learning_rate": 1.8051881993896236e-05,
      "loss": 1.7119,
      "step": 62810
    },
    {
      "epoch": 31.953204476093592,
      "grad_norm": 41.39047622680664,
      "learning_rate": 1.804679552390641e-05,
      "loss": 1.7725,
      "step": 62820
    },
    {
      "epoch": 31.95829094608342,
      "grad_norm": 43.43178176879883,
      "learning_rate": 1.8041709053916583e-05,
      "loss": 1.7452,
      "step": 62830
    },
    {
      "epoch": 31.963377416073246,
      "grad_norm": 51.88307189941406,
      "learning_rate": 1.8036622583926753e-05,
      "loss": 1.7278,
      "step": 62840
    },
    {
      "epoch": 31.968463886063073,
      "grad_norm": 43.09751892089844,
      "learning_rate": 1.803153611393693e-05,
      "loss": 1.7134,
      "step": 62850
    },
    {
      "epoch": 31.9735503560529,
      "grad_norm": 39.61703109741211,
      "learning_rate": 1.8026449643947103e-05,
      "loss": 1.6746,
      "step": 62860
    },
    {
      "epoch": 31.978636826042727,
      "grad_norm": 40.76912307739258,
      "learning_rate": 1.8021363173957276e-05,
      "loss": 1.6856,
      "step": 62870
    },
    {
      "epoch": 31.983723296032554,
      "grad_norm": 37.403099060058594,
      "learning_rate": 1.8016276703967446e-05,
      "loss": 1.7353,
      "step": 62880
    },
    {
      "epoch": 31.98880976602238,
      "grad_norm": 43.66476058959961,
      "learning_rate": 1.801119023397762e-05,
      "loss": 1.7245,
      "step": 62890
    },
    {
      "epoch": 31.99389623601221,
      "grad_norm": 32.23557662963867,
      "learning_rate": 1.8006103763987796e-05,
      "loss": 1.7584,
      "step": 62900
    },
    {
      "epoch": 31.998982706002035,
      "grad_norm": 32.97943115234375,
      "learning_rate": 1.8001017293997966e-05,
      "loss": 1.7879,
      "step": 62910
    },
    {
      "epoch": 32.0,
      "eval_loss": 4.8248291015625,
      "eval_runtime": 2.638,
      "eval_samples_per_second": 1051.947,
      "eval_steps_per_second": 131.541,
      "step": 62912
    },
    {
      "epoch": 32.00406917599186,
      "grad_norm": 47.16140365600586,
      "learning_rate": 1.799593082400814e-05,
      "loss": 1.7581,
      "step": 62920
    },
    {
      "epoch": 32.009155645981686,
      "grad_norm": 44.53266143798828,
      "learning_rate": 1.7990844354018312e-05,
      "loss": 1.6586,
      "step": 62930
    },
    {
      "epoch": 32.01424211597151,
      "grad_norm": 40.15373992919922,
      "learning_rate": 1.7985757884028486e-05,
      "loss": 1.7002,
      "step": 62940
    },
    {
      "epoch": 32.01932858596134,
      "grad_norm": 45.36066818237305,
      "learning_rate": 1.798067141403866e-05,
      "loss": 1.7389,
      "step": 62950
    },
    {
      "epoch": 32.02441505595117,
      "grad_norm": 45.10235595703125,
      "learning_rate": 1.7975584944048832e-05,
      "loss": 1.7158,
      "step": 62960
    },
    {
      "epoch": 32.029501525940994,
      "grad_norm": 35.47319412231445,
      "learning_rate": 1.7970498474059002e-05,
      "loss": 1.6846,
      "step": 62970
    },
    {
      "epoch": 32.03458799593082,
      "grad_norm": 41.15199661254883,
      "learning_rate": 1.7965412004069175e-05,
      "loss": 1.6588,
      "step": 62980
    },
    {
      "epoch": 32.03967446592065,
      "grad_norm": 33.13029479980469,
      "learning_rate": 1.796032553407935e-05,
      "loss": 1.7112,
      "step": 62990
    },
    {
      "epoch": 32.044760935910475,
      "grad_norm": 33.86288070678711,
      "learning_rate": 1.7955239064089525e-05,
      "loss": 1.8356,
      "step": 63000
    },
    {
      "epoch": 32.04984740590031,
      "grad_norm": 39.46187210083008,
      "learning_rate": 1.7950152594099695e-05,
      "loss": 1.7496,
      "step": 63010
    },
    {
      "epoch": 32.054933875890136,
      "grad_norm": 41.73454284667969,
      "learning_rate": 1.794506612410987e-05,
      "loss": 1.788,
      "step": 63020
    },
    {
      "epoch": 32.06002034587996,
      "grad_norm": 41.990699768066406,
      "learning_rate": 1.7939979654120042e-05,
      "loss": 1.7053,
      "step": 63030
    },
    {
      "epoch": 32.06510681586979,
      "grad_norm": 41.74126434326172,
      "learning_rate": 1.7934893184130215e-05,
      "loss": 1.785,
      "step": 63040
    },
    {
      "epoch": 32.07019328585962,
      "grad_norm": 43.23762130737305,
      "learning_rate": 1.792980671414039e-05,
      "loss": 1.7574,
      "step": 63050
    },
    {
      "epoch": 32.075279755849444,
      "grad_norm": 38.429710388183594,
      "learning_rate": 1.792472024415056e-05,
      "loss": 1.7303,
      "step": 63060
    },
    {
      "epoch": 32.08036622583927,
      "grad_norm": 37.13812255859375,
      "learning_rate": 1.791963377416073e-05,
      "loss": 1.7237,
      "step": 63070
    },
    {
      "epoch": 32.0854526958291,
      "grad_norm": 37.009605407714844,
      "learning_rate": 1.7914547304170905e-05,
      "loss": 1.7011,
      "step": 63080
    },
    {
      "epoch": 32.090539165818925,
      "grad_norm": 38.78252029418945,
      "learning_rate": 1.790946083418108e-05,
      "loss": 1.7171,
      "step": 63090
    },
    {
      "epoch": 32.09562563580875,
      "grad_norm": 48.447059631347656,
      "learning_rate": 1.790437436419125e-05,
      "loss": 1.6448,
      "step": 63100
    },
    {
      "epoch": 32.10071210579858,
      "grad_norm": 38.17219161987305,
      "learning_rate": 1.7899287894201425e-05,
      "loss": 1.7559,
      "step": 63110
    },
    {
      "epoch": 32.105798575788405,
      "grad_norm": 44.42240905761719,
      "learning_rate": 1.7894201424211598e-05,
      "loss": 1.6978,
      "step": 63120
    },
    {
      "epoch": 32.11088504577823,
      "grad_norm": 31.65469741821289,
      "learning_rate": 1.788911495422177e-05,
      "loss": 1.7345,
      "step": 63130
    },
    {
      "epoch": 32.11597151576806,
      "grad_norm": 40.31278610229492,
      "learning_rate": 1.7884028484231945e-05,
      "loss": 1.669,
      "step": 63140
    },
    {
      "epoch": 32.121057985757886,
      "grad_norm": 40.12299346923828,
      "learning_rate": 1.7878942014242118e-05,
      "loss": 1.7619,
      "step": 63150
    },
    {
      "epoch": 32.12614445574771,
      "grad_norm": 33.48412322998047,
      "learning_rate": 1.787385554425229e-05,
      "loss": 1.8093,
      "step": 63160
    },
    {
      "epoch": 32.13123092573754,
      "grad_norm": 40.96088409423828,
      "learning_rate": 1.786876907426246e-05,
      "loss": 1.6983,
      "step": 63170
    },
    {
      "epoch": 32.13631739572737,
      "grad_norm": 40.40130615234375,
      "learning_rate": 1.7863682604272634e-05,
      "loss": 1.7234,
      "step": 63180
    },
    {
      "epoch": 32.141403865717194,
      "grad_norm": 40.90806198120117,
      "learning_rate": 1.785859613428281e-05,
      "loss": 1.757,
      "step": 63190
    },
    {
      "epoch": 32.14649033570702,
      "grad_norm": 35.63624954223633,
      "learning_rate": 1.785350966429298e-05,
      "loss": 1.7529,
      "step": 63200
    },
    {
      "epoch": 32.15157680569685,
      "grad_norm": 27.720592498779297,
      "learning_rate": 1.7848423194303154e-05,
      "loss": 1.7872,
      "step": 63210
    },
    {
      "epoch": 32.156663275686675,
      "grad_norm": 43.13979721069336,
      "learning_rate": 1.7843336724313327e-05,
      "loss": 1.6881,
      "step": 63220
    },
    {
      "epoch": 32.1617497456765,
      "grad_norm": 37.08523941040039,
      "learning_rate": 1.78382502543235e-05,
      "loss": 1.7883,
      "step": 63230
    },
    {
      "epoch": 32.16683621566633,
      "grad_norm": 34.351104736328125,
      "learning_rate": 1.7833163784333674e-05,
      "loss": 1.7563,
      "step": 63240
    },
    {
      "epoch": 32.171922685656156,
      "grad_norm": 54.73746871948242,
      "learning_rate": 1.7828077314343847e-05,
      "loss": 1.7539,
      "step": 63250
    },
    {
      "epoch": 32.17700915564598,
      "grad_norm": 43.512733459472656,
      "learning_rate": 1.7822990844354017e-05,
      "loss": 1.7579,
      "step": 63260
    },
    {
      "epoch": 32.18209562563581,
      "grad_norm": 56.39529037475586,
      "learning_rate": 1.781790437436419e-05,
      "loss": 1.7321,
      "step": 63270
    },
    {
      "epoch": 32.18718209562564,
      "grad_norm": 33.2241325378418,
      "learning_rate": 1.7812817904374367e-05,
      "loss": 1.7144,
      "step": 63280
    },
    {
      "epoch": 32.192268565615464,
      "grad_norm": 36.95907974243164,
      "learning_rate": 1.780773143438454e-05,
      "loss": 1.7206,
      "step": 63290
    },
    {
      "epoch": 32.19735503560529,
      "grad_norm": 44.01083755493164,
      "learning_rate": 1.780264496439471e-05,
      "loss": 1.7292,
      "step": 63300
    },
    {
      "epoch": 32.20244150559512,
      "grad_norm": 31.906078338623047,
      "learning_rate": 1.7797558494404884e-05,
      "loss": 1.6971,
      "step": 63310
    },
    {
      "epoch": 32.207527975584945,
      "grad_norm": 33.389991760253906,
      "learning_rate": 1.7792472024415057e-05,
      "loss": 1.856,
      "step": 63320
    },
    {
      "epoch": 32.21261444557477,
      "grad_norm": 30.83430290222168,
      "learning_rate": 1.778738555442523e-05,
      "loss": 1.7378,
      "step": 63330
    },
    {
      "epoch": 32.2177009155646,
      "grad_norm": 38.04106140136719,
      "learning_rate": 1.7782299084435403e-05,
      "loss": 1.7421,
      "step": 63340
    },
    {
      "epoch": 32.222787385554426,
      "grad_norm": 32.2618522644043,
      "learning_rate": 1.7777212614445577e-05,
      "loss": 1.7193,
      "step": 63350
    },
    {
      "epoch": 32.22787385554425,
      "grad_norm": 32.932064056396484,
      "learning_rate": 1.7772126144455747e-05,
      "loss": 1.745,
      "step": 63360
    },
    {
      "epoch": 32.23296032553408,
      "grad_norm": 40.661903381347656,
      "learning_rate": 1.776703967446592e-05,
      "loss": 1.7412,
      "step": 63370
    },
    {
      "epoch": 32.23804679552391,
      "grad_norm": 45.31901931762695,
      "learning_rate": 1.7761953204476096e-05,
      "loss": 1.7345,
      "step": 63380
    },
    {
      "epoch": 32.243133265513734,
      "grad_norm": 37.90960693359375,
      "learning_rate": 1.7756866734486266e-05,
      "loss": 1.7496,
      "step": 63390
    },
    {
      "epoch": 32.24821973550356,
      "grad_norm": 33.9438591003418,
      "learning_rate": 1.775178026449644e-05,
      "loss": 1.6842,
      "step": 63400
    },
    {
      "epoch": 32.25330620549339,
      "grad_norm": 32.49137878417969,
      "learning_rate": 1.7746693794506613e-05,
      "loss": 1.8064,
      "step": 63410
    },
    {
      "epoch": 32.258392675483215,
      "grad_norm": 37.60181427001953,
      "learning_rate": 1.7741607324516786e-05,
      "loss": 1.7427,
      "step": 63420
    },
    {
      "epoch": 32.26347914547304,
      "grad_norm": 35.65332794189453,
      "learning_rate": 1.773652085452696e-05,
      "loss": 1.6964,
      "step": 63430
    },
    {
      "epoch": 32.26856561546287,
      "grad_norm": 42.38912582397461,
      "learning_rate": 1.7731434384537133e-05,
      "loss": 1.7797,
      "step": 63440
    },
    {
      "epoch": 32.273652085452696,
      "grad_norm": 33.841243743896484,
      "learning_rate": 1.7726347914547306e-05,
      "loss": 1.7176,
      "step": 63450
    },
    {
      "epoch": 32.27873855544252,
      "grad_norm": 34.62863540649414,
      "learning_rate": 1.7721261444557476e-05,
      "loss": 1.7546,
      "step": 63460
    },
    {
      "epoch": 32.28382502543235,
      "grad_norm": 38.697086334228516,
      "learning_rate": 1.771617497456765e-05,
      "loss": 1.8045,
      "step": 63470
    },
    {
      "epoch": 32.28891149542218,
      "grad_norm": 41.63343048095703,
      "learning_rate": 1.7711088504577826e-05,
      "loss": 1.6301,
      "step": 63480
    },
    {
      "epoch": 32.293997965412004,
      "grad_norm": 37.38759994506836,
      "learning_rate": 1.7706002034587996e-05,
      "loss": 1.7358,
      "step": 63490
    },
    {
      "epoch": 32.29908443540183,
      "grad_norm": 36.38063430786133,
      "learning_rate": 1.770091556459817e-05,
      "loss": 1.7592,
      "step": 63500
    },
    {
      "epoch": 32.30417090539166,
      "grad_norm": 34.272308349609375,
      "learning_rate": 1.7695829094608342e-05,
      "loss": 1.7428,
      "step": 63510
    },
    {
      "epoch": 32.309257375381485,
      "grad_norm": 39.53602981567383,
      "learning_rate": 1.7690742624618516e-05,
      "loss": 1.7215,
      "step": 63520
    },
    {
      "epoch": 32.31434384537131,
      "grad_norm": 40.372230529785156,
      "learning_rate": 1.768565615462869e-05,
      "loss": 1.6537,
      "step": 63530
    },
    {
      "epoch": 32.31943031536114,
      "grad_norm": 36.83397674560547,
      "learning_rate": 1.7680569684638862e-05,
      "loss": 1.7529,
      "step": 63540
    },
    {
      "epoch": 32.324516785350966,
      "grad_norm": 37.82072448730469,
      "learning_rate": 1.7675483214649035e-05,
      "loss": 1.6415,
      "step": 63550
    },
    {
      "epoch": 32.32960325534079,
      "grad_norm": 35.35897445678711,
      "learning_rate": 1.7670396744659205e-05,
      "loss": 1.6812,
      "step": 63560
    },
    {
      "epoch": 32.33468972533062,
      "grad_norm": 50.71145248413086,
      "learning_rate": 1.7665310274669382e-05,
      "loss": 1.7378,
      "step": 63570
    },
    {
      "epoch": 32.33977619532045,
      "grad_norm": 31.444597244262695,
      "learning_rate": 1.7660223804679555e-05,
      "loss": 1.6253,
      "step": 63580
    },
    {
      "epoch": 32.34486266531027,
      "grad_norm": 34.912662506103516,
      "learning_rate": 1.7655137334689725e-05,
      "loss": 1.5713,
      "step": 63590
    },
    {
      "epoch": 32.3499491353001,
      "grad_norm": 37.75759506225586,
      "learning_rate": 1.76500508646999e-05,
      "loss": 1.7959,
      "step": 63600
    },
    {
      "epoch": 32.35503560528993,
      "grad_norm": 42.61883544921875,
      "learning_rate": 1.7644964394710072e-05,
      "loss": 1.6602,
      "step": 63610
    },
    {
      "epoch": 32.360122075279754,
      "grad_norm": 42.82818603515625,
      "learning_rate": 1.7639877924720245e-05,
      "loss": 1.7365,
      "step": 63620
    },
    {
      "epoch": 32.36520854526958,
      "grad_norm": 43.86159896850586,
      "learning_rate": 1.763479145473042e-05,
      "loss": 1.7228,
      "step": 63630
    },
    {
      "epoch": 32.37029501525941,
      "grad_norm": 34.6712532043457,
      "learning_rate": 1.762970498474059e-05,
      "loss": 1.7733,
      "step": 63640
    },
    {
      "epoch": 32.375381485249235,
      "grad_norm": 31.186037063598633,
      "learning_rate": 1.762461851475076e-05,
      "loss": 1.7555,
      "step": 63650
    },
    {
      "epoch": 32.38046795523906,
      "grad_norm": 40.21357345581055,
      "learning_rate": 1.7619532044760935e-05,
      "loss": 1.7081,
      "step": 63660
    },
    {
      "epoch": 32.38555442522889,
      "grad_norm": 35.79448699951172,
      "learning_rate": 1.761444557477111e-05,
      "loss": 1.7285,
      "step": 63670
    },
    {
      "epoch": 32.390640895218716,
      "grad_norm": 49.86919021606445,
      "learning_rate": 1.7609359104781285e-05,
      "loss": 1.7529,
      "step": 63680
    },
    {
      "epoch": 32.39572736520854,
      "grad_norm": 38.269229888916016,
      "learning_rate": 1.7604272634791455e-05,
      "loss": 1.7332,
      "step": 63690
    },
    {
      "epoch": 32.40081383519837,
      "grad_norm": 39.45783615112305,
      "learning_rate": 1.7599186164801628e-05,
      "loss": 1.748,
      "step": 63700
    },
    {
      "epoch": 32.4059003051882,
      "grad_norm": 39.842655181884766,
      "learning_rate": 1.75940996948118e-05,
      "loss": 1.6612,
      "step": 63710
    },
    {
      "epoch": 32.410986775178024,
      "grad_norm": 37.114784240722656,
      "learning_rate": 1.7589013224821975e-05,
      "loss": 1.7383,
      "step": 63720
    },
    {
      "epoch": 32.41607324516785,
      "grad_norm": 43.30097579956055,
      "learning_rate": 1.7583926754832148e-05,
      "loss": 1.7185,
      "step": 63730
    },
    {
      "epoch": 32.42115971515768,
      "grad_norm": 38.00579833984375,
      "learning_rate": 1.757884028484232e-05,
      "loss": 1.661,
      "step": 63740
    },
    {
      "epoch": 32.426246185147505,
      "grad_norm": 35.222530364990234,
      "learning_rate": 1.757375381485249e-05,
      "loss": 1.6939,
      "step": 63750
    },
    {
      "epoch": 32.43133265513733,
      "grad_norm": 41.151092529296875,
      "learning_rate": 1.7568667344862668e-05,
      "loss": 1.651,
      "step": 63760
    },
    {
      "epoch": 32.43641912512716,
      "grad_norm": 37.3519287109375,
      "learning_rate": 1.756358087487284e-05,
      "loss": 1.6965,
      "step": 63770
    },
    {
      "epoch": 32.441505595116986,
      "grad_norm": 37.03276062011719,
      "learning_rate": 1.755849440488301e-05,
      "loss": 1.7529,
      "step": 63780
    },
    {
      "epoch": 32.44659206510681,
      "grad_norm": 39.365657806396484,
      "learning_rate": 1.7553407934893184e-05,
      "loss": 1.6716,
      "step": 63790
    },
    {
      "epoch": 32.45167853509664,
      "grad_norm": 40.120941162109375,
      "learning_rate": 1.7548321464903357e-05,
      "loss": 1.7573,
      "step": 63800
    },
    {
      "epoch": 32.45676500508647,
      "grad_norm": 41.78614807128906,
      "learning_rate": 1.754323499491353e-05,
      "loss": 1.7034,
      "step": 63810
    },
    {
      "epoch": 32.461851475076294,
      "grad_norm": 51.97328186035156,
      "learning_rate": 1.7538148524923704e-05,
      "loss": 1.7651,
      "step": 63820
    },
    {
      "epoch": 32.46693794506612,
      "grad_norm": 50.06690216064453,
      "learning_rate": 1.7533062054933877e-05,
      "loss": 1.8043,
      "step": 63830
    },
    {
      "epoch": 32.47202441505595,
      "grad_norm": 43.62875747680664,
      "learning_rate": 1.752797558494405e-05,
      "loss": 1.7472,
      "step": 63840
    },
    {
      "epoch": 32.477110885045775,
      "grad_norm": 43.589473724365234,
      "learning_rate": 1.752288911495422e-05,
      "loss": 1.778,
      "step": 63850
    },
    {
      "epoch": 32.4821973550356,
      "grad_norm": 38.328880310058594,
      "learning_rate": 1.7517802644964397e-05,
      "loss": 1.7111,
      "step": 63860
    },
    {
      "epoch": 32.48728382502543,
      "grad_norm": 34.35126876831055,
      "learning_rate": 1.751271617497457e-05,
      "loss": 1.7904,
      "step": 63870
    },
    {
      "epoch": 32.492370295015256,
      "grad_norm": 36.98710632324219,
      "learning_rate": 1.750762970498474e-05,
      "loss": 1.6603,
      "step": 63880
    },
    {
      "epoch": 32.49745676500508,
      "grad_norm": 38.41402053833008,
      "learning_rate": 1.7502543234994914e-05,
      "loss": 1.7034,
      "step": 63890
    },
    {
      "epoch": 32.50254323499492,
      "grad_norm": 32.664371490478516,
      "learning_rate": 1.7497456765005087e-05,
      "loss": 1.7427,
      "step": 63900
    },
    {
      "epoch": 32.507629704984744,
      "grad_norm": 31.914369583129883,
      "learning_rate": 1.749237029501526e-05,
      "loss": 1.6957,
      "step": 63910
    },
    {
      "epoch": 32.51271617497457,
      "grad_norm": 34.361083984375,
      "learning_rate": 1.7487283825025433e-05,
      "loss": 1.7173,
      "step": 63920
    },
    {
      "epoch": 32.5178026449644,
      "grad_norm": 49.26896667480469,
      "learning_rate": 1.7482197355035607e-05,
      "loss": 1.7251,
      "step": 63930
    },
    {
      "epoch": 32.522889114954225,
      "grad_norm": 39.43271255493164,
      "learning_rate": 1.747711088504578e-05,
      "loss": 1.6285,
      "step": 63940
    },
    {
      "epoch": 32.52797558494405,
      "grad_norm": 32.48823928833008,
      "learning_rate": 1.747202441505595e-05,
      "loss": 1.7099,
      "step": 63950
    },
    {
      "epoch": 32.53306205493388,
      "grad_norm": 42.21297073364258,
      "learning_rate": 1.7466937945066126e-05,
      "loss": 1.7809,
      "step": 63960
    },
    {
      "epoch": 32.538148524923706,
      "grad_norm": 36.77998733520508,
      "learning_rate": 1.74618514750763e-05,
      "loss": 1.7344,
      "step": 63970
    },
    {
      "epoch": 32.54323499491353,
      "grad_norm": 35.36341857910156,
      "learning_rate": 1.745676500508647e-05,
      "loss": 1.7301,
      "step": 63980
    },
    {
      "epoch": 32.54832146490336,
      "grad_norm": 34.4671745300293,
      "learning_rate": 1.7451678535096643e-05,
      "loss": 1.6993,
      "step": 63990
    },
    {
      "epoch": 32.55340793489319,
      "grad_norm": 37.503238677978516,
      "learning_rate": 1.7446592065106816e-05,
      "loss": 1.6425,
      "step": 64000
    },
    {
      "epoch": 32.558494404883014,
      "grad_norm": 37.873619079589844,
      "learning_rate": 1.744150559511699e-05,
      "loss": 1.6992,
      "step": 64010
    },
    {
      "epoch": 32.56358087487284,
      "grad_norm": 58.068233489990234,
      "learning_rate": 1.7436419125127163e-05,
      "loss": 1.7582,
      "step": 64020
    },
    {
      "epoch": 32.56866734486267,
      "grad_norm": 42.693809509277344,
      "learning_rate": 1.7431332655137336e-05,
      "loss": 1.7248,
      "step": 64030
    },
    {
      "epoch": 32.573753814852495,
      "grad_norm": 31.822776794433594,
      "learning_rate": 1.7426246185147506e-05,
      "loss": 1.7321,
      "step": 64040
    },
    {
      "epoch": 32.57884028484232,
      "grad_norm": 35.99188995361328,
      "learning_rate": 1.7421159715157683e-05,
      "loss": 1.7024,
      "step": 64050
    },
    {
      "epoch": 32.58392675483215,
      "grad_norm": 45.62525177001953,
      "learning_rate": 1.7416073245167856e-05,
      "loss": 1.6293,
      "step": 64060
    },
    {
      "epoch": 32.589013224821976,
      "grad_norm": 42.01107406616211,
      "learning_rate": 1.7410986775178026e-05,
      "loss": 1.7148,
      "step": 64070
    },
    {
      "epoch": 32.5940996948118,
      "grad_norm": 39.05802536010742,
      "learning_rate": 1.74059003051882e-05,
      "loss": 1.76,
      "step": 64080
    },
    {
      "epoch": 32.59918616480163,
      "grad_norm": 37.094120025634766,
      "learning_rate": 1.7400813835198372e-05,
      "loss": 1.7802,
      "step": 64090
    },
    {
      "epoch": 32.60427263479146,
      "grad_norm": 38.25458526611328,
      "learning_rate": 1.7395727365208546e-05,
      "loss": 1.7366,
      "step": 64100
    },
    {
      "epoch": 32.609359104781284,
      "grad_norm": 37.86464309692383,
      "learning_rate": 1.739064089521872e-05,
      "loss": 1.7383,
      "step": 64110
    },
    {
      "epoch": 32.61444557477111,
      "grad_norm": 40.593017578125,
      "learning_rate": 1.7385554425228892e-05,
      "loss": 1.771,
      "step": 64120
    },
    {
      "epoch": 32.61953204476094,
      "grad_norm": 32.95418930053711,
      "learning_rate": 1.7380467955239066e-05,
      "loss": 1.8671,
      "step": 64130
    },
    {
      "epoch": 32.624618514750765,
      "grad_norm": 40.468017578125,
      "learning_rate": 1.7375381485249235e-05,
      "loss": 1.7491,
      "step": 64140
    },
    {
      "epoch": 32.62970498474059,
      "grad_norm": 41.829856872558594,
      "learning_rate": 1.7370295015259412e-05,
      "loss": 1.7107,
      "step": 64150
    },
    {
      "epoch": 32.63479145473042,
      "grad_norm": 31.74521827697754,
      "learning_rate": 1.7365208545269585e-05,
      "loss": 1.7353,
      "step": 64160
    },
    {
      "epoch": 32.639877924720246,
      "grad_norm": 46.234283447265625,
      "learning_rate": 1.7360122075279755e-05,
      "loss": 1.684,
      "step": 64170
    },
    {
      "epoch": 32.64496439471007,
      "grad_norm": 41.599365234375,
      "learning_rate": 1.735503560528993e-05,
      "loss": 1.6987,
      "step": 64180
    },
    {
      "epoch": 32.6500508646999,
      "grad_norm": 38.578826904296875,
      "learning_rate": 1.7349949135300102e-05,
      "loss": 1.6642,
      "step": 64190
    },
    {
      "epoch": 32.65513733468973,
      "grad_norm": 40.11521530151367,
      "learning_rate": 1.7344862665310275e-05,
      "loss": 1.7493,
      "step": 64200
    },
    {
      "epoch": 32.66022380467955,
      "grad_norm": 37.38929748535156,
      "learning_rate": 1.733977619532045e-05,
      "loss": 1.7817,
      "step": 64210
    },
    {
      "epoch": 32.66531027466938,
      "grad_norm": 55.4124641418457,
      "learning_rate": 1.733468972533062e-05,
      "loss": 1.678,
      "step": 64220
    },
    {
      "epoch": 32.67039674465921,
      "grad_norm": 47.60393524169922,
      "learning_rate": 1.7329603255340795e-05,
      "loss": 1.7629,
      "step": 64230
    },
    {
      "epoch": 32.675483214649034,
      "grad_norm": 36.6197395324707,
      "learning_rate": 1.7324516785350968e-05,
      "loss": 1.7511,
      "step": 64240
    },
    {
      "epoch": 32.68056968463886,
      "grad_norm": 40.096153259277344,
      "learning_rate": 1.731943031536114e-05,
      "loss": 1.7268,
      "step": 64250
    },
    {
      "epoch": 32.68565615462869,
      "grad_norm": 46.5289306640625,
      "learning_rate": 1.7314343845371315e-05,
      "loss": 1.7768,
      "step": 64260
    },
    {
      "epoch": 32.690742624618515,
      "grad_norm": 37.96171951293945,
      "learning_rate": 1.7309257375381485e-05,
      "loss": 1.7878,
      "step": 64270
    },
    {
      "epoch": 32.69582909460834,
      "grad_norm": 36.95965576171875,
      "learning_rate": 1.7304170905391658e-05,
      "loss": 1.8168,
      "step": 64280
    },
    {
      "epoch": 32.70091556459817,
      "grad_norm": 42.65387725830078,
      "learning_rate": 1.729908443540183e-05,
      "loss": 1.7074,
      "step": 64290
    },
    {
      "epoch": 32.706002034587996,
      "grad_norm": 36.696346282958984,
      "learning_rate": 1.7293997965412005e-05,
      "loss": 1.7444,
      "step": 64300
    },
    {
      "epoch": 32.71108850457782,
      "grad_norm": 44.46547317504883,
      "learning_rate": 1.7288911495422178e-05,
      "loss": 1.782,
      "step": 64310
    },
    {
      "epoch": 32.71617497456765,
      "grad_norm": 48.19020462036133,
      "learning_rate": 1.728382502543235e-05,
      "loss": 1.7382,
      "step": 64320
    },
    {
      "epoch": 32.72126144455748,
      "grad_norm": 40.34846878051758,
      "learning_rate": 1.727873855544252e-05,
      "loss": 1.6905,
      "step": 64330
    },
    {
      "epoch": 32.726347914547304,
      "grad_norm": 28.23064613342285,
      "learning_rate": 1.7273652085452698e-05,
      "loss": 1.7267,
      "step": 64340
    },
    {
      "epoch": 32.73143438453713,
      "grad_norm": 36.96116638183594,
      "learning_rate": 1.726856561546287e-05,
      "loss": 1.7175,
      "step": 64350
    },
    {
      "epoch": 32.73652085452696,
      "grad_norm": 35.78957748413086,
      "learning_rate": 1.7263479145473044e-05,
      "loss": 1.6954,
      "step": 64360
    },
    {
      "epoch": 32.741607324516785,
      "grad_norm": 44.04568099975586,
      "learning_rate": 1.7258392675483214e-05,
      "loss": 1.7434,
      "step": 64370
    },
    {
      "epoch": 32.74669379450661,
      "grad_norm": 36.230899810791016,
      "learning_rate": 1.7253306205493387e-05,
      "loss": 1.7121,
      "step": 64380
    },
    {
      "epoch": 32.75178026449644,
      "grad_norm": 35.836692810058594,
      "learning_rate": 1.7248219735503564e-05,
      "loss": 1.7402,
      "step": 64390
    },
    {
      "epoch": 32.756866734486266,
      "grad_norm": 39.18072509765625,
      "learning_rate": 1.7243133265513734e-05,
      "loss": 1.6375,
      "step": 64400
    },
    {
      "epoch": 32.76195320447609,
      "grad_norm": 37.308265686035156,
      "learning_rate": 1.7238046795523907e-05,
      "loss": 1.6202,
      "step": 64410
    },
    {
      "epoch": 32.76703967446592,
      "grad_norm": 37.70729446411133,
      "learning_rate": 1.723296032553408e-05,
      "loss": 1.6939,
      "step": 64420
    },
    {
      "epoch": 32.77212614445575,
      "grad_norm": 32.18051528930664,
      "learning_rate": 1.722787385554425e-05,
      "loss": 1.6744,
      "step": 64430
    },
    {
      "epoch": 32.777212614445574,
      "grad_norm": 37.401161193847656,
      "learning_rate": 1.7222787385554427e-05,
      "loss": 1.7632,
      "step": 64440
    },
    {
      "epoch": 32.7822990844354,
      "grad_norm": 34.593753814697266,
      "learning_rate": 1.72177009155646e-05,
      "loss": 1.7464,
      "step": 64450
    },
    {
      "epoch": 32.78738555442523,
      "grad_norm": 35.07567596435547,
      "learning_rate": 1.721261444557477e-05,
      "loss": 1.6439,
      "step": 64460
    },
    {
      "epoch": 32.792472024415055,
      "grad_norm": 37.84069061279297,
      "learning_rate": 1.7207527975584944e-05,
      "loss": 1.7428,
      "step": 64470
    },
    {
      "epoch": 32.79755849440488,
      "grad_norm": 42.95680618286133,
      "learning_rate": 1.7202441505595117e-05,
      "loss": 1.7637,
      "step": 64480
    },
    {
      "epoch": 32.80264496439471,
      "grad_norm": 33.91150665283203,
      "learning_rate": 1.7197355035605293e-05,
      "loss": 1.8487,
      "step": 64490
    },
    {
      "epoch": 32.807731434384536,
      "grad_norm": 34.38775634765625,
      "learning_rate": 1.7192268565615463e-05,
      "loss": 1.7381,
      "step": 64500
    },
    {
      "epoch": 32.81281790437436,
      "grad_norm": 35.771331787109375,
      "learning_rate": 1.7187182095625637e-05,
      "loss": 1.6532,
      "step": 64510
    },
    {
      "epoch": 32.81790437436419,
      "grad_norm": 31.61294174194336,
      "learning_rate": 1.718209562563581e-05,
      "loss": 1.6112,
      "step": 64520
    },
    {
      "epoch": 32.82299084435402,
      "grad_norm": 46.988563537597656,
      "learning_rate": 1.7177009155645983e-05,
      "loss": 1.735,
      "step": 64530
    },
    {
      "epoch": 32.828077314343844,
      "grad_norm": 48.376739501953125,
      "learning_rate": 1.7171922685656156e-05,
      "loss": 1.7655,
      "step": 64540
    },
    {
      "epoch": 32.83316378433367,
      "grad_norm": 40.13807678222656,
      "learning_rate": 1.716683621566633e-05,
      "loss": 1.6953,
      "step": 64550
    },
    {
      "epoch": 32.8382502543235,
      "grad_norm": 35.86147689819336,
      "learning_rate": 1.71617497456765e-05,
      "loss": 1.6303,
      "step": 64560
    },
    {
      "epoch": 32.843336724313325,
      "grad_norm": 33.35982131958008,
      "learning_rate": 1.7156663275686673e-05,
      "loss": 1.7236,
      "step": 64570
    },
    {
      "epoch": 32.84842319430315,
      "grad_norm": 37.13798141479492,
      "learning_rate": 1.7151576805696846e-05,
      "loss": 1.694,
      "step": 64580
    },
    {
      "epoch": 32.85350966429298,
      "grad_norm": 42.41722106933594,
      "learning_rate": 1.714649033570702e-05,
      "loss": 1.7099,
      "step": 64590
    },
    {
      "epoch": 32.858596134282806,
      "grad_norm": 39.057098388671875,
      "learning_rate": 1.7141403865717193e-05,
      "loss": 1.7263,
      "step": 64600
    },
    {
      "epoch": 32.86368260427263,
      "grad_norm": 41.87001037597656,
      "learning_rate": 1.7136317395727366e-05,
      "loss": 1.6979,
      "step": 64610
    },
    {
      "epoch": 32.86876907426246,
      "grad_norm": 45.18031692504883,
      "learning_rate": 1.713123092573754e-05,
      "loss": 1.8239,
      "step": 64620
    },
    {
      "epoch": 32.87385554425229,
      "grad_norm": 50.41048812866211,
      "learning_rate": 1.7126144455747713e-05,
      "loss": 1.7522,
      "step": 64630
    },
    {
      "epoch": 32.878942014242114,
      "grad_norm": 45.03564453125,
      "learning_rate": 1.7121057985757886e-05,
      "loss": 1.6505,
      "step": 64640
    },
    {
      "epoch": 32.88402848423194,
      "grad_norm": 35.07088088989258,
      "learning_rate": 1.711597151576806e-05,
      "loss": 1.6996,
      "step": 64650
    },
    {
      "epoch": 32.88911495422177,
      "grad_norm": 41.27699661254883,
      "learning_rate": 1.711088504577823e-05,
      "loss": 1.789,
      "step": 64660
    },
    {
      "epoch": 32.894201424211595,
      "grad_norm": 49.13156509399414,
      "learning_rate": 1.7105798575788402e-05,
      "loss": 1.72,
      "step": 64670
    },
    {
      "epoch": 32.89928789420142,
      "grad_norm": 39.104393005371094,
      "learning_rate": 1.710071210579858e-05,
      "loss": 1.6919,
      "step": 64680
    },
    {
      "epoch": 32.90437436419125,
      "grad_norm": 29.09267807006836,
      "learning_rate": 1.709562563580875e-05,
      "loss": 1.7359,
      "step": 64690
    },
    {
      "epoch": 32.909460834181075,
      "grad_norm": 40.087703704833984,
      "learning_rate": 1.7090539165818922e-05,
      "loss": 1.7046,
      "step": 64700
    },
    {
      "epoch": 32.9145473041709,
      "grad_norm": 39.505470275878906,
      "learning_rate": 1.7085452695829096e-05,
      "loss": 1.6633,
      "step": 64710
    },
    {
      "epoch": 32.91963377416073,
      "grad_norm": 38.53520965576172,
      "learning_rate": 1.708036622583927e-05,
      "loss": 1.7401,
      "step": 64720
    },
    {
      "epoch": 32.924720244150556,
      "grad_norm": 37.25163269042969,
      "learning_rate": 1.7075279755849442e-05,
      "loss": 1.7468,
      "step": 64730
    },
    {
      "epoch": 32.92980671414038,
      "grad_norm": 51.02168655395508,
      "learning_rate": 1.7070193285859615e-05,
      "loss": 1.7497,
      "step": 64740
    },
    {
      "epoch": 32.93489318413021,
      "grad_norm": 42.32209014892578,
      "learning_rate": 1.706510681586979e-05,
      "loss": 1.691,
      "step": 64750
    },
    {
      "epoch": 32.93997965412004,
      "grad_norm": 57.84638595581055,
      "learning_rate": 1.706002034587996e-05,
      "loss": 1.6094,
      "step": 64760
    },
    {
      "epoch": 32.945066124109864,
      "grad_norm": 36.10597610473633,
      "learning_rate": 1.7054933875890132e-05,
      "loss": 1.7712,
      "step": 64770
    },
    {
      "epoch": 32.95015259409969,
      "grad_norm": 39.35958480834961,
      "learning_rate": 1.704984740590031e-05,
      "loss": 1.6209,
      "step": 64780
    },
    {
      "epoch": 32.955239064089525,
      "grad_norm": 34.013179779052734,
      "learning_rate": 1.704476093591048e-05,
      "loss": 1.7093,
      "step": 64790
    },
    {
      "epoch": 32.96032553407935,
      "grad_norm": 37.0648078918457,
      "learning_rate": 1.703967446592065e-05,
      "loss": 1.6737,
      "step": 64800
    },
    {
      "epoch": 32.96541200406918,
      "grad_norm": 32.90143966674805,
      "learning_rate": 1.7034587995930825e-05,
      "loss": 1.7382,
      "step": 64810
    },
    {
      "epoch": 32.970498474059006,
      "grad_norm": 36.5656852722168,
      "learning_rate": 1.7029501525940998e-05,
      "loss": 1.6058,
      "step": 64820
    },
    {
      "epoch": 32.97558494404883,
      "grad_norm": 56.78890609741211,
      "learning_rate": 1.702441505595117e-05,
      "loss": 1.7637,
      "step": 64830
    },
    {
      "epoch": 32.98067141403866,
      "grad_norm": 41.74605178833008,
      "learning_rate": 1.7019328585961345e-05,
      "loss": 1.7459,
      "step": 64840
    },
    {
      "epoch": 32.98575788402849,
      "grad_norm": 39.26008224487305,
      "learning_rate": 1.7014242115971515e-05,
      "loss": 1.7141,
      "step": 64850
    },
    {
      "epoch": 32.990844354018314,
      "grad_norm": 33.78649139404297,
      "learning_rate": 1.7009155645981688e-05,
      "loss": 1.7281,
      "step": 64860
    },
    {
      "epoch": 32.99593082400814,
      "grad_norm": 43.81324005126953,
      "learning_rate": 1.7004069175991865e-05,
      "loss": 1.7061,
      "step": 64870
    },
    {
      "epoch": 33.0,
      "eval_loss": 4.846109390258789,
      "eval_runtime": 2.6511,
      "eval_samples_per_second": 1046.754,
      "eval_steps_per_second": 130.891,
      "step": 64878
    },
    {
      "epoch": 33.00101729399797,
      "grad_norm": 52.34648132324219,
      "learning_rate": 1.6998982706002035e-05,
      "loss": 1.7716,
      "step": 64880
    },
    {
      "epoch": 33.006103763987795,
      "grad_norm": 39.8021240234375,
      "learning_rate": 1.6993896236012208e-05,
      "loss": 1.7428,
      "step": 64890
    },
    {
      "epoch": 33.01119023397762,
      "grad_norm": 34.56480407714844,
      "learning_rate": 1.698880976602238e-05,
      "loss": 1.7056,
      "step": 64900
    },
    {
      "epoch": 33.01627670396745,
      "grad_norm": 47.998104095458984,
      "learning_rate": 1.6983723296032554e-05,
      "loss": 1.7827,
      "step": 64910
    },
    {
      "epoch": 33.021363173957276,
      "grad_norm": 37.54241943359375,
      "learning_rate": 1.6978636826042728e-05,
      "loss": 1.6745,
      "step": 64920
    },
    {
      "epoch": 33.0264496439471,
      "grad_norm": 49.05995559692383,
      "learning_rate": 1.69735503560529e-05,
      "loss": 1.6717,
      "step": 64930
    },
    {
      "epoch": 33.03153611393693,
      "grad_norm": 34.62138366699219,
      "learning_rate": 1.6968463886063074e-05,
      "loss": 1.5969,
      "step": 64940
    },
    {
      "epoch": 33.03662258392676,
      "grad_norm": 51.214115142822266,
      "learning_rate": 1.6963377416073244e-05,
      "loss": 1.6557,
      "step": 64950
    },
    {
      "epoch": 33.041709053916584,
      "grad_norm": 31.82766342163086,
      "learning_rate": 1.6958290946083417e-05,
      "loss": 1.6898,
      "step": 64960
    },
    {
      "epoch": 33.04679552390641,
      "grad_norm": 37.508174896240234,
      "learning_rate": 1.6953204476093594e-05,
      "loss": 1.575,
      "step": 64970
    },
    {
      "epoch": 33.05188199389624,
      "grad_norm": 38.485618591308594,
      "learning_rate": 1.6948118006103764e-05,
      "loss": 1.7521,
      "step": 64980
    },
    {
      "epoch": 33.056968463886065,
      "grad_norm": 37.41139602661133,
      "learning_rate": 1.6943031536113937e-05,
      "loss": 1.6684,
      "step": 64990
    },
    {
      "epoch": 33.06205493387589,
      "grad_norm": 43.289634704589844,
      "learning_rate": 1.693794506612411e-05,
      "loss": 1.6533,
      "step": 65000
    },
    {
      "epoch": 33.06714140386572,
      "grad_norm": 48.099578857421875,
      "learning_rate": 1.6932858596134284e-05,
      "loss": 1.6983,
      "step": 65010
    },
    {
      "epoch": 33.072227873855546,
      "grad_norm": 39.55012893676758,
      "learning_rate": 1.6927772126144457e-05,
      "loss": 1.6304,
      "step": 65020
    },
    {
      "epoch": 33.07731434384537,
      "grad_norm": 36.404476165771484,
      "learning_rate": 1.692268565615463e-05,
      "loss": 1.6961,
      "step": 65030
    },
    {
      "epoch": 33.0824008138352,
      "grad_norm": 42.467227935791016,
      "learning_rate": 1.6917599186164804e-05,
      "loss": 1.6702,
      "step": 65040
    },
    {
      "epoch": 33.08748728382503,
      "grad_norm": 44.53964614868164,
      "learning_rate": 1.6912512716174974e-05,
      "loss": 1.6262,
      "step": 65050
    },
    {
      "epoch": 33.092573753814854,
      "grad_norm": 40.853797912597656,
      "learning_rate": 1.6907426246185147e-05,
      "loss": 1.7479,
      "step": 65060
    },
    {
      "epoch": 33.09766022380468,
      "grad_norm": 43.520015716552734,
      "learning_rate": 1.6902339776195323e-05,
      "loss": 1.7389,
      "step": 65070
    },
    {
      "epoch": 33.10274669379451,
      "grad_norm": 39.3851432800293,
      "learning_rate": 1.6897253306205493e-05,
      "loss": 1.8427,
      "step": 65080
    },
    {
      "epoch": 33.107833163784335,
      "grad_norm": 39.26421356201172,
      "learning_rate": 1.6892166836215667e-05,
      "loss": 1.715,
      "step": 65090
    },
    {
      "epoch": 33.11291963377416,
      "grad_norm": 43.32157516479492,
      "learning_rate": 1.688708036622584e-05,
      "loss": 1.7283,
      "step": 65100
    },
    {
      "epoch": 33.11800610376399,
      "grad_norm": 37.954490661621094,
      "learning_rate": 1.6881993896236013e-05,
      "loss": 1.7548,
      "step": 65110
    },
    {
      "epoch": 33.123092573753816,
      "grad_norm": 44.62963104248047,
      "learning_rate": 1.6876907426246187e-05,
      "loss": 1.731,
      "step": 65120
    },
    {
      "epoch": 33.12817904374364,
      "grad_norm": 38.598228454589844,
      "learning_rate": 1.687182095625636e-05,
      "loss": 1.6728,
      "step": 65130
    },
    {
      "epoch": 33.13326551373347,
      "grad_norm": 39.41722106933594,
      "learning_rate": 1.686673448626653e-05,
      "loss": 1.7748,
      "step": 65140
    },
    {
      "epoch": 33.1383519837233,
      "grad_norm": 35.46259307861328,
      "learning_rate": 1.6861648016276703e-05,
      "loss": 1.719,
      "step": 65150
    },
    {
      "epoch": 33.143438453713124,
      "grad_norm": 41.644004821777344,
      "learning_rate": 1.685656154628688e-05,
      "loss": 1.7304,
      "step": 65160
    },
    {
      "epoch": 33.14852492370295,
      "grad_norm": 36.13954162597656,
      "learning_rate": 1.6851475076297053e-05,
      "loss": 1.7367,
      "step": 65170
    },
    {
      "epoch": 33.15361139369278,
      "grad_norm": 35.72758102416992,
      "learning_rate": 1.6846388606307223e-05,
      "loss": 1.6401,
      "step": 65180
    },
    {
      "epoch": 33.158697863682605,
      "grad_norm": 47.051307678222656,
      "learning_rate": 1.6841302136317396e-05,
      "loss": 1.7575,
      "step": 65190
    },
    {
      "epoch": 33.16378433367243,
      "grad_norm": 44.82389450073242,
      "learning_rate": 1.683621566632757e-05,
      "loss": 1.6859,
      "step": 65200
    },
    {
      "epoch": 33.16887080366226,
      "grad_norm": 35.25846862792969,
      "learning_rate": 1.6831129196337743e-05,
      "loss": 1.7442,
      "step": 65210
    },
    {
      "epoch": 33.173957273652086,
      "grad_norm": 41.276390075683594,
      "learning_rate": 1.6826042726347916e-05,
      "loss": 1.7165,
      "step": 65220
    },
    {
      "epoch": 33.17904374364191,
      "grad_norm": 42.59907150268555,
      "learning_rate": 1.682095625635809e-05,
      "loss": 1.6693,
      "step": 65230
    },
    {
      "epoch": 33.18413021363174,
      "grad_norm": 37.5562629699707,
      "learning_rate": 1.681586978636826e-05,
      "loss": 1.7689,
      "step": 65240
    },
    {
      "epoch": 33.18921668362157,
      "grad_norm": 42.1873779296875,
      "learning_rate": 1.6810783316378432e-05,
      "loss": 1.6954,
      "step": 65250
    },
    {
      "epoch": 33.19430315361139,
      "grad_norm": 32.410091400146484,
      "learning_rate": 1.680569684638861e-05,
      "loss": 1.713,
      "step": 65260
    },
    {
      "epoch": 33.19938962360122,
      "grad_norm": 42.9240837097168,
      "learning_rate": 1.680061037639878e-05,
      "loss": 1.6576,
      "step": 65270
    },
    {
      "epoch": 33.20447609359105,
      "grad_norm": 48.310333251953125,
      "learning_rate": 1.6795523906408952e-05,
      "loss": 1.6618,
      "step": 65280
    },
    {
      "epoch": 33.209562563580874,
      "grad_norm": 39.2822151184082,
      "learning_rate": 1.6790437436419126e-05,
      "loss": 1.7557,
      "step": 65290
    },
    {
      "epoch": 33.2146490335707,
      "grad_norm": 36.14345169067383,
      "learning_rate": 1.67853509664293e-05,
      "loss": 1.685,
      "step": 65300
    },
    {
      "epoch": 33.21973550356053,
      "grad_norm": 39.74605941772461,
      "learning_rate": 1.6780264496439472e-05,
      "loss": 1.7013,
      "step": 65310
    },
    {
      "epoch": 33.224821973550355,
      "grad_norm": 58.927494049072266,
      "learning_rate": 1.6775178026449645e-05,
      "loss": 1.7695,
      "step": 65320
    },
    {
      "epoch": 33.22990844354018,
      "grad_norm": 36.058746337890625,
      "learning_rate": 1.677009155645982e-05,
      "loss": 1.6781,
      "step": 65330
    },
    {
      "epoch": 33.23499491353001,
      "grad_norm": 32.51731491088867,
      "learning_rate": 1.676500508646999e-05,
      "loss": 1.6931,
      "step": 65340
    },
    {
      "epoch": 33.240081383519836,
      "grad_norm": 40.07990646362305,
      "learning_rate": 1.6759918616480165e-05,
      "loss": 1.6953,
      "step": 65350
    },
    {
      "epoch": 33.24516785350966,
      "grad_norm": 48.78208923339844,
      "learning_rate": 1.675483214649034e-05,
      "loss": 1.6545,
      "step": 65360
    },
    {
      "epoch": 33.25025432349949,
      "grad_norm": 45.88349151611328,
      "learning_rate": 1.674974567650051e-05,
      "loss": 1.7089,
      "step": 65370
    },
    {
      "epoch": 33.25534079348932,
      "grad_norm": 40.42711639404297,
      "learning_rate": 1.674465920651068e-05,
      "loss": 1.7463,
      "step": 65380
    },
    {
      "epoch": 33.260427263479144,
      "grad_norm": 42.39227294921875,
      "learning_rate": 1.6739572736520855e-05,
      "loss": 1.7152,
      "step": 65390
    },
    {
      "epoch": 33.26551373346897,
      "grad_norm": 44.97173309326172,
      "learning_rate": 1.6734486266531028e-05,
      "loss": 1.6836,
      "step": 65400
    },
    {
      "epoch": 33.2706002034588,
      "grad_norm": 31.22888946533203,
      "learning_rate": 1.67293997965412e-05,
      "loss": 1.7107,
      "step": 65410
    },
    {
      "epoch": 33.275686673448625,
      "grad_norm": 35.48883819580078,
      "learning_rate": 1.6724313326551375e-05,
      "loss": 1.7252,
      "step": 65420
    },
    {
      "epoch": 33.28077314343845,
      "grad_norm": 38.54694747924805,
      "learning_rate": 1.6719226856561548e-05,
      "loss": 1.7827,
      "step": 65430
    },
    {
      "epoch": 33.28585961342828,
      "grad_norm": 45.06674575805664,
      "learning_rate": 1.6714140386571718e-05,
      "loss": 1.7835,
      "step": 65440
    },
    {
      "epoch": 33.290946083418106,
      "grad_norm": 38.620994567871094,
      "learning_rate": 1.6709053916581895e-05,
      "loss": 1.6904,
      "step": 65450
    },
    {
      "epoch": 33.29603255340793,
      "grad_norm": 48.86534118652344,
      "learning_rate": 1.6703967446592068e-05,
      "loss": 1.8234,
      "step": 65460
    },
    {
      "epoch": 33.30111902339776,
      "grad_norm": 39.48125076293945,
      "learning_rate": 1.6698880976602238e-05,
      "loss": 1.6929,
      "step": 65470
    },
    {
      "epoch": 33.30620549338759,
      "grad_norm": 44.7885856628418,
      "learning_rate": 1.669379450661241e-05,
      "loss": 1.7528,
      "step": 65480
    },
    {
      "epoch": 33.311291963377414,
      "grad_norm": 35.02696990966797,
      "learning_rate": 1.6688708036622584e-05,
      "loss": 1.7113,
      "step": 65490
    },
    {
      "epoch": 33.31637843336724,
      "grad_norm": 35.962989807128906,
      "learning_rate": 1.6683621566632758e-05,
      "loss": 1.8287,
      "step": 65500
    },
    {
      "epoch": 33.32146490335707,
      "grad_norm": 42.4984130859375,
      "learning_rate": 1.667853509664293e-05,
      "loss": 1.6728,
      "step": 65510
    },
    {
      "epoch": 33.326551373346895,
      "grad_norm": 41.644405364990234,
      "learning_rate": 1.6673448626653104e-05,
      "loss": 1.7926,
      "step": 65520
    },
    {
      "epoch": 33.33163784333672,
      "grad_norm": 34.954139709472656,
      "learning_rate": 1.6668362156663274e-05,
      "loss": 1.7268,
      "step": 65530
    },
    {
      "epoch": 33.33672431332655,
      "grad_norm": 35.43607711791992,
      "learning_rate": 1.6663275686673447e-05,
      "loss": 1.7755,
      "step": 65540
    },
    {
      "epoch": 33.341810783316376,
      "grad_norm": 29.273696899414062,
      "learning_rate": 1.6658189216683624e-05,
      "loss": 1.6627,
      "step": 65550
    },
    {
      "epoch": 33.3468972533062,
      "grad_norm": 42.76714324951172,
      "learning_rate": 1.6653102746693797e-05,
      "loss": 1.6434,
      "step": 65560
    },
    {
      "epoch": 33.35198372329603,
      "grad_norm": 46.51136779785156,
      "learning_rate": 1.6648016276703967e-05,
      "loss": 1.6833,
      "step": 65570
    },
    {
      "epoch": 33.35707019328586,
      "grad_norm": 38.6019401550293,
      "learning_rate": 1.664292980671414e-05,
      "loss": 1.7271,
      "step": 65580
    },
    {
      "epoch": 33.362156663275684,
      "grad_norm": 36.09532928466797,
      "learning_rate": 1.6637843336724314e-05,
      "loss": 1.6832,
      "step": 65590
    },
    {
      "epoch": 33.36724313326551,
      "grad_norm": 43.2856330871582,
      "learning_rate": 1.6632756866734487e-05,
      "loss": 1.7631,
      "step": 65600
    },
    {
      "epoch": 33.37232960325534,
      "grad_norm": 38.29560470581055,
      "learning_rate": 1.662767039674466e-05,
      "loss": 1.694,
      "step": 65610
    },
    {
      "epoch": 33.377416073245165,
      "grad_norm": 34.42974090576172,
      "learning_rate": 1.6622583926754834e-05,
      "loss": 1.7916,
      "step": 65620
    },
    {
      "epoch": 33.38250254323499,
      "grad_norm": 42.10414505004883,
      "learning_rate": 1.6617497456765004e-05,
      "loss": 1.7157,
      "step": 65630
    },
    {
      "epoch": 33.38758901322482,
      "grad_norm": 41.56431579589844,
      "learning_rate": 1.661241098677518e-05,
      "loss": 1.6769,
      "step": 65640
    },
    {
      "epoch": 33.392675483214646,
      "grad_norm": 35.91154479980469,
      "learning_rate": 1.6607324516785353e-05,
      "loss": 1.6997,
      "step": 65650
    },
    {
      "epoch": 33.39776195320447,
      "grad_norm": 36.62042999267578,
      "learning_rate": 1.6602238046795523e-05,
      "loss": 1.7331,
      "step": 65660
    },
    {
      "epoch": 33.4028484231943,
      "grad_norm": 39.30550765991211,
      "learning_rate": 1.6597151576805697e-05,
      "loss": 1.7034,
      "step": 65670
    },
    {
      "epoch": 33.407934893184134,
      "grad_norm": 39.296260833740234,
      "learning_rate": 1.659206510681587e-05,
      "loss": 1.7121,
      "step": 65680
    },
    {
      "epoch": 33.41302136317396,
      "grad_norm": 40.839115142822266,
      "learning_rate": 1.6586978636826043e-05,
      "loss": 1.7349,
      "step": 65690
    },
    {
      "epoch": 33.41810783316379,
      "grad_norm": 43.52723693847656,
      "learning_rate": 1.6581892166836217e-05,
      "loss": 1.646,
      "step": 65700
    },
    {
      "epoch": 33.423194303153615,
      "grad_norm": 37.32306671142578,
      "learning_rate": 1.657680569684639e-05,
      "loss": 1.7441,
      "step": 65710
    },
    {
      "epoch": 33.42828077314344,
      "grad_norm": 44.06597900390625,
      "learning_rate": 1.6571719226856563e-05,
      "loss": 1.6911,
      "step": 65720
    },
    {
      "epoch": 33.43336724313327,
      "grad_norm": 36.25025939941406,
      "learning_rate": 1.6566632756866733e-05,
      "loss": 1.7541,
      "step": 65730
    },
    {
      "epoch": 33.438453713123096,
      "grad_norm": 40.80827331542969,
      "learning_rate": 1.656154628687691e-05,
      "loss": 1.6395,
      "step": 65740
    },
    {
      "epoch": 33.44354018311292,
      "grad_norm": 35.80050277709961,
      "learning_rate": 1.6556459816887083e-05,
      "loss": 1.7006,
      "step": 65750
    },
    {
      "epoch": 33.44862665310275,
      "grad_norm": 49.00197982788086,
      "learning_rate": 1.6551373346897253e-05,
      "loss": 1.5826,
      "step": 65760
    },
    {
      "epoch": 33.45371312309258,
      "grad_norm": 47.19009017944336,
      "learning_rate": 1.6546286876907426e-05,
      "loss": 1.7092,
      "step": 65770
    },
    {
      "epoch": 33.458799593082404,
      "grad_norm": 33.867427825927734,
      "learning_rate": 1.65412004069176e-05,
      "loss": 1.7253,
      "step": 65780
    },
    {
      "epoch": 33.46388606307223,
      "grad_norm": 36.259971618652344,
      "learning_rate": 1.6536113936927773e-05,
      "loss": 1.6655,
      "step": 65790
    },
    {
      "epoch": 33.46897253306206,
      "grad_norm": 48.559635162353516,
      "learning_rate": 1.6531027466937946e-05,
      "loss": 1.7602,
      "step": 65800
    },
    {
      "epoch": 33.474059003051885,
      "grad_norm": 43.56916046142578,
      "learning_rate": 1.652594099694812e-05,
      "loss": 1.6269,
      "step": 65810
    },
    {
      "epoch": 33.47914547304171,
      "grad_norm": 40.77410888671875,
      "learning_rate": 1.652085452695829e-05,
      "loss": 1.6614,
      "step": 65820
    },
    {
      "epoch": 33.48423194303154,
      "grad_norm": 33.54120635986328,
      "learning_rate": 1.6515768056968466e-05,
      "loss": 1.721,
      "step": 65830
    },
    {
      "epoch": 33.489318413021365,
      "grad_norm": 40.97697448730469,
      "learning_rate": 1.651068158697864e-05,
      "loss": 1.8007,
      "step": 65840
    },
    {
      "epoch": 33.49440488301119,
      "grad_norm": 44.42280960083008,
      "learning_rate": 1.6505595116988812e-05,
      "loss": 1.7031,
      "step": 65850
    },
    {
      "epoch": 33.49949135300102,
      "grad_norm": 42.70532989501953,
      "learning_rate": 1.6500508646998982e-05,
      "loss": 1.6405,
      "step": 65860
    },
    {
      "epoch": 33.504577822990846,
      "grad_norm": 45.13911819458008,
      "learning_rate": 1.6495422177009156e-05,
      "loss": 1.7154,
      "step": 65870
    },
    {
      "epoch": 33.50966429298067,
      "grad_norm": 39.798179626464844,
      "learning_rate": 1.649033570701933e-05,
      "loss": 1.7132,
      "step": 65880
    },
    {
      "epoch": 33.5147507629705,
      "grad_norm": 36.88596725463867,
      "learning_rate": 1.6485249237029502e-05,
      "loss": 1.6991,
      "step": 65890
    },
    {
      "epoch": 33.51983723296033,
      "grad_norm": 50.1884651184082,
      "learning_rate": 1.6480162767039675e-05,
      "loss": 1.6682,
      "step": 65900
    },
    {
      "epoch": 33.524923702950154,
      "grad_norm": 43.12214660644531,
      "learning_rate": 1.647507629704985e-05,
      "loss": 1.7016,
      "step": 65910
    },
    {
      "epoch": 33.53001017293998,
      "grad_norm": 35.0809211730957,
      "learning_rate": 1.646998982706002e-05,
      "loss": 1.7284,
      "step": 65920
    },
    {
      "epoch": 33.53509664292981,
      "grad_norm": 37.60546875,
      "learning_rate": 1.6464903357070195e-05,
      "loss": 1.6952,
      "step": 65930
    },
    {
      "epoch": 33.540183112919635,
      "grad_norm": 54.206809997558594,
      "learning_rate": 1.645981688708037e-05,
      "loss": 1.6939,
      "step": 65940
    },
    {
      "epoch": 33.54526958290946,
      "grad_norm": 47.60437774658203,
      "learning_rate": 1.645473041709054e-05,
      "loss": 1.7148,
      "step": 65950
    },
    {
      "epoch": 33.55035605289929,
      "grad_norm": 40.493003845214844,
      "learning_rate": 1.644964394710071e-05,
      "loss": 1.6607,
      "step": 65960
    },
    {
      "epoch": 33.555442522889116,
      "grad_norm": 41.86994171142578,
      "learning_rate": 1.6444557477110885e-05,
      "loss": 1.7476,
      "step": 65970
    },
    {
      "epoch": 33.56052899287894,
      "grad_norm": 32.53776550292969,
      "learning_rate": 1.643947100712106e-05,
      "loss": 1.7017,
      "step": 65980
    },
    {
      "epoch": 33.56561546286877,
      "grad_norm": 36.219451904296875,
      "learning_rate": 1.643438453713123e-05,
      "loss": 1.7609,
      "step": 65990
    },
    {
      "epoch": 33.5707019328586,
      "grad_norm": 43.50033187866211,
      "learning_rate": 1.6429298067141405e-05,
      "loss": 1.6677,
      "step": 66000
    },
    {
      "epoch": 33.575788402848424,
      "grad_norm": 42.05838394165039,
      "learning_rate": 1.6424211597151578e-05,
      "loss": 1.7848,
      "step": 66010
    },
    {
      "epoch": 33.58087487283825,
      "grad_norm": 45.55509567260742,
      "learning_rate": 1.6419125127161748e-05,
      "loss": 1.7105,
      "step": 66020
    },
    {
      "epoch": 33.58596134282808,
      "grad_norm": 34.14195251464844,
      "learning_rate": 1.6414038657171925e-05,
      "loss": 1.6784,
      "step": 66030
    },
    {
      "epoch": 33.591047812817905,
      "grad_norm": 40.74403762817383,
      "learning_rate": 1.6408952187182098e-05,
      "loss": 1.6625,
      "step": 66040
    },
    {
      "epoch": 33.59613428280773,
      "grad_norm": 38.34896469116211,
      "learning_rate": 1.6403865717192268e-05,
      "loss": 1.6354,
      "step": 66050
    },
    {
      "epoch": 33.60122075279756,
      "grad_norm": 48.21662521362305,
      "learning_rate": 1.639877924720244e-05,
      "loss": 1.6887,
      "step": 66060
    },
    {
      "epoch": 33.606307222787386,
      "grad_norm": 41.214115142822266,
      "learning_rate": 1.6393692777212614e-05,
      "loss": 1.7131,
      "step": 66070
    },
    {
      "epoch": 33.61139369277721,
      "grad_norm": 44.58103942871094,
      "learning_rate": 1.6388606307222788e-05,
      "loss": 1.6142,
      "step": 66080
    },
    {
      "epoch": 33.61648016276704,
      "grad_norm": 40.103939056396484,
      "learning_rate": 1.638351983723296e-05,
      "loss": 1.7574,
      "step": 66090
    },
    {
      "epoch": 33.62156663275687,
      "grad_norm": 41.918418884277344,
      "learning_rate": 1.6378433367243134e-05,
      "loss": 1.7722,
      "step": 66100
    },
    {
      "epoch": 33.626653102746694,
      "grad_norm": 39.3565788269043,
      "learning_rate": 1.6373346897253308e-05,
      "loss": 1.651,
      "step": 66110
    },
    {
      "epoch": 33.63173957273652,
      "grad_norm": 35.64944076538086,
      "learning_rate": 1.636826042726348e-05,
      "loss": 1.655,
      "step": 66120
    },
    {
      "epoch": 33.63682604272635,
      "grad_norm": 48.91822814941406,
      "learning_rate": 1.6363173957273654e-05,
      "loss": 1.7281,
      "step": 66130
    },
    {
      "epoch": 33.641912512716175,
      "grad_norm": 35.290157318115234,
      "learning_rate": 1.6358087487283827e-05,
      "loss": 1.6486,
      "step": 66140
    },
    {
      "epoch": 33.646998982706,
      "grad_norm": 43.99591064453125,
      "learning_rate": 1.6353001017293997e-05,
      "loss": 1.6737,
      "step": 66150
    },
    {
      "epoch": 33.65208545269583,
      "grad_norm": 35.09580612182617,
      "learning_rate": 1.634791454730417e-05,
      "loss": 1.817,
      "step": 66160
    },
    {
      "epoch": 33.657171922685656,
      "grad_norm": 31.446928024291992,
      "learning_rate": 1.6342828077314347e-05,
      "loss": 1.6752,
      "step": 66170
    },
    {
      "epoch": 33.66225839267548,
      "grad_norm": 40.42402267456055,
      "learning_rate": 1.6337741607324517e-05,
      "loss": 1.682,
      "step": 66180
    },
    {
      "epoch": 33.66734486266531,
      "grad_norm": 38.92407989501953,
      "learning_rate": 1.633265513733469e-05,
      "loss": 1.6105,
      "step": 66190
    },
    {
      "epoch": 33.67243133265514,
      "grad_norm": 40.0413703918457,
      "learning_rate": 1.6327568667344864e-05,
      "loss": 1.6129,
      "step": 66200
    },
    {
      "epoch": 33.677517802644964,
      "grad_norm": 38.026580810546875,
      "learning_rate": 1.6322482197355034e-05,
      "loss": 1.715,
      "step": 66210
    },
    {
      "epoch": 33.68260427263479,
      "grad_norm": 48.15274429321289,
      "learning_rate": 1.631739572736521e-05,
      "loss": 1.7258,
      "step": 66220
    },
    {
      "epoch": 33.68769074262462,
      "grad_norm": 36.48751449584961,
      "learning_rate": 1.6312309257375383e-05,
      "loss": 1.6337,
      "step": 66230
    },
    {
      "epoch": 33.692777212614445,
      "grad_norm": 36.83578872680664,
      "learning_rate": 1.6307222787385557e-05,
      "loss": 1.6674,
      "step": 66240
    },
    {
      "epoch": 33.69786368260427,
      "grad_norm": 41.99692916870117,
      "learning_rate": 1.6302136317395727e-05,
      "loss": 1.7191,
      "step": 66250
    },
    {
      "epoch": 33.7029501525941,
      "grad_norm": 46.249046325683594,
      "learning_rate": 1.62970498474059e-05,
      "loss": 1.722,
      "step": 66260
    },
    {
      "epoch": 33.708036622583926,
      "grad_norm": 39.39461898803711,
      "learning_rate": 1.6291963377416077e-05,
      "loss": 1.7245,
      "step": 66270
    },
    {
      "epoch": 33.71312309257375,
      "grad_norm": 44.1802864074707,
      "learning_rate": 1.6286876907426247e-05,
      "loss": 1.7632,
      "step": 66280
    },
    {
      "epoch": 33.71820956256358,
      "grad_norm": 35.11503601074219,
      "learning_rate": 1.628179043743642e-05,
      "loss": 1.6273,
      "step": 66290
    },
    {
      "epoch": 33.72329603255341,
      "grad_norm": 34.17839431762695,
      "learning_rate": 1.6276703967446593e-05,
      "loss": 1.6257,
      "step": 66300
    },
    {
      "epoch": 33.728382502543234,
      "grad_norm": 43.72420120239258,
      "learning_rate": 1.6271617497456766e-05,
      "loss": 1.6757,
      "step": 66310
    },
    {
      "epoch": 33.73346897253306,
      "grad_norm": 47.77770233154297,
      "learning_rate": 1.626653102746694e-05,
      "loss": 1.6755,
      "step": 66320
    },
    {
      "epoch": 33.73855544252289,
      "grad_norm": 38.841983795166016,
      "learning_rate": 1.6261444557477113e-05,
      "loss": 1.768,
      "step": 66330
    },
    {
      "epoch": 33.743641912512714,
      "grad_norm": 38.56241989135742,
      "learning_rate": 1.6256358087487283e-05,
      "loss": 1.7344,
      "step": 66340
    },
    {
      "epoch": 33.74872838250254,
      "grad_norm": 42.47172927856445,
      "learning_rate": 1.6251271617497456e-05,
      "loss": 1.7852,
      "step": 66350
    },
    {
      "epoch": 33.75381485249237,
      "grad_norm": 42.66585922241211,
      "learning_rate": 1.624618514750763e-05,
      "loss": 1.7326,
      "step": 66360
    },
    {
      "epoch": 33.758901322482195,
      "grad_norm": 32.49430847167969,
      "learning_rate": 1.6241098677517806e-05,
      "loss": 1.7453,
      "step": 66370
    },
    {
      "epoch": 33.76398779247202,
      "grad_norm": 43.283958435058594,
      "learning_rate": 1.6236012207527976e-05,
      "loss": 1.7113,
      "step": 66380
    },
    {
      "epoch": 33.76907426246185,
      "grad_norm": 37.93527603149414,
      "learning_rate": 1.623092573753815e-05,
      "loss": 1.7776,
      "step": 66390
    },
    {
      "epoch": 33.774160732451676,
      "grad_norm": 35.19839096069336,
      "learning_rate": 1.6225839267548323e-05,
      "loss": 1.7329,
      "step": 66400
    },
    {
      "epoch": 33.7792472024415,
      "grad_norm": 34.874183654785156,
      "learning_rate": 1.6220752797558496e-05,
      "loss": 1.7506,
      "step": 66410
    },
    {
      "epoch": 33.78433367243133,
      "grad_norm": 35.31162643432617,
      "learning_rate": 1.621566632756867e-05,
      "loss": 1.6431,
      "step": 66420
    },
    {
      "epoch": 33.78942014242116,
      "grad_norm": 40.28310012817383,
      "learning_rate": 1.6210579857578842e-05,
      "loss": 1.7303,
      "step": 66430
    },
    {
      "epoch": 33.794506612410984,
      "grad_norm": 35.95439910888672,
      "learning_rate": 1.6205493387589012e-05,
      "loss": 1.6607,
      "step": 66440
    },
    {
      "epoch": 33.79959308240081,
      "grad_norm": 46.01667785644531,
      "learning_rate": 1.6200406917599186e-05,
      "loss": 1.7154,
      "step": 66450
    },
    {
      "epoch": 33.80467955239064,
      "grad_norm": 36.815643310546875,
      "learning_rate": 1.6195320447609362e-05,
      "loss": 1.6704,
      "step": 66460
    },
    {
      "epoch": 33.809766022380465,
      "grad_norm": 40.83102798461914,
      "learning_rate": 1.6190233977619532e-05,
      "loss": 1.7343,
      "step": 66470
    },
    {
      "epoch": 33.81485249237029,
      "grad_norm": 37.738677978515625,
      "learning_rate": 1.6185147507629705e-05,
      "loss": 1.7532,
      "step": 66480
    },
    {
      "epoch": 33.81993896236012,
      "grad_norm": 51.4979362487793,
      "learning_rate": 1.618006103763988e-05,
      "loss": 1.6171,
      "step": 66490
    },
    {
      "epoch": 33.825025432349946,
      "grad_norm": 37.661746978759766,
      "learning_rate": 1.6174974567650052e-05,
      "loss": 1.7218,
      "step": 66500
    },
    {
      "epoch": 33.83011190233977,
      "grad_norm": 49.706363677978516,
      "learning_rate": 1.6169888097660225e-05,
      "loss": 1.6769,
      "step": 66510
    },
    {
      "epoch": 33.8351983723296,
      "grad_norm": 39.99274444580078,
      "learning_rate": 1.61648016276704e-05,
      "loss": 1.5802,
      "step": 66520
    },
    {
      "epoch": 33.84028484231943,
      "grad_norm": 38.30485916137695,
      "learning_rate": 1.6159715157680572e-05,
      "loss": 1.676,
      "step": 66530
    },
    {
      "epoch": 33.845371312309254,
      "grad_norm": 45.74701690673828,
      "learning_rate": 1.615462868769074e-05,
      "loss": 1.624,
      "step": 66540
    },
    {
      "epoch": 33.85045778229908,
      "grad_norm": 37.84447479248047,
      "learning_rate": 1.6149542217700915e-05,
      "loss": 1.6959,
      "step": 66550
    },
    {
      "epoch": 33.85554425228891,
      "grad_norm": 39.23143768310547,
      "learning_rate": 1.614445574771109e-05,
      "loss": 1.8223,
      "step": 66560
    },
    {
      "epoch": 33.86063072227874,
      "grad_norm": 32.5533561706543,
      "learning_rate": 1.613936927772126e-05,
      "loss": 1.7357,
      "step": 66570
    },
    {
      "epoch": 33.86571719226856,
      "grad_norm": 42.486610412597656,
      "learning_rate": 1.6134282807731435e-05,
      "loss": 1.6903,
      "step": 66580
    },
    {
      "epoch": 33.870803662258396,
      "grad_norm": 38.27682876586914,
      "learning_rate": 1.6129196337741608e-05,
      "loss": 1.6733,
      "step": 66590
    },
    {
      "epoch": 33.87589013224822,
      "grad_norm": 33.239933013916016,
      "learning_rate": 1.612410986775178e-05,
      "loss": 1.6346,
      "step": 66600
    },
    {
      "epoch": 33.88097660223805,
      "grad_norm": 37.7008171081543,
      "learning_rate": 1.6119023397761955e-05,
      "loss": 1.7406,
      "step": 66610
    },
    {
      "epoch": 33.88606307222788,
      "grad_norm": 45.5153694152832,
      "learning_rate": 1.6113936927772128e-05,
      "loss": 1.6456,
      "step": 66620
    },
    {
      "epoch": 33.891149542217704,
      "grad_norm": 39.39666748046875,
      "learning_rate": 1.6108850457782298e-05,
      "loss": 1.796,
      "step": 66630
    },
    {
      "epoch": 33.89623601220753,
      "grad_norm": 36.62692642211914,
      "learning_rate": 1.610376398779247e-05,
      "loss": 1.7375,
      "step": 66640
    },
    {
      "epoch": 33.90132248219736,
      "grad_norm": 40.172203063964844,
      "learning_rate": 1.6098677517802648e-05,
      "loss": 1.766,
      "step": 66650
    },
    {
      "epoch": 33.906408952187185,
      "grad_norm": 38.509944915771484,
      "learning_rate": 1.609359104781282e-05,
      "loss": 1.7545,
      "step": 66660
    },
    {
      "epoch": 33.91149542217701,
      "grad_norm": 51.641536712646484,
      "learning_rate": 1.608850457782299e-05,
      "loss": 1.7108,
      "step": 66670
    },
    {
      "epoch": 33.91658189216684,
      "grad_norm": 34.614444732666016,
      "learning_rate": 1.6083418107833164e-05,
      "loss": 1.6141,
      "step": 66680
    },
    {
      "epoch": 33.921668362156666,
      "grad_norm": 42.39067840576172,
      "learning_rate": 1.6078331637843338e-05,
      "loss": 1.7334,
      "step": 66690
    },
    {
      "epoch": 33.92675483214649,
      "grad_norm": 47.00657272338867,
      "learning_rate": 1.607324516785351e-05,
      "loss": 1.6997,
      "step": 66700
    },
    {
      "epoch": 33.93184130213632,
      "grad_norm": 38.89923858642578,
      "learning_rate": 1.6068158697863684e-05,
      "loss": 1.6509,
      "step": 66710
    },
    {
      "epoch": 33.93692777212615,
      "grad_norm": 34.97571563720703,
      "learning_rate": 1.6063072227873857e-05,
      "loss": 1.6952,
      "step": 66720
    },
    {
      "epoch": 33.942014242115974,
      "grad_norm": 36.081050872802734,
      "learning_rate": 1.6057985757884027e-05,
      "loss": 1.6921,
      "step": 66730
    },
    {
      "epoch": 33.9471007121058,
      "grad_norm": 44.50491714477539,
      "learning_rate": 1.60528992878942e-05,
      "loss": 1.6951,
      "step": 66740
    },
    {
      "epoch": 33.95218718209563,
      "grad_norm": 33.07954025268555,
      "learning_rate": 1.6047812817904377e-05,
      "loss": 1.7879,
      "step": 66750
    },
    {
      "epoch": 33.957273652085455,
      "grad_norm": 39.60123062133789,
      "learning_rate": 1.6042726347914547e-05,
      "loss": 1.7267,
      "step": 66760
    },
    {
      "epoch": 33.96236012207528,
      "grad_norm": 34.14407730102539,
      "learning_rate": 1.603763987792472e-05,
      "loss": 1.7134,
      "step": 66770
    },
    {
      "epoch": 33.96744659206511,
      "grad_norm": 39.24464797973633,
      "learning_rate": 1.6032553407934894e-05,
      "loss": 1.6754,
      "step": 66780
    },
    {
      "epoch": 33.972533062054936,
      "grad_norm": 38.46894836425781,
      "learning_rate": 1.6027466937945067e-05,
      "loss": 1.6903,
      "step": 66790
    },
    {
      "epoch": 33.97761953204476,
      "grad_norm": 30.586071014404297,
      "learning_rate": 1.602238046795524e-05,
      "loss": 1.6272,
      "step": 66800
    },
    {
      "epoch": 33.98270600203459,
      "grad_norm": 52.37477493286133,
      "learning_rate": 1.6017293997965414e-05,
      "loss": 1.6804,
      "step": 66810
    },
    {
      "epoch": 33.98779247202442,
      "grad_norm": 38.12887954711914,
      "learning_rate": 1.6012207527975587e-05,
      "loss": 1.7008,
      "step": 66820
    },
    {
      "epoch": 33.992878942014244,
      "grad_norm": 36.063621520996094,
      "learning_rate": 1.6007121057985757e-05,
      "loss": 1.7112,
      "step": 66830
    },
    {
      "epoch": 33.99796541200407,
      "grad_norm": 56.30854415893555,
      "learning_rate": 1.600203458799593e-05,
      "loss": 1.6296,
      "step": 66840
    },
    {
      "epoch": 34.0,
      "eval_loss": 4.88789701461792,
      "eval_runtime": 2.6786,
      "eval_samples_per_second": 1036.005,
      "eval_steps_per_second": 129.547,
      "step": 66844
    },
    {
      "epoch": 34.0030518819939,
      "grad_norm": 42.18602752685547,
      "learning_rate": 1.5996948118006107e-05,
      "loss": 1.7305,
      "step": 66850
    },
    {
      "epoch": 34.008138351983725,
      "grad_norm": 32.619102478027344,
      "learning_rate": 1.5991861648016277e-05,
      "loss": 1.6504,
      "step": 66860
    },
    {
      "epoch": 34.01322482197355,
      "grad_norm": 42.50737380981445,
      "learning_rate": 1.598677517802645e-05,
      "loss": 1.6996,
      "step": 66870
    },
    {
      "epoch": 34.01831129196338,
      "grad_norm": 37.713985443115234,
      "learning_rate": 1.5981688708036623e-05,
      "loss": 1.6838,
      "step": 66880
    },
    {
      "epoch": 34.023397761953206,
      "grad_norm": 35.201438903808594,
      "learning_rate": 1.5976602238046796e-05,
      "loss": 1.7127,
      "step": 66890
    },
    {
      "epoch": 34.02848423194303,
      "grad_norm": 44.52172088623047,
      "learning_rate": 1.597151576805697e-05,
      "loss": 1.5493,
      "step": 66900
    },
    {
      "epoch": 34.03357070193286,
      "grad_norm": 45.767173767089844,
      "learning_rate": 1.5966429298067143e-05,
      "loss": 1.7131,
      "step": 66910
    },
    {
      "epoch": 34.03865717192269,
      "grad_norm": 37.02889633178711,
      "learning_rate": 1.5961342828077316e-05,
      "loss": 1.6862,
      "step": 66920
    },
    {
      "epoch": 34.04374364191251,
      "grad_norm": 38.189273834228516,
      "learning_rate": 1.5956256358087486e-05,
      "loss": 1.7557,
      "step": 66930
    },
    {
      "epoch": 34.04883011190234,
      "grad_norm": 40.38737869262695,
      "learning_rate": 1.5951169888097663e-05,
      "loss": 1.6169,
      "step": 66940
    },
    {
      "epoch": 34.05391658189217,
      "grad_norm": 43.098236083984375,
      "learning_rate": 1.5946083418107836e-05,
      "loss": 1.6901,
      "step": 66950
    },
    {
      "epoch": 34.059003051881994,
      "grad_norm": 35.61752700805664,
      "learning_rate": 1.5940996948118006e-05,
      "loss": 1.5826,
      "step": 66960
    },
    {
      "epoch": 34.06408952187182,
      "grad_norm": 44.57657241821289,
      "learning_rate": 1.593591047812818e-05,
      "loss": 1.7139,
      "step": 66970
    },
    {
      "epoch": 34.06917599186165,
      "grad_norm": 31.042888641357422,
      "learning_rate": 1.5930824008138353e-05,
      "loss": 1.701,
      "step": 66980
    },
    {
      "epoch": 34.074262461851475,
      "grad_norm": 37.587608337402344,
      "learning_rate": 1.5925737538148526e-05,
      "loss": 1.683,
      "step": 66990
    },
    {
      "epoch": 34.0793489318413,
      "grad_norm": 37.870357513427734,
      "learning_rate": 1.59206510681587e-05,
      "loss": 1.7371,
      "step": 67000
    },
    {
      "epoch": 34.08443540183113,
      "grad_norm": 31.0501651763916,
      "learning_rate": 1.5915564598168872e-05,
      "loss": 1.7288,
      "step": 67010
    },
    {
      "epoch": 34.089521871820956,
      "grad_norm": 43.47040557861328,
      "learning_rate": 1.5910478128179042e-05,
      "loss": 1.7284,
      "step": 67020
    },
    {
      "epoch": 34.09460834181078,
      "grad_norm": 49.07859420776367,
      "learning_rate": 1.5905391658189216e-05,
      "loss": 1.6462,
      "step": 67030
    },
    {
      "epoch": 34.09969481180061,
      "grad_norm": 49.1983528137207,
      "learning_rate": 1.5900305188199392e-05,
      "loss": 1.7547,
      "step": 67040
    },
    {
      "epoch": 34.10478128179044,
      "grad_norm": 46.112571716308594,
      "learning_rate": 1.5895218718209565e-05,
      "loss": 1.7107,
      "step": 67050
    },
    {
      "epoch": 34.109867751780264,
      "grad_norm": 39.52889633178711,
      "learning_rate": 1.5890132248219735e-05,
      "loss": 1.7497,
      "step": 67060
    },
    {
      "epoch": 34.11495422177009,
      "grad_norm": 40.81619644165039,
      "learning_rate": 1.588504577822991e-05,
      "loss": 1.6405,
      "step": 67070
    },
    {
      "epoch": 34.12004069175992,
      "grad_norm": 33.85390090942383,
      "learning_rate": 1.5879959308240082e-05,
      "loss": 1.7075,
      "step": 67080
    },
    {
      "epoch": 34.125127161749745,
      "grad_norm": 34.881595611572266,
      "learning_rate": 1.5874872838250255e-05,
      "loss": 1.727,
      "step": 67090
    },
    {
      "epoch": 34.13021363173957,
      "grad_norm": 42.524436950683594,
      "learning_rate": 1.586978636826043e-05,
      "loss": 1.6655,
      "step": 67100
    },
    {
      "epoch": 34.1353001017294,
      "grad_norm": 42.2000617980957,
      "learning_rate": 1.5864699898270602e-05,
      "loss": 1.7024,
      "step": 67110
    },
    {
      "epoch": 34.140386571719226,
      "grad_norm": 46.535823822021484,
      "learning_rate": 1.585961342828077e-05,
      "loss": 1.6666,
      "step": 67120
    },
    {
      "epoch": 34.14547304170905,
      "grad_norm": 42.884483337402344,
      "learning_rate": 1.585452695829095e-05,
      "loss": 1.7048,
      "step": 67130
    },
    {
      "epoch": 34.15055951169888,
      "grad_norm": 36.66842269897461,
      "learning_rate": 1.584944048830112e-05,
      "loss": 1.6361,
      "step": 67140
    },
    {
      "epoch": 34.15564598168871,
      "grad_norm": 31.17749786376953,
      "learning_rate": 1.584435401831129e-05,
      "loss": 1.781,
      "step": 67150
    },
    {
      "epoch": 34.160732451678534,
      "grad_norm": 37.54154586791992,
      "learning_rate": 1.5839267548321465e-05,
      "loss": 1.6913,
      "step": 67160
    },
    {
      "epoch": 34.16581892166836,
      "grad_norm": 42.94895935058594,
      "learning_rate": 1.5834181078331638e-05,
      "loss": 1.7111,
      "step": 67170
    },
    {
      "epoch": 34.17090539165819,
      "grad_norm": 44.95012283325195,
      "learning_rate": 1.582909460834181e-05,
      "loss": 1.6734,
      "step": 67180
    },
    {
      "epoch": 34.175991861648015,
      "grad_norm": 41.06620788574219,
      "learning_rate": 1.5824008138351985e-05,
      "loss": 1.7191,
      "step": 67190
    },
    {
      "epoch": 34.18107833163784,
      "grad_norm": 45.15015411376953,
      "learning_rate": 1.5818921668362158e-05,
      "loss": 1.7508,
      "step": 67200
    },
    {
      "epoch": 34.18616480162767,
      "grad_norm": 42.28960037231445,
      "learning_rate": 1.581383519837233e-05,
      "loss": 1.7217,
      "step": 67210
    },
    {
      "epoch": 34.191251271617496,
      "grad_norm": 40.989688873291016,
      "learning_rate": 1.58087487283825e-05,
      "loss": 1.8572,
      "step": 67220
    },
    {
      "epoch": 34.19633774160732,
      "grad_norm": 40.75460433959961,
      "learning_rate": 1.5803662258392678e-05,
      "loss": 1.6556,
      "step": 67230
    },
    {
      "epoch": 34.20142421159715,
      "grad_norm": 41.64552307128906,
      "learning_rate": 1.579857578840285e-05,
      "loss": 1.8086,
      "step": 67240
    },
    {
      "epoch": 34.20651068158698,
      "grad_norm": 36.10709762573242,
      "learning_rate": 1.579348931841302e-05,
      "loss": 1.6847,
      "step": 67250
    },
    {
      "epoch": 34.211597151576804,
      "grad_norm": 52.2971076965332,
      "learning_rate": 1.5788402848423194e-05,
      "loss": 1.753,
      "step": 67260
    },
    {
      "epoch": 34.21668362156663,
      "grad_norm": 48.65231704711914,
      "learning_rate": 1.5783316378433368e-05,
      "loss": 1.6845,
      "step": 67270
    },
    {
      "epoch": 34.22177009155646,
      "grad_norm": 41.4185905456543,
      "learning_rate": 1.577822990844354e-05,
      "loss": 1.6899,
      "step": 67280
    },
    {
      "epoch": 34.226856561546285,
      "grad_norm": 39.080055236816406,
      "learning_rate": 1.5773143438453714e-05,
      "loss": 1.6747,
      "step": 67290
    },
    {
      "epoch": 34.23194303153611,
      "grad_norm": 35.404396057128906,
      "learning_rate": 1.5768056968463887e-05,
      "loss": 1.6219,
      "step": 67300
    },
    {
      "epoch": 34.23702950152594,
      "grad_norm": 41.517555236816406,
      "learning_rate": 1.5762970498474057e-05,
      "loss": 1.7059,
      "step": 67310
    },
    {
      "epoch": 34.242115971515766,
      "grad_norm": 42.26051330566406,
      "learning_rate": 1.575788402848423e-05,
      "loss": 1.7904,
      "step": 67320
    },
    {
      "epoch": 34.24720244150559,
      "grad_norm": 35.87477493286133,
      "learning_rate": 1.5752797558494407e-05,
      "loss": 1.6679,
      "step": 67330
    },
    {
      "epoch": 34.25228891149542,
      "grad_norm": 39.741546630859375,
      "learning_rate": 1.574771108850458e-05,
      "loss": 1.7265,
      "step": 67340
    },
    {
      "epoch": 34.25737538148525,
      "grad_norm": 40.799713134765625,
      "learning_rate": 1.574262461851475e-05,
      "loss": 1.7089,
      "step": 67350
    },
    {
      "epoch": 34.262461851475074,
      "grad_norm": 42.999778747558594,
      "learning_rate": 1.5737538148524924e-05,
      "loss": 1.6318,
      "step": 67360
    },
    {
      "epoch": 34.2675483214649,
      "grad_norm": 33.16391372680664,
      "learning_rate": 1.5732451678535097e-05,
      "loss": 1.6015,
      "step": 67370
    },
    {
      "epoch": 34.27263479145473,
      "grad_norm": 35.020626068115234,
      "learning_rate": 1.572736520854527e-05,
      "loss": 1.6475,
      "step": 67380
    },
    {
      "epoch": 34.277721261444555,
      "grad_norm": 38.384033203125,
      "learning_rate": 1.5722278738555444e-05,
      "loss": 1.6524,
      "step": 67390
    },
    {
      "epoch": 34.28280773143438,
      "grad_norm": 45.26815414428711,
      "learning_rate": 1.5717192268565617e-05,
      "loss": 1.7063,
      "step": 67400
    },
    {
      "epoch": 34.28789420142421,
      "grad_norm": 40.17645263671875,
      "learning_rate": 1.5712105798575787e-05,
      "loss": 1.5962,
      "step": 67410
    },
    {
      "epoch": 34.292980671414035,
      "grad_norm": 43.7415657043457,
      "learning_rate": 1.5707019328585963e-05,
      "loss": 1.5714,
      "step": 67420
    },
    {
      "epoch": 34.29806714140386,
      "grad_norm": 37.2534294128418,
      "learning_rate": 1.5701932858596137e-05,
      "loss": 1.67,
      "step": 67430
    },
    {
      "epoch": 34.30315361139369,
      "grad_norm": 39.66645812988281,
      "learning_rate": 1.5696846388606307e-05,
      "loss": 1.6878,
      "step": 67440
    },
    {
      "epoch": 34.308240081383516,
      "grad_norm": 33.28535079956055,
      "learning_rate": 1.569175991861648e-05,
      "loss": 1.6807,
      "step": 67450
    },
    {
      "epoch": 34.31332655137334,
      "grad_norm": 35.16516876220703,
      "learning_rate": 1.5686673448626653e-05,
      "loss": 1.6117,
      "step": 67460
    },
    {
      "epoch": 34.31841302136317,
      "grad_norm": 28.576343536376953,
      "learning_rate": 1.5681586978636826e-05,
      "loss": 1.7116,
      "step": 67470
    },
    {
      "epoch": 34.323499491353004,
      "grad_norm": 44.955631256103516,
      "learning_rate": 1.5676500508647e-05,
      "loss": 1.7018,
      "step": 67480
    },
    {
      "epoch": 34.32858596134283,
      "grad_norm": 36.87089157104492,
      "learning_rate": 1.5671414038657173e-05,
      "loss": 1.6712,
      "step": 67490
    },
    {
      "epoch": 34.33367243133266,
      "grad_norm": 41.780029296875,
      "learning_rate": 1.5666327568667346e-05,
      "loss": 1.6381,
      "step": 67500
    },
    {
      "epoch": 34.338758901322485,
      "grad_norm": 37.98611831665039,
      "learning_rate": 1.5661241098677516e-05,
      "loss": 1.7264,
      "step": 67510
    },
    {
      "epoch": 34.34384537131231,
      "grad_norm": 35.85358428955078,
      "learning_rate": 1.5656154628687693e-05,
      "loss": 1.67,
      "step": 67520
    },
    {
      "epoch": 34.34893184130214,
      "grad_norm": 44.51770782470703,
      "learning_rate": 1.5651068158697866e-05,
      "loss": 1.7508,
      "step": 67530
    },
    {
      "epoch": 34.354018311291966,
      "grad_norm": 38.910438537597656,
      "learning_rate": 1.5645981688708036e-05,
      "loss": 1.6929,
      "step": 67540
    },
    {
      "epoch": 34.35910478128179,
      "grad_norm": 40.25079345703125,
      "learning_rate": 1.564089521871821e-05,
      "loss": 1.6007,
      "step": 67550
    },
    {
      "epoch": 34.36419125127162,
      "grad_norm": 48.21825408935547,
      "learning_rate": 1.5635808748728383e-05,
      "loss": 1.644,
      "step": 67560
    },
    {
      "epoch": 34.36927772126145,
      "grad_norm": 36.362674713134766,
      "learning_rate": 1.5630722278738556e-05,
      "loss": 1.6932,
      "step": 67570
    },
    {
      "epoch": 34.374364191251274,
      "grad_norm": 39.99890899658203,
      "learning_rate": 1.562563580874873e-05,
      "loss": 1.6432,
      "step": 67580
    },
    {
      "epoch": 34.3794506612411,
      "grad_norm": 49.57096481323242,
      "learning_rate": 1.5620549338758902e-05,
      "loss": 1.7088,
      "step": 67590
    },
    {
      "epoch": 34.38453713123093,
      "grad_norm": 46.07573699951172,
      "learning_rate": 1.5615462868769076e-05,
      "loss": 1.7737,
      "step": 67600
    },
    {
      "epoch": 34.389623601220755,
      "grad_norm": 36.960487365722656,
      "learning_rate": 1.561037639877925e-05,
      "loss": 1.684,
      "step": 67610
    },
    {
      "epoch": 34.39471007121058,
      "grad_norm": 33.533599853515625,
      "learning_rate": 1.5605289928789422e-05,
      "loss": 1.598,
      "step": 67620
    },
    {
      "epoch": 34.39979654120041,
      "grad_norm": 38.07860565185547,
      "learning_rate": 1.5600203458799595e-05,
      "loss": 1.7497,
      "step": 67630
    },
    {
      "epoch": 34.404883011190236,
      "grad_norm": 32.73605728149414,
      "learning_rate": 1.5595116988809765e-05,
      "loss": 1.7105,
      "step": 67640
    },
    {
      "epoch": 34.40996948118006,
      "grad_norm": 32.77303695678711,
      "learning_rate": 1.559003051881994e-05,
      "loss": 1.6383,
      "step": 67650
    },
    {
      "epoch": 34.41505595116989,
      "grad_norm": 32.24563217163086,
      "learning_rate": 1.5584944048830112e-05,
      "loss": 1.7283,
      "step": 67660
    },
    {
      "epoch": 34.42014242115972,
      "grad_norm": 35.35908126831055,
      "learning_rate": 1.5579857578840285e-05,
      "loss": 1.713,
      "step": 67670
    },
    {
      "epoch": 34.425228891149544,
      "grad_norm": 34.56514358520508,
      "learning_rate": 1.557477110885046e-05,
      "loss": 1.6302,
      "step": 67680
    },
    {
      "epoch": 34.43031536113937,
      "grad_norm": 40.39945983886719,
      "learning_rate": 1.5569684638860632e-05,
      "loss": 1.7756,
      "step": 67690
    },
    {
      "epoch": 34.4354018311292,
      "grad_norm": 39.37482833862305,
      "learning_rate": 1.55645981688708e-05,
      "loss": 1.794,
      "step": 67700
    },
    {
      "epoch": 34.440488301119025,
      "grad_norm": 47.246952056884766,
      "learning_rate": 1.555951169888098e-05,
      "loss": 1.6211,
      "step": 67710
    },
    {
      "epoch": 34.44557477110885,
      "grad_norm": 37.00178527832031,
      "learning_rate": 1.555442522889115e-05,
      "loss": 1.7513,
      "step": 67720
    },
    {
      "epoch": 34.45066124109868,
      "grad_norm": 39.11458969116211,
      "learning_rate": 1.5549338758901325e-05,
      "loss": 1.6346,
      "step": 67730
    },
    {
      "epoch": 34.455747711088506,
      "grad_norm": 31.433130264282227,
      "learning_rate": 1.5544252288911495e-05,
      "loss": 1.6607,
      "step": 67740
    },
    {
      "epoch": 34.46083418107833,
      "grad_norm": 37.15227127075195,
      "learning_rate": 1.5539165818921668e-05,
      "loss": 1.7236,
      "step": 67750
    },
    {
      "epoch": 34.46592065106816,
      "grad_norm": 37.64226150512695,
      "learning_rate": 1.5534079348931845e-05,
      "loss": 1.6906,
      "step": 67760
    },
    {
      "epoch": 34.47100712105799,
      "grad_norm": 31.242935180664062,
      "learning_rate": 1.5528992878942015e-05,
      "loss": 1.7349,
      "step": 67770
    },
    {
      "epoch": 34.476093591047814,
      "grad_norm": 46.809139251708984,
      "learning_rate": 1.5523906408952188e-05,
      "loss": 1.6342,
      "step": 67780
    },
    {
      "epoch": 34.48118006103764,
      "grad_norm": 32.57498550415039,
      "learning_rate": 1.551881993896236e-05,
      "loss": 1.6857,
      "step": 67790
    },
    {
      "epoch": 34.48626653102747,
      "grad_norm": 55.34992218017578,
      "learning_rate": 1.551373346897253e-05,
      "loss": 1.6193,
      "step": 67800
    },
    {
      "epoch": 34.491353001017295,
      "grad_norm": 36.25556945800781,
      "learning_rate": 1.5508646998982708e-05,
      "loss": 1.6805,
      "step": 67810
    },
    {
      "epoch": 34.49643947100712,
      "grad_norm": 42.844547271728516,
      "learning_rate": 1.550356052899288e-05,
      "loss": 1.6856,
      "step": 67820
    },
    {
      "epoch": 34.50152594099695,
      "grad_norm": 49.211814880371094,
      "learning_rate": 1.549847405900305e-05,
      "loss": 1.7806,
      "step": 67830
    },
    {
      "epoch": 34.506612410986776,
      "grad_norm": 43.869842529296875,
      "learning_rate": 1.5493387589013224e-05,
      "loss": 1.6915,
      "step": 67840
    },
    {
      "epoch": 34.5116988809766,
      "grad_norm": 31.132627487182617,
      "learning_rate": 1.5488301119023398e-05,
      "loss": 1.5761,
      "step": 67850
    },
    {
      "epoch": 34.51678535096643,
      "grad_norm": 38.33700180053711,
      "learning_rate": 1.5483214649033574e-05,
      "loss": 1.7677,
      "step": 67860
    },
    {
      "epoch": 34.52187182095626,
      "grad_norm": 37.02166748046875,
      "learning_rate": 1.5478128179043744e-05,
      "loss": 1.7398,
      "step": 67870
    },
    {
      "epoch": 34.526958290946084,
      "grad_norm": 40.15163803100586,
      "learning_rate": 1.5473041709053917e-05,
      "loss": 1.671,
      "step": 67880
    },
    {
      "epoch": 34.53204476093591,
      "grad_norm": 39.01082992553711,
      "learning_rate": 1.546795523906409e-05,
      "loss": 1.7183,
      "step": 67890
    },
    {
      "epoch": 34.53713123092574,
      "grad_norm": 33.154048919677734,
      "learning_rate": 1.5462868769074264e-05,
      "loss": 1.6978,
      "step": 67900
    },
    {
      "epoch": 34.542217700915565,
      "grad_norm": 45.6733283996582,
      "learning_rate": 1.5457782299084437e-05,
      "loss": 1.7126,
      "step": 67910
    },
    {
      "epoch": 34.54730417090539,
      "grad_norm": 40.67670822143555,
      "learning_rate": 1.545269582909461e-05,
      "loss": 1.6745,
      "step": 67920
    },
    {
      "epoch": 34.55239064089522,
      "grad_norm": 43.24739456176758,
      "learning_rate": 1.544760935910478e-05,
      "loss": 1.6188,
      "step": 67930
    },
    {
      "epoch": 34.557477110885046,
      "grad_norm": 48.25856018066406,
      "learning_rate": 1.5442522889114954e-05,
      "loss": 1.6998,
      "step": 67940
    },
    {
      "epoch": 34.56256358087487,
      "grad_norm": 35.585235595703125,
      "learning_rate": 1.5437436419125127e-05,
      "loss": 1.7506,
      "step": 67950
    },
    {
      "epoch": 34.5676500508647,
      "grad_norm": 54.403099060058594,
      "learning_rate": 1.54323499491353e-05,
      "loss": 1.6208,
      "step": 67960
    },
    {
      "epoch": 34.57273652085453,
      "grad_norm": 44.11832809448242,
      "learning_rate": 1.5427263479145474e-05,
      "loss": 1.6439,
      "step": 67970
    },
    {
      "epoch": 34.57782299084435,
      "grad_norm": 37.86704635620117,
      "learning_rate": 1.5422177009155647e-05,
      "loss": 1.7466,
      "step": 67980
    },
    {
      "epoch": 34.58290946083418,
      "grad_norm": 39.263057708740234,
      "learning_rate": 1.541709053916582e-05,
      "loss": 1.7229,
      "step": 67990
    },
    {
      "epoch": 34.58799593082401,
      "grad_norm": 39.2224006652832,
      "learning_rate": 1.5412004069175993e-05,
      "loss": 1.624,
      "step": 68000
    },
    {
      "epoch": 34.593082400813834,
      "grad_norm": 45.7738037109375,
      "learning_rate": 1.5406917599186167e-05,
      "loss": 1.6665,
      "step": 68010
    },
    {
      "epoch": 34.59816887080366,
      "grad_norm": 41.900115966796875,
      "learning_rate": 1.540183112919634e-05,
      "loss": 1.6297,
      "step": 68020
    },
    {
      "epoch": 34.60325534079349,
      "grad_norm": 40.716583251953125,
      "learning_rate": 1.539674465920651e-05,
      "loss": 1.6444,
      "step": 68030
    },
    {
      "epoch": 34.608341810783315,
      "grad_norm": 33.44578170776367,
      "learning_rate": 1.5391658189216683e-05,
      "loss": 1.7446,
      "step": 68040
    },
    {
      "epoch": 34.61342828077314,
      "grad_norm": 44.665870666503906,
      "learning_rate": 1.538657171922686e-05,
      "loss": 1.6449,
      "step": 68050
    },
    {
      "epoch": 34.61851475076297,
      "grad_norm": 38.80208206176758,
      "learning_rate": 1.538148524923703e-05,
      "loss": 1.6272,
      "step": 68060
    },
    {
      "epoch": 34.623601220752796,
      "grad_norm": 38.014286041259766,
      "learning_rate": 1.5376398779247203e-05,
      "loss": 1.7251,
      "step": 68070
    },
    {
      "epoch": 34.62868769074262,
      "grad_norm": 38.74619674682617,
      "learning_rate": 1.5371312309257376e-05,
      "loss": 1.7113,
      "step": 68080
    },
    {
      "epoch": 34.63377416073245,
      "grad_norm": 32.584632873535156,
      "learning_rate": 1.536622583926755e-05,
      "loss": 1.6474,
      "step": 68090
    },
    {
      "epoch": 34.63886063072228,
      "grad_norm": 38.077537536621094,
      "learning_rate": 1.5361139369277723e-05,
      "loss": 1.7484,
      "step": 68100
    },
    {
      "epoch": 34.643947100712104,
      "grad_norm": 33.97880172729492,
      "learning_rate": 1.5356052899287896e-05,
      "loss": 1.6537,
      "step": 68110
    },
    {
      "epoch": 34.64903357070193,
      "grad_norm": 37.022193908691406,
      "learning_rate": 1.5350966429298066e-05,
      "loss": 1.6876,
      "step": 68120
    },
    {
      "epoch": 34.65412004069176,
      "grad_norm": 48.74210739135742,
      "learning_rate": 1.534587995930824e-05,
      "loss": 1.7512,
      "step": 68130
    },
    {
      "epoch": 34.659206510681585,
      "grad_norm": 48.374969482421875,
      "learning_rate": 1.5340793489318413e-05,
      "loss": 1.7106,
      "step": 68140
    },
    {
      "epoch": 34.66429298067141,
      "grad_norm": 40.22390365600586,
      "learning_rate": 1.533570701932859e-05,
      "loss": 1.6508,
      "step": 68150
    },
    {
      "epoch": 34.66937945066124,
      "grad_norm": 44.73231887817383,
      "learning_rate": 1.533062054933876e-05,
      "loss": 1.6682,
      "step": 68160
    },
    {
      "epoch": 34.674465920651066,
      "grad_norm": 39.167171478271484,
      "learning_rate": 1.5325534079348932e-05,
      "loss": 1.6306,
      "step": 68170
    },
    {
      "epoch": 34.67955239064089,
      "grad_norm": 32.14079284667969,
      "learning_rate": 1.5320447609359106e-05,
      "loss": 1.6995,
      "step": 68180
    },
    {
      "epoch": 34.68463886063072,
      "grad_norm": 50.568756103515625,
      "learning_rate": 1.531536113936928e-05,
      "loss": 1.7666,
      "step": 68190
    },
    {
      "epoch": 34.68972533062055,
      "grad_norm": 36.32579040527344,
      "learning_rate": 1.5310274669379452e-05,
      "loss": 1.6389,
      "step": 68200
    },
    {
      "epoch": 34.694811800610374,
      "grad_norm": 38.1832160949707,
      "learning_rate": 1.5305188199389626e-05,
      "loss": 1.7321,
      "step": 68210
    },
    {
      "epoch": 34.6998982706002,
      "grad_norm": 46.53556823730469,
      "learning_rate": 1.5300101729399795e-05,
      "loss": 1.662,
      "step": 68220
    },
    {
      "epoch": 34.70498474059003,
      "grad_norm": 32.16352844238281,
      "learning_rate": 1.529501525940997e-05,
      "loss": 1.6775,
      "step": 68230
    },
    {
      "epoch": 34.710071210579855,
      "grad_norm": 60.44184494018555,
      "learning_rate": 1.5289928789420145e-05,
      "loss": 1.652,
      "step": 68240
    },
    {
      "epoch": 34.71515768056968,
      "grad_norm": 37.85755157470703,
      "learning_rate": 1.5284842319430315e-05,
      "loss": 1.6882,
      "step": 68250
    },
    {
      "epoch": 34.72024415055951,
      "grad_norm": 34.864959716796875,
      "learning_rate": 1.527975584944049e-05,
      "loss": 1.6813,
      "step": 68260
    },
    {
      "epoch": 34.725330620549336,
      "grad_norm": 39.65009689331055,
      "learning_rate": 1.5274669379450662e-05,
      "loss": 1.65,
      "step": 68270
    },
    {
      "epoch": 34.73041709053916,
      "grad_norm": 40.17401885986328,
      "learning_rate": 1.5269582909460835e-05,
      "loss": 1.6847,
      "step": 68280
    },
    {
      "epoch": 34.73550356052899,
      "grad_norm": 43.098751068115234,
      "learning_rate": 1.526449643947101e-05,
      "loss": 1.7655,
      "step": 68290
    },
    {
      "epoch": 34.74059003051882,
      "grad_norm": 40.2689094543457,
      "learning_rate": 1.525940996948118e-05,
      "loss": 1.6745,
      "step": 68300
    },
    {
      "epoch": 34.745676500508644,
      "grad_norm": 60.198604583740234,
      "learning_rate": 1.5254323499491355e-05,
      "loss": 1.6523,
      "step": 68310
    },
    {
      "epoch": 34.75076297049847,
      "grad_norm": 32.371944427490234,
      "learning_rate": 1.5249237029501527e-05,
      "loss": 1.6151,
      "step": 68320
    },
    {
      "epoch": 34.7558494404883,
      "grad_norm": 42.914371490478516,
      "learning_rate": 1.52441505595117e-05,
      "loss": 1.6595,
      "step": 68330
    },
    {
      "epoch": 34.760935910478125,
      "grad_norm": 31.72500228881836,
      "learning_rate": 1.5239064089521873e-05,
      "loss": 1.6435,
      "step": 68340
    },
    {
      "epoch": 34.76602238046795,
      "grad_norm": 41.38465118408203,
      "learning_rate": 1.5233977619532045e-05,
      "loss": 1.7314,
      "step": 68350
    },
    {
      "epoch": 34.77110885045778,
      "grad_norm": 33.0265007019043,
      "learning_rate": 1.5228891149542218e-05,
      "loss": 1.6886,
      "step": 68360
    },
    {
      "epoch": 34.77619532044761,
      "grad_norm": 38.9765625,
      "learning_rate": 1.5223804679552391e-05,
      "loss": 1.6157,
      "step": 68370
    },
    {
      "epoch": 34.78128179043744,
      "grad_norm": 38.810882568359375,
      "learning_rate": 1.5218718209562563e-05,
      "loss": 1.7136,
      "step": 68380
    },
    {
      "epoch": 34.78636826042727,
      "grad_norm": 38.45122528076172,
      "learning_rate": 1.5213631739572736e-05,
      "loss": 1.6709,
      "step": 68390
    },
    {
      "epoch": 34.791454730417094,
      "grad_norm": 49.46274185180664,
      "learning_rate": 1.5208545269582911e-05,
      "loss": 1.6748,
      "step": 68400
    },
    {
      "epoch": 34.79654120040692,
      "grad_norm": 50.143775939941406,
      "learning_rate": 1.5203458799593084e-05,
      "loss": 1.6113,
      "step": 68410
    },
    {
      "epoch": 34.80162767039675,
      "grad_norm": 47.48931121826172,
      "learning_rate": 1.5198372329603256e-05,
      "loss": 1.6881,
      "step": 68420
    },
    {
      "epoch": 34.806714140386575,
      "grad_norm": 37.9770622253418,
      "learning_rate": 1.519328585961343e-05,
      "loss": 1.7676,
      "step": 68430
    },
    {
      "epoch": 34.8118006103764,
      "grad_norm": 39.863548278808594,
      "learning_rate": 1.5188199389623603e-05,
      "loss": 1.7077,
      "step": 68440
    },
    {
      "epoch": 34.81688708036623,
      "grad_norm": 32.938018798828125,
      "learning_rate": 1.5183112919633774e-05,
      "loss": 1.6127,
      "step": 68450
    },
    {
      "epoch": 34.821973550356056,
      "grad_norm": 36.540985107421875,
      "learning_rate": 1.5178026449643947e-05,
      "loss": 1.6753,
      "step": 68460
    },
    {
      "epoch": 34.82706002034588,
      "grad_norm": 38.02964782714844,
      "learning_rate": 1.5172939979654122e-05,
      "loss": 1.6751,
      "step": 68470
    },
    {
      "epoch": 34.83214649033571,
      "grad_norm": 33.607845306396484,
      "learning_rate": 1.5167853509664292e-05,
      "loss": 1.6547,
      "step": 68480
    },
    {
      "epoch": 34.83723296032554,
      "grad_norm": 48.93269729614258,
      "learning_rate": 1.5162767039674467e-05,
      "loss": 1.6612,
      "step": 68490
    },
    {
      "epoch": 34.842319430315364,
      "grad_norm": 51.661376953125,
      "learning_rate": 1.515768056968464e-05,
      "loss": 1.7238,
      "step": 68500
    },
    {
      "epoch": 34.84740590030519,
      "grad_norm": 33.699462890625,
      "learning_rate": 1.5152594099694812e-05,
      "loss": 1.7676,
      "step": 68510
    },
    {
      "epoch": 34.85249237029502,
      "grad_norm": 52.61576843261719,
      "learning_rate": 1.5147507629704985e-05,
      "loss": 1.6618,
      "step": 68520
    },
    {
      "epoch": 34.857578840284845,
      "grad_norm": 45.407527923583984,
      "learning_rate": 1.5142421159715159e-05,
      "loss": 1.7843,
      "step": 68530
    },
    {
      "epoch": 34.86266531027467,
      "grad_norm": 38.908939361572266,
      "learning_rate": 1.5137334689725332e-05,
      "loss": 1.682,
      "step": 68540
    },
    {
      "epoch": 34.8677517802645,
      "grad_norm": 40.112083435058594,
      "learning_rate": 1.5132248219735504e-05,
      "loss": 1.6539,
      "step": 68550
    },
    {
      "epoch": 34.872838250254325,
      "grad_norm": 35.88336181640625,
      "learning_rate": 1.5127161749745677e-05,
      "loss": 1.6837,
      "step": 68560
    },
    {
      "epoch": 34.87792472024415,
      "grad_norm": 44.28352355957031,
      "learning_rate": 1.5122075279755852e-05,
      "loss": 1.6218,
      "step": 68570
    },
    {
      "epoch": 34.88301119023398,
      "grad_norm": 38.297821044921875,
      "learning_rate": 1.5116988809766022e-05,
      "loss": 1.7276,
      "step": 68580
    },
    {
      "epoch": 34.888097660223806,
      "grad_norm": 38.27088928222656,
      "learning_rate": 1.5111902339776197e-05,
      "loss": 1.6543,
      "step": 68590
    },
    {
      "epoch": 34.89318413021363,
      "grad_norm": 43.561161041259766,
      "learning_rate": 1.510681586978637e-05,
      "loss": 1.6213,
      "step": 68600
    },
    {
      "epoch": 34.89827060020346,
      "grad_norm": 38.68183898925781,
      "learning_rate": 1.5101729399796542e-05,
      "loss": 1.8137,
      "step": 68610
    },
    {
      "epoch": 34.90335707019329,
      "grad_norm": 42.75932693481445,
      "learning_rate": 1.5096642929806715e-05,
      "loss": 1.6383,
      "step": 68620
    },
    {
      "epoch": 34.908443540183114,
      "grad_norm": 42.70328903198242,
      "learning_rate": 1.5091556459816888e-05,
      "loss": 1.5933,
      "step": 68630
    },
    {
      "epoch": 34.91353001017294,
      "grad_norm": 34.33211135864258,
      "learning_rate": 1.508646998982706e-05,
      "loss": 1.6215,
      "step": 68640
    },
    {
      "epoch": 34.91861648016277,
      "grad_norm": 42.61029815673828,
      "learning_rate": 1.5081383519837233e-05,
      "loss": 1.6421,
      "step": 68650
    },
    {
      "epoch": 34.923702950152595,
      "grad_norm": 33.02740478515625,
      "learning_rate": 1.5076297049847408e-05,
      "loss": 1.6214,
      "step": 68660
    },
    {
      "epoch": 34.92878942014242,
      "grad_norm": 45.07072067260742,
      "learning_rate": 1.5071210579857581e-05,
      "loss": 1.6503,
      "step": 68670
    },
    {
      "epoch": 34.93387589013225,
      "grad_norm": 36.649566650390625,
      "learning_rate": 1.5066124109867751e-05,
      "loss": 1.742,
      "step": 68680
    },
    {
      "epoch": 34.938962360122076,
      "grad_norm": 31.865493774414062,
      "learning_rate": 1.5061037639877926e-05,
      "loss": 1.6592,
      "step": 68690
    },
    {
      "epoch": 34.9440488301119,
      "grad_norm": 47.27597427368164,
      "learning_rate": 1.50559511698881e-05,
      "loss": 1.6674,
      "step": 68700
    },
    {
      "epoch": 34.94913530010173,
      "grad_norm": 44.09297561645508,
      "learning_rate": 1.5050864699898271e-05,
      "loss": 1.7432,
      "step": 68710
    },
    {
      "epoch": 34.95422177009156,
      "grad_norm": 38.637088775634766,
      "learning_rate": 1.5045778229908444e-05,
      "loss": 1.6858,
      "step": 68720
    },
    {
      "epoch": 34.959308240081384,
      "grad_norm": 37.33892059326172,
      "learning_rate": 1.5040691759918618e-05,
      "loss": 1.7172,
      "step": 68730
    },
    {
      "epoch": 34.96439471007121,
      "grad_norm": 34.62971496582031,
      "learning_rate": 1.5035605289928789e-05,
      "loss": 1.7032,
      "step": 68740
    },
    {
      "epoch": 34.96948118006104,
      "grad_norm": 44.48835754394531,
      "learning_rate": 1.5030518819938962e-05,
      "loss": 1.6653,
      "step": 68750
    },
    {
      "epoch": 34.974567650050865,
      "grad_norm": 37.67148208618164,
      "learning_rate": 1.5025432349949137e-05,
      "loss": 1.6792,
      "step": 68760
    },
    {
      "epoch": 34.97965412004069,
      "grad_norm": 34.41786193847656,
      "learning_rate": 1.5020345879959307e-05,
      "loss": 1.5909,
      "step": 68770
    },
    {
      "epoch": 34.98474059003052,
      "grad_norm": 35.194061279296875,
      "learning_rate": 1.5015259409969482e-05,
      "loss": 1.6802,
      "step": 68780
    },
    {
      "epoch": 34.989827060020346,
      "grad_norm": 38.39983367919922,
      "learning_rate": 1.5010172939979656e-05,
      "loss": 1.6496,
      "step": 68790
    },
    {
      "epoch": 34.99491353001017,
      "grad_norm": 37.750831604003906,
      "learning_rate": 1.5005086469989829e-05,
      "loss": 1.6746,
      "step": 68800
    },
    {
      "epoch": 35.0,
      "grad_norm": 47.086265563964844,
      "learning_rate": 1.5e-05,
      "loss": 1.7411,
      "step": 68810
    },
    {
      "epoch": 35.0,
      "eval_loss": 4.9064412117004395,
      "eval_runtime": 3.1128,
      "eval_samples_per_second": 891.475,
      "eval_steps_per_second": 111.475,
      "step": 68810
    },
    {
      "epoch": 35.00508646998983,
      "grad_norm": 46.194549560546875,
      "learning_rate": 1.4994913530010174e-05,
      "loss": 1.6273,
      "step": 68820
    },
    {
      "epoch": 35.010172939979654,
      "grad_norm": 37.13140106201172,
      "learning_rate": 1.4989827060020347e-05,
      "loss": 1.6613,
      "step": 68830
    },
    {
      "epoch": 35.01525940996948,
      "grad_norm": 44.37198257446289,
      "learning_rate": 1.4984740590030519e-05,
      "loss": 1.6482,
      "step": 68840
    },
    {
      "epoch": 35.02034587995931,
      "grad_norm": 37.622005462646484,
      "learning_rate": 1.4979654120040692e-05,
      "loss": 1.5814,
      "step": 68850
    },
    {
      "epoch": 35.025432349949135,
      "grad_norm": 39.54697799682617,
      "learning_rate": 1.4974567650050867e-05,
      "loss": 1.6921,
      "step": 68860
    },
    {
      "epoch": 35.03051881993896,
      "grad_norm": 37.56425476074219,
      "learning_rate": 1.4969481180061037e-05,
      "loss": 1.6722,
      "step": 68870
    },
    {
      "epoch": 35.03560528992879,
      "grad_norm": 44.1610107421875,
      "learning_rate": 1.4964394710071212e-05,
      "loss": 1.6732,
      "step": 68880
    },
    {
      "epoch": 35.040691759918616,
      "grad_norm": 32.553680419921875,
      "learning_rate": 1.4959308240081385e-05,
      "loss": 1.6658,
      "step": 68890
    },
    {
      "epoch": 35.04577822990844,
      "grad_norm": 30.294170379638672,
      "learning_rate": 1.4954221770091557e-05,
      "loss": 1.7965,
      "step": 68900
    },
    {
      "epoch": 35.05086469989827,
      "grad_norm": 37.5471076965332,
      "learning_rate": 1.494913530010173e-05,
      "loss": 1.7372,
      "step": 68910
    },
    {
      "epoch": 35.0559511698881,
      "grad_norm": 41.05820083618164,
      "learning_rate": 1.4944048830111903e-05,
      "loss": 1.7614,
      "step": 68920
    },
    {
      "epoch": 35.061037639877924,
      "grad_norm": 42.630393981933594,
      "learning_rate": 1.4938962360122075e-05,
      "loss": 1.6843,
      "step": 68930
    },
    {
      "epoch": 35.06612410986775,
      "grad_norm": 39.554874420166016,
      "learning_rate": 1.4933875890132248e-05,
      "loss": 1.6883,
      "step": 68940
    },
    {
      "epoch": 35.07121057985758,
      "grad_norm": 36.58577346801758,
      "learning_rate": 1.4928789420142423e-05,
      "loss": 1.6344,
      "step": 68950
    },
    {
      "epoch": 35.076297049847405,
      "grad_norm": 37.1084098815918,
      "learning_rate": 1.4923702950152596e-05,
      "loss": 1.695,
      "step": 68960
    },
    {
      "epoch": 35.08138351983723,
      "grad_norm": 48.498992919921875,
      "learning_rate": 1.4918616480162768e-05,
      "loss": 1.601,
      "step": 68970
    },
    {
      "epoch": 35.08646998982706,
      "grad_norm": 35.48534393310547,
      "learning_rate": 1.4913530010172941e-05,
      "loss": 1.7234,
      "step": 68980
    },
    {
      "epoch": 35.091556459816886,
      "grad_norm": 38.10055923461914,
      "learning_rate": 1.4908443540183114e-05,
      "loss": 1.7117,
      "step": 68990
    },
    {
      "epoch": 35.09664292980671,
      "grad_norm": 33.41992950439453,
      "learning_rate": 1.4903357070193286e-05,
      "loss": 1.632,
      "step": 69000
    },
    {
      "epoch": 35.10172939979654,
      "grad_norm": 37.54631805419922,
      "learning_rate": 1.489827060020346e-05,
      "loss": 1.7009,
      "step": 69010
    },
    {
      "epoch": 35.10681586978637,
      "grad_norm": 41.53867721557617,
      "learning_rate": 1.4893184130213633e-05,
      "loss": 1.7057,
      "step": 69020
    },
    {
      "epoch": 35.111902339776194,
      "grad_norm": 32.42078399658203,
      "learning_rate": 1.4888097660223804e-05,
      "loss": 1.6861,
      "step": 69030
    },
    {
      "epoch": 35.11698880976602,
      "grad_norm": 32.696624755859375,
      "learning_rate": 1.4883011190233977e-05,
      "loss": 1.6832,
      "step": 69040
    },
    {
      "epoch": 35.12207527975585,
      "grad_norm": 46.315940856933594,
      "learning_rate": 1.4877924720244152e-05,
      "loss": 1.6239,
      "step": 69050
    },
    {
      "epoch": 35.127161749745675,
      "grad_norm": 39.011966705322266,
      "learning_rate": 1.4872838250254322e-05,
      "loss": 1.6573,
      "step": 69060
    },
    {
      "epoch": 35.1322482197355,
      "grad_norm": 41.19910430908203,
      "learning_rate": 1.4867751780264497e-05,
      "loss": 1.6623,
      "step": 69070
    },
    {
      "epoch": 35.13733468972533,
      "grad_norm": 45.28932189941406,
      "learning_rate": 1.486266531027467e-05,
      "loss": 1.574,
      "step": 69080
    },
    {
      "epoch": 35.142421159715155,
      "grad_norm": 38.34162139892578,
      "learning_rate": 1.4857578840284844e-05,
      "loss": 1.6557,
      "step": 69090
    },
    {
      "epoch": 35.14750762970498,
      "grad_norm": 38.75404357910156,
      "learning_rate": 1.4852492370295015e-05,
      "loss": 1.6507,
      "step": 69100
    },
    {
      "epoch": 35.15259409969481,
      "grad_norm": 41.88451385498047,
      "learning_rate": 1.4847405900305189e-05,
      "loss": 1.685,
      "step": 69110
    },
    {
      "epoch": 35.157680569684636,
      "grad_norm": 40.70561599731445,
      "learning_rate": 1.4842319430315364e-05,
      "loss": 1.6606,
      "step": 69120
    },
    {
      "epoch": 35.16276703967446,
      "grad_norm": 44.83376693725586,
      "learning_rate": 1.4837232960325534e-05,
      "loss": 1.7463,
      "step": 69130
    },
    {
      "epoch": 35.16785350966429,
      "grad_norm": 36.16136932373047,
      "learning_rate": 1.4832146490335709e-05,
      "loss": 1.6674,
      "step": 69140
    },
    {
      "epoch": 35.17293997965412,
      "grad_norm": 45.108482360839844,
      "learning_rate": 1.4827060020345882e-05,
      "loss": 1.7098,
      "step": 69150
    },
    {
      "epoch": 35.178026449643944,
      "grad_norm": 39.75054931640625,
      "learning_rate": 1.4821973550356052e-05,
      "loss": 1.6127,
      "step": 69160
    },
    {
      "epoch": 35.18311291963377,
      "grad_norm": 40.927024841308594,
      "learning_rate": 1.4816887080366227e-05,
      "loss": 1.6877,
      "step": 69170
    },
    {
      "epoch": 35.1881993896236,
      "grad_norm": 43.313880920410156,
      "learning_rate": 1.48118006103764e-05,
      "loss": 1.6954,
      "step": 69180
    },
    {
      "epoch": 35.193285859613425,
      "grad_norm": 36.791019439697266,
      "learning_rate": 1.4806714140386572e-05,
      "loss": 1.6558,
      "step": 69190
    },
    {
      "epoch": 35.19837232960325,
      "grad_norm": 39.27336502075195,
      "learning_rate": 1.4801627670396745e-05,
      "loss": 1.7076,
      "step": 69200
    },
    {
      "epoch": 35.20345879959308,
      "grad_norm": 41.99362564086914,
      "learning_rate": 1.4796541200406918e-05,
      "loss": 1.6243,
      "step": 69210
    },
    {
      "epoch": 35.208545269582906,
      "grad_norm": 39.58755874633789,
      "learning_rate": 1.4791454730417093e-05,
      "loss": 1.6864,
      "step": 69220
    },
    {
      "epoch": 35.21363173957273,
      "grad_norm": 41.0230712890625,
      "learning_rate": 1.4786368260427263e-05,
      "loss": 1.6756,
      "step": 69230
    },
    {
      "epoch": 35.21871820956256,
      "grad_norm": 46.556583404541016,
      "learning_rate": 1.4781281790437438e-05,
      "loss": 1.7323,
      "step": 69240
    },
    {
      "epoch": 35.22380467955239,
      "grad_norm": 48.46946716308594,
      "learning_rate": 1.4776195320447611e-05,
      "loss": 1.7825,
      "step": 69250
    },
    {
      "epoch": 35.22889114954222,
      "grad_norm": 32.39160919189453,
      "learning_rate": 1.4771108850457783e-05,
      "loss": 1.6874,
      "step": 69260
    },
    {
      "epoch": 35.23397761953205,
      "grad_norm": 40.96144104003906,
      "learning_rate": 1.4766022380467956e-05,
      "loss": 1.602,
      "step": 69270
    },
    {
      "epoch": 35.239064089521875,
      "grad_norm": 51.38051986694336,
      "learning_rate": 1.476093591047813e-05,
      "loss": 1.6194,
      "step": 69280
    },
    {
      "epoch": 35.2441505595117,
      "grad_norm": 47.52891159057617,
      "learning_rate": 1.4755849440488301e-05,
      "loss": 1.6989,
      "step": 69290
    },
    {
      "epoch": 35.24923702950153,
      "grad_norm": 50.17340850830078,
      "learning_rate": 1.4750762970498474e-05,
      "loss": 1.5868,
      "step": 69300
    },
    {
      "epoch": 35.254323499491356,
      "grad_norm": 45.576358795166016,
      "learning_rate": 1.4745676500508648e-05,
      "loss": 1.6282,
      "step": 69310
    },
    {
      "epoch": 35.25940996948118,
      "grad_norm": 38.08683395385742,
      "learning_rate": 1.4740590030518819e-05,
      "loss": 1.73,
      "step": 69320
    },
    {
      "epoch": 35.26449643947101,
      "grad_norm": 41.33279800415039,
      "learning_rate": 1.4735503560528992e-05,
      "loss": 1.6565,
      "step": 69330
    },
    {
      "epoch": 35.26958290946084,
      "grad_norm": 40.38740921020508,
      "learning_rate": 1.4730417090539167e-05,
      "loss": 1.6023,
      "step": 69340
    },
    {
      "epoch": 35.274669379450664,
      "grad_norm": 40.614261627197266,
      "learning_rate": 1.472533062054934e-05,
      "loss": 1.7172,
      "step": 69350
    },
    {
      "epoch": 35.27975584944049,
      "grad_norm": 41.2151985168457,
      "learning_rate": 1.4720244150559512e-05,
      "loss": 1.6423,
      "step": 69360
    },
    {
      "epoch": 35.28484231943032,
      "grad_norm": 37.3288459777832,
      "learning_rate": 1.4715157680569686e-05,
      "loss": 1.6067,
      "step": 69370
    },
    {
      "epoch": 35.289928789420145,
      "grad_norm": 44.09117126464844,
      "learning_rate": 1.4710071210579859e-05,
      "loss": 1.6739,
      "step": 69380
    },
    {
      "epoch": 35.29501525940997,
      "grad_norm": 46.20164108276367,
      "learning_rate": 1.470498474059003e-05,
      "loss": 1.6393,
      "step": 69390
    },
    {
      "epoch": 35.3001017293998,
      "grad_norm": 37.98918533325195,
      "learning_rate": 1.4699898270600204e-05,
      "loss": 1.7134,
      "step": 69400
    },
    {
      "epoch": 35.305188199389626,
      "grad_norm": 39.706626892089844,
      "learning_rate": 1.4694811800610379e-05,
      "loss": 1.6672,
      "step": 69410
    },
    {
      "epoch": 35.31027466937945,
      "grad_norm": 47.314002990722656,
      "learning_rate": 1.4689725330620549e-05,
      "loss": 1.5888,
      "step": 69420
    },
    {
      "epoch": 35.31536113936928,
      "grad_norm": 39.29085922241211,
      "learning_rate": 1.4684638860630724e-05,
      "loss": 1.6032,
      "step": 69430
    },
    {
      "epoch": 35.32044760935911,
      "grad_norm": 42.5084114074707,
      "learning_rate": 1.4679552390640897e-05,
      "loss": 1.658,
      "step": 69440
    },
    {
      "epoch": 35.325534079348934,
      "grad_norm": 35.87294006347656,
      "learning_rate": 1.4674465920651068e-05,
      "loss": 1.6784,
      "step": 69450
    },
    {
      "epoch": 35.33062054933876,
      "grad_norm": 38.560462951660156,
      "learning_rate": 1.4669379450661242e-05,
      "loss": 1.6735,
      "step": 69460
    },
    {
      "epoch": 35.33570701932859,
      "grad_norm": 40.853912353515625,
      "learning_rate": 1.4664292980671415e-05,
      "loss": 1.6802,
      "step": 69470
    },
    {
      "epoch": 35.340793489318415,
      "grad_norm": 48.30840301513672,
      "learning_rate": 1.4659206510681588e-05,
      "loss": 1.6645,
      "step": 69480
    },
    {
      "epoch": 35.34587995930824,
      "grad_norm": 43.77645492553711,
      "learning_rate": 1.465412004069176e-05,
      "loss": 1.6458,
      "step": 69490
    },
    {
      "epoch": 35.35096642929807,
      "grad_norm": 46.334510803222656,
      "learning_rate": 1.4649033570701933e-05,
      "loss": 1.614,
      "step": 69500
    },
    {
      "epoch": 35.356052899287896,
      "grad_norm": 42.74121856689453,
      "learning_rate": 1.4643947100712108e-05,
      "loss": 1.7514,
      "step": 69510
    },
    {
      "epoch": 35.36113936927772,
      "grad_norm": 32.69638442993164,
      "learning_rate": 1.4638860630722278e-05,
      "loss": 1.7173,
      "step": 69520
    },
    {
      "epoch": 35.36622583926755,
      "grad_norm": 37.85602569580078,
      "learning_rate": 1.4633774160732453e-05,
      "loss": 1.6536,
      "step": 69530
    },
    {
      "epoch": 35.37131230925738,
      "grad_norm": 47.349185943603516,
      "learning_rate": 1.4628687690742626e-05,
      "loss": 1.7471,
      "step": 69540
    },
    {
      "epoch": 35.376398779247204,
      "grad_norm": 45.73006820678711,
      "learning_rate": 1.4623601220752798e-05,
      "loss": 1.6528,
      "step": 69550
    },
    {
      "epoch": 35.38148524923703,
      "grad_norm": 39.51100158691406,
      "learning_rate": 1.4618514750762971e-05,
      "loss": 1.639,
      "step": 69560
    },
    {
      "epoch": 35.38657171922686,
      "grad_norm": 38.68461608886719,
      "learning_rate": 1.4613428280773144e-05,
      "loss": 1.6699,
      "step": 69570
    },
    {
      "epoch": 35.391658189216685,
      "grad_norm": 44.4022331237793,
      "learning_rate": 1.4608341810783316e-05,
      "loss": 1.6515,
      "step": 69580
    },
    {
      "epoch": 35.39674465920651,
      "grad_norm": 35.97605514526367,
      "learning_rate": 1.460325534079349e-05,
      "loss": 1.662,
      "step": 69590
    },
    {
      "epoch": 35.40183112919634,
      "grad_norm": 40.510746002197266,
      "learning_rate": 1.4598168870803664e-05,
      "loss": 1.6533,
      "step": 69600
    },
    {
      "epoch": 35.406917599186166,
      "grad_norm": 37.07151412963867,
      "learning_rate": 1.4593082400813838e-05,
      "loss": 1.6585,
      "step": 69610
    },
    {
      "epoch": 35.41200406917599,
      "grad_norm": 40.496700286865234,
      "learning_rate": 1.4587995930824009e-05,
      "loss": 1.6752,
      "step": 69620
    },
    {
      "epoch": 35.41709053916582,
      "grad_norm": 40.39501190185547,
      "learning_rate": 1.4582909460834182e-05,
      "loss": 1.62,
      "step": 69630
    },
    {
      "epoch": 35.42217700915565,
      "grad_norm": 30.075029373168945,
      "learning_rate": 1.4577822990844356e-05,
      "loss": 1.6706,
      "step": 69640
    },
    {
      "epoch": 35.42726347914547,
      "grad_norm": 39.43605041503906,
      "learning_rate": 1.4572736520854527e-05,
      "loss": 1.676,
      "step": 69650
    },
    {
      "epoch": 35.4323499491353,
      "grad_norm": 48.31171798706055,
      "learning_rate": 1.45676500508647e-05,
      "loss": 1.6209,
      "step": 69660
    },
    {
      "epoch": 35.43743641912513,
      "grad_norm": 42.49919891357422,
      "learning_rate": 1.4562563580874874e-05,
      "loss": 1.7562,
      "step": 69670
    },
    {
      "epoch": 35.442522889114954,
      "grad_norm": 44.02494430541992,
      "learning_rate": 1.4557477110885045e-05,
      "loss": 1.6348,
      "step": 69680
    },
    {
      "epoch": 35.44760935910478,
      "grad_norm": 37.313358306884766,
      "learning_rate": 1.4552390640895219e-05,
      "loss": 1.7199,
      "step": 69690
    },
    {
      "epoch": 35.45269582909461,
      "grad_norm": 43.07614517211914,
      "learning_rate": 1.4547304170905394e-05,
      "loss": 1.6561,
      "step": 69700
    },
    {
      "epoch": 35.457782299084435,
      "grad_norm": 37.26384353637695,
      "learning_rate": 1.4542217700915564e-05,
      "loss": 1.732,
      "step": 69710
    },
    {
      "epoch": 35.46286876907426,
      "grad_norm": 40.56892013549805,
      "learning_rate": 1.4537131230925739e-05,
      "loss": 1.6367,
      "step": 69720
    },
    {
      "epoch": 35.46795523906409,
      "grad_norm": 46.41482925415039,
      "learning_rate": 1.4532044760935912e-05,
      "loss": 1.6625,
      "step": 69730
    },
    {
      "epoch": 35.473041709053916,
      "grad_norm": 33.36591339111328,
      "learning_rate": 1.4526958290946083e-05,
      "loss": 1.6456,
      "step": 69740
    },
    {
      "epoch": 35.47812817904374,
      "grad_norm": 40.805137634277344,
      "learning_rate": 1.4521871820956257e-05,
      "loss": 1.6609,
      "step": 69750
    },
    {
      "epoch": 35.48321464903357,
      "grad_norm": 32.11684036254883,
      "learning_rate": 1.451678535096643e-05,
      "loss": 1.6037,
      "step": 69760
    },
    {
      "epoch": 35.4883011190234,
      "grad_norm": 37.204227447509766,
      "learning_rate": 1.4511698880976605e-05,
      "loss": 1.6155,
      "step": 69770
    },
    {
      "epoch": 35.493387589013224,
      "grad_norm": 58.463623046875,
      "learning_rate": 1.4506612410986775e-05,
      "loss": 1.6609,
      "step": 69780
    },
    {
      "epoch": 35.49847405900305,
      "grad_norm": 39.67823028564453,
      "learning_rate": 1.4501525940996948e-05,
      "loss": 1.6249,
      "step": 69790
    },
    {
      "epoch": 35.50356052899288,
      "grad_norm": 33.52749252319336,
      "learning_rate": 1.4496439471007123e-05,
      "loss": 1.6424,
      "step": 69800
    },
    {
      "epoch": 35.508646998982705,
      "grad_norm": 34.33553695678711,
      "learning_rate": 1.4491353001017293e-05,
      "loss": 1.6739,
      "step": 69810
    },
    {
      "epoch": 35.51373346897253,
      "grad_norm": 44.54253387451172,
      "learning_rate": 1.4486266531027468e-05,
      "loss": 1.6671,
      "step": 69820
    },
    {
      "epoch": 35.51881993896236,
      "grad_norm": 33.979312896728516,
      "learning_rate": 1.4481180061037641e-05,
      "loss": 1.7037,
      "step": 69830
    },
    {
      "epoch": 35.523906408952186,
      "grad_norm": 41.52059555053711,
      "learning_rate": 1.4476093591047813e-05,
      "loss": 1.6979,
      "step": 69840
    },
    {
      "epoch": 35.52899287894201,
      "grad_norm": 35.704959869384766,
      "learning_rate": 1.4471007121057986e-05,
      "loss": 1.6599,
      "step": 69850
    },
    {
      "epoch": 35.53407934893184,
      "grad_norm": 37.60084915161133,
      "learning_rate": 1.446592065106816e-05,
      "loss": 1.6221,
      "step": 69860
    },
    {
      "epoch": 35.53916581892167,
      "grad_norm": 34.775596618652344,
      "learning_rate": 1.4460834181078331e-05,
      "loss": 1.682,
      "step": 69870
    },
    {
      "epoch": 35.544252288911494,
      "grad_norm": 40.143924713134766,
      "learning_rate": 1.4455747711088504e-05,
      "loss": 1.7443,
      "step": 69880
    },
    {
      "epoch": 35.54933875890132,
      "grad_norm": 43.58871078491211,
      "learning_rate": 1.445066124109868e-05,
      "loss": 1.6157,
      "step": 69890
    },
    {
      "epoch": 35.55442522889115,
      "grad_norm": 49.65752410888672,
      "learning_rate": 1.4445574771108853e-05,
      "loss": 1.5643,
      "step": 69900
    },
    {
      "epoch": 35.559511698880975,
      "grad_norm": 40.0568733215332,
      "learning_rate": 1.4440488301119024e-05,
      "loss": 1.5866,
      "step": 69910
    },
    {
      "epoch": 35.5645981688708,
      "grad_norm": 35.28242874145508,
      "learning_rate": 1.4435401831129197e-05,
      "loss": 1.6739,
      "step": 69920
    },
    {
      "epoch": 35.56968463886063,
      "grad_norm": 53.4167366027832,
      "learning_rate": 1.443031536113937e-05,
      "loss": 1.6357,
      "step": 69930
    },
    {
      "epoch": 35.574771108850456,
      "grad_norm": 41.72134780883789,
      "learning_rate": 1.4425228891149542e-05,
      "loss": 1.6628,
      "step": 69940
    },
    {
      "epoch": 35.57985757884028,
      "grad_norm": 36.08495330810547,
      "learning_rate": 1.4420142421159716e-05,
      "loss": 1.6511,
      "step": 69950
    },
    {
      "epoch": 35.58494404883011,
      "grad_norm": 42.703765869140625,
      "learning_rate": 1.4415055951169889e-05,
      "loss": 1.6983,
      "step": 69960
    },
    {
      "epoch": 35.59003051881994,
      "grad_norm": 38.74987030029297,
      "learning_rate": 1.440996948118006e-05,
      "loss": 1.7439,
      "step": 69970
    },
    {
      "epoch": 35.595116988809764,
      "grad_norm": 54.06031799316406,
      "learning_rate": 1.4404883011190234e-05,
      "loss": 1.6117,
      "step": 69980
    },
    {
      "epoch": 35.60020345879959,
      "grad_norm": 42.11031723022461,
      "learning_rate": 1.4399796541200409e-05,
      "loss": 1.594,
      "step": 69990
    },
    {
      "epoch": 35.60528992878942,
      "grad_norm": 40.60256576538086,
      "learning_rate": 1.4394710071210579e-05,
      "loss": 1.6451,
      "step": 70000
    },
    {
      "epoch": 35.610376398779245,
      "grad_norm": 35.04039764404297,
      "learning_rate": 1.4389623601220754e-05,
      "loss": 1.6456,
      "step": 70010
    },
    {
      "epoch": 35.61546286876907,
      "grad_norm": 39.14747619628906,
      "learning_rate": 1.4384537131230927e-05,
      "loss": 1.5488,
      "step": 70020
    },
    {
      "epoch": 35.6205493387589,
      "grad_norm": 41.36737823486328,
      "learning_rate": 1.43794506612411e-05,
      "loss": 1.6203,
      "step": 70030
    },
    {
      "epoch": 35.625635808748726,
      "grad_norm": 33.79086685180664,
      "learning_rate": 1.4374364191251272e-05,
      "loss": 1.6327,
      "step": 70040
    },
    {
      "epoch": 35.63072227873855,
      "grad_norm": 43.223636627197266,
      "learning_rate": 1.4369277721261445e-05,
      "loss": 1.6234,
      "step": 70050
    },
    {
      "epoch": 35.63580874872838,
      "grad_norm": 38.89187240600586,
      "learning_rate": 1.436419125127162e-05,
      "loss": 1.5658,
      "step": 70060
    },
    {
      "epoch": 35.64089521871821,
      "grad_norm": 42.63321304321289,
      "learning_rate": 1.435910478128179e-05,
      "loss": 1.5995,
      "step": 70070
    },
    {
      "epoch": 35.645981688708034,
      "grad_norm": 36.068626403808594,
      "learning_rate": 1.4354018311291965e-05,
      "loss": 1.6897,
      "step": 70080
    },
    {
      "epoch": 35.65106815869786,
      "grad_norm": 50.420631408691406,
      "learning_rate": 1.4348931841302138e-05,
      "loss": 1.5656,
      "step": 70090
    },
    {
      "epoch": 35.65615462868769,
      "grad_norm": 39.209022521972656,
      "learning_rate": 1.434384537131231e-05,
      "loss": 1.613,
      "step": 70100
    },
    {
      "epoch": 35.661241098677515,
      "grad_norm": 50.76746368408203,
      "learning_rate": 1.4338758901322483e-05,
      "loss": 1.6133,
      "step": 70110
    },
    {
      "epoch": 35.66632756866734,
      "grad_norm": 51.172637939453125,
      "learning_rate": 1.4333672431332656e-05,
      "loss": 1.7521,
      "step": 70120
    },
    {
      "epoch": 35.67141403865717,
      "grad_norm": 41.64008712768555,
      "learning_rate": 1.4328585961342828e-05,
      "loss": 1.592,
      "step": 70130
    },
    {
      "epoch": 35.676500508646996,
      "grad_norm": 42.876976013183594,
      "learning_rate": 1.4323499491353001e-05,
      "loss": 1.6991,
      "step": 70140
    },
    {
      "epoch": 35.68158697863683,
      "grad_norm": 32.73419952392578,
      "learning_rate": 1.4318413021363174e-05,
      "loss": 1.6898,
      "step": 70150
    },
    {
      "epoch": 35.68667344862666,
      "grad_norm": 33.5577392578125,
      "learning_rate": 1.431332655137335e-05,
      "loss": 1.6038,
      "step": 70160
    },
    {
      "epoch": 35.691759918616484,
      "grad_norm": 46.51710891723633,
      "learning_rate": 1.430824008138352e-05,
      "loss": 1.631,
      "step": 70170
    },
    {
      "epoch": 35.69684638860631,
      "grad_norm": 33.33285140991211,
      "learning_rate": 1.4303153611393694e-05,
      "loss": 1.613,
      "step": 70180
    },
    {
      "epoch": 35.70193285859614,
      "grad_norm": 38.21199035644531,
      "learning_rate": 1.4298067141403868e-05,
      "loss": 1.6498,
      "step": 70190
    },
    {
      "epoch": 35.707019328585965,
      "grad_norm": 47.837703704833984,
      "learning_rate": 1.4292980671414039e-05,
      "loss": 1.6428,
      "step": 70200
    },
    {
      "epoch": 35.71210579857579,
      "grad_norm": 40.014892578125,
      "learning_rate": 1.4287894201424212e-05,
      "loss": 1.6391,
      "step": 70210
    },
    {
      "epoch": 35.71719226856562,
      "grad_norm": 35.62029266357422,
      "learning_rate": 1.4282807731434386e-05,
      "loss": 1.5864,
      "step": 70220
    },
    {
      "epoch": 35.722278738555445,
      "grad_norm": 34.09276580810547,
      "learning_rate": 1.4277721261444557e-05,
      "loss": 1.6211,
      "step": 70230
    },
    {
      "epoch": 35.72736520854527,
      "grad_norm": 43.863040924072266,
      "learning_rate": 1.427263479145473e-05,
      "loss": 1.5922,
      "step": 70240
    },
    {
      "epoch": 35.7324516785351,
      "grad_norm": 42.44976043701172,
      "learning_rate": 1.4267548321464906e-05,
      "loss": 1.6257,
      "step": 70250
    },
    {
      "epoch": 35.737538148524926,
      "grad_norm": 49.34571075439453,
      "learning_rate": 1.4262461851475075e-05,
      "loss": 1.7947,
      "step": 70260
    },
    {
      "epoch": 35.74262461851475,
      "grad_norm": 39.77069091796875,
      "learning_rate": 1.425737538148525e-05,
      "loss": 1.7114,
      "step": 70270
    },
    {
      "epoch": 35.74771108850458,
      "grad_norm": 31.600255966186523,
      "learning_rate": 1.4252288911495424e-05,
      "loss": 1.7268,
      "step": 70280
    },
    {
      "epoch": 35.75279755849441,
      "grad_norm": 47.23977279663086,
      "learning_rate": 1.4247202441505597e-05,
      "loss": 1.7773,
      "step": 70290
    },
    {
      "epoch": 35.757884028484234,
      "grad_norm": 44.30732727050781,
      "learning_rate": 1.4242115971515769e-05,
      "loss": 1.688,
      "step": 70300
    },
    {
      "epoch": 35.76297049847406,
      "grad_norm": 35.52900695800781,
      "learning_rate": 1.4237029501525942e-05,
      "loss": 1.6946,
      "step": 70310
    },
    {
      "epoch": 35.76805696846389,
      "grad_norm": 53.1867561340332,
      "learning_rate": 1.4231943031536115e-05,
      "loss": 1.6153,
      "step": 70320
    },
    {
      "epoch": 35.773143438453715,
      "grad_norm": 42.10702896118164,
      "learning_rate": 1.4226856561546287e-05,
      "loss": 1.7059,
      "step": 70330
    },
    {
      "epoch": 35.77822990844354,
      "grad_norm": 40.81805419921875,
      "learning_rate": 1.422177009155646e-05,
      "loss": 1.6886,
      "step": 70340
    },
    {
      "epoch": 35.78331637843337,
      "grad_norm": 44.21364212036133,
      "learning_rate": 1.4216683621566635e-05,
      "loss": 1.5496,
      "step": 70350
    },
    {
      "epoch": 35.788402848423196,
      "grad_norm": 36.013431549072266,
      "learning_rate": 1.4211597151576805e-05,
      "loss": 1.6534,
      "step": 70360
    },
    {
      "epoch": 35.79348931841302,
      "grad_norm": 36.73538589477539,
      "learning_rate": 1.420651068158698e-05,
      "loss": 1.6413,
      "step": 70370
    },
    {
      "epoch": 35.79857578840285,
      "grad_norm": 48.9643669128418,
      "learning_rate": 1.4201424211597153e-05,
      "loss": 1.6665,
      "step": 70380
    },
    {
      "epoch": 35.80366225839268,
      "grad_norm": 40.887054443359375,
      "learning_rate": 1.4196337741607325e-05,
      "loss": 1.6753,
      "step": 70390
    },
    {
      "epoch": 35.808748728382504,
      "grad_norm": 41.908424377441406,
      "learning_rate": 1.4191251271617498e-05,
      "loss": 1.6519,
      "step": 70400
    },
    {
      "epoch": 35.81383519837233,
      "grad_norm": 38.486942291259766,
      "learning_rate": 1.4186164801627671e-05,
      "loss": 1.6555,
      "step": 70410
    },
    {
      "epoch": 35.81892166836216,
      "grad_norm": 46.02730941772461,
      "learning_rate": 1.4181078331637846e-05,
      "loss": 1.7357,
      "step": 70420
    },
    {
      "epoch": 35.824008138351985,
      "grad_norm": 40.91897964477539,
      "learning_rate": 1.4175991861648016e-05,
      "loss": 1.6514,
      "step": 70430
    },
    {
      "epoch": 35.82909460834181,
      "grad_norm": 41.08108901977539,
      "learning_rate": 1.417090539165819e-05,
      "loss": 1.6897,
      "step": 70440
    },
    {
      "epoch": 35.83418107833164,
      "grad_norm": 39.95438766479492,
      "learning_rate": 1.4165818921668364e-05,
      "loss": 1.6536,
      "step": 70450
    },
    {
      "epoch": 35.839267548321466,
      "grad_norm": 37.48125457763672,
      "learning_rate": 1.4160732451678534e-05,
      "loss": 1.6896,
      "step": 70460
    },
    {
      "epoch": 35.84435401831129,
      "grad_norm": 43.75356674194336,
      "learning_rate": 1.415564598168871e-05,
      "loss": 1.693,
      "step": 70470
    },
    {
      "epoch": 35.84944048830112,
      "grad_norm": 32.05780792236328,
      "learning_rate": 1.4150559511698883e-05,
      "loss": 1.6404,
      "step": 70480
    },
    {
      "epoch": 35.85452695829095,
      "grad_norm": 39.78240203857422,
      "learning_rate": 1.4145473041709054e-05,
      "loss": 1.7068,
      "step": 70490
    },
    {
      "epoch": 35.859613428280774,
      "grad_norm": 38.028079986572266,
      "learning_rate": 1.4140386571719227e-05,
      "loss": 1.6244,
      "step": 70500
    },
    {
      "epoch": 35.8646998982706,
      "grad_norm": 44.581058502197266,
      "learning_rate": 1.41353001017294e-05,
      "loss": 1.6292,
      "step": 70510
    },
    {
      "epoch": 35.86978636826043,
      "grad_norm": 41.24004364013672,
      "learning_rate": 1.4130213631739572e-05,
      "loss": 1.7007,
      "step": 70520
    },
    {
      "epoch": 35.874872838250255,
      "grad_norm": 43.18452835083008,
      "learning_rate": 1.4125127161749746e-05,
      "loss": 1.7176,
      "step": 70530
    },
    {
      "epoch": 35.87995930824008,
      "grad_norm": 38.09096145629883,
      "learning_rate": 1.412004069175992e-05,
      "loss": 1.6584,
      "step": 70540
    },
    {
      "epoch": 35.88504577822991,
      "grad_norm": 44.91335678100586,
      "learning_rate": 1.411495422177009e-05,
      "loss": 1.6777,
      "step": 70550
    },
    {
      "epoch": 35.890132248219736,
      "grad_norm": 38.50615310668945,
      "learning_rate": 1.4109867751780265e-05,
      "loss": 1.6724,
      "step": 70560
    },
    {
      "epoch": 35.89521871820956,
      "grad_norm": 36.8127326965332,
      "learning_rate": 1.4104781281790439e-05,
      "loss": 1.5865,
      "step": 70570
    },
    {
      "epoch": 35.90030518819939,
      "grad_norm": 35.55629348754883,
      "learning_rate": 1.4099694811800612e-05,
      "loss": 1.6414,
      "step": 70580
    },
    {
      "epoch": 35.90539165818922,
      "grad_norm": 36.90606689453125,
      "learning_rate": 1.4094608341810784e-05,
      "loss": 1.6765,
      "step": 70590
    },
    {
      "epoch": 35.910478128179044,
      "grad_norm": 39.65153121948242,
      "learning_rate": 1.4089521871820957e-05,
      "loss": 1.7091,
      "step": 70600
    },
    {
      "epoch": 35.91556459816887,
      "grad_norm": 43.8476448059082,
      "learning_rate": 1.408443540183113e-05,
      "loss": 1.6736,
      "step": 70610
    },
    {
      "epoch": 35.9206510681587,
      "grad_norm": 60.9493293762207,
      "learning_rate": 1.4079348931841302e-05,
      "loss": 1.7077,
      "step": 70620
    },
    {
      "epoch": 35.925737538148525,
      "grad_norm": 48.56778335571289,
      "learning_rate": 1.4074262461851475e-05,
      "loss": 1.6772,
      "step": 70630
    },
    {
      "epoch": 35.93082400813835,
      "grad_norm": 48.39201354980469,
      "learning_rate": 1.406917599186165e-05,
      "loss": 1.6362,
      "step": 70640
    },
    {
      "epoch": 35.93591047812818,
      "grad_norm": 37.980674743652344,
      "learning_rate": 1.406408952187182e-05,
      "loss": 1.7102,
      "step": 70650
    },
    {
      "epoch": 35.940996948118006,
      "grad_norm": 36.484039306640625,
      "learning_rate": 1.4059003051881995e-05,
      "loss": 1.6652,
      "step": 70660
    },
    {
      "epoch": 35.94608341810783,
      "grad_norm": 54.97488784790039,
      "learning_rate": 1.4053916581892168e-05,
      "loss": 1.6488,
      "step": 70670
    },
    {
      "epoch": 35.95116988809766,
      "grad_norm": 45.28187942504883,
      "learning_rate": 1.404883011190234e-05,
      "loss": 1.5513,
      "step": 70680
    },
    {
      "epoch": 35.95625635808749,
      "grad_norm": 49.57429504394531,
      "learning_rate": 1.4043743641912513e-05,
      "loss": 1.6529,
      "step": 70690
    },
    {
      "epoch": 35.96134282807731,
      "grad_norm": 50.941951751708984,
      "learning_rate": 1.4038657171922686e-05,
      "loss": 1.6803,
      "step": 70700
    },
    {
      "epoch": 35.96642929806714,
      "grad_norm": 32.661319732666016,
      "learning_rate": 1.4033570701932861e-05,
      "loss": 1.7325,
      "step": 70710
    },
    {
      "epoch": 35.97151576805697,
      "grad_norm": 38.67048263549805,
      "learning_rate": 1.4028484231943031e-05,
      "loss": 1.6348,
      "step": 70720
    },
    {
      "epoch": 35.976602238046794,
      "grad_norm": 45.19954299926758,
      "learning_rate": 1.4023397761953206e-05,
      "loss": 1.6459,
      "step": 70730
    },
    {
      "epoch": 35.98168870803662,
      "grad_norm": 35.17327117919922,
      "learning_rate": 1.401831129196338e-05,
      "loss": 1.6993,
      "step": 70740
    },
    {
      "epoch": 35.98677517802645,
      "grad_norm": 33.3833122253418,
      "learning_rate": 1.4013224821973551e-05,
      "loss": 1.6766,
      "step": 70750
    },
    {
      "epoch": 35.991861648016275,
      "grad_norm": 41.27052688598633,
      "learning_rate": 1.4008138351983724e-05,
      "loss": 1.6578,
      "step": 70760
    },
    {
      "epoch": 35.9969481180061,
      "grad_norm": 41.495445251464844,
      "learning_rate": 1.4003051881993898e-05,
      "loss": 1.7347,
      "step": 70770
    },
    {
      "epoch": 36.0,
      "eval_loss": 4.92218542098999,
      "eval_runtime": 3.0135,
      "eval_samples_per_second": 920.863,
      "eval_steps_per_second": 115.149,
      "step": 70776
    },
    {
      "epoch": 36.00203458799593,
      "grad_norm": 42.63874435424805,
      "learning_rate": 1.3997965412004069e-05,
      "loss": 1.6907,
      "step": 70780
    },
    {
      "epoch": 36.007121057985756,
      "grad_norm": 51.651546478271484,
      "learning_rate": 1.3992878942014242e-05,
      "loss": 1.637,
      "step": 70790
    },
    {
      "epoch": 36.01220752797558,
      "grad_norm": 45.91990661621094,
      "learning_rate": 1.3987792472024416e-05,
      "loss": 1.6332,
      "step": 70800
    },
    {
      "epoch": 36.01729399796541,
      "grad_norm": 36.84451675415039,
      "learning_rate": 1.3982706002034587e-05,
      "loss": 1.6816,
      "step": 70810
    },
    {
      "epoch": 36.02238046795524,
      "grad_norm": 53.54664993286133,
      "learning_rate": 1.397761953204476e-05,
      "loss": 1.6779,
      "step": 70820
    },
    {
      "epoch": 36.027466937945064,
      "grad_norm": 43.14410400390625,
      "learning_rate": 1.3972533062054936e-05,
      "loss": 1.6488,
      "step": 70830
    },
    {
      "epoch": 36.03255340793489,
      "grad_norm": 45.44765853881836,
      "learning_rate": 1.3967446592065109e-05,
      "loss": 1.7262,
      "step": 70840
    },
    {
      "epoch": 36.03763987792472,
      "grad_norm": 36.22698974609375,
      "learning_rate": 1.396236012207528e-05,
      "loss": 1.7496,
      "step": 70850
    },
    {
      "epoch": 36.042726347914545,
      "grad_norm": 46.91424560546875,
      "learning_rate": 1.3957273652085454e-05,
      "loss": 1.69,
      "step": 70860
    },
    {
      "epoch": 36.04781281790437,
      "grad_norm": 52.085731506347656,
      "learning_rate": 1.3952187182095627e-05,
      "loss": 1.5865,
      "step": 70870
    },
    {
      "epoch": 36.0528992878942,
      "grad_norm": 35.88276290893555,
      "learning_rate": 1.3947100712105799e-05,
      "loss": 1.5681,
      "step": 70880
    },
    {
      "epoch": 36.057985757884026,
      "grad_norm": 36.34976577758789,
      "learning_rate": 1.3942014242115972e-05,
      "loss": 1.6247,
      "step": 70890
    },
    {
      "epoch": 36.06307222787385,
      "grad_norm": 40.49285888671875,
      "learning_rate": 1.3936927772126147e-05,
      "loss": 1.6129,
      "step": 70900
    },
    {
      "epoch": 36.06815869786368,
      "grad_norm": 37.704673767089844,
      "learning_rate": 1.3931841302136317e-05,
      "loss": 1.6468,
      "step": 70910
    },
    {
      "epoch": 36.07324516785351,
      "grad_norm": 33.53940963745117,
      "learning_rate": 1.392675483214649e-05,
      "loss": 1.709,
      "step": 70920
    },
    {
      "epoch": 36.078331637843334,
      "grad_norm": 29.835309982299805,
      "learning_rate": 1.3921668362156665e-05,
      "loss": 1.6472,
      "step": 70930
    },
    {
      "epoch": 36.08341810783316,
      "grad_norm": 37.60725021362305,
      "learning_rate": 1.3916581892166835e-05,
      "loss": 1.6501,
      "step": 70940
    },
    {
      "epoch": 36.08850457782299,
      "grad_norm": 42.80056381225586,
      "learning_rate": 1.391149542217701e-05,
      "loss": 1.5725,
      "step": 70950
    },
    {
      "epoch": 36.093591047812815,
      "grad_norm": 42.611183166503906,
      "learning_rate": 1.3906408952187183e-05,
      "loss": 1.5898,
      "step": 70960
    },
    {
      "epoch": 36.09867751780264,
      "grad_norm": 48.797061920166016,
      "learning_rate": 1.3901322482197356e-05,
      "loss": 1.716,
      "step": 70970
    },
    {
      "epoch": 36.10376398779247,
      "grad_norm": 40.031898498535156,
      "learning_rate": 1.3896236012207528e-05,
      "loss": 1.6912,
      "step": 70980
    },
    {
      "epoch": 36.108850457782296,
      "grad_norm": 38.736968994140625,
      "learning_rate": 1.3891149542217701e-05,
      "loss": 1.6169,
      "step": 70990
    },
    {
      "epoch": 36.11393692777212,
      "grad_norm": 42.1123161315918,
      "learning_rate": 1.3886063072227876e-05,
      "loss": 1.6628,
      "step": 71000
    },
    {
      "epoch": 36.11902339776195,
      "grad_norm": 38.475189208984375,
      "learning_rate": 1.3880976602238046e-05,
      "loss": 1.658,
      "step": 71010
    },
    {
      "epoch": 36.12410986775178,
      "grad_norm": 36.368717193603516,
      "learning_rate": 1.3875890132248221e-05,
      "loss": 1.5926,
      "step": 71020
    },
    {
      "epoch": 36.129196337741604,
      "grad_norm": 35.224124908447266,
      "learning_rate": 1.3870803662258394e-05,
      "loss": 1.6234,
      "step": 71030
    },
    {
      "epoch": 36.13428280773143,
      "grad_norm": 46.95145034790039,
      "learning_rate": 1.3865717192268566e-05,
      "loss": 1.6597,
      "step": 71040
    },
    {
      "epoch": 36.139369277721265,
      "grad_norm": 42.45965576171875,
      "learning_rate": 1.386063072227874e-05,
      "loss": 1.7256,
      "step": 71050
    },
    {
      "epoch": 36.14445574771109,
      "grad_norm": 40.20166778564453,
      "learning_rate": 1.3855544252288913e-05,
      "loss": 1.6838,
      "step": 71060
    },
    {
      "epoch": 36.14954221770092,
      "grad_norm": 40.655975341796875,
      "learning_rate": 1.3850457782299084e-05,
      "loss": 1.614,
      "step": 71070
    },
    {
      "epoch": 36.154628687690746,
      "grad_norm": 36.48530578613281,
      "learning_rate": 1.3845371312309257e-05,
      "loss": 1.6337,
      "step": 71080
    },
    {
      "epoch": 36.15971515768057,
      "grad_norm": 41.15682601928711,
      "learning_rate": 1.384028484231943e-05,
      "loss": 1.6803,
      "step": 71090
    },
    {
      "epoch": 36.1648016276704,
      "grad_norm": 39.45370864868164,
      "learning_rate": 1.3835198372329606e-05,
      "loss": 1.6365,
      "step": 71100
    },
    {
      "epoch": 36.16988809766023,
      "grad_norm": 44.708251953125,
      "learning_rate": 1.3830111902339776e-05,
      "loss": 1.5396,
      "step": 71110
    },
    {
      "epoch": 36.174974567650054,
      "grad_norm": 36.92531204223633,
      "learning_rate": 1.382502543234995e-05,
      "loss": 1.6591,
      "step": 71120
    },
    {
      "epoch": 36.18006103763988,
      "grad_norm": 40.45001983642578,
      "learning_rate": 1.3819938962360124e-05,
      "loss": 1.7134,
      "step": 71130
    },
    {
      "epoch": 36.18514750762971,
      "grad_norm": 58.080421447753906,
      "learning_rate": 1.3814852492370295e-05,
      "loss": 1.6287,
      "step": 71140
    },
    {
      "epoch": 36.190233977619535,
      "grad_norm": 41.19862747192383,
      "learning_rate": 1.3809766022380469e-05,
      "loss": 1.6159,
      "step": 71150
    },
    {
      "epoch": 36.19532044760936,
      "grad_norm": 37.26998519897461,
      "learning_rate": 1.3804679552390642e-05,
      "loss": 1.6957,
      "step": 71160
    },
    {
      "epoch": 36.20040691759919,
      "grad_norm": 35.87986373901367,
      "learning_rate": 1.3799593082400814e-05,
      "loss": 1.6461,
      "step": 71170
    },
    {
      "epoch": 36.205493387589016,
      "grad_norm": 45.85139465332031,
      "learning_rate": 1.3794506612410987e-05,
      "loss": 1.6473,
      "step": 71180
    },
    {
      "epoch": 36.21057985757884,
      "grad_norm": 41.575565338134766,
      "learning_rate": 1.3789420142421162e-05,
      "loss": 1.6559,
      "step": 71190
    },
    {
      "epoch": 36.21566632756867,
      "grad_norm": 35.591495513916016,
      "learning_rate": 1.3784333672431332e-05,
      "loss": 1.7155,
      "step": 71200
    },
    {
      "epoch": 36.2207527975585,
      "grad_norm": 41.15523910522461,
      "learning_rate": 1.3779247202441507e-05,
      "loss": 1.6551,
      "step": 71210
    },
    {
      "epoch": 36.225839267548324,
      "grad_norm": 35.30626678466797,
      "learning_rate": 1.377416073245168e-05,
      "loss": 1.6964,
      "step": 71220
    },
    {
      "epoch": 36.23092573753815,
      "grad_norm": 38.40923309326172,
      "learning_rate": 1.3769074262461853e-05,
      "loss": 1.6138,
      "step": 71230
    },
    {
      "epoch": 36.23601220752798,
      "grad_norm": 43.52889633178711,
      "learning_rate": 1.3763987792472025e-05,
      "loss": 1.6405,
      "step": 71240
    },
    {
      "epoch": 36.241098677517805,
      "grad_norm": 43.8492546081543,
      "learning_rate": 1.3758901322482198e-05,
      "loss": 1.6412,
      "step": 71250
    },
    {
      "epoch": 36.24618514750763,
      "grad_norm": 41.233455657958984,
      "learning_rate": 1.3753814852492371e-05,
      "loss": 1.6733,
      "step": 71260
    },
    {
      "epoch": 36.25127161749746,
      "grad_norm": 37.55735397338867,
      "learning_rate": 1.3748728382502543e-05,
      "loss": 1.6919,
      "step": 71270
    },
    {
      "epoch": 36.256358087487286,
      "grad_norm": 44.93625259399414,
      "learning_rate": 1.3743641912512716e-05,
      "loss": 1.7026,
      "step": 71280
    },
    {
      "epoch": 36.26144455747711,
      "grad_norm": 46.22471237182617,
      "learning_rate": 1.3738555442522891e-05,
      "loss": 1.6316,
      "step": 71290
    },
    {
      "epoch": 36.26653102746694,
      "grad_norm": 42.17133331298828,
      "learning_rate": 1.3733468972533061e-05,
      "loss": 1.7133,
      "step": 71300
    },
    {
      "epoch": 36.271617497456766,
      "grad_norm": 37.67510986328125,
      "learning_rate": 1.3728382502543236e-05,
      "loss": 1.7509,
      "step": 71310
    },
    {
      "epoch": 36.27670396744659,
      "grad_norm": 41.57462692260742,
      "learning_rate": 1.372329603255341e-05,
      "loss": 1.6496,
      "step": 71320
    },
    {
      "epoch": 36.28179043743642,
      "grad_norm": 36.97625732421875,
      "learning_rate": 1.3718209562563581e-05,
      "loss": 1.595,
      "step": 71330
    },
    {
      "epoch": 36.28687690742625,
      "grad_norm": 34.752872467041016,
      "learning_rate": 1.3713123092573754e-05,
      "loss": 1.6199,
      "step": 71340
    },
    {
      "epoch": 36.291963377416074,
      "grad_norm": 36.23323440551758,
      "learning_rate": 1.3708036622583928e-05,
      "loss": 1.5674,
      "step": 71350
    },
    {
      "epoch": 36.2970498474059,
      "grad_norm": 36.45059585571289,
      "learning_rate": 1.3702950152594099e-05,
      "loss": 1.6269,
      "step": 71360
    },
    {
      "epoch": 36.30213631739573,
      "grad_norm": 38.34005355834961,
      "learning_rate": 1.3697863682604272e-05,
      "loss": 1.5867,
      "step": 71370
    },
    {
      "epoch": 36.307222787385555,
      "grad_norm": 37.111690521240234,
      "learning_rate": 1.3692777212614447e-05,
      "loss": 1.5999,
      "step": 71380
    },
    {
      "epoch": 36.31230925737538,
      "grad_norm": 46.78885269165039,
      "learning_rate": 1.368769074262462e-05,
      "loss": 1.6598,
      "step": 71390
    },
    {
      "epoch": 36.31739572736521,
      "grad_norm": 32.73290252685547,
      "learning_rate": 1.368260427263479e-05,
      "loss": 1.6942,
      "step": 71400
    },
    {
      "epoch": 36.322482197355036,
      "grad_norm": 31.249570846557617,
      "learning_rate": 1.3677517802644966e-05,
      "loss": 1.6125,
      "step": 71410
    },
    {
      "epoch": 36.32756866734486,
      "grad_norm": 45.19020462036133,
      "learning_rate": 1.3672431332655139e-05,
      "loss": 1.6426,
      "step": 71420
    },
    {
      "epoch": 36.33265513733469,
      "grad_norm": 46.32311248779297,
      "learning_rate": 1.366734486266531e-05,
      "loss": 1.6341,
      "step": 71430
    },
    {
      "epoch": 36.33774160732452,
      "grad_norm": 40.984283447265625,
      "learning_rate": 1.3662258392675484e-05,
      "loss": 1.6097,
      "step": 71440
    },
    {
      "epoch": 36.342828077314344,
      "grad_norm": 35.435951232910156,
      "learning_rate": 1.3657171922685657e-05,
      "loss": 1.5961,
      "step": 71450
    },
    {
      "epoch": 36.34791454730417,
      "grad_norm": 46.95206069946289,
      "learning_rate": 1.3652085452695829e-05,
      "loss": 1.6283,
      "step": 71460
    },
    {
      "epoch": 36.353001017294,
      "grad_norm": 40.5883903503418,
      "learning_rate": 1.3646998982706002e-05,
      "loss": 1.6265,
      "step": 71470
    },
    {
      "epoch": 36.358087487283825,
      "grad_norm": 38.19541549682617,
      "learning_rate": 1.3641912512716177e-05,
      "loss": 1.5965,
      "step": 71480
    },
    {
      "epoch": 36.36317395727365,
      "grad_norm": 43.70241165161133,
      "learning_rate": 1.3636826042726347e-05,
      "loss": 1.6281,
      "step": 71490
    },
    {
      "epoch": 36.36826042726348,
      "grad_norm": 46.16619110107422,
      "learning_rate": 1.3631739572736522e-05,
      "loss": 1.6569,
      "step": 71500
    },
    {
      "epoch": 36.373346897253306,
      "grad_norm": 37.85822296142578,
      "learning_rate": 1.3626653102746695e-05,
      "loss": 1.5861,
      "step": 71510
    },
    {
      "epoch": 36.37843336724313,
      "grad_norm": 39.89045333862305,
      "learning_rate": 1.3621566632756868e-05,
      "loss": 1.5789,
      "step": 71520
    },
    {
      "epoch": 36.38351983723296,
      "grad_norm": 57.0560302734375,
      "learning_rate": 1.361648016276704e-05,
      "loss": 1.6144,
      "step": 71530
    },
    {
      "epoch": 36.38860630722279,
      "grad_norm": 40.61867904663086,
      "learning_rate": 1.3611393692777213e-05,
      "loss": 1.6394,
      "step": 71540
    },
    {
      "epoch": 36.393692777212614,
      "grad_norm": 42.16575622558594,
      "learning_rate": 1.3606307222787388e-05,
      "loss": 1.66,
      "step": 71550
    },
    {
      "epoch": 36.39877924720244,
      "grad_norm": 36.26327133178711,
      "learning_rate": 1.3601220752797558e-05,
      "loss": 1.7259,
      "step": 71560
    },
    {
      "epoch": 36.40386571719227,
      "grad_norm": 33.50514221191406,
      "learning_rate": 1.3596134282807731e-05,
      "loss": 1.6871,
      "step": 71570
    },
    {
      "epoch": 36.408952187182095,
      "grad_norm": 39.38764572143555,
      "learning_rate": 1.3591047812817906e-05,
      "loss": 1.7708,
      "step": 71580
    },
    {
      "epoch": 36.41403865717192,
      "grad_norm": 39.771846771240234,
      "learning_rate": 1.3585961342828076e-05,
      "loss": 1.6691,
      "step": 71590
    },
    {
      "epoch": 36.41912512716175,
      "grad_norm": 58.345008850097656,
      "learning_rate": 1.3580874872838251e-05,
      "loss": 1.6666,
      "step": 71600
    },
    {
      "epoch": 36.424211597151576,
      "grad_norm": 45.331031799316406,
      "learning_rate": 1.3575788402848424e-05,
      "loss": 1.6657,
      "step": 71610
    },
    {
      "epoch": 36.4292980671414,
      "grad_norm": 34.91443634033203,
      "learning_rate": 1.3570701932858596e-05,
      "loss": 1.6395,
      "step": 71620
    },
    {
      "epoch": 36.43438453713123,
      "grad_norm": 39.159156799316406,
      "learning_rate": 1.356561546286877e-05,
      "loss": 1.6522,
      "step": 71630
    },
    {
      "epoch": 36.43947100712106,
      "grad_norm": 34.73916244506836,
      "learning_rate": 1.3560528992878943e-05,
      "loss": 1.6172,
      "step": 71640
    },
    {
      "epoch": 36.444557477110884,
      "grad_norm": 42.8867301940918,
      "learning_rate": 1.3555442522889118e-05,
      "loss": 1.5586,
      "step": 71650
    },
    {
      "epoch": 36.44964394710071,
      "grad_norm": 44.390342712402344,
      "learning_rate": 1.3550356052899287e-05,
      "loss": 1.6503,
      "step": 71660
    },
    {
      "epoch": 36.45473041709054,
      "grad_norm": 42.5634765625,
      "learning_rate": 1.3545269582909462e-05,
      "loss": 1.7111,
      "step": 71670
    },
    {
      "epoch": 36.459816887080365,
      "grad_norm": 44.24265670776367,
      "learning_rate": 1.3540183112919636e-05,
      "loss": 1.566,
      "step": 71680
    },
    {
      "epoch": 36.46490335707019,
      "grad_norm": 34.722896575927734,
      "learning_rate": 1.3535096642929807e-05,
      "loss": 1.6988,
      "step": 71690
    },
    {
      "epoch": 36.46998982706002,
      "grad_norm": 34.66132354736328,
      "learning_rate": 1.353001017293998e-05,
      "loss": 1.59,
      "step": 71700
    },
    {
      "epoch": 36.475076297049846,
      "grad_norm": 55.41350173950195,
      "learning_rate": 1.3524923702950154e-05,
      "loss": 1.7024,
      "step": 71710
    },
    {
      "epoch": 36.48016276703967,
      "grad_norm": 38.527042388916016,
      "learning_rate": 1.3519837232960325e-05,
      "loss": 1.6137,
      "step": 71720
    },
    {
      "epoch": 36.4852492370295,
      "grad_norm": 31.922454833984375,
      "learning_rate": 1.3514750762970499e-05,
      "loss": 1.6541,
      "step": 71730
    },
    {
      "epoch": 36.49033570701933,
      "grad_norm": 48.512577056884766,
      "learning_rate": 1.3509664292980672e-05,
      "loss": 1.636,
      "step": 71740
    },
    {
      "epoch": 36.495422177009154,
      "grad_norm": 34.99551773071289,
      "learning_rate": 1.3504577822990844e-05,
      "loss": 1.6895,
      "step": 71750
    },
    {
      "epoch": 36.50050864699898,
      "grad_norm": 32.20065689086914,
      "learning_rate": 1.3499491353001017e-05,
      "loss": 1.6022,
      "step": 71760
    },
    {
      "epoch": 36.50559511698881,
      "grad_norm": 37.78047561645508,
      "learning_rate": 1.3494404883011192e-05,
      "loss": 1.7111,
      "step": 71770
    },
    {
      "epoch": 36.510681586978635,
      "grad_norm": 36.02092742919922,
      "learning_rate": 1.3489318413021365e-05,
      "loss": 1.6471,
      "step": 71780
    },
    {
      "epoch": 36.51576805696846,
      "grad_norm": 31.730937957763672,
      "learning_rate": 1.3484231943031537e-05,
      "loss": 1.7214,
      "step": 71790
    },
    {
      "epoch": 36.52085452695829,
      "grad_norm": 56.94103240966797,
      "learning_rate": 1.347914547304171e-05,
      "loss": 1.6857,
      "step": 71800
    },
    {
      "epoch": 36.525940996948115,
      "grad_norm": 39.20550537109375,
      "learning_rate": 1.3474059003051883e-05,
      "loss": 1.5195,
      "step": 71810
    },
    {
      "epoch": 36.53102746693794,
      "grad_norm": 45.485809326171875,
      "learning_rate": 1.3468972533062055e-05,
      "loss": 1.595,
      "step": 71820
    },
    {
      "epoch": 36.53611393692777,
      "grad_norm": 40.64946365356445,
      "learning_rate": 1.3463886063072228e-05,
      "loss": 1.6622,
      "step": 71830
    },
    {
      "epoch": 36.541200406917596,
      "grad_norm": 35.36991500854492,
      "learning_rate": 1.3458799593082403e-05,
      "loss": 1.6194,
      "step": 71840
    },
    {
      "epoch": 36.54628687690742,
      "grad_norm": 38.976661682128906,
      "learning_rate": 1.3453713123092573e-05,
      "loss": 1.758,
      "step": 71850
    },
    {
      "epoch": 36.55137334689725,
      "grad_norm": 55.302711486816406,
      "learning_rate": 1.3448626653102748e-05,
      "loss": 1.7839,
      "step": 71860
    },
    {
      "epoch": 36.55645981688708,
      "grad_norm": 46.67491912841797,
      "learning_rate": 1.3443540183112921e-05,
      "loss": 1.6823,
      "step": 71870
    },
    {
      "epoch": 36.561546286876904,
      "grad_norm": 37.778831481933594,
      "learning_rate": 1.3438453713123093e-05,
      "loss": 1.5623,
      "step": 71880
    },
    {
      "epoch": 36.56663275686673,
      "grad_norm": 39.36643600463867,
      "learning_rate": 1.3433367243133266e-05,
      "loss": 1.6679,
      "step": 71890
    },
    {
      "epoch": 36.57171922685656,
      "grad_norm": 36.63212203979492,
      "learning_rate": 1.342828077314344e-05,
      "loss": 1.6986,
      "step": 71900
    },
    {
      "epoch": 36.576805696846385,
      "grad_norm": 37.229618072509766,
      "learning_rate": 1.3423194303153613e-05,
      "loss": 1.6668,
      "step": 71910
    },
    {
      "epoch": 36.58189216683621,
      "grad_norm": 39.51224136352539,
      "learning_rate": 1.3418107833163784e-05,
      "loss": 1.6826,
      "step": 71920
    },
    {
      "epoch": 36.58697863682604,
      "grad_norm": 41.905662536621094,
      "learning_rate": 1.3413021363173958e-05,
      "loss": 1.6503,
      "step": 71930
    },
    {
      "epoch": 36.592065106815866,
      "grad_norm": 44.519649505615234,
      "learning_rate": 1.3407934893184133e-05,
      "loss": 1.6432,
      "step": 71940
    },
    {
      "epoch": 36.5971515768057,
      "grad_norm": 46.19459915161133,
      "learning_rate": 1.3402848423194302e-05,
      "loss": 1.6174,
      "step": 71950
    },
    {
      "epoch": 36.60223804679553,
      "grad_norm": 31.749879837036133,
      "learning_rate": 1.3397761953204477e-05,
      "loss": 1.6489,
      "step": 71960
    },
    {
      "epoch": 36.607324516785354,
      "grad_norm": 44.648799896240234,
      "learning_rate": 1.339267548321465e-05,
      "loss": 1.6551,
      "step": 71970
    },
    {
      "epoch": 36.61241098677518,
      "grad_norm": 42.810218811035156,
      "learning_rate": 1.3387589013224822e-05,
      "loss": 1.6039,
      "step": 71980
    },
    {
      "epoch": 36.61749745676501,
      "grad_norm": 50.53117370605469,
      "learning_rate": 1.3382502543234996e-05,
      "loss": 1.6017,
      "step": 71990
    },
    {
      "epoch": 36.622583926754835,
      "grad_norm": 46.76298141479492,
      "learning_rate": 1.3377416073245169e-05,
      "loss": 1.698,
      "step": 72000
    },
    {
      "epoch": 36.62767039674466,
      "grad_norm": 55.921546936035156,
      "learning_rate": 1.337232960325534e-05,
      "loss": 1.6553,
      "step": 72010
    },
    {
      "epoch": 36.63275686673449,
      "grad_norm": 38.96786880493164,
      "learning_rate": 1.3367243133265514e-05,
      "loss": 1.7004,
      "step": 72020
    },
    {
      "epoch": 36.637843336724316,
      "grad_norm": 44.01092529296875,
      "learning_rate": 1.3362156663275689e-05,
      "loss": 1.5758,
      "step": 72030
    },
    {
      "epoch": 36.64292980671414,
      "grad_norm": 42.806373596191406,
      "learning_rate": 1.3357070193285862e-05,
      "loss": 1.6783,
      "step": 72040
    },
    {
      "epoch": 36.64801627670397,
      "grad_norm": 47.27772903442383,
      "learning_rate": 1.3351983723296032e-05,
      "loss": 1.6836,
      "step": 72050
    },
    {
      "epoch": 36.6531027466938,
      "grad_norm": 33.61470413208008,
      "learning_rate": 1.3346897253306207e-05,
      "loss": 1.6265,
      "step": 72060
    },
    {
      "epoch": 36.658189216683624,
      "grad_norm": 41.95943069458008,
      "learning_rate": 1.334181078331638e-05,
      "loss": 1.6445,
      "step": 72070
    },
    {
      "epoch": 36.66327568667345,
      "grad_norm": 45.97286605834961,
      "learning_rate": 1.3336724313326552e-05,
      "loss": 1.6843,
      "step": 72080
    },
    {
      "epoch": 36.66836215666328,
      "grad_norm": 39.260650634765625,
      "learning_rate": 1.3331637843336725e-05,
      "loss": 1.6552,
      "step": 72090
    },
    {
      "epoch": 36.673448626653105,
      "grad_norm": 49.08243942260742,
      "learning_rate": 1.3326551373346898e-05,
      "loss": 1.6017,
      "step": 72100
    },
    {
      "epoch": 36.67853509664293,
      "grad_norm": 30.880319595336914,
      "learning_rate": 1.332146490335707e-05,
      "loss": 1.6376,
      "step": 72110
    },
    {
      "epoch": 36.68362156663276,
      "grad_norm": 42.435951232910156,
      "learning_rate": 1.3316378433367243e-05,
      "loss": 1.653,
      "step": 72120
    },
    {
      "epoch": 36.688708036622586,
      "grad_norm": 46.93937683105469,
      "learning_rate": 1.3311291963377418e-05,
      "loss": 1.6403,
      "step": 72130
    },
    {
      "epoch": 36.69379450661241,
      "grad_norm": 45.89051055908203,
      "learning_rate": 1.3306205493387588e-05,
      "loss": 1.7371,
      "step": 72140
    },
    {
      "epoch": 36.69888097660224,
      "grad_norm": 48.362186431884766,
      "learning_rate": 1.3301119023397763e-05,
      "loss": 1.5829,
      "step": 72150
    },
    {
      "epoch": 36.70396744659207,
      "grad_norm": 55.62729263305664,
      "learning_rate": 1.3296032553407936e-05,
      "loss": 1.6563,
      "step": 72160
    },
    {
      "epoch": 36.709053916581894,
      "grad_norm": 36.9700813293457,
      "learning_rate": 1.3290946083418108e-05,
      "loss": 1.653,
      "step": 72170
    },
    {
      "epoch": 36.71414038657172,
      "grad_norm": 36.550498962402344,
      "learning_rate": 1.3285859613428281e-05,
      "loss": 1.6664,
      "step": 72180
    },
    {
      "epoch": 36.71922685656155,
      "grad_norm": 34.26179122924805,
      "learning_rate": 1.3280773143438454e-05,
      "loss": 1.5976,
      "step": 72190
    },
    {
      "epoch": 36.724313326551375,
      "grad_norm": 44.80337905883789,
      "learning_rate": 1.3275686673448628e-05,
      "loss": 1.6981,
      "step": 72200
    },
    {
      "epoch": 36.7293997965412,
      "grad_norm": 53.55989074707031,
      "learning_rate": 1.32706002034588e-05,
      "loss": 1.6901,
      "step": 72210
    },
    {
      "epoch": 36.73448626653103,
      "grad_norm": 35.60678482055664,
      "learning_rate": 1.3265513733468973e-05,
      "loss": 1.5547,
      "step": 72220
    },
    {
      "epoch": 36.739572736520856,
      "grad_norm": 44.7293701171875,
      "learning_rate": 1.3260427263479148e-05,
      "loss": 1.6097,
      "step": 72230
    },
    {
      "epoch": 36.74465920651068,
      "grad_norm": 37.415794372558594,
      "learning_rate": 1.3255340793489317e-05,
      "loss": 1.5284,
      "step": 72240
    },
    {
      "epoch": 36.74974567650051,
      "grad_norm": 45.62665557861328,
      "learning_rate": 1.3250254323499492e-05,
      "loss": 1.6481,
      "step": 72250
    },
    {
      "epoch": 36.75483214649034,
      "grad_norm": 38.36404037475586,
      "learning_rate": 1.3245167853509666e-05,
      "loss": 1.7161,
      "step": 72260
    },
    {
      "epoch": 36.759918616480164,
      "grad_norm": 41.093650817871094,
      "learning_rate": 1.3240081383519837e-05,
      "loss": 1.5798,
      "step": 72270
    },
    {
      "epoch": 36.76500508646999,
      "grad_norm": 33.30546569824219,
      "learning_rate": 1.323499491353001e-05,
      "loss": 1.6507,
      "step": 72280
    },
    {
      "epoch": 36.77009155645982,
      "grad_norm": 34.48930740356445,
      "learning_rate": 1.3229908443540184e-05,
      "loss": 1.5717,
      "step": 72290
    },
    {
      "epoch": 36.775178026449645,
      "grad_norm": 37.21224594116211,
      "learning_rate": 1.3224821973550355e-05,
      "loss": 1.6756,
      "step": 72300
    },
    {
      "epoch": 36.78026449643947,
      "grad_norm": 43.079261779785156,
      "learning_rate": 1.3219735503560529e-05,
      "loss": 1.624,
      "step": 72310
    },
    {
      "epoch": 36.7853509664293,
      "grad_norm": 46.88025665283203,
      "learning_rate": 1.3214649033570704e-05,
      "loss": 1.7133,
      "step": 72320
    },
    {
      "epoch": 36.790437436419126,
      "grad_norm": 38.818458557128906,
      "learning_rate": 1.3209562563580877e-05,
      "loss": 1.6444,
      "step": 72330
    },
    {
      "epoch": 36.79552390640895,
      "grad_norm": 43.549251556396484,
      "learning_rate": 1.3204476093591049e-05,
      "loss": 1.5837,
      "step": 72340
    },
    {
      "epoch": 36.80061037639878,
      "grad_norm": 40.47061538696289,
      "learning_rate": 1.3199389623601222e-05,
      "loss": 1.6451,
      "step": 72350
    },
    {
      "epoch": 36.80569684638861,
      "grad_norm": 39.02223587036133,
      "learning_rate": 1.3194303153611395e-05,
      "loss": 1.6338,
      "step": 72360
    },
    {
      "epoch": 36.81078331637843,
      "grad_norm": 30.216331481933594,
      "learning_rate": 1.3189216683621567e-05,
      "loss": 1.628,
      "step": 72370
    },
    {
      "epoch": 36.81586978636826,
      "grad_norm": 33.589317321777344,
      "learning_rate": 1.318413021363174e-05,
      "loss": 1.6279,
      "step": 72380
    },
    {
      "epoch": 36.82095625635809,
      "grad_norm": 36.646480560302734,
      "learning_rate": 1.3179043743641913e-05,
      "loss": 1.6233,
      "step": 72390
    },
    {
      "epoch": 36.826042726347914,
      "grad_norm": 41.58903121948242,
      "learning_rate": 1.3173957273652085e-05,
      "loss": 1.5909,
      "step": 72400
    },
    {
      "epoch": 36.83112919633774,
      "grad_norm": 48.02603530883789,
      "learning_rate": 1.3168870803662258e-05,
      "loss": 1.6577,
      "step": 72410
    },
    {
      "epoch": 36.83621566632757,
      "grad_norm": 41.00141525268555,
      "learning_rate": 1.3163784333672433e-05,
      "loss": 1.5834,
      "step": 72420
    },
    {
      "epoch": 36.841302136317395,
      "grad_norm": 32.815032958984375,
      "learning_rate": 1.3158697863682603e-05,
      "loss": 1.689,
      "step": 72430
    },
    {
      "epoch": 36.84638860630722,
      "grad_norm": 43.97605895996094,
      "learning_rate": 1.3153611393692778e-05,
      "loss": 1.6908,
      "step": 72440
    },
    {
      "epoch": 36.85147507629705,
      "grad_norm": 37.780235290527344,
      "learning_rate": 1.3148524923702951e-05,
      "loss": 1.6118,
      "step": 72450
    },
    {
      "epoch": 36.856561546286876,
      "grad_norm": 36.46862030029297,
      "learning_rate": 1.3143438453713125e-05,
      "loss": 1.6654,
      "step": 72460
    },
    {
      "epoch": 36.8616480162767,
      "grad_norm": 40.25262451171875,
      "learning_rate": 1.3138351983723296e-05,
      "loss": 1.5765,
      "step": 72470
    },
    {
      "epoch": 36.86673448626653,
      "grad_norm": 40.7149772644043,
      "learning_rate": 1.313326551373347e-05,
      "loss": 1.6221,
      "step": 72480
    },
    {
      "epoch": 36.87182095625636,
      "grad_norm": 42.985626220703125,
      "learning_rate": 1.3128179043743644e-05,
      "loss": 1.7005,
      "step": 72490
    },
    {
      "epoch": 36.876907426246184,
      "grad_norm": 38.94013977050781,
      "learning_rate": 1.3123092573753814e-05,
      "loss": 1.6103,
      "step": 72500
    },
    {
      "epoch": 36.88199389623601,
      "grad_norm": 41.153377532958984,
      "learning_rate": 1.311800610376399e-05,
      "loss": 1.5793,
      "step": 72510
    },
    {
      "epoch": 36.88708036622584,
      "grad_norm": 43.551239013671875,
      "learning_rate": 1.3112919633774163e-05,
      "loss": 1.5682,
      "step": 72520
    },
    {
      "epoch": 36.892166836215665,
      "grad_norm": 43.46742248535156,
      "learning_rate": 1.3107833163784332e-05,
      "loss": 1.7175,
      "step": 72530
    },
    {
      "epoch": 36.89725330620549,
      "grad_norm": 38.521324157714844,
      "learning_rate": 1.3102746693794507e-05,
      "loss": 1.648,
      "step": 72540
    },
    {
      "epoch": 36.90233977619532,
      "grad_norm": 42.913150787353516,
      "learning_rate": 1.309766022380468e-05,
      "loss": 1.6079,
      "step": 72550
    },
    {
      "epoch": 36.907426246185146,
      "grad_norm": 37.2099494934082,
      "learning_rate": 1.3092573753814852e-05,
      "loss": 1.5793,
      "step": 72560
    },
    {
      "epoch": 36.91251271617497,
      "grad_norm": 41.98181915283203,
      "learning_rate": 1.3087487283825026e-05,
      "loss": 1.6019,
      "step": 72570
    },
    {
      "epoch": 36.9175991861648,
      "grad_norm": 39.692256927490234,
      "learning_rate": 1.3082400813835199e-05,
      "loss": 1.6436,
      "step": 72580
    },
    {
      "epoch": 36.92268565615463,
      "grad_norm": 36.39116668701172,
      "learning_rate": 1.3077314343845374e-05,
      "loss": 1.6705,
      "step": 72590
    },
    {
      "epoch": 36.927772126144454,
      "grad_norm": 42.06577682495117,
      "learning_rate": 1.3072227873855544e-05,
      "loss": 1.6874,
      "step": 72600
    },
    {
      "epoch": 36.93285859613428,
      "grad_norm": 33.12021255493164,
      "learning_rate": 1.3067141403865719e-05,
      "loss": 1.6692,
      "step": 72610
    },
    {
      "epoch": 36.93794506612411,
      "grad_norm": 42.8177375793457,
      "learning_rate": 1.3062054933875892e-05,
      "loss": 1.5856,
      "step": 72620
    },
    {
      "epoch": 36.943031536113935,
      "grad_norm": 36.83694076538086,
      "learning_rate": 1.3056968463886064e-05,
      "loss": 1.6416,
      "step": 72630
    },
    {
      "epoch": 36.94811800610376,
      "grad_norm": 37.15297317504883,
      "learning_rate": 1.3051881993896237e-05,
      "loss": 1.7053,
      "step": 72640
    },
    {
      "epoch": 36.95320447609359,
      "grad_norm": 40.45919418334961,
      "learning_rate": 1.304679552390641e-05,
      "loss": 1.59,
      "step": 72650
    },
    {
      "epoch": 36.958290946083416,
      "grad_norm": 49.250038146972656,
      "learning_rate": 1.3041709053916582e-05,
      "loss": 1.7672,
      "step": 72660
    },
    {
      "epoch": 36.96337741607324,
      "grad_norm": 28.610675811767578,
      "learning_rate": 1.3036622583926755e-05,
      "loss": 1.6302,
      "step": 72670
    },
    {
      "epoch": 36.96846388606307,
      "grad_norm": 39.281333923339844,
      "learning_rate": 1.3031536113936928e-05,
      "loss": 1.508,
      "step": 72680
    },
    {
      "epoch": 36.9735503560529,
      "grad_norm": 43.647701263427734,
      "learning_rate": 1.30264496439471e-05,
      "loss": 1.6598,
      "step": 72690
    },
    {
      "epoch": 36.978636826042724,
      "grad_norm": 50.33451461791992,
      "learning_rate": 1.3021363173957273e-05,
      "loss": 1.634,
      "step": 72700
    },
    {
      "epoch": 36.98372329603255,
      "grad_norm": 42.0603141784668,
      "learning_rate": 1.3016276703967448e-05,
      "loss": 1.7247,
      "step": 72710
    },
    {
      "epoch": 36.98880976602238,
      "grad_norm": 47.0157585144043,
      "learning_rate": 1.3011190233977621e-05,
      "loss": 1.6802,
      "step": 72720
    },
    {
      "epoch": 36.993896236012205,
      "grad_norm": 36.71793746948242,
      "learning_rate": 1.3006103763987793e-05,
      "loss": 1.6918,
      "step": 72730
    },
    {
      "epoch": 36.99898270600203,
      "grad_norm": 38.85560989379883,
      "learning_rate": 1.3001017293997966e-05,
      "loss": 1.6353,
      "step": 72740
    },
    {
      "epoch": 37.0,
      "eval_loss": 4.960134506225586,
      "eval_runtime": 2.765,
      "eval_samples_per_second": 1003.61,
      "eval_steps_per_second": 125.496,
      "step": 72742
    },
    {
      "epoch": 37.00406917599186,
      "grad_norm": 35.79235076904297,
      "learning_rate": 1.299593082400814e-05,
      "loss": 1.6017,
      "step": 72750
    },
    {
      "epoch": 37.009155645981686,
      "grad_norm": 50.31473159790039,
      "learning_rate": 1.2990844354018311e-05,
      "loss": 1.7613,
      "step": 72760
    },
    {
      "epoch": 37.01424211597151,
      "grad_norm": 51.66456985473633,
      "learning_rate": 1.2985757884028484e-05,
      "loss": 1.6721,
      "step": 72770
    },
    {
      "epoch": 37.01932858596134,
      "grad_norm": 38.42595291137695,
      "learning_rate": 1.298067141403866e-05,
      "loss": 1.592,
      "step": 72780
    },
    {
      "epoch": 37.02441505595117,
      "grad_norm": 42.79154586791992,
      "learning_rate": 1.297558494404883e-05,
      "loss": 1.6707,
      "step": 72790
    },
    {
      "epoch": 37.029501525940994,
      "grad_norm": 39.01688766479492,
      "learning_rate": 1.2970498474059004e-05,
      "loss": 1.6128,
      "step": 72800
    },
    {
      "epoch": 37.03458799593082,
      "grad_norm": 49.51651382446289,
      "learning_rate": 1.2965412004069178e-05,
      "loss": 1.5472,
      "step": 72810
    },
    {
      "epoch": 37.03967446592065,
      "grad_norm": 39.745723724365234,
      "learning_rate": 1.2960325534079349e-05,
      "loss": 1.6587,
      "step": 72820
    },
    {
      "epoch": 37.044760935910475,
      "grad_norm": 43.99376678466797,
      "learning_rate": 1.2955239064089522e-05,
      "loss": 1.6708,
      "step": 72830
    },
    {
      "epoch": 37.04984740590031,
      "grad_norm": 39.97017288208008,
      "learning_rate": 1.2950152594099696e-05,
      "loss": 1.6426,
      "step": 72840
    },
    {
      "epoch": 37.054933875890136,
      "grad_norm": 34.5540885925293,
      "learning_rate": 1.2945066124109869e-05,
      "loss": 1.7241,
      "step": 72850
    },
    {
      "epoch": 37.06002034587996,
      "grad_norm": 31.577491760253906,
      "learning_rate": 1.293997965412004e-05,
      "loss": 1.6074,
      "step": 72860
    },
    {
      "epoch": 37.06510681586979,
      "grad_norm": 33.437965393066406,
      "learning_rate": 1.2934893184130214e-05,
      "loss": 1.6571,
      "step": 72870
    },
    {
      "epoch": 37.07019328585962,
      "grad_norm": 32.64323806762695,
      "learning_rate": 1.2929806714140389e-05,
      "loss": 1.6625,
      "step": 72880
    },
    {
      "epoch": 37.075279755849444,
      "grad_norm": 41.7846794128418,
      "learning_rate": 1.2924720244150559e-05,
      "loss": 1.6593,
      "step": 72890
    },
    {
      "epoch": 37.08036622583927,
      "grad_norm": 33.27648162841797,
      "learning_rate": 1.2919633774160734e-05,
      "loss": 1.5791,
      "step": 72900
    },
    {
      "epoch": 37.0854526958291,
      "grad_norm": 42.76651382446289,
      "learning_rate": 1.2914547304170907e-05,
      "loss": 1.7232,
      "step": 72910
    },
    {
      "epoch": 37.090539165818925,
      "grad_norm": 42.367279052734375,
      "learning_rate": 1.2909460834181079e-05,
      "loss": 1.6611,
      "step": 72920
    },
    {
      "epoch": 37.09562563580875,
      "grad_norm": 44.38315963745117,
      "learning_rate": 1.2904374364191252e-05,
      "loss": 1.6079,
      "step": 72930
    },
    {
      "epoch": 37.10071210579858,
      "grad_norm": 38.31559371948242,
      "learning_rate": 1.2899287894201425e-05,
      "loss": 1.6515,
      "step": 72940
    },
    {
      "epoch": 37.105798575788405,
      "grad_norm": 43.35407638549805,
      "learning_rate": 1.2894201424211597e-05,
      "loss": 1.5891,
      "step": 72950
    },
    {
      "epoch": 37.11088504577823,
      "grad_norm": 37.67702102661133,
      "learning_rate": 1.288911495422177e-05,
      "loss": 1.5819,
      "step": 72960
    },
    {
      "epoch": 37.11597151576806,
      "grad_norm": 41.63692855834961,
      "learning_rate": 1.2884028484231945e-05,
      "loss": 1.5249,
      "step": 72970
    },
    {
      "epoch": 37.121057985757886,
      "grad_norm": 49.24812316894531,
      "learning_rate": 1.2878942014242115e-05,
      "loss": 1.597,
      "step": 72980
    },
    {
      "epoch": 37.12614445574771,
      "grad_norm": 41.75022506713867,
      "learning_rate": 1.287385554425229e-05,
      "loss": 1.6294,
      "step": 72990
    },
    {
      "epoch": 37.13123092573754,
      "grad_norm": 32.1927375793457,
      "learning_rate": 1.2868769074262463e-05,
      "loss": 1.5419,
      "step": 73000
    },
    {
      "epoch": 37.13631739572737,
      "grad_norm": 46.07515335083008,
      "learning_rate": 1.2863682604272636e-05,
      "loss": 1.6022,
      "step": 73010
    },
    {
      "epoch": 37.141403865717194,
      "grad_norm": 39.71330261230469,
      "learning_rate": 1.2858596134282808e-05,
      "loss": 1.6049,
      "step": 73020
    },
    {
      "epoch": 37.14649033570702,
      "grad_norm": 51.44453048706055,
      "learning_rate": 1.2853509664292981e-05,
      "loss": 1.6647,
      "step": 73030
    },
    {
      "epoch": 37.15157680569685,
      "grad_norm": 49.66297912597656,
      "learning_rate": 1.2848423194303155e-05,
      "loss": 1.6477,
      "step": 73040
    },
    {
      "epoch": 37.156663275686675,
      "grad_norm": 40.46477127075195,
      "learning_rate": 1.2843336724313326e-05,
      "loss": 1.643,
      "step": 73050
    },
    {
      "epoch": 37.1617497456765,
      "grad_norm": 41.892330169677734,
      "learning_rate": 1.28382502543235e-05,
      "loss": 1.6178,
      "step": 73060
    },
    {
      "epoch": 37.16683621566633,
      "grad_norm": 38.209983825683594,
      "learning_rate": 1.2833163784333674e-05,
      "loss": 1.6567,
      "step": 73070
    },
    {
      "epoch": 37.171922685656156,
      "grad_norm": 35.18525695800781,
      "learning_rate": 1.2828077314343844e-05,
      "loss": 1.6605,
      "step": 73080
    },
    {
      "epoch": 37.17700915564598,
      "grad_norm": 49.28078079223633,
      "learning_rate": 1.282299084435402e-05,
      "loss": 1.7153,
      "step": 73090
    },
    {
      "epoch": 37.18209562563581,
      "grad_norm": 41.7891845703125,
      "learning_rate": 1.2817904374364193e-05,
      "loss": 1.5033,
      "step": 73100
    },
    {
      "epoch": 37.18718209562564,
      "grad_norm": 40.252323150634766,
      "learning_rate": 1.2812817904374364e-05,
      "loss": 1.598,
      "step": 73110
    },
    {
      "epoch": 37.192268565615464,
      "grad_norm": 32.7958869934082,
      "learning_rate": 1.2807731434384537e-05,
      "loss": 1.7322,
      "step": 73120
    },
    {
      "epoch": 37.19735503560529,
      "grad_norm": 35.22532653808594,
      "learning_rate": 1.280264496439471e-05,
      "loss": 1.6548,
      "step": 73130
    },
    {
      "epoch": 37.20244150559512,
      "grad_norm": 38.1631965637207,
      "learning_rate": 1.2797558494404886e-05,
      "loss": 1.6761,
      "step": 73140
    },
    {
      "epoch": 37.207527975584945,
      "grad_norm": 33.22401809692383,
      "learning_rate": 1.2792472024415056e-05,
      "loss": 1.6185,
      "step": 73150
    },
    {
      "epoch": 37.21261444557477,
      "grad_norm": 44.8875846862793,
      "learning_rate": 1.278738555442523e-05,
      "loss": 1.6431,
      "step": 73160
    },
    {
      "epoch": 37.2177009155646,
      "grad_norm": 45.505340576171875,
      "learning_rate": 1.2782299084435404e-05,
      "loss": 1.6932,
      "step": 73170
    },
    {
      "epoch": 37.222787385554426,
      "grad_norm": 39.173118591308594,
      "learning_rate": 1.2777212614445574e-05,
      "loss": 1.6479,
      "step": 73180
    },
    {
      "epoch": 37.22787385554425,
      "grad_norm": 36.68214416503906,
      "learning_rate": 1.2772126144455749e-05,
      "loss": 1.6255,
      "step": 73190
    },
    {
      "epoch": 37.23296032553408,
      "grad_norm": 29.197172164916992,
      "learning_rate": 1.2767039674465922e-05,
      "loss": 1.6117,
      "step": 73200
    },
    {
      "epoch": 37.23804679552391,
      "grad_norm": 46.39872741699219,
      "learning_rate": 1.2761953204476094e-05,
      "loss": 1.6791,
      "step": 73210
    },
    {
      "epoch": 37.243133265513734,
      "grad_norm": 44.27215576171875,
      "learning_rate": 1.2756866734486267e-05,
      "loss": 1.5076,
      "step": 73220
    },
    {
      "epoch": 37.24821973550356,
      "grad_norm": 41.549652099609375,
      "learning_rate": 1.275178026449644e-05,
      "loss": 1.525,
      "step": 73230
    },
    {
      "epoch": 37.25330620549339,
      "grad_norm": 41.45900344848633,
      "learning_rate": 1.2746693794506612e-05,
      "loss": 1.7274,
      "step": 73240
    },
    {
      "epoch": 37.258392675483215,
      "grad_norm": 38.72400665283203,
      "learning_rate": 1.2741607324516785e-05,
      "loss": 1.5659,
      "step": 73250
    },
    {
      "epoch": 37.26347914547304,
      "grad_norm": 36.80459213256836,
      "learning_rate": 1.273652085452696e-05,
      "loss": 1.5304,
      "step": 73260
    },
    {
      "epoch": 37.26856561546287,
      "grad_norm": 48.139583587646484,
      "learning_rate": 1.2731434384537133e-05,
      "loss": 1.6598,
      "step": 73270
    },
    {
      "epoch": 37.273652085452696,
      "grad_norm": 42.88258361816406,
      "learning_rate": 1.2726347914547305e-05,
      "loss": 1.6288,
      "step": 73280
    },
    {
      "epoch": 37.27873855544252,
      "grad_norm": 40.99925994873047,
      "learning_rate": 1.2721261444557478e-05,
      "loss": 1.5689,
      "step": 73290
    },
    {
      "epoch": 37.28382502543235,
      "grad_norm": 44.754432678222656,
      "learning_rate": 1.2716174974567651e-05,
      "loss": 1.6326,
      "step": 73300
    },
    {
      "epoch": 37.28891149542218,
      "grad_norm": 40.740806579589844,
      "learning_rate": 1.2711088504577823e-05,
      "loss": 1.542,
      "step": 73310
    },
    {
      "epoch": 37.293997965412004,
      "grad_norm": 43.917057037353516,
      "learning_rate": 1.2706002034587996e-05,
      "loss": 1.562,
      "step": 73320
    },
    {
      "epoch": 37.29908443540183,
      "grad_norm": 38.2341194152832,
      "learning_rate": 1.270091556459817e-05,
      "loss": 1.638,
      "step": 73330
    },
    {
      "epoch": 37.30417090539166,
      "grad_norm": 38.446510314941406,
      "learning_rate": 1.2695829094608341e-05,
      "loss": 1.6777,
      "step": 73340
    },
    {
      "epoch": 37.309257375381485,
      "grad_norm": 37.61271286010742,
      "learning_rate": 1.2690742624618514e-05,
      "loss": 1.67,
      "step": 73350
    },
    {
      "epoch": 37.31434384537131,
      "grad_norm": 37.52482604980469,
      "learning_rate": 1.268565615462869e-05,
      "loss": 1.688,
      "step": 73360
    },
    {
      "epoch": 37.31943031536114,
      "grad_norm": 47.397315979003906,
      "learning_rate": 1.268056968463886e-05,
      "loss": 1.6313,
      "step": 73370
    },
    {
      "epoch": 37.324516785350966,
      "grad_norm": 44.22099304199219,
      "learning_rate": 1.2675483214649034e-05,
      "loss": 1.6001,
      "step": 73380
    },
    {
      "epoch": 37.32960325534079,
      "grad_norm": 35.89446258544922,
      "learning_rate": 1.2670396744659208e-05,
      "loss": 1.5769,
      "step": 73390
    },
    {
      "epoch": 37.33468972533062,
      "grad_norm": 41.601016998291016,
      "learning_rate": 1.266531027466938e-05,
      "loss": 1.6413,
      "step": 73400
    },
    {
      "epoch": 37.33977619532045,
      "grad_norm": 31.941089630126953,
      "learning_rate": 1.2660223804679552e-05,
      "loss": 1.5531,
      "step": 73410
    },
    {
      "epoch": 37.34486266531027,
      "grad_norm": 31.504413604736328,
      "learning_rate": 1.2655137334689726e-05,
      "loss": 1.6268,
      "step": 73420
    },
    {
      "epoch": 37.3499491353001,
      "grad_norm": 41.55568313598633,
      "learning_rate": 1.26500508646999e-05,
      "loss": 1.6506,
      "step": 73430
    },
    {
      "epoch": 37.35503560528993,
      "grad_norm": 39.58837890625,
      "learning_rate": 1.264496439471007e-05,
      "loss": 1.6332,
      "step": 73440
    },
    {
      "epoch": 37.360122075279754,
      "grad_norm": 41.548370361328125,
      "learning_rate": 1.2639877924720246e-05,
      "loss": 1.569,
      "step": 73450
    },
    {
      "epoch": 37.36520854526958,
      "grad_norm": 41.46062088012695,
      "learning_rate": 1.2634791454730419e-05,
      "loss": 1.5502,
      "step": 73460
    },
    {
      "epoch": 37.37029501525941,
      "grad_norm": 41.54507064819336,
      "learning_rate": 1.262970498474059e-05,
      "loss": 1.6411,
      "step": 73470
    },
    {
      "epoch": 37.375381485249235,
      "grad_norm": 46.57435607910156,
      "learning_rate": 1.2624618514750764e-05,
      "loss": 1.5415,
      "step": 73480
    },
    {
      "epoch": 37.38046795523906,
      "grad_norm": 42.32957077026367,
      "learning_rate": 1.2619532044760937e-05,
      "loss": 1.6629,
      "step": 73490
    },
    {
      "epoch": 37.38555442522889,
      "grad_norm": 33.35233688354492,
      "learning_rate": 1.2614445574771109e-05,
      "loss": 1.6421,
      "step": 73500
    },
    {
      "epoch": 37.390640895218716,
      "grad_norm": 44.24175262451172,
      "learning_rate": 1.2609359104781282e-05,
      "loss": 1.6315,
      "step": 73510
    },
    {
      "epoch": 37.39572736520854,
      "grad_norm": 45.37898635864258,
      "learning_rate": 1.2604272634791455e-05,
      "loss": 1.6284,
      "step": 73520
    },
    {
      "epoch": 37.40081383519837,
      "grad_norm": 38.18285369873047,
      "learning_rate": 1.259918616480163e-05,
      "loss": 1.561,
      "step": 73530
    },
    {
      "epoch": 37.4059003051882,
      "grad_norm": 35.10056686401367,
      "learning_rate": 1.25940996948118e-05,
      "loss": 1.6208,
      "step": 73540
    },
    {
      "epoch": 37.410986775178024,
      "grad_norm": 40.222068786621094,
      "learning_rate": 1.2589013224821975e-05,
      "loss": 1.5862,
      "step": 73550
    },
    {
      "epoch": 37.41607324516785,
      "grad_norm": 42.2701530456543,
      "learning_rate": 1.2583926754832148e-05,
      "loss": 1.6933,
      "step": 73560
    },
    {
      "epoch": 37.42115971515768,
      "grad_norm": 42.21824645996094,
      "learning_rate": 1.257884028484232e-05,
      "loss": 1.6191,
      "step": 73570
    },
    {
      "epoch": 37.426246185147505,
      "grad_norm": 44.226505279541016,
      "learning_rate": 1.2573753814852493e-05,
      "loss": 1.6266,
      "step": 73580
    },
    {
      "epoch": 37.43133265513733,
      "grad_norm": 29.99591636657715,
      "learning_rate": 1.2568667344862666e-05,
      "loss": 1.6067,
      "step": 73590
    },
    {
      "epoch": 37.43641912512716,
      "grad_norm": 38.47977066040039,
      "learning_rate": 1.2563580874872838e-05,
      "loss": 1.6622,
      "step": 73600
    },
    {
      "epoch": 37.441505595116986,
      "grad_norm": 41.44208908081055,
      "learning_rate": 1.2558494404883011e-05,
      "loss": 1.5832,
      "step": 73610
    },
    {
      "epoch": 37.44659206510681,
      "grad_norm": 45.18403244018555,
      "learning_rate": 1.2553407934893186e-05,
      "loss": 1.6005,
      "step": 73620
    },
    {
      "epoch": 37.45167853509664,
      "grad_norm": 32.19497299194336,
      "learning_rate": 1.2548321464903356e-05,
      "loss": 1.5956,
      "step": 73630
    },
    {
      "epoch": 37.45676500508647,
      "grad_norm": 46.34781265258789,
      "learning_rate": 1.2543234994913531e-05,
      "loss": 1.582,
      "step": 73640
    },
    {
      "epoch": 37.461851475076294,
      "grad_norm": 43.27733612060547,
      "learning_rate": 1.2538148524923704e-05,
      "loss": 1.6484,
      "step": 73650
    },
    {
      "epoch": 37.46693794506612,
      "grad_norm": 31.896242141723633,
      "learning_rate": 1.2533062054933878e-05,
      "loss": 1.5627,
      "step": 73660
    },
    {
      "epoch": 37.47202441505595,
      "grad_norm": 44.29815673828125,
      "learning_rate": 1.252797558494405e-05,
      "loss": 1.6512,
      "step": 73670
    },
    {
      "epoch": 37.477110885045775,
      "grad_norm": 39.872196197509766,
      "learning_rate": 1.2522889114954223e-05,
      "loss": 1.6662,
      "step": 73680
    },
    {
      "epoch": 37.4821973550356,
      "grad_norm": 44.32762145996094,
      "learning_rate": 1.2517802644964396e-05,
      "loss": 1.6494,
      "step": 73690
    },
    {
      "epoch": 37.48728382502543,
      "grad_norm": 38.86140823364258,
      "learning_rate": 1.2512716174974567e-05,
      "loss": 1.7365,
      "step": 73700
    },
    {
      "epoch": 37.492370295015256,
      "grad_norm": 35.040157318115234,
      "learning_rate": 1.250762970498474e-05,
      "loss": 1.6662,
      "step": 73710
    },
    {
      "epoch": 37.49745676500508,
      "grad_norm": 52.89335632324219,
      "learning_rate": 1.2502543234994916e-05,
      "loss": 1.6467,
      "step": 73720
    },
    {
      "epoch": 37.50254323499492,
      "grad_norm": 40.300453186035156,
      "learning_rate": 1.2497456765005087e-05,
      "loss": 1.6773,
      "step": 73730
    },
    {
      "epoch": 37.507629704984744,
      "grad_norm": 35.67185592651367,
      "learning_rate": 1.249237029501526e-05,
      "loss": 1.6123,
      "step": 73740
    },
    {
      "epoch": 37.51271617497457,
      "grad_norm": 48.04446029663086,
      "learning_rate": 1.2487283825025432e-05,
      "loss": 1.6505,
      "step": 73750
    },
    {
      "epoch": 37.5178026449644,
      "grad_norm": 42.370452880859375,
      "learning_rate": 1.2482197355035605e-05,
      "loss": 1.584,
      "step": 73760
    },
    {
      "epoch": 37.522889114954225,
      "grad_norm": 33.29470443725586,
      "learning_rate": 1.2477110885045779e-05,
      "loss": 1.5668,
      "step": 73770
    },
    {
      "epoch": 37.52797558494405,
      "grad_norm": 51.374935150146484,
      "learning_rate": 1.2472024415055952e-05,
      "loss": 1.7152,
      "step": 73780
    },
    {
      "epoch": 37.53306205493388,
      "grad_norm": 50.26485824584961,
      "learning_rate": 1.2466937945066125e-05,
      "loss": 1.6593,
      "step": 73790
    },
    {
      "epoch": 37.538148524923706,
      "grad_norm": 47.50307083129883,
      "learning_rate": 1.2461851475076297e-05,
      "loss": 1.6347,
      "step": 73800
    },
    {
      "epoch": 37.54323499491353,
      "grad_norm": 44.05206298828125,
      "learning_rate": 1.245676500508647e-05,
      "loss": 1.6038,
      "step": 73810
    },
    {
      "epoch": 37.54832146490336,
      "grad_norm": 41.60687255859375,
      "learning_rate": 1.2451678535096643e-05,
      "loss": 1.6083,
      "step": 73820
    },
    {
      "epoch": 37.55340793489319,
      "grad_norm": 31.740297317504883,
      "learning_rate": 1.2446592065106817e-05,
      "loss": 1.6828,
      "step": 73830
    },
    {
      "epoch": 37.558494404883014,
      "grad_norm": 42.201698303222656,
      "learning_rate": 1.244150559511699e-05,
      "loss": 1.6013,
      "step": 73840
    },
    {
      "epoch": 37.56358087487284,
      "grad_norm": 47.83748245239258,
      "learning_rate": 1.2436419125127162e-05,
      "loss": 1.5777,
      "step": 73850
    },
    {
      "epoch": 37.56866734486267,
      "grad_norm": 39.29426956176758,
      "learning_rate": 1.2431332655137337e-05,
      "loss": 1.6433,
      "step": 73860
    },
    {
      "epoch": 37.573753814852495,
      "grad_norm": 39.65970993041992,
      "learning_rate": 1.2426246185147508e-05,
      "loss": 1.6562,
      "step": 73870
    },
    {
      "epoch": 37.57884028484232,
      "grad_norm": 40.122406005859375,
      "learning_rate": 1.2421159715157681e-05,
      "loss": 1.6513,
      "step": 73880
    },
    {
      "epoch": 37.58392675483215,
      "grad_norm": 34.33361053466797,
      "learning_rate": 1.2416073245167855e-05,
      "loss": 1.6391,
      "step": 73890
    },
    {
      "epoch": 37.589013224821976,
      "grad_norm": 41.50886917114258,
      "learning_rate": 1.2410986775178026e-05,
      "loss": 1.7305,
      "step": 73900
    },
    {
      "epoch": 37.5940996948118,
      "grad_norm": 34.19529724121094,
      "learning_rate": 1.2405900305188201e-05,
      "loss": 1.6926,
      "step": 73910
    },
    {
      "epoch": 37.59918616480163,
      "grad_norm": 41.23286819458008,
      "learning_rate": 1.2400813835198373e-05,
      "loss": 1.5308,
      "step": 73920
    },
    {
      "epoch": 37.60427263479146,
      "grad_norm": 42.21871566772461,
      "learning_rate": 1.2395727365208546e-05,
      "loss": 1.6651,
      "step": 73930
    },
    {
      "epoch": 37.609359104781284,
      "grad_norm": 42.70059585571289,
      "learning_rate": 1.239064089521872e-05,
      "loss": 1.6229,
      "step": 73940
    },
    {
      "epoch": 37.61444557477111,
      "grad_norm": 35.219398498535156,
      "learning_rate": 1.2385554425228891e-05,
      "loss": 1.524,
      "step": 73950
    },
    {
      "epoch": 37.61953204476094,
      "grad_norm": 35.2756233215332,
      "learning_rate": 1.2380467955239064e-05,
      "loss": 1.5951,
      "step": 73960
    },
    {
      "epoch": 37.624618514750765,
      "grad_norm": 45.23139572143555,
      "learning_rate": 1.2375381485249238e-05,
      "loss": 1.6326,
      "step": 73970
    },
    {
      "epoch": 37.62970498474059,
      "grad_norm": 43.908935546875,
      "learning_rate": 1.237029501525941e-05,
      "loss": 1.5429,
      "step": 73980
    },
    {
      "epoch": 37.63479145473042,
      "grad_norm": 38.898101806640625,
      "learning_rate": 1.2365208545269584e-05,
      "loss": 1.6146,
      "step": 73990
    },
    {
      "epoch": 37.639877924720246,
      "grad_norm": 41.4384880065918,
      "learning_rate": 1.2360122075279756e-05,
      "loss": 1.6539,
      "step": 74000
    },
    {
      "epoch": 37.64496439471007,
      "grad_norm": 38.3634033203125,
      "learning_rate": 1.2355035605289929e-05,
      "loss": 1.673,
      "step": 74010
    },
    {
      "epoch": 37.6500508646999,
      "grad_norm": 48.28666305541992,
      "learning_rate": 1.2349949135300102e-05,
      "loss": 1.6899,
      "step": 74020
    },
    {
      "epoch": 37.65513733468973,
      "grad_norm": 34.349945068359375,
      "learning_rate": 1.2344862665310276e-05,
      "loss": 1.6701,
      "step": 74030
    },
    {
      "epoch": 37.66022380467955,
      "grad_norm": 40.36454772949219,
      "learning_rate": 1.2339776195320449e-05,
      "loss": 1.6879,
      "step": 74040
    },
    {
      "epoch": 37.66531027466938,
      "grad_norm": 41.225669860839844,
      "learning_rate": 1.233468972533062e-05,
      "loss": 1.6223,
      "step": 74050
    },
    {
      "epoch": 37.67039674465921,
      "grad_norm": 40.72791290283203,
      "learning_rate": 1.2329603255340794e-05,
      "loss": 1.6275,
      "step": 74060
    },
    {
      "epoch": 37.675483214649034,
      "grad_norm": 50.826107025146484,
      "learning_rate": 1.2324516785350967e-05,
      "loss": 1.6462,
      "step": 74070
    },
    {
      "epoch": 37.68056968463886,
      "grad_norm": 36.193092346191406,
      "learning_rate": 1.231943031536114e-05,
      "loss": 1.566,
      "step": 74080
    },
    {
      "epoch": 37.68565615462869,
      "grad_norm": 37.94556427001953,
      "learning_rate": 1.2314343845371312e-05,
      "loss": 1.7127,
      "step": 74090
    },
    {
      "epoch": 37.690742624618515,
      "grad_norm": 40.60776901245117,
      "learning_rate": 1.2309257375381487e-05,
      "loss": 1.6893,
      "step": 74100
    },
    {
      "epoch": 37.69582909460834,
      "grad_norm": 39.73081970214844,
      "learning_rate": 1.2304170905391658e-05,
      "loss": 1.5732,
      "step": 74110
    },
    {
      "epoch": 37.70091556459817,
      "grad_norm": 42.3757209777832,
      "learning_rate": 1.2299084435401832e-05,
      "loss": 1.6659,
      "step": 74120
    },
    {
      "epoch": 37.706002034587996,
      "grad_norm": 42.39674377441406,
      "learning_rate": 1.2293997965412005e-05,
      "loss": 1.689,
      "step": 74130
    },
    {
      "epoch": 37.71108850457782,
      "grad_norm": 45.865108489990234,
      "learning_rate": 1.2288911495422177e-05,
      "loss": 1.697,
      "step": 74140
    },
    {
      "epoch": 37.71617497456765,
      "grad_norm": 35.3874397277832,
      "learning_rate": 1.2283825025432352e-05,
      "loss": 1.6447,
      "step": 74150
    },
    {
      "epoch": 37.72126144455748,
      "grad_norm": 47.42862319946289,
      "learning_rate": 1.2278738555442523e-05,
      "loss": 1.5878,
      "step": 74160
    },
    {
      "epoch": 37.726347914547304,
      "grad_norm": 37.91910171508789,
      "learning_rate": 1.2273652085452696e-05,
      "loss": 1.6839,
      "step": 74170
    },
    {
      "epoch": 37.73143438453713,
      "grad_norm": 41.117855072021484,
      "learning_rate": 1.226856561546287e-05,
      "loss": 1.5923,
      "step": 74180
    },
    {
      "epoch": 37.73652085452696,
      "grad_norm": 40.63595962524414,
      "learning_rate": 1.2263479145473041e-05,
      "loss": 1.6124,
      "step": 74190
    },
    {
      "epoch": 37.741607324516785,
      "grad_norm": 34.45851516723633,
      "learning_rate": 1.2258392675483216e-05,
      "loss": 1.65,
      "step": 74200
    },
    {
      "epoch": 37.74669379450661,
      "grad_norm": 36.53782272338867,
      "learning_rate": 1.2253306205493388e-05,
      "loss": 1.6492,
      "step": 74210
    },
    {
      "epoch": 37.75178026449644,
      "grad_norm": 48.08311080932617,
      "learning_rate": 1.2248219735503561e-05,
      "loss": 1.6269,
      "step": 74220
    },
    {
      "epoch": 37.756866734486266,
      "grad_norm": 32.70152282714844,
      "learning_rate": 1.2243133265513734e-05,
      "loss": 1.5828,
      "step": 74230
    },
    {
      "epoch": 37.76195320447609,
      "grad_norm": 43.33201217651367,
      "learning_rate": 1.2238046795523906e-05,
      "loss": 1.631,
      "step": 74240
    },
    {
      "epoch": 37.76703967446592,
      "grad_norm": 39.966796875,
      "learning_rate": 1.2232960325534081e-05,
      "loss": 1.5328,
      "step": 74250
    },
    {
      "epoch": 37.77212614445575,
      "grad_norm": 36.500328063964844,
      "learning_rate": 1.2227873855544253e-05,
      "loss": 1.6885,
      "step": 74260
    },
    {
      "epoch": 37.777212614445574,
      "grad_norm": 36.372493743896484,
      "learning_rate": 1.2222787385554426e-05,
      "loss": 1.5858,
      "step": 74270
    },
    {
      "epoch": 37.7822990844354,
      "grad_norm": 59.17725372314453,
      "learning_rate": 1.2217700915564599e-05,
      "loss": 1.6078,
      "step": 74280
    },
    {
      "epoch": 37.78738555442523,
      "grad_norm": 33.303855895996094,
      "learning_rate": 1.221261444557477e-05,
      "loss": 1.6299,
      "step": 74290
    },
    {
      "epoch": 37.792472024415055,
      "grad_norm": 36.747657775878906,
      "learning_rate": 1.2207527975584946e-05,
      "loss": 1.6694,
      "step": 74300
    },
    {
      "epoch": 37.79755849440488,
      "grad_norm": 46.3707160949707,
      "learning_rate": 1.2202441505595117e-05,
      "loss": 1.6244,
      "step": 74310
    },
    {
      "epoch": 37.80264496439471,
      "grad_norm": 43.98405075073242,
      "learning_rate": 1.219735503560529e-05,
      "loss": 1.6189,
      "step": 74320
    },
    {
      "epoch": 37.807731434384536,
      "grad_norm": 46.133846282958984,
      "learning_rate": 1.2192268565615464e-05,
      "loss": 1.5849,
      "step": 74330
    },
    {
      "epoch": 37.81281790437436,
      "grad_norm": 35.221282958984375,
      "learning_rate": 1.2187182095625637e-05,
      "loss": 1.6605,
      "step": 74340
    },
    {
      "epoch": 37.81790437436419,
      "grad_norm": 43.033203125,
      "learning_rate": 1.2182095625635809e-05,
      "loss": 1.6407,
      "step": 74350
    },
    {
      "epoch": 37.82299084435402,
      "grad_norm": 47.818763732910156,
      "learning_rate": 1.2177009155645982e-05,
      "loss": 1.5815,
      "step": 74360
    },
    {
      "epoch": 37.828077314343844,
      "grad_norm": 37.899723052978516,
      "learning_rate": 1.2171922685656155e-05,
      "loss": 1.5739,
      "step": 74370
    },
    {
      "epoch": 37.83316378433367,
      "grad_norm": 40.9927864074707,
      "learning_rate": 1.2166836215666329e-05,
      "loss": 1.5856,
      "step": 74380
    },
    {
      "epoch": 37.8382502543235,
      "grad_norm": 53.7661018371582,
      "learning_rate": 1.2161749745676502e-05,
      "loss": 1.6707,
      "step": 74390
    },
    {
      "epoch": 37.843336724313325,
      "grad_norm": 44.149375915527344,
      "learning_rate": 1.2156663275686673e-05,
      "loss": 1.5585,
      "step": 74400
    },
    {
      "epoch": 37.84842319430315,
      "grad_norm": 51.086727142333984,
      "learning_rate": 1.2151576805696847e-05,
      "loss": 1.6645,
      "step": 74410
    },
    {
      "epoch": 37.85350966429298,
      "grad_norm": 52.99188995361328,
      "learning_rate": 1.214649033570702e-05,
      "loss": 1.6751,
      "step": 74420
    },
    {
      "epoch": 37.858596134282806,
      "grad_norm": 45.49179458618164,
      "learning_rate": 1.2141403865717192e-05,
      "loss": 1.626,
      "step": 74430
    },
    {
      "epoch": 37.86368260427263,
      "grad_norm": 33.027339935302734,
      "learning_rate": 1.2136317395727367e-05,
      "loss": 1.7041,
      "step": 74440
    },
    {
      "epoch": 37.86876907426246,
      "grad_norm": 39.407588958740234,
      "learning_rate": 1.2131230925737538e-05,
      "loss": 1.6164,
      "step": 74450
    },
    {
      "epoch": 37.87385554425229,
      "grad_norm": 50.325008392333984,
      "learning_rate": 1.2126144455747711e-05,
      "loss": 1.6135,
      "step": 74460
    },
    {
      "epoch": 37.878942014242114,
      "grad_norm": 44.31383514404297,
      "learning_rate": 1.2121057985757885e-05,
      "loss": 1.641,
      "step": 74470
    },
    {
      "epoch": 37.88402848423194,
      "grad_norm": 41.33689498901367,
      "learning_rate": 1.2115971515768056e-05,
      "loss": 1.6308,
      "step": 74480
    },
    {
      "epoch": 37.88911495422177,
      "grad_norm": 37.93122863769531,
      "learning_rate": 1.2110885045778231e-05,
      "loss": 1.5809,
      "step": 74490
    },
    {
      "epoch": 37.894201424211595,
      "grad_norm": 44.756317138671875,
      "learning_rate": 1.2105798575788403e-05,
      "loss": 1.5888,
      "step": 74500
    },
    {
      "epoch": 37.89928789420142,
      "grad_norm": 35.84611129760742,
      "learning_rate": 1.2100712105798578e-05,
      "loss": 1.5341,
      "step": 74510
    },
    {
      "epoch": 37.90437436419125,
      "grad_norm": 46.35436248779297,
      "learning_rate": 1.209562563580875e-05,
      "loss": 1.6399,
      "step": 74520
    },
    {
      "epoch": 37.909460834181075,
      "grad_norm": 43.866188049316406,
      "learning_rate": 1.2090539165818923e-05,
      "loss": 1.611,
      "step": 74530
    },
    {
      "epoch": 37.9145473041709,
      "grad_norm": 33.76785659790039,
      "learning_rate": 1.2085452695829096e-05,
      "loss": 1.6791,
      "step": 74540
    },
    {
      "epoch": 37.91963377416073,
      "grad_norm": 38.022396087646484,
      "learning_rate": 1.2080366225839268e-05,
      "loss": 1.5762,
      "step": 74550
    },
    {
      "epoch": 37.924720244150556,
      "grad_norm": 37.9489631652832,
      "learning_rate": 1.207527975584944e-05,
      "loss": 1.5861,
      "step": 74560
    },
    {
      "epoch": 37.92980671414038,
      "grad_norm": 42.342288970947266,
      "learning_rate": 1.2070193285859614e-05,
      "loss": 1.5437,
      "step": 74570
    },
    {
      "epoch": 37.93489318413021,
      "grad_norm": 35.2487907409668,
      "learning_rate": 1.2065106815869787e-05,
      "loss": 1.7443,
      "step": 74580
    },
    {
      "epoch": 37.93997965412004,
      "grad_norm": 42.164833068847656,
      "learning_rate": 1.206002034587996e-05,
      "loss": 1.6066,
      "step": 74590
    },
    {
      "epoch": 37.945066124109864,
      "grad_norm": 48.99155044555664,
      "learning_rate": 1.2054933875890132e-05,
      "loss": 1.5428,
      "step": 74600
    },
    {
      "epoch": 37.95015259409969,
      "grad_norm": 39.18290710449219,
      "learning_rate": 1.2049847405900306e-05,
      "loss": 1.6173,
      "step": 74610
    },
    {
      "epoch": 37.955239064089525,
      "grad_norm": 47.88172149658203,
      "learning_rate": 1.2044760935910479e-05,
      "loss": 1.5688,
      "step": 74620
    },
    {
      "epoch": 37.96032553407935,
      "grad_norm": 39.024471282958984,
      "learning_rate": 1.2039674465920652e-05,
      "loss": 1.6118,
      "step": 74630
    },
    {
      "epoch": 37.96541200406918,
      "grad_norm": 39.562923431396484,
      "learning_rate": 1.2034587995930825e-05,
      "loss": 1.5439,
      "step": 74640
    },
    {
      "epoch": 37.970498474059006,
      "grad_norm": 39.37109375,
      "learning_rate": 1.2029501525940997e-05,
      "loss": 1.6312,
      "step": 74650
    },
    {
      "epoch": 37.97558494404883,
      "grad_norm": 34.805213928222656,
      "learning_rate": 1.202441505595117e-05,
      "loss": 1.6073,
      "step": 74660
    },
    {
      "epoch": 37.98067141403866,
      "grad_norm": 48.062442779541016,
      "learning_rate": 1.2019328585961344e-05,
      "loss": 1.6792,
      "step": 74670
    },
    {
      "epoch": 37.98575788402849,
      "grad_norm": 40.44149398803711,
      "learning_rate": 1.2014242115971517e-05,
      "loss": 1.641,
      "step": 74680
    },
    {
      "epoch": 37.990844354018314,
      "grad_norm": 34.336063385009766,
      "learning_rate": 1.2009155645981688e-05,
      "loss": 1.6003,
      "step": 74690
    },
    {
      "epoch": 37.99593082400814,
      "grad_norm": 38.635520935058594,
      "learning_rate": 1.2004069175991862e-05,
      "loss": 1.5984,
      "step": 74700
    },
    {
      "epoch": 38.0,
      "eval_loss": 4.966403961181641,
      "eval_runtime": 2.7741,
      "eval_samples_per_second": 1000.321,
      "eval_steps_per_second": 125.085,
      "step": 74708
    },
    {
      "epoch": 38.00101729399797,
      "grad_norm": 36.46537399291992,
      "learning_rate": 1.1998982706002035e-05,
      "loss": 1.5983,
      "step": 74710
    },
    {
      "epoch": 38.006103763987795,
      "grad_norm": 35.22541046142578,
      "learning_rate": 1.1993896236012208e-05,
      "loss": 1.6905,
      "step": 74720
    },
    {
      "epoch": 38.01119023397762,
      "grad_norm": 41.689762115478516,
      "learning_rate": 1.1988809766022382e-05,
      "loss": 1.5309,
      "step": 74730
    },
    {
      "epoch": 38.01627670396745,
      "grad_norm": 33.28923416137695,
      "learning_rate": 1.1983723296032553e-05,
      "loss": 1.7012,
      "step": 74740
    },
    {
      "epoch": 38.021363173957276,
      "grad_norm": 52.32869338989258,
      "learning_rate": 1.1978636826042728e-05,
      "loss": 1.6533,
      "step": 74750
    },
    {
      "epoch": 38.0264496439471,
      "grad_norm": 46.01689910888672,
      "learning_rate": 1.19735503560529e-05,
      "loss": 1.5509,
      "step": 74760
    },
    {
      "epoch": 38.03153611393693,
      "grad_norm": 41.2628059387207,
      "learning_rate": 1.1968463886063073e-05,
      "loss": 1.6519,
      "step": 74770
    },
    {
      "epoch": 38.03662258392676,
      "grad_norm": 39.26573944091797,
      "learning_rate": 1.1963377416073246e-05,
      "loss": 1.5936,
      "step": 74780
    },
    {
      "epoch": 38.041709053916584,
      "grad_norm": 38.662498474121094,
      "learning_rate": 1.1958290946083418e-05,
      "loss": 1.6091,
      "step": 74790
    },
    {
      "epoch": 38.04679552390641,
      "grad_norm": 32.7969856262207,
      "learning_rate": 1.1953204476093593e-05,
      "loss": 1.639,
      "step": 74800
    },
    {
      "epoch": 38.05188199389624,
      "grad_norm": 34.65224838256836,
      "learning_rate": 1.1948118006103764e-05,
      "loss": 1.6128,
      "step": 74810
    },
    {
      "epoch": 38.056968463886065,
      "grad_norm": 37.634307861328125,
      "learning_rate": 1.1943031536113938e-05,
      "loss": 1.585,
      "step": 74820
    },
    {
      "epoch": 38.06205493387589,
      "grad_norm": 49.33979034423828,
      "learning_rate": 1.1937945066124111e-05,
      "loss": 1.5813,
      "step": 74830
    },
    {
      "epoch": 38.06714140386572,
      "grad_norm": 47.907527923583984,
      "learning_rate": 1.1932858596134283e-05,
      "loss": 1.5516,
      "step": 74840
    },
    {
      "epoch": 38.072227873855546,
      "grad_norm": 43.5344352722168,
      "learning_rate": 1.1927772126144458e-05,
      "loss": 1.5021,
      "step": 74850
    },
    {
      "epoch": 38.07731434384537,
      "grad_norm": 33.01184844970703,
      "learning_rate": 1.1922685656154629e-05,
      "loss": 1.6662,
      "step": 74860
    },
    {
      "epoch": 38.0824008138352,
      "grad_norm": 43.02469253540039,
      "learning_rate": 1.1917599186164802e-05,
      "loss": 1.6509,
      "step": 74870
    },
    {
      "epoch": 38.08748728382503,
      "grad_norm": 50.135738372802734,
      "learning_rate": 1.1912512716174976e-05,
      "loss": 1.5766,
      "step": 74880
    },
    {
      "epoch": 38.092573753814854,
      "grad_norm": 39.18230056762695,
      "learning_rate": 1.1907426246185147e-05,
      "loss": 1.5809,
      "step": 74890
    },
    {
      "epoch": 38.09766022380468,
      "grad_norm": 41.22932815551758,
      "learning_rate": 1.190233977619532e-05,
      "loss": 1.6256,
      "step": 74900
    },
    {
      "epoch": 38.10274669379451,
      "grad_norm": 36.87725830078125,
      "learning_rate": 1.1897253306205494e-05,
      "loss": 1.6716,
      "step": 74910
    },
    {
      "epoch": 38.107833163784335,
      "grad_norm": 36.440128326416016,
      "learning_rate": 1.1892166836215667e-05,
      "loss": 1.567,
      "step": 74920
    },
    {
      "epoch": 38.11291963377416,
      "grad_norm": 43.071502685546875,
      "learning_rate": 1.188708036622584e-05,
      "loss": 1.5926,
      "step": 74930
    },
    {
      "epoch": 38.11800610376399,
      "grad_norm": 35.18429183959961,
      "learning_rate": 1.1881993896236012e-05,
      "loss": 1.6943,
      "step": 74940
    },
    {
      "epoch": 38.123092573753816,
      "grad_norm": 55.04594802856445,
      "learning_rate": 1.1876907426246185e-05,
      "loss": 1.5366,
      "step": 74950
    },
    {
      "epoch": 38.12817904374364,
      "grad_norm": 36.38602828979492,
      "learning_rate": 1.1871820956256359e-05,
      "loss": 1.5914,
      "step": 74960
    },
    {
      "epoch": 38.13326551373347,
      "grad_norm": 38.727821350097656,
      "learning_rate": 1.1866734486266532e-05,
      "loss": 1.6127,
      "step": 74970
    },
    {
      "epoch": 38.1383519837233,
      "grad_norm": 43.65107727050781,
      "learning_rate": 1.1861648016276705e-05,
      "loss": 1.6936,
      "step": 74980
    },
    {
      "epoch": 38.143438453713124,
      "grad_norm": 34.4246711730957,
      "learning_rate": 1.1856561546286878e-05,
      "loss": 1.6241,
      "step": 74990
    },
    {
      "epoch": 38.14852492370295,
      "grad_norm": 37.65354919433594,
      "learning_rate": 1.185147507629705e-05,
      "loss": 1.6198,
      "step": 75000
    },
    {
      "epoch": 38.15361139369278,
      "grad_norm": 41.884490966796875,
      "learning_rate": 1.1846388606307223e-05,
      "loss": 1.5931,
      "step": 75010
    },
    {
      "epoch": 38.158697863682605,
      "grad_norm": 42.84629440307617,
      "learning_rate": 1.1841302136317397e-05,
      "loss": 1.6524,
      "step": 75020
    },
    {
      "epoch": 38.16378433367243,
      "grad_norm": 43.761512756347656,
      "learning_rate": 1.1836215666327568e-05,
      "loss": 1.6471,
      "step": 75030
    },
    {
      "epoch": 38.16887080366226,
      "grad_norm": 40.460060119628906,
      "learning_rate": 1.1831129196337743e-05,
      "loss": 1.6187,
      "step": 75040
    },
    {
      "epoch": 38.173957273652086,
      "grad_norm": 36.71621322631836,
      "learning_rate": 1.1826042726347915e-05,
      "loss": 1.5902,
      "step": 75050
    },
    {
      "epoch": 38.17904374364191,
      "grad_norm": 33.47703552246094,
      "learning_rate": 1.1820956256358088e-05,
      "loss": 1.7459,
      "step": 75060
    },
    {
      "epoch": 38.18413021363174,
      "grad_norm": 42.08603286743164,
      "learning_rate": 1.1815869786368261e-05,
      "loss": 1.5972,
      "step": 75070
    },
    {
      "epoch": 38.18921668362157,
      "grad_norm": 42.132320404052734,
      "learning_rate": 1.1810783316378433e-05,
      "loss": 1.6521,
      "step": 75080
    },
    {
      "epoch": 38.19430315361139,
      "grad_norm": 32.452205657958984,
      "learning_rate": 1.1805696846388608e-05,
      "loss": 1.6642,
      "step": 75090
    },
    {
      "epoch": 38.19938962360122,
      "grad_norm": 44.79433822631836,
      "learning_rate": 1.180061037639878e-05,
      "loss": 1.5424,
      "step": 75100
    },
    {
      "epoch": 38.20447609359105,
      "grad_norm": 35.05586242675781,
      "learning_rate": 1.1795523906408953e-05,
      "loss": 1.5835,
      "step": 75110
    },
    {
      "epoch": 38.209562563580874,
      "grad_norm": 38.9759521484375,
      "learning_rate": 1.1790437436419126e-05,
      "loss": 1.6865,
      "step": 75120
    },
    {
      "epoch": 38.2146490335707,
      "grad_norm": 43.831642150878906,
      "learning_rate": 1.1785350966429298e-05,
      "loss": 1.5761,
      "step": 75130
    },
    {
      "epoch": 38.21973550356053,
      "grad_norm": 39.820377349853516,
      "learning_rate": 1.1780264496439473e-05,
      "loss": 1.5736,
      "step": 75140
    },
    {
      "epoch": 38.224821973550355,
      "grad_norm": 36.78472137451172,
      "learning_rate": 1.1775178026449644e-05,
      "loss": 1.5899,
      "step": 75150
    },
    {
      "epoch": 38.22990844354018,
      "grad_norm": 43.3702278137207,
      "learning_rate": 1.1770091556459817e-05,
      "loss": 1.6211,
      "step": 75160
    },
    {
      "epoch": 38.23499491353001,
      "grad_norm": 39.31529998779297,
      "learning_rate": 1.176500508646999e-05,
      "loss": 1.5651,
      "step": 75170
    },
    {
      "epoch": 38.240081383519836,
      "grad_norm": 46.30925369262695,
      "learning_rate": 1.1759918616480162e-05,
      "loss": 1.5941,
      "step": 75180
    },
    {
      "epoch": 38.24516785350966,
      "grad_norm": 38.037227630615234,
      "learning_rate": 1.1754832146490337e-05,
      "loss": 1.5927,
      "step": 75190
    },
    {
      "epoch": 38.25025432349949,
      "grad_norm": 35.727455139160156,
      "learning_rate": 1.1749745676500509e-05,
      "loss": 1.6231,
      "step": 75200
    },
    {
      "epoch": 38.25534079348932,
      "grad_norm": 35.44413375854492,
      "learning_rate": 1.1744659206510682e-05,
      "loss": 1.576,
      "step": 75210
    },
    {
      "epoch": 38.260427263479144,
      "grad_norm": 42.24225616455078,
      "learning_rate": 1.1739572736520855e-05,
      "loss": 1.6314,
      "step": 75220
    },
    {
      "epoch": 38.26551373346897,
      "grad_norm": 72.87013244628906,
      "learning_rate": 1.1734486266531029e-05,
      "loss": 1.6123,
      "step": 75230
    },
    {
      "epoch": 38.2706002034588,
      "grad_norm": 34.94051742553711,
      "learning_rate": 1.17293997965412e-05,
      "loss": 1.596,
      "step": 75240
    },
    {
      "epoch": 38.275686673448625,
      "grad_norm": 37.89372253417969,
      "learning_rate": 1.1724313326551374e-05,
      "loss": 1.6918,
      "step": 75250
    },
    {
      "epoch": 38.28077314343845,
      "grad_norm": 46.306488037109375,
      "learning_rate": 1.1719226856561547e-05,
      "loss": 1.5231,
      "step": 75260
    },
    {
      "epoch": 38.28585961342828,
      "grad_norm": 41.632904052734375,
      "learning_rate": 1.171414038657172e-05,
      "loss": 1.5687,
      "step": 75270
    },
    {
      "epoch": 38.290946083418106,
      "grad_norm": 37.0986328125,
      "learning_rate": 1.1709053916581893e-05,
      "loss": 1.5827,
      "step": 75280
    },
    {
      "epoch": 38.29603255340793,
      "grad_norm": 36.45403289794922,
      "learning_rate": 1.1703967446592065e-05,
      "loss": 1.4948,
      "step": 75290
    },
    {
      "epoch": 38.30111902339776,
      "grad_norm": 46.35652160644531,
      "learning_rate": 1.1698880976602238e-05,
      "loss": 1.5465,
      "step": 75300
    },
    {
      "epoch": 38.30620549338759,
      "grad_norm": 38.2411994934082,
      "learning_rate": 1.1693794506612412e-05,
      "loss": 1.5668,
      "step": 75310
    },
    {
      "epoch": 38.311291963377414,
      "grad_norm": 36.30205535888672,
      "learning_rate": 1.1688708036622585e-05,
      "loss": 1.5834,
      "step": 75320
    },
    {
      "epoch": 38.31637843336724,
      "grad_norm": 39.1345100402832,
      "learning_rate": 1.1683621566632758e-05,
      "loss": 1.5382,
      "step": 75330
    },
    {
      "epoch": 38.32146490335707,
      "grad_norm": 47.005550384521484,
      "learning_rate": 1.167853509664293e-05,
      "loss": 1.6114,
      "step": 75340
    },
    {
      "epoch": 38.326551373346895,
      "grad_norm": 38.00993728637695,
      "learning_rate": 1.1673448626653103e-05,
      "loss": 1.6755,
      "step": 75350
    },
    {
      "epoch": 38.33163784333672,
      "grad_norm": 54.68378829956055,
      "learning_rate": 1.1668362156663276e-05,
      "loss": 1.637,
      "step": 75360
    },
    {
      "epoch": 38.33672431332655,
      "grad_norm": 32.07414627075195,
      "learning_rate": 1.1663275686673448e-05,
      "loss": 1.6485,
      "step": 75370
    },
    {
      "epoch": 38.341810783316376,
      "grad_norm": 41.54826736450195,
      "learning_rate": 1.1658189216683623e-05,
      "loss": 1.7178,
      "step": 75380
    },
    {
      "epoch": 38.3468972533062,
      "grad_norm": 33.88151550292969,
      "learning_rate": 1.1653102746693794e-05,
      "loss": 1.6948,
      "step": 75390
    },
    {
      "epoch": 38.35198372329603,
      "grad_norm": 42.15250778198242,
      "learning_rate": 1.164801627670397e-05,
      "loss": 1.681,
      "step": 75400
    },
    {
      "epoch": 38.35707019328586,
      "grad_norm": 38.24174118041992,
      "learning_rate": 1.1642929806714141e-05,
      "loss": 1.6868,
      "step": 75410
    },
    {
      "epoch": 38.362156663275684,
      "grad_norm": 35.20383071899414,
      "learning_rate": 1.1637843336724313e-05,
      "loss": 1.654,
      "step": 75420
    },
    {
      "epoch": 38.36724313326551,
      "grad_norm": 41.35604476928711,
      "learning_rate": 1.1632756866734488e-05,
      "loss": 1.6301,
      "step": 75430
    },
    {
      "epoch": 38.37232960325534,
      "grad_norm": 41.148719787597656,
      "learning_rate": 1.1627670396744659e-05,
      "loss": 1.6016,
      "step": 75440
    },
    {
      "epoch": 38.377416073245165,
      "grad_norm": 38.20328140258789,
      "learning_rate": 1.1622583926754834e-05,
      "loss": 1.6378,
      "step": 75450
    },
    {
      "epoch": 38.38250254323499,
      "grad_norm": 37.5035514831543,
      "learning_rate": 1.1617497456765006e-05,
      "loss": 1.5261,
      "step": 75460
    },
    {
      "epoch": 38.38758901322482,
      "grad_norm": 45.197200775146484,
      "learning_rate": 1.1612410986775179e-05,
      "loss": 1.7189,
      "step": 75470
    },
    {
      "epoch": 38.392675483214646,
      "grad_norm": 47.80266189575195,
      "learning_rate": 1.1607324516785352e-05,
      "loss": 1.5864,
      "step": 75480
    },
    {
      "epoch": 38.39776195320447,
      "grad_norm": 36.60590744018555,
      "learning_rate": 1.1602238046795524e-05,
      "loss": 1.5695,
      "step": 75490
    },
    {
      "epoch": 38.4028484231943,
      "grad_norm": 40.6974983215332,
      "learning_rate": 1.1597151576805697e-05,
      "loss": 1.6168,
      "step": 75500
    },
    {
      "epoch": 38.407934893184134,
      "grad_norm": 49.51627731323242,
      "learning_rate": 1.159206510681587e-05,
      "loss": 1.6534,
      "step": 75510
    },
    {
      "epoch": 38.41302136317396,
      "grad_norm": 35.945335388183594,
      "learning_rate": 1.1586978636826044e-05,
      "loss": 1.6374,
      "step": 75520
    },
    {
      "epoch": 38.41810783316379,
      "grad_norm": 36.514739990234375,
      "learning_rate": 1.1581892166836217e-05,
      "loss": 1.5625,
      "step": 75530
    },
    {
      "epoch": 38.423194303153615,
      "grad_norm": 36.432518005371094,
      "learning_rate": 1.1576805696846389e-05,
      "loss": 1.5334,
      "step": 75540
    },
    {
      "epoch": 38.42828077314344,
      "grad_norm": 36.877838134765625,
      "learning_rate": 1.1571719226856562e-05,
      "loss": 1.5866,
      "step": 75550
    },
    {
      "epoch": 38.43336724313327,
      "grad_norm": 41.1168212890625,
      "learning_rate": 1.1566632756866735e-05,
      "loss": 1.5614,
      "step": 75560
    },
    {
      "epoch": 38.438453713123096,
      "grad_norm": 41.66416549682617,
      "learning_rate": 1.1561546286876908e-05,
      "loss": 1.6119,
      "step": 75570
    },
    {
      "epoch": 38.44354018311292,
      "grad_norm": 50.45920181274414,
      "learning_rate": 1.155645981688708e-05,
      "loss": 1.5649,
      "step": 75580
    },
    {
      "epoch": 38.44862665310275,
      "grad_norm": 47.090335845947266,
      "learning_rate": 1.1551373346897253e-05,
      "loss": 1.6403,
      "step": 75590
    },
    {
      "epoch": 38.45371312309258,
      "grad_norm": 39.988468170166016,
      "learning_rate": 1.1546286876907427e-05,
      "loss": 1.6162,
      "step": 75600
    },
    {
      "epoch": 38.458799593082404,
      "grad_norm": 34.88469314575195,
      "learning_rate": 1.15412004069176e-05,
      "loss": 1.5563,
      "step": 75610
    },
    {
      "epoch": 38.46388606307223,
      "grad_norm": 40.55098342895508,
      "learning_rate": 1.1536113936927773e-05,
      "loss": 1.5774,
      "step": 75620
    },
    {
      "epoch": 38.46897253306206,
      "grad_norm": 45.17523956298828,
      "learning_rate": 1.1531027466937945e-05,
      "loss": 1.577,
      "step": 75630
    },
    {
      "epoch": 38.474059003051885,
      "grad_norm": 35.75102615356445,
      "learning_rate": 1.152594099694812e-05,
      "loss": 1.6348,
      "step": 75640
    },
    {
      "epoch": 38.47914547304171,
      "grad_norm": 50.631141662597656,
      "learning_rate": 1.1520854526958291e-05,
      "loss": 1.7015,
      "step": 75650
    },
    {
      "epoch": 38.48423194303154,
      "grad_norm": 45.162166595458984,
      "learning_rate": 1.1515768056968465e-05,
      "loss": 1.5807,
      "step": 75660
    },
    {
      "epoch": 38.489318413021365,
      "grad_norm": 39.215145111083984,
      "learning_rate": 1.1510681586978638e-05,
      "loss": 1.6783,
      "step": 75670
    },
    {
      "epoch": 38.49440488301119,
      "grad_norm": 36.67285919189453,
      "learning_rate": 1.150559511698881e-05,
      "loss": 1.6161,
      "step": 75680
    },
    {
      "epoch": 38.49949135300102,
      "grad_norm": 34.61448669433594,
      "learning_rate": 1.1500508646998984e-05,
      "loss": 1.6087,
      "step": 75690
    },
    {
      "epoch": 38.504577822990846,
      "grad_norm": 37.098876953125,
      "learning_rate": 1.1495422177009156e-05,
      "loss": 1.6351,
      "step": 75700
    },
    {
      "epoch": 38.50966429298067,
      "grad_norm": 46.92292404174805,
      "learning_rate": 1.149033570701933e-05,
      "loss": 1.5949,
      "step": 75710
    },
    {
      "epoch": 38.5147507629705,
      "grad_norm": 46.80281066894531,
      "learning_rate": 1.1485249237029503e-05,
      "loss": 1.6419,
      "step": 75720
    },
    {
      "epoch": 38.51983723296033,
      "grad_norm": 38.83180618286133,
      "learning_rate": 1.1480162767039674e-05,
      "loss": 1.5916,
      "step": 75730
    },
    {
      "epoch": 38.524923702950154,
      "grad_norm": 34.144508361816406,
      "learning_rate": 1.1475076297049849e-05,
      "loss": 1.6266,
      "step": 75740
    },
    {
      "epoch": 38.53001017293998,
      "grad_norm": 33.05010223388672,
      "learning_rate": 1.146998982706002e-05,
      "loss": 1.647,
      "step": 75750
    },
    {
      "epoch": 38.53509664292981,
      "grad_norm": 57.69636535644531,
      "learning_rate": 1.1464903357070194e-05,
      "loss": 1.7136,
      "step": 75760
    },
    {
      "epoch": 38.540183112919635,
      "grad_norm": 36.30020523071289,
      "learning_rate": 1.1459816887080367e-05,
      "loss": 1.6145,
      "step": 75770
    },
    {
      "epoch": 38.54526958290946,
      "grad_norm": 38.29094696044922,
      "learning_rate": 1.1454730417090539e-05,
      "loss": 1.5589,
      "step": 75780
    },
    {
      "epoch": 38.55035605289929,
      "grad_norm": 43.63901138305664,
      "learning_rate": 1.1449643947100714e-05,
      "loss": 1.6713,
      "step": 75790
    },
    {
      "epoch": 38.555442522889116,
      "grad_norm": 39.79669952392578,
      "learning_rate": 1.1444557477110885e-05,
      "loss": 1.5797,
      "step": 75800
    },
    {
      "epoch": 38.56052899287894,
      "grad_norm": 36.736995697021484,
      "learning_rate": 1.1439471007121059e-05,
      "loss": 1.5958,
      "step": 75810
    },
    {
      "epoch": 38.56561546286877,
      "grad_norm": 36.355491638183594,
      "learning_rate": 1.1434384537131232e-05,
      "loss": 1.5868,
      "step": 75820
    },
    {
      "epoch": 38.5707019328586,
      "grad_norm": 36.8134765625,
      "learning_rate": 1.1429298067141404e-05,
      "loss": 1.6063,
      "step": 75830
    },
    {
      "epoch": 38.575788402848424,
      "grad_norm": 48.85516357421875,
      "learning_rate": 1.1424211597151577e-05,
      "loss": 1.6225,
      "step": 75840
    },
    {
      "epoch": 38.58087487283825,
      "grad_norm": 46.16156005859375,
      "learning_rate": 1.141912512716175e-05,
      "loss": 1.6682,
      "step": 75850
    },
    {
      "epoch": 38.58596134282808,
      "grad_norm": 41.14194869995117,
      "learning_rate": 1.1414038657171923e-05,
      "loss": 1.6051,
      "step": 75860
    },
    {
      "epoch": 38.591047812817905,
      "grad_norm": 48.0041389465332,
      "learning_rate": 1.1408952187182097e-05,
      "loss": 1.6347,
      "step": 75870
    },
    {
      "epoch": 38.59613428280773,
      "grad_norm": 44.95072555541992,
      "learning_rate": 1.140386571719227e-05,
      "loss": 1.6169,
      "step": 75880
    },
    {
      "epoch": 38.60122075279756,
      "grad_norm": 48.166996002197266,
      "learning_rate": 1.1398779247202442e-05,
      "loss": 1.5954,
      "step": 75890
    },
    {
      "epoch": 38.606307222787386,
      "grad_norm": 32.461151123046875,
      "learning_rate": 1.1393692777212615e-05,
      "loss": 1.6946,
      "step": 75900
    },
    {
      "epoch": 38.61139369277721,
      "grad_norm": 40.8011474609375,
      "learning_rate": 1.1388606307222788e-05,
      "loss": 1.5509,
      "step": 75910
    },
    {
      "epoch": 38.61648016276704,
      "grad_norm": 48.85319519042969,
      "learning_rate": 1.1383519837232961e-05,
      "loss": 1.5858,
      "step": 75920
    },
    {
      "epoch": 38.62156663275687,
      "grad_norm": 43.32421112060547,
      "learning_rate": 1.1378433367243135e-05,
      "loss": 1.6653,
      "step": 75930
    },
    {
      "epoch": 38.626653102746694,
      "grad_norm": 43.431907653808594,
      "learning_rate": 1.1373346897253306e-05,
      "loss": 1.6214,
      "step": 75940
    },
    {
      "epoch": 38.63173957273652,
      "grad_norm": 38.74971008300781,
      "learning_rate": 1.136826042726348e-05,
      "loss": 1.7044,
      "step": 75950
    },
    {
      "epoch": 38.63682604272635,
      "grad_norm": 34.905025482177734,
      "learning_rate": 1.1363173957273653e-05,
      "loss": 1.6308,
      "step": 75960
    },
    {
      "epoch": 38.641912512716175,
      "grad_norm": 46.175376892089844,
      "learning_rate": 1.1358087487283824e-05,
      "loss": 1.6573,
      "step": 75970
    },
    {
      "epoch": 38.646998982706,
      "grad_norm": 47.5940055847168,
      "learning_rate": 1.1353001017294e-05,
      "loss": 1.7392,
      "step": 75980
    },
    {
      "epoch": 38.65208545269583,
      "grad_norm": 41.25896072387695,
      "learning_rate": 1.1347914547304171e-05,
      "loss": 1.6255,
      "step": 75990
    },
    {
      "epoch": 38.657171922685656,
      "grad_norm": 40.658119201660156,
      "learning_rate": 1.1342828077314344e-05,
      "loss": 1.6332,
      "step": 76000
    },
    {
      "epoch": 38.66225839267548,
      "grad_norm": 54.231231689453125,
      "learning_rate": 1.1337741607324518e-05,
      "loss": 1.5421,
      "step": 76010
    },
    {
      "epoch": 38.66734486266531,
      "grad_norm": 37.24070358276367,
      "learning_rate": 1.1332655137334689e-05,
      "loss": 1.5403,
      "step": 76020
    },
    {
      "epoch": 38.67243133265514,
      "grad_norm": 41.59565734863281,
      "learning_rate": 1.1327568667344864e-05,
      "loss": 1.6464,
      "step": 76030
    },
    {
      "epoch": 38.677517802644964,
      "grad_norm": 40.16333770751953,
      "learning_rate": 1.1322482197355036e-05,
      "loss": 1.5461,
      "step": 76040
    },
    {
      "epoch": 38.68260427263479,
      "grad_norm": 63.72776794433594,
      "learning_rate": 1.1317395727365209e-05,
      "loss": 1.614,
      "step": 76050
    },
    {
      "epoch": 38.68769074262462,
      "grad_norm": 37.478782653808594,
      "learning_rate": 1.1312309257375382e-05,
      "loss": 1.5873,
      "step": 76060
    },
    {
      "epoch": 38.692777212614445,
      "grad_norm": 39.79343032836914,
      "learning_rate": 1.1307222787385554e-05,
      "loss": 1.5838,
      "step": 76070
    },
    {
      "epoch": 38.69786368260427,
      "grad_norm": 36.94269943237305,
      "learning_rate": 1.1302136317395729e-05,
      "loss": 1.5526,
      "step": 76080
    },
    {
      "epoch": 38.7029501525941,
      "grad_norm": 33.58088302612305,
      "learning_rate": 1.12970498474059e-05,
      "loss": 1.6343,
      "step": 76090
    },
    {
      "epoch": 38.708036622583926,
      "grad_norm": 42.868988037109375,
      "learning_rate": 1.1291963377416074e-05,
      "loss": 1.6273,
      "step": 76100
    },
    {
      "epoch": 38.71312309257375,
      "grad_norm": 55.56624221801758,
      "learning_rate": 1.1286876907426247e-05,
      "loss": 1.6101,
      "step": 76110
    },
    {
      "epoch": 38.71820956256358,
      "grad_norm": 57.80672073364258,
      "learning_rate": 1.128179043743642e-05,
      "loss": 1.6766,
      "step": 76120
    },
    {
      "epoch": 38.72329603255341,
      "grad_norm": 40.23250198364258,
      "learning_rate": 1.1276703967446594e-05,
      "loss": 1.6711,
      "step": 76130
    },
    {
      "epoch": 38.728382502543234,
      "grad_norm": 38.655521392822266,
      "learning_rate": 1.1271617497456765e-05,
      "loss": 1.6631,
      "step": 76140
    },
    {
      "epoch": 38.73346897253306,
      "grad_norm": 39.395381927490234,
      "learning_rate": 1.1266531027466938e-05,
      "loss": 1.6198,
      "step": 76150
    },
    {
      "epoch": 38.73855544252289,
      "grad_norm": 37.65275955200195,
      "learning_rate": 1.1261444557477112e-05,
      "loss": 1.6606,
      "step": 76160
    },
    {
      "epoch": 38.743641912512714,
      "grad_norm": 42.33873748779297,
      "learning_rate": 1.1256358087487285e-05,
      "loss": 1.6227,
      "step": 76170
    },
    {
      "epoch": 38.74872838250254,
      "grad_norm": 42.50300979614258,
      "learning_rate": 1.1251271617497457e-05,
      "loss": 1.5301,
      "step": 76180
    },
    {
      "epoch": 38.75381485249237,
      "grad_norm": 42.1103630065918,
      "learning_rate": 1.124618514750763e-05,
      "loss": 1.6401,
      "step": 76190
    },
    {
      "epoch": 38.758901322482195,
      "grad_norm": 44.032310485839844,
      "learning_rate": 1.1241098677517803e-05,
      "loss": 1.689,
      "step": 76200
    },
    {
      "epoch": 38.76398779247202,
      "grad_norm": 38.830535888671875,
      "learning_rate": 1.1236012207527976e-05,
      "loss": 1.6338,
      "step": 76210
    },
    {
      "epoch": 38.76907426246185,
      "grad_norm": 37.73894500732422,
      "learning_rate": 1.123092573753815e-05,
      "loss": 1.5286,
      "step": 76220
    },
    {
      "epoch": 38.774160732451676,
      "grad_norm": 39.94057083129883,
      "learning_rate": 1.1225839267548321e-05,
      "loss": 1.6177,
      "step": 76230
    },
    {
      "epoch": 38.7792472024415,
      "grad_norm": 42.735443115234375,
      "learning_rate": 1.1220752797558495e-05,
      "loss": 1.6077,
      "step": 76240
    },
    {
      "epoch": 38.78433367243133,
      "grad_norm": 39.47871780395508,
      "learning_rate": 1.1215666327568668e-05,
      "loss": 1.6355,
      "step": 76250
    },
    {
      "epoch": 38.78942014242116,
      "grad_norm": 38.44456100463867,
      "learning_rate": 1.1210579857578841e-05,
      "loss": 1.7358,
      "step": 76260
    },
    {
      "epoch": 38.794506612410984,
      "grad_norm": 57.20690155029297,
      "learning_rate": 1.1205493387589014e-05,
      "loss": 1.6172,
      "step": 76270
    },
    {
      "epoch": 38.79959308240081,
      "grad_norm": 36.800968170166016,
      "learning_rate": 1.1200406917599186e-05,
      "loss": 1.6081,
      "step": 76280
    },
    {
      "epoch": 38.80467955239064,
      "grad_norm": 36.0275993347168,
      "learning_rate": 1.1195320447609361e-05,
      "loss": 1.5663,
      "step": 76290
    },
    {
      "epoch": 38.809766022380465,
      "grad_norm": 40.866424560546875,
      "learning_rate": 1.1190233977619533e-05,
      "loss": 1.6246,
      "step": 76300
    },
    {
      "epoch": 38.81485249237029,
      "grad_norm": 38.68342208862305,
      "learning_rate": 1.1185147507629704e-05,
      "loss": 1.6184,
      "step": 76310
    },
    {
      "epoch": 38.81993896236012,
      "grad_norm": 37.44565200805664,
      "learning_rate": 1.1180061037639879e-05,
      "loss": 1.6042,
      "step": 76320
    },
    {
      "epoch": 38.825025432349946,
      "grad_norm": 43.262638092041016,
      "learning_rate": 1.117497456765005e-05,
      "loss": 1.6172,
      "step": 76330
    },
    {
      "epoch": 38.83011190233977,
      "grad_norm": 48.82591247558594,
      "learning_rate": 1.1169888097660226e-05,
      "loss": 1.6011,
      "step": 76340
    },
    {
      "epoch": 38.8351983723296,
      "grad_norm": 33.597442626953125,
      "learning_rate": 1.1164801627670397e-05,
      "loss": 1.6235,
      "step": 76350
    },
    {
      "epoch": 38.84028484231943,
      "grad_norm": 36.70900344848633,
      "learning_rate": 1.115971515768057e-05,
      "loss": 1.4992,
      "step": 76360
    },
    {
      "epoch": 38.845371312309254,
      "grad_norm": 51.04205322265625,
      "learning_rate": 1.1154628687690744e-05,
      "loss": 1.4926,
      "step": 76370
    },
    {
      "epoch": 38.85045778229908,
      "grad_norm": 32.86320114135742,
      "learning_rate": 1.1149542217700915e-05,
      "loss": 1.6186,
      "step": 76380
    },
    {
      "epoch": 38.85554425228891,
      "grad_norm": 38.41908264160156,
      "learning_rate": 1.1144455747711089e-05,
      "loss": 1.552,
      "step": 76390
    },
    {
      "epoch": 38.86063072227874,
      "grad_norm": 36.568729400634766,
      "learning_rate": 1.1139369277721262e-05,
      "loss": 1.5508,
      "step": 76400
    },
    {
      "epoch": 38.86571719226856,
      "grad_norm": 36.05832290649414,
      "learning_rate": 1.1134282807731435e-05,
      "loss": 1.6035,
      "step": 76410
    },
    {
      "epoch": 38.870803662258396,
      "grad_norm": 38.54530715942383,
      "learning_rate": 1.1129196337741609e-05,
      "loss": 1.705,
      "step": 76420
    },
    {
      "epoch": 38.87589013224822,
      "grad_norm": 39.62910842895508,
      "learning_rate": 1.112410986775178e-05,
      "loss": 1.6148,
      "step": 76430
    },
    {
      "epoch": 38.88097660223805,
      "grad_norm": 55.0116081237793,
      "learning_rate": 1.1119023397761953e-05,
      "loss": 1.5082,
      "step": 76440
    },
    {
      "epoch": 38.88606307222788,
      "grad_norm": 39.3297004699707,
      "learning_rate": 1.1113936927772127e-05,
      "loss": 1.5672,
      "step": 76450
    },
    {
      "epoch": 38.891149542217704,
      "grad_norm": 34.95030975341797,
      "learning_rate": 1.11088504577823e-05,
      "loss": 1.6697,
      "step": 76460
    },
    {
      "epoch": 38.89623601220753,
      "grad_norm": 38.85585403442383,
      "learning_rate": 1.1103763987792473e-05,
      "loss": 1.5125,
      "step": 76470
    },
    {
      "epoch": 38.90132248219736,
      "grad_norm": 39.27765655517578,
      "learning_rate": 1.1098677517802645e-05,
      "loss": 1.518,
      "step": 76480
    },
    {
      "epoch": 38.906408952187185,
      "grad_norm": 49.42181396484375,
      "learning_rate": 1.1093591047812818e-05,
      "loss": 1.6182,
      "step": 76490
    },
    {
      "epoch": 38.91149542217701,
      "grad_norm": 41.079986572265625,
      "learning_rate": 1.1088504577822991e-05,
      "loss": 1.5706,
      "step": 76500
    },
    {
      "epoch": 38.91658189216684,
      "grad_norm": 46.730411529541016,
      "learning_rate": 1.1083418107833165e-05,
      "loss": 1.6215,
      "step": 76510
    },
    {
      "epoch": 38.921668362156666,
      "grad_norm": 45.19309616088867,
      "learning_rate": 1.1078331637843336e-05,
      "loss": 1.5352,
      "step": 76520
    },
    {
      "epoch": 38.92675483214649,
      "grad_norm": 35.063316345214844,
      "learning_rate": 1.1073245167853511e-05,
      "loss": 1.6738,
      "step": 76530
    },
    {
      "epoch": 38.93184130213632,
      "grad_norm": 45.64394760131836,
      "learning_rate": 1.1068158697863683e-05,
      "loss": 1.5346,
      "step": 76540
    },
    {
      "epoch": 38.93692777212615,
      "grad_norm": 42.067466735839844,
      "learning_rate": 1.1063072227873856e-05,
      "loss": 1.5882,
      "step": 76550
    },
    {
      "epoch": 38.942014242115974,
      "grad_norm": 35.529239654541016,
      "learning_rate": 1.105798575788403e-05,
      "loss": 1.5783,
      "step": 76560
    },
    {
      "epoch": 38.9471007121058,
      "grad_norm": 35.3875732421875,
      "learning_rate": 1.1052899287894201e-05,
      "loss": 1.6141,
      "step": 76570
    },
    {
      "epoch": 38.95218718209563,
      "grad_norm": 51.367801666259766,
      "learning_rate": 1.1047812817904376e-05,
      "loss": 1.7029,
      "step": 76580
    },
    {
      "epoch": 38.957273652085455,
      "grad_norm": 40.06204605102539,
      "learning_rate": 1.1042726347914548e-05,
      "loss": 1.6421,
      "step": 76590
    },
    {
      "epoch": 38.96236012207528,
      "grad_norm": 61.174564361572266,
      "learning_rate": 1.103763987792472e-05,
      "loss": 1.5974,
      "step": 76600
    },
    {
      "epoch": 38.96744659206511,
      "grad_norm": 39.46749496459961,
      "learning_rate": 1.1032553407934894e-05,
      "loss": 1.586,
      "step": 76610
    },
    {
      "epoch": 38.972533062054936,
      "grad_norm": 42.769287109375,
      "learning_rate": 1.1027466937945066e-05,
      "loss": 1.5069,
      "step": 76620
    },
    {
      "epoch": 38.97761953204476,
      "grad_norm": 44.46570587158203,
      "learning_rate": 1.102238046795524e-05,
      "loss": 1.6294,
      "step": 76630
    },
    {
      "epoch": 38.98270600203459,
      "grad_norm": 38.333946228027344,
      "learning_rate": 1.1017293997965412e-05,
      "loss": 1.714,
      "step": 76640
    },
    {
      "epoch": 38.98779247202442,
      "grad_norm": 45.44023132324219,
      "learning_rate": 1.1012207527975586e-05,
      "loss": 1.6361,
      "step": 76650
    },
    {
      "epoch": 38.992878942014244,
      "grad_norm": 37.05904769897461,
      "learning_rate": 1.1007121057985759e-05,
      "loss": 1.6521,
      "step": 76660
    },
    {
      "epoch": 38.99796541200407,
      "grad_norm": 35.829017639160156,
      "learning_rate": 1.100203458799593e-05,
      "loss": 1.5671,
      "step": 76670
    },
    {
      "epoch": 39.0,
      "eval_loss": 4.994237899780273,
      "eval_runtime": 2.7527,
      "eval_samples_per_second": 1008.114,
      "eval_steps_per_second": 126.06,
      "step": 76674
    },
    {
      "epoch": 39.0030518819939,
      "grad_norm": 40.8057975769043,
      "learning_rate": 1.0996948118006105e-05,
      "loss": 1.617,
      "step": 76680
    },
    {
      "epoch": 39.008138351983725,
      "grad_norm": 41.301612854003906,
      "learning_rate": 1.0991861648016277e-05,
      "loss": 1.7062,
      "step": 76690
    },
    {
      "epoch": 39.01322482197355,
      "grad_norm": 45.509666442871094,
      "learning_rate": 1.098677517802645e-05,
      "loss": 1.604,
      "step": 76700
    },
    {
      "epoch": 39.01831129196338,
      "grad_norm": 34.33264923095703,
      "learning_rate": 1.0981688708036624e-05,
      "loss": 1.6046,
      "step": 76710
    },
    {
      "epoch": 39.023397761953206,
      "grad_norm": 40.45454025268555,
      "learning_rate": 1.0976602238046795e-05,
      "loss": 1.5087,
      "step": 76720
    },
    {
      "epoch": 39.02848423194303,
      "grad_norm": 37.59013748168945,
      "learning_rate": 1.097151576805697e-05,
      "loss": 1.6509,
      "step": 76730
    },
    {
      "epoch": 39.03357070193286,
      "grad_norm": 30.50422477722168,
      "learning_rate": 1.0966429298067142e-05,
      "loss": 1.6021,
      "step": 76740
    },
    {
      "epoch": 39.03865717192269,
      "grad_norm": 39.46794891357422,
      "learning_rate": 1.0961342828077315e-05,
      "loss": 1.6118,
      "step": 76750
    },
    {
      "epoch": 39.04374364191251,
      "grad_norm": 35.293060302734375,
      "learning_rate": 1.0956256358087488e-05,
      "loss": 1.6047,
      "step": 76760
    },
    {
      "epoch": 39.04883011190234,
      "grad_norm": 40.76036834716797,
      "learning_rate": 1.0951169888097662e-05,
      "loss": 1.6288,
      "step": 76770
    },
    {
      "epoch": 39.05391658189217,
      "grad_norm": 41.51137924194336,
      "learning_rate": 1.0946083418107833e-05,
      "loss": 1.6317,
      "step": 76780
    },
    {
      "epoch": 39.059003051881994,
      "grad_norm": 48.73244857788086,
      "learning_rate": 1.0940996948118006e-05,
      "loss": 1.5244,
      "step": 76790
    },
    {
      "epoch": 39.06408952187182,
      "grad_norm": 38.63288497924805,
      "learning_rate": 1.093591047812818e-05,
      "loss": 1.5879,
      "step": 76800
    },
    {
      "epoch": 39.06917599186165,
      "grad_norm": 44.25737762451172,
      "learning_rate": 1.0930824008138353e-05,
      "loss": 1.5673,
      "step": 76810
    },
    {
      "epoch": 39.074262461851475,
      "grad_norm": 38.77045822143555,
      "learning_rate": 1.0925737538148526e-05,
      "loss": 1.61,
      "step": 76820
    },
    {
      "epoch": 39.0793489318413,
      "grad_norm": 49.52751541137695,
      "learning_rate": 1.0920651068158698e-05,
      "loss": 1.5898,
      "step": 76830
    },
    {
      "epoch": 39.08443540183113,
      "grad_norm": 56.3883056640625,
      "learning_rate": 1.0915564598168871e-05,
      "loss": 1.5505,
      "step": 76840
    },
    {
      "epoch": 39.089521871820956,
      "grad_norm": 41.38671875,
      "learning_rate": 1.0910478128179044e-05,
      "loss": 1.576,
      "step": 76850
    },
    {
      "epoch": 39.09460834181078,
      "grad_norm": 45.194942474365234,
      "learning_rate": 1.0905391658189216e-05,
      "loss": 1.6442,
      "step": 76860
    },
    {
      "epoch": 39.09969481180061,
      "grad_norm": 32.81201171875,
      "learning_rate": 1.0900305188199391e-05,
      "loss": 1.6607,
      "step": 76870
    },
    {
      "epoch": 39.10478128179044,
      "grad_norm": 45.037757873535156,
      "learning_rate": 1.0895218718209563e-05,
      "loss": 1.5481,
      "step": 76880
    },
    {
      "epoch": 39.109867751780264,
      "grad_norm": 45.73140335083008,
      "learning_rate": 1.0890132248219736e-05,
      "loss": 1.6455,
      "step": 76890
    },
    {
      "epoch": 39.11495422177009,
      "grad_norm": 39.10914611816406,
      "learning_rate": 1.0885045778229909e-05,
      "loss": 1.5479,
      "step": 76900
    },
    {
      "epoch": 39.12004069175992,
      "grad_norm": 39.29317855834961,
      "learning_rate": 1.087995930824008e-05,
      "loss": 1.5838,
      "step": 76910
    },
    {
      "epoch": 39.125127161749745,
      "grad_norm": 39.153282165527344,
      "learning_rate": 1.0874872838250256e-05,
      "loss": 1.6844,
      "step": 76920
    },
    {
      "epoch": 39.13021363173957,
      "grad_norm": 46.586307525634766,
      "learning_rate": 1.0869786368260427e-05,
      "loss": 1.5461,
      "step": 76930
    },
    {
      "epoch": 39.1353001017294,
      "grad_norm": 45.78940200805664,
      "learning_rate": 1.08646998982706e-05,
      "loss": 1.6137,
      "step": 76940
    },
    {
      "epoch": 39.140386571719226,
      "grad_norm": 41.672122955322266,
      "learning_rate": 1.0859613428280774e-05,
      "loss": 1.5549,
      "step": 76950
    },
    {
      "epoch": 39.14547304170905,
      "grad_norm": 43.44888687133789,
      "learning_rate": 1.0854526958290945e-05,
      "loss": 1.5702,
      "step": 76960
    },
    {
      "epoch": 39.15055951169888,
      "grad_norm": 44.998046875,
      "learning_rate": 1.084944048830112e-05,
      "loss": 1.5807,
      "step": 76970
    },
    {
      "epoch": 39.15564598168871,
      "grad_norm": 34.43526840209961,
      "learning_rate": 1.0844354018311292e-05,
      "loss": 1.5904,
      "step": 76980
    },
    {
      "epoch": 39.160732451678534,
      "grad_norm": 38.63175582885742,
      "learning_rate": 1.0839267548321465e-05,
      "loss": 1.5428,
      "step": 76990
    },
    {
      "epoch": 39.16581892166836,
      "grad_norm": 43.39353561401367,
      "learning_rate": 1.0834181078331639e-05,
      "loss": 1.6484,
      "step": 77000
    },
    {
      "epoch": 39.17090539165819,
      "grad_norm": 41.04584503173828,
      "learning_rate": 1.0829094608341812e-05,
      "loss": 1.5506,
      "step": 77010
    },
    {
      "epoch": 39.175991861648015,
      "grad_norm": 46.28148651123047,
      "learning_rate": 1.0824008138351985e-05,
      "loss": 1.587,
      "step": 77020
    },
    {
      "epoch": 39.18107833163784,
      "grad_norm": 40.97111129760742,
      "learning_rate": 1.0818921668362157e-05,
      "loss": 1.5992,
      "step": 77030
    },
    {
      "epoch": 39.18616480162767,
      "grad_norm": 41.570796966552734,
      "learning_rate": 1.081383519837233e-05,
      "loss": 1.5514,
      "step": 77040
    },
    {
      "epoch": 39.191251271617496,
      "grad_norm": 46.664878845214844,
      "learning_rate": 1.0808748728382503e-05,
      "loss": 1.5668,
      "step": 77050
    },
    {
      "epoch": 39.19633774160732,
      "grad_norm": 45.665733337402344,
      "learning_rate": 1.0803662258392677e-05,
      "loss": 1.5161,
      "step": 77060
    },
    {
      "epoch": 39.20142421159715,
      "grad_norm": 38.03252410888672,
      "learning_rate": 1.079857578840285e-05,
      "loss": 1.576,
      "step": 77070
    },
    {
      "epoch": 39.20651068158698,
      "grad_norm": 43.1388053894043,
      "learning_rate": 1.0793489318413021e-05,
      "loss": 1.6822,
      "step": 77080
    },
    {
      "epoch": 39.211597151576804,
      "grad_norm": 38.22296905517578,
      "learning_rate": 1.0788402848423195e-05,
      "loss": 1.5666,
      "step": 77090
    },
    {
      "epoch": 39.21668362156663,
      "grad_norm": 41.097808837890625,
      "learning_rate": 1.0783316378433368e-05,
      "loss": 1.5676,
      "step": 77100
    },
    {
      "epoch": 39.22177009155646,
      "grad_norm": 49.54548645019531,
      "learning_rate": 1.0778229908443541e-05,
      "loss": 1.6513,
      "step": 77110
    },
    {
      "epoch": 39.226856561546285,
      "grad_norm": 41.065101623535156,
      "learning_rate": 1.0773143438453713e-05,
      "loss": 1.5826,
      "step": 77120
    },
    {
      "epoch": 39.23194303153611,
      "grad_norm": 37.024864196777344,
      "learning_rate": 1.0768056968463886e-05,
      "loss": 1.6042,
      "step": 77130
    },
    {
      "epoch": 39.23702950152594,
      "grad_norm": 38.645809173583984,
      "learning_rate": 1.076297049847406e-05,
      "loss": 1.6364,
      "step": 77140
    },
    {
      "epoch": 39.242115971515766,
      "grad_norm": 40.24690628051758,
      "learning_rate": 1.0757884028484233e-05,
      "loss": 1.644,
      "step": 77150
    },
    {
      "epoch": 39.24720244150559,
      "grad_norm": 35.15559387207031,
      "learning_rate": 1.0752797558494406e-05,
      "loss": 1.5821,
      "step": 77160
    },
    {
      "epoch": 39.25228891149542,
      "grad_norm": 37.92195129394531,
      "learning_rate": 1.0747711088504578e-05,
      "loss": 1.5854,
      "step": 77170
    },
    {
      "epoch": 39.25737538148525,
      "grad_norm": 37.905879974365234,
      "learning_rate": 1.074262461851475e-05,
      "loss": 1.5516,
      "step": 77180
    },
    {
      "epoch": 39.262461851475074,
      "grad_norm": 38.37110900878906,
      "learning_rate": 1.0737538148524924e-05,
      "loss": 1.6007,
      "step": 77190
    },
    {
      "epoch": 39.2675483214649,
      "grad_norm": 35.045166015625,
      "learning_rate": 1.0732451678535096e-05,
      "loss": 1.6387,
      "step": 77200
    },
    {
      "epoch": 39.27263479145473,
      "grad_norm": 35.580963134765625,
      "learning_rate": 1.072736520854527e-05,
      "loss": 1.5235,
      "step": 77210
    },
    {
      "epoch": 39.277721261444555,
      "grad_norm": 43.26802062988281,
      "learning_rate": 1.0722278738555442e-05,
      "loss": 1.6339,
      "step": 77220
    },
    {
      "epoch": 39.28280773143438,
      "grad_norm": 37.93382263183594,
      "learning_rate": 1.0717192268565617e-05,
      "loss": 1.5726,
      "step": 77230
    },
    {
      "epoch": 39.28789420142421,
      "grad_norm": 36.12221908569336,
      "learning_rate": 1.0712105798575789e-05,
      "loss": 1.477,
      "step": 77240
    },
    {
      "epoch": 39.292980671414035,
      "grad_norm": 44.201019287109375,
      "learning_rate": 1.0707019328585962e-05,
      "loss": 1.5479,
      "step": 77250
    },
    {
      "epoch": 39.29806714140386,
      "grad_norm": 36.04311752319336,
      "learning_rate": 1.0701932858596135e-05,
      "loss": 1.5747,
      "step": 77260
    },
    {
      "epoch": 39.30315361139369,
      "grad_norm": 42.3603515625,
      "learning_rate": 1.0696846388606307e-05,
      "loss": 1.6383,
      "step": 77270
    },
    {
      "epoch": 39.308240081383516,
      "grad_norm": 40.05363082885742,
      "learning_rate": 1.0691759918616482e-05,
      "loss": 1.5121,
      "step": 77280
    },
    {
      "epoch": 39.31332655137334,
      "grad_norm": 40.06126022338867,
      "learning_rate": 1.0686673448626654e-05,
      "loss": 1.6105,
      "step": 77290
    },
    {
      "epoch": 39.31841302136317,
      "grad_norm": 35.14181137084961,
      "learning_rate": 1.0681586978636827e-05,
      "loss": 1.6206,
      "step": 77300
    },
    {
      "epoch": 39.323499491353004,
      "grad_norm": 48.306026458740234,
      "learning_rate": 1.0676500508647e-05,
      "loss": 1.5984,
      "step": 77310
    },
    {
      "epoch": 39.32858596134283,
      "grad_norm": 47.54384231567383,
      "learning_rate": 1.0671414038657172e-05,
      "loss": 1.5834,
      "step": 77320
    },
    {
      "epoch": 39.33367243133266,
      "grad_norm": 36.52968215942383,
      "learning_rate": 1.0666327568667345e-05,
      "loss": 1.604,
      "step": 77330
    },
    {
      "epoch": 39.338758901322485,
      "grad_norm": 45.82957077026367,
      "learning_rate": 1.0661241098677518e-05,
      "loss": 1.6186,
      "step": 77340
    },
    {
      "epoch": 39.34384537131231,
      "grad_norm": 52.9815788269043,
      "learning_rate": 1.0656154628687692e-05,
      "loss": 1.6244,
      "step": 77350
    },
    {
      "epoch": 39.34893184130214,
      "grad_norm": 36.56314468383789,
      "learning_rate": 1.0651068158697865e-05,
      "loss": 1.5114,
      "step": 77360
    },
    {
      "epoch": 39.354018311291966,
      "grad_norm": 39.90623474121094,
      "learning_rate": 1.0645981688708036e-05,
      "loss": 1.6212,
      "step": 77370
    },
    {
      "epoch": 39.35910478128179,
      "grad_norm": 43.15107345581055,
      "learning_rate": 1.064089521871821e-05,
      "loss": 1.5858,
      "step": 77380
    },
    {
      "epoch": 39.36419125127162,
      "grad_norm": 43.46828842163086,
      "learning_rate": 1.0635808748728383e-05,
      "loss": 1.6641,
      "step": 77390
    },
    {
      "epoch": 39.36927772126145,
      "grad_norm": 44.40073776245117,
      "learning_rate": 1.0630722278738556e-05,
      "loss": 1.6168,
      "step": 77400
    },
    {
      "epoch": 39.374364191251274,
      "grad_norm": 40.93168640136719,
      "learning_rate": 1.062563580874873e-05,
      "loss": 1.6152,
      "step": 77410
    },
    {
      "epoch": 39.3794506612411,
      "grad_norm": 34.69818878173828,
      "learning_rate": 1.0620549338758901e-05,
      "loss": 1.6434,
      "step": 77420
    },
    {
      "epoch": 39.38453713123093,
      "grad_norm": 33.674102783203125,
      "learning_rate": 1.0615462868769074e-05,
      "loss": 1.609,
      "step": 77430
    },
    {
      "epoch": 39.389623601220755,
      "grad_norm": 43.01898193359375,
      "learning_rate": 1.0610376398779248e-05,
      "loss": 1.6337,
      "step": 77440
    },
    {
      "epoch": 39.39471007121058,
      "grad_norm": 43.170799255371094,
      "learning_rate": 1.0605289928789421e-05,
      "loss": 1.6045,
      "step": 77450
    },
    {
      "epoch": 39.39979654120041,
      "grad_norm": 39.22672653198242,
      "learning_rate": 1.0600203458799593e-05,
      "loss": 1.6601,
      "step": 77460
    },
    {
      "epoch": 39.404883011190236,
      "grad_norm": 44.87347412109375,
      "learning_rate": 1.0595116988809768e-05,
      "loss": 1.5727,
      "step": 77470
    },
    {
      "epoch": 39.40996948118006,
      "grad_norm": 45.878021240234375,
      "learning_rate": 1.0590030518819939e-05,
      "loss": 1.6103,
      "step": 77480
    },
    {
      "epoch": 39.41505595116989,
      "grad_norm": 32.413291931152344,
      "learning_rate": 1.0584944048830112e-05,
      "loss": 1.5833,
      "step": 77490
    },
    {
      "epoch": 39.42014242115972,
      "grad_norm": 47.36430740356445,
      "learning_rate": 1.0579857578840286e-05,
      "loss": 1.6465,
      "step": 77500
    },
    {
      "epoch": 39.425228891149544,
      "grad_norm": 42.00898361206055,
      "learning_rate": 1.0574771108850457e-05,
      "loss": 1.5224,
      "step": 77510
    },
    {
      "epoch": 39.43031536113937,
      "grad_norm": 42.79998779296875,
      "learning_rate": 1.0569684638860632e-05,
      "loss": 1.6553,
      "step": 77520
    },
    {
      "epoch": 39.4354018311292,
      "grad_norm": 48.309547424316406,
      "learning_rate": 1.0564598168870804e-05,
      "loss": 1.6981,
      "step": 77530
    },
    {
      "epoch": 39.440488301119025,
      "grad_norm": 46.90449142456055,
      "learning_rate": 1.0559511698880977e-05,
      "loss": 1.5913,
      "step": 77540
    },
    {
      "epoch": 39.44557477110885,
      "grad_norm": 40.06707763671875,
      "learning_rate": 1.055442522889115e-05,
      "loss": 1.5662,
      "step": 77550
    },
    {
      "epoch": 39.45066124109868,
      "grad_norm": 40.833396911621094,
      "learning_rate": 1.0549338758901322e-05,
      "loss": 1.5463,
      "step": 77560
    },
    {
      "epoch": 39.455747711088506,
      "grad_norm": 46.66583251953125,
      "learning_rate": 1.0544252288911497e-05,
      "loss": 1.7,
      "step": 77570
    },
    {
      "epoch": 39.46083418107833,
      "grad_norm": 40.14229965209961,
      "learning_rate": 1.0539165818921669e-05,
      "loss": 1.5014,
      "step": 77580
    },
    {
      "epoch": 39.46592065106816,
      "grad_norm": 42.46878433227539,
      "learning_rate": 1.0534079348931842e-05,
      "loss": 1.5918,
      "step": 77590
    },
    {
      "epoch": 39.47100712105799,
      "grad_norm": 34.31199264526367,
      "learning_rate": 1.0528992878942015e-05,
      "loss": 1.5576,
      "step": 77600
    },
    {
      "epoch": 39.476093591047814,
      "grad_norm": 35.559173583984375,
      "learning_rate": 1.0523906408952187e-05,
      "loss": 1.6696,
      "step": 77610
    },
    {
      "epoch": 39.48118006103764,
      "grad_norm": 32.89820861816406,
      "learning_rate": 1.0518819938962362e-05,
      "loss": 1.5789,
      "step": 77620
    },
    {
      "epoch": 39.48626653102747,
      "grad_norm": 34.29158020019531,
      "learning_rate": 1.0513733468972533e-05,
      "loss": 1.6353,
      "step": 77630
    },
    {
      "epoch": 39.491353001017295,
      "grad_norm": 47.1392936706543,
      "learning_rate": 1.0508646998982707e-05,
      "loss": 1.5627,
      "step": 77640
    },
    {
      "epoch": 39.49643947100712,
      "grad_norm": 38.02565002441406,
      "learning_rate": 1.050356052899288e-05,
      "loss": 1.5799,
      "step": 77650
    },
    {
      "epoch": 39.50152594099695,
      "grad_norm": 32.50350570678711,
      "learning_rate": 1.0498474059003053e-05,
      "loss": 1.5743,
      "step": 77660
    },
    {
      "epoch": 39.506612410986776,
      "grad_norm": 33.779056549072266,
      "learning_rate": 1.0493387589013225e-05,
      "loss": 1.5839,
      "step": 77670
    },
    {
      "epoch": 39.5116988809766,
      "grad_norm": 48.151615142822266,
      "learning_rate": 1.0488301119023398e-05,
      "loss": 1.5975,
      "step": 77680
    },
    {
      "epoch": 39.51678535096643,
      "grad_norm": 46.47880554199219,
      "learning_rate": 1.0483214649033571e-05,
      "loss": 1.6241,
      "step": 77690
    },
    {
      "epoch": 39.52187182095626,
      "grad_norm": 46.88677215576172,
      "learning_rate": 1.0478128179043745e-05,
      "loss": 1.5629,
      "step": 77700
    },
    {
      "epoch": 39.526958290946084,
      "grad_norm": 55.56526184082031,
      "learning_rate": 1.0473041709053918e-05,
      "loss": 1.6412,
      "step": 77710
    },
    {
      "epoch": 39.53204476093591,
      "grad_norm": 38.1456298828125,
      "learning_rate": 1.046795523906409e-05,
      "loss": 1.6956,
      "step": 77720
    },
    {
      "epoch": 39.53713123092574,
      "grad_norm": 52.03538131713867,
      "learning_rate": 1.0462868769074263e-05,
      "loss": 1.5654,
      "step": 77730
    },
    {
      "epoch": 39.542217700915565,
      "grad_norm": 45.020809173583984,
      "learning_rate": 1.0457782299084436e-05,
      "loss": 1.5688,
      "step": 77740
    },
    {
      "epoch": 39.54730417090539,
      "grad_norm": 49.43972396850586,
      "learning_rate": 1.045269582909461e-05,
      "loss": 1.5823,
      "step": 77750
    },
    {
      "epoch": 39.55239064089522,
      "grad_norm": 43.774200439453125,
      "learning_rate": 1.0447609359104783e-05,
      "loss": 1.5913,
      "step": 77760
    },
    {
      "epoch": 39.557477110885046,
      "grad_norm": 42.15627670288086,
      "learning_rate": 1.0442522889114954e-05,
      "loss": 1.653,
      "step": 77770
    },
    {
      "epoch": 39.56256358087487,
      "grad_norm": 42.10301971435547,
      "learning_rate": 1.0437436419125127e-05,
      "loss": 1.6956,
      "step": 77780
    },
    {
      "epoch": 39.5676500508647,
      "grad_norm": 45.166690826416016,
      "learning_rate": 1.04323499491353e-05,
      "loss": 1.5633,
      "step": 77790
    },
    {
      "epoch": 39.57273652085453,
      "grad_norm": 47.7222785949707,
      "learning_rate": 1.0427263479145472e-05,
      "loss": 1.5688,
      "step": 77800
    },
    {
      "epoch": 39.57782299084435,
      "grad_norm": 38.84807205200195,
      "learning_rate": 1.0422177009155647e-05,
      "loss": 1.5606,
      "step": 77810
    },
    {
      "epoch": 39.58290946083418,
      "grad_norm": 39.68854904174805,
      "learning_rate": 1.0417090539165819e-05,
      "loss": 1.5902,
      "step": 77820
    },
    {
      "epoch": 39.58799593082401,
      "grad_norm": 41.12030792236328,
      "learning_rate": 1.0412004069175992e-05,
      "loss": 1.6608,
      "step": 77830
    },
    {
      "epoch": 39.593082400813834,
      "grad_norm": 46.14213180541992,
      "learning_rate": 1.0406917599186165e-05,
      "loss": 1.5753,
      "step": 77840
    },
    {
      "epoch": 39.59816887080366,
      "grad_norm": 37.18977355957031,
      "learning_rate": 1.0401831129196337e-05,
      "loss": 1.6219,
      "step": 77850
    },
    {
      "epoch": 39.60325534079349,
      "grad_norm": 40.21096420288086,
      "learning_rate": 1.0396744659206512e-05,
      "loss": 1.65,
      "step": 77860
    },
    {
      "epoch": 39.608341810783315,
      "grad_norm": 37.158260345458984,
      "learning_rate": 1.0391658189216684e-05,
      "loss": 1.5795,
      "step": 77870
    },
    {
      "epoch": 39.61342828077314,
      "grad_norm": 41.813873291015625,
      "learning_rate": 1.0386571719226859e-05,
      "loss": 1.5548,
      "step": 77880
    },
    {
      "epoch": 39.61851475076297,
      "grad_norm": 39.358211517333984,
      "learning_rate": 1.038148524923703e-05,
      "loss": 1.5109,
      "step": 77890
    },
    {
      "epoch": 39.623601220752796,
      "grad_norm": 57.8826789855957,
      "learning_rate": 1.0376398779247203e-05,
      "loss": 1.6348,
      "step": 77900
    },
    {
      "epoch": 39.62868769074262,
      "grad_norm": 35.86629104614258,
      "learning_rate": 1.0371312309257377e-05,
      "loss": 1.7046,
      "step": 77910
    },
    {
      "epoch": 39.63377416073245,
      "grad_norm": 48.74250411987305,
      "learning_rate": 1.0366225839267548e-05,
      "loss": 1.6394,
      "step": 77920
    },
    {
      "epoch": 39.63886063072228,
      "grad_norm": 44.17793273925781,
      "learning_rate": 1.0361139369277722e-05,
      "loss": 1.6303,
      "step": 77930
    },
    {
      "epoch": 39.643947100712104,
      "grad_norm": 42.94096374511719,
      "learning_rate": 1.0356052899287895e-05,
      "loss": 1.5772,
      "step": 77940
    },
    {
      "epoch": 39.64903357070193,
      "grad_norm": 38.607425689697266,
      "learning_rate": 1.0350966429298068e-05,
      "loss": 1.6588,
      "step": 77950
    },
    {
      "epoch": 39.65412004069176,
      "grad_norm": 41.80307388305664,
      "learning_rate": 1.0345879959308241e-05,
      "loss": 1.709,
      "step": 77960
    },
    {
      "epoch": 39.659206510681585,
      "grad_norm": 38.900794982910156,
      "learning_rate": 1.0340793489318413e-05,
      "loss": 1.584,
      "step": 77970
    },
    {
      "epoch": 39.66429298067141,
      "grad_norm": 49.81612014770508,
      "learning_rate": 1.0335707019328586e-05,
      "loss": 1.5884,
      "step": 77980
    },
    {
      "epoch": 39.66937945066124,
      "grad_norm": 43.75346374511719,
      "learning_rate": 1.033062054933876e-05,
      "loss": 1.5805,
      "step": 77990
    },
    {
      "epoch": 39.674465920651066,
      "grad_norm": 38.174930572509766,
      "learning_rate": 1.0325534079348933e-05,
      "loss": 1.6565,
      "step": 78000
    },
    {
      "epoch": 39.67955239064089,
      "grad_norm": 46.69095230102539,
      "learning_rate": 1.0320447609359104e-05,
      "loss": 1.676,
      "step": 78010
    },
    {
      "epoch": 39.68463886063072,
      "grad_norm": 39.118247985839844,
      "learning_rate": 1.0315361139369278e-05,
      "loss": 1.5762,
      "step": 78020
    },
    {
      "epoch": 39.68972533062055,
      "grad_norm": 41.914798736572266,
      "learning_rate": 1.0310274669379451e-05,
      "loss": 1.5774,
      "step": 78030
    },
    {
      "epoch": 39.694811800610374,
      "grad_norm": 48.81087112426758,
      "learning_rate": 1.0305188199389624e-05,
      "loss": 1.5948,
      "step": 78040
    },
    {
      "epoch": 39.6998982706002,
      "grad_norm": 46.671875,
      "learning_rate": 1.0300101729399798e-05,
      "loss": 1.614,
      "step": 78050
    },
    {
      "epoch": 39.70498474059003,
      "grad_norm": 42.67978286743164,
      "learning_rate": 1.0295015259409969e-05,
      "loss": 1.5363,
      "step": 78060
    },
    {
      "epoch": 39.710071210579855,
      "grad_norm": 37.419227600097656,
      "learning_rate": 1.0289928789420142e-05,
      "loss": 1.5645,
      "step": 78070
    },
    {
      "epoch": 39.71515768056968,
      "grad_norm": 33.90019989013672,
      "learning_rate": 1.0284842319430316e-05,
      "loss": 1.6012,
      "step": 78080
    },
    {
      "epoch": 39.72024415055951,
      "grad_norm": 42.63424301147461,
      "learning_rate": 1.0279755849440489e-05,
      "loss": 1.5976,
      "step": 78090
    },
    {
      "epoch": 39.725330620549336,
      "grad_norm": 44.54334259033203,
      "learning_rate": 1.0274669379450662e-05,
      "loss": 1.5798,
      "step": 78100
    },
    {
      "epoch": 39.73041709053916,
      "grad_norm": 37.932918548583984,
      "learning_rate": 1.0269582909460834e-05,
      "loss": 1.5128,
      "step": 78110
    },
    {
      "epoch": 39.73550356052899,
      "grad_norm": 39.549800872802734,
      "learning_rate": 1.0264496439471009e-05,
      "loss": 1.5647,
      "step": 78120
    },
    {
      "epoch": 39.74059003051882,
      "grad_norm": 34.073509216308594,
      "learning_rate": 1.025940996948118e-05,
      "loss": 1.6142,
      "step": 78130
    },
    {
      "epoch": 39.745676500508644,
      "grad_norm": 44.0945930480957,
      "learning_rate": 1.0254323499491354e-05,
      "loss": 1.5811,
      "step": 78140
    },
    {
      "epoch": 39.75076297049847,
      "grad_norm": 29.86556053161621,
      "learning_rate": 1.0249237029501527e-05,
      "loss": 1.6179,
      "step": 78150
    },
    {
      "epoch": 39.7558494404883,
      "grad_norm": 44.89766311645508,
      "learning_rate": 1.0244150559511699e-05,
      "loss": 1.5739,
      "step": 78160
    },
    {
      "epoch": 39.760935910478125,
      "grad_norm": 37.34054183959961,
      "learning_rate": 1.0239064089521874e-05,
      "loss": 1.6118,
      "step": 78170
    },
    {
      "epoch": 39.76602238046795,
      "grad_norm": 46.2697868347168,
      "learning_rate": 1.0233977619532045e-05,
      "loss": 1.5207,
      "step": 78180
    },
    {
      "epoch": 39.77110885045778,
      "grad_norm": 49.378700256347656,
      "learning_rate": 1.0228891149542218e-05,
      "loss": 1.6047,
      "step": 78190
    },
    {
      "epoch": 39.77619532044761,
      "grad_norm": 48.345951080322266,
      "learning_rate": 1.0223804679552392e-05,
      "loss": 1.6013,
      "step": 78200
    },
    {
      "epoch": 39.78128179043744,
      "grad_norm": 27.05874252319336,
      "learning_rate": 1.0218718209562563e-05,
      "loss": 1.7003,
      "step": 78210
    },
    {
      "epoch": 39.78636826042727,
      "grad_norm": 49.967803955078125,
      "learning_rate": 1.0213631739572738e-05,
      "loss": 1.6397,
      "step": 78220
    },
    {
      "epoch": 39.791454730417094,
      "grad_norm": 48.13840103149414,
      "learning_rate": 1.020854526958291e-05,
      "loss": 1.5317,
      "step": 78230
    },
    {
      "epoch": 39.79654120040692,
      "grad_norm": 39.58027267456055,
      "learning_rate": 1.0203458799593083e-05,
      "loss": 1.6234,
      "step": 78240
    },
    {
      "epoch": 39.80162767039675,
      "grad_norm": 53.482933044433594,
      "learning_rate": 1.0198372329603256e-05,
      "loss": 1.6175,
      "step": 78250
    },
    {
      "epoch": 39.806714140386575,
      "grad_norm": 54.84103775024414,
      "learning_rate": 1.0193285859613428e-05,
      "loss": 1.5227,
      "step": 78260
    },
    {
      "epoch": 39.8118006103764,
      "grad_norm": 36.4128303527832,
      "learning_rate": 1.0188199389623601e-05,
      "loss": 1.6073,
      "step": 78270
    },
    {
      "epoch": 39.81688708036623,
      "grad_norm": 43.166603088378906,
      "learning_rate": 1.0183112919633775e-05,
      "loss": 1.4955,
      "step": 78280
    },
    {
      "epoch": 39.821973550356056,
      "grad_norm": 30.14174461364746,
      "learning_rate": 1.0178026449643948e-05,
      "loss": 1.6066,
      "step": 78290
    },
    {
      "epoch": 39.82706002034588,
      "grad_norm": 40.386566162109375,
      "learning_rate": 1.0172939979654121e-05,
      "loss": 1.6061,
      "step": 78300
    },
    {
      "epoch": 39.83214649033571,
      "grad_norm": 41.815589904785156,
      "learning_rate": 1.0167853509664293e-05,
      "loss": 1.6186,
      "step": 78310
    },
    {
      "epoch": 39.83723296032554,
      "grad_norm": 42.48247528076172,
      "learning_rate": 1.0162767039674466e-05,
      "loss": 1.6304,
      "step": 78320
    },
    {
      "epoch": 39.842319430315364,
      "grad_norm": 38.98564147949219,
      "learning_rate": 1.015768056968464e-05,
      "loss": 1.5225,
      "step": 78330
    },
    {
      "epoch": 39.84740590030519,
      "grad_norm": 40.9623908996582,
      "learning_rate": 1.0152594099694813e-05,
      "loss": 1.6197,
      "step": 78340
    },
    {
      "epoch": 39.85249237029502,
      "grad_norm": 37.040042877197266,
      "learning_rate": 1.0147507629704986e-05,
      "loss": 1.5381,
      "step": 78350
    },
    {
      "epoch": 39.857578840284845,
      "grad_norm": 33.108245849609375,
      "learning_rate": 1.0142421159715159e-05,
      "loss": 1.5973,
      "step": 78360
    },
    {
      "epoch": 39.86266531027467,
      "grad_norm": 35.681236267089844,
      "learning_rate": 1.013733468972533e-05,
      "loss": 1.5961,
      "step": 78370
    },
    {
      "epoch": 39.8677517802645,
      "grad_norm": 44.781917572021484,
      "learning_rate": 1.0132248219735504e-05,
      "loss": 1.5904,
      "step": 78380
    },
    {
      "epoch": 39.872838250254325,
      "grad_norm": 41.821693420410156,
      "learning_rate": 1.0127161749745677e-05,
      "loss": 1.527,
      "step": 78390
    },
    {
      "epoch": 39.87792472024415,
      "grad_norm": 37.4482307434082,
      "learning_rate": 1.0122075279755849e-05,
      "loss": 1.5739,
      "step": 78400
    },
    {
      "epoch": 39.88301119023398,
      "grad_norm": 40.067684173583984,
      "learning_rate": 1.0116988809766024e-05,
      "loss": 1.604,
      "step": 78410
    },
    {
      "epoch": 39.888097660223806,
      "grad_norm": 37.054744720458984,
      "learning_rate": 1.0111902339776195e-05,
      "loss": 1.636,
      "step": 78420
    },
    {
      "epoch": 39.89318413021363,
      "grad_norm": 40.74773406982422,
      "learning_rate": 1.0106815869786369e-05,
      "loss": 1.6284,
      "step": 78430
    },
    {
      "epoch": 39.89827060020346,
      "grad_norm": 38.40162658691406,
      "learning_rate": 1.0101729399796542e-05,
      "loss": 1.6681,
      "step": 78440
    },
    {
      "epoch": 39.90335707019329,
      "grad_norm": 36.14625549316406,
      "learning_rate": 1.0096642929806714e-05,
      "loss": 1.6014,
      "step": 78450
    },
    {
      "epoch": 39.908443540183114,
      "grad_norm": 37.3924674987793,
      "learning_rate": 1.0091556459816889e-05,
      "loss": 1.6568,
      "step": 78460
    },
    {
      "epoch": 39.91353001017294,
      "grad_norm": 51.58484649658203,
      "learning_rate": 1.008646998982706e-05,
      "loss": 1.6962,
      "step": 78470
    },
    {
      "epoch": 39.91861648016277,
      "grad_norm": 43.023658752441406,
      "learning_rate": 1.0081383519837233e-05,
      "loss": 1.5715,
      "step": 78480
    },
    {
      "epoch": 39.923702950152595,
      "grad_norm": 41.52256774902344,
      "learning_rate": 1.0076297049847407e-05,
      "loss": 1.5342,
      "step": 78490
    },
    {
      "epoch": 39.92878942014242,
      "grad_norm": 38.57428741455078,
      "learning_rate": 1.0071210579857578e-05,
      "loss": 1.5708,
      "step": 78500
    },
    {
      "epoch": 39.93387589013225,
      "grad_norm": 38.571189880371094,
      "learning_rate": 1.0066124109867753e-05,
      "loss": 1.5649,
      "step": 78510
    },
    {
      "epoch": 39.938962360122076,
      "grad_norm": 58.71178436279297,
      "learning_rate": 1.0061037639877925e-05,
      "loss": 1.5524,
      "step": 78520
    },
    {
      "epoch": 39.9440488301119,
      "grad_norm": 35.948341369628906,
      "learning_rate": 1.0055951169888098e-05,
      "loss": 1.6446,
      "step": 78530
    },
    {
      "epoch": 39.94913530010173,
      "grad_norm": 34.23584747314453,
      "learning_rate": 1.0050864699898271e-05,
      "loss": 1.6431,
      "step": 78540
    },
    {
      "epoch": 39.95422177009156,
      "grad_norm": 36.25859451293945,
      "learning_rate": 1.0045778229908443e-05,
      "loss": 1.5897,
      "step": 78550
    },
    {
      "epoch": 39.959308240081384,
      "grad_norm": 34.43894958496094,
      "learning_rate": 1.0040691759918618e-05,
      "loss": 1.5282,
      "step": 78560
    },
    {
      "epoch": 39.96439471007121,
      "grad_norm": 41.34760665893555,
      "learning_rate": 1.003560528992879e-05,
      "loss": 1.5345,
      "step": 78570
    },
    {
      "epoch": 39.96948118006104,
      "grad_norm": 38.623077392578125,
      "learning_rate": 1.0030518819938963e-05,
      "loss": 1.6103,
      "step": 78580
    },
    {
      "epoch": 39.974567650050865,
      "grad_norm": 52.39212417602539,
      "learning_rate": 1.0025432349949136e-05,
      "loss": 1.6023,
      "step": 78590
    },
    {
      "epoch": 39.97965412004069,
      "grad_norm": 36.130619049072266,
      "learning_rate": 1.002034587995931e-05,
      "loss": 1.5519,
      "step": 78600
    },
    {
      "epoch": 39.98474059003052,
      "grad_norm": 33.73444366455078,
      "learning_rate": 1.0015259409969481e-05,
      "loss": 1.5085,
      "step": 78610
    },
    {
      "epoch": 39.989827060020346,
      "grad_norm": 39.24897766113281,
      "learning_rate": 1.0010172939979654e-05,
      "loss": 1.6341,
      "step": 78620
    },
    {
      "epoch": 39.99491353001017,
      "grad_norm": 40.198585510253906,
      "learning_rate": 1.0005086469989828e-05,
      "loss": 1.6071,
      "step": 78630
    },
    {
      "epoch": 40.0,
      "grad_norm": 50.092742919921875,
      "learning_rate": 1e-05,
      "loss": 1.5867,
      "step": 78640
    },
    {
      "epoch": 40.0,
      "eval_loss": 5.016186714172363,
      "eval_runtime": 2.7455,
      "eval_samples_per_second": 1010.752,
      "eval_steps_per_second": 126.39,
      "step": 78640
    },
    {
      "epoch": 40.00508646998983,
      "grad_norm": 47.3552131652832,
      "learning_rate": 9.994913530010174e-06,
      "loss": 1.5667,
      "step": 78650
    },
    {
      "epoch": 40.010172939979654,
      "grad_norm": 44.03990173339844,
      "learning_rate": 9.989827060020346e-06,
      "loss": 1.5714,
      "step": 78660
    },
    {
      "epoch": 40.01525940996948,
      "grad_norm": 35.189937591552734,
      "learning_rate": 9.984740590030519e-06,
      "loss": 1.6117,
      "step": 78670
    },
    {
      "epoch": 40.02034587995931,
      "grad_norm": 38.215579986572266,
      "learning_rate": 9.979654120040692e-06,
      "loss": 1.601,
      "step": 78680
    },
    {
      "epoch": 40.025432349949135,
      "grad_norm": 50.25175094604492,
      "learning_rate": 9.974567650050866e-06,
      "loss": 1.5418,
      "step": 78690
    },
    {
      "epoch": 40.03051881993896,
      "grad_norm": 41.27378463745117,
      "learning_rate": 9.969481180061039e-06,
      "loss": 1.6119,
      "step": 78700
    },
    {
      "epoch": 40.03560528992879,
      "grad_norm": 42.03886032104492,
      "learning_rate": 9.96439471007121e-06,
      "loss": 1.5288,
      "step": 78710
    },
    {
      "epoch": 40.040691759918616,
      "grad_norm": 34.96434020996094,
      "learning_rate": 9.959308240081384e-06,
      "loss": 1.6177,
      "step": 78720
    },
    {
      "epoch": 40.04577822990844,
      "grad_norm": 63.54673767089844,
      "learning_rate": 9.954221770091557e-06,
      "loss": 1.6727,
      "step": 78730
    },
    {
      "epoch": 40.05086469989827,
      "grad_norm": 41.0985221862793,
      "learning_rate": 9.949135300101729e-06,
      "loss": 1.6048,
      "step": 78740
    },
    {
      "epoch": 40.0559511698881,
      "grad_norm": 40.00837326049805,
      "learning_rate": 9.944048830111904e-06,
      "loss": 1.5507,
      "step": 78750
    },
    {
      "epoch": 40.061037639877924,
      "grad_norm": 42.10939407348633,
      "learning_rate": 9.938962360122075e-06,
      "loss": 1.5916,
      "step": 78760
    },
    {
      "epoch": 40.06612410986775,
      "grad_norm": 38.20819091796875,
      "learning_rate": 9.93387589013225e-06,
      "loss": 1.5237,
      "step": 78770
    },
    {
      "epoch": 40.07121057985758,
      "grad_norm": 39.23701095581055,
      "learning_rate": 9.928789420142422e-06,
      "loss": 1.51,
      "step": 78780
    },
    {
      "epoch": 40.076297049847405,
      "grad_norm": 47.38991928100586,
      "learning_rate": 9.923702950152593e-06,
      "loss": 1.5287,
      "step": 78790
    },
    {
      "epoch": 40.08138351983723,
      "grad_norm": 34.67472839355469,
      "learning_rate": 9.918616480162768e-06,
      "loss": 1.6385,
      "step": 78800
    },
    {
      "epoch": 40.08646998982706,
      "grad_norm": 36.57542037963867,
      "learning_rate": 9.91353001017294e-06,
      "loss": 1.6449,
      "step": 78810
    },
    {
      "epoch": 40.091556459816886,
      "grad_norm": 40.89065933227539,
      "learning_rate": 9.908443540183113e-06,
      "loss": 1.5376,
      "step": 78820
    },
    {
      "epoch": 40.09664292980671,
      "grad_norm": 43.96721267700195,
      "learning_rate": 9.903357070193286e-06,
      "loss": 1.5883,
      "step": 78830
    },
    {
      "epoch": 40.10172939979654,
      "grad_norm": 43.342124938964844,
      "learning_rate": 9.89827060020346e-06,
      "loss": 1.479,
      "step": 78840
    },
    {
      "epoch": 40.10681586978637,
      "grad_norm": 51.695594787597656,
      "learning_rate": 9.893184130213633e-06,
      "loss": 1.595,
      "step": 78850
    },
    {
      "epoch": 40.111902339776194,
      "grad_norm": 53.87610626220703,
      "learning_rate": 9.888097660223805e-06,
      "loss": 1.6436,
      "step": 78860
    },
    {
      "epoch": 40.11698880976602,
      "grad_norm": 36.61774826049805,
      "learning_rate": 9.883011190233978e-06,
      "loss": 1.697,
      "step": 78870
    },
    {
      "epoch": 40.12207527975585,
      "grad_norm": 41.05662155151367,
      "learning_rate": 9.877924720244151e-06,
      "loss": 1.5904,
      "step": 78880
    },
    {
      "epoch": 40.127161749745675,
      "grad_norm": 39.09761047363281,
      "learning_rate": 9.872838250254324e-06,
      "loss": 1.5349,
      "step": 78890
    },
    {
      "epoch": 40.1322482197355,
      "grad_norm": 45.203407287597656,
      "learning_rate": 9.867751780264498e-06,
      "loss": 1.6025,
      "step": 78900
    },
    {
      "epoch": 40.13733468972533,
      "grad_norm": 35.68452072143555,
      "learning_rate": 9.86266531027467e-06,
      "loss": 1.5954,
      "step": 78910
    },
    {
      "epoch": 40.142421159715155,
      "grad_norm": 36.67466735839844,
      "learning_rate": 9.857578840284843e-06,
      "loss": 1.6734,
      "step": 78920
    },
    {
      "epoch": 40.14750762970498,
      "grad_norm": 37.63294982910156,
      "learning_rate": 9.852492370295016e-06,
      "loss": 1.6443,
      "step": 78930
    },
    {
      "epoch": 40.15259409969481,
      "grad_norm": 39.63985061645508,
      "learning_rate": 9.847405900305189e-06,
      "loss": 1.6592,
      "step": 78940
    },
    {
      "epoch": 40.157680569684636,
      "grad_norm": 36.5694580078125,
      "learning_rate": 9.84231943031536e-06,
      "loss": 1.6304,
      "step": 78950
    },
    {
      "epoch": 40.16276703967446,
      "grad_norm": 39.343544006347656,
      "learning_rate": 9.837232960325534e-06,
      "loss": 1.6051,
      "step": 78960
    },
    {
      "epoch": 40.16785350966429,
      "grad_norm": 37.237945556640625,
      "learning_rate": 9.832146490335707e-06,
      "loss": 1.6176,
      "step": 78970
    },
    {
      "epoch": 40.17293997965412,
      "grad_norm": 43.02674102783203,
      "learning_rate": 9.82706002034588e-06,
      "loss": 1.6027,
      "step": 78980
    },
    {
      "epoch": 40.178026449643944,
      "grad_norm": 38.37127685546875,
      "learning_rate": 9.821973550356054e-06,
      "loss": 1.5435,
      "step": 78990
    },
    {
      "epoch": 40.18311291963377,
      "grad_norm": 39.02693557739258,
      "learning_rate": 9.816887080366225e-06,
      "loss": 1.5837,
      "step": 79000
    },
    {
      "epoch": 40.1881993896236,
      "grad_norm": 38.5979118347168,
      "learning_rate": 9.8118006103764e-06,
      "loss": 1.4892,
      "step": 79010
    },
    {
      "epoch": 40.193285859613425,
      "grad_norm": 34.22773361206055,
      "learning_rate": 9.806714140386572e-06,
      "loss": 1.5606,
      "step": 79020
    },
    {
      "epoch": 40.19837232960325,
      "grad_norm": 43.28508377075195,
      "learning_rate": 9.801627670396745e-06,
      "loss": 1.6168,
      "step": 79030
    },
    {
      "epoch": 40.20345879959308,
      "grad_norm": 39.180721282958984,
      "learning_rate": 9.796541200406919e-06,
      "loss": 1.5898,
      "step": 79040
    },
    {
      "epoch": 40.208545269582906,
      "grad_norm": 35.372154235839844,
      "learning_rate": 9.79145473041709e-06,
      "loss": 1.5453,
      "step": 79050
    },
    {
      "epoch": 40.21363173957273,
      "grad_norm": 45.526329040527344,
      "learning_rate": 9.786368260427265e-06,
      "loss": 1.5879,
      "step": 79060
    },
    {
      "epoch": 40.21871820956256,
      "grad_norm": 37.384925842285156,
      "learning_rate": 9.781281790437437e-06,
      "loss": 1.5926,
      "step": 79070
    },
    {
      "epoch": 40.22380467955239,
      "grad_norm": 42.085880279541016,
      "learning_rate": 9.77619532044761e-06,
      "loss": 1.5426,
      "step": 79080
    },
    {
      "epoch": 40.22889114954222,
      "grad_norm": 43.03842544555664,
      "learning_rate": 9.771108850457783e-06,
      "loss": 1.5782,
      "step": 79090
    },
    {
      "epoch": 40.23397761953205,
      "grad_norm": 37.67443084716797,
      "learning_rate": 9.766022380467955e-06,
      "loss": 1.574,
      "step": 79100
    },
    {
      "epoch": 40.239064089521875,
      "grad_norm": 39.74253845214844,
      "learning_rate": 9.76093591047813e-06,
      "loss": 1.6201,
      "step": 79110
    },
    {
      "epoch": 40.2441505595117,
      "grad_norm": 38.140262603759766,
      "learning_rate": 9.755849440488301e-06,
      "loss": 1.5932,
      "step": 79120
    },
    {
      "epoch": 40.24923702950153,
      "grad_norm": 41.19241714477539,
      "learning_rate": 9.750762970498475e-06,
      "loss": 1.5557,
      "step": 79130
    },
    {
      "epoch": 40.254323499491356,
      "grad_norm": 32.675716400146484,
      "learning_rate": 9.745676500508648e-06,
      "loss": 1.6182,
      "step": 79140
    },
    {
      "epoch": 40.25940996948118,
      "grad_norm": 41.17835235595703,
      "learning_rate": 9.74059003051882e-06,
      "loss": 1.5454,
      "step": 79150
    },
    {
      "epoch": 40.26449643947101,
      "grad_norm": 36.78458023071289,
      "learning_rate": 9.735503560528995e-06,
      "loss": 1.5891,
      "step": 79160
    },
    {
      "epoch": 40.26958290946084,
      "grad_norm": 46.003753662109375,
      "learning_rate": 9.730417090539166e-06,
      "loss": 1.5834,
      "step": 79170
    },
    {
      "epoch": 40.274669379450664,
      "grad_norm": 37.88734817504883,
      "learning_rate": 9.72533062054934e-06,
      "loss": 1.5919,
      "step": 79180
    },
    {
      "epoch": 40.27975584944049,
      "grad_norm": 49.336490631103516,
      "learning_rate": 9.720244150559513e-06,
      "loss": 1.5421,
      "step": 79190
    },
    {
      "epoch": 40.28484231943032,
      "grad_norm": 33.21005630493164,
      "learning_rate": 9.715157680569684e-06,
      "loss": 1.5687,
      "step": 79200
    },
    {
      "epoch": 40.289928789420145,
      "grad_norm": 48.130470275878906,
      "learning_rate": 9.710071210579858e-06,
      "loss": 1.5102,
      "step": 79210
    },
    {
      "epoch": 40.29501525940997,
      "grad_norm": 46.073970794677734,
      "learning_rate": 9.70498474059003e-06,
      "loss": 1.5777,
      "step": 79220
    },
    {
      "epoch": 40.3001017293998,
      "grad_norm": 36.39727020263672,
      "learning_rate": 9.699898270600204e-06,
      "loss": 1.6263,
      "step": 79230
    },
    {
      "epoch": 40.305188199389626,
      "grad_norm": 50.16539001464844,
      "learning_rate": 9.694811800610377e-06,
      "loss": 1.6692,
      "step": 79240
    },
    {
      "epoch": 40.31027466937945,
      "grad_norm": 38.18470764160156,
      "learning_rate": 9.68972533062055e-06,
      "loss": 1.5856,
      "step": 79250
    },
    {
      "epoch": 40.31536113936928,
      "grad_norm": 42.4177131652832,
      "learning_rate": 9.684638860630722e-06,
      "loss": 1.6253,
      "step": 79260
    },
    {
      "epoch": 40.32044760935911,
      "grad_norm": 44.86607360839844,
      "learning_rate": 9.679552390640896e-06,
      "loss": 1.5224,
      "step": 79270
    },
    {
      "epoch": 40.325534079348934,
      "grad_norm": 38.816646575927734,
      "learning_rate": 9.674465920651069e-06,
      "loss": 1.5755,
      "step": 79280
    },
    {
      "epoch": 40.33062054933876,
      "grad_norm": 40.841590881347656,
      "learning_rate": 9.66937945066124e-06,
      "loss": 1.5596,
      "step": 79290
    },
    {
      "epoch": 40.33570701932859,
      "grad_norm": 37.910423278808594,
      "learning_rate": 9.664292980671415e-06,
      "loss": 1.5853,
      "step": 79300
    },
    {
      "epoch": 40.340793489318415,
      "grad_norm": 38.60456085205078,
      "learning_rate": 9.659206510681587e-06,
      "loss": 1.6506,
      "step": 79310
    },
    {
      "epoch": 40.34587995930824,
      "grad_norm": 41.42513656616211,
      "learning_rate": 9.65412004069176e-06,
      "loss": 1.5701,
      "step": 79320
    },
    {
      "epoch": 40.35096642929807,
      "grad_norm": 29.690969467163086,
      "learning_rate": 9.649033570701934e-06,
      "loss": 1.5887,
      "step": 79330
    },
    {
      "epoch": 40.356052899287896,
      "grad_norm": 45.41038513183594,
      "learning_rate": 9.643947100712105e-06,
      "loss": 1.5373,
      "step": 79340
    },
    {
      "epoch": 40.36113936927772,
      "grad_norm": 37.84324264526367,
      "learning_rate": 9.63886063072228e-06,
      "loss": 1.5917,
      "step": 79350
    },
    {
      "epoch": 40.36622583926755,
      "grad_norm": 41.25737380981445,
      "learning_rate": 9.633774160732452e-06,
      "loss": 1.5038,
      "step": 79360
    },
    {
      "epoch": 40.37131230925738,
      "grad_norm": 40.58866500854492,
      "learning_rate": 9.628687690742625e-06,
      "loss": 1.5629,
      "step": 79370
    },
    {
      "epoch": 40.376398779247204,
      "grad_norm": 39.8932991027832,
      "learning_rate": 9.623601220752798e-06,
      "loss": 1.5913,
      "step": 79380
    },
    {
      "epoch": 40.38148524923703,
      "grad_norm": 36.138816833496094,
      "learning_rate": 9.61851475076297e-06,
      "loss": 1.5734,
      "step": 79390
    },
    {
      "epoch": 40.38657171922686,
      "grad_norm": 44.448883056640625,
      "learning_rate": 9.613428280773145e-06,
      "loss": 1.6056,
      "step": 79400
    },
    {
      "epoch": 40.391658189216685,
      "grad_norm": 40.47529220581055,
      "learning_rate": 9.608341810783316e-06,
      "loss": 1.6465,
      "step": 79410
    },
    {
      "epoch": 40.39674465920651,
      "grad_norm": 49.92434310913086,
      "learning_rate": 9.60325534079349e-06,
      "loss": 1.5866,
      "step": 79420
    },
    {
      "epoch": 40.40183112919634,
      "grad_norm": 38.743370056152344,
      "learning_rate": 9.598168870803663e-06,
      "loss": 1.6557,
      "step": 79430
    },
    {
      "epoch": 40.406917599186166,
      "grad_norm": 45.98167419433594,
      "learning_rate": 9.593082400813835e-06,
      "loss": 1.438,
      "step": 79440
    },
    {
      "epoch": 40.41200406917599,
      "grad_norm": 44.436092376708984,
      "learning_rate": 9.58799593082401e-06,
      "loss": 1.6151,
      "step": 79450
    },
    {
      "epoch": 40.41709053916582,
      "grad_norm": 37.87992858886719,
      "learning_rate": 9.582909460834181e-06,
      "loss": 1.5774,
      "step": 79460
    },
    {
      "epoch": 40.42217700915565,
      "grad_norm": 35.35828399658203,
      "learning_rate": 9.577822990844354e-06,
      "loss": 1.5234,
      "step": 79470
    },
    {
      "epoch": 40.42726347914547,
      "grad_norm": 32.73310470581055,
      "learning_rate": 9.572736520854528e-06,
      "loss": 1.5963,
      "step": 79480
    },
    {
      "epoch": 40.4323499491353,
      "grad_norm": 43.66289138793945,
      "learning_rate": 9.567650050864701e-06,
      "loss": 1.6009,
      "step": 79490
    },
    {
      "epoch": 40.43743641912513,
      "grad_norm": 36.44292449951172,
      "learning_rate": 9.562563580874874e-06,
      "loss": 1.5892,
      "step": 79500
    },
    {
      "epoch": 40.442522889114954,
      "grad_norm": 42.98191833496094,
      "learning_rate": 9.557477110885046e-06,
      "loss": 1.5616,
      "step": 79510
    },
    {
      "epoch": 40.44760935910478,
      "grad_norm": 44.29327392578125,
      "learning_rate": 9.552390640895219e-06,
      "loss": 1.5539,
      "step": 79520
    },
    {
      "epoch": 40.45269582909461,
      "grad_norm": 35.78367614746094,
      "learning_rate": 9.547304170905392e-06,
      "loss": 1.6667,
      "step": 79530
    },
    {
      "epoch": 40.457782299084435,
      "grad_norm": 41.12353515625,
      "learning_rate": 9.542217700915566e-06,
      "loss": 1.5829,
      "step": 79540
    },
    {
      "epoch": 40.46286876907426,
      "grad_norm": 36.154388427734375,
      "learning_rate": 9.537131230925737e-06,
      "loss": 1.5906,
      "step": 79550
    },
    {
      "epoch": 40.46795523906409,
      "grad_norm": 35.928489685058594,
      "learning_rate": 9.53204476093591e-06,
      "loss": 1.5804,
      "step": 79560
    },
    {
      "epoch": 40.473041709053916,
      "grad_norm": 57.65235900878906,
      "learning_rate": 9.526958290946084e-06,
      "loss": 1.4996,
      "step": 79570
    },
    {
      "epoch": 40.47812817904374,
      "grad_norm": 42.28546142578125,
      "learning_rate": 9.521871820956257e-06,
      "loss": 1.586,
      "step": 79580
    },
    {
      "epoch": 40.48321464903357,
      "grad_norm": 50.15138244628906,
      "learning_rate": 9.51678535096643e-06,
      "loss": 1.6227,
      "step": 79590
    },
    {
      "epoch": 40.4883011190234,
      "grad_norm": 45.329742431640625,
      "learning_rate": 9.511698880976602e-06,
      "loss": 1.6711,
      "step": 79600
    },
    {
      "epoch": 40.493387589013224,
      "grad_norm": 36.39295959472656,
      "learning_rate": 9.506612410986775e-06,
      "loss": 1.5596,
      "step": 79610
    },
    {
      "epoch": 40.49847405900305,
      "grad_norm": 54.5805778503418,
      "learning_rate": 9.501525940996949e-06,
      "loss": 1.5986,
      "step": 79620
    },
    {
      "epoch": 40.50356052899288,
      "grad_norm": 43.79706573486328,
      "learning_rate": 9.49643947100712e-06,
      "loss": 1.6149,
      "step": 79630
    },
    {
      "epoch": 40.508646998982705,
      "grad_norm": 47.18581771850586,
      "learning_rate": 9.491353001017295e-06,
      "loss": 1.6264,
      "step": 79640
    },
    {
      "epoch": 40.51373346897253,
      "grad_norm": 38.67958068847656,
      "learning_rate": 9.486266531027467e-06,
      "loss": 1.5286,
      "step": 79650
    },
    {
      "epoch": 40.51881993896236,
      "grad_norm": 33.8104248046875,
      "learning_rate": 9.481180061037642e-06,
      "loss": 1.5576,
      "step": 79660
    },
    {
      "epoch": 40.523906408952186,
      "grad_norm": 40.05754852294922,
      "learning_rate": 9.476093591047813e-06,
      "loss": 1.6198,
      "step": 79670
    },
    {
      "epoch": 40.52899287894201,
      "grad_norm": 41.766319274902344,
      "learning_rate": 9.471007121057985e-06,
      "loss": 1.5563,
      "step": 79680
    },
    {
      "epoch": 40.53407934893184,
      "grad_norm": 36.832244873046875,
      "learning_rate": 9.46592065106816e-06,
      "loss": 1.5469,
      "step": 79690
    },
    {
      "epoch": 40.53916581892167,
      "grad_norm": 38.309513092041016,
      "learning_rate": 9.460834181078331e-06,
      "loss": 1.6766,
      "step": 79700
    },
    {
      "epoch": 40.544252288911494,
      "grad_norm": 34.77119064331055,
      "learning_rate": 9.455747711088506e-06,
      "loss": 1.5619,
      "step": 79710
    },
    {
      "epoch": 40.54933875890132,
      "grad_norm": 34.27770233154297,
      "learning_rate": 9.450661241098678e-06,
      "loss": 1.5703,
      "step": 79720
    },
    {
      "epoch": 40.55442522889115,
      "grad_norm": 36.116886138916016,
      "learning_rate": 9.445574771108851e-06,
      "loss": 1.6725,
      "step": 79730
    },
    {
      "epoch": 40.559511698880975,
      "grad_norm": 43.82825469970703,
      "learning_rate": 9.440488301119025e-06,
      "loss": 1.617,
      "step": 79740
    },
    {
      "epoch": 40.5645981688708,
      "grad_norm": 39.50017166137695,
      "learning_rate": 9.435401831129196e-06,
      "loss": 1.5995,
      "step": 79750
    },
    {
      "epoch": 40.56968463886063,
      "grad_norm": 38.860389709472656,
      "learning_rate": 9.43031536113937e-06,
      "loss": 1.5738,
      "step": 79760
    },
    {
      "epoch": 40.574771108850456,
      "grad_norm": 42.13882827758789,
      "learning_rate": 9.425228891149543e-06,
      "loss": 1.5033,
      "step": 79770
    },
    {
      "epoch": 40.57985757884028,
      "grad_norm": 42.88705062866211,
      "learning_rate": 9.420142421159716e-06,
      "loss": 1.5201,
      "step": 79780
    },
    {
      "epoch": 40.58494404883011,
      "grad_norm": 30.375974655151367,
      "learning_rate": 9.41505595116989e-06,
      "loss": 1.5741,
      "step": 79790
    },
    {
      "epoch": 40.59003051881994,
      "grad_norm": 45.76857376098633,
      "learning_rate": 9.40996948118006e-06,
      "loss": 1.5494,
      "step": 79800
    },
    {
      "epoch": 40.595116988809764,
      "grad_norm": 41.767024993896484,
      "learning_rate": 9.404883011190234e-06,
      "loss": 1.5207,
      "step": 79810
    },
    {
      "epoch": 40.60020345879959,
      "grad_norm": 42.95218276977539,
      "learning_rate": 9.399796541200407e-06,
      "loss": 1.5885,
      "step": 79820
    },
    {
      "epoch": 40.60528992878942,
      "grad_norm": 47.80046463012695,
      "learning_rate": 9.39471007121058e-06,
      "loss": 1.6302,
      "step": 79830
    },
    {
      "epoch": 40.610376398779245,
      "grad_norm": 40.22829055786133,
      "learning_rate": 9.389623601220754e-06,
      "loss": 1.5092,
      "step": 79840
    },
    {
      "epoch": 40.61546286876907,
      "grad_norm": 39.11659240722656,
      "learning_rate": 9.384537131230926e-06,
      "loss": 1.5749,
      "step": 79850
    },
    {
      "epoch": 40.6205493387589,
      "grad_norm": 41.848716735839844,
      "learning_rate": 9.379450661241099e-06,
      "loss": 1.6235,
      "step": 79860
    },
    {
      "epoch": 40.625635808748726,
      "grad_norm": 49.9570426940918,
      "learning_rate": 9.374364191251272e-06,
      "loss": 1.5618,
      "step": 79870
    },
    {
      "epoch": 40.63072227873855,
      "grad_norm": 45.959041595458984,
      "learning_rate": 9.369277721261445e-06,
      "loss": 1.5662,
      "step": 79880
    },
    {
      "epoch": 40.63580874872838,
      "grad_norm": 42.14970779418945,
      "learning_rate": 9.364191251271617e-06,
      "loss": 1.5988,
      "step": 79890
    },
    {
      "epoch": 40.64089521871821,
      "grad_norm": 41.58266830444336,
      "learning_rate": 9.359104781281792e-06,
      "loss": 1.5316,
      "step": 79900
    },
    {
      "epoch": 40.645981688708034,
      "grad_norm": 41.360557556152344,
      "learning_rate": 9.354018311291964e-06,
      "loss": 1.603,
      "step": 79910
    },
    {
      "epoch": 40.65106815869786,
      "grad_norm": 37.711734771728516,
      "learning_rate": 9.348931841302137e-06,
      "loss": 1.5869,
      "step": 79920
    },
    {
      "epoch": 40.65615462868769,
      "grad_norm": 43.468421936035156,
      "learning_rate": 9.34384537131231e-06,
      "loss": 1.5403,
      "step": 79930
    },
    {
      "epoch": 40.661241098677515,
      "grad_norm": 37.845977783203125,
      "learning_rate": 9.338758901322482e-06,
      "loss": 1.6292,
      "step": 79940
    },
    {
      "epoch": 40.66632756866734,
      "grad_norm": 48.87345504760742,
      "learning_rate": 9.333672431332657e-06,
      "loss": 1.573,
      "step": 79950
    },
    {
      "epoch": 40.67141403865717,
      "grad_norm": 39.05113220214844,
      "learning_rate": 9.328585961342828e-06,
      "loss": 1.5345,
      "step": 79960
    },
    {
      "epoch": 40.676500508646996,
      "grad_norm": 38.03004455566406,
      "learning_rate": 9.323499491353002e-06,
      "loss": 1.5962,
      "step": 79970
    },
    {
      "epoch": 40.68158697863683,
      "grad_norm": 35.78755187988281,
      "learning_rate": 9.318413021363175e-06,
      "loss": 1.6364,
      "step": 79980
    },
    {
      "epoch": 40.68667344862666,
      "grad_norm": 37.71561050415039,
      "learning_rate": 9.313326551373346e-06,
      "loss": 1.5951,
      "step": 79990
    },
    {
      "epoch": 40.691759918616484,
      "grad_norm": 36.022769927978516,
      "learning_rate": 9.308240081383521e-06,
      "loss": 1.6436,
      "step": 80000
    },
    {
      "epoch": 40.69684638860631,
      "grad_norm": 39.06837463378906,
      "learning_rate": 9.303153611393693e-06,
      "loss": 1.5644,
      "step": 80010
    },
    {
      "epoch": 40.70193285859614,
      "grad_norm": 47.734100341796875,
      "learning_rate": 9.298067141403866e-06,
      "loss": 1.6059,
      "step": 80020
    },
    {
      "epoch": 40.707019328585965,
      "grad_norm": 49.46630859375,
      "learning_rate": 9.29298067141404e-06,
      "loss": 1.5218,
      "step": 80030
    },
    {
      "epoch": 40.71210579857579,
      "grad_norm": 36.52177047729492,
      "learning_rate": 9.287894201424211e-06,
      "loss": 1.6828,
      "step": 80040
    },
    {
      "epoch": 40.71719226856562,
      "grad_norm": 50.70021438598633,
      "learning_rate": 9.282807731434386e-06,
      "loss": 1.6154,
      "step": 80050
    },
    {
      "epoch": 40.722278738555445,
      "grad_norm": 42.20299530029297,
      "learning_rate": 9.277721261444558e-06,
      "loss": 1.6532,
      "step": 80060
    },
    {
      "epoch": 40.72736520854527,
      "grad_norm": 33.80137634277344,
      "learning_rate": 9.272634791454731e-06,
      "loss": 1.5151,
      "step": 80070
    },
    {
      "epoch": 40.7324516785351,
      "grad_norm": 40.03190612792969,
      "learning_rate": 9.267548321464904e-06,
      "loss": 1.5957,
      "step": 80080
    },
    {
      "epoch": 40.737538148524926,
      "grad_norm": 46.606082916259766,
      "learning_rate": 9.262461851475076e-06,
      "loss": 1.5253,
      "step": 80090
    },
    {
      "epoch": 40.74262461851475,
      "grad_norm": 36.35344696044922,
      "learning_rate": 9.257375381485249e-06,
      "loss": 1.5153,
      "step": 80100
    },
    {
      "epoch": 40.74771108850458,
      "grad_norm": 48.75804901123047,
      "learning_rate": 9.252288911495422e-06,
      "loss": 1.6596,
      "step": 80110
    },
    {
      "epoch": 40.75279755849441,
      "grad_norm": 47.89812469482422,
      "learning_rate": 9.247202441505596e-06,
      "loss": 1.5664,
      "step": 80120
    },
    {
      "epoch": 40.757884028484234,
      "grad_norm": 33.30125427246094,
      "learning_rate": 9.242115971515769e-06,
      "loss": 1.5153,
      "step": 80130
    },
    {
      "epoch": 40.76297049847406,
      "grad_norm": 48.435577392578125,
      "learning_rate": 9.237029501525942e-06,
      "loss": 1.607,
      "step": 80140
    },
    {
      "epoch": 40.76805696846389,
      "grad_norm": 42.47274398803711,
      "learning_rate": 9.231943031536114e-06,
      "loss": 1.5038,
      "step": 80150
    },
    {
      "epoch": 40.773143438453715,
      "grad_norm": 34.48115158081055,
      "learning_rate": 9.226856561546287e-06,
      "loss": 1.5053,
      "step": 80160
    },
    {
      "epoch": 40.77822990844354,
      "grad_norm": 37.58723831176758,
      "learning_rate": 9.22177009155646e-06,
      "loss": 1.6222,
      "step": 80170
    },
    {
      "epoch": 40.78331637843337,
      "grad_norm": 46.413150787353516,
      "learning_rate": 9.216683621566634e-06,
      "loss": 1.648,
      "step": 80180
    },
    {
      "epoch": 40.788402848423196,
      "grad_norm": 44.451698303222656,
      "learning_rate": 9.211597151576807e-06,
      "loss": 1.6146,
      "step": 80190
    },
    {
      "epoch": 40.79348931841302,
      "grad_norm": 38.466373443603516,
      "learning_rate": 9.206510681586979e-06,
      "loss": 1.5514,
      "step": 80200
    },
    {
      "epoch": 40.79857578840285,
      "grad_norm": 45.23573684692383,
      "learning_rate": 9.201424211597152e-06,
      "loss": 1.6189,
      "step": 80210
    },
    {
      "epoch": 40.80366225839268,
      "grad_norm": 39.2885627746582,
      "learning_rate": 9.196337741607325e-06,
      "loss": 1.6249,
      "step": 80220
    },
    {
      "epoch": 40.808748728382504,
      "grad_norm": 57.288692474365234,
      "learning_rate": 9.191251271617497e-06,
      "loss": 1.5549,
      "step": 80230
    },
    {
      "epoch": 40.81383519837233,
      "grad_norm": 36.07501983642578,
      "learning_rate": 9.186164801627672e-06,
      "loss": 1.4649,
      "step": 80240
    },
    {
      "epoch": 40.81892166836216,
      "grad_norm": 45.115631103515625,
      "learning_rate": 9.181078331637843e-06,
      "loss": 1.5204,
      "step": 80250
    },
    {
      "epoch": 40.824008138351985,
      "grad_norm": 40.78061294555664,
      "learning_rate": 9.175991861648017e-06,
      "loss": 1.5593,
      "step": 80260
    },
    {
      "epoch": 40.82909460834181,
      "grad_norm": 48.207767486572266,
      "learning_rate": 9.17090539165819e-06,
      "loss": 1.5629,
      "step": 80270
    },
    {
      "epoch": 40.83418107833164,
      "grad_norm": 43.34372329711914,
      "learning_rate": 9.165818921668361e-06,
      "loss": 1.589,
      "step": 80280
    },
    {
      "epoch": 40.839267548321466,
      "grad_norm": 44.015899658203125,
      "learning_rate": 9.160732451678536e-06,
      "loss": 1.626,
      "step": 80290
    },
    {
      "epoch": 40.84435401831129,
      "grad_norm": 37.111568450927734,
      "learning_rate": 9.155645981688708e-06,
      "loss": 1.5182,
      "step": 80300
    },
    {
      "epoch": 40.84944048830112,
      "grad_norm": 42.59583282470703,
      "learning_rate": 9.150559511698881e-06,
      "loss": 1.5612,
      "step": 80310
    },
    {
      "epoch": 40.85452695829095,
      "grad_norm": 48.01546859741211,
      "learning_rate": 9.145473041709055e-06,
      "loss": 1.6475,
      "step": 80320
    },
    {
      "epoch": 40.859613428280774,
      "grad_norm": 40.19955825805664,
      "learning_rate": 9.140386571719226e-06,
      "loss": 1.5253,
      "step": 80330
    },
    {
      "epoch": 40.8646998982706,
      "grad_norm": 35.76896286010742,
      "learning_rate": 9.135300101729401e-06,
      "loss": 1.6952,
      "step": 80340
    },
    {
      "epoch": 40.86978636826043,
      "grad_norm": 37.761131286621094,
      "learning_rate": 9.130213631739573e-06,
      "loss": 1.479,
      "step": 80350
    },
    {
      "epoch": 40.874872838250255,
      "grad_norm": 42.37215042114258,
      "learning_rate": 9.125127161749746e-06,
      "loss": 1.6513,
      "step": 80360
    },
    {
      "epoch": 40.87995930824008,
      "grad_norm": 33.0274772644043,
      "learning_rate": 9.12004069175992e-06,
      "loss": 1.5394,
      "step": 80370
    },
    {
      "epoch": 40.88504577822991,
      "grad_norm": 35.40464782714844,
      "learning_rate": 9.114954221770093e-06,
      "loss": 1.555,
      "step": 80380
    },
    {
      "epoch": 40.890132248219736,
      "grad_norm": 40.22603225708008,
      "learning_rate": 9.109867751780266e-06,
      "loss": 1.6036,
      "step": 80390
    },
    {
      "epoch": 40.89521871820956,
      "grad_norm": 40.02986145019531,
      "learning_rate": 9.104781281790437e-06,
      "loss": 1.6717,
      "step": 80400
    },
    {
      "epoch": 40.90030518819939,
      "grad_norm": 39.443660736083984,
      "learning_rate": 9.09969481180061e-06,
      "loss": 1.5439,
      "step": 80410
    },
    {
      "epoch": 40.90539165818922,
      "grad_norm": 46.80329513549805,
      "learning_rate": 9.094608341810784e-06,
      "loss": 1.5267,
      "step": 80420
    },
    {
      "epoch": 40.910478128179044,
      "grad_norm": 36.7477912902832,
      "learning_rate": 9.089521871820957e-06,
      "loss": 1.5443,
      "step": 80430
    },
    {
      "epoch": 40.91556459816887,
      "grad_norm": 45.989593505859375,
      "learning_rate": 9.084435401831129e-06,
      "loss": 1.5509,
      "step": 80440
    },
    {
      "epoch": 40.9206510681587,
      "grad_norm": 39.56972885131836,
      "learning_rate": 9.079348931841302e-06,
      "loss": 1.5339,
      "step": 80450
    },
    {
      "epoch": 40.925737538148525,
      "grad_norm": 44.79224395751953,
      "learning_rate": 9.074262461851475e-06,
      "loss": 1.4905,
      "step": 80460
    },
    {
      "epoch": 40.93082400813835,
      "grad_norm": 41.98232650756836,
      "learning_rate": 9.069175991861649e-06,
      "loss": 1.5924,
      "step": 80470
    },
    {
      "epoch": 40.93591047812818,
      "grad_norm": 43.662837982177734,
      "learning_rate": 9.064089521871822e-06,
      "loss": 1.5913,
      "step": 80480
    },
    {
      "epoch": 40.940996948118006,
      "grad_norm": 37.333675384521484,
      "learning_rate": 9.059003051881994e-06,
      "loss": 1.6509,
      "step": 80490
    },
    {
      "epoch": 40.94608341810783,
      "grad_norm": 44.164710998535156,
      "learning_rate": 9.053916581892167e-06,
      "loss": 1.658,
      "step": 80500
    },
    {
      "epoch": 40.95116988809766,
      "grad_norm": 44.303646087646484,
      "learning_rate": 9.04883011190234e-06,
      "loss": 1.5721,
      "step": 80510
    },
    {
      "epoch": 40.95625635808749,
      "grad_norm": 37.8338508605957,
      "learning_rate": 9.043743641912513e-06,
      "loss": 1.5155,
      "step": 80520
    },
    {
      "epoch": 40.96134282807731,
      "grad_norm": 38.082000732421875,
      "learning_rate": 9.038657171922687e-06,
      "loss": 1.4874,
      "step": 80530
    },
    {
      "epoch": 40.96642929806714,
      "grad_norm": 42.54790115356445,
      "learning_rate": 9.033570701932858e-06,
      "loss": 1.5753,
      "step": 80540
    },
    {
      "epoch": 40.97151576805697,
      "grad_norm": 42.4788703918457,
      "learning_rate": 9.028484231943032e-06,
      "loss": 1.5418,
      "step": 80550
    },
    {
      "epoch": 40.976602238046794,
      "grad_norm": 38.549583435058594,
      "learning_rate": 9.023397761953205e-06,
      "loss": 1.5611,
      "step": 80560
    },
    {
      "epoch": 40.98168870803662,
      "grad_norm": 36.551963806152344,
      "learning_rate": 9.018311291963376e-06,
      "loss": 1.5597,
      "step": 80570
    },
    {
      "epoch": 40.98677517802645,
      "grad_norm": 32.40705871582031,
      "learning_rate": 9.013224821973551e-06,
      "loss": 1.5899,
      "step": 80580
    },
    {
      "epoch": 40.991861648016275,
      "grad_norm": 53.830379486083984,
      "learning_rate": 9.008138351983723e-06,
      "loss": 1.6267,
      "step": 80590
    },
    {
      "epoch": 40.9969481180061,
      "grad_norm": 35.65609359741211,
      "learning_rate": 9.003051881993898e-06,
      "loss": 1.58,
      "step": 80600
    },
    {
      "epoch": 41.0,
      "eval_loss": 5.0234150886535645,
      "eval_runtime": 2.7892,
      "eval_samples_per_second": 994.918,
      "eval_steps_per_second": 124.41,
      "step": 80606
    },
    {
      "epoch": 41.00203458799593,
      "grad_norm": 42.77058792114258,
      "learning_rate": 8.99796541200407e-06,
      "loss": 1.5446,
      "step": 80610
    },
    {
      "epoch": 41.007121057985756,
      "grad_norm": 42.49536895751953,
      "learning_rate": 8.992878942014243e-06,
      "loss": 1.5656,
      "step": 80620
    },
    {
      "epoch": 41.01220752797558,
      "grad_norm": 50.54215621948242,
      "learning_rate": 8.987792472024416e-06,
      "loss": 1.639,
      "step": 80630
    },
    {
      "epoch": 41.01729399796541,
      "grad_norm": 32.82172775268555,
      "learning_rate": 8.982706002034588e-06,
      "loss": 1.6074,
      "step": 80640
    },
    {
      "epoch": 41.02238046795524,
      "grad_norm": 38.51063919067383,
      "learning_rate": 8.977619532044763e-06,
      "loss": 1.6281,
      "step": 80650
    },
    {
      "epoch": 41.027466937945064,
      "grad_norm": 44.983482360839844,
      "learning_rate": 8.972533062054934e-06,
      "loss": 1.4899,
      "step": 80660
    },
    {
      "epoch": 41.03255340793489,
      "grad_norm": 41.27717971801758,
      "learning_rate": 8.967446592065108e-06,
      "loss": 1.5855,
      "step": 80670
    },
    {
      "epoch": 41.03763987792472,
      "grad_norm": 46.13063049316406,
      "learning_rate": 8.96236012207528e-06,
      "loss": 1.4671,
      "step": 80680
    },
    {
      "epoch": 41.042726347914545,
      "grad_norm": 45.5025634765625,
      "learning_rate": 8.957273652085452e-06,
      "loss": 1.5435,
      "step": 80690
    },
    {
      "epoch": 41.04781281790437,
      "grad_norm": 42.1672477722168,
      "learning_rate": 8.952187182095626e-06,
      "loss": 1.5442,
      "step": 80700
    },
    {
      "epoch": 41.0528992878942,
      "grad_norm": 39.19989776611328,
      "learning_rate": 8.947100712105799e-06,
      "loss": 1.4924,
      "step": 80710
    },
    {
      "epoch": 41.057985757884026,
      "grad_norm": 40.80883026123047,
      "learning_rate": 8.942014242115972e-06,
      "loss": 1.6204,
      "step": 80720
    },
    {
      "epoch": 41.06307222787385,
      "grad_norm": 35.53460693359375,
      "learning_rate": 8.936927772126146e-06,
      "loss": 1.5779,
      "step": 80730
    },
    {
      "epoch": 41.06815869786368,
      "grad_norm": 45.356624603271484,
      "learning_rate": 8.931841302136317e-06,
      "loss": 1.6125,
      "step": 80740
    },
    {
      "epoch": 41.07324516785351,
      "grad_norm": 38.95440673828125,
      "learning_rate": 8.92675483214649e-06,
      "loss": 1.5515,
      "step": 80750
    },
    {
      "epoch": 41.078331637843334,
      "grad_norm": 42.026432037353516,
      "learning_rate": 8.921668362156664e-06,
      "loss": 1.5282,
      "step": 80760
    },
    {
      "epoch": 41.08341810783316,
      "grad_norm": 35.27308654785156,
      "learning_rate": 8.916581892166837e-06,
      "loss": 1.5118,
      "step": 80770
    },
    {
      "epoch": 41.08850457782299,
      "grad_norm": 35.49921417236328,
      "learning_rate": 8.911495422177009e-06,
      "loss": 1.534,
      "step": 80780
    },
    {
      "epoch": 41.093591047812815,
      "grad_norm": 46.247764587402344,
      "learning_rate": 8.906408952187184e-06,
      "loss": 1.5442,
      "step": 80790
    },
    {
      "epoch": 41.09867751780264,
      "grad_norm": 42.231441497802734,
      "learning_rate": 8.901322482197355e-06,
      "loss": 1.6347,
      "step": 80800
    },
    {
      "epoch": 41.10376398779247,
      "grad_norm": 42.391414642333984,
      "learning_rate": 8.896236012207528e-06,
      "loss": 1.5809,
      "step": 80810
    },
    {
      "epoch": 41.108850457782296,
      "grad_norm": 32.156768798828125,
      "learning_rate": 8.891149542217702e-06,
      "loss": 1.4725,
      "step": 80820
    },
    {
      "epoch": 41.11393692777212,
      "grad_norm": 46.31458282470703,
      "learning_rate": 8.886063072227873e-06,
      "loss": 1.6177,
      "step": 80830
    },
    {
      "epoch": 41.11902339776195,
      "grad_norm": 49.18510437011719,
      "learning_rate": 8.880976602238048e-06,
      "loss": 1.5475,
      "step": 80840
    },
    {
      "epoch": 41.12410986775178,
      "grad_norm": 44.38456344604492,
      "learning_rate": 8.87589013224822e-06,
      "loss": 1.5506,
      "step": 80850
    },
    {
      "epoch": 41.129196337741604,
      "grad_norm": 48.63620376586914,
      "learning_rate": 8.870803662258393e-06,
      "loss": 1.5948,
      "step": 80860
    },
    {
      "epoch": 41.13428280773143,
      "grad_norm": 41.26530838012695,
      "learning_rate": 8.865717192268566e-06,
      "loss": 1.5574,
      "step": 80870
    },
    {
      "epoch": 41.139369277721265,
      "grad_norm": 40.921485900878906,
      "learning_rate": 8.860630722278738e-06,
      "loss": 1.5789,
      "step": 80880
    },
    {
      "epoch": 41.14445574771109,
      "grad_norm": 37.678401947021484,
      "learning_rate": 8.855544252288913e-06,
      "loss": 1.5529,
      "step": 80890
    },
    {
      "epoch": 41.14954221770092,
      "grad_norm": 47.16286087036133,
      "learning_rate": 8.850457782299085e-06,
      "loss": 1.5395,
      "step": 80900
    },
    {
      "epoch": 41.154628687690746,
      "grad_norm": 42.69684600830078,
      "learning_rate": 8.845371312309258e-06,
      "loss": 1.5058,
      "step": 80910
    },
    {
      "epoch": 41.15971515768057,
      "grad_norm": 39.247432708740234,
      "learning_rate": 8.840284842319431e-06,
      "loss": 1.5292,
      "step": 80920
    },
    {
      "epoch": 41.1648016276704,
      "grad_norm": 35.742218017578125,
      "learning_rate": 8.835198372329603e-06,
      "loss": 1.595,
      "step": 80930
    },
    {
      "epoch": 41.16988809766023,
      "grad_norm": 40.476806640625,
      "learning_rate": 8.830111902339778e-06,
      "loss": 1.5762,
      "step": 80940
    },
    {
      "epoch": 41.174974567650054,
      "grad_norm": 37.167720794677734,
      "learning_rate": 8.82502543234995e-06,
      "loss": 1.6017,
      "step": 80950
    },
    {
      "epoch": 41.18006103763988,
      "grad_norm": 52.34335708618164,
      "learning_rate": 8.819938962360123e-06,
      "loss": 1.6304,
      "step": 80960
    },
    {
      "epoch": 41.18514750762971,
      "grad_norm": 39.68095397949219,
      "learning_rate": 8.814852492370296e-06,
      "loss": 1.6302,
      "step": 80970
    },
    {
      "epoch": 41.190233977619535,
      "grad_norm": 36.02146911621094,
      "learning_rate": 8.809766022380467e-06,
      "loss": 1.5552,
      "step": 80980
    },
    {
      "epoch": 41.19532044760936,
      "grad_norm": 45.857757568359375,
      "learning_rate": 8.804679552390642e-06,
      "loss": 1.5112,
      "step": 80990
    },
    {
      "epoch": 41.20040691759919,
      "grad_norm": 41.755638122558594,
      "learning_rate": 8.799593082400814e-06,
      "loss": 1.5173,
      "step": 81000
    },
    {
      "epoch": 41.205493387589016,
      "grad_norm": 55.45198440551758,
      "learning_rate": 8.794506612410987e-06,
      "loss": 1.7128,
      "step": 81010
    },
    {
      "epoch": 41.21057985757884,
      "grad_norm": 37.075984954833984,
      "learning_rate": 8.78942014242116e-06,
      "loss": 1.5221,
      "step": 81020
    },
    {
      "epoch": 41.21566632756867,
      "grad_norm": 50.53702163696289,
      "learning_rate": 8.784333672431334e-06,
      "loss": 1.6551,
      "step": 81030
    },
    {
      "epoch": 41.2207527975585,
      "grad_norm": 37.290958404541016,
      "learning_rate": 8.779247202441505e-06,
      "loss": 1.5712,
      "step": 81040
    },
    {
      "epoch": 41.225839267548324,
      "grad_norm": 41.59103775024414,
      "learning_rate": 8.774160732451679e-06,
      "loss": 1.5398,
      "step": 81050
    },
    {
      "epoch": 41.23092573753815,
      "grad_norm": 36.90068435668945,
      "learning_rate": 8.769074262461852e-06,
      "loss": 1.5383,
      "step": 81060
    },
    {
      "epoch": 41.23601220752798,
      "grad_norm": 40.178958892822266,
      "learning_rate": 8.763987792472025e-06,
      "loss": 1.5723,
      "step": 81070
    },
    {
      "epoch": 41.241098677517805,
      "grad_norm": 54.23869705200195,
      "learning_rate": 8.758901322482199e-06,
      "loss": 1.512,
      "step": 81080
    },
    {
      "epoch": 41.24618514750763,
      "grad_norm": 44.38300323486328,
      "learning_rate": 8.75381485249237e-06,
      "loss": 1.5856,
      "step": 81090
    },
    {
      "epoch": 41.25127161749746,
      "grad_norm": 42.210845947265625,
      "learning_rate": 8.748728382502543e-06,
      "loss": 1.5962,
      "step": 81100
    },
    {
      "epoch": 41.256358087487286,
      "grad_norm": 44.36652755737305,
      "learning_rate": 8.743641912512717e-06,
      "loss": 1.5167,
      "step": 81110
    },
    {
      "epoch": 41.26144455747711,
      "grad_norm": 47.85051727294922,
      "learning_rate": 8.73855544252289e-06,
      "loss": 1.5426,
      "step": 81120
    },
    {
      "epoch": 41.26653102746694,
      "grad_norm": 46.03573226928711,
      "learning_rate": 8.733468972533063e-06,
      "loss": 1.5306,
      "step": 81130
    },
    {
      "epoch": 41.271617497456766,
      "grad_norm": 53.01826858520508,
      "learning_rate": 8.728382502543235e-06,
      "loss": 1.5778,
      "step": 81140
    },
    {
      "epoch": 41.27670396744659,
      "grad_norm": 44.18729019165039,
      "learning_rate": 8.723296032553408e-06,
      "loss": 1.5979,
      "step": 81150
    },
    {
      "epoch": 41.28179043743642,
      "grad_norm": 41.334747314453125,
      "learning_rate": 8.718209562563581e-06,
      "loss": 1.6216,
      "step": 81160
    },
    {
      "epoch": 41.28687690742625,
      "grad_norm": 41.88787078857422,
      "learning_rate": 8.713123092573753e-06,
      "loss": 1.5648,
      "step": 81170
    },
    {
      "epoch": 41.291963377416074,
      "grad_norm": 48.62639236450195,
      "learning_rate": 8.708036622583928e-06,
      "loss": 1.5172,
      "step": 81180
    },
    {
      "epoch": 41.2970498474059,
      "grad_norm": 51.49895477294922,
      "learning_rate": 8.7029501525941e-06,
      "loss": 1.561,
      "step": 81190
    },
    {
      "epoch": 41.30213631739573,
      "grad_norm": 36.567771911621094,
      "learning_rate": 8.697863682604273e-06,
      "loss": 1.6,
      "step": 81200
    },
    {
      "epoch": 41.307222787385555,
      "grad_norm": 45.847564697265625,
      "learning_rate": 8.692777212614446e-06,
      "loss": 1.529,
      "step": 81210
    },
    {
      "epoch": 41.31230925737538,
      "grad_norm": 34.30034255981445,
      "learning_rate": 8.687690742624618e-06,
      "loss": 1.5946,
      "step": 81220
    },
    {
      "epoch": 41.31739572736521,
      "grad_norm": 37.416561126708984,
      "learning_rate": 8.682604272634793e-06,
      "loss": 1.535,
      "step": 81230
    },
    {
      "epoch": 41.322482197355036,
      "grad_norm": 40.04874038696289,
      "learning_rate": 8.677517802644964e-06,
      "loss": 1.5271,
      "step": 81240
    },
    {
      "epoch": 41.32756866734486,
      "grad_norm": 51.44602584838867,
      "learning_rate": 8.672431332655138e-06,
      "loss": 1.6618,
      "step": 81250
    },
    {
      "epoch": 41.33265513733469,
      "grad_norm": 46.18653869628906,
      "learning_rate": 8.66734486266531e-06,
      "loss": 1.4727,
      "step": 81260
    },
    {
      "epoch": 41.33774160732452,
      "grad_norm": 46.47734069824219,
      "learning_rate": 8.662258392675484e-06,
      "loss": 1.6248,
      "step": 81270
    },
    {
      "epoch": 41.342828077314344,
      "grad_norm": 40.691123962402344,
      "learning_rate": 8.657171922685657e-06,
      "loss": 1.4897,
      "step": 81280
    },
    {
      "epoch": 41.34791454730417,
      "grad_norm": 42.78330993652344,
      "learning_rate": 8.652085452695829e-06,
      "loss": 1.5406,
      "step": 81290
    },
    {
      "epoch": 41.353001017294,
      "grad_norm": 42.0486946105957,
      "learning_rate": 8.646998982706002e-06,
      "loss": 1.6203,
      "step": 81300
    },
    {
      "epoch": 41.358087487283825,
      "grad_norm": 39.46177673339844,
      "learning_rate": 8.641912512716176e-06,
      "loss": 1.5569,
      "step": 81310
    },
    {
      "epoch": 41.36317395727365,
      "grad_norm": 32.0696907043457,
      "learning_rate": 8.636826042726349e-06,
      "loss": 1.5272,
      "step": 81320
    },
    {
      "epoch": 41.36826042726348,
      "grad_norm": 44.75160217285156,
      "learning_rate": 8.631739572736522e-06,
      "loss": 1.71,
      "step": 81330
    },
    {
      "epoch": 41.373346897253306,
      "grad_norm": 34.672386169433594,
      "learning_rate": 8.626653102746694e-06,
      "loss": 1.5147,
      "step": 81340
    },
    {
      "epoch": 41.37843336724313,
      "grad_norm": 39.5402717590332,
      "learning_rate": 8.621566632756867e-06,
      "loss": 1.5131,
      "step": 81350
    },
    {
      "epoch": 41.38351983723296,
      "grad_norm": 37.0203857421875,
      "learning_rate": 8.61648016276704e-06,
      "loss": 1.6164,
      "step": 81360
    },
    {
      "epoch": 41.38860630722279,
      "grad_norm": 53.458457946777344,
      "learning_rate": 8.611393692777214e-06,
      "loss": 1.5867,
      "step": 81370
    },
    {
      "epoch": 41.393692777212614,
      "grad_norm": 51.98501968383789,
      "learning_rate": 8.606307222787385e-06,
      "loss": 1.6226,
      "step": 81380
    },
    {
      "epoch": 41.39877924720244,
      "grad_norm": 42.639896392822266,
      "learning_rate": 8.601220752797558e-06,
      "loss": 1.5672,
      "step": 81390
    },
    {
      "epoch": 41.40386571719227,
      "grad_norm": 39.902889251708984,
      "learning_rate": 8.596134282807732e-06,
      "loss": 1.5863,
      "step": 81400
    },
    {
      "epoch": 41.408952187182095,
      "grad_norm": 43.374427795410156,
      "learning_rate": 8.591047812817905e-06,
      "loss": 1.6163,
      "step": 81410
    },
    {
      "epoch": 41.41403865717192,
      "grad_norm": 37.26813888549805,
      "learning_rate": 8.585961342828078e-06,
      "loss": 1.6241,
      "step": 81420
    },
    {
      "epoch": 41.41912512716175,
      "grad_norm": 37.3296012878418,
      "learning_rate": 8.58087487283825e-06,
      "loss": 1.5558,
      "step": 81430
    },
    {
      "epoch": 41.424211597151576,
      "grad_norm": 51.71532440185547,
      "learning_rate": 8.575788402848423e-06,
      "loss": 1.6219,
      "step": 81440
    },
    {
      "epoch": 41.4292980671414,
      "grad_norm": 41.138187408447266,
      "learning_rate": 8.570701932858596e-06,
      "loss": 1.5712,
      "step": 81450
    },
    {
      "epoch": 41.43438453713123,
      "grad_norm": 36.36491775512695,
      "learning_rate": 8.56561546286877e-06,
      "loss": 1.5876,
      "step": 81460
    },
    {
      "epoch": 41.43947100712106,
      "grad_norm": 44.44789505004883,
      "learning_rate": 8.560528992878943e-06,
      "loss": 1.6197,
      "step": 81470
    },
    {
      "epoch": 41.444557477110884,
      "grad_norm": 35.810848236083984,
      "learning_rate": 8.555442522889115e-06,
      "loss": 1.5373,
      "step": 81480
    },
    {
      "epoch": 41.44964394710071,
      "grad_norm": 43.00939178466797,
      "learning_rate": 8.55035605289929e-06,
      "loss": 1.4522,
      "step": 81490
    },
    {
      "epoch": 41.45473041709054,
      "grad_norm": 36.78870391845703,
      "learning_rate": 8.545269582909461e-06,
      "loss": 1.5626,
      "step": 81500
    },
    {
      "epoch": 41.459816887080365,
      "grad_norm": 42.10335159301758,
      "learning_rate": 8.540183112919634e-06,
      "loss": 1.6663,
      "step": 81510
    },
    {
      "epoch": 41.46490335707019,
      "grad_norm": 38.06535720825195,
      "learning_rate": 8.535096642929808e-06,
      "loss": 1.5922,
      "step": 81520
    },
    {
      "epoch": 41.46998982706002,
      "grad_norm": 40.595794677734375,
      "learning_rate": 8.53001017293998e-06,
      "loss": 1.5744,
      "step": 81530
    },
    {
      "epoch": 41.475076297049846,
      "grad_norm": 43.4346809387207,
      "learning_rate": 8.524923702950154e-06,
      "loss": 1.6073,
      "step": 81540
    },
    {
      "epoch": 41.48016276703967,
      "grad_norm": 31.89714241027832,
      "learning_rate": 8.519837232960326e-06,
      "loss": 1.6229,
      "step": 81550
    },
    {
      "epoch": 41.4852492370295,
      "grad_norm": 43.80292510986328,
      "learning_rate": 8.514750762970499e-06,
      "loss": 1.627,
      "step": 81560
    },
    {
      "epoch": 41.49033570701933,
      "grad_norm": 43.56521987915039,
      "learning_rate": 8.509664292980672e-06,
      "loss": 1.6419,
      "step": 81570
    },
    {
      "epoch": 41.495422177009154,
      "grad_norm": 33.59184646606445,
      "learning_rate": 8.504577822990844e-06,
      "loss": 1.5837,
      "step": 81580
    },
    {
      "epoch": 41.50050864699898,
      "grad_norm": 47.33580017089844,
      "learning_rate": 8.499491353001017e-06,
      "loss": 1.6074,
      "step": 81590
    },
    {
      "epoch": 41.50559511698881,
      "grad_norm": 34.79542922973633,
      "learning_rate": 8.49440488301119e-06,
      "loss": 1.6159,
      "step": 81600
    },
    {
      "epoch": 41.510681586978635,
      "grad_norm": 38.098716735839844,
      "learning_rate": 8.489318413021364e-06,
      "loss": 1.5979,
      "step": 81610
    },
    {
      "epoch": 41.51576805696846,
      "grad_norm": 49.87555694580078,
      "learning_rate": 8.484231943031537e-06,
      "loss": 1.6573,
      "step": 81620
    },
    {
      "epoch": 41.52085452695829,
      "grad_norm": 45.82016372680664,
      "learning_rate": 8.479145473041709e-06,
      "loss": 1.5872,
      "step": 81630
    },
    {
      "epoch": 41.525940996948115,
      "grad_norm": 42.62326431274414,
      "learning_rate": 8.474059003051882e-06,
      "loss": 1.5923,
      "step": 81640
    },
    {
      "epoch": 41.53102746693794,
      "grad_norm": 30.318716049194336,
      "learning_rate": 8.468972533062055e-06,
      "loss": 1.5409,
      "step": 81650
    },
    {
      "epoch": 41.53611393692777,
      "grad_norm": 39.97039794921875,
      "learning_rate": 8.463886063072229e-06,
      "loss": 1.5351,
      "step": 81660
    },
    {
      "epoch": 41.541200406917596,
      "grad_norm": 49.4116096496582,
      "learning_rate": 8.458799593082402e-06,
      "loss": 1.655,
      "step": 81670
    },
    {
      "epoch": 41.54628687690742,
      "grad_norm": 35.871620178222656,
      "learning_rate": 8.453713123092573e-06,
      "loss": 1.5558,
      "step": 81680
    },
    {
      "epoch": 41.55137334689725,
      "grad_norm": 39.172428131103516,
      "learning_rate": 8.448626653102747e-06,
      "loss": 1.5896,
      "step": 81690
    },
    {
      "epoch": 41.55645981688708,
      "grad_norm": 44.7796745300293,
      "learning_rate": 8.44354018311292e-06,
      "loss": 1.6502,
      "step": 81700
    },
    {
      "epoch": 41.561546286876904,
      "grad_norm": 44.990318298339844,
      "learning_rate": 8.438453713123093e-06,
      "loss": 1.5663,
      "step": 81710
    },
    {
      "epoch": 41.56663275686673,
      "grad_norm": 35.86408233642578,
      "learning_rate": 8.433367243133265e-06,
      "loss": 1.5991,
      "step": 81720
    },
    {
      "epoch": 41.57171922685656,
      "grad_norm": 44.002140045166016,
      "learning_rate": 8.42828077314344e-06,
      "loss": 1.5636,
      "step": 81730
    },
    {
      "epoch": 41.576805696846385,
      "grad_norm": 38.82289505004883,
      "learning_rate": 8.423194303153611e-06,
      "loss": 1.5809,
      "step": 81740
    },
    {
      "epoch": 41.58189216683621,
      "grad_norm": 43.863929748535156,
      "learning_rate": 8.418107833163785e-06,
      "loss": 1.5547,
      "step": 81750
    },
    {
      "epoch": 41.58697863682604,
      "grad_norm": 46.5657844543457,
      "learning_rate": 8.413021363173958e-06,
      "loss": 1.5921,
      "step": 81760
    },
    {
      "epoch": 41.592065106815866,
      "grad_norm": 44.162574768066406,
      "learning_rate": 8.40793489318413e-06,
      "loss": 1.5871,
      "step": 81770
    },
    {
      "epoch": 41.5971515768057,
      "grad_norm": 34.434268951416016,
      "learning_rate": 8.402848423194305e-06,
      "loss": 1.5538,
      "step": 81780
    },
    {
      "epoch": 41.60223804679553,
      "grad_norm": 36.96600341796875,
      "learning_rate": 8.397761953204476e-06,
      "loss": 1.4475,
      "step": 81790
    },
    {
      "epoch": 41.607324516785354,
      "grad_norm": 37.92509841918945,
      "learning_rate": 8.39267548321465e-06,
      "loss": 1.5373,
      "step": 81800
    },
    {
      "epoch": 41.61241098677518,
      "grad_norm": 39.766326904296875,
      "learning_rate": 8.387589013224823e-06,
      "loss": 1.5524,
      "step": 81810
    },
    {
      "epoch": 41.61749745676501,
      "grad_norm": 46.298744201660156,
      "learning_rate": 8.382502543234994e-06,
      "loss": 1.5489,
      "step": 81820
    },
    {
      "epoch": 41.622583926754835,
      "grad_norm": 33.606727600097656,
      "learning_rate": 8.37741607324517e-06,
      "loss": 1.6823,
      "step": 81830
    },
    {
      "epoch": 41.62767039674466,
      "grad_norm": 49.189456939697266,
      "learning_rate": 8.37232960325534e-06,
      "loss": 1.5926,
      "step": 81840
    },
    {
      "epoch": 41.63275686673449,
      "grad_norm": 38.62835693359375,
      "learning_rate": 8.367243133265514e-06,
      "loss": 1.6103,
      "step": 81850
    },
    {
      "epoch": 41.637843336724316,
      "grad_norm": 46.8218879699707,
      "learning_rate": 8.362156663275687e-06,
      "loss": 1.4658,
      "step": 81860
    },
    {
      "epoch": 41.64292980671414,
      "grad_norm": 41.92707443237305,
      "learning_rate": 8.357070193285859e-06,
      "loss": 1.6134,
      "step": 81870
    },
    {
      "epoch": 41.64801627670397,
      "grad_norm": 51.957794189453125,
      "learning_rate": 8.351983723296034e-06,
      "loss": 1.5323,
      "step": 81880
    },
    {
      "epoch": 41.6531027466938,
      "grad_norm": 40.41117477416992,
      "learning_rate": 8.346897253306206e-06,
      "loss": 1.5783,
      "step": 81890
    },
    {
      "epoch": 41.658189216683624,
      "grad_norm": 48.45440673828125,
      "learning_rate": 8.341810783316379e-06,
      "loss": 1.6102,
      "step": 81900
    },
    {
      "epoch": 41.66327568667345,
      "grad_norm": 33.63221740722656,
      "learning_rate": 8.336724313326552e-06,
      "loss": 1.5876,
      "step": 81910
    },
    {
      "epoch": 41.66836215666328,
      "grad_norm": 45.25480270385742,
      "learning_rate": 8.331637843336724e-06,
      "loss": 1.5545,
      "step": 81920
    },
    {
      "epoch": 41.673448626653105,
      "grad_norm": 39.81806564331055,
      "learning_rate": 8.326551373346899e-06,
      "loss": 1.6271,
      "step": 81930
    },
    {
      "epoch": 41.67853509664293,
      "grad_norm": 38.38217544555664,
      "learning_rate": 8.32146490335707e-06,
      "loss": 1.5966,
      "step": 81940
    },
    {
      "epoch": 41.68362156663276,
      "grad_norm": 41.45283889770508,
      "learning_rate": 8.316378433367244e-06,
      "loss": 1.5154,
      "step": 81950
    },
    {
      "epoch": 41.688708036622586,
      "grad_norm": 35.70600128173828,
      "learning_rate": 8.311291963377417e-06,
      "loss": 1.626,
      "step": 81960
    },
    {
      "epoch": 41.69379450661241,
      "grad_norm": 40.46795654296875,
      "learning_rate": 8.30620549338759e-06,
      "loss": 1.5938,
      "step": 81970
    },
    {
      "epoch": 41.69888097660224,
      "grad_norm": 49.51364517211914,
      "learning_rate": 8.301119023397762e-06,
      "loss": 1.5816,
      "step": 81980
    },
    {
      "epoch": 41.70396744659207,
      "grad_norm": 37.008995056152344,
      "learning_rate": 8.296032553407935e-06,
      "loss": 1.5432,
      "step": 81990
    },
    {
      "epoch": 41.709053916581894,
      "grad_norm": 45.549530029296875,
      "learning_rate": 8.290946083418108e-06,
      "loss": 1.5305,
      "step": 82000
    },
    {
      "epoch": 41.71414038657172,
      "grad_norm": 41.67691421508789,
      "learning_rate": 8.285859613428282e-06,
      "loss": 1.4754,
      "step": 82010
    },
    {
      "epoch": 41.71922685656155,
      "grad_norm": 36.76809310913086,
      "learning_rate": 8.280773143438455e-06,
      "loss": 1.4472,
      "step": 82020
    },
    {
      "epoch": 41.724313326551375,
      "grad_norm": 40.36514663696289,
      "learning_rate": 8.275686673448626e-06,
      "loss": 1.5163,
      "step": 82030
    },
    {
      "epoch": 41.7293997965412,
      "grad_norm": 48.72698211669922,
      "learning_rate": 8.2706002034588e-06,
      "loss": 1.611,
      "step": 82040
    },
    {
      "epoch": 41.73448626653103,
      "grad_norm": 38.100189208984375,
      "learning_rate": 8.265513733468973e-06,
      "loss": 1.5393,
      "step": 82050
    },
    {
      "epoch": 41.739572736520856,
      "grad_norm": 40.03690719604492,
      "learning_rate": 8.260427263479145e-06,
      "loss": 1.5685,
      "step": 82060
    },
    {
      "epoch": 41.74465920651068,
      "grad_norm": 45.85044479370117,
      "learning_rate": 8.25534079348932e-06,
      "loss": 1.578,
      "step": 82070
    },
    {
      "epoch": 41.74974567650051,
      "grad_norm": 37.54561233520508,
      "learning_rate": 8.250254323499491e-06,
      "loss": 1.5926,
      "step": 82080
    },
    {
      "epoch": 41.75483214649034,
      "grad_norm": 43.18586349487305,
      "learning_rate": 8.245167853509664e-06,
      "loss": 1.543,
      "step": 82090
    },
    {
      "epoch": 41.759918616480164,
      "grad_norm": 36.90546798706055,
      "learning_rate": 8.240081383519838e-06,
      "loss": 1.5378,
      "step": 82100
    },
    {
      "epoch": 41.76500508646999,
      "grad_norm": 33.36735153198242,
      "learning_rate": 8.23499491353001e-06,
      "loss": 1.5781,
      "step": 82110
    },
    {
      "epoch": 41.77009155645982,
      "grad_norm": 36.48533630371094,
      "learning_rate": 8.229908443540184e-06,
      "loss": 1.5709,
      "step": 82120
    },
    {
      "epoch": 41.775178026449645,
      "grad_norm": 44.227848052978516,
      "learning_rate": 8.224821973550356e-06,
      "loss": 1.5911,
      "step": 82130
    },
    {
      "epoch": 41.78026449643947,
      "grad_norm": 38.4553337097168,
      "learning_rate": 8.21973550356053e-06,
      "loss": 1.5689,
      "step": 82140
    },
    {
      "epoch": 41.7853509664293,
      "grad_norm": 58.35482406616211,
      "learning_rate": 8.214649033570702e-06,
      "loss": 1.5883,
      "step": 82150
    },
    {
      "epoch": 41.790437436419126,
      "grad_norm": 40.779869079589844,
      "learning_rate": 8.209562563580874e-06,
      "loss": 1.5905,
      "step": 82160
    },
    {
      "epoch": 41.79552390640895,
      "grad_norm": 52.7586669921875,
      "learning_rate": 8.204476093591049e-06,
      "loss": 1.5511,
      "step": 82170
    },
    {
      "epoch": 41.80061037639878,
      "grad_norm": 39.80785369873047,
      "learning_rate": 8.19938962360122e-06,
      "loss": 1.5468,
      "step": 82180
    },
    {
      "epoch": 41.80569684638861,
      "grad_norm": 46.62602996826172,
      "learning_rate": 8.194303153611394e-06,
      "loss": 1.5364,
      "step": 82190
    },
    {
      "epoch": 41.81078331637843,
      "grad_norm": 46.40359115600586,
      "learning_rate": 8.189216683621567e-06,
      "loss": 1.5387,
      "step": 82200
    },
    {
      "epoch": 41.81586978636826,
      "grad_norm": 45.649681091308594,
      "learning_rate": 8.18413021363174e-06,
      "loss": 1.5843,
      "step": 82210
    },
    {
      "epoch": 41.82095625635809,
      "grad_norm": 40.47728729248047,
      "learning_rate": 8.179043743641914e-06,
      "loss": 1.5729,
      "step": 82220
    },
    {
      "epoch": 41.826042726347914,
      "grad_norm": 43.949913024902344,
      "learning_rate": 8.173957273652085e-06,
      "loss": 1.5955,
      "step": 82230
    },
    {
      "epoch": 41.83112919633774,
      "grad_norm": 37.26937484741211,
      "learning_rate": 8.168870803662259e-06,
      "loss": 1.629,
      "step": 82240
    },
    {
      "epoch": 41.83621566632757,
      "grad_norm": 45.72428894042969,
      "learning_rate": 8.163784333672432e-06,
      "loss": 1.5588,
      "step": 82250
    },
    {
      "epoch": 41.841302136317395,
      "grad_norm": 38.064613342285156,
      "learning_rate": 8.158697863682605e-06,
      "loss": 1.5628,
      "step": 82260
    },
    {
      "epoch": 41.84638860630722,
      "grad_norm": 38.72627258300781,
      "learning_rate": 8.153611393692778e-06,
      "loss": 1.587,
      "step": 82270
    },
    {
      "epoch": 41.85147507629705,
      "grad_norm": 48.35449981689453,
      "learning_rate": 8.14852492370295e-06,
      "loss": 1.5926,
      "step": 82280
    },
    {
      "epoch": 41.856561546286876,
      "grad_norm": 36.02879333496094,
      "learning_rate": 8.143438453713123e-06,
      "loss": 1.5013,
      "step": 82290
    },
    {
      "epoch": 41.8616480162767,
      "grad_norm": 43.94801330566406,
      "learning_rate": 8.138351983723297e-06,
      "loss": 1.5207,
      "step": 82300
    },
    {
      "epoch": 41.86673448626653,
      "grad_norm": 46.826026916503906,
      "learning_rate": 8.13326551373347e-06,
      "loss": 1.604,
      "step": 82310
    },
    {
      "epoch": 41.87182095625636,
      "grad_norm": 47.63771057128906,
      "learning_rate": 8.128179043743641e-06,
      "loss": 1.5681,
      "step": 82320
    },
    {
      "epoch": 41.876907426246184,
      "grad_norm": 37.45138168334961,
      "learning_rate": 8.123092573753815e-06,
      "loss": 1.64,
      "step": 82330
    },
    {
      "epoch": 41.88199389623601,
      "grad_norm": 45.948646545410156,
      "learning_rate": 8.118006103763988e-06,
      "loss": 1.591,
      "step": 82340
    },
    {
      "epoch": 41.88708036622584,
      "grad_norm": 42.145240783691406,
      "learning_rate": 8.112919633774161e-06,
      "loss": 1.5853,
      "step": 82350
    },
    {
      "epoch": 41.892166836215665,
      "grad_norm": 54.04398727416992,
      "learning_rate": 8.107833163784335e-06,
      "loss": 1.5931,
      "step": 82360
    },
    {
      "epoch": 41.89725330620549,
      "grad_norm": 36.73344802856445,
      "learning_rate": 8.102746693794506e-06,
      "loss": 1.6321,
      "step": 82370
    },
    {
      "epoch": 41.90233977619532,
      "grad_norm": 41.93459701538086,
      "learning_rate": 8.097660223804681e-06,
      "loss": 1.6421,
      "step": 82380
    },
    {
      "epoch": 41.907426246185146,
      "grad_norm": 37.93568801879883,
      "learning_rate": 8.092573753814853e-06,
      "loss": 1.4787,
      "step": 82390
    },
    {
      "epoch": 41.91251271617497,
      "grad_norm": 35.906803131103516,
      "learning_rate": 8.087487283825026e-06,
      "loss": 1.5318,
      "step": 82400
    },
    {
      "epoch": 41.9175991861648,
      "grad_norm": 40.755123138427734,
      "learning_rate": 8.0824008138352e-06,
      "loss": 1.551,
      "step": 82410
    },
    {
      "epoch": 41.92268565615463,
      "grad_norm": 39.5367431640625,
      "learning_rate": 8.07731434384537e-06,
      "loss": 1.5918,
      "step": 82420
    },
    {
      "epoch": 41.927772126144454,
      "grad_norm": 38.8935546875,
      "learning_rate": 8.072227873855546e-06,
      "loss": 1.5427,
      "step": 82430
    },
    {
      "epoch": 41.93285859613428,
      "grad_norm": 35.217063903808594,
      "learning_rate": 8.067141403865717e-06,
      "loss": 1.4858,
      "step": 82440
    },
    {
      "epoch": 41.93794506612411,
      "grad_norm": 37.36148452758789,
      "learning_rate": 8.06205493387589e-06,
      "loss": 1.6554,
      "step": 82450
    },
    {
      "epoch": 41.943031536113935,
      "grad_norm": 42.830810546875,
      "learning_rate": 8.056968463886064e-06,
      "loss": 1.5745,
      "step": 82460
    },
    {
      "epoch": 41.94811800610376,
      "grad_norm": 38.96164321899414,
      "learning_rate": 8.051881993896236e-06,
      "loss": 1.5446,
      "step": 82470
    },
    {
      "epoch": 41.95320447609359,
      "grad_norm": 44.08156204223633,
      "learning_rate": 8.04679552390641e-06,
      "loss": 1.6307,
      "step": 82480
    },
    {
      "epoch": 41.958290946083416,
      "grad_norm": 41.327152252197266,
      "learning_rate": 8.041709053916582e-06,
      "loss": 1.5471,
      "step": 82490
    },
    {
      "epoch": 41.96337741607324,
      "grad_norm": 41.97596740722656,
      "learning_rate": 8.036622583926755e-06,
      "loss": 1.6364,
      "step": 82500
    },
    {
      "epoch": 41.96846388606307,
      "grad_norm": 34.2213134765625,
      "learning_rate": 8.031536113936929e-06,
      "loss": 1.6437,
      "step": 82510
    },
    {
      "epoch": 41.9735503560529,
      "grad_norm": 35.45975875854492,
      "learning_rate": 8.0264496439471e-06,
      "loss": 1.538,
      "step": 82520
    },
    {
      "epoch": 41.978636826042724,
      "grad_norm": 38.116119384765625,
      "learning_rate": 8.021363173957274e-06,
      "loss": 1.6369,
      "step": 82530
    },
    {
      "epoch": 41.98372329603255,
      "grad_norm": 43.29913330078125,
      "learning_rate": 8.016276703967447e-06,
      "loss": 1.5858,
      "step": 82540
    },
    {
      "epoch": 41.98880976602238,
      "grad_norm": 45.080787658691406,
      "learning_rate": 8.01119023397762e-06,
      "loss": 1.5527,
      "step": 82550
    },
    {
      "epoch": 41.993896236012205,
      "grad_norm": 46.49330139160156,
      "learning_rate": 8.006103763987793e-06,
      "loss": 1.5341,
      "step": 82560
    },
    {
      "epoch": 41.99898270600203,
      "grad_norm": 48.76645278930664,
      "learning_rate": 8.001017293997965e-06,
      "loss": 1.6472,
      "step": 82570
    },
    {
      "epoch": 42.0,
      "eval_loss": 5.0539093017578125,
      "eval_runtime": 2.9079,
      "eval_samples_per_second": 954.287,
      "eval_steps_per_second": 119.329,
      "step": 82572
    },
    {
      "epoch": 42.00406917599186,
      "grad_norm": 44.64682388305664,
      "learning_rate": 7.995930824008138e-06,
      "loss": 1.5667,
      "step": 82580
    },
    {
      "epoch": 42.009155645981686,
      "grad_norm": 40.65506362915039,
      "learning_rate": 7.990844354018312e-06,
      "loss": 1.5094,
      "step": 82590
    },
    {
      "epoch": 42.01424211597151,
      "grad_norm": 39.32514190673828,
      "learning_rate": 7.985757884028485e-06,
      "loss": 1.5372,
      "step": 82600
    },
    {
      "epoch": 42.01932858596134,
      "grad_norm": 43.71854019165039,
      "learning_rate": 7.980671414038658e-06,
      "loss": 1.5191,
      "step": 82610
    },
    {
      "epoch": 42.02441505595117,
      "grad_norm": 39.63142013549805,
      "learning_rate": 7.975584944048831e-06,
      "loss": 1.6281,
      "step": 82620
    },
    {
      "epoch": 42.029501525940994,
      "grad_norm": 33.42668533325195,
      "learning_rate": 7.970498474059003e-06,
      "loss": 1.4805,
      "step": 82630
    },
    {
      "epoch": 42.03458799593082,
      "grad_norm": 37.869144439697266,
      "learning_rate": 7.965412004069176e-06,
      "loss": 1.6298,
      "step": 82640
    },
    {
      "epoch": 42.03967446592065,
      "grad_norm": 40.00543212890625,
      "learning_rate": 7.96032553407935e-06,
      "loss": 1.5487,
      "step": 82650
    },
    {
      "epoch": 42.044760935910475,
      "grad_norm": 37.44770431518555,
      "learning_rate": 7.955239064089521e-06,
      "loss": 1.5221,
      "step": 82660
    },
    {
      "epoch": 42.04984740590031,
      "grad_norm": 42.2357177734375,
      "learning_rate": 7.950152594099696e-06,
      "loss": 1.5263,
      "step": 82670
    },
    {
      "epoch": 42.054933875890136,
      "grad_norm": 49.649208068847656,
      "learning_rate": 7.945066124109868e-06,
      "loss": 1.4909,
      "step": 82680
    },
    {
      "epoch": 42.06002034587996,
      "grad_norm": 40.56117248535156,
      "learning_rate": 7.939979654120041e-06,
      "loss": 1.6126,
      "step": 82690
    },
    {
      "epoch": 42.06510681586979,
      "grad_norm": 37.85416030883789,
      "learning_rate": 7.934893184130214e-06,
      "loss": 1.4558,
      "step": 82700
    },
    {
      "epoch": 42.07019328585962,
      "grad_norm": 46.92495346069336,
      "learning_rate": 7.929806714140386e-06,
      "loss": 1.5563,
      "step": 82710
    },
    {
      "epoch": 42.075279755849444,
      "grad_norm": 39.3129768371582,
      "learning_rate": 7.92472024415056e-06,
      "loss": 1.5069,
      "step": 82720
    },
    {
      "epoch": 42.08036622583927,
      "grad_norm": 39.57539367675781,
      "learning_rate": 7.919633774160732e-06,
      "loss": 1.5773,
      "step": 82730
    },
    {
      "epoch": 42.0854526958291,
      "grad_norm": 46.562660217285156,
      "learning_rate": 7.914547304170906e-06,
      "loss": 1.5847,
      "step": 82740
    },
    {
      "epoch": 42.090539165818925,
      "grad_norm": 44.48339080810547,
      "learning_rate": 7.909460834181079e-06,
      "loss": 1.5574,
      "step": 82750
    },
    {
      "epoch": 42.09562563580875,
      "grad_norm": 49.10613250732422,
      "learning_rate": 7.90437436419125e-06,
      "loss": 1.563,
      "step": 82760
    },
    {
      "epoch": 42.10071210579858,
      "grad_norm": 36.940162658691406,
      "learning_rate": 7.899287894201426e-06,
      "loss": 1.5935,
      "step": 82770
    },
    {
      "epoch": 42.105798575788405,
      "grad_norm": 43.14913558959961,
      "learning_rate": 7.894201424211597e-06,
      "loss": 1.5848,
      "step": 82780
    },
    {
      "epoch": 42.11088504577823,
      "grad_norm": 37.2688102722168,
      "learning_rate": 7.88911495422177e-06,
      "loss": 1.5278,
      "step": 82790
    },
    {
      "epoch": 42.11597151576806,
      "grad_norm": 40.27058029174805,
      "learning_rate": 7.884028484231944e-06,
      "loss": 1.5416,
      "step": 82800
    },
    {
      "epoch": 42.121057985757886,
      "grad_norm": 36.76235580444336,
      "learning_rate": 7.878942014242115e-06,
      "loss": 1.6198,
      "step": 82810
    },
    {
      "epoch": 42.12614445574771,
      "grad_norm": 37.28742980957031,
      "learning_rate": 7.87385554425229e-06,
      "loss": 1.57,
      "step": 82820
    },
    {
      "epoch": 42.13123092573754,
      "grad_norm": 51.136531829833984,
      "learning_rate": 7.868769074262462e-06,
      "loss": 1.5978,
      "step": 82830
    },
    {
      "epoch": 42.13631739572737,
      "grad_norm": 45.92378234863281,
      "learning_rate": 7.863682604272635e-06,
      "loss": 1.5046,
      "step": 82840
    },
    {
      "epoch": 42.141403865717194,
      "grad_norm": 41.68406295776367,
      "learning_rate": 7.858596134282808e-06,
      "loss": 1.5784,
      "step": 82850
    },
    {
      "epoch": 42.14649033570702,
      "grad_norm": 43.246192932128906,
      "learning_rate": 7.853509664292982e-06,
      "loss": 1.6096,
      "step": 82860
    },
    {
      "epoch": 42.15157680569685,
      "grad_norm": 43.225563049316406,
      "learning_rate": 7.848423194303153e-06,
      "loss": 1.6347,
      "step": 82870
    },
    {
      "epoch": 42.156663275686675,
      "grad_norm": 39.69158172607422,
      "learning_rate": 7.843336724313327e-06,
      "loss": 1.5115,
      "step": 82880
    },
    {
      "epoch": 42.1617497456765,
      "grad_norm": 36.642364501953125,
      "learning_rate": 7.8382502543235e-06,
      "loss": 1.5451,
      "step": 82890
    },
    {
      "epoch": 42.16683621566633,
      "grad_norm": 52.05581283569336,
      "learning_rate": 7.833163784333673e-06,
      "loss": 1.5146,
      "step": 82900
    },
    {
      "epoch": 42.171922685656156,
      "grad_norm": 46.20207214355469,
      "learning_rate": 7.828077314343846e-06,
      "loss": 1.6354,
      "step": 82910
    },
    {
      "epoch": 42.17700915564598,
      "grad_norm": 37.21116638183594,
      "learning_rate": 7.822990844354018e-06,
      "loss": 1.5405,
      "step": 82920
    },
    {
      "epoch": 42.18209562563581,
      "grad_norm": 42.36872482299805,
      "learning_rate": 7.817904374364191e-06,
      "loss": 1.6046,
      "step": 82930
    },
    {
      "epoch": 42.18718209562564,
      "grad_norm": 42.58277893066406,
      "learning_rate": 7.812817904374365e-06,
      "loss": 1.5885,
      "step": 82940
    },
    {
      "epoch": 42.192268565615464,
      "grad_norm": 27.190460205078125,
      "learning_rate": 7.807731434384538e-06,
      "loss": 1.5467,
      "step": 82950
    },
    {
      "epoch": 42.19735503560529,
      "grad_norm": 40.385318756103516,
      "learning_rate": 7.802644964394711e-06,
      "loss": 1.5395,
      "step": 82960
    },
    {
      "epoch": 42.20244150559512,
      "grad_norm": 57.16835021972656,
      "learning_rate": 7.797558494404883e-06,
      "loss": 1.5169,
      "step": 82970
    },
    {
      "epoch": 42.207527975584945,
      "grad_norm": 39.458133697509766,
      "learning_rate": 7.792472024415056e-06,
      "loss": 1.5501,
      "step": 82980
    },
    {
      "epoch": 42.21261444557477,
      "grad_norm": 43.78998565673828,
      "learning_rate": 7.78738555442523e-06,
      "loss": 1.5704,
      "step": 82990
    },
    {
      "epoch": 42.2177009155646,
      "grad_norm": 40.24565887451172,
      "learning_rate": 7.7822990844354e-06,
      "loss": 1.5458,
      "step": 83000
    },
    {
      "epoch": 42.222787385554426,
      "grad_norm": 43.64789962768555,
      "learning_rate": 7.777212614445576e-06,
      "loss": 1.5404,
      "step": 83010
    },
    {
      "epoch": 42.22787385554425,
      "grad_norm": 43.87642288208008,
      "learning_rate": 7.772126144455747e-06,
      "loss": 1.518,
      "step": 83020
    },
    {
      "epoch": 42.23296032553408,
      "grad_norm": 50.15739440917969,
      "learning_rate": 7.767039674465922e-06,
      "loss": 1.5026,
      "step": 83030
    },
    {
      "epoch": 42.23804679552391,
      "grad_norm": 35.231590270996094,
      "learning_rate": 7.761953204476094e-06,
      "loss": 1.5558,
      "step": 83040
    },
    {
      "epoch": 42.243133265513734,
      "grad_norm": 37.18450927734375,
      "learning_rate": 7.756866734486266e-06,
      "loss": 1.5222,
      "step": 83050
    },
    {
      "epoch": 42.24821973550356,
      "grad_norm": 38.94535827636719,
      "learning_rate": 7.75178026449644e-06,
      "loss": 1.5201,
      "step": 83060
    },
    {
      "epoch": 42.25330620549339,
      "grad_norm": 44.82362365722656,
      "learning_rate": 7.746693794506612e-06,
      "loss": 1.548,
      "step": 83070
    },
    {
      "epoch": 42.258392675483215,
      "grad_norm": 44.182037353515625,
      "learning_rate": 7.741607324516787e-06,
      "loss": 1.5089,
      "step": 83080
    },
    {
      "epoch": 42.26347914547304,
      "grad_norm": 63.02207565307617,
      "learning_rate": 7.736520854526959e-06,
      "loss": 1.6091,
      "step": 83090
    },
    {
      "epoch": 42.26856561546287,
      "grad_norm": 38.571556091308594,
      "learning_rate": 7.731434384537132e-06,
      "loss": 1.539,
      "step": 83100
    },
    {
      "epoch": 42.273652085452696,
      "grad_norm": 45.177711486816406,
      "learning_rate": 7.726347914547305e-06,
      "loss": 1.5558,
      "step": 83110
    },
    {
      "epoch": 42.27873855544252,
      "grad_norm": 41.5341911315918,
      "learning_rate": 7.721261444557477e-06,
      "loss": 1.5299,
      "step": 83120
    },
    {
      "epoch": 42.28382502543235,
      "grad_norm": 39.77400207519531,
      "learning_rate": 7.71617497456765e-06,
      "loss": 1.534,
      "step": 83130
    },
    {
      "epoch": 42.28891149542218,
      "grad_norm": 35.071475982666016,
      "learning_rate": 7.711088504577823e-06,
      "loss": 1.5439,
      "step": 83140
    },
    {
      "epoch": 42.293997965412004,
      "grad_norm": 41.37957763671875,
      "learning_rate": 7.706002034587997e-06,
      "loss": 1.5694,
      "step": 83150
    },
    {
      "epoch": 42.29908443540183,
      "grad_norm": 49.36626052856445,
      "learning_rate": 7.70091556459817e-06,
      "loss": 1.5239,
      "step": 83160
    },
    {
      "epoch": 42.30417090539166,
      "grad_norm": 40.13457107543945,
      "learning_rate": 7.695829094608342e-06,
      "loss": 1.4985,
      "step": 83170
    },
    {
      "epoch": 42.309257375381485,
      "grad_norm": 42.50833511352539,
      "learning_rate": 7.690742624618515e-06,
      "loss": 1.5819,
      "step": 83180
    },
    {
      "epoch": 42.31434384537131,
      "grad_norm": 48.451324462890625,
      "learning_rate": 7.685656154628688e-06,
      "loss": 1.6068,
      "step": 83190
    },
    {
      "epoch": 42.31943031536114,
      "grad_norm": 36.39675521850586,
      "learning_rate": 7.680569684638861e-06,
      "loss": 1.5397,
      "step": 83200
    },
    {
      "epoch": 42.324516785350966,
      "grad_norm": 45.20192337036133,
      "learning_rate": 7.675483214649033e-06,
      "loss": 1.5489,
      "step": 83210
    },
    {
      "epoch": 42.32960325534079,
      "grad_norm": 48.16987991333008,
      "learning_rate": 7.670396744659206e-06,
      "loss": 1.5635,
      "step": 83220
    },
    {
      "epoch": 42.33468972533062,
      "grad_norm": 43.89596176147461,
      "learning_rate": 7.66531027466938e-06,
      "loss": 1.5489,
      "step": 83230
    },
    {
      "epoch": 42.33977619532045,
      "grad_norm": 43.34267807006836,
      "learning_rate": 7.660223804679553e-06,
      "loss": 1.5767,
      "step": 83240
    },
    {
      "epoch": 42.34486266531027,
      "grad_norm": 38.461769104003906,
      "learning_rate": 7.655137334689726e-06,
      "loss": 1.487,
      "step": 83250
    },
    {
      "epoch": 42.3499491353001,
      "grad_norm": 35.26322937011719,
      "learning_rate": 7.650050864699898e-06,
      "loss": 1.6043,
      "step": 83260
    },
    {
      "epoch": 42.35503560528993,
      "grad_norm": 40.04499435424805,
      "learning_rate": 7.644964394710073e-06,
      "loss": 1.5854,
      "step": 83270
    },
    {
      "epoch": 42.360122075279754,
      "grad_norm": 46.452606201171875,
      "learning_rate": 7.639877924720244e-06,
      "loss": 1.5035,
      "step": 83280
    },
    {
      "epoch": 42.36520854526958,
      "grad_norm": 37.74367141723633,
      "learning_rate": 7.634791454730418e-06,
      "loss": 1.5973,
      "step": 83290
    },
    {
      "epoch": 42.37029501525941,
      "grad_norm": 47.51755142211914,
      "learning_rate": 7.62970498474059e-06,
      "loss": 1.5895,
      "step": 83300
    },
    {
      "epoch": 42.375381485249235,
      "grad_norm": 44.12284851074219,
      "learning_rate": 7.624618514750763e-06,
      "loss": 1.5811,
      "step": 83310
    },
    {
      "epoch": 42.38046795523906,
      "grad_norm": 44.33111572265625,
      "learning_rate": 7.6195320447609365e-06,
      "loss": 1.5867,
      "step": 83320
    },
    {
      "epoch": 42.38555442522889,
      "grad_norm": 44.16753387451172,
      "learning_rate": 7.614445574771109e-06,
      "loss": 1.5481,
      "step": 83330
    },
    {
      "epoch": 42.390640895218716,
      "grad_norm": 37.04269790649414,
      "learning_rate": 7.609359104781281e-06,
      "loss": 1.6372,
      "step": 83340
    },
    {
      "epoch": 42.39572736520854,
      "grad_norm": 50.439971923828125,
      "learning_rate": 7.6042726347914555e-06,
      "loss": 1.5821,
      "step": 83350
    },
    {
      "epoch": 42.40081383519837,
      "grad_norm": 46.492374420166016,
      "learning_rate": 7.599186164801628e-06,
      "loss": 1.5577,
      "step": 83360
    },
    {
      "epoch": 42.4059003051882,
      "grad_norm": 42.229496002197266,
      "learning_rate": 7.594099694811801e-06,
      "loss": 1.4932,
      "step": 83370
    },
    {
      "epoch": 42.410986775178024,
      "grad_norm": 44.9065055847168,
      "learning_rate": 7.589013224821974e-06,
      "loss": 1.5456,
      "step": 83380
    },
    {
      "epoch": 42.41607324516785,
      "grad_norm": 35.28645324707031,
      "learning_rate": 7.583926754832146e-06,
      "loss": 1.5263,
      "step": 83390
    },
    {
      "epoch": 42.42115971515768,
      "grad_norm": 50.173744201660156,
      "learning_rate": 7.57884028484232e-06,
      "loss": 1.6038,
      "step": 83400
    },
    {
      "epoch": 42.426246185147505,
      "grad_norm": 36.459415435791016,
      "learning_rate": 7.573753814852493e-06,
      "loss": 1.5889,
      "step": 83410
    },
    {
      "epoch": 42.43133265513733,
      "grad_norm": 49.08733367919922,
      "learning_rate": 7.568667344862666e-06,
      "loss": 1.5735,
      "step": 83420
    },
    {
      "epoch": 42.43641912512716,
      "grad_norm": 43.28739929199219,
      "learning_rate": 7.563580874872838e-06,
      "loss": 1.5164,
      "step": 83430
    },
    {
      "epoch": 42.441505595116986,
      "grad_norm": 42.455570220947266,
      "learning_rate": 7.558494404883011e-06,
      "loss": 1.6319,
      "step": 83440
    },
    {
      "epoch": 42.44659206510681,
      "grad_norm": 42.30148696899414,
      "learning_rate": 7.553407934893185e-06,
      "loss": 1.5597,
      "step": 83450
    },
    {
      "epoch": 42.45167853509664,
      "grad_norm": 40.50177764892578,
      "learning_rate": 7.548321464903357e-06,
      "loss": 1.5476,
      "step": 83460
    },
    {
      "epoch": 42.45676500508647,
      "grad_norm": 43.67573928833008,
      "learning_rate": 7.54323499491353e-06,
      "loss": 1.5823,
      "step": 83470
    },
    {
      "epoch": 42.461851475076294,
      "grad_norm": 34.359432220458984,
      "learning_rate": 7.538148524923704e-06,
      "loss": 1.6142,
      "step": 83480
    },
    {
      "epoch": 42.46693794506612,
      "grad_norm": 44.61830139160156,
      "learning_rate": 7.5330620549338756e-06,
      "loss": 1.6086,
      "step": 83490
    },
    {
      "epoch": 42.47202441505595,
      "grad_norm": 37.68667984008789,
      "learning_rate": 7.52797558494405e-06,
      "loss": 1.5184,
      "step": 83500
    },
    {
      "epoch": 42.477110885045775,
      "grad_norm": 34.94719696044922,
      "learning_rate": 7.522889114954222e-06,
      "loss": 1.5484,
      "step": 83510
    },
    {
      "epoch": 42.4821973550356,
      "grad_norm": 39.81378173828125,
      "learning_rate": 7.5178026449643946e-06,
      "loss": 1.6678,
      "step": 83520
    },
    {
      "epoch": 42.48728382502543,
      "grad_norm": 47.88956069946289,
      "learning_rate": 7.512716174974569e-06,
      "loss": 1.54,
      "step": 83530
    },
    {
      "epoch": 42.492370295015256,
      "grad_norm": 43.90085983276367,
      "learning_rate": 7.507629704984741e-06,
      "loss": 1.5731,
      "step": 83540
    },
    {
      "epoch": 42.49745676500508,
      "grad_norm": 49.62684631347656,
      "learning_rate": 7.502543234994914e-06,
      "loss": 1.5031,
      "step": 83550
    },
    {
      "epoch": 42.50254323499492,
      "grad_norm": 47.574806213378906,
      "learning_rate": 7.497456765005087e-06,
      "loss": 1.5702,
      "step": 83560
    },
    {
      "epoch": 42.507629704984744,
      "grad_norm": 36.02314376831055,
      "learning_rate": 7.492370295015259e-06,
      "loss": 1.5141,
      "step": 83570
    },
    {
      "epoch": 42.51271617497457,
      "grad_norm": 41.87639236450195,
      "learning_rate": 7.487283825025433e-06,
      "loss": 1.5769,
      "step": 83580
    },
    {
      "epoch": 42.5178026449644,
      "grad_norm": 44.52082443237305,
      "learning_rate": 7.482197355035606e-06,
      "loss": 1.5676,
      "step": 83590
    },
    {
      "epoch": 42.522889114954225,
      "grad_norm": 45.97563934326172,
      "learning_rate": 7.477110885045778e-06,
      "loss": 1.6135,
      "step": 83600
    },
    {
      "epoch": 42.52797558494405,
      "grad_norm": 43.94615936279297,
      "learning_rate": 7.4720244150559515e-06,
      "loss": 1.6169,
      "step": 83610
    },
    {
      "epoch": 42.53306205493388,
      "grad_norm": 40.154422760009766,
      "learning_rate": 7.466937945066124e-06,
      "loss": 1.6002,
      "step": 83620
    },
    {
      "epoch": 42.538148524923706,
      "grad_norm": 41.446014404296875,
      "learning_rate": 7.461851475076298e-06,
      "loss": 1.6545,
      "step": 83630
    },
    {
      "epoch": 42.54323499491353,
      "grad_norm": 33.21293258666992,
      "learning_rate": 7.4567650050864705e-06,
      "loss": 1.5886,
      "step": 83640
    },
    {
      "epoch": 42.54832146490336,
      "grad_norm": 36.83583068847656,
      "learning_rate": 7.451678535096643e-06,
      "loss": 1.525,
      "step": 83650
    },
    {
      "epoch": 42.55340793489319,
      "grad_norm": 55.04896926879883,
      "learning_rate": 7.446592065106816e-06,
      "loss": 1.613,
      "step": 83660
    },
    {
      "epoch": 42.558494404883014,
      "grad_norm": 59.790428161621094,
      "learning_rate": 7.441505595116989e-06,
      "loss": 1.4688,
      "step": 83670
    },
    {
      "epoch": 42.56358087487284,
      "grad_norm": 37.92287063598633,
      "learning_rate": 7.436419125127161e-06,
      "loss": 1.6185,
      "step": 83680
    },
    {
      "epoch": 42.56866734486267,
      "grad_norm": 43.25898742675781,
      "learning_rate": 7.431332655137335e-06,
      "loss": 1.5321,
      "step": 83690
    },
    {
      "epoch": 42.573753814852495,
      "grad_norm": 43.95405197143555,
      "learning_rate": 7.426246185147508e-06,
      "loss": 1.5897,
      "step": 83700
    },
    {
      "epoch": 42.57884028484232,
      "grad_norm": 36.668853759765625,
      "learning_rate": 7.421159715157682e-06,
      "loss": 1.6178,
      "step": 83710
    },
    {
      "epoch": 42.58392675483215,
      "grad_norm": 39.50349426269531,
      "learning_rate": 7.416073245167854e-06,
      "loss": 1.574,
      "step": 83720
    },
    {
      "epoch": 42.589013224821976,
      "grad_norm": 51.261878967285156,
      "learning_rate": 7.410986775178026e-06,
      "loss": 1.5666,
      "step": 83730
    },
    {
      "epoch": 42.5940996948118,
      "grad_norm": 39.32022476196289,
      "learning_rate": 7.4059003051882e-06,
      "loss": 1.5338,
      "step": 83740
    },
    {
      "epoch": 42.59918616480163,
      "grad_norm": 44.37998580932617,
      "learning_rate": 7.400813835198372e-06,
      "loss": 1.6318,
      "step": 83750
    },
    {
      "epoch": 42.60427263479146,
      "grad_norm": 40.665679931640625,
      "learning_rate": 7.3957273652085465e-06,
      "loss": 1.5766,
      "step": 83760
    },
    {
      "epoch": 42.609359104781284,
      "grad_norm": 44.30683517456055,
      "learning_rate": 7.390640895218719e-06,
      "loss": 1.4948,
      "step": 83770
    },
    {
      "epoch": 42.61444557477111,
      "grad_norm": 42.16691589355469,
      "learning_rate": 7.385554425228891e-06,
      "loss": 1.5533,
      "step": 83780
    },
    {
      "epoch": 42.61953204476094,
      "grad_norm": 37.402557373046875,
      "learning_rate": 7.380467955239065e-06,
      "loss": 1.5757,
      "step": 83790
    },
    {
      "epoch": 42.624618514750765,
      "grad_norm": 44.8670539855957,
      "learning_rate": 7.375381485249237e-06,
      "loss": 1.5885,
      "step": 83800
    },
    {
      "epoch": 42.62970498474059,
      "grad_norm": 50.351356506347656,
      "learning_rate": 7.3702950152594096e-06,
      "loss": 1.5239,
      "step": 83810
    },
    {
      "epoch": 42.63479145473042,
      "grad_norm": 39.50466537475586,
      "learning_rate": 7.365208545269584e-06,
      "loss": 1.4486,
      "step": 83820
    },
    {
      "epoch": 42.639877924720246,
      "grad_norm": 40.35117721557617,
      "learning_rate": 7.360122075279756e-06,
      "loss": 1.5669,
      "step": 83830
    },
    {
      "epoch": 42.64496439471007,
      "grad_norm": 42.43669891357422,
      "learning_rate": 7.355035605289929e-06,
      "loss": 1.5449,
      "step": 83840
    },
    {
      "epoch": 42.6500508646999,
      "grad_norm": 43.35322952270508,
      "learning_rate": 7.349949135300102e-06,
      "loss": 1.5615,
      "step": 83850
    },
    {
      "epoch": 42.65513733468973,
      "grad_norm": 38.988739013671875,
      "learning_rate": 7.344862665310274e-06,
      "loss": 1.5169,
      "step": 83860
    },
    {
      "epoch": 42.66022380467955,
      "grad_norm": 47.91244888305664,
      "learning_rate": 7.339776195320448e-06,
      "loss": 1.5405,
      "step": 83870
    },
    {
      "epoch": 42.66531027466938,
      "grad_norm": 38.69990921020508,
      "learning_rate": 7.334689725330621e-06,
      "loss": 1.4973,
      "step": 83880
    },
    {
      "epoch": 42.67039674465921,
      "grad_norm": 42.813865661621094,
      "learning_rate": 7.329603255340794e-06,
      "loss": 1.5408,
      "step": 83890
    },
    {
      "epoch": 42.675483214649034,
      "grad_norm": 36.34762191772461,
      "learning_rate": 7.3245167853509665e-06,
      "loss": 1.6237,
      "step": 83900
    },
    {
      "epoch": 42.68056968463886,
      "grad_norm": 42.7385139465332,
      "learning_rate": 7.319430315361139e-06,
      "loss": 1.5492,
      "step": 83910
    },
    {
      "epoch": 42.68565615462869,
      "grad_norm": 44.760223388671875,
      "learning_rate": 7.314343845371313e-06,
      "loss": 1.5386,
      "step": 83920
    },
    {
      "epoch": 42.690742624618515,
      "grad_norm": 40.846832275390625,
      "learning_rate": 7.3092573753814855e-06,
      "loss": 1.5589,
      "step": 83930
    },
    {
      "epoch": 42.69582909460834,
      "grad_norm": 50.5850715637207,
      "learning_rate": 7.304170905391658e-06,
      "loss": 1.4821,
      "step": 83940
    },
    {
      "epoch": 42.70091556459817,
      "grad_norm": 41.21669006347656,
      "learning_rate": 7.299084435401832e-06,
      "loss": 1.4971,
      "step": 83950
    },
    {
      "epoch": 42.706002034587996,
      "grad_norm": 47.162784576416016,
      "learning_rate": 7.2939979654120045e-06,
      "loss": 1.5363,
      "step": 83960
    },
    {
      "epoch": 42.71108850457782,
      "grad_norm": 41.799564361572266,
      "learning_rate": 7.288911495422178e-06,
      "loss": 1.5839,
      "step": 83970
    },
    {
      "epoch": 42.71617497456765,
      "grad_norm": 40.2485466003418,
      "learning_rate": 7.28382502543235e-06,
      "loss": 1.5903,
      "step": 83980
    },
    {
      "epoch": 42.72126144455748,
      "grad_norm": 56.21811294555664,
      "learning_rate": 7.278738555442523e-06,
      "loss": 1.5684,
      "step": 83990
    },
    {
      "epoch": 42.726347914547304,
      "grad_norm": 42.93793487548828,
      "learning_rate": 7.273652085452697e-06,
      "loss": 1.5189,
      "step": 84000
    },
    {
      "epoch": 42.73143438453713,
      "grad_norm": 40.1952018737793,
      "learning_rate": 7.268565615462869e-06,
      "loss": 1.5068,
      "step": 84010
    },
    {
      "epoch": 42.73652085452696,
      "grad_norm": 37.12041473388672,
      "learning_rate": 7.263479145473042e-06,
      "loss": 1.5429,
      "step": 84020
    },
    {
      "epoch": 42.741607324516785,
      "grad_norm": 43.569580078125,
      "learning_rate": 7.258392675483215e-06,
      "loss": 1.6356,
      "step": 84030
    },
    {
      "epoch": 42.74669379450661,
      "grad_norm": 54.67950439453125,
      "learning_rate": 7.253306205493387e-06,
      "loss": 1.5958,
      "step": 84040
    },
    {
      "epoch": 42.75178026449644,
      "grad_norm": 34.466278076171875,
      "learning_rate": 7.2482197355035615e-06,
      "loss": 1.6077,
      "step": 84050
    },
    {
      "epoch": 42.756866734486266,
      "grad_norm": 50.60432052612305,
      "learning_rate": 7.243133265513734e-06,
      "loss": 1.5784,
      "step": 84060
    },
    {
      "epoch": 42.76195320447609,
      "grad_norm": 44.159271240234375,
      "learning_rate": 7.238046795523906e-06,
      "loss": 1.5371,
      "step": 84070
    },
    {
      "epoch": 42.76703967446592,
      "grad_norm": 44.83648681640625,
      "learning_rate": 7.23296032553408e-06,
      "loss": 1.6021,
      "step": 84080
    },
    {
      "epoch": 42.77212614445575,
      "grad_norm": 35.0796012878418,
      "learning_rate": 7.227873855544252e-06,
      "loss": 1.5407,
      "step": 84090
    },
    {
      "epoch": 42.777212614445574,
      "grad_norm": 39.4890022277832,
      "learning_rate": 7.222787385554426e-06,
      "loss": 1.5547,
      "step": 84100
    },
    {
      "epoch": 42.7822990844354,
      "grad_norm": 50.72475051879883,
      "learning_rate": 7.217700915564599e-06,
      "loss": 1.6311,
      "step": 84110
    },
    {
      "epoch": 42.78738555442523,
      "grad_norm": 47.580928802490234,
      "learning_rate": 7.212614445574771e-06,
      "loss": 1.534,
      "step": 84120
    },
    {
      "epoch": 42.792472024415055,
      "grad_norm": 39.379371643066406,
      "learning_rate": 7.207527975584944e-06,
      "loss": 1.664,
      "step": 84130
    },
    {
      "epoch": 42.79755849440488,
      "grad_norm": 40.37694549560547,
      "learning_rate": 7.202441505595117e-06,
      "loss": 1.6071,
      "step": 84140
    },
    {
      "epoch": 42.80264496439471,
      "grad_norm": 40.85978317260742,
      "learning_rate": 7.197355035605289e-06,
      "loss": 1.4793,
      "step": 84150
    },
    {
      "epoch": 42.807731434384536,
      "grad_norm": 46.345176696777344,
      "learning_rate": 7.192268565615463e-06,
      "loss": 1.5303,
      "step": 84160
    },
    {
      "epoch": 42.81281790437436,
      "grad_norm": 40.644187927246094,
      "learning_rate": 7.187182095625636e-06,
      "loss": 1.5077,
      "step": 84170
    },
    {
      "epoch": 42.81790437436419,
      "grad_norm": 50.10322189331055,
      "learning_rate": 7.18209562563581e-06,
      "loss": 1.6463,
      "step": 84180
    },
    {
      "epoch": 42.82299084435402,
      "grad_norm": 40.79107666015625,
      "learning_rate": 7.177009155645982e-06,
      "loss": 1.6148,
      "step": 84190
    },
    {
      "epoch": 42.828077314343844,
      "grad_norm": 47.997249603271484,
      "learning_rate": 7.171922685656155e-06,
      "loss": 1.5294,
      "step": 84200
    },
    {
      "epoch": 42.83316378433367,
      "grad_norm": 53.89052200317383,
      "learning_rate": 7.166836215666328e-06,
      "loss": 1.5787,
      "step": 84210
    },
    {
      "epoch": 42.8382502543235,
      "grad_norm": 44.314388275146484,
      "learning_rate": 7.1617497456765005e-06,
      "loss": 1.6027,
      "step": 84220
    },
    {
      "epoch": 42.843336724313325,
      "grad_norm": 40.508018493652344,
      "learning_rate": 7.156663275686675e-06,
      "loss": 1.5324,
      "step": 84230
    },
    {
      "epoch": 42.84842319430315,
      "grad_norm": 61.38996505737305,
      "learning_rate": 7.151576805696847e-06,
      "loss": 1.5922,
      "step": 84240
    },
    {
      "epoch": 42.85350966429298,
      "grad_norm": 41.85647964477539,
      "learning_rate": 7.1464903357070195e-06,
      "loss": 1.5408,
      "step": 84250
    },
    {
      "epoch": 42.858596134282806,
      "grad_norm": 40.69748306274414,
      "learning_rate": 7.141403865717193e-06,
      "loss": 1.4822,
      "step": 84260
    },
    {
      "epoch": 42.86368260427263,
      "grad_norm": 56.76471710205078,
      "learning_rate": 7.136317395727365e-06,
      "loss": 1.6176,
      "step": 84270
    },
    {
      "epoch": 42.86876907426246,
      "grad_norm": 39.71642303466797,
      "learning_rate": 7.131230925737538e-06,
      "loss": 1.479,
      "step": 84280
    },
    {
      "epoch": 42.87385554425229,
      "grad_norm": 43.19898986816406,
      "learning_rate": 7.126144455747712e-06,
      "loss": 1.6142,
      "step": 84290
    },
    {
      "epoch": 42.878942014242114,
      "grad_norm": 45.21533203125,
      "learning_rate": 7.121057985757884e-06,
      "loss": 1.5864,
      "step": 84300
    },
    {
      "epoch": 42.88402848423194,
      "grad_norm": 42.414615631103516,
      "learning_rate": 7.1159715157680575e-06,
      "loss": 1.5439,
      "step": 84310
    },
    {
      "epoch": 42.88911495422177,
      "grad_norm": 40.71816635131836,
      "learning_rate": 7.11088504577823e-06,
      "loss": 1.5517,
      "step": 84320
    },
    {
      "epoch": 42.894201424211595,
      "grad_norm": 37.10969161987305,
      "learning_rate": 7.105798575788402e-06,
      "loss": 1.6113,
      "step": 84330
    },
    {
      "epoch": 42.89928789420142,
      "grad_norm": 41.792850494384766,
      "learning_rate": 7.1007121057985765e-06,
      "loss": 1.5455,
      "step": 84340
    },
    {
      "epoch": 42.90437436419125,
      "grad_norm": 44.408103942871094,
      "learning_rate": 7.095625635808749e-06,
      "loss": 1.5677,
      "step": 84350
    },
    {
      "epoch": 42.909460834181075,
      "grad_norm": 40.02362823486328,
      "learning_rate": 7.090539165818923e-06,
      "loss": 1.5736,
      "step": 84360
    },
    {
      "epoch": 42.9145473041709,
      "grad_norm": 47.12769317626953,
      "learning_rate": 7.085452695829095e-06,
      "loss": 1.5324,
      "step": 84370
    },
    {
      "epoch": 42.91963377416073,
      "grad_norm": 50.462989807128906,
      "learning_rate": 7.080366225839267e-06,
      "loss": 1.6288,
      "step": 84380
    },
    {
      "epoch": 42.924720244150556,
      "grad_norm": 49.23732376098633,
      "learning_rate": 7.075279755849441e-06,
      "loss": 1.5259,
      "step": 84390
    },
    {
      "epoch": 42.92980671414038,
      "grad_norm": 39.32955551147461,
      "learning_rate": 7.070193285859614e-06,
      "loss": 1.555,
      "step": 84400
    },
    {
      "epoch": 42.93489318413021,
      "grad_norm": 39.853004455566406,
      "learning_rate": 7.065106815869786e-06,
      "loss": 1.467,
      "step": 84410
    },
    {
      "epoch": 42.93997965412004,
      "grad_norm": 36.38093948364258,
      "learning_rate": 7.06002034587996e-06,
      "loss": 1.5737,
      "step": 84420
    },
    {
      "epoch": 42.945066124109864,
      "grad_norm": 43.17484664916992,
      "learning_rate": 7.054933875890133e-06,
      "loss": 1.6088,
      "step": 84430
    },
    {
      "epoch": 42.95015259409969,
      "grad_norm": 44.460968017578125,
      "learning_rate": 7.049847405900306e-06,
      "loss": 1.6175,
      "step": 84440
    },
    {
      "epoch": 42.955239064089525,
      "grad_norm": 53.45992660522461,
      "learning_rate": 7.044760935910478e-06,
      "loss": 1.5831,
      "step": 84450
    },
    {
      "epoch": 42.96032553407935,
      "grad_norm": 34.215911865234375,
      "learning_rate": 7.039674465920651e-06,
      "loss": 1.613,
      "step": 84460
    },
    {
      "epoch": 42.96541200406918,
      "grad_norm": 40.67665100097656,
      "learning_rate": 7.034587995930825e-06,
      "loss": 1.5096,
      "step": 84470
    },
    {
      "epoch": 42.970498474059006,
      "grad_norm": 43.71611404418945,
      "learning_rate": 7.029501525940997e-06,
      "loss": 1.572,
      "step": 84480
    },
    {
      "epoch": 42.97558494404883,
      "grad_norm": 40.919795989990234,
      "learning_rate": 7.02441505595117e-06,
      "loss": 1.6149,
      "step": 84490
    },
    {
      "epoch": 42.98067141403866,
      "grad_norm": 37.63511276245117,
      "learning_rate": 7.019328585961343e-06,
      "loss": 1.5706,
      "step": 84500
    },
    {
      "epoch": 42.98575788402849,
      "grad_norm": 36.83631134033203,
      "learning_rate": 7.0142421159715156e-06,
      "loss": 1.5716,
      "step": 84510
    },
    {
      "epoch": 42.990844354018314,
      "grad_norm": 40.3325080871582,
      "learning_rate": 7.00915564598169e-06,
      "loss": 1.5139,
      "step": 84520
    },
    {
      "epoch": 42.99593082400814,
      "grad_norm": 40.50200271606445,
      "learning_rate": 7.004069175991862e-06,
      "loss": 1.448,
      "step": 84530
    },
    {
      "epoch": 43.0,
      "eval_loss": 5.069759368896484,
      "eval_runtime": 2.8758,
      "eval_samples_per_second": 964.957,
      "eval_steps_per_second": 120.663,
      "step": 84538
    },
    {
      "epoch": 43.00101729399797,
      "grad_norm": 36.46674728393555,
      "learning_rate": 6.9989827060020346e-06,
      "loss": 1.668,
      "step": 84540
    },
    {
      "epoch": 43.006103763987795,
      "grad_norm": 39.00887680053711,
      "learning_rate": 6.993896236012208e-06,
      "loss": 1.5737,
      "step": 84550
    },
    {
      "epoch": 43.01119023397762,
      "grad_norm": 34.10009002685547,
      "learning_rate": 6.98880976602238e-06,
      "loss": 1.4876,
      "step": 84560
    },
    {
      "epoch": 43.01627670396745,
      "grad_norm": 34.6662712097168,
      "learning_rate": 6.983723296032554e-06,
      "loss": 1.5903,
      "step": 84570
    },
    {
      "epoch": 43.021363173957276,
      "grad_norm": 49.907955169677734,
      "learning_rate": 6.978636826042727e-06,
      "loss": 1.6005,
      "step": 84580
    },
    {
      "epoch": 43.0264496439471,
      "grad_norm": 40.90534591674805,
      "learning_rate": 6.973550356052899e-06,
      "loss": 1.5582,
      "step": 84590
    },
    {
      "epoch": 43.03153611393693,
      "grad_norm": 59.747962951660156,
      "learning_rate": 6.968463886063073e-06,
      "loss": 1.5499,
      "step": 84600
    },
    {
      "epoch": 43.03662258392676,
      "grad_norm": 51.963600158691406,
      "learning_rate": 6.963377416073245e-06,
      "loss": 1.5831,
      "step": 84610
    },
    {
      "epoch": 43.041709053916584,
      "grad_norm": 35.92172622680664,
      "learning_rate": 6.958290946083417e-06,
      "loss": 1.5436,
      "step": 84620
    },
    {
      "epoch": 43.04679552390641,
      "grad_norm": 38.9348258972168,
      "learning_rate": 6.9532044760935915e-06,
      "loss": 1.5152,
      "step": 84630
    },
    {
      "epoch": 43.05188199389624,
      "grad_norm": 42.39753341674805,
      "learning_rate": 6.948118006103764e-06,
      "loss": 1.5989,
      "step": 84640
    },
    {
      "epoch": 43.056968463886065,
      "grad_norm": 31.175628662109375,
      "learning_rate": 6.943031536113938e-06,
      "loss": 1.5656,
      "step": 84650
    },
    {
      "epoch": 43.06205493387589,
      "grad_norm": 31.39844512939453,
      "learning_rate": 6.9379450661241105e-06,
      "loss": 1.6066,
      "step": 84660
    },
    {
      "epoch": 43.06714140386572,
      "grad_norm": 51.43611145019531,
      "learning_rate": 6.932858596134283e-06,
      "loss": 1.4234,
      "step": 84670
    },
    {
      "epoch": 43.072227873855546,
      "grad_norm": 39.36445236206055,
      "learning_rate": 6.927772126144456e-06,
      "loss": 1.5735,
      "step": 84680
    },
    {
      "epoch": 43.07731434384537,
      "grad_norm": 46.275123596191406,
      "learning_rate": 6.922685656154629e-06,
      "loss": 1.4825,
      "step": 84690
    },
    {
      "epoch": 43.0824008138352,
      "grad_norm": 38.60537338256836,
      "learning_rate": 6.917599186164803e-06,
      "loss": 1.5325,
      "step": 84700
    },
    {
      "epoch": 43.08748728382503,
      "grad_norm": 43.64627456665039,
      "learning_rate": 6.912512716174975e-06,
      "loss": 1.6119,
      "step": 84710
    },
    {
      "epoch": 43.092573753814854,
      "grad_norm": 46.6423454284668,
      "learning_rate": 6.907426246185148e-06,
      "loss": 1.519,
      "step": 84720
    },
    {
      "epoch": 43.09766022380468,
      "grad_norm": 36.96440887451172,
      "learning_rate": 6.902339776195321e-06,
      "loss": 1.5025,
      "step": 84730
    },
    {
      "epoch": 43.10274669379451,
      "grad_norm": 38.531620025634766,
      "learning_rate": 6.897253306205493e-06,
      "loss": 1.5562,
      "step": 84740
    },
    {
      "epoch": 43.107833163784335,
      "grad_norm": 39.52323532104492,
      "learning_rate": 6.892166836215666e-06,
      "loss": 1.5347,
      "step": 84750
    },
    {
      "epoch": 43.11291963377416,
      "grad_norm": 40.88222122192383,
      "learning_rate": 6.88708036622584e-06,
      "loss": 1.5979,
      "step": 84760
    },
    {
      "epoch": 43.11800610376399,
      "grad_norm": 36.35357666015625,
      "learning_rate": 6.881993896236012e-06,
      "loss": 1.4402,
      "step": 84770
    },
    {
      "epoch": 43.123092573753816,
      "grad_norm": 41.9826774597168,
      "learning_rate": 6.876907426246186e-06,
      "loss": 1.5051,
      "step": 84780
    },
    {
      "epoch": 43.12817904374364,
      "grad_norm": 43.95674514770508,
      "learning_rate": 6.871820956256358e-06,
      "loss": 1.5886,
      "step": 84790
    },
    {
      "epoch": 43.13326551373347,
      "grad_norm": 41.17108917236328,
      "learning_rate": 6.8667344862665306e-06,
      "loss": 1.5215,
      "step": 84800
    },
    {
      "epoch": 43.1383519837233,
      "grad_norm": 34.96697998046875,
      "learning_rate": 6.861648016276705e-06,
      "loss": 1.5199,
      "step": 84810
    },
    {
      "epoch": 43.143438453713124,
      "grad_norm": 42.55274963378906,
      "learning_rate": 6.856561546286877e-06,
      "loss": 1.5907,
      "step": 84820
    },
    {
      "epoch": 43.14852492370295,
      "grad_norm": 41.098819732666016,
      "learning_rate": 6.8514750762970496e-06,
      "loss": 1.4943,
      "step": 84830
    },
    {
      "epoch": 43.15361139369278,
      "grad_norm": 39.30190658569336,
      "learning_rate": 6.846388606307224e-06,
      "loss": 1.4517,
      "step": 84840
    },
    {
      "epoch": 43.158697863682605,
      "grad_norm": 43.65767288208008,
      "learning_rate": 6.841302136317395e-06,
      "loss": 1.5405,
      "step": 84850
    },
    {
      "epoch": 43.16378433367243,
      "grad_norm": 52.34211349487305,
      "learning_rate": 6.836215666327569e-06,
      "loss": 1.6048,
      "step": 84860
    },
    {
      "epoch": 43.16887080366226,
      "grad_norm": 33.971126556396484,
      "learning_rate": 6.831129196337742e-06,
      "loss": 1.4817,
      "step": 84870
    },
    {
      "epoch": 43.173957273652086,
      "grad_norm": 41.34273910522461,
      "learning_rate": 6.826042726347914e-06,
      "loss": 1.4449,
      "step": 84880
    },
    {
      "epoch": 43.17904374364191,
      "grad_norm": 45.591468811035156,
      "learning_rate": 6.820956256358088e-06,
      "loss": 1.5504,
      "step": 84890
    },
    {
      "epoch": 43.18413021363174,
      "grad_norm": 41.3431510925293,
      "learning_rate": 6.815869786368261e-06,
      "loss": 1.5469,
      "step": 84900
    },
    {
      "epoch": 43.18921668362157,
      "grad_norm": 32.88360595703125,
      "learning_rate": 6.810783316378434e-06,
      "loss": 1.5818,
      "step": 84910
    },
    {
      "epoch": 43.19430315361139,
      "grad_norm": 42.48585891723633,
      "learning_rate": 6.8056968463886065e-06,
      "loss": 1.5862,
      "step": 84920
    },
    {
      "epoch": 43.19938962360122,
      "grad_norm": 34.27195739746094,
      "learning_rate": 6.800610376398779e-06,
      "loss": 1.5629,
      "step": 84930
    },
    {
      "epoch": 43.20447609359105,
      "grad_norm": 40.901859283447266,
      "learning_rate": 6.795523906408953e-06,
      "loss": 1.5515,
      "step": 84940
    },
    {
      "epoch": 43.209562563580874,
      "grad_norm": 40.632808685302734,
      "learning_rate": 6.7904374364191255e-06,
      "loss": 1.5493,
      "step": 84950
    },
    {
      "epoch": 43.2146490335707,
      "grad_norm": 35.87813949584961,
      "learning_rate": 6.785350966429298e-06,
      "loss": 1.5569,
      "step": 84960
    },
    {
      "epoch": 43.21973550356053,
      "grad_norm": 39.034725189208984,
      "learning_rate": 6.780264496439471e-06,
      "loss": 1.5818,
      "step": 84970
    },
    {
      "epoch": 43.224821973550355,
      "grad_norm": 43.94853973388672,
      "learning_rate": 6.775178026449644e-06,
      "loss": 1.5994,
      "step": 84980
    },
    {
      "epoch": 43.22990844354018,
      "grad_norm": 37.45707702636719,
      "learning_rate": 6.770091556459818e-06,
      "loss": 1.4549,
      "step": 84990
    },
    {
      "epoch": 43.23499491353001,
      "grad_norm": 47.703338623046875,
      "learning_rate": 6.76500508646999e-06,
      "loss": 1.5861,
      "step": 85000
    },
    {
      "epoch": 43.240081383519836,
      "grad_norm": 45.97636413574219,
      "learning_rate": 6.759918616480163e-06,
      "loss": 1.6304,
      "step": 85010
    },
    {
      "epoch": 43.24516785350966,
      "grad_norm": 35.91561508178711,
      "learning_rate": 6.754832146490336e-06,
      "loss": 1.4677,
      "step": 85020
    },
    {
      "epoch": 43.25025432349949,
      "grad_norm": 43.3198127746582,
      "learning_rate": 6.749745676500508e-06,
      "loss": 1.5758,
      "step": 85030
    },
    {
      "epoch": 43.25534079348932,
      "grad_norm": 43.34193801879883,
      "learning_rate": 6.7446592065106825e-06,
      "loss": 1.5362,
      "step": 85040
    },
    {
      "epoch": 43.260427263479144,
      "grad_norm": 50.47801208496094,
      "learning_rate": 6.739572736520855e-06,
      "loss": 1.5995,
      "step": 85050
    },
    {
      "epoch": 43.26551373346897,
      "grad_norm": 45.69951629638672,
      "learning_rate": 6.734486266531027e-06,
      "loss": 1.553,
      "step": 85060
    },
    {
      "epoch": 43.2706002034588,
      "grad_norm": 35.76703643798828,
      "learning_rate": 6.7293997965412015e-06,
      "loss": 1.5397,
      "step": 85070
    },
    {
      "epoch": 43.275686673448625,
      "grad_norm": 38.01152038574219,
      "learning_rate": 6.724313326551374e-06,
      "loss": 1.6333,
      "step": 85080
    },
    {
      "epoch": 43.28077314343845,
      "grad_norm": 40.1659049987793,
      "learning_rate": 6.719226856561546e-06,
      "loss": 1.5917,
      "step": 85090
    },
    {
      "epoch": 43.28585961342828,
      "grad_norm": 42.910945892333984,
      "learning_rate": 6.71414038657172e-06,
      "loss": 1.5303,
      "step": 85100
    },
    {
      "epoch": 43.290946083418106,
      "grad_norm": 40.619808197021484,
      "learning_rate": 6.709053916581892e-06,
      "loss": 1.4924,
      "step": 85110
    },
    {
      "epoch": 43.29603255340793,
      "grad_norm": 37.35618591308594,
      "learning_rate": 6.703967446592066e-06,
      "loss": 1.6311,
      "step": 85120
    },
    {
      "epoch": 43.30111902339776,
      "grad_norm": 47.993370056152344,
      "learning_rate": 6.698880976602239e-06,
      "loss": 1.5263,
      "step": 85130
    },
    {
      "epoch": 43.30620549338759,
      "grad_norm": 35.192874908447266,
      "learning_rate": 6.693794506612411e-06,
      "loss": 1.602,
      "step": 85140
    },
    {
      "epoch": 43.311291963377414,
      "grad_norm": 44.965911865234375,
      "learning_rate": 6.688708036622584e-06,
      "loss": 1.5502,
      "step": 85150
    },
    {
      "epoch": 43.31637843336724,
      "grad_norm": 43.53632736206055,
      "learning_rate": 6.683621566632757e-06,
      "loss": 1.4647,
      "step": 85160
    },
    {
      "epoch": 43.32146490335707,
      "grad_norm": 35.64403533935547,
      "learning_rate": 6.678535096642931e-06,
      "loss": 1.5288,
      "step": 85170
    },
    {
      "epoch": 43.326551373346895,
      "grad_norm": 50.9533805847168,
      "learning_rate": 6.673448626653103e-06,
      "loss": 1.5679,
      "step": 85180
    },
    {
      "epoch": 43.33163784333672,
      "grad_norm": 41.638450622558594,
      "learning_rate": 6.668362156663276e-06,
      "loss": 1.5686,
      "step": 85190
    },
    {
      "epoch": 43.33672431332655,
      "grad_norm": 32.815101623535156,
      "learning_rate": 6.663275686673449e-06,
      "loss": 1.5518,
      "step": 85200
    },
    {
      "epoch": 43.341810783316376,
      "grad_norm": 37.98115921020508,
      "learning_rate": 6.6581892166836216e-06,
      "loss": 1.5655,
      "step": 85210
    },
    {
      "epoch": 43.3468972533062,
      "grad_norm": 44.981597900390625,
      "learning_rate": 6.653102746693794e-06,
      "loss": 1.4962,
      "step": 85220
    },
    {
      "epoch": 43.35198372329603,
      "grad_norm": 44.240718841552734,
      "learning_rate": 6.648016276703968e-06,
      "loss": 1.6021,
      "step": 85230
    },
    {
      "epoch": 43.35707019328586,
      "grad_norm": 47.06202697753906,
      "learning_rate": 6.6429298067141405e-06,
      "loss": 1.6017,
      "step": 85240
    },
    {
      "epoch": 43.362156663275684,
      "grad_norm": 39.554725646972656,
      "learning_rate": 6.637843336724314e-06,
      "loss": 1.4815,
      "step": 85250
    },
    {
      "epoch": 43.36724313326551,
      "grad_norm": 53.77546310424805,
      "learning_rate": 6.632756866734486e-06,
      "loss": 1.6027,
      "step": 85260
    },
    {
      "epoch": 43.37232960325534,
      "grad_norm": 35.413841247558594,
      "learning_rate": 6.627670396744659e-06,
      "loss": 1.5379,
      "step": 85270
    },
    {
      "epoch": 43.377416073245165,
      "grad_norm": 38.994041442871094,
      "learning_rate": 6.622583926754833e-06,
      "loss": 1.5159,
      "step": 85280
    },
    {
      "epoch": 43.38250254323499,
      "grad_norm": 41.56379699707031,
      "learning_rate": 6.617497456765005e-06,
      "loss": 1.5518,
      "step": 85290
    },
    {
      "epoch": 43.38758901322482,
      "grad_norm": 42.45911407470703,
      "learning_rate": 6.612410986775178e-06,
      "loss": 1.6928,
      "step": 85300
    },
    {
      "epoch": 43.392675483214646,
      "grad_norm": 38.503021240234375,
      "learning_rate": 6.607324516785352e-06,
      "loss": 1.5549,
      "step": 85310
    },
    {
      "epoch": 43.39776195320447,
      "grad_norm": 49.80087661743164,
      "learning_rate": 6.602238046795524e-06,
      "loss": 1.5117,
      "step": 85320
    },
    {
      "epoch": 43.4028484231943,
      "grad_norm": 38.18627166748047,
      "learning_rate": 6.5971515768056975e-06,
      "loss": 1.4292,
      "step": 85330
    },
    {
      "epoch": 43.407934893184134,
      "grad_norm": 37.83330154418945,
      "learning_rate": 6.59206510681587e-06,
      "loss": 1.5942,
      "step": 85340
    },
    {
      "epoch": 43.41302136317396,
      "grad_norm": 66.47785949707031,
      "learning_rate": 6.586978636826042e-06,
      "loss": 1.6057,
      "step": 85350
    },
    {
      "epoch": 43.41810783316379,
      "grad_norm": 42.26130294799805,
      "learning_rate": 6.5818921668362165e-06,
      "loss": 1.5094,
      "step": 85360
    },
    {
      "epoch": 43.423194303153615,
      "grad_norm": 45.04730224609375,
      "learning_rate": 6.576805696846389e-06,
      "loss": 1.5688,
      "step": 85370
    },
    {
      "epoch": 43.42828077314344,
      "grad_norm": 39.30567932128906,
      "learning_rate": 6.571719226856562e-06,
      "loss": 1.5927,
      "step": 85380
    },
    {
      "epoch": 43.43336724313327,
      "grad_norm": 33.959922790527344,
      "learning_rate": 6.566632756866735e-06,
      "loss": 1.5479,
      "step": 85390
    },
    {
      "epoch": 43.438453713123096,
      "grad_norm": 38.7885627746582,
      "learning_rate": 6.561546286876907e-06,
      "loss": 1.535,
      "step": 85400
    },
    {
      "epoch": 43.44354018311292,
      "grad_norm": 36.86141586303711,
      "learning_rate": 6.556459816887081e-06,
      "loss": 1.4966,
      "step": 85410
    },
    {
      "epoch": 43.44862665310275,
      "grad_norm": 43.786380767822266,
      "learning_rate": 6.551373346897254e-06,
      "loss": 1.5963,
      "step": 85420
    },
    {
      "epoch": 43.45371312309258,
      "grad_norm": 45.70310974121094,
      "learning_rate": 6.546286876907426e-06,
      "loss": 1.5579,
      "step": 85430
    },
    {
      "epoch": 43.458799593082404,
      "grad_norm": 57.82872009277344,
      "learning_rate": 6.541200406917599e-06,
      "loss": 1.5419,
      "step": 85440
    },
    {
      "epoch": 43.46388606307223,
      "grad_norm": 47.12080764770508,
      "learning_rate": 6.536113936927772e-06,
      "loss": 1.6053,
      "step": 85450
    },
    {
      "epoch": 43.46897253306206,
      "grad_norm": 33.521541595458984,
      "learning_rate": 6.531027466937946e-06,
      "loss": 1.5369,
      "step": 85460
    },
    {
      "epoch": 43.474059003051885,
      "grad_norm": 43.52170944213867,
      "learning_rate": 6.525940996948118e-06,
      "loss": 1.5461,
      "step": 85470
    },
    {
      "epoch": 43.47914547304171,
      "grad_norm": 40.12917709350586,
      "learning_rate": 6.520854526958291e-06,
      "loss": 1.6025,
      "step": 85480
    },
    {
      "epoch": 43.48423194303154,
      "grad_norm": 39.298255920410156,
      "learning_rate": 6.515768056968464e-06,
      "loss": 1.5387,
      "step": 85490
    },
    {
      "epoch": 43.489318413021365,
      "grad_norm": 38.50341033935547,
      "learning_rate": 6.5106815869786366e-06,
      "loss": 1.5687,
      "step": 85500
    },
    {
      "epoch": 43.49440488301119,
      "grad_norm": 37.62424850463867,
      "learning_rate": 6.505595116988811e-06,
      "loss": 1.6375,
      "step": 85510
    },
    {
      "epoch": 43.49949135300102,
      "grad_norm": 43.24211883544922,
      "learning_rate": 6.500508646998983e-06,
      "loss": 1.6221,
      "step": 85520
    },
    {
      "epoch": 43.504577822990846,
      "grad_norm": 41.54798126220703,
      "learning_rate": 6.4954221770091556e-06,
      "loss": 1.5672,
      "step": 85530
    },
    {
      "epoch": 43.50966429298067,
      "grad_norm": 43.594459533691406,
      "learning_rate": 6.49033570701933e-06,
      "loss": 1.4666,
      "step": 85540
    },
    {
      "epoch": 43.5147507629705,
      "grad_norm": 46.48825454711914,
      "learning_rate": 6.485249237029502e-06,
      "loss": 1.555,
      "step": 85550
    },
    {
      "epoch": 43.51983723296033,
      "grad_norm": 46.828407287597656,
      "learning_rate": 6.4801627670396746e-06,
      "loss": 1.5731,
      "step": 85560
    },
    {
      "epoch": 43.524923702950154,
      "grad_norm": 48.083431243896484,
      "learning_rate": 6.475076297049848e-06,
      "loss": 1.4444,
      "step": 85570
    },
    {
      "epoch": 43.53001017293998,
      "grad_norm": 42.74485397338867,
      "learning_rate": 6.46998982706002e-06,
      "loss": 1.5629,
      "step": 85580
    },
    {
      "epoch": 43.53509664292981,
      "grad_norm": 42.69081497192383,
      "learning_rate": 6.464903357070194e-06,
      "loss": 1.5987,
      "step": 85590
    },
    {
      "epoch": 43.540183112919635,
      "grad_norm": 42.166053771972656,
      "learning_rate": 6.459816887080367e-06,
      "loss": 1.5334,
      "step": 85600
    },
    {
      "epoch": 43.54526958290946,
      "grad_norm": 39.52622604370117,
      "learning_rate": 6.454730417090539e-06,
      "loss": 1.4834,
      "step": 85610
    },
    {
      "epoch": 43.55035605289929,
      "grad_norm": 49.37334442138672,
      "learning_rate": 6.4496439471007125e-06,
      "loss": 1.4784,
      "step": 85620
    },
    {
      "epoch": 43.555442522889116,
      "grad_norm": 45.275028228759766,
      "learning_rate": 6.444557477110885e-06,
      "loss": 1.5691,
      "step": 85630
    },
    {
      "epoch": 43.56052899287894,
      "grad_norm": 40.43648910522461,
      "learning_rate": 6.439471007121057e-06,
      "loss": 1.5129,
      "step": 85640
    },
    {
      "epoch": 43.56561546286877,
      "grad_norm": 33.65497970581055,
      "learning_rate": 6.4343845371312315e-06,
      "loss": 1.5569,
      "step": 85650
    },
    {
      "epoch": 43.5707019328586,
      "grad_norm": 40.338069915771484,
      "learning_rate": 6.429298067141404e-06,
      "loss": 1.5098,
      "step": 85660
    },
    {
      "epoch": 43.575788402848424,
      "grad_norm": 40.52028274536133,
      "learning_rate": 6.424211597151577e-06,
      "loss": 1.4529,
      "step": 85670
    },
    {
      "epoch": 43.58087487283825,
      "grad_norm": 41.00530242919922,
      "learning_rate": 6.41912512716175e-06,
      "loss": 1.6068,
      "step": 85680
    },
    {
      "epoch": 43.58596134282808,
      "grad_norm": 38.99740219116211,
      "learning_rate": 6.414038657171922e-06,
      "loss": 1.4846,
      "step": 85690
    },
    {
      "epoch": 43.591047812817905,
      "grad_norm": 40.91437911987305,
      "learning_rate": 6.408952187182096e-06,
      "loss": 1.5022,
      "step": 85700
    },
    {
      "epoch": 43.59613428280773,
      "grad_norm": 37.18199920654297,
      "learning_rate": 6.403865717192269e-06,
      "loss": 1.5235,
      "step": 85710
    },
    {
      "epoch": 43.60122075279756,
      "grad_norm": 34.862098693847656,
      "learning_rate": 6.398779247202443e-06,
      "loss": 1.4951,
      "step": 85720
    },
    {
      "epoch": 43.606307222787386,
      "grad_norm": 39.20363235473633,
      "learning_rate": 6.393692777212615e-06,
      "loss": 1.5198,
      "step": 85730
    },
    {
      "epoch": 43.61139369277721,
      "grad_norm": 44.173126220703125,
      "learning_rate": 6.388606307222787e-06,
      "loss": 1.5898,
      "step": 85740
    },
    {
      "epoch": 43.61648016276704,
      "grad_norm": 38.86286926269531,
      "learning_rate": 6.383519837232961e-06,
      "loss": 1.4659,
      "step": 85750
    },
    {
      "epoch": 43.62156663275687,
      "grad_norm": 43.260677337646484,
      "learning_rate": 6.378433367243133e-06,
      "loss": 1.5093,
      "step": 85760
    },
    {
      "epoch": 43.626653102746694,
      "grad_norm": 45.00699234008789,
      "learning_rate": 6.373346897253306e-06,
      "loss": 1.5707,
      "step": 85770
    },
    {
      "epoch": 43.63173957273652,
      "grad_norm": 40.74738693237305,
      "learning_rate": 6.36826042726348e-06,
      "loss": 1.5327,
      "step": 85780
    },
    {
      "epoch": 43.63682604272635,
      "grad_norm": 36.69459915161133,
      "learning_rate": 6.363173957273652e-06,
      "loss": 1.5107,
      "step": 85790
    },
    {
      "epoch": 43.641912512716175,
      "grad_norm": 44.292213439941406,
      "learning_rate": 6.358087487283826e-06,
      "loss": 1.5735,
      "step": 85800
    },
    {
      "epoch": 43.646998982706,
      "grad_norm": 34.790321350097656,
      "learning_rate": 6.353001017293998e-06,
      "loss": 1.5895,
      "step": 85810
    },
    {
      "epoch": 43.65208545269583,
      "grad_norm": 40.412025451660156,
      "learning_rate": 6.3479145473041706e-06,
      "loss": 1.4908,
      "step": 85820
    },
    {
      "epoch": 43.657171922685656,
      "grad_norm": 33.95146942138672,
      "learning_rate": 6.342828077314345e-06,
      "loss": 1.5694,
      "step": 85830
    },
    {
      "epoch": 43.66225839267548,
      "grad_norm": 49.85297393798828,
      "learning_rate": 6.337741607324517e-06,
      "loss": 1.6388,
      "step": 85840
    },
    {
      "epoch": 43.66734486266531,
      "grad_norm": 38.55412292480469,
      "learning_rate": 6.33265513733469e-06,
      "loss": 1.476,
      "step": 85850
    },
    {
      "epoch": 43.67243133265514,
      "grad_norm": 32.546546936035156,
      "learning_rate": 6.327568667344863e-06,
      "loss": 1.5471,
      "step": 85860
    },
    {
      "epoch": 43.677517802644964,
      "grad_norm": 43.77940368652344,
      "learning_rate": 6.322482197355035e-06,
      "loss": 1.6002,
      "step": 85870
    },
    {
      "epoch": 43.68260427263479,
      "grad_norm": 38.27687454223633,
      "learning_rate": 6.317395727365209e-06,
      "loss": 1.5401,
      "step": 85880
    },
    {
      "epoch": 43.68769074262462,
      "grad_norm": 38.94513702392578,
      "learning_rate": 6.312309257375382e-06,
      "loss": 1.5665,
      "step": 85890
    },
    {
      "epoch": 43.692777212614445,
      "grad_norm": 42.618202209472656,
      "learning_rate": 6.307222787385554e-06,
      "loss": 1.5942,
      "step": 85900
    },
    {
      "epoch": 43.69786368260427,
      "grad_norm": 35.66570281982422,
      "learning_rate": 6.3021363173957276e-06,
      "loss": 1.5517,
      "step": 85910
    },
    {
      "epoch": 43.7029501525941,
      "grad_norm": 40.37845230102539,
      "learning_rate": 6.2970498474059e-06,
      "loss": 1.5817,
      "step": 85920
    },
    {
      "epoch": 43.708036622583926,
      "grad_norm": 40.04424285888672,
      "learning_rate": 6.291963377416074e-06,
      "loss": 1.5976,
      "step": 85930
    },
    {
      "epoch": 43.71312309257375,
      "grad_norm": 35.71460723876953,
      "learning_rate": 6.2868769074262465e-06,
      "loss": 1.5527,
      "step": 85940
    },
    {
      "epoch": 43.71820956256358,
      "grad_norm": 42.59284973144531,
      "learning_rate": 6.281790437436419e-06,
      "loss": 1.5185,
      "step": 85950
    },
    {
      "epoch": 43.72329603255341,
      "grad_norm": 44.87122344970703,
      "learning_rate": 6.276703967446593e-06,
      "loss": 1.5912,
      "step": 85960
    },
    {
      "epoch": 43.728382502543234,
      "grad_norm": 34.37744903564453,
      "learning_rate": 6.2716174974567655e-06,
      "loss": 1.5011,
      "step": 85970
    },
    {
      "epoch": 43.73346897253306,
      "grad_norm": 41.9691047668457,
      "learning_rate": 6.266531027466939e-06,
      "loss": 1.5673,
      "step": 85980
    },
    {
      "epoch": 43.73855544252289,
      "grad_norm": 40.44816207885742,
      "learning_rate": 6.261444557477111e-06,
      "loss": 1.5546,
      "step": 85990
    },
    {
      "epoch": 43.743641912512714,
      "grad_norm": 41.592952728271484,
      "learning_rate": 6.256358087487284e-06,
      "loss": 1.5354,
      "step": 86000
    },
    {
      "epoch": 43.74872838250254,
      "grad_norm": 49.62009811401367,
      "learning_rate": 6.251271617497458e-06,
      "loss": 1.5936,
      "step": 86010
    },
    {
      "epoch": 43.75381485249237,
      "grad_norm": 46.03821563720703,
      "learning_rate": 6.24618514750763e-06,
      "loss": 1.4993,
      "step": 86020
    },
    {
      "epoch": 43.758901322482195,
      "grad_norm": 48.23740005493164,
      "learning_rate": 6.241098677517803e-06,
      "loss": 1.5354,
      "step": 86030
    },
    {
      "epoch": 43.76398779247202,
      "grad_norm": 40.15184783935547,
      "learning_rate": 6.236012207527976e-06,
      "loss": 1.4988,
      "step": 86040
    },
    {
      "epoch": 43.76907426246185,
      "grad_norm": 45.30085372924805,
      "learning_rate": 6.230925737538148e-06,
      "loss": 1.5931,
      "step": 86050
    },
    {
      "epoch": 43.774160732451676,
      "grad_norm": 42.197349548339844,
      "learning_rate": 6.225839267548322e-06,
      "loss": 1.5657,
      "step": 86060
    },
    {
      "epoch": 43.7792472024415,
      "grad_norm": 47.334716796875,
      "learning_rate": 6.220752797558495e-06,
      "loss": 1.5141,
      "step": 86070
    },
    {
      "epoch": 43.78433367243133,
      "grad_norm": 41.35076141357422,
      "learning_rate": 6.215666327568668e-06,
      "loss": 1.4728,
      "step": 86080
    },
    {
      "epoch": 43.78942014242116,
      "grad_norm": 57.85540771484375,
      "learning_rate": 6.210579857578841e-06,
      "loss": 1.5795,
      "step": 86090
    },
    {
      "epoch": 43.794506612410984,
      "grad_norm": 43.67039108276367,
      "learning_rate": 6.205493387589013e-06,
      "loss": 1.5094,
      "step": 86100
    },
    {
      "epoch": 43.79959308240081,
      "grad_norm": 41.65856170654297,
      "learning_rate": 6.200406917599186e-06,
      "loss": 1.537,
      "step": 86110
    },
    {
      "epoch": 43.80467955239064,
      "grad_norm": 41.1019401550293,
      "learning_rate": 6.19532044760936e-06,
      "loss": 1.6396,
      "step": 86120
    },
    {
      "epoch": 43.809766022380465,
      "grad_norm": 42.141510009765625,
      "learning_rate": 6.190233977619532e-06,
      "loss": 1.5203,
      "step": 86130
    },
    {
      "epoch": 43.81485249237029,
      "grad_norm": 50.462005615234375,
      "learning_rate": 6.185147507629705e-06,
      "loss": 1.5512,
      "step": 86140
    },
    {
      "epoch": 43.81993896236012,
      "grad_norm": 40.346221923828125,
      "learning_rate": 6.180061037639878e-06,
      "loss": 1.5284,
      "step": 86150
    },
    {
      "epoch": 43.825025432349946,
      "grad_norm": 49.83351135253906,
      "learning_rate": 6.174974567650051e-06,
      "loss": 1.5424,
      "step": 86160
    },
    {
      "epoch": 43.83011190233977,
      "grad_norm": 42.05516815185547,
      "learning_rate": 6.169888097660224e-06,
      "loss": 1.5653,
      "step": 86170
    },
    {
      "epoch": 43.8351983723296,
      "grad_norm": 46.40208435058594,
      "learning_rate": 6.164801627670397e-06,
      "loss": 1.5017,
      "step": 86180
    },
    {
      "epoch": 43.84028484231943,
      "grad_norm": 46.72860336303711,
      "learning_rate": 6.15971515768057e-06,
      "loss": 1.5407,
      "step": 86190
    },
    {
      "epoch": 43.845371312309254,
      "grad_norm": 37.81832504272461,
      "learning_rate": 6.154628687690743e-06,
      "loss": 1.5143,
      "step": 86200
    },
    {
      "epoch": 43.85045778229908,
      "grad_norm": 40.90095901489258,
      "learning_rate": 6.149542217700916e-06,
      "loss": 1.5637,
      "step": 86210
    },
    {
      "epoch": 43.85554425228891,
      "grad_norm": 39.944095611572266,
      "learning_rate": 6.144455747711088e-06,
      "loss": 1.5443,
      "step": 86220
    },
    {
      "epoch": 43.86063072227874,
      "grad_norm": 42.87883758544922,
      "learning_rate": 6.1393692777212616e-06,
      "loss": 1.5592,
      "step": 86230
    },
    {
      "epoch": 43.86571719226856,
      "grad_norm": 46.302146911621094,
      "learning_rate": 6.134282807731435e-06,
      "loss": 1.4257,
      "step": 86240
    },
    {
      "epoch": 43.870803662258396,
      "grad_norm": 40.36687469482422,
      "learning_rate": 6.129196337741608e-06,
      "loss": 1.5498,
      "step": 86250
    },
    {
      "epoch": 43.87589013224822,
      "grad_norm": 37.75239944458008,
      "learning_rate": 6.1241098677517806e-06,
      "loss": 1.5186,
      "step": 86260
    },
    {
      "epoch": 43.88097660223805,
      "grad_norm": 31.64984703063965,
      "learning_rate": 6.119023397761953e-06,
      "loss": 1.5519,
      "step": 86270
    },
    {
      "epoch": 43.88606307222788,
      "grad_norm": 46.296146392822266,
      "learning_rate": 6.113936927772126e-06,
      "loss": 1.5586,
      "step": 86280
    },
    {
      "epoch": 43.891149542217704,
      "grad_norm": 66.88421630859375,
      "learning_rate": 6.1088504577822995e-06,
      "loss": 1.5114,
      "step": 86290
    },
    {
      "epoch": 43.89623601220753,
      "grad_norm": 38.092132568359375,
      "learning_rate": 6.103763987792473e-06,
      "loss": 1.4711,
      "step": 86300
    },
    {
      "epoch": 43.90132248219736,
      "grad_norm": 38.478206634521484,
      "learning_rate": 6.098677517802645e-06,
      "loss": 1.4965,
      "step": 86310
    },
    {
      "epoch": 43.906408952187185,
      "grad_norm": 43.69674301147461,
      "learning_rate": 6.0935910478128185e-06,
      "loss": 1.5472,
      "step": 86320
    },
    {
      "epoch": 43.91149542217701,
      "grad_norm": 52.138214111328125,
      "learning_rate": 6.088504577822991e-06,
      "loss": 1.5156,
      "step": 86330
    },
    {
      "epoch": 43.91658189216684,
      "grad_norm": 52.14011001586914,
      "learning_rate": 6.083418107833164e-06,
      "loss": 1.5847,
      "step": 86340
    },
    {
      "epoch": 43.921668362156666,
      "grad_norm": 41.59332275390625,
      "learning_rate": 6.078331637843337e-06,
      "loss": 1.6894,
      "step": 86350
    },
    {
      "epoch": 43.92675483214649,
      "grad_norm": 48.389190673828125,
      "learning_rate": 6.07324516785351e-06,
      "loss": 1.6175,
      "step": 86360
    },
    {
      "epoch": 43.93184130213632,
      "grad_norm": 37.88903045654297,
      "learning_rate": 6.068158697863683e-06,
      "loss": 1.5154,
      "step": 86370
    },
    {
      "epoch": 43.93692777212615,
      "grad_norm": 40.55778884887695,
      "learning_rate": 6.063072227873856e-06,
      "loss": 1.5119,
      "step": 86380
    },
    {
      "epoch": 43.942014242115974,
      "grad_norm": 28.71072769165039,
      "learning_rate": 6.057985757884028e-06,
      "loss": 1.5533,
      "step": 86390
    },
    {
      "epoch": 43.9471007121058,
      "grad_norm": 36.61000442504883,
      "learning_rate": 6.052899287894201e-06,
      "loss": 1.5334,
      "step": 86400
    },
    {
      "epoch": 43.95218718209563,
      "grad_norm": 43.7614631652832,
      "learning_rate": 6.047812817904375e-06,
      "loss": 1.4762,
      "step": 86410
    },
    {
      "epoch": 43.957273652085455,
      "grad_norm": 40.63706588745117,
      "learning_rate": 6.042726347914548e-06,
      "loss": 1.6108,
      "step": 86420
    },
    {
      "epoch": 43.96236012207528,
      "grad_norm": 44.723609924316406,
      "learning_rate": 6.03763987792472e-06,
      "loss": 1.4779,
      "step": 86430
    },
    {
      "epoch": 43.96744659206511,
      "grad_norm": 44.5496711730957,
      "learning_rate": 6.032553407934894e-06,
      "loss": 1.571,
      "step": 86440
    },
    {
      "epoch": 43.972533062054936,
      "grad_norm": 35.690528869628906,
      "learning_rate": 6.027466937945066e-06,
      "loss": 1.5806,
      "step": 86450
    },
    {
      "epoch": 43.97761953204476,
      "grad_norm": 41.657649993896484,
      "learning_rate": 6.022380467955239e-06,
      "loss": 1.55,
      "step": 86460
    },
    {
      "epoch": 43.98270600203459,
      "grad_norm": 40.506229400634766,
      "learning_rate": 6.017293997965413e-06,
      "loss": 1.5362,
      "step": 86470
    },
    {
      "epoch": 43.98779247202442,
      "grad_norm": 36.09736251831055,
      "learning_rate": 6.012207527975585e-06,
      "loss": 1.5103,
      "step": 86480
    },
    {
      "epoch": 43.992878942014244,
      "grad_norm": 39.34883499145508,
      "learning_rate": 6.007121057985758e-06,
      "loss": 1.4675,
      "step": 86490
    },
    {
      "epoch": 43.99796541200407,
      "grad_norm": 48.02366256713867,
      "learning_rate": 6.002034587995931e-06,
      "loss": 1.5936,
      "step": 86500
    },
    {
      "epoch": 44.0,
      "eval_loss": 5.077314853668213,
      "eval_runtime": 2.827,
      "eval_samples_per_second": 981.594,
      "eval_steps_per_second": 122.743,
      "step": 86504
    },
    {
      "epoch": 44.0030518819939,
      "grad_norm": 39.12459945678711,
      "learning_rate": 5.996948118006104e-06,
      "loss": 1.543,
      "step": 86510
    },
    {
      "epoch": 44.008138351983725,
      "grad_norm": 33.4580078125,
      "learning_rate": 5.9918616480162766e-06,
      "loss": 1.5266,
      "step": 86520
    },
    {
      "epoch": 44.01322482197355,
      "grad_norm": 38.76619338989258,
      "learning_rate": 5.98677517802645e-06,
      "loss": 1.4839,
      "step": 86530
    },
    {
      "epoch": 44.01831129196338,
      "grad_norm": 32.05809020996094,
      "learning_rate": 5.981688708036623e-06,
      "loss": 1.5683,
      "step": 86540
    },
    {
      "epoch": 44.023397761953206,
      "grad_norm": 43.65463638305664,
      "learning_rate": 5.976602238046796e-06,
      "loss": 1.5342,
      "step": 86550
    },
    {
      "epoch": 44.02848423194303,
      "grad_norm": 55.86868667602539,
      "learning_rate": 5.971515768056969e-06,
      "loss": 1.5568,
      "step": 86560
    },
    {
      "epoch": 44.03357070193286,
      "grad_norm": 42.538394927978516,
      "learning_rate": 5.966429298067141e-06,
      "loss": 1.5395,
      "step": 86570
    },
    {
      "epoch": 44.03865717192269,
      "grad_norm": 35.49245834350586,
      "learning_rate": 5.9613428280773146e-06,
      "loss": 1.5747,
      "step": 86580
    },
    {
      "epoch": 44.04374364191251,
      "grad_norm": 48.1456298828125,
      "learning_rate": 5.956256358087488e-06,
      "loss": 1.5611,
      "step": 86590
    },
    {
      "epoch": 44.04883011190234,
      "grad_norm": 57.692726135253906,
      "learning_rate": 5.95116988809766e-06,
      "loss": 1.5704,
      "step": 86600
    },
    {
      "epoch": 44.05391658189217,
      "grad_norm": 38.1767463684082,
      "learning_rate": 5.9460834181078335e-06,
      "loss": 1.5332,
      "step": 86610
    },
    {
      "epoch": 44.059003051881994,
      "grad_norm": 35.76420593261719,
      "learning_rate": 5.940996948118006e-06,
      "loss": 1.5631,
      "step": 86620
    },
    {
      "epoch": 44.06408952187182,
      "grad_norm": 40.55791473388672,
      "learning_rate": 5.935910478128179e-06,
      "loss": 1.5473,
      "step": 86630
    },
    {
      "epoch": 44.06917599186165,
      "grad_norm": 42.24021911621094,
      "learning_rate": 5.9308240081383525e-06,
      "loss": 1.4995,
      "step": 86640
    },
    {
      "epoch": 44.074262461851475,
      "grad_norm": 42.179954528808594,
      "learning_rate": 5.925737538148525e-06,
      "loss": 1.5556,
      "step": 86650
    },
    {
      "epoch": 44.0793489318413,
      "grad_norm": 55.717857360839844,
      "learning_rate": 5.920651068158698e-06,
      "loss": 1.5416,
      "step": 86660
    },
    {
      "epoch": 44.08443540183113,
      "grad_norm": 47.101444244384766,
      "learning_rate": 5.9155645981688715e-06,
      "loss": 1.5788,
      "step": 86670
    },
    {
      "epoch": 44.089521871820956,
      "grad_norm": 36.34359359741211,
      "learning_rate": 5.910478128179044e-06,
      "loss": 1.503,
      "step": 86680
    },
    {
      "epoch": 44.09460834181078,
      "grad_norm": 35.30205154418945,
      "learning_rate": 5.905391658189216e-06,
      "loss": 1.4472,
      "step": 86690
    },
    {
      "epoch": 44.09969481180061,
      "grad_norm": 41.10918045043945,
      "learning_rate": 5.90030518819939e-06,
      "loss": 1.5606,
      "step": 86700
    },
    {
      "epoch": 44.10478128179044,
      "grad_norm": 43.99940490722656,
      "learning_rate": 5.895218718209563e-06,
      "loss": 1.4755,
      "step": 86710
    },
    {
      "epoch": 44.109867751780264,
      "grad_norm": 34.51686477661133,
      "learning_rate": 5.890132248219736e-06,
      "loss": 1.557,
      "step": 86720
    },
    {
      "epoch": 44.11495422177009,
      "grad_norm": 36.9138298034668,
      "learning_rate": 5.885045778229909e-06,
      "loss": 1.5457,
      "step": 86730
    },
    {
      "epoch": 44.12004069175992,
      "grad_norm": 50.180419921875,
      "learning_rate": 5.879959308240081e-06,
      "loss": 1.5256,
      "step": 86740
    },
    {
      "epoch": 44.125127161749745,
      "grad_norm": 43.65876770019531,
      "learning_rate": 5.874872838250254e-06,
      "loss": 1.4864,
      "step": 86750
    },
    {
      "epoch": 44.13021363173957,
      "grad_norm": 29.428266525268555,
      "learning_rate": 5.869786368260428e-06,
      "loss": 1.5566,
      "step": 86760
    },
    {
      "epoch": 44.1353001017294,
      "grad_norm": 40.37870407104492,
      "learning_rate": 5.8646998982706e-06,
      "loss": 1.4839,
      "step": 86770
    },
    {
      "epoch": 44.140386571719226,
      "grad_norm": 48.48893356323242,
      "learning_rate": 5.859613428280773e-06,
      "loss": 1.4976,
      "step": 86780
    },
    {
      "epoch": 44.14547304170905,
      "grad_norm": 40.701141357421875,
      "learning_rate": 5.854526958290947e-06,
      "loss": 1.5293,
      "step": 86790
    },
    {
      "epoch": 44.15055951169888,
      "grad_norm": 36.53594207763672,
      "learning_rate": 5.849440488301119e-06,
      "loss": 1.6085,
      "step": 86800
    },
    {
      "epoch": 44.15564598168871,
      "grad_norm": 38.20799255371094,
      "learning_rate": 5.844354018311292e-06,
      "loss": 1.5258,
      "step": 86810
    },
    {
      "epoch": 44.160732451678534,
      "grad_norm": 54.81426239013672,
      "learning_rate": 5.839267548321465e-06,
      "loss": 1.5188,
      "step": 86820
    },
    {
      "epoch": 44.16581892166836,
      "grad_norm": 47.627933502197266,
      "learning_rate": 5.834181078331638e-06,
      "loss": 1.5664,
      "step": 86830
    },
    {
      "epoch": 44.17090539165819,
      "grad_norm": 38.12476348876953,
      "learning_rate": 5.829094608341811e-06,
      "loss": 1.5523,
      "step": 86840
    },
    {
      "epoch": 44.175991861648015,
      "grad_norm": 48.24998092651367,
      "learning_rate": 5.824008138351985e-06,
      "loss": 1.4915,
      "step": 86850
    },
    {
      "epoch": 44.18107833163784,
      "grad_norm": 52.06932067871094,
      "learning_rate": 5.818921668362156e-06,
      "loss": 1.5285,
      "step": 86860
    },
    {
      "epoch": 44.18616480162767,
      "grad_norm": 41.29526138305664,
      "learning_rate": 5.8138351983723296e-06,
      "loss": 1.5094,
      "step": 86870
    },
    {
      "epoch": 44.191251271617496,
      "grad_norm": 36.25183868408203,
      "learning_rate": 5.808748728382503e-06,
      "loss": 1.5725,
      "step": 86880
    },
    {
      "epoch": 44.19633774160732,
      "grad_norm": 36.234256744384766,
      "learning_rate": 5.803662258392676e-06,
      "loss": 1.5358,
      "step": 86890
    },
    {
      "epoch": 44.20142421159715,
      "grad_norm": 44.39023208618164,
      "learning_rate": 5.7985757884028486e-06,
      "loss": 1.5206,
      "step": 86900
    },
    {
      "epoch": 44.20651068158698,
      "grad_norm": 52.48586654663086,
      "learning_rate": 5.793489318413022e-06,
      "loss": 1.5296,
      "step": 86910
    },
    {
      "epoch": 44.211597151576804,
      "grad_norm": 42.03011703491211,
      "learning_rate": 5.788402848423194e-06,
      "loss": 1.493,
      "step": 86920
    },
    {
      "epoch": 44.21668362156663,
      "grad_norm": 41.82007598876953,
      "learning_rate": 5.7833163784333676e-06,
      "loss": 1.5783,
      "step": 86930
    },
    {
      "epoch": 44.22177009155646,
      "grad_norm": 41.28014373779297,
      "learning_rate": 5.77822990844354e-06,
      "loss": 1.5427,
      "step": 86940
    },
    {
      "epoch": 44.226856561546285,
      "grad_norm": 48.679710388183594,
      "learning_rate": 5.773143438453713e-06,
      "loss": 1.5907,
      "step": 86950
    },
    {
      "epoch": 44.23194303153611,
      "grad_norm": 46.92583084106445,
      "learning_rate": 5.7680569684638865e-06,
      "loss": 1.5236,
      "step": 86960
    },
    {
      "epoch": 44.23702950152594,
      "grad_norm": 44.316490173339844,
      "learning_rate": 5.76297049847406e-06,
      "loss": 1.4931,
      "step": 86970
    },
    {
      "epoch": 44.242115971515766,
      "grad_norm": 34.61090850830078,
      "learning_rate": 5.757884028484232e-06,
      "loss": 1.5875,
      "step": 86980
    },
    {
      "epoch": 44.24720244150559,
      "grad_norm": 38.076812744140625,
      "learning_rate": 5.752797558494405e-06,
      "loss": 1.5149,
      "step": 86990
    },
    {
      "epoch": 44.25228891149542,
      "grad_norm": 40.745704650878906,
      "learning_rate": 5.747711088504578e-06,
      "loss": 1.4728,
      "step": 87000
    },
    {
      "epoch": 44.25737538148525,
      "grad_norm": 41.19529724121094,
      "learning_rate": 5.742624618514751e-06,
      "loss": 1.527,
      "step": 87010
    },
    {
      "epoch": 44.262461851475074,
      "grad_norm": 45.298675537109375,
      "learning_rate": 5.7375381485249245e-06,
      "loss": 1.5609,
      "step": 87020
    },
    {
      "epoch": 44.2675483214649,
      "grad_norm": 46.17354965209961,
      "learning_rate": 5.732451678535097e-06,
      "loss": 1.5099,
      "step": 87030
    },
    {
      "epoch": 44.27263479145473,
      "grad_norm": 39.70136260986328,
      "learning_rate": 5.727365208545269e-06,
      "loss": 1.5106,
      "step": 87040
    },
    {
      "epoch": 44.277721261444555,
      "grad_norm": 41.20527648925781,
      "learning_rate": 5.722278738555443e-06,
      "loss": 1.5244,
      "step": 87050
    },
    {
      "epoch": 44.28280773143438,
      "grad_norm": 41.38026809692383,
      "learning_rate": 5.717192268565616e-06,
      "loss": 1.4969,
      "step": 87060
    },
    {
      "epoch": 44.28789420142421,
      "grad_norm": 41.82206344604492,
      "learning_rate": 5.712105798575788e-06,
      "loss": 1.5245,
      "step": 87070
    },
    {
      "epoch": 44.292980671414035,
      "grad_norm": 32.862735748291016,
      "learning_rate": 5.707019328585962e-06,
      "loss": 1.5497,
      "step": 87080
    },
    {
      "epoch": 44.29806714140386,
      "grad_norm": 39.01376724243164,
      "learning_rate": 5.701932858596135e-06,
      "loss": 1.5111,
      "step": 87090
    },
    {
      "epoch": 44.30315361139369,
      "grad_norm": 41.70489501953125,
      "learning_rate": 5.696846388606307e-06,
      "loss": 1.4293,
      "step": 87100
    },
    {
      "epoch": 44.308240081383516,
      "grad_norm": 39.12053298950195,
      "learning_rate": 5.691759918616481e-06,
      "loss": 1.4443,
      "step": 87110
    },
    {
      "epoch": 44.31332655137334,
      "grad_norm": 39.158329010009766,
      "learning_rate": 5.686673448626653e-06,
      "loss": 1.4916,
      "step": 87120
    },
    {
      "epoch": 44.31841302136317,
      "grad_norm": 39.619422912597656,
      "learning_rate": 5.681586978636826e-06,
      "loss": 1.5548,
      "step": 87130
    },
    {
      "epoch": 44.323499491353004,
      "grad_norm": 41.68036651611328,
      "learning_rate": 5.676500508647e-06,
      "loss": 1.4056,
      "step": 87140
    },
    {
      "epoch": 44.32858596134283,
      "grad_norm": 40.874813079833984,
      "learning_rate": 5.671414038657172e-06,
      "loss": 1.6034,
      "step": 87150
    },
    {
      "epoch": 44.33367243133266,
      "grad_norm": 45.46149444580078,
      "learning_rate": 5.6663275686673446e-06,
      "loss": 1.6124,
      "step": 87160
    },
    {
      "epoch": 44.338758901322485,
      "grad_norm": 44.525936126708984,
      "learning_rate": 5.661241098677518e-06,
      "loss": 1.5384,
      "step": 87170
    },
    {
      "epoch": 44.34384537131231,
      "grad_norm": 45.715755462646484,
      "learning_rate": 5.656154628687691e-06,
      "loss": 1.4979,
      "step": 87180
    },
    {
      "epoch": 44.34893184130214,
      "grad_norm": 41.43006896972656,
      "learning_rate": 5.651068158697864e-06,
      "loss": 1.5891,
      "step": 87190
    },
    {
      "epoch": 44.354018311291966,
      "grad_norm": 43.00767517089844,
      "learning_rate": 5.645981688708037e-06,
      "loss": 1.6288,
      "step": 87200
    },
    {
      "epoch": 44.35910478128179,
      "grad_norm": 41.72510528564453,
      "learning_rate": 5.64089521871821e-06,
      "loss": 1.4598,
      "step": 87210
    },
    {
      "epoch": 44.36419125127162,
      "grad_norm": 37.49533462524414,
      "learning_rate": 5.6358087487283826e-06,
      "loss": 1.6163,
      "step": 87220
    },
    {
      "epoch": 44.36927772126145,
      "grad_norm": 43.91511535644531,
      "learning_rate": 5.630722278738556e-06,
      "loss": 1.5874,
      "step": 87230
    },
    {
      "epoch": 44.374364191251274,
      "grad_norm": 43.651702880859375,
      "learning_rate": 5.625635808748728e-06,
      "loss": 1.5467,
      "step": 87240
    },
    {
      "epoch": 44.3794506612411,
      "grad_norm": 59.24306106567383,
      "learning_rate": 5.6205493387589016e-06,
      "loss": 1.6512,
      "step": 87250
    },
    {
      "epoch": 44.38453713123093,
      "grad_norm": 37.90517044067383,
      "learning_rate": 5.615462868769075e-06,
      "loss": 1.5629,
      "step": 87260
    },
    {
      "epoch": 44.389623601220755,
      "grad_norm": 37.24227523803711,
      "learning_rate": 5.610376398779247e-06,
      "loss": 1.4531,
      "step": 87270
    },
    {
      "epoch": 44.39471007121058,
      "grad_norm": 55.25510025024414,
      "learning_rate": 5.6052899287894206e-06,
      "loss": 1.489,
      "step": 87280
    },
    {
      "epoch": 44.39979654120041,
      "grad_norm": 48.24286651611328,
      "learning_rate": 5.600203458799593e-06,
      "loss": 1.53,
      "step": 87290
    },
    {
      "epoch": 44.404883011190236,
      "grad_norm": 40.48094940185547,
      "learning_rate": 5.595116988809766e-06,
      "loss": 1.5531,
      "step": 87300
    },
    {
      "epoch": 44.40996948118006,
      "grad_norm": 43.44719696044922,
      "learning_rate": 5.5900305188199395e-06,
      "loss": 1.5134,
      "step": 87310
    },
    {
      "epoch": 44.41505595116989,
      "grad_norm": 45.43085861206055,
      "learning_rate": 5.584944048830113e-06,
      "loss": 1.5691,
      "step": 87320
    },
    {
      "epoch": 44.42014242115972,
      "grad_norm": 38.49247741699219,
      "learning_rate": 5.579857578840285e-06,
      "loss": 1.5106,
      "step": 87330
    },
    {
      "epoch": 44.425228891149544,
      "grad_norm": 30.634916305541992,
      "learning_rate": 5.574771108850458e-06,
      "loss": 1.5878,
      "step": 87340
    },
    {
      "epoch": 44.43031536113937,
      "grad_norm": 34.08714294433594,
      "learning_rate": 5.569684638860631e-06,
      "loss": 1.5766,
      "step": 87350
    },
    {
      "epoch": 44.4354018311292,
      "grad_norm": 50.27655029296875,
      "learning_rate": 5.564598168870804e-06,
      "loss": 1.6386,
      "step": 87360
    },
    {
      "epoch": 44.440488301119025,
      "grad_norm": 51.58953857421875,
      "learning_rate": 5.559511698880977e-06,
      "loss": 1.4711,
      "step": 87370
    },
    {
      "epoch": 44.44557477110885,
      "grad_norm": 41.97238540649414,
      "learning_rate": 5.55442522889115e-06,
      "loss": 1.4536,
      "step": 87380
    },
    {
      "epoch": 44.45066124109868,
      "grad_norm": 54.98432540893555,
      "learning_rate": 5.549338758901322e-06,
      "loss": 1.511,
      "step": 87390
    },
    {
      "epoch": 44.455747711088506,
      "grad_norm": 42.411582946777344,
      "learning_rate": 5.544252288911496e-06,
      "loss": 1.5533,
      "step": 87400
    },
    {
      "epoch": 44.46083418107833,
      "grad_norm": 37.98771667480469,
      "learning_rate": 5.539165818921668e-06,
      "loss": 1.4538,
      "step": 87410
    },
    {
      "epoch": 44.46592065106816,
      "grad_norm": 46.440650939941406,
      "learning_rate": 5.534079348931841e-06,
      "loss": 1.5349,
      "step": 87420
    },
    {
      "epoch": 44.47100712105799,
      "grad_norm": 41.11201477050781,
      "learning_rate": 5.528992878942015e-06,
      "loss": 1.5774,
      "step": 87430
    },
    {
      "epoch": 44.476093591047814,
      "grad_norm": 40.55006790161133,
      "learning_rate": 5.523906408952188e-06,
      "loss": 1.4884,
      "step": 87440
    },
    {
      "epoch": 44.48118006103764,
      "grad_norm": 33.42988967895508,
      "learning_rate": 5.51881993896236e-06,
      "loss": 1.6524,
      "step": 87450
    },
    {
      "epoch": 44.48626653102747,
      "grad_norm": 47.215145111083984,
      "learning_rate": 5.513733468972533e-06,
      "loss": 1.5317,
      "step": 87460
    },
    {
      "epoch": 44.491353001017295,
      "grad_norm": 38.266475677490234,
      "learning_rate": 5.508646998982706e-06,
      "loss": 1.5695,
      "step": 87470
    },
    {
      "epoch": 44.49643947100712,
      "grad_norm": 48.701133728027344,
      "learning_rate": 5.503560528992879e-06,
      "loss": 1.5749,
      "step": 87480
    },
    {
      "epoch": 44.50152594099695,
      "grad_norm": 48.93088150024414,
      "learning_rate": 5.498474059003053e-06,
      "loss": 1.5597,
      "step": 87490
    },
    {
      "epoch": 44.506612410986776,
      "grad_norm": 42.759521484375,
      "learning_rate": 5.493387589013225e-06,
      "loss": 1.622,
      "step": 87500
    },
    {
      "epoch": 44.5116988809766,
      "grad_norm": 42.347450256347656,
      "learning_rate": 5.4883011190233976e-06,
      "loss": 1.5306,
      "step": 87510
    },
    {
      "epoch": 44.51678535096643,
      "grad_norm": 42.605224609375,
      "learning_rate": 5.483214649033571e-06,
      "loss": 1.5987,
      "step": 87520
    },
    {
      "epoch": 44.52187182095626,
      "grad_norm": 32.31217575073242,
      "learning_rate": 5.478128179043744e-06,
      "loss": 1.6011,
      "step": 87530
    },
    {
      "epoch": 44.526958290946084,
      "grad_norm": 58.315792083740234,
      "learning_rate": 5.4730417090539166e-06,
      "loss": 1.537,
      "step": 87540
    },
    {
      "epoch": 44.53204476093591,
      "grad_norm": 44.116600036621094,
      "learning_rate": 5.46795523906409e-06,
      "loss": 1.5606,
      "step": 87550
    },
    {
      "epoch": 44.53713123092574,
      "grad_norm": 38.26140594482422,
      "learning_rate": 5.462868769074263e-06,
      "loss": 1.4849,
      "step": 87560
    },
    {
      "epoch": 44.542217700915565,
      "grad_norm": 51.10148620605469,
      "learning_rate": 5.4577822990844356e-06,
      "loss": 1.5214,
      "step": 87570
    },
    {
      "epoch": 44.54730417090539,
      "grad_norm": 51.279632568359375,
      "learning_rate": 5.452695829094608e-06,
      "loss": 1.5746,
      "step": 87580
    },
    {
      "epoch": 44.55239064089522,
      "grad_norm": 45.852691650390625,
      "learning_rate": 5.447609359104781e-06,
      "loss": 1.548,
      "step": 87590
    },
    {
      "epoch": 44.557477110885046,
      "grad_norm": 37.79771041870117,
      "learning_rate": 5.4425228891149546e-06,
      "loss": 1.553,
      "step": 87600
    },
    {
      "epoch": 44.56256358087487,
      "grad_norm": 41.56159210205078,
      "learning_rate": 5.437436419125128e-06,
      "loss": 1.5359,
      "step": 87610
    },
    {
      "epoch": 44.5676500508647,
      "grad_norm": 39.913509368896484,
      "learning_rate": 5.4323499491353e-06,
      "loss": 1.4942,
      "step": 87620
    },
    {
      "epoch": 44.57273652085453,
      "grad_norm": 40.95035171508789,
      "learning_rate": 5.427263479145473e-06,
      "loss": 1.5035,
      "step": 87630
    },
    {
      "epoch": 44.57782299084435,
      "grad_norm": 41.7423095703125,
      "learning_rate": 5.422177009155646e-06,
      "loss": 1.5096,
      "step": 87640
    },
    {
      "epoch": 44.58290946083418,
      "grad_norm": 45.5180549621582,
      "learning_rate": 5.417090539165819e-06,
      "loss": 1.5306,
      "step": 87650
    },
    {
      "epoch": 44.58799593082401,
      "grad_norm": 34.369110107421875,
      "learning_rate": 5.4120040691759925e-06,
      "loss": 1.5157,
      "step": 87660
    },
    {
      "epoch": 44.593082400813834,
      "grad_norm": 44.487430572509766,
      "learning_rate": 5.406917599186165e-06,
      "loss": 1.4745,
      "step": 87670
    },
    {
      "epoch": 44.59816887080366,
      "grad_norm": 45.7199592590332,
      "learning_rate": 5.401831129196338e-06,
      "loss": 1.5263,
      "step": 87680
    },
    {
      "epoch": 44.60325534079349,
      "grad_norm": 38.33134460449219,
      "learning_rate": 5.396744659206511e-06,
      "loss": 1.7124,
      "step": 87690
    },
    {
      "epoch": 44.608341810783315,
      "grad_norm": 38.15276336669922,
      "learning_rate": 5.391658189216684e-06,
      "loss": 1.5002,
      "step": 87700
    },
    {
      "epoch": 44.61342828077314,
      "grad_norm": 47.458614349365234,
      "learning_rate": 5.386571719226856e-06,
      "loss": 1.5326,
      "step": 87710
    },
    {
      "epoch": 44.61851475076297,
      "grad_norm": 38.26399230957031,
      "learning_rate": 5.38148524923703e-06,
      "loss": 1.5946,
      "step": 87720
    },
    {
      "epoch": 44.623601220752796,
      "grad_norm": 42.704437255859375,
      "learning_rate": 5.376398779247203e-06,
      "loss": 1.5689,
      "step": 87730
    },
    {
      "epoch": 44.62868769074262,
      "grad_norm": 43.8988037109375,
      "learning_rate": 5.371312309257375e-06,
      "loss": 1.527,
      "step": 87740
    },
    {
      "epoch": 44.63377416073245,
      "grad_norm": 34.87309265136719,
      "learning_rate": 5.366225839267548e-06,
      "loss": 1.5422,
      "step": 87750
    },
    {
      "epoch": 44.63886063072228,
      "grad_norm": 45.55729293823242,
      "learning_rate": 5.361139369277721e-06,
      "loss": 1.6009,
      "step": 87760
    },
    {
      "epoch": 44.643947100712104,
      "grad_norm": 32.113338470458984,
      "learning_rate": 5.356052899287894e-06,
      "loss": 1.5276,
      "step": 87770
    },
    {
      "epoch": 44.64903357070193,
      "grad_norm": 48.383270263671875,
      "learning_rate": 5.350966429298068e-06,
      "loss": 1.4619,
      "step": 87780
    },
    {
      "epoch": 44.65412004069176,
      "grad_norm": 44.0469856262207,
      "learning_rate": 5.345879959308241e-06,
      "loss": 1.5071,
      "step": 87790
    },
    {
      "epoch": 44.659206510681585,
      "grad_norm": 33.12123489379883,
      "learning_rate": 5.340793489318413e-06,
      "loss": 1.619,
      "step": 87800
    },
    {
      "epoch": 44.66429298067141,
      "grad_norm": 33.69136047363281,
      "learning_rate": 5.335707019328586e-06,
      "loss": 1.5731,
      "step": 87810
    },
    {
      "epoch": 44.66937945066124,
      "grad_norm": 44.700618743896484,
      "learning_rate": 5.330620549338759e-06,
      "loss": 1.5862,
      "step": 87820
    },
    {
      "epoch": 44.674465920651066,
      "grad_norm": 35.135520935058594,
      "learning_rate": 5.325534079348932e-06,
      "loss": 1.5029,
      "step": 87830
    },
    {
      "epoch": 44.67955239064089,
      "grad_norm": 43.427490234375,
      "learning_rate": 5.320447609359105e-06,
      "loss": 1.4749,
      "step": 87840
    },
    {
      "epoch": 44.68463886063072,
      "grad_norm": 36.875675201416016,
      "learning_rate": 5.315361139369278e-06,
      "loss": 1.5298,
      "step": 87850
    },
    {
      "epoch": 44.68972533062055,
      "grad_norm": 37.59480285644531,
      "learning_rate": 5.3102746693794506e-06,
      "loss": 1.5867,
      "step": 87860
    },
    {
      "epoch": 44.694811800610374,
      "grad_norm": 45.15717697143555,
      "learning_rate": 5.305188199389624e-06,
      "loss": 1.5181,
      "step": 87870
    },
    {
      "epoch": 44.6998982706002,
      "grad_norm": 45.13227081298828,
      "learning_rate": 5.300101729399796e-06,
      "loss": 1.5302,
      "step": 87880
    },
    {
      "epoch": 44.70498474059003,
      "grad_norm": 41.98971939086914,
      "learning_rate": 5.2950152594099696e-06,
      "loss": 1.4694,
      "step": 87890
    },
    {
      "epoch": 44.710071210579855,
      "grad_norm": 36.564971923828125,
      "learning_rate": 5.289928789420143e-06,
      "loss": 1.6089,
      "step": 87900
    },
    {
      "epoch": 44.71515768056968,
      "grad_norm": 39.452880859375,
      "learning_rate": 5.284842319430316e-06,
      "loss": 1.4962,
      "step": 87910
    },
    {
      "epoch": 44.72024415055951,
      "grad_norm": 34.06733322143555,
      "learning_rate": 5.2797558494404886e-06,
      "loss": 1.5331,
      "step": 87920
    },
    {
      "epoch": 44.725330620549336,
      "grad_norm": 39.69053268432617,
      "learning_rate": 5.274669379450661e-06,
      "loss": 1.5213,
      "step": 87930
    },
    {
      "epoch": 44.73041709053916,
      "grad_norm": 43.75339889526367,
      "learning_rate": 5.269582909460834e-06,
      "loss": 1.5016,
      "step": 87940
    },
    {
      "epoch": 44.73550356052899,
      "grad_norm": 35.16667175292969,
      "learning_rate": 5.2644964394710076e-06,
      "loss": 1.5411,
      "step": 87950
    },
    {
      "epoch": 44.74059003051882,
      "grad_norm": 38.44456100463867,
      "learning_rate": 5.259409969481181e-06,
      "loss": 1.6032,
      "step": 87960
    },
    {
      "epoch": 44.745676500508644,
      "grad_norm": 41.54056167602539,
      "learning_rate": 5.254323499491353e-06,
      "loss": 1.5255,
      "step": 87970
    },
    {
      "epoch": 44.75076297049847,
      "grad_norm": 42.457664489746094,
      "learning_rate": 5.2492370295015265e-06,
      "loss": 1.5051,
      "step": 87980
    },
    {
      "epoch": 44.7558494404883,
      "grad_norm": 38.24869155883789,
      "learning_rate": 5.244150559511699e-06,
      "loss": 1.6742,
      "step": 87990
    },
    {
      "epoch": 44.760935910478125,
      "grad_norm": 37.07134246826172,
      "learning_rate": 5.239064089521872e-06,
      "loss": 1.4768,
      "step": 88000
    },
    {
      "epoch": 44.76602238046795,
      "grad_norm": 41.91644287109375,
      "learning_rate": 5.233977619532045e-06,
      "loss": 1.5883,
      "step": 88010
    },
    {
      "epoch": 44.77110885045778,
      "grad_norm": 53.33334732055664,
      "learning_rate": 5.228891149542218e-06,
      "loss": 1.5305,
      "step": 88020
    },
    {
      "epoch": 44.77619532044761,
      "grad_norm": 38.9892463684082,
      "learning_rate": 5.223804679552391e-06,
      "loss": 1.517,
      "step": 88030
    },
    {
      "epoch": 44.78128179043744,
      "grad_norm": 50.786766052246094,
      "learning_rate": 5.218718209562564e-06,
      "loss": 1.5256,
      "step": 88040
    },
    {
      "epoch": 44.78636826042727,
      "grad_norm": 35.55604934692383,
      "learning_rate": 5.213631739572736e-06,
      "loss": 1.5605,
      "step": 88050
    },
    {
      "epoch": 44.791454730417094,
      "grad_norm": 45.227684020996094,
      "learning_rate": 5.208545269582909e-06,
      "loss": 1.4934,
      "step": 88060
    },
    {
      "epoch": 44.79654120040692,
      "grad_norm": 48.80860137939453,
      "learning_rate": 5.203458799593083e-06,
      "loss": 1.5807,
      "step": 88070
    },
    {
      "epoch": 44.80162767039675,
      "grad_norm": 52.87456512451172,
      "learning_rate": 5.198372329603256e-06,
      "loss": 1.5705,
      "step": 88080
    },
    {
      "epoch": 44.806714140386575,
      "grad_norm": 37.8388557434082,
      "learning_rate": 5.193285859613429e-06,
      "loss": 1.5823,
      "step": 88090
    },
    {
      "epoch": 44.8118006103764,
      "grad_norm": 46.546661376953125,
      "learning_rate": 5.188199389623602e-06,
      "loss": 1.4796,
      "step": 88100
    },
    {
      "epoch": 44.81688708036623,
      "grad_norm": 38.41853713989258,
      "learning_rate": 5.183112919633774e-06,
      "loss": 1.6272,
      "step": 88110
    },
    {
      "epoch": 44.821973550356056,
      "grad_norm": 42.42654037475586,
      "learning_rate": 5.178026449643947e-06,
      "loss": 1.4663,
      "step": 88120
    },
    {
      "epoch": 44.82706002034588,
      "grad_norm": 40.597312927246094,
      "learning_rate": 5.172939979654121e-06,
      "loss": 1.5506,
      "step": 88130
    },
    {
      "epoch": 44.83214649033571,
      "grad_norm": 44.79102325439453,
      "learning_rate": 5.167853509664293e-06,
      "loss": 1.5675,
      "step": 88140
    },
    {
      "epoch": 44.83723296032554,
      "grad_norm": 60.350059509277344,
      "learning_rate": 5.162767039674466e-06,
      "loss": 1.5495,
      "step": 88150
    },
    {
      "epoch": 44.842319430315364,
      "grad_norm": 39.33697509765625,
      "learning_rate": 5.157680569684639e-06,
      "loss": 1.5304,
      "step": 88160
    },
    {
      "epoch": 44.84740590030519,
      "grad_norm": 39.1163215637207,
      "learning_rate": 5.152594099694812e-06,
      "loss": 1.5798,
      "step": 88170
    },
    {
      "epoch": 44.85249237029502,
      "grad_norm": 41.42777633666992,
      "learning_rate": 5.1475076297049846e-06,
      "loss": 1.5741,
      "step": 88180
    },
    {
      "epoch": 44.857578840284845,
      "grad_norm": 36.84898376464844,
      "learning_rate": 5.142421159715158e-06,
      "loss": 1.5675,
      "step": 88190
    },
    {
      "epoch": 44.86266531027467,
      "grad_norm": 43.839542388916016,
      "learning_rate": 5.137334689725331e-06,
      "loss": 1.5576,
      "step": 88200
    },
    {
      "epoch": 44.8677517802645,
      "grad_norm": 35.429718017578125,
      "learning_rate": 5.132248219735504e-06,
      "loss": 1.5767,
      "step": 88210
    },
    {
      "epoch": 44.872838250254325,
      "grad_norm": 43.14178466796875,
      "learning_rate": 5.127161749745677e-06,
      "loss": 1.4854,
      "step": 88220
    },
    {
      "epoch": 44.87792472024415,
      "grad_norm": 60.67192459106445,
      "learning_rate": 5.122075279755849e-06,
      "loss": 1.4123,
      "step": 88230
    },
    {
      "epoch": 44.88301119023398,
      "grad_norm": 40.60719680786133,
      "learning_rate": 5.1169888097660226e-06,
      "loss": 1.5683,
      "step": 88240
    },
    {
      "epoch": 44.888097660223806,
      "grad_norm": 39.93647384643555,
      "learning_rate": 5.111902339776196e-06,
      "loss": 1.5681,
      "step": 88250
    },
    {
      "epoch": 44.89318413021363,
      "grad_norm": 51.93971252441406,
      "learning_rate": 5.106815869786369e-06,
      "loss": 1.5443,
      "step": 88260
    },
    {
      "epoch": 44.89827060020346,
      "grad_norm": 45.48564147949219,
      "learning_rate": 5.1017293997965416e-06,
      "loss": 1.577,
      "step": 88270
    },
    {
      "epoch": 44.90335707019329,
      "grad_norm": 48.31940841674805,
      "learning_rate": 5.096642929806714e-06,
      "loss": 1.5324,
      "step": 88280
    },
    {
      "epoch": 44.908443540183114,
      "grad_norm": 49.25099182128906,
      "learning_rate": 5.091556459816887e-06,
      "loss": 1.5009,
      "step": 88290
    },
    {
      "epoch": 44.91353001017294,
      "grad_norm": 35.082149505615234,
      "learning_rate": 5.0864699898270606e-06,
      "loss": 1.5616,
      "step": 88300
    },
    {
      "epoch": 44.91861648016277,
      "grad_norm": 47.94741439819336,
      "learning_rate": 5.081383519837233e-06,
      "loss": 1.5683,
      "step": 88310
    },
    {
      "epoch": 44.923702950152595,
      "grad_norm": 38.508113861083984,
      "learning_rate": 5.076297049847406e-06,
      "loss": 1.4675,
      "step": 88320
    },
    {
      "epoch": 44.92878942014242,
      "grad_norm": 38.6858024597168,
      "learning_rate": 5.0712105798575795e-06,
      "loss": 1.4601,
      "step": 88330
    },
    {
      "epoch": 44.93387589013225,
      "grad_norm": 41.72829818725586,
      "learning_rate": 5.066124109867752e-06,
      "loss": 1.5668,
      "step": 88340
    },
    {
      "epoch": 44.938962360122076,
      "grad_norm": 41.44234848022461,
      "learning_rate": 5.061037639877924e-06,
      "loss": 1.5887,
      "step": 88350
    },
    {
      "epoch": 44.9440488301119,
      "grad_norm": 38.37257766723633,
      "learning_rate": 5.055951169888098e-06,
      "loss": 1.5479,
      "step": 88360
    },
    {
      "epoch": 44.94913530010173,
      "grad_norm": 44.24829864501953,
      "learning_rate": 5.050864699898271e-06,
      "loss": 1.6095,
      "step": 88370
    },
    {
      "epoch": 44.95422177009156,
      "grad_norm": 33.77909469604492,
      "learning_rate": 5.045778229908444e-06,
      "loss": 1.5907,
      "step": 88380
    },
    {
      "epoch": 44.959308240081384,
      "grad_norm": 42.48131561279297,
      "learning_rate": 5.040691759918617e-06,
      "loss": 1.5575,
      "step": 88390
    },
    {
      "epoch": 44.96439471007121,
      "grad_norm": 39.9183464050293,
      "learning_rate": 5.035605289928789e-06,
      "loss": 1.6516,
      "step": 88400
    },
    {
      "epoch": 44.96948118006104,
      "grad_norm": 32.68280029296875,
      "learning_rate": 5.030518819938962e-06,
      "loss": 1.4767,
      "step": 88410
    },
    {
      "epoch": 44.974567650050865,
      "grad_norm": 40.99480438232422,
      "learning_rate": 5.025432349949136e-06,
      "loss": 1.5556,
      "step": 88420
    },
    {
      "epoch": 44.97965412004069,
      "grad_norm": 48.04515075683594,
      "learning_rate": 5.020345879959309e-06,
      "loss": 1.5632,
      "step": 88430
    },
    {
      "epoch": 44.98474059003052,
      "grad_norm": 46.313472747802734,
      "learning_rate": 5.015259409969481e-06,
      "loss": 1.5433,
      "step": 88440
    },
    {
      "epoch": 44.989827060020346,
      "grad_norm": 36.696353912353516,
      "learning_rate": 5.010172939979655e-06,
      "loss": 1.5202,
      "step": 88450
    },
    {
      "epoch": 44.99491353001017,
      "grad_norm": 35.594417572021484,
      "learning_rate": 5.005086469989827e-06,
      "loss": 1.5309,
      "step": 88460
    },
    {
      "epoch": 45.0,
      "grad_norm": 56.24549865722656,
      "learning_rate": 5e-06,
      "loss": 1.552,
      "step": 88470
    },
    {
      "epoch": 45.0,
      "eval_loss": 5.093989849090576,
      "eval_runtime": 2.9668,
      "eval_samples_per_second": 935.36,
      "eval_steps_per_second": 116.962,
      "step": 88470
    },
    {
      "epoch": 45.00508646998983,
      "grad_norm": 46.47511291503906,
      "learning_rate": 4.994913530010173e-06,
      "loss": 1.3842,
      "step": 88480
    },
    {
      "epoch": 45.010172939979654,
      "grad_norm": 59.68733215332031,
      "learning_rate": 4.989827060020346e-06,
      "loss": 1.601,
      "step": 88490
    },
    {
      "epoch": 45.01525940996948,
      "grad_norm": 41.182647705078125,
      "learning_rate": 4.984740590030519e-06,
      "loss": 1.6053,
      "step": 88500
    },
    {
      "epoch": 45.02034587995931,
      "grad_norm": 45.12348556518555,
      "learning_rate": 4.979654120040692e-06,
      "loss": 1.5257,
      "step": 88510
    },
    {
      "epoch": 45.025432349949135,
      "grad_norm": 40.33538055419922,
      "learning_rate": 4.974567650050864e-06,
      "loss": 1.5035,
      "step": 88520
    },
    {
      "epoch": 45.03051881993896,
      "grad_norm": 41.111358642578125,
      "learning_rate": 4.9694811800610376e-06,
      "loss": 1.5337,
      "step": 88530
    },
    {
      "epoch": 45.03560528992879,
      "grad_norm": 39.200439453125,
      "learning_rate": 4.964394710071211e-06,
      "loss": 1.4931,
      "step": 88540
    },
    {
      "epoch": 45.040691759918616,
      "grad_norm": 37.513240814208984,
      "learning_rate": 4.959308240081384e-06,
      "loss": 1.5732,
      "step": 88550
    },
    {
      "epoch": 45.04577822990844,
      "grad_norm": 39.44995880126953,
      "learning_rate": 4.9542217700915566e-06,
      "loss": 1.6227,
      "step": 88560
    },
    {
      "epoch": 45.05086469989827,
      "grad_norm": 33.67570877075195,
      "learning_rate": 4.94913530010173e-06,
      "loss": 1.4691,
      "step": 88570
    },
    {
      "epoch": 45.0559511698881,
      "grad_norm": 37.52020263671875,
      "learning_rate": 4.944048830111902e-06,
      "loss": 1.5462,
      "step": 88580
    },
    {
      "epoch": 45.061037639877924,
      "grad_norm": 55.68443298339844,
      "learning_rate": 4.9389623601220756e-06,
      "loss": 1.5486,
      "step": 88590
    },
    {
      "epoch": 45.06612410986775,
      "grad_norm": 37.972633361816406,
      "learning_rate": 4.933875890132249e-06,
      "loss": 1.4078,
      "step": 88600
    },
    {
      "epoch": 45.07121057985758,
      "grad_norm": 42.52413558959961,
      "learning_rate": 4.928789420142421e-06,
      "loss": 1.4349,
      "step": 88610
    },
    {
      "epoch": 45.076297049847405,
      "grad_norm": 58.71095657348633,
      "learning_rate": 4.9237029501525946e-06,
      "loss": 1.5049,
      "step": 88620
    },
    {
      "epoch": 45.08138351983723,
      "grad_norm": 36.82020568847656,
      "learning_rate": 4.918616480162767e-06,
      "loss": 1.5385,
      "step": 88630
    },
    {
      "epoch": 45.08646998982706,
      "grad_norm": 39.20235824584961,
      "learning_rate": 4.91353001017294e-06,
      "loss": 1.4887,
      "step": 88640
    },
    {
      "epoch": 45.091556459816886,
      "grad_norm": 44.49776077270508,
      "learning_rate": 4.908443540183113e-06,
      "loss": 1.566,
      "step": 88650
    },
    {
      "epoch": 45.09664292980671,
      "grad_norm": 42.19017028808594,
      "learning_rate": 4.903357070193286e-06,
      "loss": 1.578,
      "step": 88660
    },
    {
      "epoch": 45.10172939979654,
      "grad_norm": 36.84286880493164,
      "learning_rate": 4.898270600203459e-06,
      "loss": 1.5677,
      "step": 88670
    },
    {
      "epoch": 45.10681586978637,
      "grad_norm": 42.872501373291016,
      "learning_rate": 4.8931841302136325e-06,
      "loss": 1.5564,
      "step": 88680
    },
    {
      "epoch": 45.111902339776194,
      "grad_norm": 40.35206985473633,
      "learning_rate": 4.888097660223805e-06,
      "loss": 1.5765,
      "step": 88690
    },
    {
      "epoch": 45.11698880976602,
      "grad_norm": 46.84192657470703,
      "learning_rate": 4.883011190233977e-06,
      "loss": 1.5368,
      "step": 88700
    },
    {
      "epoch": 45.12207527975585,
      "grad_norm": 40.658592224121094,
      "learning_rate": 4.877924720244151e-06,
      "loss": 1.5681,
      "step": 88710
    },
    {
      "epoch": 45.127161749745675,
      "grad_norm": 40.35023498535156,
      "learning_rate": 4.872838250254324e-06,
      "loss": 1.5863,
      "step": 88720
    },
    {
      "epoch": 45.1322482197355,
      "grad_norm": 34.815555572509766,
      "learning_rate": 4.867751780264497e-06,
      "loss": 1.5322,
      "step": 88730
    },
    {
      "epoch": 45.13733468972533,
      "grad_norm": 35.24650573730469,
      "learning_rate": 4.86266531027467e-06,
      "loss": 1.4741,
      "step": 88740
    },
    {
      "epoch": 45.142421159715155,
      "grad_norm": 56.00014114379883,
      "learning_rate": 4.857578840284842e-06,
      "loss": 1.6293,
      "step": 88750
    },
    {
      "epoch": 45.14750762970498,
      "grad_norm": 48.87771224975586,
      "learning_rate": 4.852492370295015e-06,
      "loss": 1.5135,
      "step": 88760
    },
    {
      "epoch": 45.15259409969481,
      "grad_norm": 60.28199768066406,
      "learning_rate": 4.847405900305189e-06,
      "loss": 1.462,
      "step": 88770
    },
    {
      "epoch": 45.157680569684636,
      "grad_norm": 38.16189193725586,
      "learning_rate": 4.842319430315361e-06,
      "loss": 1.5345,
      "step": 88780
    },
    {
      "epoch": 45.16276703967446,
      "grad_norm": 36.90254211425781,
      "learning_rate": 4.837232960325534e-06,
      "loss": 1.5563,
      "step": 88790
    },
    {
      "epoch": 45.16785350966429,
      "grad_norm": 47.184452056884766,
      "learning_rate": 4.832146490335708e-06,
      "loss": 1.6482,
      "step": 88800
    },
    {
      "epoch": 45.17293997965412,
      "grad_norm": 48.485721588134766,
      "learning_rate": 4.82706002034588e-06,
      "loss": 1.5331,
      "step": 88810
    },
    {
      "epoch": 45.178026449643944,
      "grad_norm": 39.58040237426758,
      "learning_rate": 4.8219735503560526e-06,
      "loss": 1.511,
      "step": 88820
    },
    {
      "epoch": 45.18311291963377,
      "grad_norm": 39.324825286865234,
      "learning_rate": 4.816887080366226e-06,
      "loss": 1.5243,
      "step": 88830
    },
    {
      "epoch": 45.1881993896236,
      "grad_norm": 34.49219512939453,
      "learning_rate": 4.811800610376399e-06,
      "loss": 1.4474,
      "step": 88840
    },
    {
      "epoch": 45.193285859613425,
      "grad_norm": 35.044551849365234,
      "learning_rate": 4.806714140386572e-06,
      "loss": 1.4709,
      "step": 88850
    },
    {
      "epoch": 45.19837232960325,
      "grad_norm": 34.71863555908203,
      "learning_rate": 4.801627670396745e-06,
      "loss": 1.5138,
      "step": 88860
    },
    {
      "epoch": 45.20345879959308,
      "grad_norm": 36.839656829833984,
      "learning_rate": 4.796541200406917e-06,
      "loss": 1.5939,
      "step": 88870
    },
    {
      "epoch": 45.208545269582906,
      "grad_norm": 44.82649612426758,
      "learning_rate": 4.7914547304170906e-06,
      "loss": 1.548,
      "step": 88880
    },
    {
      "epoch": 45.21363173957273,
      "grad_norm": 41.36574172973633,
      "learning_rate": 4.786368260427264e-06,
      "loss": 1.5252,
      "step": 88890
    },
    {
      "epoch": 45.21871820956256,
      "grad_norm": 38.89982604980469,
      "learning_rate": 4.781281790437437e-06,
      "loss": 1.5596,
      "step": 88900
    },
    {
      "epoch": 45.22380467955239,
      "grad_norm": 34.52314758300781,
      "learning_rate": 4.7761953204476096e-06,
      "loss": 1.5247,
      "step": 88910
    },
    {
      "epoch": 45.22889114954222,
      "grad_norm": 42.375999450683594,
      "learning_rate": 4.771108850457783e-06,
      "loss": 1.5462,
      "step": 88920
    },
    {
      "epoch": 45.23397761953205,
      "grad_norm": 40.40367889404297,
      "learning_rate": 4.766022380467955e-06,
      "loss": 1.6066,
      "step": 88930
    },
    {
      "epoch": 45.239064089521875,
      "grad_norm": 42.155242919921875,
      "learning_rate": 4.7609359104781286e-06,
      "loss": 1.5099,
      "step": 88940
    },
    {
      "epoch": 45.2441505595117,
      "grad_norm": 46.137550354003906,
      "learning_rate": 4.755849440488301e-06,
      "loss": 1.5817,
      "step": 88950
    },
    {
      "epoch": 45.24923702950153,
      "grad_norm": 40.36712646484375,
      "learning_rate": 4.750762970498474e-06,
      "loss": 1.543,
      "step": 88960
    },
    {
      "epoch": 45.254323499491356,
      "grad_norm": 36.909698486328125,
      "learning_rate": 4.7456765005086476e-06,
      "loss": 1.6438,
      "step": 88970
    },
    {
      "epoch": 45.25940996948118,
      "grad_norm": 45.06081771850586,
      "learning_rate": 4.740590030518821e-06,
      "loss": 1.5149,
      "step": 88980
    },
    {
      "epoch": 45.26449643947101,
      "grad_norm": 39.620887756347656,
      "learning_rate": 4.735503560528992e-06,
      "loss": 1.5091,
      "step": 88990
    },
    {
      "epoch": 45.26958290946084,
      "grad_norm": 57.7980842590332,
      "learning_rate": 4.730417090539166e-06,
      "loss": 1.5002,
      "step": 89000
    },
    {
      "epoch": 45.274669379450664,
      "grad_norm": 42.18132781982422,
      "learning_rate": 4.725330620549339e-06,
      "loss": 1.527,
      "step": 89010
    },
    {
      "epoch": 45.27975584944049,
      "grad_norm": 35.73700714111328,
      "learning_rate": 4.720244150559512e-06,
      "loss": 1.52,
      "step": 89020
    },
    {
      "epoch": 45.28484231943032,
      "grad_norm": 39.78742218017578,
      "learning_rate": 4.715157680569685e-06,
      "loss": 1.6152,
      "step": 89030
    },
    {
      "epoch": 45.289928789420145,
      "grad_norm": 43.275062561035156,
      "learning_rate": 4.710071210579858e-06,
      "loss": 1.5653,
      "step": 89040
    },
    {
      "epoch": 45.29501525940997,
      "grad_norm": 44.33333969116211,
      "learning_rate": 4.70498474059003e-06,
      "loss": 1.5021,
      "step": 89050
    },
    {
      "epoch": 45.3001017293998,
      "grad_norm": 41.77934646606445,
      "learning_rate": 4.699898270600204e-06,
      "loss": 1.587,
      "step": 89060
    },
    {
      "epoch": 45.305188199389626,
      "grad_norm": 33.842342376708984,
      "learning_rate": 4.694811800610377e-06,
      "loss": 1.538,
      "step": 89070
    },
    {
      "epoch": 45.31027466937945,
      "grad_norm": 49.774166107177734,
      "learning_rate": 4.689725330620549e-06,
      "loss": 1.503,
      "step": 89080
    },
    {
      "epoch": 45.31536113936928,
      "grad_norm": 45.971954345703125,
      "learning_rate": 4.684638860630723e-06,
      "loss": 1.5415,
      "step": 89090
    },
    {
      "epoch": 45.32044760935911,
      "grad_norm": 45.61203384399414,
      "learning_rate": 4.679552390640896e-06,
      "loss": 1.5067,
      "step": 89100
    },
    {
      "epoch": 45.325534079348934,
      "grad_norm": 33.50916290283203,
      "learning_rate": 4.674465920651068e-06,
      "loss": 1.5609,
      "step": 89110
    },
    {
      "epoch": 45.33062054933876,
      "grad_norm": 36.958160400390625,
      "learning_rate": 4.669379450661241e-06,
      "loss": 1.5195,
      "step": 89120
    },
    {
      "epoch": 45.33570701932859,
      "grad_norm": 37.40457534790039,
      "learning_rate": 4.664292980671414e-06,
      "loss": 1.5932,
      "step": 89130
    },
    {
      "epoch": 45.340793489318415,
      "grad_norm": 44.2151985168457,
      "learning_rate": 4.659206510681587e-06,
      "loss": 1.4654,
      "step": 89140
    },
    {
      "epoch": 45.34587995930824,
      "grad_norm": 37.562049865722656,
      "learning_rate": 4.654120040691761e-06,
      "loss": 1.5755,
      "step": 89150
    },
    {
      "epoch": 45.35096642929807,
      "grad_norm": 43.328269958496094,
      "learning_rate": 4.649033570701933e-06,
      "loss": 1.5126,
      "step": 89160
    },
    {
      "epoch": 45.356052899287896,
      "grad_norm": 46.96538543701172,
      "learning_rate": 4.6439471007121056e-06,
      "loss": 1.4716,
      "step": 89170
    },
    {
      "epoch": 45.36113936927772,
      "grad_norm": 36.44528579711914,
      "learning_rate": 4.638860630722279e-06,
      "loss": 1.48,
      "step": 89180
    },
    {
      "epoch": 45.36622583926755,
      "grad_norm": 42.9342041015625,
      "learning_rate": 4.633774160732452e-06,
      "loss": 1.5413,
      "step": 89190
    },
    {
      "epoch": 45.37131230925738,
      "grad_norm": 39.23350524902344,
      "learning_rate": 4.6286876907426246e-06,
      "loss": 1.5425,
      "step": 89200
    },
    {
      "epoch": 45.376398779247204,
      "grad_norm": 37.512447357177734,
      "learning_rate": 4.623601220752798e-06,
      "loss": 1.5028,
      "step": 89210
    },
    {
      "epoch": 45.38148524923703,
      "grad_norm": 48.57202911376953,
      "learning_rate": 4.618514750762971e-06,
      "loss": 1.4837,
      "step": 89220
    },
    {
      "epoch": 45.38657171922686,
      "grad_norm": 49.3680534362793,
      "learning_rate": 4.6134282807731436e-06,
      "loss": 1.5252,
      "step": 89230
    },
    {
      "epoch": 45.391658189216685,
      "grad_norm": 51.512882232666016,
      "learning_rate": 4.608341810783317e-06,
      "loss": 1.4572,
      "step": 89240
    },
    {
      "epoch": 45.39674465920651,
      "grad_norm": 35.616451263427734,
      "learning_rate": 4.603255340793489e-06,
      "loss": 1.5392,
      "step": 89250
    },
    {
      "epoch": 45.40183112919634,
      "grad_norm": 42.46259307861328,
      "learning_rate": 4.5981688708036626e-06,
      "loss": 1.5945,
      "step": 89260
    },
    {
      "epoch": 45.406917599186166,
      "grad_norm": 48.196800231933594,
      "learning_rate": 4.593082400813836e-06,
      "loss": 1.536,
      "step": 89270
    },
    {
      "epoch": 45.41200406917599,
      "grad_norm": 34.39004135131836,
      "learning_rate": 4.587995930824008e-06,
      "loss": 1.5046,
      "step": 89280
    },
    {
      "epoch": 45.41709053916582,
      "grad_norm": 44.61153030395508,
      "learning_rate": 4.582909460834181e-06,
      "loss": 1.567,
      "step": 89290
    },
    {
      "epoch": 45.42217700915565,
      "grad_norm": 44.16111755371094,
      "learning_rate": 4.577822990844354e-06,
      "loss": 1.4961,
      "step": 89300
    },
    {
      "epoch": 45.42726347914547,
      "grad_norm": 42.75712966918945,
      "learning_rate": 4.572736520854527e-06,
      "loss": 1.5369,
      "step": 89310
    },
    {
      "epoch": 45.4323499491353,
      "grad_norm": 44.24708938598633,
      "learning_rate": 4.5676500508647006e-06,
      "loss": 1.5686,
      "step": 89320
    },
    {
      "epoch": 45.43743641912513,
      "grad_norm": 35.696529388427734,
      "learning_rate": 4.562563580874873e-06,
      "loss": 1.4913,
      "step": 89330
    },
    {
      "epoch": 45.442522889114954,
      "grad_norm": 42.99140548706055,
      "learning_rate": 4.557477110885046e-06,
      "loss": 1.4967,
      "step": 89340
    },
    {
      "epoch": 45.44760935910478,
      "grad_norm": 34.3355712890625,
      "learning_rate": 4.552390640895219e-06,
      "loss": 1.5343,
      "step": 89350
    },
    {
      "epoch": 45.45269582909461,
      "grad_norm": 42.88712692260742,
      "learning_rate": 4.547304170905392e-06,
      "loss": 1.5631,
      "step": 89360
    },
    {
      "epoch": 45.457782299084435,
      "grad_norm": 53.48223876953125,
      "learning_rate": 4.542217700915564e-06,
      "loss": 1.5973,
      "step": 89370
    },
    {
      "epoch": 45.46286876907426,
      "grad_norm": 49.27330780029297,
      "learning_rate": 4.537131230925738e-06,
      "loss": 1.4847,
      "step": 89380
    },
    {
      "epoch": 45.46795523906409,
      "grad_norm": 45.296504974365234,
      "learning_rate": 4.532044760935911e-06,
      "loss": 1.5,
      "step": 89390
    },
    {
      "epoch": 45.473041709053916,
      "grad_norm": 42.649635314941406,
      "learning_rate": 4.526958290946083e-06,
      "loss": 1.5903,
      "step": 89400
    },
    {
      "epoch": 45.47812817904374,
      "grad_norm": 33.75382614135742,
      "learning_rate": 4.521871820956257e-06,
      "loss": 1.5933,
      "step": 89410
    },
    {
      "epoch": 45.48321464903357,
      "grad_norm": 39.385005950927734,
      "learning_rate": 4.516785350966429e-06,
      "loss": 1.5594,
      "step": 89420
    },
    {
      "epoch": 45.4883011190234,
      "grad_norm": 36.521934509277344,
      "learning_rate": 4.511698880976602e-06,
      "loss": 1.561,
      "step": 89430
    },
    {
      "epoch": 45.493387589013224,
      "grad_norm": 42.36467361450195,
      "learning_rate": 4.506612410986776e-06,
      "loss": 1.4641,
      "step": 89440
    },
    {
      "epoch": 45.49847405900305,
      "grad_norm": 40.8221321105957,
      "learning_rate": 4.501525940996949e-06,
      "loss": 1.5254,
      "step": 89450
    },
    {
      "epoch": 45.50356052899288,
      "grad_norm": 39.78973388671875,
      "learning_rate": 4.496439471007121e-06,
      "loss": 1.498,
      "step": 89460
    },
    {
      "epoch": 45.508646998982705,
      "grad_norm": 41.138343811035156,
      "learning_rate": 4.491353001017294e-06,
      "loss": 1.4874,
      "step": 89470
    },
    {
      "epoch": 45.51373346897253,
      "grad_norm": 42.48979949951172,
      "learning_rate": 4.486266531027467e-06,
      "loss": 1.5828,
      "step": 89480
    },
    {
      "epoch": 45.51881993896236,
      "grad_norm": 34.5498046875,
      "learning_rate": 4.48118006103764e-06,
      "loss": 1.5269,
      "step": 89490
    },
    {
      "epoch": 45.523906408952186,
      "grad_norm": 45.608211517333984,
      "learning_rate": 4.476093591047813e-06,
      "loss": 1.5254,
      "step": 89500
    },
    {
      "epoch": 45.52899287894201,
      "grad_norm": 47.565185546875,
      "learning_rate": 4.471007121057986e-06,
      "loss": 1.4752,
      "step": 89510
    },
    {
      "epoch": 45.53407934893184,
      "grad_norm": 40.10314178466797,
      "learning_rate": 4.4659206510681586e-06,
      "loss": 1.5132,
      "step": 89520
    },
    {
      "epoch": 45.53916581892167,
      "grad_norm": 38.27754211425781,
      "learning_rate": 4.460834181078332e-06,
      "loss": 1.6105,
      "step": 89530
    },
    {
      "epoch": 45.544252288911494,
      "grad_norm": 45.2034912109375,
      "learning_rate": 4.455747711088504e-06,
      "loss": 1.5914,
      "step": 89540
    },
    {
      "epoch": 45.54933875890132,
      "grad_norm": 40.95773696899414,
      "learning_rate": 4.4506612410986776e-06,
      "loss": 1.529,
      "step": 89550
    },
    {
      "epoch": 45.55442522889115,
      "grad_norm": 34.15352249145508,
      "learning_rate": 4.445574771108851e-06,
      "loss": 1.5337,
      "step": 89560
    },
    {
      "epoch": 45.559511698880975,
      "grad_norm": 35.86591339111328,
      "learning_rate": 4.440488301119024e-06,
      "loss": 1.5446,
      "step": 89570
    },
    {
      "epoch": 45.5645981688708,
      "grad_norm": 40.88465881347656,
      "learning_rate": 4.4354018311291966e-06,
      "loss": 1.5208,
      "step": 89580
    },
    {
      "epoch": 45.56968463886063,
      "grad_norm": 48.22875213623047,
      "learning_rate": 4.430315361139369e-06,
      "loss": 1.5414,
      "step": 89590
    },
    {
      "epoch": 45.574771108850456,
      "grad_norm": 41.898929595947266,
      "learning_rate": 4.425228891149542e-06,
      "loss": 1.6205,
      "step": 89600
    },
    {
      "epoch": 45.57985757884028,
      "grad_norm": 39.14078140258789,
      "learning_rate": 4.4201424211597156e-06,
      "loss": 1.5012,
      "step": 89610
    },
    {
      "epoch": 45.58494404883011,
      "grad_norm": 49.318870544433594,
      "learning_rate": 4.415055951169889e-06,
      "loss": 1.6025,
      "step": 89620
    },
    {
      "epoch": 45.59003051881994,
      "grad_norm": 44.372962951660156,
      "learning_rate": 4.409969481180061e-06,
      "loss": 1.5239,
      "step": 89630
    },
    {
      "epoch": 45.595116988809764,
      "grad_norm": 41.674560546875,
      "learning_rate": 4.404883011190234e-06,
      "loss": 1.5334,
      "step": 89640
    },
    {
      "epoch": 45.60020345879959,
      "grad_norm": 37.672935485839844,
      "learning_rate": 4.399796541200407e-06,
      "loss": 1.5371,
      "step": 89650
    },
    {
      "epoch": 45.60528992878942,
      "grad_norm": 44.70058822631836,
      "learning_rate": 4.39471007121058e-06,
      "loss": 1.5647,
      "step": 89660
    },
    {
      "epoch": 45.610376398779245,
      "grad_norm": 39.07621383666992,
      "learning_rate": 4.389623601220753e-06,
      "loss": 1.4991,
      "step": 89670
    },
    {
      "epoch": 45.61546286876907,
      "grad_norm": 32.6088752746582,
      "learning_rate": 4.384537131230926e-06,
      "loss": 1.5493,
      "step": 89680
    },
    {
      "epoch": 45.6205493387589,
      "grad_norm": 54.749359130859375,
      "learning_rate": 4.379450661241099e-06,
      "loss": 1.4544,
      "step": 89690
    },
    {
      "epoch": 45.625635808748726,
      "grad_norm": 43.01134490966797,
      "learning_rate": 4.374364191251272e-06,
      "loss": 1.5429,
      "step": 89700
    },
    {
      "epoch": 45.63072227873855,
      "grad_norm": 42.52850341796875,
      "learning_rate": 4.369277721261445e-06,
      "loss": 1.5529,
      "step": 89710
    },
    {
      "epoch": 45.63580874872838,
      "grad_norm": 48.40120315551758,
      "learning_rate": 4.364191251271617e-06,
      "loss": 1.4706,
      "step": 89720
    },
    {
      "epoch": 45.64089521871821,
      "grad_norm": 33.81947708129883,
      "learning_rate": 4.359104781281791e-06,
      "loss": 1.5052,
      "step": 89730
    },
    {
      "epoch": 45.645981688708034,
      "grad_norm": 39.2589111328125,
      "learning_rate": 4.354018311291964e-06,
      "loss": 1.5972,
      "step": 89740
    },
    {
      "epoch": 45.65106815869786,
      "grad_norm": 41.174713134765625,
      "learning_rate": 4.348931841302136e-06,
      "loss": 1.5175,
      "step": 89750
    },
    {
      "epoch": 45.65615462868769,
      "grad_norm": 33.76212692260742,
      "learning_rate": 4.343845371312309e-06,
      "loss": 1.6063,
      "step": 89760
    },
    {
      "epoch": 45.661241098677515,
      "grad_norm": 38.8608283996582,
      "learning_rate": 4.338758901322482e-06,
      "loss": 1.5449,
      "step": 89770
    },
    {
      "epoch": 45.66632756866734,
      "grad_norm": 42.15883255004883,
      "learning_rate": 4.333672431332655e-06,
      "loss": 1.5765,
      "step": 89780
    },
    {
      "epoch": 45.67141403865717,
      "grad_norm": 38.66046905517578,
      "learning_rate": 4.328585961342829e-06,
      "loss": 1.5389,
      "step": 89790
    },
    {
      "epoch": 45.676500508646996,
      "grad_norm": 42.927249908447266,
      "learning_rate": 4.323499491353001e-06,
      "loss": 1.5176,
      "step": 89800
    },
    {
      "epoch": 45.68158697863683,
      "grad_norm": 41.98051452636719,
      "learning_rate": 4.318413021363174e-06,
      "loss": 1.4838,
      "step": 89810
    },
    {
      "epoch": 45.68667344862666,
      "grad_norm": 40.866676330566406,
      "learning_rate": 4.313326551373347e-06,
      "loss": 1.5341,
      "step": 89820
    },
    {
      "epoch": 45.691759918616484,
      "grad_norm": 39.479488372802734,
      "learning_rate": 4.30824008138352e-06,
      "loss": 1.4676,
      "step": 89830
    },
    {
      "epoch": 45.69684638860631,
      "grad_norm": 40.89692687988281,
      "learning_rate": 4.3031536113936926e-06,
      "loss": 1.481,
      "step": 89840
    },
    {
      "epoch": 45.70193285859614,
      "grad_norm": 41.56648254394531,
      "learning_rate": 4.298067141403866e-06,
      "loss": 1.6068,
      "step": 89850
    },
    {
      "epoch": 45.707019328585965,
      "grad_norm": 36.203372955322266,
      "learning_rate": 4.292980671414039e-06,
      "loss": 1.5306,
      "step": 89860
    },
    {
      "epoch": 45.71210579857579,
      "grad_norm": 44.456241607666016,
      "learning_rate": 4.2878942014242116e-06,
      "loss": 1.6163,
      "step": 89870
    },
    {
      "epoch": 45.71719226856562,
      "grad_norm": 48.559669494628906,
      "learning_rate": 4.282807731434385e-06,
      "loss": 1.5066,
      "step": 89880
    },
    {
      "epoch": 45.722278738555445,
      "grad_norm": 41.852481842041016,
      "learning_rate": 4.277721261444557e-06,
      "loss": 1.584,
      "step": 89890
    },
    {
      "epoch": 45.72736520854527,
      "grad_norm": 45.83131408691406,
      "learning_rate": 4.2726347914547306e-06,
      "loss": 1.578,
      "step": 89900
    },
    {
      "epoch": 45.7324516785351,
      "grad_norm": 50.19821548461914,
      "learning_rate": 4.267548321464904e-06,
      "loss": 1.5311,
      "step": 89910
    },
    {
      "epoch": 45.737538148524926,
      "grad_norm": 40.746341705322266,
      "learning_rate": 4.262461851475077e-06,
      "loss": 1.5328,
      "step": 89920
    },
    {
      "epoch": 45.74262461851475,
      "grad_norm": 43.63516616821289,
      "learning_rate": 4.2573753814852496e-06,
      "loss": 1.6204,
      "step": 89930
    },
    {
      "epoch": 45.74771108850458,
      "grad_norm": 46.881282806396484,
      "learning_rate": 4.252288911495422e-06,
      "loss": 1.5697,
      "step": 89940
    },
    {
      "epoch": 45.75279755849441,
      "grad_norm": 42.175601959228516,
      "learning_rate": 4.247202441505595e-06,
      "loss": 1.52,
      "step": 89950
    },
    {
      "epoch": 45.757884028484234,
      "grad_norm": 36.86276626586914,
      "learning_rate": 4.2421159715157686e-06,
      "loss": 1.4537,
      "step": 89960
    },
    {
      "epoch": 45.76297049847406,
      "grad_norm": 59.66595458984375,
      "learning_rate": 4.237029501525941e-06,
      "loss": 1.4741,
      "step": 89970
    },
    {
      "epoch": 45.76805696846389,
      "grad_norm": 49.27077865600586,
      "learning_rate": 4.231943031536114e-06,
      "loss": 1.5268,
      "step": 89980
    },
    {
      "epoch": 45.773143438453715,
      "grad_norm": 50.93927764892578,
      "learning_rate": 4.226856561546287e-06,
      "loss": 1.5344,
      "step": 89990
    },
    {
      "epoch": 45.77822990844354,
      "grad_norm": 35.80443572998047,
      "learning_rate": 4.22177009155646e-06,
      "loss": 1.5531,
      "step": 90000
    },
    {
      "epoch": 45.78331637843337,
      "grad_norm": 49.97017288208008,
      "learning_rate": 4.216683621566632e-06,
      "loss": 1.4739,
      "step": 90010
    },
    {
      "epoch": 45.788402848423196,
      "grad_norm": 50.15482711791992,
      "learning_rate": 4.211597151576806e-06,
      "loss": 1.5383,
      "step": 90020
    },
    {
      "epoch": 45.79348931841302,
      "grad_norm": 38.191471099853516,
      "learning_rate": 4.206510681586979e-06,
      "loss": 1.6017,
      "step": 90030
    },
    {
      "epoch": 45.79857578840285,
      "grad_norm": 50.10052490234375,
      "learning_rate": 4.201424211597152e-06,
      "loss": 1.4308,
      "step": 90040
    },
    {
      "epoch": 45.80366225839268,
      "grad_norm": 46.5721321105957,
      "learning_rate": 4.196337741607325e-06,
      "loss": 1.4961,
      "step": 90050
    },
    {
      "epoch": 45.808748728382504,
      "grad_norm": 38.58320999145508,
      "learning_rate": 4.191251271617497e-06,
      "loss": 1.5707,
      "step": 90060
    },
    {
      "epoch": 45.81383519837233,
      "grad_norm": 41.06324768066406,
      "learning_rate": 4.18616480162767e-06,
      "loss": 1.4778,
      "step": 90070
    },
    {
      "epoch": 45.81892166836216,
      "grad_norm": 55.09328842163086,
      "learning_rate": 4.181078331637844e-06,
      "loss": 1.4927,
      "step": 90080
    },
    {
      "epoch": 45.824008138351985,
      "grad_norm": 44.35150146484375,
      "learning_rate": 4.175991861648017e-06,
      "loss": 1.5173,
      "step": 90090
    },
    {
      "epoch": 45.82909460834181,
      "grad_norm": 48.08249282836914,
      "learning_rate": 4.170905391658189e-06,
      "loss": 1.5656,
      "step": 90100
    },
    {
      "epoch": 45.83418107833164,
      "grad_norm": 50.646141052246094,
      "learning_rate": 4.165818921668362e-06,
      "loss": 1.5157,
      "step": 90110
    },
    {
      "epoch": 45.839267548321466,
      "grad_norm": 38.72975158691406,
      "learning_rate": 4.160732451678535e-06,
      "loss": 1.5021,
      "step": 90120
    },
    {
      "epoch": 45.84435401831129,
      "grad_norm": 40.346866607666016,
      "learning_rate": 4.155645981688708e-06,
      "loss": 1.4854,
      "step": 90130
    },
    {
      "epoch": 45.84944048830112,
      "grad_norm": 41.485809326171875,
      "learning_rate": 4.150559511698881e-06,
      "loss": 1.5711,
      "step": 90140
    },
    {
      "epoch": 45.85452695829095,
      "grad_norm": 37.730926513671875,
      "learning_rate": 4.145473041709054e-06,
      "loss": 1.4788,
      "step": 90150
    },
    {
      "epoch": 45.859613428280774,
      "grad_norm": 36.73368835449219,
      "learning_rate": 4.140386571719227e-06,
      "loss": 1.491,
      "step": 90160
    },
    {
      "epoch": 45.8646998982706,
      "grad_norm": 41.77024841308594,
      "learning_rate": 4.1353001017294e-06,
      "loss": 1.5279,
      "step": 90170
    },
    {
      "epoch": 45.86978636826043,
      "grad_norm": 39.792537689208984,
      "learning_rate": 4.130213631739572e-06,
      "loss": 1.4937,
      "step": 90180
    },
    {
      "epoch": 45.874872838250255,
      "grad_norm": 48.95017623901367,
      "learning_rate": 4.1251271617497456e-06,
      "loss": 1.5934,
      "step": 90190
    },
    {
      "epoch": 45.87995930824008,
      "grad_norm": 40.5301399230957,
      "learning_rate": 4.120040691759919e-06,
      "loss": 1.5134,
      "step": 90200
    },
    {
      "epoch": 45.88504577822991,
      "grad_norm": 34.9007568359375,
      "learning_rate": 4.114954221770092e-06,
      "loss": 1.5598,
      "step": 90210
    },
    {
      "epoch": 45.890132248219736,
      "grad_norm": 37.384788513183594,
      "learning_rate": 4.109867751780265e-06,
      "loss": 1.5026,
      "step": 90220
    },
    {
      "epoch": 45.89521871820956,
      "grad_norm": 36.7296142578125,
      "learning_rate": 4.104781281790437e-06,
      "loss": 1.4647,
      "step": 90230
    },
    {
      "epoch": 45.90030518819939,
      "grad_norm": 43.09592056274414,
      "learning_rate": 4.09969481180061e-06,
      "loss": 1.5621,
      "step": 90240
    },
    {
      "epoch": 45.90539165818922,
      "grad_norm": 47.16233444213867,
      "learning_rate": 4.0946083418107836e-06,
      "loss": 1.452,
      "step": 90250
    },
    {
      "epoch": 45.910478128179044,
      "grad_norm": 40.394996643066406,
      "learning_rate": 4.089521871820957e-06,
      "loss": 1.5628,
      "step": 90260
    },
    {
      "epoch": 45.91556459816887,
      "grad_norm": 41.37417221069336,
      "learning_rate": 4.084435401831129e-06,
      "loss": 1.6136,
      "step": 90270
    },
    {
      "epoch": 45.9206510681587,
      "grad_norm": 41.14729309082031,
      "learning_rate": 4.0793489318413026e-06,
      "loss": 1.5231,
      "step": 90280
    },
    {
      "epoch": 45.925737538148525,
      "grad_norm": 38.526119232177734,
      "learning_rate": 4.074262461851475e-06,
      "loss": 1.5013,
      "step": 90290
    },
    {
      "epoch": 45.93082400813835,
      "grad_norm": 44.80995559692383,
      "learning_rate": 4.069175991861648e-06,
      "loss": 1.5066,
      "step": 90300
    },
    {
      "epoch": 45.93591047812818,
      "grad_norm": 46.97532653808594,
      "learning_rate": 4.064089521871821e-06,
      "loss": 1.4974,
      "step": 90310
    },
    {
      "epoch": 45.940996948118006,
      "grad_norm": 45.73535919189453,
      "learning_rate": 4.059003051881994e-06,
      "loss": 1.5491,
      "step": 90320
    },
    {
      "epoch": 45.94608341810783,
      "grad_norm": 48.47193908691406,
      "learning_rate": 4.053916581892167e-06,
      "loss": 1.5459,
      "step": 90330
    },
    {
      "epoch": 45.95116988809766,
      "grad_norm": 38.335262298583984,
      "learning_rate": 4.0488301119023406e-06,
      "loss": 1.5536,
      "step": 90340
    },
    {
      "epoch": 45.95625635808749,
      "grad_norm": 36.86525344848633,
      "learning_rate": 4.043743641912513e-06,
      "loss": 1.5064,
      "step": 90350
    },
    {
      "epoch": 45.96134282807731,
      "grad_norm": 33.50246047973633,
      "learning_rate": 4.038657171922685e-06,
      "loss": 1.4609,
      "step": 90360
    },
    {
      "epoch": 45.96642929806714,
      "grad_norm": 38.57298278808594,
      "learning_rate": 4.033570701932859e-06,
      "loss": 1.4345,
      "step": 90370
    },
    {
      "epoch": 45.97151576805697,
      "grad_norm": 37.702999114990234,
      "learning_rate": 4.028484231943032e-06,
      "loss": 1.573,
      "step": 90380
    },
    {
      "epoch": 45.976602238046794,
      "grad_norm": 40.782222747802734,
      "learning_rate": 4.023397761953205e-06,
      "loss": 1.5331,
      "step": 90390
    },
    {
      "epoch": 45.98168870803662,
      "grad_norm": 45.27494812011719,
      "learning_rate": 4.018311291963378e-06,
      "loss": 1.6531,
      "step": 90400
    },
    {
      "epoch": 45.98677517802645,
      "grad_norm": 44.93870544433594,
      "learning_rate": 4.01322482197355e-06,
      "loss": 1.4181,
      "step": 90410
    },
    {
      "epoch": 45.991861648016275,
      "grad_norm": 37.79243087768555,
      "learning_rate": 4.008138351983723e-06,
      "loss": 1.6085,
      "step": 90420
    },
    {
      "epoch": 45.9969481180061,
      "grad_norm": 37.62648010253906,
      "learning_rate": 4.003051881993897e-06,
      "loss": 1.6387,
      "step": 90430
    },
    {
      "epoch": 46.0,
      "eval_loss": 5.102329730987549,
      "eval_runtime": 2.8752,
      "eval_samples_per_second": 965.166,
      "eval_steps_per_second": 120.689,
      "step": 90436
    },
    {
      "epoch": 46.00203458799593,
      "grad_norm": 35.19894790649414,
      "learning_rate": 3.997965412004069e-06,
      "loss": 1.4636,
      "step": 90440
    },
    {
      "epoch": 46.007121057985756,
      "grad_norm": 48.83572769165039,
      "learning_rate": 3.992878942014242e-06,
      "loss": 1.5387,
      "step": 90450
    },
    {
      "epoch": 46.01220752797558,
      "grad_norm": 41.8465690612793,
      "learning_rate": 3.987792472024416e-06,
      "loss": 1.5677,
      "step": 90460
    },
    {
      "epoch": 46.01729399796541,
      "grad_norm": 29.396224975585938,
      "learning_rate": 3.982706002034588e-06,
      "loss": 1.4395,
      "step": 90470
    },
    {
      "epoch": 46.02238046795524,
      "grad_norm": 44.721683502197266,
      "learning_rate": 3.9776195320447606e-06,
      "loss": 1.5361,
      "step": 90480
    },
    {
      "epoch": 46.027466937945064,
      "grad_norm": 35.15915298461914,
      "learning_rate": 3.972533062054934e-06,
      "loss": 1.4669,
      "step": 90490
    },
    {
      "epoch": 46.03255340793489,
      "grad_norm": 38.44381332397461,
      "learning_rate": 3.967446592065107e-06,
      "loss": 1.4674,
      "step": 90500
    },
    {
      "epoch": 46.03763987792472,
      "grad_norm": 38.32972717285156,
      "learning_rate": 3.96236012207528e-06,
      "loss": 1.5822,
      "step": 90510
    },
    {
      "epoch": 46.042726347914545,
      "grad_norm": 51.01749801635742,
      "learning_rate": 3.957273652085453e-06,
      "loss": 1.5394,
      "step": 90520
    },
    {
      "epoch": 46.04781281790437,
      "grad_norm": 45.514381408691406,
      "learning_rate": 3.952187182095625e-06,
      "loss": 1.5834,
      "step": 90530
    },
    {
      "epoch": 46.0528992878942,
      "grad_norm": 35.106117248535156,
      "learning_rate": 3.9471007121057986e-06,
      "loss": 1.5367,
      "step": 90540
    },
    {
      "epoch": 46.057985757884026,
      "grad_norm": 40.77891159057617,
      "learning_rate": 3.942014242115972e-06,
      "loss": 1.4631,
      "step": 90550
    },
    {
      "epoch": 46.06307222787385,
      "grad_norm": 36.77606964111328,
      "learning_rate": 3.936927772126145e-06,
      "loss": 1.4749,
      "step": 90560
    },
    {
      "epoch": 46.06815869786368,
      "grad_norm": 37.894283294677734,
      "learning_rate": 3.9318413021363176e-06,
      "loss": 1.6151,
      "step": 90570
    },
    {
      "epoch": 46.07324516785351,
      "grad_norm": 46.2198600769043,
      "learning_rate": 3.926754832146491e-06,
      "loss": 1.5913,
      "step": 90580
    },
    {
      "epoch": 46.078331637843334,
      "grad_norm": 40.07435989379883,
      "learning_rate": 3.921668362156663e-06,
      "loss": 1.506,
      "step": 90590
    },
    {
      "epoch": 46.08341810783316,
      "grad_norm": 47.10776138305664,
      "learning_rate": 3.9165818921668366e-06,
      "loss": 1.4998,
      "step": 90600
    },
    {
      "epoch": 46.08850457782299,
      "grad_norm": 47.57010269165039,
      "learning_rate": 3.911495422177009e-06,
      "loss": 1.4651,
      "step": 90610
    },
    {
      "epoch": 46.093591047812815,
      "grad_norm": 37.76283264160156,
      "learning_rate": 3.906408952187182e-06,
      "loss": 1.5791,
      "step": 90620
    },
    {
      "epoch": 46.09867751780264,
      "grad_norm": 36.665992736816406,
      "learning_rate": 3.9013224821973556e-06,
      "loss": 1.5753,
      "step": 90630
    },
    {
      "epoch": 46.10376398779247,
      "grad_norm": 41.288665771484375,
      "learning_rate": 3.896236012207528e-06,
      "loss": 1.5411,
      "step": 90640
    },
    {
      "epoch": 46.108850457782296,
      "grad_norm": 31.40384292602539,
      "learning_rate": 3.8911495422177e-06,
      "loss": 1.5113,
      "step": 90650
    },
    {
      "epoch": 46.11393692777212,
      "grad_norm": 36.81268310546875,
      "learning_rate": 3.886063072227874e-06,
      "loss": 1.4955,
      "step": 90660
    },
    {
      "epoch": 46.11902339776195,
      "grad_norm": 41.33707809448242,
      "learning_rate": 3.880976602238047e-06,
      "loss": 1.5496,
      "step": 90670
    },
    {
      "epoch": 46.12410986775178,
      "grad_norm": 32.42790603637695,
      "learning_rate": 3.87589013224822e-06,
      "loss": 1.5432,
      "step": 90680
    },
    {
      "epoch": 46.129196337741604,
      "grad_norm": 33.68802261352539,
      "learning_rate": 3.8708036622583935e-06,
      "loss": 1.4376,
      "step": 90690
    },
    {
      "epoch": 46.13428280773143,
      "grad_norm": 33.64441680908203,
      "learning_rate": 3.865717192268566e-06,
      "loss": 1.5257,
      "step": 90700
    },
    {
      "epoch": 46.139369277721265,
      "grad_norm": 39.88079833984375,
      "learning_rate": 3.860630722278738e-06,
      "loss": 1.5806,
      "step": 90710
    },
    {
      "epoch": 46.14445574771109,
      "grad_norm": 36.742271423339844,
      "learning_rate": 3.855544252288912e-06,
      "loss": 1.4591,
      "step": 90720
    },
    {
      "epoch": 46.14954221770092,
      "grad_norm": 67.52888488769531,
      "learning_rate": 3.850457782299085e-06,
      "loss": 1.4979,
      "step": 90730
    },
    {
      "epoch": 46.154628687690746,
      "grad_norm": 37.038578033447266,
      "learning_rate": 3.845371312309257e-06,
      "loss": 1.5001,
      "step": 90740
    },
    {
      "epoch": 46.15971515768057,
      "grad_norm": 37.23524856567383,
      "learning_rate": 3.840284842319431e-06,
      "loss": 1.4872,
      "step": 90750
    },
    {
      "epoch": 46.1648016276704,
      "grad_norm": 49.97307205200195,
      "learning_rate": 3.835198372329603e-06,
      "loss": 1.5402,
      "step": 90760
    },
    {
      "epoch": 46.16988809766023,
      "grad_norm": 35.2523193359375,
      "learning_rate": 3.830111902339776e-06,
      "loss": 1.5727,
      "step": 90770
    },
    {
      "epoch": 46.174974567650054,
      "grad_norm": 41.02824783325195,
      "learning_rate": 3.825025432349949e-06,
      "loss": 1.5174,
      "step": 90780
    },
    {
      "epoch": 46.18006103763988,
      "grad_norm": 36.09246063232422,
      "learning_rate": 3.819938962360122e-06,
      "loss": 1.5365,
      "step": 90790
    },
    {
      "epoch": 46.18514750762971,
      "grad_norm": 40.93178176879883,
      "learning_rate": 3.814852492370295e-06,
      "loss": 1.5399,
      "step": 90800
    },
    {
      "epoch": 46.190233977619535,
      "grad_norm": 45.773494720458984,
      "learning_rate": 3.8097660223804683e-06,
      "loss": 1.536,
      "step": 90810
    },
    {
      "epoch": 46.19532044760936,
      "grad_norm": 40.03303909301758,
      "learning_rate": 3.8046795523906407e-06,
      "loss": 1.561,
      "step": 90820
    },
    {
      "epoch": 46.20040691759919,
      "grad_norm": 44.48802185058594,
      "learning_rate": 3.799593082400814e-06,
      "loss": 1.5575,
      "step": 90830
    },
    {
      "epoch": 46.205493387589016,
      "grad_norm": 41.7793083190918,
      "learning_rate": 3.794506612410987e-06,
      "loss": 1.5216,
      "step": 90840
    },
    {
      "epoch": 46.21057985757884,
      "grad_norm": 47.5677490234375,
      "learning_rate": 3.78942014242116e-06,
      "loss": 1.5369,
      "step": 90850
    },
    {
      "epoch": 46.21566632756867,
      "grad_norm": 52.2459602355957,
      "learning_rate": 3.784333672431333e-06,
      "loss": 1.5301,
      "step": 90860
    },
    {
      "epoch": 46.2207527975585,
      "grad_norm": 44.35506820678711,
      "learning_rate": 3.7792472024415054e-06,
      "loss": 1.498,
      "step": 90870
    },
    {
      "epoch": 46.225839267548324,
      "grad_norm": 35.451210021972656,
      "learning_rate": 3.7741607324516787e-06,
      "loss": 1.5234,
      "step": 90880
    },
    {
      "epoch": 46.23092573753815,
      "grad_norm": 41.254310607910156,
      "learning_rate": 3.769074262461852e-06,
      "loss": 1.5786,
      "step": 90890
    },
    {
      "epoch": 46.23601220752798,
      "grad_norm": 49.15362548828125,
      "learning_rate": 3.763987792472025e-06,
      "loss": 1.5663,
      "step": 90900
    },
    {
      "epoch": 46.241098677517805,
      "grad_norm": 53.23158264160156,
      "learning_rate": 3.7589013224821973e-06,
      "loss": 1.5419,
      "step": 90910
    },
    {
      "epoch": 46.24618514750763,
      "grad_norm": 38.0169563293457,
      "learning_rate": 3.7538148524923706e-06,
      "loss": 1.5399,
      "step": 90920
    },
    {
      "epoch": 46.25127161749746,
      "grad_norm": 43.13860321044922,
      "learning_rate": 3.7487283825025434e-06,
      "loss": 1.5498,
      "step": 90930
    },
    {
      "epoch": 46.256358087487286,
      "grad_norm": 54.28709030151367,
      "learning_rate": 3.7436419125127167e-06,
      "loss": 1.4587,
      "step": 90940
    },
    {
      "epoch": 46.26144455747711,
      "grad_norm": 41.49247360229492,
      "learning_rate": 3.738555442522889e-06,
      "loss": 1.5007,
      "step": 90950
    },
    {
      "epoch": 46.26653102746694,
      "grad_norm": 52.61604309082031,
      "learning_rate": 3.733468972533062e-06,
      "loss": 1.5929,
      "step": 90960
    },
    {
      "epoch": 46.271617497456766,
      "grad_norm": 37.332889556884766,
      "learning_rate": 3.7283825025432353e-06,
      "loss": 1.6476,
      "step": 90970
    },
    {
      "epoch": 46.27670396744659,
      "grad_norm": 37.77180480957031,
      "learning_rate": 3.723296032553408e-06,
      "loss": 1.5456,
      "step": 90980
    },
    {
      "epoch": 46.28179043743642,
      "grad_norm": 41.368080139160156,
      "learning_rate": 3.7182095625635806e-06,
      "loss": 1.4706,
      "step": 90990
    },
    {
      "epoch": 46.28687690742625,
      "grad_norm": 35.794857025146484,
      "learning_rate": 3.713123092573754e-06,
      "loss": 1.5393,
      "step": 91000
    },
    {
      "epoch": 46.291963377416074,
      "grad_norm": 38.77851104736328,
      "learning_rate": 3.708036622583927e-06,
      "loss": 1.5131,
      "step": 91010
    },
    {
      "epoch": 46.2970498474059,
      "grad_norm": 47.93614196777344,
      "learning_rate": 3.7029501525941e-06,
      "loss": 1.4939,
      "step": 91020
    },
    {
      "epoch": 46.30213631739573,
      "grad_norm": 49.197479248046875,
      "learning_rate": 3.6978636826042733e-06,
      "loss": 1.4869,
      "step": 91030
    },
    {
      "epoch": 46.307222787385555,
      "grad_norm": 32.53594207763672,
      "learning_rate": 3.6927772126144457e-06,
      "loss": 1.5023,
      "step": 91040
    },
    {
      "epoch": 46.31230925737538,
      "grad_norm": 42.353511810302734,
      "learning_rate": 3.6876907426246186e-06,
      "loss": 1.5313,
      "step": 91050
    },
    {
      "epoch": 46.31739572736521,
      "grad_norm": 52.54743576049805,
      "learning_rate": 3.682604272634792e-06,
      "loss": 1.4908,
      "step": 91060
    },
    {
      "epoch": 46.322482197355036,
      "grad_norm": 38.54533004760742,
      "learning_rate": 3.6775178026449647e-06,
      "loss": 1.5035,
      "step": 91070
    },
    {
      "epoch": 46.32756866734486,
      "grad_norm": 43.974422454833984,
      "learning_rate": 3.672431332655137e-06,
      "loss": 1.5724,
      "step": 91080
    },
    {
      "epoch": 46.33265513733469,
      "grad_norm": 34.324615478515625,
      "learning_rate": 3.6673448626653104e-06,
      "loss": 1.5152,
      "step": 91090
    },
    {
      "epoch": 46.33774160732452,
      "grad_norm": 43.609310150146484,
      "learning_rate": 3.6622583926754833e-06,
      "loss": 1.5062,
      "step": 91100
    },
    {
      "epoch": 46.342828077314344,
      "grad_norm": 43.05369186401367,
      "learning_rate": 3.6571719226856566e-06,
      "loss": 1.5648,
      "step": 91110
    },
    {
      "epoch": 46.34791454730417,
      "grad_norm": 29.86646270751953,
      "learning_rate": 3.652085452695829e-06,
      "loss": 1.5084,
      "step": 91120
    },
    {
      "epoch": 46.353001017294,
      "grad_norm": 42.87235641479492,
      "learning_rate": 3.6469989827060023e-06,
      "loss": 1.6104,
      "step": 91130
    },
    {
      "epoch": 46.358087487283825,
      "grad_norm": 44.706512451171875,
      "learning_rate": 3.641912512716175e-06,
      "loss": 1.5948,
      "step": 91140
    },
    {
      "epoch": 46.36317395727365,
      "grad_norm": 42.35037612915039,
      "learning_rate": 3.6368260427263484e-06,
      "loss": 1.5202,
      "step": 91150
    },
    {
      "epoch": 46.36826042726348,
      "grad_norm": 38.80778884887695,
      "learning_rate": 3.631739572736521e-06,
      "loss": 1.4962,
      "step": 91160
    },
    {
      "epoch": 46.373346897253306,
      "grad_norm": 31.548219680786133,
      "learning_rate": 3.6266531027466937e-06,
      "loss": 1.507,
      "step": 91170
    },
    {
      "epoch": 46.37843336724313,
      "grad_norm": 40.80213165283203,
      "learning_rate": 3.621566632756867e-06,
      "loss": 1.448,
      "step": 91180
    },
    {
      "epoch": 46.38351983723296,
      "grad_norm": 49.06124496459961,
      "learning_rate": 3.61648016276704e-06,
      "loss": 1.4819,
      "step": 91190
    },
    {
      "epoch": 46.38860630722279,
      "grad_norm": 43.75441360473633,
      "learning_rate": 3.611393692777213e-06,
      "loss": 1.5268,
      "step": 91200
    },
    {
      "epoch": 46.393692777212614,
      "grad_norm": 36.97951889038086,
      "learning_rate": 3.6063072227873856e-06,
      "loss": 1.5462,
      "step": 91210
    },
    {
      "epoch": 46.39877924720244,
      "grad_norm": 34.680599212646484,
      "learning_rate": 3.6012207527975584e-06,
      "loss": 1.5441,
      "step": 91220
    },
    {
      "epoch": 46.40386571719227,
      "grad_norm": 40.891292572021484,
      "learning_rate": 3.5961342828077317e-06,
      "loss": 1.4769,
      "step": 91230
    },
    {
      "epoch": 46.408952187182095,
      "grad_norm": 47.20964431762695,
      "learning_rate": 3.591047812817905e-06,
      "loss": 1.6733,
      "step": 91240
    },
    {
      "epoch": 46.41403865717192,
      "grad_norm": 40.945980072021484,
      "learning_rate": 3.5859613428280774e-06,
      "loss": 1.5021,
      "step": 91250
    },
    {
      "epoch": 46.41912512716175,
      "grad_norm": 37.72732162475586,
      "learning_rate": 3.5808748728382503e-06,
      "loss": 1.4856,
      "step": 91260
    },
    {
      "epoch": 46.424211597151576,
      "grad_norm": 32.534088134765625,
      "learning_rate": 3.5757884028484236e-06,
      "loss": 1.4858,
      "step": 91270
    },
    {
      "epoch": 46.4292980671414,
      "grad_norm": 35.328590393066406,
      "learning_rate": 3.5707019328585964e-06,
      "loss": 1.6171,
      "step": 91280
    },
    {
      "epoch": 46.43438453713123,
      "grad_norm": 38.48192596435547,
      "learning_rate": 3.565615462868769e-06,
      "loss": 1.4934,
      "step": 91290
    },
    {
      "epoch": 46.43947100712106,
      "grad_norm": 40.91389083862305,
      "learning_rate": 3.560528992878942e-06,
      "loss": 1.5155,
      "step": 91300
    },
    {
      "epoch": 46.444557477110884,
      "grad_norm": 39.34583282470703,
      "learning_rate": 3.555442522889115e-06,
      "loss": 1.4897,
      "step": 91310
    },
    {
      "epoch": 46.44964394710071,
      "grad_norm": 36.8924674987793,
      "learning_rate": 3.5503560528992883e-06,
      "loss": 1.523,
      "step": 91320
    },
    {
      "epoch": 46.45473041709054,
      "grad_norm": 42.89107131958008,
      "learning_rate": 3.5452695829094616e-06,
      "loss": 1.558,
      "step": 91330
    },
    {
      "epoch": 46.459816887080365,
      "grad_norm": 41.40670394897461,
      "learning_rate": 3.5401831129196336e-06,
      "loss": 1.4879,
      "step": 91340
    },
    {
      "epoch": 46.46490335707019,
      "grad_norm": 36.287452697753906,
      "learning_rate": 3.535096642929807e-06,
      "loss": 1.4641,
      "step": 91350
    },
    {
      "epoch": 46.46998982706002,
      "grad_norm": 39.05718994140625,
      "learning_rate": 3.53001017293998e-06,
      "loss": 1.5623,
      "step": 91360
    },
    {
      "epoch": 46.475076297049846,
      "grad_norm": 36.808223724365234,
      "learning_rate": 3.524923702950153e-06,
      "loss": 1.6158,
      "step": 91370
    },
    {
      "epoch": 46.48016276703967,
      "grad_norm": 39.33447265625,
      "learning_rate": 3.5198372329603254e-06,
      "loss": 1.518,
      "step": 91380
    },
    {
      "epoch": 46.4852492370295,
      "grad_norm": 45.73912811279297,
      "learning_rate": 3.5147507629704987e-06,
      "loss": 1.5773,
      "step": 91390
    },
    {
      "epoch": 46.49033570701933,
      "grad_norm": 44.56229019165039,
      "learning_rate": 3.5096642929806716e-06,
      "loss": 1.5299,
      "step": 91400
    },
    {
      "epoch": 46.495422177009154,
      "grad_norm": 46.555477142333984,
      "learning_rate": 3.504577822990845e-06,
      "loss": 1.5071,
      "step": 91410
    },
    {
      "epoch": 46.50050864699898,
      "grad_norm": 40.0085334777832,
      "learning_rate": 3.4994913530010173e-06,
      "loss": 1.4652,
      "step": 91420
    },
    {
      "epoch": 46.50559511698881,
      "grad_norm": 41.27342224121094,
      "learning_rate": 3.49440488301119e-06,
      "loss": 1.5544,
      "step": 91430
    },
    {
      "epoch": 46.510681586978635,
      "grad_norm": 55.14092254638672,
      "learning_rate": 3.4893184130213634e-06,
      "loss": 1.4902,
      "step": 91440
    },
    {
      "epoch": 46.51576805696846,
      "grad_norm": 33.99159622192383,
      "learning_rate": 3.4842319430315367e-06,
      "loss": 1.5416,
      "step": 91450
    },
    {
      "epoch": 46.52085452695829,
      "grad_norm": 46.90439224243164,
      "learning_rate": 3.4791454730417087e-06,
      "loss": 1.5573,
      "step": 91460
    },
    {
      "epoch": 46.525940996948115,
      "grad_norm": 41.894752502441406,
      "learning_rate": 3.474059003051882e-06,
      "loss": 1.5279,
      "step": 91470
    },
    {
      "epoch": 46.53102746693794,
      "grad_norm": 41.82291793823242,
      "learning_rate": 3.4689725330620553e-06,
      "loss": 1.559,
      "step": 91480
    },
    {
      "epoch": 46.53611393692777,
      "grad_norm": 43.95388412475586,
      "learning_rate": 3.463886063072228e-06,
      "loss": 1.5934,
      "step": 91490
    },
    {
      "epoch": 46.541200406917596,
      "grad_norm": 50.794578552246094,
      "learning_rate": 3.4587995930824014e-06,
      "loss": 1.4972,
      "step": 91500
    },
    {
      "epoch": 46.54628687690742,
      "grad_norm": 40.56542205810547,
      "learning_rate": 3.453713123092574e-06,
      "loss": 1.5177,
      "step": 91510
    },
    {
      "epoch": 46.55137334689725,
      "grad_norm": 63.602272033691406,
      "learning_rate": 3.4486266531027467e-06,
      "loss": 1.5213,
      "step": 91520
    },
    {
      "epoch": 46.55645981688708,
      "grad_norm": 41.093467712402344,
      "learning_rate": 3.44354018311292e-06,
      "loss": 1.523,
      "step": 91530
    },
    {
      "epoch": 46.561546286876904,
      "grad_norm": 34.81897735595703,
      "learning_rate": 3.438453713123093e-06,
      "loss": 1.5689,
      "step": 91540
    },
    {
      "epoch": 46.56663275686673,
      "grad_norm": 45.26014709472656,
      "learning_rate": 3.4333672431332653e-06,
      "loss": 1.5226,
      "step": 91550
    },
    {
      "epoch": 46.57171922685656,
      "grad_norm": 39.069488525390625,
      "learning_rate": 3.4282807731434386e-06,
      "loss": 1.5349,
      "step": 91560
    },
    {
      "epoch": 46.576805696846385,
      "grad_norm": 41.684783935546875,
      "learning_rate": 3.423194303153612e-06,
      "loss": 1.574,
      "step": 91570
    },
    {
      "epoch": 46.58189216683621,
      "grad_norm": 34.02196502685547,
      "learning_rate": 3.4181078331637847e-06,
      "loss": 1.5319,
      "step": 91580
    },
    {
      "epoch": 46.58697863682604,
      "grad_norm": 43.25517272949219,
      "learning_rate": 3.413021363173957e-06,
      "loss": 1.4982,
      "step": 91590
    },
    {
      "epoch": 46.592065106815866,
      "grad_norm": 46.51438522338867,
      "learning_rate": 3.4079348931841304e-06,
      "loss": 1.5028,
      "step": 91600
    },
    {
      "epoch": 46.5971515768057,
      "grad_norm": 39.303279876708984,
      "learning_rate": 3.4028484231943033e-06,
      "loss": 1.4868,
      "step": 91610
    },
    {
      "epoch": 46.60223804679553,
      "grad_norm": 36.367549896240234,
      "learning_rate": 3.3977619532044766e-06,
      "loss": 1.4125,
      "step": 91620
    },
    {
      "epoch": 46.607324516785354,
      "grad_norm": 47.707984924316406,
      "learning_rate": 3.392675483214649e-06,
      "loss": 1.5469,
      "step": 91630
    },
    {
      "epoch": 46.61241098677518,
      "grad_norm": 39.677757263183594,
      "learning_rate": 3.387589013224822e-06,
      "loss": 1.4979,
      "step": 91640
    },
    {
      "epoch": 46.61749745676501,
      "grad_norm": 42.87449264526367,
      "learning_rate": 3.382502543234995e-06,
      "loss": 1.5925,
      "step": 91650
    },
    {
      "epoch": 46.622583926754835,
      "grad_norm": 49.4433708190918,
      "learning_rate": 3.377416073245168e-06,
      "loss": 1.6095,
      "step": 91660
    },
    {
      "epoch": 46.62767039674466,
      "grad_norm": 38.702213287353516,
      "learning_rate": 3.3723296032553413e-06,
      "loss": 1.5148,
      "step": 91670
    },
    {
      "epoch": 46.63275686673449,
      "grad_norm": 42.06110382080078,
      "learning_rate": 3.3672431332655137e-06,
      "loss": 1.5719,
      "step": 91680
    },
    {
      "epoch": 46.637843336724316,
      "grad_norm": 39.42973327636719,
      "learning_rate": 3.362156663275687e-06,
      "loss": 1.4771,
      "step": 91690
    },
    {
      "epoch": 46.64292980671414,
      "grad_norm": 42.377098083496094,
      "learning_rate": 3.35707019328586e-06,
      "loss": 1.5219,
      "step": 91700
    },
    {
      "epoch": 46.64801627670397,
      "grad_norm": 42.816837310791016,
      "learning_rate": 3.351983723296033e-06,
      "loss": 1.4627,
      "step": 91710
    },
    {
      "epoch": 46.6531027466938,
      "grad_norm": 44.27884292602539,
      "learning_rate": 3.3468972533062056e-06,
      "loss": 1.5432,
      "step": 91720
    },
    {
      "epoch": 46.658189216683624,
      "grad_norm": 43.158260345458984,
      "learning_rate": 3.3418107833163784e-06,
      "loss": 1.4977,
      "step": 91730
    },
    {
      "epoch": 46.66327568667345,
      "grad_norm": 42.593814849853516,
      "learning_rate": 3.3367243133265517e-06,
      "loss": 1.5802,
      "step": 91740
    },
    {
      "epoch": 46.66836215666328,
      "grad_norm": 49.96186447143555,
      "learning_rate": 3.3316378433367246e-06,
      "loss": 1.6066,
      "step": 91750
    },
    {
      "epoch": 46.673448626653105,
      "grad_norm": 41.57737350463867,
      "learning_rate": 3.326551373346897e-06,
      "loss": 1.5307,
      "step": 91760
    },
    {
      "epoch": 46.67853509664293,
      "grad_norm": 42.592159271240234,
      "learning_rate": 3.3214649033570703e-06,
      "loss": 1.456,
      "step": 91770
    },
    {
      "epoch": 46.68362156663276,
      "grad_norm": 35.23035430908203,
      "learning_rate": 3.316378433367243e-06,
      "loss": 1.5379,
      "step": 91780
    },
    {
      "epoch": 46.688708036622586,
      "grad_norm": 47.23276138305664,
      "learning_rate": 3.3112919633774164e-06,
      "loss": 1.5288,
      "step": 91790
    },
    {
      "epoch": 46.69379450661241,
      "grad_norm": 42.81565856933594,
      "learning_rate": 3.306205493387589e-06,
      "loss": 1.4422,
      "step": 91800
    },
    {
      "epoch": 46.69888097660224,
      "grad_norm": 48.1595344543457,
      "learning_rate": 3.301119023397762e-06,
      "loss": 1.4433,
      "step": 91810
    },
    {
      "epoch": 46.70396744659207,
      "grad_norm": 38.96350860595703,
      "learning_rate": 3.296032553407935e-06,
      "loss": 1.4358,
      "step": 91820
    },
    {
      "epoch": 46.709053916581894,
      "grad_norm": 45.33610153198242,
      "learning_rate": 3.2909460834181083e-06,
      "loss": 1.5317,
      "step": 91830
    },
    {
      "epoch": 46.71414038657172,
      "grad_norm": 45.72882843017578,
      "learning_rate": 3.285859613428281e-06,
      "loss": 1.5425,
      "step": 91840
    },
    {
      "epoch": 46.71922685656155,
      "grad_norm": 32.957340240478516,
      "learning_rate": 3.2807731434384536e-06,
      "loss": 1.5091,
      "step": 91850
    },
    {
      "epoch": 46.724313326551375,
      "grad_norm": 45.368839263916016,
      "learning_rate": 3.275686673448627e-06,
      "loss": 1.5059,
      "step": 91860
    },
    {
      "epoch": 46.7293997965412,
      "grad_norm": 36.573787689208984,
      "learning_rate": 3.2706002034587997e-06,
      "loss": 1.5503,
      "step": 91870
    },
    {
      "epoch": 46.73448626653103,
      "grad_norm": 51.0583381652832,
      "learning_rate": 3.265513733468973e-06,
      "loss": 1.4494,
      "step": 91880
    },
    {
      "epoch": 46.739572736520856,
      "grad_norm": 38.3484992980957,
      "learning_rate": 3.2604272634791454e-06,
      "loss": 1.5351,
      "step": 91890
    },
    {
      "epoch": 46.74465920651068,
      "grad_norm": 39.1738395690918,
      "learning_rate": 3.2553407934893183e-06,
      "loss": 1.5853,
      "step": 91900
    },
    {
      "epoch": 46.74974567650051,
      "grad_norm": 38.37001037597656,
      "learning_rate": 3.2502543234994916e-06,
      "loss": 1.4633,
      "step": 91910
    },
    {
      "epoch": 46.75483214649034,
      "grad_norm": 46.19482421875,
      "learning_rate": 3.245167853509665e-06,
      "loss": 1.4933,
      "step": 91920
    },
    {
      "epoch": 46.759918616480164,
      "grad_norm": 36.773841857910156,
      "learning_rate": 3.2400813835198373e-06,
      "loss": 1.554,
      "step": 91930
    },
    {
      "epoch": 46.76500508646999,
      "grad_norm": 40.26961135864258,
      "learning_rate": 3.23499491353001e-06,
      "loss": 1.5572,
      "step": 91940
    },
    {
      "epoch": 46.77009155645982,
      "grad_norm": 41.25335693359375,
      "learning_rate": 3.2299084435401834e-06,
      "loss": 1.4809,
      "step": 91950
    },
    {
      "epoch": 46.775178026449645,
      "grad_norm": 44.63240051269531,
      "learning_rate": 3.2248219735503563e-06,
      "loss": 1.5844,
      "step": 91960
    },
    {
      "epoch": 46.78026449643947,
      "grad_norm": 52.298831939697266,
      "learning_rate": 3.2197355035605287e-06,
      "loss": 1.4563,
      "step": 91970
    },
    {
      "epoch": 46.7853509664293,
      "grad_norm": 43.365257263183594,
      "learning_rate": 3.214649033570702e-06,
      "loss": 1.5281,
      "step": 91980
    },
    {
      "epoch": 46.790437436419126,
      "grad_norm": 37.96268844604492,
      "learning_rate": 3.209562563580875e-06,
      "loss": 1.5365,
      "step": 91990
    },
    {
      "epoch": 46.79552390640895,
      "grad_norm": 48.99778366088867,
      "learning_rate": 3.204476093591048e-06,
      "loss": 1.4347,
      "step": 92000
    },
    {
      "epoch": 46.80061037639878,
      "grad_norm": 40.44953536987305,
      "learning_rate": 3.1993896236012214e-06,
      "loss": 1.4883,
      "step": 92010
    },
    {
      "epoch": 46.80569684638861,
      "grad_norm": 38.612884521484375,
      "learning_rate": 3.1943031536113934e-06,
      "loss": 1.5118,
      "step": 92020
    },
    {
      "epoch": 46.81078331637843,
      "grad_norm": 42.42335891723633,
      "learning_rate": 3.1892166836215667e-06,
      "loss": 1.4743,
      "step": 92030
    },
    {
      "epoch": 46.81586978636826,
      "grad_norm": 33.62421798706055,
      "learning_rate": 3.18413021363174e-06,
      "loss": 1.5269,
      "step": 92040
    },
    {
      "epoch": 46.82095625635809,
      "grad_norm": 50.763187408447266,
      "learning_rate": 3.179043743641913e-06,
      "loss": 1.6103,
      "step": 92050
    },
    {
      "epoch": 46.826042726347914,
      "grad_norm": 40.522674560546875,
      "learning_rate": 3.1739572736520853e-06,
      "loss": 1.5855,
      "step": 92060
    },
    {
      "epoch": 46.83112919633774,
      "grad_norm": 39.28422546386719,
      "learning_rate": 3.1688708036622586e-06,
      "loss": 1.4806,
      "step": 92070
    },
    {
      "epoch": 46.83621566632757,
      "grad_norm": 35.095394134521484,
      "learning_rate": 3.1637843336724314e-06,
      "loss": 1.4927,
      "step": 92080
    },
    {
      "epoch": 46.841302136317395,
      "grad_norm": 45.32880783081055,
      "learning_rate": 3.1586978636826047e-06,
      "loss": 1.5618,
      "step": 92090
    },
    {
      "epoch": 46.84638860630722,
      "grad_norm": 38.97208023071289,
      "learning_rate": 3.153611393692777e-06,
      "loss": 1.4884,
      "step": 92100
    },
    {
      "epoch": 46.85147507629705,
      "grad_norm": 46.65908432006836,
      "learning_rate": 3.14852492370295e-06,
      "loss": 1.5684,
      "step": 92110
    },
    {
      "epoch": 46.856561546286876,
      "grad_norm": 50.95444107055664,
      "learning_rate": 3.1434384537131233e-06,
      "loss": 1.4813,
      "step": 92120
    },
    {
      "epoch": 46.8616480162767,
      "grad_norm": 47.13271713256836,
      "learning_rate": 3.1383519837232966e-06,
      "loss": 1.551,
      "step": 92130
    },
    {
      "epoch": 46.86673448626653,
      "grad_norm": 43.226375579833984,
      "learning_rate": 3.1332655137334694e-06,
      "loss": 1.4451,
      "step": 92140
    },
    {
      "epoch": 46.87182095625636,
      "grad_norm": 31.701467514038086,
      "learning_rate": 3.128179043743642e-06,
      "loss": 1.5076,
      "step": 92150
    },
    {
      "epoch": 46.876907426246184,
      "grad_norm": 44.737083435058594,
      "learning_rate": 3.123092573753815e-06,
      "loss": 1.4791,
      "step": 92160
    },
    {
      "epoch": 46.88199389623601,
      "grad_norm": 37.570064544677734,
      "learning_rate": 3.118006103763988e-06,
      "loss": 1.6359,
      "step": 92170
    },
    {
      "epoch": 46.88708036622584,
      "grad_norm": 35.91264724731445,
      "learning_rate": 3.112919633774161e-06,
      "loss": 1.6081,
      "step": 92180
    },
    {
      "epoch": 46.892166836215665,
      "grad_norm": 63.53915023803711,
      "learning_rate": 3.107833163784334e-06,
      "loss": 1.4628,
      "step": 92190
    },
    {
      "epoch": 46.89725330620549,
      "grad_norm": 41.161075592041016,
      "learning_rate": 3.1027466937945066e-06,
      "loss": 1.5029,
      "step": 92200
    },
    {
      "epoch": 46.90233977619532,
      "grad_norm": 37.735313415527344,
      "learning_rate": 3.09766022380468e-06,
      "loss": 1.5871,
      "step": 92210
    },
    {
      "epoch": 46.907426246185146,
      "grad_norm": 41.28170394897461,
      "learning_rate": 3.0925737538148527e-06,
      "loss": 1.615,
      "step": 92220
    },
    {
      "epoch": 46.91251271617497,
      "grad_norm": 46.03076934814453,
      "learning_rate": 3.0874872838250256e-06,
      "loss": 1.6305,
      "step": 92230
    },
    {
      "epoch": 46.9175991861648,
      "grad_norm": 33.71485137939453,
      "learning_rate": 3.0824008138351984e-06,
      "loss": 1.4336,
      "step": 92240
    },
    {
      "epoch": 46.92268565615463,
      "grad_norm": 44.138431549072266,
      "learning_rate": 3.0773143438453717e-06,
      "loss": 1.4888,
      "step": 92250
    },
    {
      "epoch": 46.927772126144454,
      "grad_norm": 39.45410919189453,
      "learning_rate": 3.072227873855544e-06,
      "loss": 1.5688,
      "step": 92260
    },
    {
      "epoch": 46.93285859613428,
      "grad_norm": 36.66324234008789,
      "learning_rate": 3.0671414038657174e-06,
      "loss": 1.5335,
      "step": 92270
    },
    {
      "epoch": 46.93794506612411,
      "grad_norm": 32.84357452392578,
      "learning_rate": 3.0620549338758903e-06,
      "loss": 1.557,
      "step": 92280
    },
    {
      "epoch": 46.943031536113935,
      "grad_norm": 41.79903030395508,
      "learning_rate": 3.056968463886063e-06,
      "loss": 1.4124,
      "step": 92290
    },
    {
      "epoch": 46.94811800610376,
      "grad_norm": 39.38301467895508,
      "learning_rate": 3.0518819938962364e-06,
      "loss": 1.5419,
      "step": 92300
    },
    {
      "epoch": 46.95320447609359,
      "grad_norm": 39.53817367553711,
      "learning_rate": 3.0467955239064093e-06,
      "loss": 1.462,
      "step": 92310
    },
    {
      "epoch": 46.958290946083416,
      "grad_norm": 42.864173889160156,
      "learning_rate": 3.041709053916582e-06,
      "loss": 1.5838,
      "step": 92320
    },
    {
      "epoch": 46.96337741607324,
      "grad_norm": 37.24028778076172,
      "learning_rate": 3.036622583926755e-06,
      "loss": 1.5327,
      "step": 92330
    },
    {
      "epoch": 46.96846388606307,
      "grad_norm": 51.777469635009766,
      "learning_rate": 3.031536113936928e-06,
      "loss": 1.589,
      "step": 92340
    },
    {
      "epoch": 46.9735503560529,
      "grad_norm": 46.400596618652344,
      "learning_rate": 3.0264496439471007e-06,
      "loss": 1.5446,
      "step": 92350
    },
    {
      "epoch": 46.978636826042724,
      "grad_norm": 42.655128479003906,
      "learning_rate": 3.021363173957274e-06,
      "loss": 1.5332,
      "step": 92360
    },
    {
      "epoch": 46.98372329603255,
      "grad_norm": 40.77027893066406,
      "learning_rate": 3.016276703967447e-06,
      "loss": 1.4665,
      "step": 92370
    },
    {
      "epoch": 46.98880976602238,
      "grad_norm": 42.97386932373047,
      "learning_rate": 3.0111902339776197e-06,
      "loss": 1.5164,
      "step": 92380
    },
    {
      "epoch": 46.993896236012205,
      "grad_norm": 34.78055953979492,
      "learning_rate": 3.0061037639877926e-06,
      "loss": 1.594,
      "step": 92390
    },
    {
      "epoch": 46.99898270600203,
      "grad_norm": 35.79267883300781,
      "learning_rate": 3.0010172939979654e-06,
      "loss": 1.5093,
      "step": 92400
    },
    {
      "epoch": 47.0,
      "eval_loss": 5.109984874725342,
      "eval_runtime": 2.8255,
      "eval_samples_per_second": 982.132,
      "eval_steps_per_second": 122.811,
      "step": 92402
    },
    {
      "epoch": 47.00406917599186,
      "grad_norm": 46.9239387512207,
      "learning_rate": 2.9959308240081383e-06,
      "loss": 1.464,
      "step": 92410
    },
    {
      "epoch": 47.009155645981686,
      "grad_norm": 35.09404754638672,
      "learning_rate": 2.9908443540183116e-06,
      "loss": 1.6253,
      "step": 92420
    },
    {
      "epoch": 47.01424211597151,
      "grad_norm": 48.82675552368164,
      "learning_rate": 2.9857578840284844e-06,
      "loss": 1.6051,
      "step": 92430
    },
    {
      "epoch": 47.01932858596134,
      "grad_norm": 35.53091049194336,
      "learning_rate": 2.9806714140386573e-06,
      "loss": 1.4819,
      "step": 92440
    },
    {
      "epoch": 47.02441505595117,
      "grad_norm": 37.209754943847656,
      "learning_rate": 2.97558494404883e-06,
      "loss": 1.5862,
      "step": 92450
    },
    {
      "epoch": 47.029501525940994,
      "grad_norm": 45.33391571044922,
      "learning_rate": 2.970498474059003e-06,
      "loss": 1.4864,
      "step": 92460
    },
    {
      "epoch": 47.03458799593082,
      "grad_norm": 42.727821350097656,
      "learning_rate": 2.9654120040691763e-06,
      "loss": 1.4758,
      "step": 92470
    },
    {
      "epoch": 47.03967446592065,
      "grad_norm": 49.4864387512207,
      "learning_rate": 2.960325534079349e-06,
      "loss": 1.5418,
      "step": 92480
    },
    {
      "epoch": 47.044760935910475,
      "grad_norm": 50.03324508666992,
      "learning_rate": 2.955239064089522e-06,
      "loss": 1.5635,
      "step": 92490
    },
    {
      "epoch": 47.04984740590031,
      "grad_norm": 36.037654876708984,
      "learning_rate": 2.950152594099695e-06,
      "loss": 1.482,
      "step": 92500
    },
    {
      "epoch": 47.054933875890136,
      "grad_norm": 48.424110412597656,
      "learning_rate": 2.945066124109868e-06,
      "loss": 1.5021,
      "step": 92510
    },
    {
      "epoch": 47.06002034587996,
      "grad_norm": 42.371517181396484,
      "learning_rate": 2.9399796541200406e-06,
      "loss": 1.6501,
      "step": 92520
    },
    {
      "epoch": 47.06510681586979,
      "grad_norm": 40.7451057434082,
      "learning_rate": 2.934893184130214e-06,
      "loss": 1.5228,
      "step": 92530
    },
    {
      "epoch": 47.07019328585962,
      "grad_norm": 39.96413040161133,
      "learning_rate": 2.9298067141403867e-06,
      "loss": 1.5565,
      "step": 92540
    },
    {
      "epoch": 47.075279755849444,
      "grad_norm": 40.8251953125,
      "learning_rate": 2.9247202441505596e-06,
      "loss": 1.5141,
      "step": 92550
    },
    {
      "epoch": 47.08036622583927,
      "grad_norm": 42.3076171875,
      "learning_rate": 2.9196337741607324e-06,
      "loss": 1.4852,
      "step": 92560
    },
    {
      "epoch": 47.0854526958291,
      "grad_norm": 41.748924255371094,
      "learning_rate": 2.9145473041709057e-06,
      "loss": 1.4913,
      "step": 92570
    },
    {
      "epoch": 47.090539165818925,
      "grad_norm": 45.67842102050781,
      "learning_rate": 2.909460834181078e-06,
      "loss": 1.5379,
      "step": 92580
    },
    {
      "epoch": 47.09562563580875,
      "grad_norm": 38.336734771728516,
      "learning_rate": 2.9043743641912514e-06,
      "loss": 1.5343,
      "step": 92590
    },
    {
      "epoch": 47.10071210579858,
      "grad_norm": 49.915287017822266,
      "learning_rate": 2.8992878942014243e-06,
      "loss": 1.5202,
      "step": 92600
    },
    {
      "epoch": 47.105798575788405,
      "grad_norm": 46.721866607666016,
      "learning_rate": 2.894201424211597e-06,
      "loss": 1.4866,
      "step": 92610
    },
    {
      "epoch": 47.11088504577823,
      "grad_norm": 40.29541778564453,
      "learning_rate": 2.88911495422177e-06,
      "loss": 1.4769,
      "step": 92620
    },
    {
      "epoch": 47.11597151576806,
      "grad_norm": 44.77888488769531,
      "learning_rate": 2.8840284842319433e-06,
      "loss": 1.5845,
      "step": 92630
    },
    {
      "epoch": 47.121057985757886,
      "grad_norm": 41.35541534423828,
      "learning_rate": 2.878942014242116e-06,
      "loss": 1.4977,
      "step": 92640
    },
    {
      "epoch": 47.12614445574771,
      "grad_norm": 33.44795227050781,
      "learning_rate": 2.873855544252289e-06,
      "loss": 1.3952,
      "step": 92650
    },
    {
      "epoch": 47.13123092573754,
      "grad_norm": 41.74268341064453,
      "learning_rate": 2.8687690742624623e-06,
      "loss": 1.5353,
      "step": 92660
    },
    {
      "epoch": 47.13631739572737,
      "grad_norm": 47.235740661621094,
      "learning_rate": 2.8636826042726347e-06,
      "loss": 1.5118,
      "step": 92670
    },
    {
      "epoch": 47.141403865717194,
      "grad_norm": 48.87807083129883,
      "learning_rate": 2.858596134282808e-06,
      "loss": 1.5355,
      "step": 92680
    },
    {
      "epoch": 47.14649033570702,
      "grad_norm": 38.850425720214844,
      "learning_rate": 2.853509664292981e-06,
      "loss": 1.4984,
      "step": 92690
    },
    {
      "epoch": 47.15157680569685,
      "grad_norm": 37.650535583496094,
      "learning_rate": 2.8484231943031537e-06,
      "loss": 1.4714,
      "step": 92700
    },
    {
      "epoch": 47.156663275686675,
      "grad_norm": 37.387508392333984,
      "learning_rate": 2.8433367243133266e-06,
      "loss": 1.5528,
      "step": 92710
    },
    {
      "epoch": 47.1617497456765,
      "grad_norm": 33.13053512573242,
      "learning_rate": 2.8382502543235e-06,
      "loss": 1.495,
      "step": 92720
    },
    {
      "epoch": 47.16683621566633,
      "grad_norm": 46.765167236328125,
      "learning_rate": 2.8331637843336723e-06,
      "loss": 1.4701,
      "step": 92730
    },
    {
      "epoch": 47.171922685656156,
      "grad_norm": 41.9502067565918,
      "learning_rate": 2.8280773143438456e-06,
      "loss": 1.5464,
      "step": 92740
    },
    {
      "epoch": 47.17700915564598,
      "grad_norm": 37.555747985839844,
      "learning_rate": 2.8229908443540184e-06,
      "loss": 1.5368,
      "step": 92750
    },
    {
      "epoch": 47.18209562563581,
      "grad_norm": 45.6114616394043,
      "learning_rate": 2.8179043743641913e-06,
      "loss": 1.5383,
      "step": 92760
    },
    {
      "epoch": 47.18718209562564,
      "grad_norm": 45.92079162597656,
      "learning_rate": 2.812817904374364e-06,
      "loss": 1.5191,
      "step": 92770
    },
    {
      "epoch": 47.192268565615464,
      "grad_norm": 51.6347541809082,
      "learning_rate": 2.8077314343845374e-06,
      "loss": 1.5927,
      "step": 92780
    },
    {
      "epoch": 47.19735503560529,
      "grad_norm": 36.7956657409668,
      "learning_rate": 2.8026449643947103e-06,
      "loss": 1.531,
      "step": 92790
    },
    {
      "epoch": 47.20244150559512,
      "grad_norm": 51.75559616088867,
      "learning_rate": 2.797558494404883e-06,
      "loss": 1.5336,
      "step": 92800
    },
    {
      "epoch": 47.207527975584945,
      "grad_norm": 43.31183624267578,
      "learning_rate": 2.7924720244150564e-06,
      "loss": 1.6281,
      "step": 92810
    },
    {
      "epoch": 47.21261444557477,
      "grad_norm": 38.41659927368164,
      "learning_rate": 2.787385554425229e-06,
      "loss": 1.6103,
      "step": 92820
    },
    {
      "epoch": 47.2177009155646,
      "grad_norm": 38.685142517089844,
      "learning_rate": 2.782299084435402e-06,
      "loss": 1.638,
      "step": 92830
    },
    {
      "epoch": 47.222787385554426,
      "grad_norm": 48.35789489746094,
      "learning_rate": 2.777212614445575e-06,
      "loss": 1.5106,
      "step": 92840
    },
    {
      "epoch": 47.22787385554425,
      "grad_norm": 38.85343933105469,
      "learning_rate": 2.772126144455748e-06,
      "loss": 1.5878,
      "step": 92850
    },
    {
      "epoch": 47.23296032553408,
      "grad_norm": 44.32841110229492,
      "learning_rate": 2.7670396744659207e-06,
      "loss": 1.4441,
      "step": 92860
    },
    {
      "epoch": 47.23804679552391,
      "grad_norm": 35.444557189941406,
      "learning_rate": 2.761953204476094e-06,
      "loss": 1.4654,
      "step": 92870
    },
    {
      "epoch": 47.243133265513734,
      "grad_norm": 34.59727096557617,
      "learning_rate": 2.7568667344862664e-06,
      "loss": 1.4866,
      "step": 92880
    },
    {
      "epoch": 47.24821973550356,
      "grad_norm": 37.10762023925781,
      "learning_rate": 2.7517802644964397e-06,
      "loss": 1.5653,
      "step": 92890
    },
    {
      "epoch": 47.25330620549339,
      "grad_norm": 42.47682571411133,
      "learning_rate": 2.7466937945066126e-06,
      "loss": 1.5067,
      "step": 92900
    },
    {
      "epoch": 47.258392675483215,
      "grad_norm": 41.160770416259766,
      "learning_rate": 2.7416073245167854e-06,
      "loss": 1.483,
      "step": 92910
    },
    {
      "epoch": 47.26347914547304,
      "grad_norm": 37.522579193115234,
      "learning_rate": 2.7365208545269583e-06,
      "loss": 1.4982,
      "step": 92920
    },
    {
      "epoch": 47.26856561546287,
      "grad_norm": 49.804874420166016,
      "learning_rate": 2.7314343845371316e-06,
      "loss": 1.5766,
      "step": 92930
    },
    {
      "epoch": 47.273652085452696,
      "grad_norm": 42.85721206665039,
      "learning_rate": 2.726347914547304e-06,
      "loss": 1.5085,
      "step": 92940
    },
    {
      "epoch": 47.27873855544252,
      "grad_norm": 38.75701904296875,
      "learning_rate": 2.7212614445574773e-06,
      "loss": 1.4925,
      "step": 92950
    },
    {
      "epoch": 47.28382502543235,
      "grad_norm": 38.30017852783203,
      "learning_rate": 2.71617497456765e-06,
      "loss": 1.5594,
      "step": 92960
    },
    {
      "epoch": 47.28891149542218,
      "grad_norm": 47.113651275634766,
      "learning_rate": 2.711088504577823e-06,
      "loss": 1.5085,
      "step": 92970
    },
    {
      "epoch": 47.293997965412004,
      "grad_norm": 33.47508239746094,
      "learning_rate": 2.7060020345879963e-06,
      "loss": 1.5873,
      "step": 92980
    },
    {
      "epoch": 47.29908443540183,
      "grad_norm": 50.99949645996094,
      "learning_rate": 2.700915564598169e-06,
      "loss": 1.4845,
      "step": 92990
    },
    {
      "epoch": 47.30417090539166,
      "grad_norm": 38.304168701171875,
      "learning_rate": 2.695829094608342e-06,
      "loss": 1.5132,
      "step": 93000
    },
    {
      "epoch": 47.309257375381485,
      "grad_norm": 49.557518005371094,
      "learning_rate": 2.690742624618515e-06,
      "loss": 1.5506,
      "step": 93010
    },
    {
      "epoch": 47.31434384537131,
      "grad_norm": 42.47370910644531,
      "learning_rate": 2.6856561546286877e-06,
      "loss": 1.4858,
      "step": 93020
    },
    {
      "epoch": 47.31943031536114,
      "grad_norm": 43.177467346191406,
      "learning_rate": 2.6805696846388606e-06,
      "loss": 1.4986,
      "step": 93030
    },
    {
      "epoch": 47.324516785350966,
      "grad_norm": 34.79829406738281,
      "learning_rate": 2.675483214649034e-06,
      "loss": 1.5599,
      "step": 93040
    },
    {
      "epoch": 47.32960325534079,
      "grad_norm": 46.73136901855469,
      "learning_rate": 2.6703967446592067e-06,
      "loss": 1.5233,
      "step": 93050
    },
    {
      "epoch": 47.33468972533062,
      "grad_norm": 42.6390380859375,
      "learning_rate": 2.6653102746693796e-06,
      "loss": 1.69,
      "step": 93060
    },
    {
      "epoch": 47.33977619532045,
      "grad_norm": 35.436222076416016,
      "learning_rate": 2.6602238046795524e-06,
      "loss": 1.5065,
      "step": 93070
    },
    {
      "epoch": 47.34486266531027,
      "grad_norm": 52.52901077270508,
      "learning_rate": 2.6551373346897253e-06,
      "loss": 1.4653,
      "step": 93080
    },
    {
      "epoch": 47.3499491353001,
      "grad_norm": 31.36802864074707,
      "learning_rate": 2.650050864699898e-06,
      "loss": 1.5239,
      "step": 93090
    },
    {
      "epoch": 47.35503560528993,
      "grad_norm": 33.479835510253906,
      "learning_rate": 2.6449643947100714e-06,
      "loss": 1.4293,
      "step": 93100
    },
    {
      "epoch": 47.360122075279754,
      "grad_norm": 37.99824905395508,
      "learning_rate": 2.6398779247202443e-06,
      "loss": 1.556,
      "step": 93110
    },
    {
      "epoch": 47.36520854526958,
      "grad_norm": 37.997066497802734,
      "learning_rate": 2.634791454730417e-06,
      "loss": 1.5296,
      "step": 93120
    },
    {
      "epoch": 47.37029501525941,
      "grad_norm": 44.43752670288086,
      "learning_rate": 2.6297049847405904e-06,
      "loss": 1.4934,
      "step": 93130
    },
    {
      "epoch": 47.375381485249235,
      "grad_norm": 39.150421142578125,
      "learning_rate": 2.6246185147507633e-06,
      "loss": 1.5003,
      "step": 93140
    },
    {
      "epoch": 47.38046795523906,
      "grad_norm": 40.94590759277344,
      "learning_rate": 2.619532044760936e-06,
      "loss": 1.5313,
      "step": 93150
    },
    {
      "epoch": 47.38555442522889,
      "grad_norm": 46.896366119384766,
      "learning_rate": 2.614445574771109e-06,
      "loss": 1.4594,
      "step": 93160
    },
    {
      "epoch": 47.390640895218716,
      "grad_norm": 47.75472640991211,
      "learning_rate": 2.609359104781282e-06,
      "loss": 1.4818,
      "step": 93170
    },
    {
      "epoch": 47.39572736520854,
      "grad_norm": 45.92353820800781,
      "learning_rate": 2.6042726347914547e-06,
      "loss": 1.4881,
      "step": 93180
    },
    {
      "epoch": 47.40081383519837,
      "grad_norm": 46.41421890258789,
      "learning_rate": 2.599186164801628e-06,
      "loss": 1.5155,
      "step": 93190
    },
    {
      "epoch": 47.4059003051882,
      "grad_norm": 45.81816101074219,
      "learning_rate": 2.594099694811801e-06,
      "loss": 1.6328,
      "step": 93200
    },
    {
      "epoch": 47.410986775178024,
      "grad_norm": 42.674739837646484,
      "learning_rate": 2.5890132248219737e-06,
      "loss": 1.5908,
      "step": 93210
    },
    {
      "epoch": 47.41607324516785,
      "grad_norm": 36.54566955566406,
      "learning_rate": 2.5839267548321466e-06,
      "loss": 1.5963,
      "step": 93220
    },
    {
      "epoch": 47.42115971515768,
      "grad_norm": 41.02561950683594,
      "learning_rate": 2.5788402848423194e-06,
      "loss": 1.5081,
      "step": 93230
    },
    {
      "epoch": 47.426246185147505,
      "grad_norm": 39.430908203125,
      "learning_rate": 2.5737538148524923e-06,
      "loss": 1.5374,
      "step": 93240
    },
    {
      "epoch": 47.43133265513733,
      "grad_norm": 39.84454345703125,
      "learning_rate": 2.5686673448626656e-06,
      "loss": 1.5886,
      "step": 93250
    },
    {
      "epoch": 47.43641912512716,
      "grad_norm": 33.861427307128906,
      "learning_rate": 2.5635808748728384e-06,
      "loss": 1.4914,
      "step": 93260
    },
    {
      "epoch": 47.441505595116986,
      "grad_norm": 41.65339279174805,
      "learning_rate": 2.5584944048830113e-06,
      "loss": 1.5109,
      "step": 93270
    },
    {
      "epoch": 47.44659206510681,
      "grad_norm": 47.27334976196289,
      "learning_rate": 2.5534079348931846e-06,
      "loss": 1.4392,
      "step": 93280
    },
    {
      "epoch": 47.45167853509664,
      "grad_norm": 38.50165557861328,
      "learning_rate": 2.548321464903357e-06,
      "loss": 1.4889,
      "step": 93290
    },
    {
      "epoch": 47.45676500508647,
      "grad_norm": 40.475502014160156,
      "learning_rate": 2.5432349949135303e-06,
      "loss": 1.5437,
      "step": 93300
    },
    {
      "epoch": 47.461851475076294,
      "grad_norm": 43.3555908203125,
      "learning_rate": 2.538148524923703e-06,
      "loss": 1.5068,
      "step": 93310
    },
    {
      "epoch": 47.46693794506612,
      "grad_norm": 37.38874053955078,
      "learning_rate": 2.533062054933876e-06,
      "loss": 1.4766,
      "step": 93320
    },
    {
      "epoch": 47.47202441505595,
      "grad_norm": 39.16135787963867,
      "learning_rate": 2.527975584944049e-06,
      "loss": 1.5238,
      "step": 93330
    },
    {
      "epoch": 47.477110885045775,
      "grad_norm": 43.51511001586914,
      "learning_rate": 2.522889114954222e-06,
      "loss": 1.4846,
      "step": 93340
    },
    {
      "epoch": 47.4821973550356,
      "grad_norm": 40.041664123535156,
      "learning_rate": 2.5178026449643946e-06,
      "loss": 1.5388,
      "step": 93350
    },
    {
      "epoch": 47.48728382502543,
      "grad_norm": 38.720672607421875,
      "learning_rate": 2.512716174974568e-06,
      "loss": 1.5889,
      "step": 93360
    },
    {
      "epoch": 47.492370295015256,
      "grad_norm": 47.62962341308594,
      "learning_rate": 2.5076297049847407e-06,
      "loss": 1.5737,
      "step": 93370
    },
    {
      "epoch": 47.49745676500508,
      "grad_norm": 44.51232147216797,
      "learning_rate": 2.5025432349949136e-06,
      "loss": 1.4461,
      "step": 93380
    },
    {
      "epoch": 47.50254323499492,
      "grad_norm": 38.51587677001953,
      "learning_rate": 2.4974567650050864e-06,
      "loss": 1.5145,
      "step": 93390
    },
    {
      "epoch": 47.507629704984744,
      "grad_norm": 39.30105209350586,
      "learning_rate": 2.4923702950152597e-06,
      "loss": 1.4984,
      "step": 93400
    },
    {
      "epoch": 47.51271617497457,
      "grad_norm": 36.914485931396484,
      "learning_rate": 2.487283825025432e-06,
      "loss": 1.5368,
      "step": 93410
    },
    {
      "epoch": 47.5178026449644,
      "grad_norm": 48.2487907409668,
      "learning_rate": 2.4821973550356054e-06,
      "loss": 1.4969,
      "step": 93420
    },
    {
      "epoch": 47.522889114954225,
      "grad_norm": 43.77689743041992,
      "learning_rate": 2.4771108850457783e-06,
      "loss": 1.5723,
      "step": 93430
    },
    {
      "epoch": 47.52797558494405,
      "grad_norm": 33.880184173583984,
      "learning_rate": 2.472024415055951e-06,
      "loss": 1.5836,
      "step": 93440
    },
    {
      "epoch": 47.53306205493388,
      "grad_norm": 40.338531494140625,
      "learning_rate": 2.4669379450661244e-06,
      "loss": 1.5282,
      "step": 93450
    },
    {
      "epoch": 47.538148524923706,
      "grad_norm": 43.99540710449219,
      "learning_rate": 2.4618514750762973e-06,
      "loss": 1.5782,
      "step": 93460
    },
    {
      "epoch": 47.54323499491353,
      "grad_norm": 36.69821548461914,
      "learning_rate": 2.45676500508647e-06,
      "loss": 1.4503,
      "step": 93470
    },
    {
      "epoch": 47.54832146490336,
      "grad_norm": 42.860595703125,
      "learning_rate": 2.451678535096643e-06,
      "loss": 1.4821,
      "step": 93480
    },
    {
      "epoch": 47.55340793489319,
      "grad_norm": 41.75209426879883,
      "learning_rate": 2.4465920651068163e-06,
      "loss": 1.4858,
      "step": 93490
    },
    {
      "epoch": 47.558494404883014,
      "grad_norm": 35.58285903930664,
      "learning_rate": 2.4415055951169887e-06,
      "loss": 1.458,
      "step": 93500
    },
    {
      "epoch": 47.56358087487284,
      "grad_norm": 40.452701568603516,
      "learning_rate": 2.436419125127162e-06,
      "loss": 1.4258,
      "step": 93510
    },
    {
      "epoch": 47.56866734486267,
      "grad_norm": 34.33848190307617,
      "learning_rate": 2.431332655137335e-06,
      "loss": 1.5525,
      "step": 93520
    },
    {
      "epoch": 47.573753814852495,
      "grad_norm": 36.966575622558594,
      "learning_rate": 2.4262461851475077e-06,
      "loss": 1.5404,
      "step": 93530
    },
    {
      "epoch": 47.57884028484232,
      "grad_norm": 39.95947265625,
      "learning_rate": 2.4211597151576806e-06,
      "loss": 1.5779,
      "step": 93540
    },
    {
      "epoch": 47.58392675483215,
      "grad_norm": 44.69162368774414,
      "learning_rate": 2.416073245167854e-06,
      "loss": 1.5362,
      "step": 93550
    },
    {
      "epoch": 47.589013224821976,
      "grad_norm": 39.84964370727539,
      "learning_rate": 2.4109867751780263e-06,
      "loss": 1.563,
      "step": 93560
    },
    {
      "epoch": 47.5940996948118,
      "grad_norm": 42.758445739746094,
      "learning_rate": 2.4059003051881996e-06,
      "loss": 1.5138,
      "step": 93570
    },
    {
      "epoch": 47.59918616480163,
      "grad_norm": 41.753108978271484,
      "learning_rate": 2.4008138351983724e-06,
      "loss": 1.5906,
      "step": 93580
    },
    {
      "epoch": 47.60427263479146,
      "grad_norm": 49.506656646728516,
      "learning_rate": 2.3957273652085453e-06,
      "loss": 1.5676,
      "step": 93590
    },
    {
      "epoch": 47.609359104781284,
      "grad_norm": 36.870487213134766,
      "learning_rate": 2.3906408952187186e-06,
      "loss": 1.4841,
      "step": 93600
    },
    {
      "epoch": 47.61444557477111,
      "grad_norm": 49.02155685424805,
      "learning_rate": 2.3855544252288914e-06,
      "loss": 1.4843,
      "step": 93610
    },
    {
      "epoch": 47.61953204476094,
      "grad_norm": 39.26513671875,
      "learning_rate": 2.3804679552390643e-06,
      "loss": 1.4178,
      "step": 93620
    },
    {
      "epoch": 47.624618514750765,
      "grad_norm": 32.80106735229492,
      "learning_rate": 2.375381485249237e-06,
      "loss": 1.453,
      "step": 93630
    },
    {
      "epoch": 47.62970498474059,
      "grad_norm": 44.037235260009766,
      "learning_rate": 2.3702950152594104e-06,
      "loss": 1.5253,
      "step": 93640
    },
    {
      "epoch": 47.63479145473042,
      "grad_norm": 40.31159973144531,
      "learning_rate": 2.365208545269583e-06,
      "loss": 1.5465,
      "step": 93650
    },
    {
      "epoch": 47.639877924720246,
      "grad_norm": 43.84637451171875,
      "learning_rate": 2.360122075279756e-06,
      "loss": 1.4918,
      "step": 93660
    },
    {
      "epoch": 47.64496439471007,
      "grad_norm": 46.09245681762695,
      "learning_rate": 2.355035605289929e-06,
      "loss": 1.5267,
      "step": 93670
    },
    {
      "epoch": 47.6500508646999,
      "grad_norm": 49.11958312988281,
      "learning_rate": 2.349949135300102e-06,
      "loss": 1.4878,
      "step": 93680
    },
    {
      "epoch": 47.65513733468973,
      "grad_norm": 32.65800857543945,
      "learning_rate": 2.3448626653102747e-06,
      "loss": 1.4991,
      "step": 93690
    },
    {
      "epoch": 47.66022380467955,
      "grad_norm": 40.74240493774414,
      "learning_rate": 2.339776195320448e-06,
      "loss": 1.5701,
      "step": 93700
    },
    {
      "epoch": 47.66531027466938,
      "grad_norm": 43.8128662109375,
      "learning_rate": 2.3346897253306204e-06,
      "loss": 1.5778,
      "step": 93710
    },
    {
      "epoch": 47.67039674465921,
      "grad_norm": 45.73312759399414,
      "learning_rate": 2.3296032553407937e-06,
      "loss": 1.4897,
      "step": 93720
    },
    {
      "epoch": 47.675483214649034,
      "grad_norm": 48.2176399230957,
      "learning_rate": 2.3245167853509666e-06,
      "loss": 1.4989,
      "step": 93730
    },
    {
      "epoch": 47.68056968463886,
      "grad_norm": 43.50602722167969,
      "learning_rate": 2.3194303153611394e-06,
      "loss": 1.4426,
      "step": 93740
    },
    {
      "epoch": 47.68565615462869,
      "grad_norm": 35.302101135253906,
      "learning_rate": 2.3143438453713123e-06,
      "loss": 1.4882,
      "step": 93750
    },
    {
      "epoch": 47.690742624618515,
      "grad_norm": 48.74701690673828,
      "learning_rate": 2.3092573753814856e-06,
      "loss": 1.5037,
      "step": 93760
    },
    {
      "epoch": 47.69582909460834,
      "grad_norm": 33.84003448486328,
      "learning_rate": 2.3041709053916584e-06,
      "loss": 1.5459,
      "step": 93770
    },
    {
      "epoch": 47.70091556459817,
      "grad_norm": 50.258853912353516,
      "learning_rate": 2.2990844354018313e-06,
      "loss": 1.531,
      "step": 93780
    },
    {
      "epoch": 47.706002034587996,
      "grad_norm": 45.49480438232422,
      "learning_rate": 2.293997965412004e-06,
      "loss": 1.5495,
      "step": 93790
    },
    {
      "epoch": 47.71108850457782,
      "grad_norm": 48.359840393066406,
      "learning_rate": 2.288911495422177e-06,
      "loss": 1.5358,
      "step": 93800
    },
    {
      "epoch": 47.71617497456765,
      "grad_norm": 42.4709587097168,
      "learning_rate": 2.2838250254323503e-06,
      "loss": 1.4463,
      "step": 93810
    },
    {
      "epoch": 47.72126144455748,
      "grad_norm": 37.96723175048828,
      "learning_rate": 2.278738555442523e-06,
      "loss": 1.5801,
      "step": 93820
    },
    {
      "epoch": 47.726347914547304,
      "grad_norm": 46.32962417602539,
      "learning_rate": 2.273652085452696e-06,
      "loss": 1.4732,
      "step": 93830
    },
    {
      "epoch": 47.73143438453713,
      "grad_norm": 45.70041275024414,
      "learning_rate": 2.268565615462869e-06,
      "loss": 1.5319,
      "step": 93840
    },
    {
      "epoch": 47.73652085452696,
      "grad_norm": 37.66880416870117,
      "learning_rate": 2.2634791454730417e-06,
      "loss": 1.5552,
      "step": 93850
    },
    {
      "epoch": 47.741607324516785,
      "grad_norm": 38.54086685180664,
      "learning_rate": 2.2583926754832146e-06,
      "loss": 1.4789,
      "step": 93860
    },
    {
      "epoch": 47.74669379450661,
      "grad_norm": 37.64994430541992,
      "learning_rate": 2.253306205493388e-06,
      "loss": 1.615,
      "step": 93870
    },
    {
      "epoch": 47.75178026449644,
      "grad_norm": 43.696311950683594,
      "learning_rate": 2.2482197355035607e-06,
      "loss": 1.5752,
      "step": 93880
    },
    {
      "epoch": 47.756866734486266,
      "grad_norm": 38.57237243652344,
      "learning_rate": 2.2431332655137336e-06,
      "loss": 1.5804,
      "step": 93890
    },
    {
      "epoch": 47.76195320447609,
      "grad_norm": 42.51856231689453,
      "learning_rate": 2.2380467955239064e-06,
      "loss": 1.3999,
      "step": 93900
    },
    {
      "epoch": 47.76703967446592,
      "grad_norm": 35.51106643676758,
      "learning_rate": 2.2329603255340793e-06,
      "loss": 1.5291,
      "step": 93910
    },
    {
      "epoch": 47.77212614445575,
      "grad_norm": 39.25388717651367,
      "learning_rate": 2.227873855544252e-06,
      "loss": 1.4957,
      "step": 93920
    },
    {
      "epoch": 47.777212614445574,
      "grad_norm": 36.497257232666016,
      "learning_rate": 2.2227873855544254e-06,
      "loss": 1.4662,
      "step": 93930
    },
    {
      "epoch": 47.7822990844354,
      "grad_norm": 42.09028244018555,
      "learning_rate": 2.2177009155645983e-06,
      "loss": 1.5763,
      "step": 93940
    },
    {
      "epoch": 47.78738555442523,
      "grad_norm": 47.98349380493164,
      "learning_rate": 2.212614445574771e-06,
      "loss": 1.5584,
      "step": 93950
    },
    {
      "epoch": 47.792472024415055,
      "grad_norm": 39.13668441772461,
      "learning_rate": 2.2075279755849444e-06,
      "loss": 1.5273,
      "step": 93960
    },
    {
      "epoch": 47.79755849440488,
      "grad_norm": 50.40971374511719,
      "learning_rate": 2.202441505595117e-06,
      "loss": 1.5948,
      "step": 93970
    },
    {
      "epoch": 47.80264496439471,
      "grad_norm": 47.031150817871094,
      "learning_rate": 2.19735503560529e-06,
      "loss": 1.5924,
      "step": 93980
    },
    {
      "epoch": 47.807731434384536,
      "grad_norm": 39.690921783447266,
      "learning_rate": 2.192268565615463e-06,
      "loss": 1.5534,
      "step": 93990
    },
    {
      "epoch": 47.81281790437436,
      "grad_norm": 38.725921630859375,
      "learning_rate": 2.187182095625636e-06,
      "loss": 1.5019,
      "step": 94000
    },
    {
      "epoch": 47.81790437436419,
      "grad_norm": 34.88633728027344,
      "learning_rate": 2.1820956256358087e-06,
      "loss": 1.4869,
      "step": 94010
    },
    {
      "epoch": 47.82299084435402,
      "grad_norm": 45.248130798339844,
      "learning_rate": 2.177009155645982e-06,
      "loss": 1.4359,
      "step": 94020
    },
    {
      "epoch": 47.828077314343844,
      "grad_norm": 43.416141510009766,
      "learning_rate": 2.1719226856561544e-06,
      "loss": 1.5021,
      "step": 94030
    },
    {
      "epoch": 47.83316378433367,
      "grad_norm": 38.91999435424805,
      "learning_rate": 2.1668362156663277e-06,
      "loss": 1.5093,
      "step": 94040
    },
    {
      "epoch": 47.8382502543235,
      "grad_norm": 36.680397033691406,
      "learning_rate": 2.1617497456765006e-06,
      "loss": 1.5069,
      "step": 94050
    },
    {
      "epoch": 47.843336724313325,
      "grad_norm": 38.90558624267578,
      "learning_rate": 2.1566632756866734e-06,
      "loss": 1.4328,
      "step": 94060
    },
    {
      "epoch": 47.84842319430315,
      "grad_norm": 38.68086242675781,
      "learning_rate": 2.1515768056968463e-06,
      "loss": 1.5362,
      "step": 94070
    },
    {
      "epoch": 47.85350966429298,
      "grad_norm": 47.80506896972656,
      "learning_rate": 2.1464903357070196e-06,
      "loss": 1.4549,
      "step": 94080
    },
    {
      "epoch": 47.858596134282806,
      "grad_norm": 37.60388946533203,
      "learning_rate": 2.1414038657171924e-06,
      "loss": 1.522,
      "step": 94090
    },
    {
      "epoch": 47.86368260427263,
      "grad_norm": 46.086334228515625,
      "learning_rate": 2.1363173957273653e-06,
      "loss": 1.4553,
      "step": 94100
    },
    {
      "epoch": 47.86876907426246,
      "grad_norm": 48.645023345947266,
      "learning_rate": 2.1312309257375386e-06,
      "loss": 1.475,
      "step": 94110
    },
    {
      "epoch": 47.87385554425229,
      "grad_norm": 35.653289794921875,
      "learning_rate": 2.126144455747711e-06,
      "loss": 1.5324,
      "step": 94120
    },
    {
      "epoch": 47.878942014242114,
      "grad_norm": 43.34944152832031,
      "learning_rate": 2.1210579857578843e-06,
      "loss": 1.5967,
      "step": 94130
    },
    {
      "epoch": 47.88402848423194,
      "grad_norm": 39.53941345214844,
      "learning_rate": 2.115971515768057e-06,
      "loss": 1.4429,
      "step": 94140
    },
    {
      "epoch": 47.88911495422177,
      "grad_norm": 38.56479263305664,
      "learning_rate": 2.11088504577823e-06,
      "loss": 1.4815,
      "step": 94150
    },
    {
      "epoch": 47.894201424211595,
      "grad_norm": 41.55690383911133,
      "learning_rate": 2.105798575788403e-06,
      "loss": 1.5813,
      "step": 94160
    },
    {
      "epoch": 47.89928789420142,
      "grad_norm": 51.358924865722656,
      "learning_rate": 2.100712105798576e-06,
      "loss": 1.5659,
      "step": 94170
    },
    {
      "epoch": 47.90437436419125,
      "grad_norm": 45.7252082824707,
      "learning_rate": 2.0956256358087486e-06,
      "loss": 1.5716,
      "step": 94180
    },
    {
      "epoch": 47.909460834181075,
      "grad_norm": 39.8469352722168,
      "learning_rate": 2.090539165818922e-06,
      "loss": 1.5216,
      "step": 94190
    },
    {
      "epoch": 47.9145473041709,
      "grad_norm": 39.95292663574219,
      "learning_rate": 2.0854526958290947e-06,
      "loss": 1.4454,
      "step": 94200
    },
    {
      "epoch": 47.91963377416073,
      "grad_norm": 41.78566360473633,
      "learning_rate": 2.0803662258392676e-06,
      "loss": 1.4945,
      "step": 94210
    },
    {
      "epoch": 47.924720244150556,
      "grad_norm": 36.64011001586914,
      "learning_rate": 2.0752797558494404e-06,
      "loss": 1.4904,
      "step": 94220
    },
    {
      "epoch": 47.92980671414038,
      "grad_norm": 47.46877670288086,
      "learning_rate": 2.0701932858596137e-06,
      "loss": 1.4578,
      "step": 94230
    },
    {
      "epoch": 47.93489318413021,
      "grad_norm": 45.66102600097656,
      "learning_rate": 2.065106815869786e-06,
      "loss": 1.6374,
      "step": 94240
    },
    {
      "epoch": 47.93997965412004,
      "grad_norm": 41.361690521240234,
      "learning_rate": 2.0600203458799594e-06,
      "loss": 1.4951,
      "step": 94250
    },
    {
      "epoch": 47.945066124109864,
      "grad_norm": 44.04166793823242,
      "learning_rate": 2.0549338758901327e-06,
      "loss": 1.5503,
      "step": 94260
    },
    {
      "epoch": 47.95015259409969,
      "grad_norm": 36.461360931396484,
      "learning_rate": 2.049847405900305e-06,
      "loss": 1.4806,
      "step": 94270
    },
    {
      "epoch": 47.955239064089525,
      "grad_norm": 33.575103759765625,
      "learning_rate": 2.0447609359104784e-06,
      "loss": 1.4712,
      "step": 94280
    },
    {
      "epoch": 47.96032553407935,
      "grad_norm": 39.60208511352539,
      "learning_rate": 2.0396744659206513e-06,
      "loss": 1.4043,
      "step": 94290
    },
    {
      "epoch": 47.96541200406918,
      "grad_norm": 43.423377990722656,
      "learning_rate": 2.034587995930824e-06,
      "loss": 1.5209,
      "step": 94300
    },
    {
      "epoch": 47.970498474059006,
      "grad_norm": 38.82781982421875,
      "learning_rate": 2.029501525940997e-06,
      "loss": 1.5281,
      "step": 94310
    },
    {
      "epoch": 47.97558494404883,
      "grad_norm": 35.8714599609375,
      "learning_rate": 2.0244150559511703e-06,
      "loss": 1.4733,
      "step": 94320
    },
    {
      "epoch": 47.98067141403866,
      "grad_norm": 56.2165641784668,
      "learning_rate": 2.0193285859613427e-06,
      "loss": 1.5657,
      "step": 94330
    },
    {
      "epoch": 47.98575788402849,
      "grad_norm": 42.0366325378418,
      "learning_rate": 2.014242115971516e-06,
      "loss": 1.5774,
      "step": 94340
    },
    {
      "epoch": 47.990844354018314,
      "grad_norm": 53.26210403442383,
      "learning_rate": 2.009155645981689e-06,
      "loss": 1.627,
      "step": 94350
    },
    {
      "epoch": 47.99593082400814,
      "grad_norm": 33.23952865600586,
      "learning_rate": 2.0040691759918617e-06,
      "loss": 1.541,
      "step": 94360
    },
    {
      "epoch": 48.0,
      "eval_loss": 5.111384391784668,
      "eval_runtime": 2.8848,
      "eval_samples_per_second": 961.937,
      "eval_steps_per_second": 120.285,
      "step": 94368
    },
    {
      "epoch": 48.00101729399797,
      "grad_norm": 41.60486602783203,
      "learning_rate": 1.9989827060020346e-06,
      "loss": 1.5897,
      "step": 94370
    },
    {
      "epoch": 48.006103763987795,
      "grad_norm": 46.52789306640625,
      "learning_rate": 1.993896236012208e-06,
      "loss": 1.4973,
      "step": 94380
    },
    {
      "epoch": 48.01119023397762,
      "grad_norm": 49.63157653808594,
      "learning_rate": 1.9888097660223803e-06,
      "loss": 1.5218,
      "step": 94390
    },
    {
      "epoch": 48.01627670396745,
      "grad_norm": 45.888458251953125,
      "learning_rate": 1.9837232960325536e-06,
      "loss": 1.5076,
      "step": 94400
    },
    {
      "epoch": 48.021363173957276,
      "grad_norm": 35.169471740722656,
      "learning_rate": 1.9786368260427264e-06,
      "loss": 1.4712,
      "step": 94410
    },
    {
      "epoch": 48.0264496439471,
      "grad_norm": 37.732479095458984,
      "learning_rate": 1.9735503560528993e-06,
      "loss": 1.4446,
      "step": 94420
    },
    {
      "epoch": 48.03153611393693,
      "grad_norm": 42.060874938964844,
      "learning_rate": 1.9684638860630726e-06,
      "loss": 1.5288,
      "step": 94430
    },
    {
      "epoch": 48.03662258392676,
      "grad_norm": 48.3909912109375,
      "learning_rate": 1.9633774160732454e-06,
      "loss": 1.4692,
      "step": 94440
    },
    {
      "epoch": 48.041709053916584,
      "grad_norm": 47.227455139160156,
      "learning_rate": 1.9582909460834183e-06,
      "loss": 1.5533,
      "step": 94450
    },
    {
      "epoch": 48.04679552390641,
      "grad_norm": 44.803550720214844,
      "learning_rate": 1.953204476093591e-06,
      "loss": 1.4109,
      "step": 94460
    },
    {
      "epoch": 48.05188199389624,
      "grad_norm": 54.70620346069336,
      "learning_rate": 1.948118006103764e-06,
      "loss": 1.5179,
      "step": 94470
    },
    {
      "epoch": 48.056968463886065,
      "grad_norm": 43.30601501464844,
      "learning_rate": 1.943031536113937e-06,
      "loss": 1.5176,
      "step": 94480
    },
    {
      "epoch": 48.06205493387589,
      "grad_norm": 47.858028411865234,
      "learning_rate": 1.93794506612411e-06,
      "loss": 1.484,
      "step": 94490
    },
    {
      "epoch": 48.06714140386572,
      "grad_norm": 43.481163024902344,
      "learning_rate": 1.932858596134283e-06,
      "loss": 1.5775,
      "step": 94500
    },
    {
      "epoch": 48.072227873855546,
      "grad_norm": 44.35306930541992,
      "learning_rate": 1.927772126144456e-06,
      "loss": 1.6007,
      "step": 94510
    },
    {
      "epoch": 48.07731434384537,
      "grad_norm": 42.01152420043945,
      "learning_rate": 1.9226856561546287e-06,
      "loss": 1.4607,
      "step": 94520
    },
    {
      "epoch": 48.0824008138352,
      "grad_norm": 34.71167755126953,
      "learning_rate": 1.9175991861648016e-06,
      "loss": 1.6167,
      "step": 94530
    },
    {
      "epoch": 48.08748728382503,
      "grad_norm": 33.9749870300293,
      "learning_rate": 1.9125127161749744e-06,
      "loss": 1.4393,
      "step": 94540
    },
    {
      "epoch": 48.092573753814854,
      "grad_norm": 36.707027435302734,
      "learning_rate": 1.9074262461851477e-06,
      "loss": 1.4932,
      "step": 94550
    },
    {
      "epoch": 48.09766022380468,
      "grad_norm": 40.71788787841797,
      "learning_rate": 1.9023397761953204e-06,
      "loss": 1.439,
      "step": 94560
    },
    {
      "epoch": 48.10274669379451,
      "grad_norm": 34.5958137512207,
      "learning_rate": 1.8972533062054934e-06,
      "loss": 1.5983,
      "step": 94570
    },
    {
      "epoch": 48.107833163784335,
      "grad_norm": 47.641380310058594,
      "learning_rate": 1.8921668362156665e-06,
      "loss": 1.4896,
      "step": 94580
    },
    {
      "epoch": 48.11291963377416,
      "grad_norm": 42.81078338623047,
      "learning_rate": 1.8870803662258394e-06,
      "loss": 1.5392,
      "step": 94590
    },
    {
      "epoch": 48.11800610376399,
      "grad_norm": 48.63579559326172,
      "learning_rate": 1.8819938962360124e-06,
      "loss": 1.4724,
      "step": 94600
    },
    {
      "epoch": 48.123092573753816,
      "grad_norm": 38.147865295410156,
      "learning_rate": 1.8769074262461853e-06,
      "loss": 1.5894,
      "step": 94610
    },
    {
      "epoch": 48.12817904374364,
      "grad_norm": 50.07758331298828,
      "learning_rate": 1.8718209562563583e-06,
      "loss": 1.552,
      "step": 94620
    },
    {
      "epoch": 48.13326551373347,
      "grad_norm": 43.143001556396484,
      "learning_rate": 1.866734486266531e-06,
      "loss": 1.5005,
      "step": 94630
    },
    {
      "epoch": 48.1383519837233,
      "grad_norm": 42.5212516784668,
      "learning_rate": 1.861648016276704e-06,
      "loss": 1.5309,
      "step": 94640
    },
    {
      "epoch": 48.143438453713124,
      "grad_norm": 37.98957443237305,
      "learning_rate": 1.856561546286877e-06,
      "loss": 1.443,
      "step": 94650
    },
    {
      "epoch": 48.14852492370295,
      "grad_norm": 40.62143325805664,
      "learning_rate": 1.85147507629705e-06,
      "loss": 1.5591,
      "step": 94660
    },
    {
      "epoch": 48.15361139369278,
      "grad_norm": 45.77799606323242,
      "learning_rate": 1.8463886063072229e-06,
      "loss": 1.4992,
      "step": 94670
    },
    {
      "epoch": 48.158697863682605,
      "grad_norm": 45.11086654663086,
      "learning_rate": 1.841302136317396e-06,
      "loss": 1.5133,
      "step": 94680
    },
    {
      "epoch": 48.16378433367243,
      "grad_norm": 41.90157699584961,
      "learning_rate": 1.8362156663275686e-06,
      "loss": 1.5804,
      "step": 94690
    },
    {
      "epoch": 48.16887080366226,
      "grad_norm": 45.85737609863281,
      "learning_rate": 1.8311291963377416e-06,
      "loss": 1.5344,
      "step": 94700
    },
    {
      "epoch": 48.173957273652086,
      "grad_norm": 44.7332878112793,
      "learning_rate": 1.8260427263479145e-06,
      "loss": 1.407,
      "step": 94710
    },
    {
      "epoch": 48.17904374364191,
      "grad_norm": 37.65630340576172,
      "learning_rate": 1.8209562563580876e-06,
      "loss": 1.5141,
      "step": 94720
    },
    {
      "epoch": 48.18413021363174,
      "grad_norm": 39.842010498046875,
      "learning_rate": 1.8158697863682604e-06,
      "loss": 1.5042,
      "step": 94730
    },
    {
      "epoch": 48.18921668362157,
      "grad_norm": 38.986167907714844,
      "learning_rate": 1.8107833163784335e-06,
      "loss": 1.5503,
      "step": 94740
    },
    {
      "epoch": 48.19430315361139,
      "grad_norm": 34.70601272583008,
      "learning_rate": 1.8056968463886066e-06,
      "loss": 1.5268,
      "step": 94750
    },
    {
      "epoch": 48.19938962360122,
      "grad_norm": 36.181297302246094,
      "learning_rate": 1.8006103763987792e-06,
      "loss": 1.4853,
      "step": 94760
    },
    {
      "epoch": 48.20447609359105,
      "grad_norm": 46.435420989990234,
      "learning_rate": 1.7955239064089525e-06,
      "loss": 1.5554,
      "step": 94770
    },
    {
      "epoch": 48.209562563580874,
      "grad_norm": 38.554168701171875,
      "learning_rate": 1.7904374364191251e-06,
      "loss": 1.5201,
      "step": 94780
    },
    {
      "epoch": 48.2146490335707,
      "grad_norm": 36.030540466308594,
      "learning_rate": 1.7853509664292982e-06,
      "loss": 1.5425,
      "step": 94790
    },
    {
      "epoch": 48.21973550356053,
      "grad_norm": 58.63816833496094,
      "learning_rate": 1.780264496439471e-06,
      "loss": 1.5768,
      "step": 94800
    },
    {
      "epoch": 48.224821973550355,
      "grad_norm": 41.787689208984375,
      "learning_rate": 1.7751780264496441e-06,
      "loss": 1.5499,
      "step": 94810
    },
    {
      "epoch": 48.22990844354018,
      "grad_norm": 39.21071243286133,
      "learning_rate": 1.7700915564598168e-06,
      "loss": 1.5967,
      "step": 94820
    },
    {
      "epoch": 48.23499491353001,
      "grad_norm": 43.913551330566406,
      "learning_rate": 1.76500508646999e-06,
      "loss": 1.6007,
      "step": 94830
    },
    {
      "epoch": 48.240081383519836,
      "grad_norm": 50.15065002441406,
      "learning_rate": 1.7599186164801627e-06,
      "loss": 1.5015,
      "step": 94840
    },
    {
      "epoch": 48.24516785350966,
      "grad_norm": 49.40852355957031,
      "learning_rate": 1.7548321464903358e-06,
      "loss": 1.5164,
      "step": 94850
    },
    {
      "epoch": 48.25025432349949,
      "grad_norm": 41.71949768066406,
      "learning_rate": 1.7497456765005086e-06,
      "loss": 1.5048,
      "step": 94860
    },
    {
      "epoch": 48.25534079348932,
      "grad_norm": 39.153480529785156,
      "learning_rate": 1.7446592065106817e-06,
      "loss": 1.5525,
      "step": 94870
    },
    {
      "epoch": 48.260427263479144,
      "grad_norm": 39.66682052612305,
      "learning_rate": 1.7395727365208544e-06,
      "loss": 1.5412,
      "step": 94880
    },
    {
      "epoch": 48.26551373346897,
      "grad_norm": 41.23663330078125,
      "learning_rate": 1.7344862665310276e-06,
      "loss": 1.4514,
      "step": 94890
    },
    {
      "epoch": 48.2706002034588,
      "grad_norm": 37.75033187866211,
      "learning_rate": 1.7293997965412007e-06,
      "loss": 1.4902,
      "step": 94900
    },
    {
      "epoch": 48.275686673448625,
      "grad_norm": 33.08814239501953,
      "learning_rate": 1.7243133265513734e-06,
      "loss": 1.4617,
      "step": 94910
    },
    {
      "epoch": 48.28077314343845,
      "grad_norm": 62.06917953491211,
      "learning_rate": 1.7192268565615464e-06,
      "loss": 1.506,
      "step": 94920
    },
    {
      "epoch": 48.28585961342828,
      "grad_norm": 37.69023132324219,
      "learning_rate": 1.7141403865717193e-06,
      "loss": 1.4722,
      "step": 94930
    },
    {
      "epoch": 48.290946083418106,
      "grad_norm": 44.16585922241211,
      "learning_rate": 1.7090539165818923e-06,
      "loss": 1.5043,
      "step": 94940
    },
    {
      "epoch": 48.29603255340793,
      "grad_norm": 55.14332962036133,
      "learning_rate": 1.7039674465920652e-06,
      "loss": 1.5723,
      "step": 94950
    },
    {
      "epoch": 48.30111902339776,
      "grad_norm": 42.511993408203125,
      "learning_rate": 1.6988809766022383e-06,
      "loss": 1.5162,
      "step": 94960
    },
    {
      "epoch": 48.30620549338759,
      "grad_norm": 47.75196838378906,
      "learning_rate": 1.693794506612411e-06,
      "loss": 1.4862,
      "step": 94970
    },
    {
      "epoch": 48.311291963377414,
      "grad_norm": 43.4217414855957,
      "learning_rate": 1.688708036622584e-06,
      "loss": 1.4747,
      "step": 94980
    },
    {
      "epoch": 48.31637843336724,
      "grad_norm": 43.42439270019531,
      "learning_rate": 1.6836215666327569e-06,
      "loss": 1.4642,
      "step": 94990
    },
    {
      "epoch": 48.32146490335707,
      "grad_norm": 39.58015441894531,
      "learning_rate": 1.67853509664293e-06,
      "loss": 1.5746,
      "step": 95000
    },
    {
      "epoch": 48.326551373346895,
      "grad_norm": 43.42382049560547,
      "learning_rate": 1.6734486266531028e-06,
      "loss": 1.5495,
      "step": 95010
    },
    {
      "epoch": 48.33163784333672,
      "grad_norm": 45.86300277709961,
      "learning_rate": 1.6683621566632759e-06,
      "loss": 1.5134,
      "step": 95020
    },
    {
      "epoch": 48.33672431332655,
      "grad_norm": 50.63338088989258,
      "learning_rate": 1.6632756866734485e-06,
      "loss": 1.5257,
      "step": 95030
    },
    {
      "epoch": 48.341810783316376,
      "grad_norm": 44.00861740112305,
      "learning_rate": 1.6581892166836216e-06,
      "loss": 1.5306,
      "step": 95040
    },
    {
      "epoch": 48.3468972533062,
      "grad_norm": 44.73305130004883,
      "learning_rate": 1.6531027466937944e-06,
      "loss": 1.4772,
      "step": 95050
    },
    {
      "epoch": 48.35198372329603,
      "grad_norm": 38.51060104370117,
      "learning_rate": 1.6480162767039675e-06,
      "loss": 1.487,
      "step": 95060
    },
    {
      "epoch": 48.35707019328586,
      "grad_norm": 49.2356071472168,
      "learning_rate": 1.6429298067141406e-06,
      "loss": 1.5658,
      "step": 95070
    },
    {
      "epoch": 48.362156663275684,
      "grad_norm": 36.1317253112793,
      "learning_rate": 1.6378433367243134e-06,
      "loss": 1.4476,
      "step": 95080
    },
    {
      "epoch": 48.36724313326551,
      "grad_norm": 37.29705810546875,
      "learning_rate": 1.6327568667344865e-06,
      "loss": 1.4747,
      "step": 95090
    },
    {
      "epoch": 48.37232960325534,
      "grad_norm": 45.45994186401367,
      "learning_rate": 1.6276703967446591e-06,
      "loss": 1.4464,
      "step": 95100
    },
    {
      "epoch": 48.377416073245165,
      "grad_norm": 45.5046501159668,
      "learning_rate": 1.6225839267548324e-06,
      "loss": 1.538,
      "step": 95110
    },
    {
      "epoch": 48.38250254323499,
      "grad_norm": 43.989166259765625,
      "learning_rate": 1.617497456765005e-06,
      "loss": 1.5138,
      "step": 95120
    },
    {
      "epoch": 48.38758901322482,
      "grad_norm": 37.49700927734375,
      "learning_rate": 1.6124109867751781e-06,
      "loss": 1.4822,
      "step": 95130
    },
    {
      "epoch": 48.392675483214646,
      "grad_norm": 52.9537353515625,
      "learning_rate": 1.607324516785351e-06,
      "loss": 1.5196,
      "step": 95140
    },
    {
      "epoch": 48.39776195320447,
      "grad_norm": 42.432090759277344,
      "learning_rate": 1.602238046795524e-06,
      "loss": 1.5091,
      "step": 95150
    },
    {
      "epoch": 48.4028484231943,
      "grad_norm": 34.1448860168457,
      "learning_rate": 1.5971515768056967e-06,
      "loss": 1.568,
      "step": 95160
    },
    {
      "epoch": 48.407934893184134,
      "grad_norm": 40.65998077392578,
      "learning_rate": 1.59206510681587e-06,
      "loss": 1.4559,
      "step": 95170
    },
    {
      "epoch": 48.41302136317396,
      "grad_norm": 37.09980773925781,
      "learning_rate": 1.5869786368260426e-06,
      "loss": 1.4827,
      "step": 95180
    },
    {
      "epoch": 48.41810783316379,
      "grad_norm": 46.72449493408203,
      "learning_rate": 1.5818921668362157e-06,
      "loss": 1.5581,
      "step": 95190
    },
    {
      "epoch": 48.423194303153615,
      "grad_norm": 37.58445358276367,
      "learning_rate": 1.5768056968463886e-06,
      "loss": 1.4732,
      "step": 95200
    },
    {
      "epoch": 48.42828077314344,
      "grad_norm": 70.57360076904297,
      "learning_rate": 1.5717192268565616e-06,
      "loss": 1.5489,
      "step": 95210
    },
    {
      "epoch": 48.43336724313327,
      "grad_norm": 50.56105422973633,
      "learning_rate": 1.5666327568667347e-06,
      "loss": 1.5776,
      "step": 95220
    },
    {
      "epoch": 48.438453713123096,
      "grad_norm": 40.63809585571289,
      "learning_rate": 1.5615462868769076e-06,
      "loss": 1.5825,
      "step": 95230
    },
    {
      "epoch": 48.44354018311292,
      "grad_norm": 43.23056411743164,
      "learning_rate": 1.5564598168870804e-06,
      "loss": 1.5431,
      "step": 95240
    },
    {
      "epoch": 48.44862665310275,
      "grad_norm": 38.36647033691406,
      "learning_rate": 1.5513733468972533e-06,
      "loss": 1.4891,
      "step": 95250
    },
    {
      "epoch": 48.45371312309258,
      "grad_norm": 50.119163513183594,
      "learning_rate": 1.5462868769074264e-06,
      "loss": 1.5169,
      "step": 95260
    },
    {
      "epoch": 48.458799593082404,
      "grad_norm": 37.6967658996582,
      "learning_rate": 1.5412004069175992e-06,
      "loss": 1.4741,
      "step": 95270
    },
    {
      "epoch": 48.46388606307223,
      "grad_norm": 48.100215911865234,
      "learning_rate": 1.536113936927772e-06,
      "loss": 1.5288,
      "step": 95280
    },
    {
      "epoch": 48.46897253306206,
      "grad_norm": 38.46038818359375,
      "learning_rate": 1.5310274669379451e-06,
      "loss": 1.4995,
      "step": 95290
    },
    {
      "epoch": 48.474059003051885,
      "grad_norm": 35.16983413696289,
      "learning_rate": 1.5259409969481182e-06,
      "loss": 1.5657,
      "step": 95300
    },
    {
      "epoch": 48.47914547304171,
      "grad_norm": 33.798519134521484,
      "learning_rate": 1.520854526958291e-06,
      "loss": 1.4956,
      "step": 95310
    },
    {
      "epoch": 48.48423194303154,
      "grad_norm": 43.78757095336914,
      "learning_rate": 1.515768056968464e-06,
      "loss": 1.544,
      "step": 95320
    },
    {
      "epoch": 48.489318413021365,
      "grad_norm": 40.16729736328125,
      "learning_rate": 1.510681586978637e-06,
      "loss": 1.5786,
      "step": 95330
    },
    {
      "epoch": 48.49440488301119,
      "grad_norm": 49.245033264160156,
      "learning_rate": 1.5055951169888099e-06,
      "loss": 1.4827,
      "step": 95340
    },
    {
      "epoch": 48.49949135300102,
      "grad_norm": 39.05729293823242,
      "learning_rate": 1.5005086469989827e-06,
      "loss": 1.5584,
      "step": 95350
    },
    {
      "epoch": 48.504577822990846,
      "grad_norm": 36.8165168762207,
      "learning_rate": 1.4954221770091558e-06,
      "loss": 1.5173,
      "step": 95360
    },
    {
      "epoch": 48.50966429298067,
      "grad_norm": 47.765010833740234,
      "learning_rate": 1.4903357070193286e-06,
      "loss": 1.5147,
      "step": 95370
    },
    {
      "epoch": 48.5147507629705,
      "grad_norm": 35.43549346923828,
      "learning_rate": 1.4852492370295015e-06,
      "loss": 1.4637,
      "step": 95380
    },
    {
      "epoch": 48.51983723296033,
      "grad_norm": 45.8565788269043,
      "learning_rate": 1.4801627670396746e-06,
      "loss": 1.5357,
      "step": 95390
    },
    {
      "epoch": 48.524923702950154,
      "grad_norm": 41.688236236572266,
      "learning_rate": 1.4750762970498474e-06,
      "loss": 1.5052,
      "step": 95400
    },
    {
      "epoch": 48.53001017293998,
      "grad_norm": 37.339080810546875,
      "learning_rate": 1.4699898270600203e-06,
      "loss": 1.5656,
      "step": 95410
    },
    {
      "epoch": 48.53509664292981,
      "grad_norm": 41.4952507019043,
      "learning_rate": 1.4649033570701934e-06,
      "loss": 1.4794,
      "step": 95420
    },
    {
      "epoch": 48.540183112919635,
      "grad_norm": 36.66382598876953,
      "learning_rate": 1.4598168870803662e-06,
      "loss": 1.4269,
      "step": 95430
    },
    {
      "epoch": 48.54526958290946,
      "grad_norm": 35.899696350097656,
      "learning_rate": 1.454730417090539e-06,
      "loss": 1.5363,
      "step": 95440
    },
    {
      "epoch": 48.55035605289929,
      "grad_norm": 38.67414855957031,
      "learning_rate": 1.4496439471007121e-06,
      "loss": 1.5019,
      "step": 95450
    },
    {
      "epoch": 48.555442522889116,
      "grad_norm": 46.57392501831055,
      "learning_rate": 1.444557477110885e-06,
      "loss": 1.6014,
      "step": 95460
    },
    {
      "epoch": 48.56052899287894,
      "grad_norm": 38.33026123046875,
      "learning_rate": 1.439471007121058e-06,
      "loss": 1.4896,
      "step": 95470
    },
    {
      "epoch": 48.56561546286877,
      "grad_norm": 36.7852783203125,
      "learning_rate": 1.4343845371312311e-06,
      "loss": 1.5919,
      "step": 95480
    },
    {
      "epoch": 48.5707019328586,
      "grad_norm": 35.47065734863281,
      "learning_rate": 1.429298067141404e-06,
      "loss": 1.5704,
      "step": 95490
    },
    {
      "epoch": 48.575788402848424,
      "grad_norm": 44.78437805175781,
      "learning_rate": 1.4242115971515769e-06,
      "loss": 1.5751,
      "step": 95500
    },
    {
      "epoch": 48.58087487283825,
      "grad_norm": 36.28364944458008,
      "learning_rate": 1.41912512716175e-06,
      "loss": 1.5452,
      "step": 95510
    },
    {
      "epoch": 48.58596134282808,
      "grad_norm": 42.01081085205078,
      "learning_rate": 1.4140386571719228e-06,
      "loss": 1.446,
      "step": 95520
    },
    {
      "epoch": 48.591047812817905,
      "grad_norm": 46.57859802246094,
      "learning_rate": 1.4089521871820956e-06,
      "loss": 1.5837,
      "step": 95530
    },
    {
      "epoch": 48.59613428280773,
      "grad_norm": 46.70197677612305,
      "learning_rate": 1.4038657171922687e-06,
      "loss": 1.4469,
      "step": 95540
    },
    {
      "epoch": 48.60122075279756,
      "grad_norm": 38.023250579833984,
      "learning_rate": 1.3987792472024416e-06,
      "loss": 1.5246,
      "step": 95550
    },
    {
      "epoch": 48.606307222787386,
      "grad_norm": 41.161251068115234,
      "learning_rate": 1.3936927772126144e-06,
      "loss": 1.523,
      "step": 95560
    },
    {
      "epoch": 48.61139369277721,
      "grad_norm": 38.02271270751953,
      "learning_rate": 1.3886063072227875e-06,
      "loss": 1.4951,
      "step": 95570
    },
    {
      "epoch": 48.61648016276704,
      "grad_norm": 42.618770599365234,
      "learning_rate": 1.3835198372329604e-06,
      "loss": 1.53,
      "step": 95580
    },
    {
      "epoch": 48.62156663275687,
      "grad_norm": 40.220680236816406,
      "learning_rate": 1.3784333672431332e-06,
      "loss": 1.5396,
      "step": 95590
    },
    {
      "epoch": 48.626653102746694,
      "grad_norm": 40.1065788269043,
      "learning_rate": 1.3733468972533063e-06,
      "loss": 1.5731,
      "step": 95600
    },
    {
      "epoch": 48.63173957273652,
      "grad_norm": 45.516990661621094,
      "learning_rate": 1.3682604272634791e-06,
      "loss": 1.4471,
      "step": 95610
    },
    {
      "epoch": 48.63682604272635,
      "grad_norm": 40.345577239990234,
      "learning_rate": 1.363173957273652e-06,
      "loss": 1.5266,
      "step": 95620
    },
    {
      "epoch": 48.641912512716175,
      "grad_norm": 45.83810043334961,
      "learning_rate": 1.358087487283825e-06,
      "loss": 1.5777,
      "step": 95630
    },
    {
      "epoch": 48.646998982706,
      "grad_norm": 36.153621673583984,
      "learning_rate": 1.3530010172939981e-06,
      "loss": 1.5984,
      "step": 95640
    },
    {
      "epoch": 48.65208545269583,
      "grad_norm": 41.00075149536133,
      "learning_rate": 1.347914547304171e-06,
      "loss": 1.4917,
      "step": 95650
    },
    {
      "epoch": 48.657171922685656,
      "grad_norm": 37.807308197021484,
      "learning_rate": 1.3428280773143439e-06,
      "loss": 1.5056,
      "step": 95660
    },
    {
      "epoch": 48.66225839267548,
      "grad_norm": 47.997798919677734,
      "learning_rate": 1.337741607324517e-06,
      "loss": 1.4759,
      "step": 95670
    },
    {
      "epoch": 48.66734486266531,
      "grad_norm": 43.895851135253906,
      "learning_rate": 1.3326551373346898e-06,
      "loss": 1.5456,
      "step": 95680
    },
    {
      "epoch": 48.67243133265514,
      "grad_norm": 39.559295654296875,
      "learning_rate": 1.3275686673448626e-06,
      "loss": 1.5512,
      "step": 95690
    },
    {
      "epoch": 48.677517802644964,
      "grad_norm": 45.7379264831543,
      "learning_rate": 1.3224821973550357e-06,
      "loss": 1.4987,
      "step": 95700
    },
    {
      "epoch": 48.68260427263479,
      "grad_norm": 45.97394943237305,
      "learning_rate": 1.3173957273652086e-06,
      "loss": 1.6135,
      "step": 95710
    },
    {
      "epoch": 48.68769074262462,
      "grad_norm": 38.78490447998047,
      "learning_rate": 1.3123092573753816e-06,
      "loss": 1.4644,
      "step": 95720
    },
    {
      "epoch": 48.692777212614445,
      "grad_norm": 41.88380432128906,
      "learning_rate": 1.3072227873855545e-06,
      "loss": 1.6307,
      "step": 95730
    },
    {
      "epoch": 48.69786368260427,
      "grad_norm": 34.31365203857422,
      "learning_rate": 1.3021363173957274e-06,
      "loss": 1.4214,
      "step": 95740
    },
    {
      "epoch": 48.7029501525941,
      "grad_norm": 48.4473762512207,
      "learning_rate": 1.2970498474059004e-06,
      "loss": 1.5217,
      "step": 95750
    },
    {
      "epoch": 48.708036622583926,
      "grad_norm": 37.838836669921875,
      "learning_rate": 1.2919633774160733e-06,
      "loss": 1.3864,
      "step": 95760
    },
    {
      "epoch": 48.71312309257375,
      "grad_norm": 37.17190170288086,
      "learning_rate": 1.2868769074262461e-06,
      "loss": 1.5205,
      "step": 95770
    },
    {
      "epoch": 48.71820956256358,
      "grad_norm": 46.58270263671875,
      "learning_rate": 1.2817904374364192e-06,
      "loss": 1.6203,
      "step": 95780
    },
    {
      "epoch": 48.72329603255341,
      "grad_norm": 41.70423889160156,
      "learning_rate": 1.2767039674465923e-06,
      "loss": 1.4956,
      "step": 95790
    },
    {
      "epoch": 48.728382502543234,
      "grad_norm": 33.22488021850586,
      "learning_rate": 1.2716174974567651e-06,
      "loss": 1.4675,
      "step": 95800
    },
    {
      "epoch": 48.73346897253306,
      "grad_norm": 48.3160285949707,
      "learning_rate": 1.266531027466938e-06,
      "loss": 1.5924,
      "step": 95810
    },
    {
      "epoch": 48.73855544252289,
      "grad_norm": 43.702598571777344,
      "learning_rate": 1.261444557477111e-06,
      "loss": 1.5127,
      "step": 95820
    },
    {
      "epoch": 48.743641912512714,
      "grad_norm": 36.76554870605469,
      "learning_rate": 1.256358087487284e-06,
      "loss": 1.5391,
      "step": 95830
    },
    {
      "epoch": 48.74872838250254,
      "grad_norm": 46.909889221191406,
      "learning_rate": 1.2512716174974568e-06,
      "loss": 1.5507,
      "step": 95840
    },
    {
      "epoch": 48.75381485249237,
      "grad_norm": 40.44359588623047,
      "learning_rate": 1.2461851475076299e-06,
      "loss": 1.5381,
      "step": 95850
    },
    {
      "epoch": 48.758901322482195,
      "grad_norm": 37.24116134643555,
      "learning_rate": 1.2410986775178027e-06,
      "loss": 1.4891,
      "step": 95860
    },
    {
      "epoch": 48.76398779247202,
      "grad_norm": 34.73875427246094,
      "learning_rate": 1.2360122075279756e-06,
      "loss": 1.5387,
      "step": 95870
    },
    {
      "epoch": 48.76907426246185,
      "grad_norm": 32.91359329223633,
      "learning_rate": 1.2309257375381486e-06,
      "loss": 1.5611,
      "step": 95880
    },
    {
      "epoch": 48.774160732451676,
      "grad_norm": 46.06526565551758,
      "learning_rate": 1.2258392675483215e-06,
      "loss": 1.5432,
      "step": 95890
    },
    {
      "epoch": 48.7792472024415,
      "grad_norm": 46.65830612182617,
      "learning_rate": 1.2207527975584944e-06,
      "loss": 1.532,
      "step": 95900
    },
    {
      "epoch": 48.78433367243133,
      "grad_norm": 44.26915740966797,
      "learning_rate": 1.2156663275686674e-06,
      "loss": 1.4627,
      "step": 95910
    },
    {
      "epoch": 48.78942014242116,
      "grad_norm": 46.67909240722656,
      "learning_rate": 1.2105798575788403e-06,
      "loss": 1.474,
      "step": 95920
    },
    {
      "epoch": 48.794506612410984,
      "grad_norm": 33.32362365722656,
      "learning_rate": 1.2054933875890131e-06,
      "loss": 1.4781,
      "step": 95930
    },
    {
      "epoch": 48.79959308240081,
      "grad_norm": 42.55963897705078,
      "learning_rate": 1.2004069175991862e-06,
      "loss": 1.5522,
      "step": 95940
    },
    {
      "epoch": 48.80467955239064,
      "grad_norm": 46.22879409790039,
      "learning_rate": 1.1953204476093593e-06,
      "loss": 1.5357,
      "step": 95950
    },
    {
      "epoch": 48.809766022380465,
      "grad_norm": 36.93772888183594,
      "learning_rate": 1.1902339776195321e-06,
      "loss": 1.5581,
      "step": 95960
    },
    {
      "epoch": 48.81485249237029,
      "grad_norm": 38.22366714477539,
      "learning_rate": 1.1851475076297052e-06,
      "loss": 1.5203,
      "step": 95970
    },
    {
      "epoch": 48.81993896236012,
      "grad_norm": 44.29228591918945,
      "learning_rate": 1.180061037639878e-06,
      "loss": 1.433,
      "step": 95980
    },
    {
      "epoch": 48.825025432349946,
      "grad_norm": 58.59868621826172,
      "learning_rate": 1.174974567650051e-06,
      "loss": 1.5824,
      "step": 95990
    },
    {
      "epoch": 48.83011190233977,
      "grad_norm": 36.08042526245117,
      "learning_rate": 1.169888097660224e-06,
      "loss": 1.5074,
      "step": 96000
    },
    {
      "epoch": 48.8351983723296,
      "grad_norm": 43.770450592041016,
      "learning_rate": 1.1648016276703969e-06,
      "loss": 1.5457,
      "step": 96010
    },
    {
      "epoch": 48.84028484231943,
      "grad_norm": 34.69834899902344,
      "learning_rate": 1.1597151576805697e-06,
      "loss": 1.4877,
      "step": 96020
    },
    {
      "epoch": 48.845371312309254,
      "grad_norm": 42.852909088134766,
      "learning_rate": 1.1546286876907428e-06,
      "loss": 1.5163,
      "step": 96030
    },
    {
      "epoch": 48.85045778229908,
      "grad_norm": 44.99074172973633,
      "learning_rate": 1.1495422177009156e-06,
      "loss": 1.4937,
      "step": 96040
    },
    {
      "epoch": 48.85554425228891,
      "grad_norm": 45.43397903442383,
      "learning_rate": 1.1444557477110885e-06,
      "loss": 1.4888,
      "step": 96050
    },
    {
      "epoch": 48.86063072227874,
      "grad_norm": 45.46773147583008,
      "learning_rate": 1.1393692777212616e-06,
      "loss": 1.4867,
      "step": 96060
    },
    {
      "epoch": 48.86571719226856,
      "grad_norm": 41.38447952270508,
      "learning_rate": 1.1342828077314344e-06,
      "loss": 1.4886,
      "step": 96070
    },
    {
      "epoch": 48.870803662258396,
      "grad_norm": 40.74451446533203,
      "learning_rate": 1.1291963377416073e-06,
      "loss": 1.5521,
      "step": 96080
    },
    {
      "epoch": 48.87589013224822,
      "grad_norm": 41.50637435913086,
      "learning_rate": 1.1241098677517804e-06,
      "loss": 1.5404,
      "step": 96090
    },
    {
      "epoch": 48.88097660223805,
      "grad_norm": 32.25771713256836,
      "learning_rate": 1.1190233977619532e-06,
      "loss": 1.5172,
      "step": 96100
    },
    {
      "epoch": 48.88606307222788,
      "grad_norm": 34.917640686035156,
      "learning_rate": 1.113936927772126e-06,
      "loss": 1.3955,
      "step": 96110
    },
    {
      "epoch": 48.891149542217704,
      "grad_norm": 38.914363861083984,
      "learning_rate": 1.1088504577822991e-06,
      "loss": 1.5629,
      "step": 96120
    },
    {
      "epoch": 48.89623601220753,
      "grad_norm": 44.079166412353516,
      "learning_rate": 1.1037639877924722e-06,
      "loss": 1.5361,
      "step": 96130
    },
    {
      "epoch": 48.90132248219736,
      "grad_norm": 42.251007080078125,
      "learning_rate": 1.098677517802645e-06,
      "loss": 1.4893,
      "step": 96140
    },
    {
      "epoch": 48.906408952187185,
      "grad_norm": 38.61302947998047,
      "learning_rate": 1.093591047812818e-06,
      "loss": 1.5232,
      "step": 96150
    },
    {
      "epoch": 48.91149542217701,
      "grad_norm": 45.59626007080078,
      "learning_rate": 1.088504577822991e-06,
      "loss": 1.5565,
      "step": 96160
    },
    {
      "epoch": 48.91658189216684,
      "grad_norm": 47.1563720703125,
      "learning_rate": 1.0834181078331639e-06,
      "loss": 1.5728,
      "step": 96170
    },
    {
      "epoch": 48.921668362156666,
      "grad_norm": 39.93404006958008,
      "learning_rate": 1.0783316378433367e-06,
      "loss": 1.534,
      "step": 96180
    },
    {
      "epoch": 48.92675483214649,
      "grad_norm": 41.826141357421875,
      "learning_rate": 1.0732451678535098e-06,
      "loss": 1.4892,
      "step": 96190
    },
    {
      "epoch": 48.93184130213632,
      "grad_norm": 43.49599838256836,
      "learning_rate": 1.0681586978636826e-06,
      "loss": 1.4871,
      "step": 96200
    },
    {
      "epoch": 48.93692777212615,
      "grad_norm": 44.96360778808594,
      "learning_rate": 1.0630722278738555e-06,
      "loss": 1.5436,
      "step": 96210
    },
    {
      "epoch": 48.942014242115974,
      "grad_norm": 43.68711853027344,
      "learning_rate": 1.0579857578840286e-06,
      "loss": 1.5134,
      "step": 96220
    },
    {
      "epoch": 48.9471007121058,
      "grad_norm": 53.09291076660156,
      "learning_rate": 1.0528992878942014e-06,
      "loss": 1.6006,
      "step": 96230
    },
    {
      "epoch": 48.95218718209563,
      "grad_norm": 31.520227432250977,
      "learning_rate": 1.0478128179043743e-06,
      "loss": 1.577,
      "step": 96240
    },
    {
      "epoch": 48.957273652085455,
      "grad_norm": 42.539913177490234,
      "learning_rate": 1.0427263479145474e-06,
      "loss": 1.6002,
      "step": 96250
    },
    {
      "epoch": 48.96236012207528,
      "grad_norm": 39.84145736694336,
      "learning_rate": 1.0376398779247202e-06,
      "loss": 1.5725,
      "step": 96260
    },
    {
      "epoch": 48.96744659206511,
      "grad_norm": 39.990116119384766,
      "learning_rate": 1.032553407934893e-06,
      "loss": 1.4228,
      "step": 96270
    },
    {
      "epoch": 48.972533062054936,
      "grad_norm": 35.830753326416016,
      "learning_rate": 1.0274669379450664e-06,
      "loss": 1.4812,
      "step": 96280
    },
    {
      "epoch": 48.97761953204476,
      "grad_norm": 53.03485107421875,
      "learning_rate": 1.0223804679552392e-06,
      "loss": 1.5808,
      "step": 96290
    },
    {
      "epoch": 48.98270600203459,
      "grad_norm": 61.389366149902344,
      "learning_rate": 1.017293997965412e-06,
      "loss": 1.5853,
      "step": 96300
    },
    {
      "epoch": 48.98779247202442,
      "grad_norm": 38.186336517333984,
      "learning_rate": 1.0122075279755851e-06,
      "loss": 1.4391,
      "step": 96310
    },
    {
      "epoch": 48.992878942014244,
      "grad_norm": 47.42745590209961,
      "learning_rate": 1.007121057985758e-06,
      "loss": 1.5443,
      "step": 96320
    },
    {
      "epoch": 48.99796541200407,
      "grad_norm": 42.072998046875,
      "learning_rate": 1.0020345879959309e-06,
      "loss": 1.514,
      "step": 96330
    },
    {
      "epoch": 49.0,
      "eval_loss": 5.1166276931762695,
      "eval_runtime": 2.764,
      "eval_samples_per_second": 1003.988,
      "eval_steps_per_second": 125.544,
      "step": 96334
    },
    {
      "epoch": 49.0030518819939,
      "grad_norm": 43.56007385253906,
      "learning_rate": 9.96948118006104e-07,
      "loss": 1.4845,
      "step": 96340
    },
    {
      "epoch": 49.008138351983725,
      "grad_norm": 41.05961608886719,
      "learning_rate": 9.918616480162768e-07,
      "loss": 1.4827,
      "step": 96350
    },
    {
      "epoch": 49.01322482197355,
      "grad_norm": 51.7258186340332,
      "learning_rate": 9.867751780264496e-07,
      "loss": 1.5305,
      "step": 96360
    },
    {
      "epoch": 49.01831129196338,
      "grad_norm": 38.11032485961914,
      "learning_rate": 9.816887080366227e-07,
      "loss": 1.537,
      "step": 96370
    },
    {
      "epoch": 49.023397761953206,
      "grad_norm": 39.843544006347656,
      "learning_rate": 9.766022380467956e-07,
      "loss": 1.4666,
      "step": 96380
    },
    {
      "epoch": 49.02848423194303,
      "grad_norm": 55.9447021484375,
      "learning_rate": 9.715157680569684e-07,
      "loss": 1.5538,
      "step": 96390
    },
    {
      "epoch": 49.03357070193286,
      "grad_norm": 40.98356246948242,
      "learning_rate": 9.664292980671415e-07,
      "loss": 1.5108,
      "step": 96400
    },
    {
      "epoch": 49.03865717192269,
      "grad_norm": 43.79412841796875,
      "learning_rate": 9.613428280773144e-07,
      "loss": 1.5205,
      "step": 96410
    },
    {
      "epoch": 49.04374364191251,
      "grad_norm": 34.8179931640625,
      "learning_rate": 9.562563580874872e-07,
      "loss": 1.5417,
      "step": 96420
    },
    {
      "epoch": 49.04883011190234,
      "grad_norm": 48.68169403076172,
      "learning_rate": 9.511698880976602e-07,
      "loss": 1.5044,
      "step": 96430
    },
    {
      "epoch": 49.05391658189217,
      "grad_norm": 46.180362701416016,
      "learning_rate": 9.460834181078332e-07,
      "loss": 1.5564,
      "step": 96440
    },
    {
      "epoch": 49.059003051881994,
      "grad_norm": 38.34090042114258,
      "learning_rate": 9.409969481180062e-07,
      "loss": 1.5081,
      "step": 96450
    },
    {
      "epoch": 49.06408952187182,
      "grad_norm": 41.06688690185547,
      "learning_rate": 9.359104781281792e-07,
      "loss": 1.5085,
      "step": 96460
    },
    {
      "epoch": 49.06917599186165,
      "grad_norm": 41.953269958496094,
      "learning_rate": 9.30824008138352e-07,
      "loss": 1.4727,
      "step": 96470
    },
    {
      "epoch": 49.074262461851475,
      "grad_norm": 37.790496826171875,
      "learning_rate": 9.25737538148525e-07,
      "loss": 1.4835,
      "step": 96480
    },
    {
      "epoch": 49.0793489318413,
      "grad_norm": 46.28541564941406,
      "learning_rate": 9.20651068158698e-07,
      "loss": 1.6021,
      "step": 96490
    },
    {
      "epoch": 49.08443540183113,
      "grad_norm": 39.47475051879883,
      "learning_rate": 9.155645981688708e-07,
      "loss": 1.4554,
      "step": 96500
    },
    {
      "epoch": 49.089521871820956,
      "grad_norm": 52.67396926879883,
      "learning_rate": 9.104781281790438e-07,
      "loss": 1.5084,
      "step": 96510
    },
    {
      "epoch": 49.09460834181078,
      "grad_norm": 38.83731460571289,
      "learning_rate": 9.053916581892167e-07,
      "loss": 1.5088,
      "step": 96520
    },
    {
      "epoch": 49.09969481180061,
      "grad_norm": 39.72653579711914,
      "learning_rate": 9.003051881993896e-07,
      "loss": 1.4471,
      "step": 96530
    },
    {
      "epoch": 49.10478128179044,
      "grad_norm": 38.282737731933594,
      "learning_rate": 8.952187182095626e-07,
      "loss": 1.5085,
      "step": 96540
    },
    {
      "epoch": 49.109867751780264,
      "grad_norm": 40.89369201660156,
      "learning_rate": 8.901322482197355e-07,
      "loss": 1.5152,
      "step": 96550
    },
    {
      "epoch": 49.11495422177009,
      "grad_norm": 40.33875274658203,
      "learning_rate": 8.850457782299084e-07,
      "loss": 1.5539,
      "step": 96560
    },
    {
      "epoch": 49.12004069175992,
      "grad_norm": 55.88255310058594,
      "learning_rate": 8.799593082400814e-07,
      "loss": 1.4801,
      "step": 96570
    },
    {
      "epoch": 49.125127161749745,
      "grad_norm": 38.922157287597656,
      "learning_rate": 8.748728382502543e-07,
      "loss": 1.4221,
      "step": 96580
    },
    {
      "epoch": 49.13021363173957,
      "grad_norm": 42.10750198364258,
      "learning_rate": 8.697863682604272e-07,
      "loss": 1.5192,
      "step": 96590
    },
    {
      "epoch": 49.1353001017294,
      "grad_norm": 47.19512176513672,
      "learning_rate": 8.646998982706004e-07,
      "loss": 1.54,
      "step": 96600
    },
    {
      "epoch": 49.140386571719226,
      "grad_norm": 38.47539138793945,
      "learning_rate": 8.596134282807732e-07,
      "loss": 1.5043,
      "step": 96610
    },
    {
      "epoch": 49.14547304170905,
      "grad_norm": 54.52251052856445,
      "learning_rate": 8.545269582909462e-07,
      "loss": 1.5132,
      "step": 96620
    },
    {
      "epoch": 49.15055951169888,
      "grad_norm": 35.60345458984375,
      "learning_rate": 8.494404883011191e-07,
      "loss": 1.4937,
      "step": 96630
    },
    {
      "epoch": 49.15564598168871,
      "grad_norm": 41.177486419677734,
      "learning_rate": 8.44354018311292e-07,
      "loss": 1.4457,
      "step": 96640
    },
    {
      "epoch": 49.160732451678534,
      "grad_norm": 38.29490661621094,
      "learning_rate": 8.39267548321465e-07,
      "loss": 1.4572,
      "step": 96650
    },
    {
      "epoch": 49.16581892166836,
      "grad_norm": 47.7564582824707,
      "learning_rate": 8.341810783316379e-07,
      "loss": 1.5543,
      "step": 96660
    },
    {
      "epoch": 49.17090539165819,
      "grad_norm": 37.619110107421875,
      "learning_rate": 8.290946083418108e-07,
      "loss": 1.467,
      "step": 96670
    },
    {
      "epoch": 49.175991861648015,
      "grad_norm": 33.56734848022461,
      "learning_rate": 8.240081383519837e-07,
      "loss": 1.6069,
      "step": 96680
    },
    {
      "epoch": 49.18107833163784,
      "grad_norm": 37.96586227416992,
      "learning_rate": 8.189216683621567e-07,
      "loss": 1.394,
      "step": 96690
    },
    {
      "epoch": 49.18616480162767,
      "grad_norm": 34.23133087158203,
      "learning_rate": 8.138351983723296e-07,
      "loss": 1.6017,
      "step": 96700
    },
    {
      "epoch": 49.191251271617496,
      "grad_norm": 38.30701446533203,
      "learning_rate": 8.087487283825025e-07,
      "loss": 1.4898,
      "step": 96710
    },
    {
      "epoch": 49.19633774160732,
      "grad_norm": 31.663135528564453,
      "learning_rate": 8.036622583926755e-07,
      "loss": 1.5951,
      "step": 96720
    },
    {
      "epoch": 49.20142421159715,
      "grad_norm": 41.27459716796875,
      "learning_rate": 7.985757884028484e-07,
      "loss": 1.5463,
      "step": 96730
    },
    {
      "epoch": 49.20651068158698,
      "grad_norm": 39.86172103881836,
      "learning_rate": 7.934893184130213e-07,
      "loss": 1.5854,
      "step": 96740
    },
    {
      "epoch": 49.211597151576804,
      "grad_norm": 43.25644302368164,
      "learning_rate": 7.884028484231943e-07,
      "loss": 1.6365,
      "step": 96750
    },
    {
      "epoch": 49.21668362156663,
      "grad_norm": 32.299476623535156,
      "learning_rate": 7.833163784333674e-07,
      "loss": 1.449,
      "step": 96760
    },
    {
      "epoch": 49.22177009155646,
      "grad_norm": 34.79375457763672,
      "learning_rate": 7.782299084435402e-07,
      "loss": 1.5513,
      "step": 96770
    },
    {
      "epoch": 49.226856561546285,
      "grad_norm": 39.00508117675781,
      "learning_rate": 7.731434384537132e-07,
      "loss": 1.5215,
      "step": 96780
    },
    {
      "epoch": 49.23194303153611,
      "grad_norm": 56.96639633178711,
      "learning_rate": 7.68056968463886e-07,
      "loss": 1.5068,
      "step": 96790
    },
    {
      "epoch": 49.23702950152594,
      "grad_norm": 39.8257942199707,
      "learning_rate": 7.629704984740591e-07,
      "loss": 1.496,
      "step": 96800
    },
    {
      "epoch": 49.242115971515766,
      "grad_norm": 54.41780471801758,
      "learning_rate": 7.57884028484232e-07,
      "loss": 1.466,
      "step": 96810
    },
    {
      "epoch": 49.24720244150559,
      "grad_norm": 39.48025894165039,
      "learning_rate": 7.527975584944049e-07,
      "loss": 1.4704,
      "step": 96820
    },
    {
      "epoch": 49.25228891149542,
      "grad_norm": 39.809200286865234,
      "learning_rate": 7.477110885045779e-07,
      "loss": 1.5051,
      "step": 96830
    },
    {
      "epoch": 49.25737538148525,
      "grad_norm": 39.02171325683594,
      "learning_rate": 7.426246185147507e-07,
      "loss": 1.5691,
      "step": 96840
    },
    {
      "epoch": 49.262461851475074,
      "grad_norm": 50.342689514160156,
      "learning_rate": 7.375381485249237e-07,
      "loss": 1.4637,
      "step": 96850
    },
    {
      "epoch": 49.2675483214649,
      "grad_norm": 46.239906311035156,
      "learning_rate": 7.324516785350967e-07,
      "loss": 1.5135,
      "step": 96860
    },
    {
      "epoch": 49.27263479145473,
      "grad_norm": 43.73748016357422,
      "learning_rate": 7.273652085452695e-07,
      "loss": 1.5061,
      "step": 96870
    },
    {
      "epoch": 49.277721261444555,
      "grad_norm": 54.81074142456055,
      "learning_rate": 7.222787385554425e-07,
      "loss": 1.4676,
      "step": 96880
    },
    {
      "epoch": 49.28280773143438,
      "grad_norm": 39.012603759765625,
      "learning_rate": 7.171922685656156e-07,
      "loss": 1.4906,
      "step": 96890
    },
    {
      "epoch": 49.28789420142421,
      "grad_norm": 33.680908203125,
      "learning_rate": 7.121057985757884e-07,
      "loss": 1.4737,
      "step": 96900
    },
    {
      "epoch": 49.292980671414035,
      "grad_norm": 46.62953186035156,
      "learning_rate": 7.070193285859614e-07,
      "loss": 1.5116,
      "step": 96910
    },
    {
      "epoch": 49.29806714140386,
      "grad_norm": 56.801204681396484,
      "learning_rate": 7.019328585961344e-07,
      "loss": 1.4604,
      "step": 96920
    },
    {
      "epoch": 49.30315361139369,
      "grad_norm": 51.4744873046875,
      "learning_rate": 6.968463886063072e-07,
      "loss": 1.5178,
      "step": 96930
    },
    {
      "epoch": 49.308240081383516,
      "grad_norm": 50.103477478027344,
      "learning_rate": 6.917599186164802e-07,
      "loss": 1.5345,
      "step": 96940
    },
    {
      "epoch": 49.31332655137334,
      "grad_norm": 39.966880798339844,
      "learning_rate": 6.866734486266531e-07,
      "loss": 1.5252,
      "step": 96950
    },
    {
      "epoch": 49.31841302136317,
      "grad_norm": 35.67676544189453,
      "learning_rate": 6.81586978636826e-07,
      "loss": 1.4461,
      "step": 96960
    },
    {
      "epoch": 49.323499491353004,
      "grad_norm": 42.92863845825195,
      "learning_rate": 6.765005086469991e-07,
      "loss": 1.4919,
      "step": 96970
    },
    {
      "epoch": 49.32858596134283,
      "grad_norm": 44.21934509277344,
      "learning_rate": 6.714140386571719e-07,
      "loss": 1.5399,
      "step": 96980
    },
    {
      "epoch": 49.33367243133266,
      "grad_norm": 46.6609001159668,
      "learning_rate": 6.663275686673449e-07,
      "loss": 1.5782,
      "step": 96990
    },
    {
      "epoch": 49.338758901322485,
      "grad_norm": 36.694252014160156,
      "learning_rate": 6.612410986775179e-07,
      "loss": 1.4698,
      "step": 97000
    },
    {
      "epoch": 49.34384537131231,
      "grad_norm": 42.92012023925781,
      "learning_rate": 6.561546286876908e-07,
      "loss": 1.5522,
      "step": 97010
    },
    {
      "epoch": 49.34893184130214,
      "grad_norm": 47.13905715942383,
      "learning_rate": 6.510681586978637e-07,
      "loss": 1.5034,
      "step": 97020
    },
    {
      "epoch": 49.354018311291966,
      "grad_norm": 50.29478454589844,
      "learning_rate": 6.459816887080366e-07,
      "loss": 1.5311,
      "step": 97030
    },
    {
      "epoch": 49.35910478128179,
      "grad_norm": 55.334197998046875,
      "learning_rate": 6.408952187182096e-07,
      "loss": 1.4925,
      "step": 97040
    },
    {
      "epoch": 49.36419125127162,
      "grad_norm": 42.32926559448242,
      "learning_rate": 6.358087487283826e-07,
      "loss": 1.5531,
      "step": 97050
    },
    {
      "epoch": 49.36927772126145,
      "grad_norm": 50.553985595703125,
      "learning_rate": 6.307222787385555e-07,
      "loss": 1.5835,
      "step": 97060
    },
    {
      "epoch": 49.374364191251274,
      "grad_norm": 45.889678955078125,
      "learning_rate": 6.256358087487284e-07,
      "loss": 1.4981,
      "step": 97070
    },
    {
      "epoch": 49.3794506612411,
      "grad_norm": 43.94552993774414,
      "learning_rate": 6.205493387589014e-07,
      "loss": 1.4949,
      "step": 97080
    },
    {
      "epoch": 49.38453713123093,
      "grad_norm": 37.01838302612305,
      "learning_rate": 6.154628687690743e-07,
      "loss": 1.5532,
      "step": 97090
    },
    {
      "epoch": 49.389623601220755,
      "grad_norm": 46.883609771728516,
      "learning_rate": 6.103763987792472e-07,
      "loss": 1.5602,
      "step": 97100
    },
    {
      "epoch": 49.39471007121058,
      "grad_norm": 36.613548278808594,
      "learning_rate": 6.052899287894201e-07,
      "loss": 1.4539,
      "step": 97110
    },
    {
      "epoch": 49.39979654120041,
      "grad_norm": 43.69220733642578,
      "learning_rate": 6.002034587995931e-07,
      "loss": 1.5331,
      "step": 97120
    },
    {
      "epoch": 49.404883011190236,
      "grad_norm": 43.07814025878906,
      "learning_rate": 5.951169888097661e-07,
      "loss": 1.544,
      "step": 97130
    },
    {
      "epoch": 49.40996948118006,
      "grad_norm": 41.04734802246094,
      "learning_rate": 5.90030518819939e-07,
      "loss": 1.5176,
      "step": 97140
    },
    {
      "epoch": 49.41505595116989,
      "grad_norm": 35.304954528808594,
      "learning_rate": 5.84944048830112e-07,
      "loss": 1.5864,
      "step": 97150
    },
    {
      "epoch": 49.42014242115972,
      "grad_norm": 37.02009201049805,
      "learning_rate": 5.798575788402849e-07,
      "loss": 1.5541,
      "step": 97160
    },
    {
      "epoch": 49.425228891149544,
      "grad_norm": 45.55807113647461,
      "learning_rate": 5.747711088504578e-07,
      "loss": 1.6242,
      "step": 97170
    },
    {
      "epoch": 49.43031536113937,
      "grad_norm": 49.092689514160156,
      "learning_rate": 5.696846388606308e-07,
      "loss": 1.5116,
      "step": 97180
    },
    {
      "epoch": 49.4354018311292,
      "grad_norm": 40.46063995361328,
      "learning_rate": 5.645981688708036e-07,
      "loss": 1.4534,
      "step": 97190
    },
    {
      "epoch": 49.440488301119025,
      "grad_norm": 45.129085540771484,
      "learning_rate": 5.595116988809766e-07,
      "loss": 1.5473,
      "step": 97200
    },
    {
      "epoch": 49.44557477110885,
      "grad_norm": 35.80038070678711,
      "learning_rate": 5.544252288911496e-07,
      "loss": 1.4882,
      "step": 97210
    },
    {
      "epoch": 49.45066124109868,
      "grad_norm": 51.13239288330078,
      "learning_rate": 5.493387589013225e-07,
      "loss": 1.4679,
      "step": 97220
    },
    {
      "epoch": 49.455747711088506,
      "grad_norm": 41.261173248291016,
      "learning_rate": 5.442522889114955e-07,
      "loss": 1.4631,
      "step": 97230
    },
    {
      "epoch": 49.46083418107833,
      "grad_norm": 41.028167724609375,
      "learning_rate": 5.391658189216684e-07,
      "loss": 1.6028,
      "step": 97240
    },
    {
      "epoch": 49.46592065106816,
      "grad_norm": 40.80573272705078,
      "learning_rate": 5.340793489318413e-07,
      "loss": 1.5058,
      "step": 97250
    },
    {
      "epoch": 49.47100712105799,
      "grad_norm": 39.941890716552734,
      "learning_rate": 5.289928789420143e-07,
      "loss": 1.5335,
      "step": 97260
    },
    {
      "epoch": 49.476093591047814,
      "grad_norm": 38.18297576904297,
      "learning_rate": 5.239064089521871e-07,
      "loss": 1.4592,
      "step": 97270
    },
    {
      "epoch": 49.48118006103764,
      "grad_norm": 66.98664855957031,
      "learning_rate": 5.188199389623601e-07,
      "loss": 1.5616,
      "step": 97280
    },
    {
      "epoch": 49.48626653102747,
      "grad_norm": 38.71837615966797,
      "learning_rate": 5.137334689725332e-07,
      "loss": 1.4565,
      "step": 97290
    },
    {
      "epoch": 49.491353001017295,
      "grad_norm": 42.0516471862793,
      "learning_rate": 5.08646998982706e-07,
      "loss": 1.4856,
      "step": 97300
    },
    {
      "epoch": 49.49643947100712,
      "grad_norm": 46.340553283691406,
      "learning_rate": 5.03560528992879e-07,
      "loss": 1.5339,
      "step": 97310
    },
    {
      "epoch": 49.50152594099695,
      "grad_norm": 40.005653381347656,
      "learning_rate": 4.98474059003052e-07,
      "loss": 1.5343,
      "step": 97320
    },
    {
      "epoch": 49.506612410986776,
      "grad_norm": 43.34125518798828,
      "learning_rate": 4.933875890132248e-07,
      "loss": 1.5029,
      "step": 97330
    },
    {
      "epoch": 49.5116988809766,
      "grad_norm": 29.45884132385254,
      "learning_rate": 4.883011190233978e-07,
      "loss": 1.514,
      "step": 97340
    },
    {
      "epoch": 49.51678535096643,
      "grad_norm": 48.331687927246094,
      "learning_rate": 4.832146490335707e-07,
      "loss": 1.5004,
      "step": 97350
    },
    {
      "epoch": 49.52187182095626,
      "grad_norm": 44.00035095214844,
      "learning_rate": 4.781281790437436e-07,
      "loss": 1.4979,
      "step": 97360
    },
    {
      "epoch": 49.526958290946084,
      "grad_norm": 42.95918655395508,
      "learning_rate": 4.730417090539166e-07,
      "loss": 1.6155,
      "step": 97370
    },
    {
      "epoch": 49.53204476093591,
      "grad_norm": 44.08292770385742,
      "learning_rate": 4.679552390640896e-07,
      "loss": 1.4503,
      "step": 97380
    },
    {
      "epoch": 49.53713123092574,
      "grad_norm": 47.82990264892578,
      "learning_rate": 4.628687690742625e-07,
      "loss": 1.4852,
      "step": 97390
    },
    {
      "epoch": 49.542217700915565,
      "grad_norm": 42.434146881103516,
      "learning_rate": 4.577822990844354e-07,
      "loss": 1.5459,
      "step": 97400
    },
    {
      "epoch": 49.54730417090539,
      "grad_norm": 40.22382736206055,
      "learning_rate": 4.5269582909460837e-07,
      "loss": 1.456,
      "step": 97410
    },
    {
      "epoch": 49.55239064089522,
      "grad_norm": 44.20616912841797,
      "learning_rate": 4.476093591047813e-07,
      "loss": 1.5136,
      "step": 97420
    },
    {
      "epoch": 49.557477110885046,
      "grad_norm": 45.7608642578125,
      "learning_rate": 4.425228891149542e-07,
      "loss": 1.514,
      "step": 97430
    },
    {
      "epoch": 49.56256358087487,
      "grad_norm": 34.24717712402344,
      "learning_rate": 4.3743641912512716e-07,
      "loss": 1.4873,
      "step": 97440
    },
    {
      "epoch": 49.5676500508647,
      "grad_norm": 37.23749923706055,
      "learning_rate": 4.323499491353002e-07,
      "loss": 1.5028,
      "step": 97450
    },
    {
      "epoch": 49.57273652085453,
      "grad_norm": 38.34147262573242,
      "learning_rate": 4.272634791454731e-07,
      "loss": 1.5493,
      "step": 97460
    },
    {
      "epoch": 49.57782299084435,
      "grad_norm": 40.665836334228516,
      "learning_rate": 4.22177009155646e-07,
      "loss": 1.4677,
      "step": 97470
    },
    {
      "epoch": 49.58290946083418,
      "grad_norm": 38.55234146118164,
      "learning_rate": 4.1709053916581896e-07,
      "loss": 1.4461,
      "step": 97480
    },
    {
      "epoch": 49.58799593082401,
      "grad_norm": 41.475791931152344,
      "learning_rate": 4.120040691759919e-07,
      "loss": 1.5076,
      "step": 97490
    },
    {
      "epoch": 49.593082400813834,
      "grad_norm": 43.422325134277344,
      "learning_rate": 4.069175991861648e-07,
      "loss": 1.5477,
      "step": 97500
    },
    {
      "epoch": 49.59816887080366,
      "grad_norm": 41.84269332885742,
      "learning_rate": 4.0183112919633775e-07,
      "loss": 1.5883,
      "step": 97510
    },
    {
      "epoch": 49.60325534079349,
      "grad_norm": 42.91409683227539,
      "learning_rate": 3.9674465920651066e-07,
      "loss": 1.5421,
      "step": 97520
    },
    {
      "epoch": 49.608341810783315,
      "grad_norm": 34.90620040893555,
      "learning_rate": 3.916581892166837e-07,
      "loss": 1.4958,
      "step": 97530
    },
    {
      "epoch": 49.61342828077314,
      "grad_norm": 44.9826774597168,
      "learning_rate": 3.865717192268566e-07,
      "loss": 1.4616,
      "step": 97540
    },
    {
      "epoch": 49.61851475076297,
      "grad_norm": 38.198646545410156,
      "learning_rate": 3.8148524923702955e-07,
      "loss": 1.5613,
      "step": 97550
    },
    {
      "epoch": 49.623601220752796,
      "grad_norm": 39.1563835144043,
      "learning_rate": 3.7639877924720246e-07,
      "loss": 1.5309,
      "step": 97560
    },
    {
      "epoch": 49.62868769074262,
      "grad_norm": 41.8530158996582,
      "learning_rate": 3.713123092573754e-07,
      "loss": 1.4891,
      "step": 97570
    },
    {
      "epoch": 49.63377416073245,
      "grad_norm": 39.18427658081055,
      "learning_rate": 3.6622583926754834e-07,
      "loss": 1.542,
      "step": 97580
    },
    {
      "epoch": 49.63886063072228,
      "grad_norm": 43.0590934753418,
      "learning_rate": 3.6113936927772125e-07,
      "loss": 1.5156,
      "step": 97590
    },
    {
      "epoch": 49.643947100712104,
      "grad_norm": 36.018062591552734,
      "learning_rate": 3.560528992878942e-07,
      "loss": 1.5242,
      "step": 97600
    },
    {
      "epoch": 49.64903357070193,
      "grad_norm": 48.176456451416016,
      "learning_rate": 3.509664292980672e-07,
      "loss": 1.4895,
      "step": 97610
    },
    {
      "epoch": 49.65412004069176,
      "grad_norm": 50.236854553222656,
      "learning_rate": 3.458799593082401e-07,
      "loss": 1.5128,
      "step": 97620
    },
    {
      "epoch": 49.659206510681585,
      "grad_norm": 49.71867370605469,
      "learning_rate": 3.40793489318413e-07,
      "loss": 1.5466,
      "step": 97630
    },
    {
      "epoch": 49.66429298067141,
      "grad_norm": 45.078189849853516,
      "learning_rate": 3.3570701932858596e-07,
      "loss": 1.5601,
      "step": 97640
    },
    {
      "epoch": 49.66937945066124,
      "grad_norm": 34.825782775878906,
      "learning_rate": 3.3062054933875893e-07,
      "loss": 1.4967,
      "step": 97650
    },
    {
      "epoch": 49.674465920651066,
      "grad_norm": 44.219703674316406,
      "learning_rate": 3.2553407934893184e-07,
      "loss": 1.498,
      "step": 97660
    },
    {
      "epoch": 49.67955239064089,
      "grad_norm": 46.79476547241211,
      "learning_rate": 3.204476093591048e-07,
      "loss": 1.5644,
      "step": 97670
    },
    {
      "epoch": 49.68463886063072,
      "grad_norm": 41.18278884887695,
      "learning_rate": 3.1536113936927777e-07,
      "loss": 1.5329,
      "step": 97680
    },
    {
      "epoch": 49.68972533062055,
      "grad_norm": 38.76044464111328,
      "learning_rate": 3.102746693794507e-07,
      "loss": 1.5059,
      "step": 97690
    },
    {
      "epoch": 49.694811800610374,
      "grad_norm": 34.451080322265625,
      "learning_rate": 3.051881993896236e-07,
      "loss": 1.4739,
      "step": 97700
    },
    {
      "epoch": 49.6998982706002,
      "grad_norm": 47.519718170166016,
      "learning_rate": 3.0010172939979655e-07,
      "loss": 1.5772,
      "step": 97710
    },
    {
      "epoch": 49.70498474059003,
      "grad_norm": 43.37468338012695,
      "learning_rate": 2.950152594099695e-07,
      "loss": 1.5707,
      "step": 97720
    },
    {
      "epoch": 49.710071210579855,
      "grad_norm": 32.606632232666016,
      "learning_rate": 2.8992878942014243e-07,
      "loss": 1.5116,
      "step": 97730
    },
    {
      "epoch": 49.71515768056968,
      "grad_norm": 51.64072799682617,
      "learning_rate": 2.848423194303154e-07,
      "loss": 1.5051,
      "step": 97740
    },
    {
      "epoch": 49.72024415055951,
      "grad_norm": 49.59694290161133,
      "learning_rate": 2.797558494404883e-07,
      "loss": 1.5851,
      "step": 97750
    },
    {
      "epoch": 49.725330620549336,
      "grad_norm": 36.01083755493164,
      "learning_rate": 2.7466937945066127e-07,
      "loss": 1.499,
      "step": 97760
    },
    {
      "epoch": 49.73041709053916,
      "grad_norm": 42.062843322753906,
      "learning_rate": 2.695829094608342e-07,
      "loss": 1.5669,
      "step": 97770
    },
    {
      "epoch": 49.73550356052899,
      "grad_norm": 51.34760665893555,
      "learning_rate": 2.6449643947100714e-07,
      "loss": 1.471,
      "step": 97780
    },
    {
      "epoch": 49.74059003051882,
      "grad_norm": 36.25880432128906,
      "learning_rate": 2.5940996948118005e-07,
      "loss": 1.5751,
      "step": 97790
    },
    {
      "epoch": 49.745676500508644,
      "grad_norm": 48.03308868408203,
      "learning_rate": 2.54323499491353e-07,
      "loss": 1.4872,
      "step": 97800
    },
    {
      "epoch": 49.75076297049847,
      "grad_norm": 32.48625946044922,
      "learning_rate": 2.49237029501526e-07,
      "loss": 1.4406,
      "step": 97810
    },
    {
      "epoch": 49.7558494404883,
      "grad_norm": 42.16415023803711,
      "learning_rate": 2.441505595116989e-07,
      "loss": 1.5067,
      "step": 97820
    },
    {
      "epoch": 49.760935910478125,
      "grad_norm": 37.787906646728516,
      "learning_rate": 2.390640895218718e-07,
      "loss": 1.5627,
      "step": 97830
    },
    {
      "epoch": 49.76602238046795,
      "grad_norm": 34.13755798339844,
      "learning_rate": 2.339776195320448e-07,
      "loss": 1.4759,
      "step": 97840
    },
    {
      "epoch": 49.77110885045778,
      "grad_norm": 38.182762145996094,
      "learning_rate": 2.288911495422177e-07,
      "loss": 1.5939,
      "step": 97850
    },
    {
      "epoch": 49.77619532044761,
      "grad_norm": 37.166751861572266,
      "learning_rate": 2.2380467955239064e-07,
      "loss": 1.5315,
      "step": 97860
    },
    {
      "epoch": 49.78128179043744,
      "grad_norm": 35.83713912963867,
      "learning_rate": 2.1871820956256358e-07,
      "loss": 1.4582,
      "step": 97870
    },
    {
      "epoch": 49.78636826042727,
      "grad_norm": 44.927978515625,
      "learning_rate": 2.1363173957273654e-07,
      "loss": 1.5359,
      "step": 97880
    },
    {
      "epoch": 49.791454730417094,
      "grad_norm": 52.7019157409668,
      "learning_rate": 2.0854526958290948e-07,
      "loss": 1.4953,
      "step": 97890
    },
    {
      "epoch": 49.79654120040692,
      "grad_norm": 38.85926818847656,
      "learning_rate": 2.034587995930824e-07,
      "loss": 1.4685,
      "step": 97900
    },
    {
      "epoch": 49.80162767039675,
      "grad_norm": 42.546260833740234,
      "learning_rate": 1.9837232960325533e-07,
      "loss": 1.4269,
      "step": 97910
    },
    {
      "epoch": 49.806714140386575,
      "grad_norm": 46.424983978271484,
      "learning_rate": 1.932858596134283e-07,
      "loss": 1.4755,
      "step": 97920
    },
    {
      "epoch": 49.8118006103764,
      "grad_norm": 31.237192153930664,
      "learning_rate": 1.8819938962360123e-07,
      "loss": 1.5288,
      "step": 97930
    },
    {
      "epoch": 49.81688708036623,
      "grad_norm": 33.67061233520508,
      "learning_rate": 1.8311291963377417e-07,
      "loss": 1.4783,
      "step": 97940
    },
    {
      "epoch": 49.821973550356056,
      "grad_norm": 44.61705017089844,
      "learning_rate": 1.780264496439471e-07,
      "loss": 1.492,
      "step": 97950
    },
    {
      "epoch": 49.82706002034588,
      "grad_norm": 33.34596252441406,
      "learning_rate": 1.7293997965412004e-07,
      "loss": 1.475,
      "step": 97960
    },
    {
      "epoch": 49.83214649033571,
      "grad_norm": 48.922882080078125,
      "learning_rate": 1.6785350966429298e-07,
      "loss": 1.5531,
      "step": 97970
    },
    {
      "epoch": 49.83723296032554,
      "grad_norm": 50.82876968383789,
      "learning_rate": 1.6276703967446592e-07,
      "loss": 1.532,
      "step": 97980
    },
    {
      "epoch": 49.842319430315364,
      "grad_norm": 39.898101806640625,
      "learning_rate": 1.5768056968463888e-07,
      "loss": 1.4966,
      "step": 97990
    },
    {
      "epoch": 49.84740590030519,
      "grad_norm": 49.21501541137695,
      "learning_rate": 1.525940996948118e-07,
      "loss": 1.5944,
      "step": 98000
    },
    {
      "epoch": 49.85249237029502,
      "grad_norm": 38.12929916381836,
      "learning_rate": 1.4750762970498476e-07,
      "loss": 1.6171,
      "step": 98010
    },
    {
      "epoch": 49.857578840284845,
      "grad_norm": 42.456809997558594,
      "learning_rate": 1.424211597151577e-07,
      "loss": 1.5573,
      "step": 98020
    },
    {
      "epoch": 49.86266531027467,
      "grad_norm": 39.96305847167969,
      "learning_rate": 1.3733468972533063e-07,
      "loss": 1.4984,
      "step": 98030
    },
    {
      "epoch": 49.8677517802645,
      "grad_norm": 40.52434539794922,
      "learning_rate": 1.3224821973550357e-07,
      "loss": 1.5888,
      "step": 98040
    },
    {
      "epoch": 49.872838250254325,
      "grad_norm": 36.40353012084961,
      "learning_rate": 1.271617497456765e-07,
      "loss": 1.508,
      "step": 98050
    },
    {
      "epoch": 49.87792472024415,
      "grad_norm": 46.17135238647461,
      "learning_rate": 1.2207527975584945e-07,
      "loss": 1.409,
      "step": 98060
    },
    {
      "epoch": 49.88301119023398,
      "grad_norm": 45.32924270629883,
      "learning_rate": 1.169888097660224e-07,
      "loss": 1.4639,
      "step": 98070
    },
    {
      "epoch": 49.888097660223806,
      "grad_norm": 41.3829345703125,
      "learning_rate": 1.1190233977619532e-07,
      "loss": 1.5743,
      "step": 98080
    },
    {
      "epoch": 49.89318413021363,
      "grad_norm": 49.779476165771484,
      "learning_rate": 1.0681586978636827e-07,
      "loss": 1.5301,
      "step": 98090
    },
    {
      "epoch": 49.89827060020346,
      "grad_norm": 40.0899772644043,
      "learning_rate": 1.017293997965412e-07,
      "loss": 1.5904,
      "step": 98100
    },
    {
      "epoch": 49.90335707019329,
      "grad_norm": 43.83673858642578,
      "learning_rate": 9.664292980671415e-08,
      "loss": 1.5028,
      "step": 98110
    },
    {
      "epoch": 49.908443540183114,
      "grad_norm": 42.50938415527344,
      "learning_rate": 9.155645981688708e-08,
      "loss": 1.49,
      "step": 98120
    },
    {
      "epoch": 49.91353001017294,
      "grad_norm": 43.68179702758789,
      "learning_rate": 8.646998982706002e-08,
      "loss": 1.4348,
      "step": 98130
    },
    {
      "epoch": 49.91861648016277,
      "grad_norm": 38.89467239379883,
      "learning_rate": 8.138351983723296e-08,
      "loss": 1.5151,
      "step": 98140
    },
    {
      "epoch": 49.923702950152595,
      "grad_norm": 34.65533447265625,
      "learning_rate": 7.62970498474059e-08,
      "loss": 1.5468,
      "step": 98150
    },
    {
      "epoch": 49.92878942014242,
      "grad_norm": 46.6346321105957,
      "learning_rate": 7.121057985757885e-08,
      "loss": 1.456,
      "step": 98160
    },
    {
      "epoch": 49.93387589013225,
      "grad_norm": 59.18046188354492,
      "learning_rate": 6.612410986775179e-08,
      "loss": 1.5192,
      "step": 98170
    },
    {
      "epoch": 49.938962360122076,
      "grad_norm": 37.873756408691406,
      "learning_rate": 6.103763987792472e-08,
      "loss": 1.4403,
      "step": 98180
    },
    {
      "epoch": 49.9440488301119,
      "grad_norm": 51.015602111816406,
      "learning_rate": 5.595116988809766e-08,
      "loss": 1.4254,
      "step": 98190
    },
    {
      "epoch": 49.94913530010173,
      "grad_norm": 38.77540588378906,
      "learning_rate": 5.08646998982706e-08,
      "loss": 1.5059,
      "step": 98200
    },
    {
      "epoch": 49.95422177009156,
      "grad_norm": 55.62747573852539,
      "learning_rate": 4.577822990844354e-08,
      "loss": 1.5366,
      "step": 98210
    },
    {
      "epoch": 49.959308240081384,
      "grad_norm": 36.957489013671875,
      "learning_rate": 4.069175991861648e-08,
      "loss": 1.4805,
      "step": 98220
    },
    {
      "epoch": 49.96439471007121,
      "grad_norm": 40.69196319580078,
      "learning_rate": 3.5605289928789424e-08,
      "loss": 1.4397,
      "step": 98230
    },
    {
      "epoch": 49.96948118006104,
      "grad_norm": 37.8253288269043,
      "learning_rate": 3.051881993896236e-08,
      "loss": 1.4512,
      "step": 98240
    },
    {
      "epoch": 49.974567650050865,
      "grad_norm": 51.26540756225586,
      "learning_rate": 2.54323499491353e-08,
      "loss": 1.4858,
      "step": 98250
    },
    {
      "epoch": 49.97965412004069,
      "grad_norm": 34.627681732177734,
      "learning_rate": 2.034587995930824e-08,
      "loss": 1.5109,
      "step": 98260
    },
    {
      "epoch": 49.98474059003052,
      "grad_norm": 49.5474853515625,
      "learning_rate": 1.525940996948118e-08,
      "loss": 1.5042,
      "step": 98270
    },
    {
      "epoch": 49.989827060020346,
      "grad_norm": 51.315921783447266,
      "learning_rate": 1.017293997965412e-08,
      "loss": 1.434,
      "step": 98280
    },
    {
      "epoch": 49.99491353001017,
      "grad_norm": 43.690635681152344,
      "learning_rate": 5.08646998982706e-09,
      "loss": 1.3735,
      "step": 98290
    },
    {
      "epoch": 50.0,
      "grad_norm": 39.386749267578125,
      "learning_rate": 0.0,
      "loss": 1.4876,
      "step": 98300
    },
    {
      "epoch": 50.0,
      "eval_loss": 5.11643648147583,
      "eval_runtime": 2.8761,
      "eval_samples_per_second": 964.839,
      "eval_steps_per_second": 120.648,
      "step": 98300
    }
  ],
  "logging_steps": 10,
  "max_steps": 98300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 32674990080000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
