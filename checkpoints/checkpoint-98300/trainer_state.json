{
  "best_metric": 3.6637704372406006,
  "best_model_checkpoint": "./checkpoints\\checkpoint-11796",
  "epoch": 50.0,
  "eval_steps": 500,
  "global_step": 98300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00508646998982706,
      "grad_norm": 1.9585089683532715,
      "learning_rate": 4.999491353001017e-05,
      "loss": 5.5444,
      "step": 10
    },
    {
      "epoch": 0.01017293997965412,
      "grad_norm": 1.7009533643722534,
      "learning_rate": 4.998982706002035e-05,
      "loss": 5.5018,
      "step": 20
    },
    {
      "epoch": 0.015259409969481181,
      "grad_norm": 1.4952682256698608,
      "learning_rate": 4.998474059003052e-05,
      "loss": 5.464,
      "step": 30
    },
    {
      "epoch": 0.02034587995930824,
      "grad_norm": 1.4348140954971313,
      "learning_rate": 4.997965412004069e-05,
      "loss": 5.4295,
      "step": 40
    },
    {
      "epoch": 0.0254323499491353,
      "grad_norm": 1.1581816673278809,
      "learning_rate": 4.9974567650050865e-05,
      "loss": 5.4153,
      "step": 50
    },
    {
      "epoch": 0.030518819938962362,
      "grad_norm": 1.569205403327942,
      "learning_rate": 4.996948118006104e-05,
      "loss": 5.3885,
      "step": 60
    },
    {
      "epoch": 0.03560528992878942,
      "grad_norm": 1.3471859693527222,
      "learning_rate": 4.996439471007121e-05,
      "loss": 5.3574,
      "step": 70
    },
    {
      "epoch": 0.04069175991861648,
      "grad_norm": 1.1945815086364746,
      "learning_rate": 4.995930824008139e-05,
      "loss": 5.3385,
      "step": 80
    },
    {
      "epoch": 0.04577822990844354,
      "grad_norm": 1.233575701713562,
      "learning_rate": 4.995422177009156e-05,
      "loss": 5.32,
      "step": 90
    },
    {
      "epoch": 0.0508646998982706,
      "grad_norm": 1.2731891870498657,
      "learning_rate": 4.994913530010173e-05,
      "loss": 5.2897,
      "step": 100
    },
    {
      "epoch": 0.05595116988809766,
      "grad_norm": 1.095229148864746,
      "learning_rate": 4.9944048830111905e-05,
      "loss": 5.2792,
      "step": 110
    },
    {
      "epoch": 0.061037639877924724,
      "grad_norm": 1.194229006767273,
      "learning_rate": 4.9938962360122075e-05,
      "loss": 5.2332,
      "step": 120
    },
    {
      "epoch": 0.06612410986775177,
      "grad_norm": 1.4456599950790405,
      "learning_rate": 4.9933875890132245e-05,
      "loss": 5.2304,
      "step": 130
    },
    {
      "epoch": 0.07121057985757884,
      "grad_norm": 1.413408637046814,
      "learning_rate": 4.992878942014242e-05,
      "loss": 5.2069,
      "step": 140
    },
    {
      "epoch": 0.0762970498474059,
      "grad_norm": 1.565890908241272,
      "learning_rate": 4.99237029501526e-05,
      "loss": 5.19,
      "step": 150
    },
    {
      "epoch": 0.08138351983723296,
      "grad_norm": 1.3241851329803467,
      "learning_rate": 4.9918616480162775e-05,
      "loss": 5.1631,
      "step": 160
    },
    {
      "epoch": 0.08646998982706001,
      "grad_norm": 1.2150278091430664,
      "learning_rate": 4.9913530010172945e-05,
      "loss": 5.1278,
      "step": 170
    },
    {
      "epoch": 0.09155645981688708,
      "grad_norm": 1.2402805089950562,
      "learning_rate": 4.9908443540183115e-05,
      "loss": 5.1118,
      "step": 180
    },
    {
      "epoch": 0.09664292980671414,
      "grad_norm": 1.0788289308547974,
      "learning_rate": 4.990335707019329e-05,
      "loss": 5.1017,
      "step": 190
    },
    {
      "epoch": 0.1017293997965412,
      "grad_norm": 1.2812668085098267,
      "learning_rate": 4.989827060020346e-05,
      "loss": 5.0792,
      "step": 200
    },
    {
      "epoch": 0.10681586978636826,
      "grad_norm": 1.1210490465164185,
      "learning_rate": 4.989318413021363e-05,
      "loss": 5.0611,
      "step": 210
    },
    {
      "epoch": 0.11190233977619532,
      "grad_norm": 1.0064949989318848,
      "learning_rate": 4.988809766022381e-05,
      "loss": 5.0538,
      "step": 220
    },
    {
      "epoch": 0.11698880976602238,
      "grad_norm": 1.5250755548477173,
      "learning_rate": 4.988301119023398e-05,
      "loss": 4.9935,
      "step": 230
    },
    {
      "epoch": 0.12207527975584945,
      "grad_norm": 1.2483479976654053,
      "learning_rate": 4.9877924720244154e-05,
      "loss": 5.0059,
      "step": 240
    },
    {
      "epoch": 0.1271617497456765,
      "grad_norm": 0.9861146807670593,
      "learning_rate": 4.987283825025433e-05,
      "loss": 4.986,
      "step": 250
    },
    {
      "epoch": 0.13224821973550355,
      "grad_norm": 1.2158012390136719,
      "learning_rate": 4.98677517802645e-05,
      "loss": 4.9749,
      "step": 260
    },
    {
      "epoch": 0.1373346897253306,
      "grad_norm": 1.07657790184021,
      "learning_rate": 4.986266531027467e-05,
      "loss": 4.9685,
      "step": 270
    },
    {
      "epoch": 0.14242115971515767,
      "grad_norm": 1.1107748746871948,
      "learning_rate": 4.985757884028485e-05,
      "loss": 4.9142,
      "step": 280
    },
    {
      "epoch": 0.14750762970498474,
      "grad_norm": 1.2115124464035034,
      "learning_rate": 4.985249237029502e-05,
      "loss": 4.9266,
      "step": 290
    },
    {
      "epoch": 0.1525940996948118,
      "grad_norm": 1.0620908737182617,
      "learning_rate": 4.984740590030519e-05,
      "loss": 4.8965,
      "step": 300
    },
    {
      "epoch": 0.15768056968463887,
      "grad_norm": 1.3235645294189453,
      "learning_rate": 4.9842319430315364e-05,
      "loss": 4.8439,
      "step": 310
    },
    {
      "epoch": 0.16276703967446593,
      "grad_norm": 1.256437063217163,
      "learning_rate": 4.9837232960325534e-05,
      "loss": 4.8681,
      "step": 320
    },
    {
      "epoch": 0.167853509664293,
      "grad_norm": 1.0278855562210083,
      "learning_rate": 4.983214649033571e-05,
      "loss": 4.8625,
      "step": 330
    },
    {
      "epoch": 0.17293997965412003,
      "grad_norm": 1.2098768949508667,
      "learning_rate": 4.982706002034588e-05,
      "loss": 4.8021,
      "step": 340
    },
    {
      "epoch": 0.1780264496439471,
      "grad_norm": 1.29997718334198,
      "learning_rate": 4.982197355035606e-05,
      "loss": 4.7978,
      "step": 350
    },
    {
      "epoch": 0.18311291963377416,
      "grad_norm": 1.237276315689087,
      "learning_rate": 4.981688708036623e-05,
      "loss": 4.7685,
      "step": 360
    },
    {
      "epoch": 0.18819938962360122,
      "grad_norm": 1.0698118209838867,
      "learning_rate": 4.9811800610376404e-05,
      "loss": 4.761,
      "step": 370
    },
    {
      "epoch": 0.19328585961342828,
      "grad_norm": 0.9539771676063538,
      "learning_rate": 4.9806714140386574e-05,
      "loss": 4.7812,
      "step": 380
    },
    {
      "epoch": 0.19837232960325535,
      "grad_norm": 0.9739999771118164,
      "learning_rate": 4.9801627670396743e-05,
      "loss": 4.7757,
      "step": 390
    },
    {
      "epoch": 0.2034587995930824,
      "grad_norm": 1.190354347229004,
      "learning_rate": 4.979654120040692e-05,
      "loss": 4.7024,
      "step": 400
    },
    {
      "epoch": 0.20854526958290945,
      "grad_norm": 1.0434629917144775,
      "learning_rate": 4.979145473041709e-05,
      "loss": 4.7009,
      "step": 410
    },
    {
      "epoch": 0.2136317395727365,
      "grad_norm": 1.2235456705093384,
      "learning_rate": 4.978636826042727e-05,
      "loss": 4.7084,
      "step": 420
    },
    {
      "epoch": 0.21871820956256358,
      "grad_norm": 1.4132357835769653,
      "learning_rate": 4.9781281790437437e-05,
      "loss": 4.6598,
      "step": 430
    },
    {
      "epoch": 0.22380467955239064,
      "grad_norm": 0.9326409697532654,
      "learning_rate": 4.977619532044761e-05,
      "loss": 4.6951,
      "step": 440
    },
    {
      "epoch": 0.2288911495422177,
      "grad_norm": 1.050897240638733,
      "learning_rate": 4.977110885045779e-05,
      "loss": 4.6434,
      "step": 450
    },
    {
      "epoch": 0.23397761953204477,
      "grad_norm": 1.0765312910079956,
      "learning_rate": 4.976602238046796e-05,
      "loss": 4.5976,
      "step": 460
    },
    {
      "epoch": 0.23906408952187183,
      "grad_norm": 1.2914726734161377,
      "learning_rate": 4.976093591047813e-05,
      "loss": 4.6072,
      "step": 470
    },
    {
      "epoch": 0.2441505595116989,
      "grad_norm": 1.1661275625228882,
      "learning_rate": 4.9755849440488306e-05,
      "loss": 4.6281,
      "step": 480
    },
    {
      "epoch": 0.24923702950152593,
      "grad_norm": 1.1334600448608398,
      "learning_rate": 4.9750762970498476e-05,
      "loss": 4.594,
      "step": 490
    },
    {
      "epoch": 0.254323499491353,
      "grad_norm": 0.956544816493988,
      "learning_rate": 4.9745676500508646e-05,
      "loss": 4.5851,
      "step": 500
    },
    {
      "epoch": 0.25940996948118006,
      "grad_norm": 1.0469050407409668,
      "learning_rate": 4.974059003051882e-05,
      "loss": 4.5527,
      "step": 510
    },
    {
      "epoch": 0.2644964394710071,
      "grad_norm": 1.0449284315109253,
      "learning_rate": 4.973550356052899e-05,
      "loss": 4.5929,
      "step": 520
    },
    {
      "epoch": 0.2695829094608342,
      "grad_norm": 1.2400704622268677,
      "learning_rate": 4.973041709053917e-05,
      "loss": 4.5203,
      "step": 530
    },
    {
      "epoch": 0.2746693794506612,
      "grad_norm": 0.9109295606613159,
      "learning_rate": 4.9725330620549346e-05,
      "loss": 4.5771,
      "step": 540
    },
    {
      "epoch": 0.2797558494404883,
      "grad_norm": 1.120289921760559,
      "learning_rate": 4.9720244150559516e-05,
      "loss": 4.4708,
      "step": 550
    },
    {
      "epoch": 0.28484231943031535,
      "grad_norm": 0.9644604325294495,
      "learning_rate": 4.9715157680569686e-05,
      "loss": 4.539,
      "step": 560
    },
    {
      "epoch": 0.28992878942014244,
      "grad_norm": 1.2098420858383179,
      "learning_rate": 4.971007121057986e-05,
      "loss": 4.5541,
      "step": 570
    },
    {
      "epoch": 0.2950152594099695,
      "grad_norm": 0.9133832454681396,
      "learning_rate": 4.970498474059003e-05,
      "loss": 4.5027,
      "step": 580
    },
    {
      "epoch": 0.30010172939979657,
      "grad_norm": 0.8536376953125,
      "learning_rate": 4.96998982706002e-05,
      "loss": 4.4776,
      "step": 590
    },
    {
      "epoch": 0.3051881993896236,
      "grad_norm": 0.881970226764679,
      "learning_rate": 4.969481180061038e-05,
      "loss": 4.4669,
      "step": 600
    },
    {
      "epoch": 0.31027466937945064,
      "grad_norm": 1.07752525806427,
      "learning_rate": 4.968972533062055e-05,
      "loss": 4.4765,
      "step": 610
    },
    {
      "epoch": 0.31536113936927773,
      "grad_norm": 1.1978000402450562,
      "learning_rate": 4.9684638860630726e-05,
      "loss": 4.5212,
      "step": 620
    },
    {
      "epoch": 0.32044760935910477,
      "grad_norm": 1.0351076126098633,
      "learning_rate": 4.96795523906409e-05,
      "loss": 4.4634,
      "step": 630
    },
    {
      "epoch": 0.32553407934893186,
      "grad_norm": 0.9918464422225952,
      "learning_rate": 4.967446592065107e-05,
      "loss": 4.3966,
      "step": 640
    },
    {
      "epoch": 0.3306205493387589,
      "grad_norm": 1.0583484172821045,
      "learning_rate": 4.966937945066124e-05,
      "loss": 4.5152,
      "step": 650
    },
    {
      "epoch": 0.335707019328586,
      "grad_norm": 1.057262659072876,
      "learning_rate": 4.966429298067142e-05,
      "loss": 4.4448,
      "step": 660
    },
    {
      "epoch": 0.340793489318413,
      "grad_norm": 1.005101203918457,
      "learning_rate": 4.965920651068159e-05,
      "loss": 4.4238,
      "step": 670
    },
    {
      "epoch": 0.34587995930824006,
      "grad_norm": 0.9661780595779419,
      "learning_rate": 4.9654120040691765e-05,
      "loss": 4.3938,
      "step": 680
    },
    {
      "epoch": 0.35096642929806715,
      "grad_norm": 0.987967848777771,
      "learning_rate": 4.9649033570701935e-05,
      "loss": 4.3688,
      "step": 690
    },
    {
      "epoch": 0.3560528992878942,
      "grad_norm": 1.2252821922302246,
      "learning_rate": 4.9643947100712105e-05,
      "loss": 4.3957,
      "step": 700
    },
    {
      "epoch": 0.3611393692777213,
      "grad_norm": 1.2010424137115479,
      "learning_rate": 4.963886063072228e-05,
      "loss": 4.4713,
      "step": 710
    },
    {
      "epoch": 0.3662258392675483,
      "grad_norm": 1.1826695203781128,
      "learning_rate": 4.963377416073245e-05,
      "loss": 4.3908,
      "step": 720
    },
    {
      "epoch": 0.3713123092573754,
      "grad_norm": 1.2025344371795654,
      "learning_rate": 4.962868769074263e-05,
      "loss": 4.4077,
      "step": 730
    },
    {
      "epoch": 0.37639877924720244,
      "grad_norm": 1.0716888904571533,
      "learning_rate": 4.9623601220752805e-05,
      "loss": 4.3235,
      "step": 740
    },
    {
      "epoch": 0.3814852492370295,
      "grad_norm": 0.9116369485855103,
      "learning_rate": 4.9618514750762975e-05,
      "loss": 4.3541,
      "step": 750
    },
    {
      "epoch": 0.38657171922685657,
      "grad_norm": 1.0556226968765259,
      "learning_rate": 4.9613428280773145e-05,
      "loss": 4.3337,
      "step": 760
    },
    {
      "epoch": 0.3916581892166836,
      "grad_norm": 0.9915569424629211,
      "learning_rate": 4.960834181078332e-05,
      "loss": 4.4359,
      "step": 770
    },
    {
      "epoch": 0.3967446592065107,
      "grad_norm": 1.0425828695297241,
      "learning_rate": 4.960325534079349e-05,
      "loss": 4.3788,
      "step": 780
    },
    {
      "epoch": 0.40183112919633773,
      "grad_norm": 0.860231339931488,
      "learning_rate": 4.959816887080366e-05,
      "loss": 4.4026,
      "step": 790
    },
    {
      "epoch": 0.4069175991861648,
      "grad_norm": 0.9238550066947937,
      "learning_rate": 4.959308240081384e-05,
      "loss": 4.3583,
      "step": 800
    },
    {
      "epoch": 0.41200406917599186,
      "grad_norm": 1.0526010990142822,
      "learning_rate": 4.958799593082401e-05,
      "loss": 4.3531,
      "step": 810
    },
    {
      "epoch": 0.4170905391658189,
      "grad_norm": 1.1386793851852417,
      "learning_rate": 4.9582909460834184e-05,
      "loss": 4.322,
      "step": 820
    },
    {
      "epoch": 0.422177009155646,
      "grad_norm": 1.1627914905548096,
      "learning_rate": 4.957782299084436e-05,
      "loss": 4.2742,
      "step": 830
    },
    {
      "epoch": 0.427263479145473,
      "grad_norm": 1.2516367435455322,
      "learning_rate": 4.957273652085453e-05,
      "loss": 4.2997,
      "step": 840
    },
    {
      "epoch": 0.4323499491353001,
      "grad_norm": 1.1359845399856567,
      "learning_rate": 4.95676500508647e-05,
      "loss": 4.3493,
      "step": 850
    },
    {
      "epoch": 0.43743641912512715,
      "grad_norm": 1.4033501148223877,
      "learning_rate": 4.956256358087488e-05,
      "loss": 4.3401,
      "step": 860
    },
    {
      "epoch": 0.44252288911495424,
      "grad_norm": 1.059059500694275,
      "learning_rate": 4.955747711088505e-05,
      "loss": 4.2688,
      "step": 870
    },
    {
      "epoch": 0.4476093591047813,
      "grad_norm": 1.1205987930297852,
      "learning_rate": 4.955239064089522e-05,
      "loss": 4.3024,
      "step": 880
    },
    {
      "epoch": 0.4526958290946083,
      "grad_norm": 1.1574594974517822,
      "learning_rate": 4.9547304170905394e-05,
      "loss": 4.3315,
      "step": 890
    },
    {
      "epoch": 0.4577822990844354,
      "grad_norm": 0.9600428938865662,
      "learning_rate": 4.9542217700915564e-05,
      "loss": 4.2731,
      "step": 900
    },
    {
      "epoch": 0.46286876907426244,
      "grad_norm": 1.2586506605148315,
      "learning_rate": 4.953713123092574e-05,
      "loss": 4.2756,
      "step": 910
    },
    {
      "epoch": 0.46795523906408953,
      "grad_norm": 1.302880883216858,
      "learning_rate": 4.953204476093592e-05,
      "loss": 4.2366,
      "step": 920
    },
    {
      "epoch": 0.47304170905391657,
      "grad_norm": 1.7742618322372437,
      "learning_rate": 4.952695829094609e-05,
      "loss": 4.2277,
      "step": 930
    },
    {
      "epoch": 0.47812817904374366,
      "grad_norm": 1.244584083557129,
      "learning_rate": 4.952187182095626e-05,
      "loss": 4.2625,
      "step": 940
    },
    {
      "epoch": 0.4832146490335707,
      "grad_norm": 1.0780242681503296,
      "learning_rate": 4.9516785350966434e-05,
      "loss": 4.2794,
      "step": 950
    },
    {
      "epoch": 0.4883011190233978,
      "grad_norm": 1.3982036113739014,
      "learning_rate": 4.9511698880976604e-05,
      "loss": 4.2644,
      "step": 960
    },
    {
      "epoch": 0.4933875890132248,
      "grad_norm": 1.6520549058914185,
      "learning_rate": 4.950661241098678e-05,
      "loss": 4.2507,
      "step": 970
    },
    {
      "epoch": 0.49847405900305186,
      "grad_norm": 0.9269866347312927,
      "learning_rate": 4.950152594099695e-05,
      "loss": 4.2482,
      "step": 980
    },
    {
      "epoch": 0.503560528992879,
      "grad_norm": 1.022214412689209,
      "learning_rate": 4.949643947100712e-05,
      "loss": 4.2032,
      "step": 990
    },
    {
      "epoch": 0.508646998982706,
      "grad_norm": 1.2174458503723145,
      "learning_rate": 4.94913530010173e-05,
      "loss": 4.2069,
      "step": 1000
    },
    {
      "epoch": 0.513733468972533,
      "grad_norm": 1.3003227710723877,
      "learning_rate": 4.9486266531027467e-05,
      "loss": 4.2186,
      "step": 1010
    },
    {
      "epoch": 0.5188199389623601,
      "grad_norm": 1.5599457025527954,
      "learning_rate": 4.948118006103764e-05,
      "loss": 4.2508,
      "step": 1020
    },
    {
      "epoch": 0.5239064089521872,
      "grad_norm": 1.1026594638824463,
      "learning_rate": 4.947609359104782e-05,
      "loss": 4.2113,
      "step": 1030
    },
    {
      "epoch": 0.5289928789420142,
      "grad_norm": 1.0611540079116821,
      "learning_rate": 4.947100712105799e-05,
      "loss": 4.2,
      "step": 1040
    },
    {
      "epoch": 0.5340793489318413,
      "grad_norm": 1.2830498218536377,
      "learning_rate": 4.946592065106816e-05,
      "loss": 4.1854,
      "step": 1050
    },
    {
      "epoch": 0.5391658189216684,
      "grad_norm": 1.0995076894760132,
      "learning_rate": 4.9460834181078336e-05,
      "loss": 4.2178,
      "step": 1060
    },
    {
      "epoch": 0.5442522889114955,
      "grad_norm": 1.1036007404327393,
      "learning_rate": 4.9455747711088506e-05,
      "loss": 4.1803,
      "step": 1070
    },
    {
      "epoch": 0.5493387589013224,
      "grad_norm": 1.3285057544708252,
      "learning_rate": 4.9450661241098676e-05,
      "loss": 4.211,
      "step": 1080
    },
    {
      "epoch": 0.5544252288911495,
      "grad_norm": 1.3299161195755005,
      "learning_rate": 4.944557477110885e-05,
      "loss": 4.188,
      "step": 1090
    },
    {
      "epoch": 0.5595116988809766,
      "grad_norm": 1.1388696432113647,
      "learning_rate": 4.944048830111902e-05,
      "loss": 4.2074,
      "step": 1100
    },
    {
      "epoch": 0.5645981688708036,
      "grad_norm": 1.0197968482971191,
      "learning_rate": 4.94354018311292e-05,
      "loss": 4.1387,
      "step": 1110
    },
    {
      "epoch": 0.5696846388606307,
      "grad_norm": 1.29355788230896,
      "learning_rate": 4.9430315361139376e-05,
      "loss": 4.1868,
      "step": 1120
    },
    {
      "epoch": 0.5747711088504578,
      "grad_norm": 1.3100465536117554,
      "learning_rate": 4.9425228891149546e-05,
      "loss": 4.1097,
      "step": 1130
    },
    {
      "epoch": 0.5798575788402849,
      "grad_norm": 1.1799715757369995,
      "learning_rate": 4.9420142421159716e-05,
      "loss": 4.1706,
      "step": 1140
    },
    {
      "epoch": 0.5849440488301119,
      "grad_norm": 1.2754360437393188,
      "learning_rate": 4.941505595116989e-05,
      "loss": 4.2091,
      "step": 1150
    },
    {
      "epoch": 0.590030518819939,
      "grad_norm": 1.024857759475708,
      "learning_rate": 4.940996948118006e-05,
      "loss": 4.1714,
      "step": 1160
    },
    {
      "epoch": 0.595116988809766,
      "grad_norm": 1.5393249988555908,
      "learning_rate": 4.940488301119023e-05,
      "loss": 4.1591,
      "step": 1170
    },
    {
      "epoch": 0.6002034587995931,
      "grad_norm": 1.2726995944976807,
      "learning_rate": 4.939979654120041e-05,
      "loss": 4.1788,
      "step": 1180
    },
    {
      "epoch": 0.6052899287894201,
      "grad_norm": 1.228076696395874,
      "learning_rate": 4.939471007121058e-05,
      "loss": 4.1604,
      "step": 1190
    },
    {
      "epoch": 0.6103763987792472,
      "grad_norm": 1.3904155492782593,
      "learning_rate": 4.9389623601220756e-05,
      "loss": 4.1445,
      "step": 1200
    },
    {
      "epoch": 0.6154628687690743,
      "grad_norm": 1.4824177026748657,
      "learning_rate": 4.938453713123093e-05,
      "loss": 4.2071,
      "step": 1210
    },
    {
      "epoch": 0.6205493387589013,
      "grad_norm": 1.1739479303359985,
      "learning_rate": 4.93794506612411e-05,
      "loss": 4.2445,
      "step": 1220
    },
    {
      "epoch": 0.6256358087487284,
      "grad_norm": 1.1372936964035034,
      "learning_rate": 4.937436419125128e-05,
      "loss": 4.1683,
      "step": 1230
    },
    {
      "epoch": 0.6307222787385555,
      "grad_norm": 1.6379144191741943,
      "learning_rate": 4.936927772126145e-05,
      "loss": 4.2249,
      "step": 1240
    },
    {
      "epoch": 0.6358087487283826,
      "grad_norm": 1.340135097503662,
      "learning_rate": 4.936419125127162e-05,
      "loss": 4.209,
      "step": 1250
    },
    {
      "epoch": 0.6408952187182095,
      "grad_norm": 1.7233679294586182,
      "learning_rate": 4.9359104781281795e-05,
      "loss": 4.1444,
      "step": 1260
    },
    {
      "epoch": 0.6459816887080366,
      "grad_norm": 1.2008136510849,
      "learning_rate": 4.9354018311291965e-05,
      "loss": 4.1167,
      "step": 1270
    },
    {
      "epoch": 0.6510681586978637,
      "grad_norm": 1.4558030366897583,
      "learning_rate": 4.9348931841302135e-05,
      "loss": 4.2075,
      "step": 1280
    },
    {
      "epoch": 0.6561546286876907,
      "grad_norm": 1.1162328720092773,
      "learning_rate": 4.934384537131231e-05,
      "loss": 4.1029,
      "step": 1290
    },
    {
      "epoch": 0.6612410986775178,
      "grad_norm": 1.3151744604110718,
      "learning_rate": 4.933875890132248e-05,
      "loss": 4.2085,
      "step": 1300
    },
    {
      "epoch": 0.6663275686673449,
      "grad_norm": 1.8034764528274536,
      "learning_rate": 4.933367243133266e-05,
      "loss": 4.1405,
      "step": 1310
    },
    {
      "epoch": 0.671414038657172,
      "grad_norm": 1.2844878435134888,
      "learning_rate": 4.9328585961342835e-05,
      "loss": 4.1374,
      "step": 1320
    },
    {
      "epoch": 0.676500508646999,
      "grad_norm": 1.515757441520691,
      "learning_rate": 4.9323499491353005e-05,
      "loss": 4.1828,
      "step": 1330
    },
    {
      "epoch": 0.681586978636826,
      "grad_norm": 1.2853244543075562,
      "learning_rate": 4.9318413021363175e-05,
      "loss": 4.1547,
      "step": 1340
    },
    {
      "epoch": 0.6866734486266531,
      "grad_norm": 1.2468208074569702,
      "learning_rate": 4.931332655137335e-05,
      "loss": 4.1324,
      "step": 1350
    },
    {
      "epoch": 0.6917599186164801,
      "grad_norm": 1.2399073839187622,
      "learning_rate": 4.930824008138352e-05,
      "loss": 4.2425,
      "step": 1360
    },
    {
      "epoch": 0.6968463886063072,
      "grad_norm": 1.524145245552063,
      "learning_rate": 4.930315361139369e-05,
      "loss": 4.1049,
      "step": 1370
    },
    {
      "epoch": 0.7019328585961343,
      "grad_norm": 1.2506672143936157,
      "learning_rate": 4.929806714140387e-05,
      "loss": 4.0974,
      "step": 1380
    },
    {
      "epoch": 0.7070193285859614,
      "grad_norm": 1.8325817584991455,
      "learning_rate": 4.929298067141404e-05,
      "loss": 4.1568,
      "step": 1390
    },
    {
      "epoch": 0.7121057985757884,
      "grad_norm": 1.0411946773529053,
      "learning_rate": 4.9287894201424214e-05,
      "loss": 4.0931,
      "step": 1400
    },
    {
      "epoch": 0.7171922685656155,
      "grad_norm": 2.354320526123047,
      "learning_rate": 4.928280773143439e-05,
      "loss": 4.0813,
      "step": 1410
    },
    {
      "epoch": 0.7222787385554426,
      "grad_norm": 1.5156207084655762,
      "learning_rate": 4.927772126144456e-05,
      "loss": 4.1268,
      "step": 1420
    },
    {
      "epoch": 0.7273652085452695,
      "grad_norm": 1.4479687213897705,
      "learning_rate": 4.927263479145473e-05,
      "loss": 4.1882,
      "step": 1430
    },
    {
      "epoch": 0.7324516785350966,
      "grad_norm": 1.4128406047821045,
      "learning_rate": 4.926754832146491e-05,
      "loss": 4.1001,
      "step": 1440
    },
    {
      "epoch": 0.7375381485249237,
      "grad_norm": 1.3605612516403198,
      "learning_rate": 4.926246185147508e-05,
      "loss": 4.0763,
      "step": 1450
    },
    {
      "epoch": 0.7426246185147508,
      "grad_norm": 1.2540223598480225,
      "learning_rate": 4.925737538148525e-05,
      "loss": 4.1341,
      "step": 1460
    },
    {
      "epoch": 0.7477110885045778,
      "grad_norm": 2.1127843856811523,
      "learning_rate": 4.9252288911495424e-05,
      "loss": 4.0764,
      "step": 1470
    },
    {
      "epoch": 0.7527975584944049,
      "grad_norm": 1.5416548252105713,
      "learning_rate": 4.9247202441505594e-05,
      "loss": 4.0845,
      "step": 1480
    },
    {
      "epoch": 0.757884028484232,
      "grad_norm": 1.9581156969070435,
      "learning_rate": 4.924211597151577e-05,
      "loss": 4.0685,
      "step": 1490
    },
    {
      "epoch": 0.762970498474059,
      "grad_norm": 1.3578999042510986,
      "learning_rate": 4.923702950152595e-05,
      "loss": 4.1751,
      "step": 1500
    },
    {
      "epoch": 0.768056968463886,
      "grad_norm": 1.1729875802993774,
      "learning_rate": 4.923194303153612e-05,
      "loss": 4.1706,
      "step": 1510
    },
    {
      "epoch": 0.7731434384537131,
      "grad_norm": 1.3527897596359253,
      "learning_rate": 4.9226856561546294e-05,
      "loss": 4.0323,
      "step": 1520
    },
    {
      "epoch": 0.7782299084435402,
      "grad_norm": 1.3012056350708008,
      "learning_rate": 4.9221770091556464e-05,
      "loss": 4.1256,
      "step": 1530
    },
    {
      "epoch": 0.7833163784333672,
      "grad_norm": 1.7433809041976929,
      "learning_rate": 4.9216683621566634e-05,
      "loss": 4.1122,
      "step": 1540
    },
    {
      "epoch": 0.7884028484231943,
      "grad_norm": 1.2754298448562622,
      "learning_rate": 4.921159715157681e-05,
      "loss": 4.0388,
      "step": 1550
    },
    {
      "epoch": 0.7934893184130214,
      "grad_norm": 1.454426884651184,
      "learning_rate": 4.920651068158698e-05,
      "loss": 4.0637,
      "step": 1560
    },
    {
      "epoch": 0.7985757884028484,
      "grad_norm": 1.352404236793518,
      "learning_rate": 4.920142421159715e-05,
      "loss": 4.0225,
      "step": 1570
    },
    {
      "epoch": 0.8036622583926755,
      "grad_norm": 1.3086756467819214,
      "learning_rate": 4.919633774160733e-05,
      "loss": 4.0847,
      "step": 1580
    },
    {
      "epoch": 0.8087487283825026,
      "grad_norm": 1.6145128011703491,
      "learning_rate": 4.91912512716175e-05,
      "loss": 4.0928,
      "step": 1590
    },
    {
      "epoch": 0.8138351983723296,
      "grad_norm": 1.6500154733657837,
      "learning_rate": 4.918616480162767e-05,
      "loss": 4.0666,
      "step": 1600
    },
    {
      "epoch": 0.8189216683621566,
      "grad_norm": 1.7474112510681152,
      "learning_rate": 4.918107833163785e-05,
      "loss": 4.1195,
      "step": 1610
    },
    {
      "epoch": 0.8240081383519837,
      "grad_norm": 1.3464937210083008,
      "learning_rate": 4.917599186164802e-05,
      "loss": 4.0685,
      "step": 1620
    },
    {
      "epoch": 0.8290946083418108,
      "grad_norm": 1.6267820596694946,
      "learning_rate": 4.917090539165819e-05,
      "loss": 4.12,
      "step": 1630
    },
    {
      "epoch": 0.8341810783316378,
      "grad_norm": 1.491764783859253,
      "learning_rate": 4.9165818921668366e-05,
      "loss": 4.1061,
      "step": 1640
    },
    {
      "epoch": 0.8392675483214649,
      "grad_norm": 1.5330615043640137,
      "learning_rate": 4.9160732451678536e-05,
      "loss": 4.0751,
      "step": 1650
    },
    {
      "epoch": 0.844354018311292,
      "grad_norm": 1.8004283905029297,
      "learning_rate": 4.9155645981688706e-05,
      "loss": 4.1599,
      "step": 1660
    },
    {
      "epoch": 0.8494404883011191,
      "grad_norm": 1.5963318347930908,
      "learning_rate": 4.915055951169888e-05,
      "loss": 4.0844,
      "step": 1670
    },
    {
      "epoch": 0.854526958290946,
      "grad_norm": 1.2102068662643433,
      "learning_rate": 4.914547304170905e-05,
      "loss": 4.0799,
      "step": 1680
    },
    {
      "epoch": 0.8596134282807731,
      "grad_norm": 1.4623253345489502,
      "learning_rate": 4.914038657171923e-05,
      "loss": 4.0773,
      "step": 1690
    },
    {
      "epoch": 0.8646998982706002,
      "grad_norm": 1.9084428548812866,
      "learning_rate": 4.9135300101729406e-05,
      "loss": 4.0293,
      "step": 1700
    },
    {
      "epoch": 0.8697863682604272,
      "grad_norm": 1.6433120965957642,
      "learning_rate": 4.9130213631739576e-05,
      "loss": 4.0589,
      "step": 1710
    },
    {
      "epoch": 0.8748728382502543,
      "grad_norm": 1.7939417362213135,
      "learning_rate": 4.9125127161749746e-05,
      "loss": 4.0127,
      "step": 1720
    },
    {
      "epoch": 0.8799593082400814,
      "grad_norm": 2.129500150680542,
      "learning_rate": 4.912004069175992e-05,
      "loss": 4.0082,
      "step": 1730
    },
    {
      "epoch": 0.8850457782299085,
      "grad_norm": 1.7895512580871582,
      "learning_rate": 4.911495422177009e-05,
      "loss": 4.0832,
      "step": 1740
    },
    {
      "epoch": 0.8901322482197355,
      "grad_norm": 2.037670135498047,
      "learning_rate": 4.910986775178026e-05,
      "loss": 4.0918,
      "step": 1750
    },
    {
      "epoch": 0.8952187182095626,
      "grad_norm": 1.5972774028778076,
      "learning_rate": 4.910478128179044e-05,
      "loss": 4.1087,
      "step": 1760
    },
    {
      "epoch": 0.9003051881993896,
      "grad_norm": 2.431774616241455,
      "learning_rate": 4.909969481180061e-05,
      "loss": 4.0779,
      "step": 1770
    },
    {
      "epoch": 0.9053916581892166,
      "grad_norm": 1.5935935974121094,
      "learning_rate": 4.9094608341810786e-05,
      "loss": 4.1047,
      "step": 1780
    },
    {
      "epoch": 0.9104781281790437,
      "grad_norm": 1.4424139261245728,
      "learning_rate": 4.908952187182096e-05,
      "loss": 4.0734,
      "step": 1790
    },
    {
      "epoch": 0.9155645981688708,
      "grad_norm": 1.7132132053375244,
      "learning_rate": 4.908443540183113e-05,
      "loss": 4.067,
      "step": 1800
    },
    {
      "epoch": 0.9206510681586979,
      "grad_norm": 1.969544768333435,
      "learning_rate": 4.907934893184131e-05,
      "loss": 4.0115,
      "step": 1810
    },
    {
      "epoch": 0.9257375381485249,
      "grad_norm": 1.8443292379379272,
      "learning_rate": 4.907426246185148e-05,
      "loss": 4.046,
      "step": 1820
    },
    {
      "epoch": 0.930824008138352,
      "grad_norm": 1.3656728267669678,
      "learning_rate": 4.906917599186165e-05,
      "loss": 4.0337,
      "step": 1830
    },
    {
      "epoch": 0.9359104781281791,
      "grad_norm": 1.4383519887924194,
      "learning_rate": 4.9064089521871825e-05,
      "loss": 4.1255,
      "step": 1840
    },
    {
      "epoch": 0.940996948118006,
      "grad_norm": 2.0288400650024414,
      "learning_rate": 4.9059003051881995e-05,
      "loss": 4.0633,
      "step": 1850
    },
    {
      "epoch": 0.9460834181078331,
      "grad_norm": 1.3376163244247437,
      "learning_rate": 4.9053916581892165e-05,
      "loss": 4.0134,
      "step": 1860
    },
    {
      "epoch": 0.9511698880976602,
      "grad_norm": 2.0227010250091553,
      "learning_rate": 4.904883011190234e-05,
      "loss": 4.0001,
      "step": 1870
    },
    {
      "epoch": 0.9562563580874873,
      "grad_norm": 2.045563220977783,
      "learning_rate": 4.904374364191252e-05,
      "loss": 3.9864,
      "step": 1880
    },
    {
      "epoch": 0.9613428280773143,
      "grad_norm": 2.089078426361084,
      "learning_rate": 4.903865717192269e-05,
      "loss": 4.0078,
      "step": 1890
    },
    {
      "epoch": 0.9664292980671414,
      "grad_norm": 1.4491204023361206,
      "learning_rate": 4.9033570701932865e-05,
      "loss": 4.1164,
      "step": 1900
    },
    {
      "epoch": 0.9715157680569685,
      "grad_norm": 2.1739985942840576,
      "learning_rate": 4.9028484231943035e-05,
      "loss": 4.0323,
      "step": 1910
    },
    {
      "epoch": 0.9766022380467956,
      "grad_norm": 1.5660347938537598,
      "learning_rate": 4.9023397761953205e-05,
      "loss": 3.9893,
      "step": 1920
    },
    {
      "epoch": 0.9816887080366226,
      "grad_norm": 1.423434853553772,
      "learning_rate": 4.901831129196338e-05,
      "loss": 4.0604,
      "step": 1930
    },
    {
      "epoch": 0.9867751780264497,
      "grad_norm": 2.103409767150879,
      "learning_rate": 4.901322482197355e-05,
      "loss": 3.9823,
      "step": 1940
    },
    {
      "epoch": 0.9918616480162767,
      "grad_norm": 1.478571891784668,
      "learning_rate": 4.900813835198372e-05,
      "loss": 4.0099,
      "step": 1950
    },
    {
      "epoch": 0.9969481180061037,
      "grad_norm": 2.681691884994507,
      "learning_rate": 4.90030518819939e-05,
      "loss": 4.0214,
      "step": 1960
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.9944815635681152,
      "eval_runtime": 2.637,
      "eval_samples_per_second": 1052.336,
      "eval_steps_per_second": 131.589,
      "step": 1966
    },
    {
      "epoch": 1.002034587995931,
      "grad_norm": 1.5439386367797852,
      "learning_rate": 4.899796541200407e-05,
      "loss": 3.962,
      "step": 1970
    },
    {
      "epoch": 1.007121057985758,
      "grad_norm": 1.8132206201553345,
      "learning_rate": 4.8992878942014244e-05,
      "loss": 4.0555,
      "step": 1980
    },
    {
      "epoch": 1.0122075279755849,
      "grad_norm": 1.8010733127593994,
      "learning_rate": 4.898779247202442e-05,
      "loss": 4.0329,
      "step": 1990
    },
    {
      "epoch": 1.017293997965412,
      "grad_norm": 2.2718300819396973,
      "learning_rate": 4.898270600203459e-05,
      "loss": 4.0538,
      "step": 2000
    },
    {
      "epoch": 1.022380467955239,
      "grad_norm": 1.835971474647522,
      "learning_rate": 4.897761953204476e-05,
      "loss": 4.0172,
      "step": 2010
    },
    {
      "epoch": 1.027466937945066,
      "grad_norm": 1.8991100788116455,
      "learning_rate": 4.897253306205494e-05,
      "loss": 4.0043,
      "step": 2020
    },
    {
      "epoch": 1.0325534079348933,
      "grad_norm": 2.124861478805542,
      "learning_rate": 4.896744659206511e-05,
      "loss": 3.9585,
      "step": 2030
    },
    {
      "epoch": 1.0376398779247202,
      "grad_norm": 1.7480398416519165,
      "learning_rate": 4.8962360122075284e-05,
      "loss": 4.0108,
      "step": 2040
    },
    {
      "epoch": 1.0427263479145472,
      "grad_norm": 3.101414203643799,
      "learning_rate": 4.8957273652085454e-05,
      "loss": 4.0298,
      "step": 2050
    },
    {
      "epoch": 1.0478128179043744,
      "grad_norm": 2.015698194503784,
      "learning_rate": 4.8952187182095624e-05,
      "loss": 4.0388,
      "step": 2060
    },
    {
      "epoch": 1.0528992878942014,
      "grad_norm": 1.9052571058273315,
      "learning_rate": 4.89471007121058e-05,
      "loss": 4.0278,
      "step": 2070
    },
    {
      "epoch": 1.0579857578840284,
      "grad_norm": 2.2769241333007812,
      "learning_rate": 4.894201424211598e-05,
      "loss": 4.0523,
      "step": 2080
    },
    {
      "epoch": 1.0630722278738556,
      "grad_norm": 1.8658063411712646,
      "learning_rate": 4.893692777212615e-05,
      "loss": 4.0593,
      "step": 2090
    },
    {
      "epoch": 1.0681586978636826,
      "grad_norm": 1.8654835224151611,
      "learning_rate": 4.8931841302136324e-05,
      "loss": 4.013,
      "step": 2100
    },
    {
      "epoch": 1.0732451678535098,
      "grad_norm": 1.6722782850265503,
      "learning_rate": 4.8926754832146494e-05,
      "loss": 3.9998,
      "step": 2110
    },
    {
      "epoch": 1.0783316378433367,
      "grad_norm": 2.4552009105682373,
      "learning_rate": 4.8921668362156664e-05,
      "loss": 3.9824,
      "step": 2120
    },
    {
      "epoch": 1.0834181078331637,
      "grad_norm": 3.1544547080993652,
      "learning_rate": 4.891658189216684e-05,
      "loss": 3.9884,
      "step": 2130
    },
    {
      "epoch": 1.088504577822991,
      "grad_norm": 2.443894624710083,
      "learning_rate": 4.891149542217701e-05,
      "loss": 4.0525,
      "step": 2140
    },
    {
      "epoch": 1.093591047812818,
      "grad_norm": 1.3459848165512085,
      "learning_rate": 4.890640895218718e-05,
      "loss": 3.9722,
      "step": 2150
    },
    {
      "epoch": 1.0986775178026449,
      "grad_norm": 2.145956039428711,
      "learning_rate": 4.890132248219736e-05,
      "loss": 3.9298,
      "step": 2160
    },
    {
      "epoch": 1.103763987792472,
      "grad_norm": 1.4805586338043213,
      "learning_rate": 4.889623601220753e-05,
      "loss": 4.0408,
      "step": 2170
    },
    {
      "epoch": 1.108850457782299,
      "grad_norm": 1.902617335319519,
      "learning_rate": 4.88911495422177e-05,
      "loss": 3.9652,
      "step": 2180
    },
    {
      "epoch": 1.113936927772126,
      "grad_norm": 2.057846784591675,
      "learning_rate": 4.888606307222788e-05,
      "loss": 3.9803,
      "step": 2190
    },
    {
      "epoch": 1.1190233977619533,
      "grad_norm": 1.7683820724487305,
      "learning_rate": 4.888097660223805e-05,
      "loss": 3.973,
      "step": 2200
    },
    {
      "epoch": 1.1241098677517802,
      "grad_norm": 2.033362627029419,
      "learning_rate": 4.887589013224822e-05,
      "loss": 4.0147,
      "step": 2210
    },
    {
      "epoch": 1.1291963377416074,
      "grad_norm": 1.5628010034561157,
      "learning_rate": 4.8870803662258396e-05,
      "loss": 3.8702,
      "step": 2220
    },
    {
      "epoch": 1.1342828077314344,
      "grad_norm": 2.378798246383667,
      "learning_rate": 4.8865717192268566e-05,
      "loss": 3.964,
      "step": 2230
    },
    {
      "epoch": 1.1393692777212614,
      "grad_norm": 2.093319892883301,
      "learning_rate": 4.8860630722278736e-05,
      "loss": 4.0114,
      "step": 2240
    },
    {
      "epoch": 1.1444557477110886,
      "grad_norm": 2.6308491230010986,
      "learning_rate": 4.885554425228891e-05,
      "loss": 4.0142,
      "step": 2250
    },
    {
      "epoch": 1.1495422177009156,
      "grad_norm": 2.726793050765991,
      "learning_rate": 4.885045778229908e-05,
      "loss": 4.0365,
      "step": 2260
    },
    {
      "epoch": 1.1546286876907426,
      "grad_norm": 2.5190415382385254,
      "learning_rate": 4.884537131230926e-05,
      "loss": 3.9481,
      "step": 2270
    },
    {
      "epoch": 1.1597151576805698,
      "grad_norm": 2.6700727939605713,
      "learning_rate": 4.8840284842319436e-05,
      "loss": 3.9796,
      "step": 2280
    },
    {
      "epoch": 1.1648016276703967,
      "grad_norm": 2.0161519050598145,
      "learning_rate": 4.8835198372329606e-05,
      "loss": 3.9047,
      "step": 2290
    },
    {
      "epoch": 1.1698880976602237,
      "grad_norm": 5.05422306060791,
      "learning_rate": 4.883011190233978e-05,
      "loss": 4.0376,
      "step": 2300
    },
    {
      "epoch": 1.174974567650051,
      "grad_norm": 2.1214497089385986,
      "learning_rate": 4.882502543234995e-05,
      "loss": 3.944,
      "step": 2310
    },
    {
      "epoch": 1.180061037639878,
      "grad_norm": 1.6276074647903442,
      "learning_rate": 4.881993896236012e-05,
      "loss": 3.9378,
      "step": 2320
    },
    {
      "epoch": 1.1851475076297049,
      "grad_norm": 1.5870987176895142,
      "learning_rate": 4.88148524923703e-05,
      "loss": 3.9637,
      "step": 2330
    },
    {
      "epoch": 1.190233977619532,
      "grad_norm": 2.0462143421173096,
      "learning_rate": 4.880976602238047e-05,
      "loss": 3.9431,
      "step": 2340
    },
    {
      "epoch": 1.195320447609359,
      "grad_norm": 2.922961711883545,
      "learning_rate": 4.880467955239064e-05,
      "loss": 3.9334,
      "step": 2350
    },
    {
      "epoch": 1.200406917599186,
      "grad_norm": 1.9806867837905884,
      "learning_rate": 4.8799593082400816e-05,
      "loss": 3.9257,
      "step": 2360
    },
    {
      "epoch": 1.2054933875890133,
      "grad_norm": 2.251918077468872,
      "learning_rate": 4.879450661241099e-05,
      "loss": 4.068,
      "step": 2370
    },
    {
      "epoch": 1.2105798575788402,
      "grad_norm": 2.077712297439575,
      "learning_rate": 4.878942014242116e-05,
      "loss": 3.9651,
      "step": 2380
    },
    {
      "epoch": 1.2156663275686674,
      "grad_norm": 2.7639362812042236,
      "learning_rate": 4.878433367243134e-05,
      "loss": 3.9453,
      "step": 2390
    },
    {
      "epoch": 1.2207527975584944,
      "grad_norm": 3.0231478214263916,
      "learning_rate": 4.877924720244151e-05,
      "loss": 3.967,
      "step": 2400
    },
    {
      "epoch": 1.2258392675483214,
      "grad_norm": 2.7401299476623535,
      "learning_rate": 4.877416073245168e-05,
      "loss": 3.8482,
      "step": 2410
    },
    {
      "epoch": 1.2309257375381486,
      "grad_norm": 2.4448115825653076,
      "learning_rate": 4.8769074262461855e-05,
      "loss": 3.9619,
      "step": 2420
    },
    {
      "epoch": 1.2360122075279756,
      "grad_norm": 2.610426664352417,
      "learning_rate": 4.8763987792472025e-05,
      "loss": 3.8927,
      "step": 2430
    },
    {
      "epoch": 1.2410986775178026,
      "grad_norm": 2.169914960861206,
      "learning_rate": 4.8758901322482195e-05,
      "loss": 3.9062,
      "step": 2440
    },
    {
      "epoch": 1.2461851475076298,
      "grad_norm": 1.9923087358474731,
      "learning_rate": 4.875381485249237e-05,
      "loss": 3.9691,
      "step": 2450
    },
    {
      "epoch": 1.2512716174974567,
      "grad_norm": 1.861140489578247,
      "learning_rate": 4.874872838250255e-05,
      "loss": 3.9622,
      "step": 2460
    },
    {
      "epoch": 1.2563580874872837,
      "grad_norm": 2.7623417377471924,
      "learning_rate": 4.874364191251272e-05,
      "loss": 4.0254,
      "step": 2470
    },
    {
      "epoch": 1.261444557477111,
      "grad_norm": 2.691542148590088,
      "learning_rate": 4.8738555442522895e-05,
      "loss": 3.978,
      "step": 2480
    },
    {
      "epoch": 1.266531027466938,
      "grad_norm": 2.4289944171905518,
      "learning_rate": 4.8733468972533065e-05,
      "loss": 3.9974,
      "step": 2490
    },
    {
      "epoch": 1.2716174974567651,
      "grad_norm": 1.9797954559326172,
      "learning_rate": 4.8728382502543235e-05,
      "loss": 3.9843,
      "step": 2500
    },
    {
      "epoch": 1.276703967446592,
      "grad_norm": 2.807321310043335,
      "learning_rate": 4.872329603255341e-05,
      "loss": 3.9557,
      "step": 2510
    },
    {
      "epoch": 1.281790437436419,
      "grad_norm": 2.413821220397949,
      "learning_rate": 4.871820956256358e-05,
      "loss": 3.9128,
      "step": 2520
    },
    {
      "epoch": 1.286876907426246,
      "grad_norm": 2.500555992126465,
      "learning_rate": 4.871312309257375e-05,
      "loss": 4.0065,
      "step": 2530
    },
    {
      "epoch": 1.2919633774160733,
      "grad_norm": 2.4103293418884277,
      "learning_rate": 4.870803662258393e-05,
      "loss": 3.9594,
      "step": 2540
    },
    {
      "epoch": 1.2970498474059002,
      "grad_norm": 2.211968421936035,
      "learning_rate": 4.8702950152594105e-05,
      "loss": 3.9439,
      "step": 2550
    },
    {
      "epoch": 1.3021363173957274,
      "grad_norm": 1.8204437494277954,
      "learning_rate": 4.8697863682604274e-05,
      "loss": 3.8638,
      "step": 2560
    },
    {
      "epoch": 1.3072227873855544,
      "grad_norm": 2.9566521644592285,
      "learning_rate": 4.869277721261445e-05,
      "loss": 3.9171,
      "step": 2570
    },
    {
      "epoch": 1.3123092573753814,
      "grad_norm": 2.778378963470459,
      "learning_rate": 4.868769074262462e-05,
      "loss": 3.9873,
      "step": 2580
    },
    {
      "epoch": 1.3173957273652086,
      "grad_norm": 2.2707302570343018,
      "learning_rate": 4.86826042726348e-05,
      "loss": 3.897,
      "step": 2590
    },
    {
      "epoch": 1.3224821973550356,
      "grad_norm": 2.2261154651641846,
      "learning_rate": 4.867751780264497e-05,
      "loss": 3.9147,
      "step": 2600
    },
    {
      "epoch": 1.3275686673448628,
      "grad_norm": 2.997498035430908,
      "learning_rate": 4.867243133265514e-05,
      "loss": 3.9852,
      "step": 2610
    },
    {
      "epoch": 1.3326551373346898,
      "grad_norm": 2.813624143600464,
      "learning_rate": 4.8667344862665314e-05,
      "loss": 3.9209,
      "step": 2620
    },
    {
      "epoch": 1.3377416073245167,
      "grad_norm": 2.183887004852295,
      "learning_rate": 4.8662258392675484e-05,
      "loss": 4.0062,
      "step": 2630
    },
    {
      "epoch": 1.3428280773143437,
      "grad_norm": 1.90959632396698,
      "learning_rate": 4.8657171922685654e-05,
      "loss": 3.9272,
      "step": 2640
    },
    {
      "epoch": 1.347914547304171,
      "grad_norm": 2.0961248874664307,
      "learning_rate": 4.865208545269583e-05,
      "loss": 3.9862,
      "step": 2650
    },
    {
      "epoch": 1.353001017293998,
      "grad_norm": 3.1883022785186768,
      "learning_rate": 4.864699898270601e-05,
      "loss": 3.9133,
      "step": 2660
    },
    {
      "epoch": 1.3580874872838251,
      "grad_norm": 3.58553409576416,
      "learning_rate": 4.864191251271618e-05,
      "loss": 3.9675,
      "step": 2670
    },
    {
      "epoch": 1.363173957273652,
      "grad_norm": 2.64648699760437,
      "learning_rate": 4.8636826042726354e-05,
      "loss": 3.9066,
      "step": 2680
    },
    {
      "epoch": 1.368260427263479,
      "grad_norm": 3.3454434871673584,
      "learning_rate": 4.8631739572736524e-05,
      "loss": 3.9468,
      "step": 2690
    },
    {
      "epoch": 1.3733468972533063,
      "grad_norm": 2.500591278076172,
      "learning_rate": 4.8626653102746694e-05,
      "loss": 3.8322,
      "step": 2700
    },
    {
      "epoch": 1.3784333672431333,
      "grad_norm": 2.02000093460083,
      "learning_rate": 4.862156663275687e-05,
      "loss": 3.9598,
      "step": 2710
    },
    {
      "epoch": 1.3835198372329605,
      "grad_norm": 3.3726367950439453,
      "learning_rate": 4.861648016276704e-05,
      "loss": 3.9125,
      "step": 2720
    },
    {
      "epoch": 1.3886063072227874,
      "grad_norm": 3.754370927810669,
      "learning_rate": 4.861139369277721e-05,
      "loss": 3.9597,
      "step": 2730
    },
    {
      "epoch": 1.3936927772126144,
      "grad_norm": 2.5952484607696533,
      "learning_rate": 4.860630722278739e-05,
      "loss": 3.8853,
      "step": 2740
    },
    {
      "epoch": 1.3987792472024414,
      "grad_norm": 3.5110955238342285,
      "learning_rate": 4.860122075279756e-05,
      "loss": 3.9132,
      "step": 2750
    },
    {
      "epoch": 1.4038657171922686,
      "grad_norm": 2.1168408393859863,
      "learning_rate": 4.859613428280773e-05,
      "loss": 3.9108,
      "step": 2760
    },
    {
      "epoch": 1.4089521871820956,
      "grad_norm": 2.5722568035125732,
      "learning_rate": 4.859104781281791e-05,
      "loss": 3.9318,
      "step": 2770
    },
    {
      "epoch": 1.4140386571719228,
      "grad_norm": 2.604870557785034,
      "learning_rate": 4.858596134282808e-05,
      "loss": 3.9304,
      "step": 2780
    },
    {
      "epoch": 1.4191251271617498,
      "grad_norm": 3.0501840114593506,
      "learning_rate": 4.858087487283825e-05,
      "loss": 3.8976,
      "step": 2790
    },
    {
      "epoch": 1.4242115971515767,
      "grad_norm": 3.528156280517578,
      "learning_rate": 4.8575788402848426e-05,
      "loss": 3.9468,
      "step": 2800
    },
    {
      "epoch": 1.4292980671414037,
      "grad_norm": 2.1598286628723145,
      "learning_rate": 4.8570701932858596e-05,
      "loss": 3.8417,
      "step": 2810
    },
    {
      "epoch": 1.434384537131231,
      "grad_norm": 2.7477807998657227,
      "learning_rate": 4.8565615462868766e-05,
      "loss": 3.8616,
      "step": 2820
    },
    {
      "epoch": 1.439471007121058,
      "grad_norm": 2.1633222103118896,
      "learning_rate": 4.856052899287894e-05,
      "loss": 3.9145,
      "step": 2830
    },
    {
      "epoch": 1.4445574771108851,
      "grad_norm": 3.123568296432495,
      "learning_rate": 4.855544252288912e-05,
      "loss": 3.8761,
      "step": 2840
    },
    {
      "epoch": 1.449643947100712,
      "grad_norm": 2.3529410362243652,
      "learning_rate": 4.8550356052899296e-05,
      "loss": 3.9291,
      "step": 2850
    },
    {
      "epoch": 1.454730417090539,
      "grad_norm": 2.1702964305877686,
      "learning_rate": 4.8545269582909466e-05,
      "loss": 3.9836,
      "step": 2860
    },
    {
      "epoch": 1.4598168870803663,
      "grad_norm": 6.299477577209473,
      "learning_rate": 4.8540183112919636e-05,
      "loss": 3.8715,
      "step": 2870
    },
    {
      "epoch": 1.4649033570701933,
      "grad_norm": 1.9678689241409302,
      "learning_rate": 4.853509664292981e-05,
      "loss": 3.8707,
      "step": 2880
    },
    {
      "epoch": 1.4699898270600205,
      "grad_norm": 2.019385576248169,
      "learning_rate": 4.853001017293998e-05,
      "loss": 3.8956,
      "step": 2890
    },
    {
      "epoch": 1.4750762970498474,
      "grad_norm": 2.589125633239746,
      "learning_rate": 4.852492370295015e-05,
      "loss": 3.856,
      "step": 2900
    },
    {
      "epoch": 1.4801627670396744,
      "grad_norm": 2.6016361713409424,
      "learning_rate": 4.851983723296033e-05,
      "loss": 3.8752,
      "step": 2910
    },
    {
      "epoch": 1.4852492370295014,
      "grad_norm": 1.8504096269607544,
      "learning_rate": 4.85147507629705e-05,
      "loss": 3.8341,
      "step": 2920
    },
    {
      "epoch": 1.4903357070193286,
      "grad_norm": 2.361558437347412,
      "learning_rate": 4.850966429298067e-05,
      "loss": 3.903,
      "step": 2930
    },
    {
      "epoch": 1.4954221770091556,
      "grad_norm": 2.77169132232666,
      "learning_rate": 4.8504577822990846e-05,
      "loss": 3.9602,
      "step": 2940
    },
    {
      "epoch": 1.5005086469989828,
      "grad_norm": 2.268280506134033,
      "learning_rate": 4.849949135300102e-05,
      "loss": 3.8591,
      "step": 2950
    },
    {
      "epoch": 1.5055951169888098,
      "grad_norm": 2.139366388320923,
      "learning_rate": 4.849440488301119e-05,
      "loss": 3.8656,
      "step": 2960
    },
    {
      "epoch": 1.5106815869786367,
      "grad_norm": 2.2815325260162354,
      "learning_rate": 4.848931841302137e-05,
      "loss": 3.9288,
      "step": 2970
    },
    {
      "epoch": 1.5157680569684637,
      "grad_norm": 3.4343669414520264,
      "learning_rate": 4.848423194303154e-05,
      "loss": 3.8513,
      "step": 2980
    },
    {
      "epoch": 1.520854526958291,
      "grad_norm": 1.9289299249649048,
      "learning_rate": 4.847914547304171e-05,
      "loss": 3.9442,
      "step": 2990
    },
    {
      "epoch": 1.5259409969481181,
      "grad_norm": 3.3456056118011475,
      "learning_rate": 4.8474059003051885e-05,
      "loss": 3.8703,
      "step": 3000
    },
    {
      "epoch": 1.5310274669379451,
      "grad_norm": 3.178622007369995,
      "learning_rate": 4.8468972533062055e-05,
      "loss": 3.8683,
      "step": 3010
    },
    {
      "epoch": 1.536113936927772,
      "grad_norm": 2.766413450241089,
      "learning_rate": 4.8463886063072225e-05,
      "loss": 3.8761,
      "step": 3020
    },
    {
      "epoch": 1.541200406917599,
      "grad_norm": 2.2417070865631104,
      "learning_rate": 4.84587995930824e-05,
      "loss": 3.9268,
      "step": 3030
    },
    {
      "epoch": 1.5462868769074263,
      "grad_norm": 2.6189918518066406,
      "learning_rate": 4.845371312309258e-05,
      "loss": 3.9012,
      "step": 3040
    },
    {
      "epoch": 1.5513733468972533,
      "grad_norm": 1.843309760093689,
      "learning_rate": 4.844862665310275e-05,
      "loss": 3.9043,
      "step": 3050
    },
    {
      "epoch": 1.5564598168870805,
      "grad_norm": 3.2836098670959473,
      "learning_rate": 4.8443540183112925e-05,
      "loss": 3.8718,
      "step": 3060
    },
    {
      "epoch": 1.5615462868769074,
      "grad_norm": 2.080643892288208,
      "learning_rate": 4.8438453713123095e-05,
      "loss": 3.8666,
      "step": 3070
    },
    {
      "epoch": 1.5666327568667344,
      "grad_norm": 2.10274600982666,
      "learning_rate": 4.8433367243133265e-05,
      "loss": 3.9361,
      "step": 3080
    },
    {
      "epoch": 1.5717192268565614,
      "grad_norm": 3.276583671569824,
      "learning_rate": 4.842828077314344e-05,
      "loss": 3.8648,
      "step": 3090
    },
    {
      "epoch": 1.5768056968463886,
      "grad_norm": 2.5222623348236084,
      "learning_rate": 4.842319430315361e-05,
      "loss": 3.89,
      "step": 3100
    },
    {
      "epoch": 1.5818921668362158,
      "grad_norm": 2.2307581901550293,
      "learning_rate": 4.841810783316379e-05,
      "loss": 3.8316,
      "step": 3110
    },
    {
      "epoch": 1.5869786368260428,
      "grad_norm": 3.31823468208313,
      "learning_rate": 4.841302136317396e-05,
      "loss": 3.8627,
      "step": 3120
    },
    {
      "epoch": 1.5920651068158698,
      "grad_norm": 3.992385149002075,
      "learning_rate": 4.8407934893184135e-05,
      "loss": 3.8764,
      "step": 3130
    },
    {
      "epoch": 1.5971515768056967,
      "grad_norm": 3.4003562927246094,
      "learning_rate": 4.840284842319431e-05,
      "loss": 3.8693,
      "step": 3140
    },
    {
      "epoch": 1.602238046795524,
      "grad_norm": 1.8446124792099,
      "learning_rate": 4.839776195320448e-05,
      "loss": 3.9082,
      "step": 3150
    },
    {
      "epoch": 1.607324516785351,
      "grad_norm": 2.199260711669922,
      "learning_rate": 4.839267548321465e-05,
      "loss": 3.9227,
      "step": 3160
    },
    {
      "epoch": 1.6124109867751781,
      "grad_norm": 2.4016170501708984,
      "learning_rate": 4.838758901322483e-05,
      "loss": 3.8335,
      "step": 3170
    },
    {
      "epoch": 1.6174974567650051,
      "grad_norm": 2.32049298286438,
      "learning_rate": 4.8382502543235e-05,
      "loss": 3.8902,
      "step": 3180
    },
    {
      "epoch": 1.622583926754832,
      "grad_norm": 2.3983724117279053,
      "learning_rate": 4.837741607324517e-05,
      "loss": 3.782,
      "step": 3190
    },
    {
      "epoch": 1.627670396744659,
      "grad_norm": 2.9348959922790527,
      "learning_rate": 4.8372329603255344e-05,
      "loss": 3.8834,
      "step": 3200
    },
    {
      "epoch": 1.6327568667344863,
      "grad_norm": 2.4127590656280518,
      "learning_rate": 4.8367243133265514e-05,
      "loss": 3.8701,
      "step": 3210
    },
    {
      "epoch": 1.6378433367243135,
      "grad_norm": 2.564382553100586,
      "learning_rate": 4.836215666327569e-05,
      "loss": 3.8865,
      "step": 3220
    },
    {
      "epoch": 1.6429298067141405,
      "grad_norm": 2.2278523445129395,
      "learning_rate": 4.835707019328586e-05,
      "loss": 3.9217,
      "step": 3230
    },
    {
      "epoch": 1.6480162767039674,
      "grad_norm": 3.367666006088257,
      "learning_rate": 4.835198372329604e-05,
      "loss": 3.8624,
      "step": 3240
    },
    {
      "epoch": 1.6531027466937944,
      "grad_norm": 2.4702365398406982,
      "learning_rate": 4.834689725330621e-05,
      "loss": 3.8736,
      "step": 3250
    },
    {
      "epoch": 1.6581892166836214,
      "grad_norm": 4.007655143737793,
      "learning_rate": 4.8341810783316384e-05,
      "loss": 3.8503,
      "step": 3260
    },
    {
      "epoch": 1.6632756866734486,
      "grad_norm": 3.676967144012451,
      "learning_rate": 4.8336724313326554e-05,
      "loss": 3.8622,
      "step": 3270
    },
    {
      "epoch": 1.6683621566632758,
      "grad_norm": 2.4301090240478516,
      "learning_rate": 4.8331637843336724e-05,
      "loss": 3.8197,
      "step": 3280
    },
    {
      "epoch": 1.6734486266531028,
      "grad_norm": 3.7800233364105225,
      "learning_rate": 4.83265513733469e-05,
      "loss": 3.8499,
      "step": 3290
    },
    {
      "epoch": 1.6785350966429298,
      "grad_norm": 3.440908432006836,
      "learning_rate": 4.832146490335707e-05,
      "loss": 3.8585,
      "step": 3300
    },
    {
      "epoch": 1.6836215666327567,
      "grad_norm": 2.779834508895874,
      "learning_rate": 4.831637843336724e-05,
      "loss": 3.9192,
      "step": 3310
    },
    {
      "epoch": 1.688708036622584,
      "grad_norm": 2.7942798137664795,
      "learning_rate": 4.831129196337742e-05,
      "loss": 3.8236,
      "step": 3320
    },
    {
      "epoch": 1.693794506612411,
      "grad_norm": 2.871171236038208,
      "learning_rate": 4.830620549338759e-05,
      "loss": 3.8742,
      "step": 3330
    },
    {
      "epoch": 1.6988809766022381,
      "grad_norm": 2.3870112895965576,
      "learning_rate": 4.830111902339776e-05,
      "loss": 3.8669,
      "step": 3340
    },
    {
      "epoch": 1.7039674465920651,
      "grad_norm": 4.039936542510986,
      "learning_rate": 4.829603255340794e-05,
      "loss": 3.917,
      "step": 3350
    },
    {
      "epoch": 1.709053916581892,
      "grad_norm": 3.6603236198425293,
      "learning_rate": 4.829094608341811e-05,
      "loss": 3.8463,
      "step": 3360
    },
    {
      "epoch": 1.714140386571719,
      "grad_norm": 2.9734718799591064,
      "learning_rate": 4.828585961342828e-05,
      "loss": 3.8646,
      "step": 3370
    },
    {
      "epoch": 1.7192268565615463,
      "grad_norm": 2.11411452293396,
      "learning_rate": 4.8280773143438456e-05,
      "loss": 3.82,
      "step": 3380
    },
    {
      "epoch": 1.7243133265513735,
      "grad_norm": 2.353867530822754,
      "learning_rate": 4.8275686673448626e-05,
      "loss": 3.8583,
      "step": 3390
    },
    {
      "epoch": 1.7293997965412005,
      "grad_norm": 4.884743690490723,
      "learning_rate": 4.82706002034588e-05,
      "loss": 3.8391,
      "step": 3400
    },
    {
      "epoch": 1.7344862665310274,
      "grad_norm": 2.719420909881592,
      "learning_rate": 4.826551373346897e-05,
      "loss": 3.8841,
      "step": 3410
    },
    {
      "epoch": 1.7395727365208544,
      "grad_norm": 3.1500298976898193,
      "learning_rate": 4.826042726347915e-05,
      "loss": 3.9163,
      "step": 3420
    },
    {
      "epoch": 1.7446592065106816,
      "grad_norm": 3.3296663761138916,
      "learning_rate": 4.8255340793489326e-05,
      "loss": 3.8473,
      "step": 3430
    },
    {
      "epoch": 1.7497456765005086,
      "grad_norm": 2.9359757900238037,
      "learning_rate": 4.8250254323499496e-05,
      "loss": 3.891,
      "step": 3440
    },
    {
      "epoch": 1.7548321464903358,
      "grad_norm": 2.534726858139038,
      "learning_rate": 4.8245167853509666e-05,
      "loss": 3.8741,
      "step": 3450
    },
    {
      "epoch": 1.7599186164801628,
      "grad_norm": 3.068474531173706,
      "learning_rate": 4.824008138351984e-05,
      "loss": 3.8028,
      "step": 3460
    },
    {
      "epoch": 1.7650050864699898,
      "grad_norm": 2.4252750873565674,
      "learning_rate": 4.823499491353001e-05,
      "loss": 3.8805,
      "step": 3470
    },
    {
      "epoch": 1.7700915564598168,
      "grad_norm": 2.3944170475006104,
      "learning_rate": 4.822990844354018e-05,
      "loss": 3.8117,
      "step": 3480
    },
    {
      "epoch": 1.775178026449644,
      "grad_norm": 2.5047669410705566,
      "learning_rate": 4.822482197355036e-05,
      "loss": 3.8235,
      "step": 3490
    },
    {
      "epoch": 1.7802644964394712,
      "grad_norm": 3.883359432220459,
      "learning_rate": 4.821973550356053e-05,
      "loss": 3.8281,
      "step": 3500
    },
    {
      "epoch": 1.7853509664292981,
      "grad_norm": 3.714405059814453,
      "learning_rate": 4.8214649033570706e-05,
      "loss": 3.7933,
      "step": 3510
    },
    {
      "epoch": 1.7904374364191251,
      "grad_norm": 2.343216896057129,
      "learning_rate": 4.820956256358088e-05,
      "loss": 3.8734,
      "step": 3520
    },
    {
      "epoch": 1.795523906408952,
      "grad_norm": 2.821183443069458,
      "learning_rate": 4.820447609359105e-05,
      "loss": 3.7989,
      "step": 3530
    },
    {
      "epoch": 1.8006103763987793,
      "grad_norm": 3.595947742462158,
      "learning_rate": 4.819938962360122e-05,
      "loss": 3.8246,
      "step": 3540
    },
    {
      "epoch": 1.8056968463886063,
      "grad_norm": 3.038120746612549,
      "learning_rate": 4.81943031536114e-05,
      "loss": 3.7674,
      "step": 3550
    },
    {
      "epoch": 1.8107833163784335,
      "grad_norm": 2.997492790222168,
      "learning_rate": 4.818921668362157e-05,
      "loss": 3.8784,
      "step": 3560
    },
    {
      "epoch": 1.8158697863682605,
      "grad_norm": 2.9268176555633545,
      "learning_rate": 4.818413021363174e-05,
      "loss": 3.8091,
      "step": 3570
    },
    {
      "epoch": 1.8209562563580874,
      "grad_norm": 2.6341984272003174,
      "learning_rate": 4.8179043743641915e-05,
      "loss": 3.7538,
      "step": 3580
    },
    {
      "epoch": 1.8260427263479144,
      "grad_norm": 3.3541431427001953,
      "learning_rate": 4.8173957273652085e-05,
      "loss": 3.7694,
      "step": 3590
    },
    {
      "epoch": 1.8311291963377416,
      "grad_norm": 3.993887186050415,
      "learning_rate": 4.8168870803662255e-05,
      "loss": 3.8071,
      "step": 3600
    },
    {
      "epoch": 1.8362156663275688,
      "grad_norm": 2.154033899307251,
      "learning_rate": 4.816378433367243e-05,
      "loss": 3.8258,
      "step": 3610
    },
    {
      "epoch": 1.8413021363173958,
      "grad_norm": 2.2375717163085938,
      "learning_rate": 4.815869786368261e-05,
      "loss": 3.8102,
      "step": 3620
    },
    {
      "epoch": 1.8463886063072228,
      "grad_norm": 2.48102068901062,
      "learning_rate": 4.815361139369278e-05,
      "loss": 3.8123,
      "step": 3630
    },
    {
      "epoch": 1.8514750762970498,
      "grad_norm": 2.991802215576172,
      "learning_rate": 4.8148524923702955e-05,
      "loss": 3.8595,
      "step": 3640
    },
    {
      "epoch": 1.8565615462868768,
      "grad_norm": 3.0669162273406982,
      "learning_rate": 4.8143438453713125e-05,
      "loss": 3.7474,
      "step": 3650
    },
    {
      "epoch": 1.861648016276704,
      "grad_norm": 2.7170755863189697,
      "learning_rate": 4.81383519837233e-05,
      "loss": 3.7943,
      "step": 3660
    },
    {
      "epoch": 1.8667344862665312,
      "grad_norm": 3.6425702571868896,
      "learning_rate": 4.813326551373347e-05,
      "loss": 3.833,
      "step": 3670
    },
    {
      "epoch": 1.8718209562563581,
      "grad_norm": 2.2230606079101562,
      "learning_rate": 4.812817904374364e-05,
      "loss": 3.863,
      "step": 3680
    },
    {
      "epoch": 1.8769074262461851,
      "grad_norm": 2.9699721336364746,
      "learning_rate": 4.812309257375382e-05,
      "loss": 3.8508,
      "step": 3690
    },
    {
      "epoch": 1.881993896236012,
      "grad_norm": 2.8466637134552,
      "learning_rate": 4.811800610376399e-05,
      "loss": 3.8133,
      "step": 3700
    },
    {
      "epoch": 1.8870803662258393,
      "grad_norm": 3.647420883178711,
      "learning_rate": 4.8112919633774165e-05,
      "loss": 3.8016,
      "step": 3710
    },
    {
      "epoch": 1.8921668362156663,
      "grad_norm": 3.686527729034424,
      "learning_rate": 4.810783316378434e-05,
      "loss": 3.8999,
      "step": 3720
    },
    {
      "epoch": 1.8972533062054935,
      "grad_norm": 2.251647710800171,
      "learning_rate": 4.810274669379451e-05,
      "loss": 3.7827,
      "step": 3730
    },
    {
      "epoch": 1.9023397761953205,
      "grad_norm": 3.028019428253174,
      "learning_rate": 4.809766022380468e-05,
      "loss": 3.7994,
      "step": 3740
    },
    {
      "epoch": 1.9074262461851474,
      "grad_norm": 3.234785318374634,
      "learning_rate": 4.809257375381486e-05,
      "loss": 3.8321,
      "step": 3750
    },
    {
      "epoch": 1.9125127161749744,
      "grad_norm": 3.6764044761657715,
      "learning_rate": 4.808748728382503e-05,
      "loss": 3.8554,
      "step": 3760
    },
    {
      "epoch": 1.9175991861648016,
      "grad_norm": 3.336867570877075,
      "learning_rate": 4.80824008138352e-05,
      "loss": 3.7937,
      "step": 3770
    },
    {
      "epoch": 1.9226856561546288,
      "grad_norm": 3.244828701019287,
      "learning_rate": 4.8077314343845374e-05,
      "loss": 3.789,
      "step": 3780
    },
    {
      "epoch": 1.9277721261444558,
      "grad_norm": 2.5136430263519287,
      "learning_rate": 4.8072227873855544e-05,
      "loss": 3.7683,
      "step": 3790
    },
    {
      "epoch": 1.9328585961342828,
      "grad_norm": 5.260274410247803,
      "learning_rate": 4.806714140386572e-05,
      "loss": 3.8014,
      "step": 3800
    },
    {
      "epoch": 1.9379450661241098,
      "grad_norm": 3.4694857597351074,
      "learning_rate": 4.80620549338759e-05,
      "loss": 3.8004,
      "step": 3810
    },
    {
      "epoch": 1.943031536113937,
      "grad_norm": 2.9836699962615967,
      "learning_rate": 4.805696846388607e-05,
      "loss": 3.7309,
      "step": 3820
    },
    {
      "epoch": 1.948118006103764,
      "grad_norm": 3.3965353965759277,
      "learning_rate": 4.805188199389624e-05,
      "loss": 3.8469,
      "step": 3830
    },
    {
      "epoch": 1.9532044760935912,
      "grad_norm": 3.9317257404327393,
      "learning_rate": 4.8046795523906414e-05,
      "loss": 3.8225,
      "step": 3840
    },
    {
      "epoch": 1.9582909460834181,
      "grad_norm": 5.062075614929199,
      "learning_rate": 4.8041709053916584e-05,
      "loss": 3.7682,
      "step": 3850
    },
    {
      "epoch": 1.9633774160732451,
      "grad_norm": 3.7126967906951904,
      "learning_rate": 4.8036622583926754e-05,
      "loss": 3.892,
      "step": 3860
    },
    {
      "epoch": 1.968463886063072,
      "grad_norm": 4.513299465179443,
      "learning_rate": 4.803153611393693e-05,
      "loss": 3.8108,
      "step": 3870
    },
    {
      "epoch": 1.9735503560528993,
      "grad_norm": 2.908249616622925,
      "learning_rate": 4.80264496439471e-05,
      "loss": 3.8366,
      "step": 3880
    },
    {
      "epoch": 1.9786368260427265,
      "grad_norm": 5.220104694366455,
      "learning_rate": 4.802136317395727e-05,
      "loss": 3.826,
      "step": 3890
    },
    {
      "epoch": 1.9837232960325535,
      "grad_norm": 3.4382638931274414,
      "learning_rate": 4.801627670396745e-05,
      "loss": 3.7856,
      "step": 3900
    },
    {
      "epoch": 1.9888097660223805,
      "grad_norm": 3.960299015045166,
      "learning_rate": 4.801119023397762e-05,
      "loss": 3.843,
      "step": 3910
    },
    {
      "epoch": 1.9938962360122074,
      "grad_norm": 5.232454776763916,
      "learning_rate": 4.80061037639878e-05,
      "loss": 3.7792,
      "step": 3920
    },
    {
      "epoch": 1.9989827060020344,
      "grad_norm": 2.63761043548584,
      "learning_rate": 4.800101729399797e-05,
      "loss": 3.8942,
      "step": 3930
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.822338104248047,
      "eval_runtime": 2.6649,
      "eval_samples_per_second": 1041.326,
      "eval_steps_per_second": 130.213,
      "step": 3932
    },
    {
      "epoch": 2.004069175991862,
      "grad_norm": 3.3009870052337646,
      "learning_rate": 4.799593082400814e-05,
      "loss": 3.7859,
      "step": 3940
    },
    {
      "epoch": 2.009155645981689,
      "grad_norm": 2.3990509510040283,
      "learning_rate": 4.7990844354018317e-05,
      "loss": 3.6946,
      "step": 3950
    },
    {
      "epoch": 2.014242115971516,
      "grad_norm": 3.3798155784606934,
      "learning_rate": 4.7985757884028486e-05,
      "loss": 3.824,
      "step": 3960
    },
    {
      "epoch": 2.019328585961343,
      "grad_norm": 4.404094696044922,
      "learning_rate": 4.7980671414038656e-05,
      "loss": 3.8429,
      "step": 3970
    },
    {
      "epoch": 2.0244150559511698,
      "grad_norm": 3.420275926589966,
      "learning_rate": 4.797558494404883e-05,
      "loss": 3.8101,
      "step": 3980
    },
    {
      "epoch": 2.0295015259409968,
      "grad_norm": 3.7091214656829834,
      "learning_rate": 4.7970498474059e-05,
      "loss": 3.7007,
      "step": 3990
    },
    {
      "epoch": 2.034587995930824,
      "grad_norm": 5.376887321472168,
      "learning_rate": 4.796541200406918e-05,
      "loss": 3.789,
      "step": 4000
    },
    {
      "epoch": 2.039674465920651,
      "grad_norm": 2.9828460216522217,
      "learning_rate": 4.7960325534079356e-05,
      "loss": 3.9175,
      "step": 4010
    },
    {
      "epoch": 2.044760935910478,
      "grad_norm": 3.4961628913879395,
      "learning_rate": 4.7955239064089526e-05,
      "loss": 3.7196,
      "step": 4020
    },
    {
      "epoch": 2.049847405900305,
      "grad_norm": 3.5820329189300537,
      "learning_rate": 4.7950152594099696e-05,
      "loss": 3.721,
      "step": 4030
    },
    {
      "epoch": 2.054933875890132,
      "grad_norm": 3.178952932357788,
      "learning_rate": 4.794506612410987e-05,
      "loss": 3.7873,
      "step": 4040
    },
    {
      "epoch": 2.0600203458799595,
      "grad_norm": 4.098269462585449,
      "learning_rate": 4.793997965412004e-05,
      "loss": 3.7829,
      "step": 4050
    },
    {
      "epoch": 2.0651068158697865,
      "grad_norm": 3.055269479751587,
      "learning_rate": 4.793489318413021e-05,
      "loss": 3.7502,
      "step": 4060
    },
    {
      "epoch": 2.0701932858596135,
      "grad_norm": 2.66963529586792,
      "learning_rate": 4.792980671414039e-05,
      "loss": 3.8583,
      "step": 4070
    },
    {
      "epoch": 2.0752797558494405,
      "grad_norm": 3.066627264022827,
      "learning_rate": 4.792472024415056e-05,
      "loss": 3.8102,
      "step": 4080
    },
    {
      "epoch": 2.0803662258392674,
      "grad_norm": 3.998633623123169,
      "learning_rate": 4.7919633774160736e-05,
      "loss": 3.7599,
      "step": 4090
    },
    {
      "epoch": 2.0854526958290944,
      "grad_norm": 3.2907497882843018,
      "learning_rate": 4.791454730417091e-05,
      "loss": 3.7392,
      "step": 4100
    },
    {
      "epoch": 2.090539165818922,
      "grad_norm": 3.4080381393432617,
      "learning_rate": 4.790946083418108e-05,
      "loss": 3.7252,
      "step": 4110
    },
    {
      "epoch": 2.095625635808749,
      "grad_norm": 3.682750701904297,
      "learning_rate": 4.790437436419125e-05,
      "loss": 3.7968,
      "step": 4120
    },
    {
      "epoch": 2.100712105798576,
      "grad_norm": 3.1150565147399902,
      "learning_rate": 4.789928789420143e-05,
      "loss": 3.8283,
      "step": 4130
    },
    {
      "epoch": 2.105798575788403,
      "grad_norm": 3.0871498584747314,
      "learning_rate": 4.78942014242116e-05,
      "loss": 3.8037,
      "step": 4140
    },
    {
      "epoch": 2.1108850457782298,
      "grad_norm": 3.301619291305542,
      "learning_rate": 4.788911495422177e-05,
      "loss": 3.8016,
      "step": 4150
    },
    {
      "epoch": 2.1159715157680568,
      "grad_norm": 5.515467643737793,
      "learning_rate": 4.7884028484231945e-05,
      "loss": 3.7351,
      "step": 4160
    },
    {
      "epoch": 2.121057985757884,
      "grad_norm": 2.8090403079986572,
      "learning_rate": 4.7878942014242115e-05,
      "loss": 3.7198,
      "step": 4170
    },
    {
      "epoch": 2.126144455747711,
      "grad_norm": 4.266490459442139,
      "learning_rate": 4.787385554425229e-05,
      "loss": 3.7448,
      "step": 4180
    },
    {
      "epoch": 2.131230925737538,
      "grad_norm": 4.604043483734131,
      "learning_rate": 4.786876907426246e-05,
      "loss": 3.8793,
      "step": 4190
    },
    {
      "epoch": 2.136317395727365,
      "grad_norm": 3.466778516769409,
      "learning_rate": 4.786368260427264e-05,
      "loss": 3.7978,
      "step": 4200
    },
    {
      "epoch": 2.141403865717192,
      "grad_norm": 3.406524896621704,
      "learning_rate": 4.7858596134282815e-05,
      "loss": 3.672,
      "step": 4210
    },
    {
      "epoch": 2.1464903357070195,
      "grad_norm": 3.2007479667663574,
      "learning_rate": 4.7853509664292985e-05,
      "loss": 3.7458,
      "step": 4220
    },
    {
      "epoch": 2.1515768056968465,
      "grad_norm": 4.488451957702637,
      "learning_rate": 4.7848423194303155e-05,
      "loss": 3.7815,
      "step": 4230
    },
    {
      "epoch": 2.1566632756866735,
      "grad_norm": 4.18934440612793,
      "learning_rate": 4.784333672431333e-05,
      "loss": 3.768,
      "step": 4240
    },
    {
      "epoch": 2.1617497456765005,
      "grad_norm": 4.366158962249756,
      "learning_rate": 4.78382502543235e-05,
      "loss": 3.6843,
      "step": 4250
    },
    {
      "epoch": 2.1668362156663274,
      "grad_norm": 4.3815202713012695,
      "learning_rate": 4.783316378433367e-05,
      "loss": 3.8156,
      "step": 4260
    },
    {
      "epoch": 2.1719226856561544,
      "grad_norm": 2.2758522033691406,
      "learning_rate": 4.782807731434385e-05,
      "loss": 3.7765,
      "step": 4270
    },
    {
      "epoch": 2.177009155645982,
      "grad_norm": 3.107891082763672,
      "learning_rate": 4.782299084435402e-05,
      "loss": 3.7466,
      "step": 4280
    },
    {
      "epoch": 2.182095625635809,
      "grad_norm": 3.354189157485962,
      "learning_rate": 4.7817904374364195e-05,
      "loss": 3.7595,
      "step": 4290
    },
    {
      "epoch": 2.187182095625636,
      "grad_norm": 3.506438970565796,
      "learning_rate": 4.781281790437437e-05,
      "loss": 3.7106,
      "step": 4300
    },
    {
      "epoch": 2.192268565615463,
      "grad_norm": 3.2889552116394043,
      "learning_rate": 4.780773143438454e-05,
      "loss": 3.7488,
      "step": 4310
    },
    {
      "epoch": 2.1973550356052898,
      "grad_norm": 2.5478384494781494,
      "learning_rate": 4.780264496439471e-05,
      "loss": 3.8366,
      "step": 4320
    },
    {
      "epoch": 2.202441505595117,
      "grad_norm": 3.5290231704711914,
      "learning_rate": 4.779755849440489e-05,
      "loss": 3.7865,
      "step": 4330
    },
    {
      "epoch": 2.207527975584944,
      "grad_norm": 3.3598546981811523,
      "learning_rate": 4.779247202441506e-05,
      "loss": 3.8662,
      "step": 4340
    },
    {
      "epoch": 2.212614445574771,
      "grad_norm": 3.1380226612091064,
      "learning_rate": 4.778738555442523e-05,
      "loss": 3.7645,
      "step": 4350
    },
    {
      "epoch": 2.217700915564598,
      "grad_norm": 2.6953513622283936,
      "learning_rate": 4.7782299084435404e-05,
      "loss": 3.8181,
      "step": 4360
    },
    {
      "epoch": 2.222787385554425,
      "grad_norm": 3.2051572799682617,
      "learning_rate": 4.7777212614445574e-05,
      "loss": 3.6611,
      "step": 4370
    },
    {
      "epoch": 2.227873855544252,
      "grad_norm": 3.859872341156006,
      "learning_rate": 4.777212614445575e-05,
      "loss": 3.787,
      "step": 4380
    },
    {
      "epoch": 2.2329603255340795,
      "grad_norm": 4.926129341125488,
      "learning_rate": 4.776703967446593e-05,
      "loss": 3.7507,
      "step": 4390
    },
    {
      "epoch": 2.2380467955239065,
      "grad_norm": 2.5377938747406006,
      "learning_rate": 4.77619532044761e-05,
      "loss": 3.6993,
      "step": 4400
    },
    {
      "epoch": 2.2431332655137335,
      "grad_norm": 4.288913249969482,
      "learning_rate": 4.775686673448627e-05,
      "loss": 3.8625,
      "step": 4410
    },
    {
      "epoch": 2.2482197355035605,
      "grad_norm": 3.177579164505005,
      "learning_rate": 4.7751780264496444e-05,
      "loss": 3.6934,
      "step": 4420
    },
    {
      "epoch": 2.2533062054933874,
      "grad_norm": 3.5934462547302246,
      "learning_rate": 4.7746693794506614e-05,
      "loss": 3.7367,
      "step": 4430
    },
    {
      "epoch": 2.258392675483215,
      "grad_norm": 3.7067041397094727,
      "learning_rate": 4.7741607324516784e-05,
      "loss": 3.7389,
      "step": 4440
    },
    {
      "epoch": 2.263479145473042,
      "grad_norm": 3.248070240020752,
      "learning_rate": 4.773652085452696e-05,
      "loss": 3.7455,
      "step": 4450
    },
    {
      "epoch": 2.268565615462869,
      "grad_norm": 4.805527687072754,
      "learning_rate": 4.773143438453713e-05,
      "loss": 3.7711,
      "step": 4460
    },
    {
      "epoch": 2.273652085452696,
      "grad_norm": 4.811013698577881,
      "learning_rate": 4.772634791454731e-05,
      "loss": 3.8216,
      "step": 4470
    },
    {
      "epoch": 2.278738555442523,
      "grad_norm": 4.649559020996094,
      "learning_rate": 4.7721261444557483e-05,
      "loss": 3.7682,
      "step": 4480
    },
    {
      "epoch": 2.2838250254323498,
      "grad_norm": 4.402706623077393,
      "learning_rate": 4.7716174974567653e-05,
      "loss": 3.7639,
      "step": 4490
    },
    {
      "epoch": 2.288911495422177,
      "grad_norm": 4.548746585845947,
      "learning_rate": 4.771108850457783e-05,
      "loss": 3.7394,
      "step": 4500
    },
    {
      "epoch": 2.293997965412004,
      "grad_norm": 3.320382833480835,
      "learning_rate": 4.7706002034588e-05,
      "loss": 3.7268,
      "step": 4510
    },
    {
      "epoch": 2.299084435401831,
      "grad_norm": 2.421847105026245,
      "learning_rate": 4.770091556459817e-05,
      "loss": 3.7891,
      "step": 4520
    },
    {
      "epoch": 2.304170905391658,
      "grad_norm": 3.777848958969116,
      "learning_rate": 4.7695829094608347e-05,
      "loss": 3.7576,
      "step": 4530
    },
    {
      "epoch": 2.309257375381485,
      "grad_norm": 4.143126487731934,
      "learning_rate": 4.7690742624618516e-05,
      "loss": 3.7131,
      "step": 4540
    },
    {
      "epoch": 2.3143438453713125,
      "grad_norm": 6.256880760192871,
      "learning_rate": 4.7685656154628686e-05,
      "loss": 3.6934,
      "step": 4550
    },
    {
      "epoch": 2.3194303153611395,
      "grad_norm": 4.804635524749756,
      "learning_rate": 4.768056968463886e-05,
      "loss": 3.716,
      "step": 4560
    },
    {
      "epoch": 2.3245167853509665,
      "grad_norm": 2.7259509563446045,
      "learning_rate": 4.767548321464903e-05,
      "loss": 3.641,
      "step": 4570
    },
    {
      "epoch": 2.3296032553407935,
      "grad_norm": 5.308385848999023,
      "learning_rate": 4.767039674465921e-05,
      "loss": 3.655,
      "step": 4580
    },
    {
      "epoch": 2.3346897253306205,
      "grad_norm": 3.2066640853881836,
      "learning_rate": 4.7665310274669386e-05,
      "loss": 3.8049,
      "step": 4590
    },
    {
      "epoch": 2.3397761953204474,
      "grad_norm": 3.672332286834717,
      "learning_rate": 4.7660223804679556e-05,
      "loss": 3.7147,
      "step": 4600
    },
    {
      "epoch": 2.3448626653102744,
      "grad_norm": 3.9188222885131836,
      "learning_rate": 4.7655137334689726e-05,
      "loss": 3.795,
      "step": 4610
    },
    {
      "epoch": 2.349949135300102,
      "grad_norm": 3.4528794288635254,
      "learning_rate": 4.76500508646999e-05,
      "loss": 3.715,
      "step": 4620
    },
    {
      "epoch": 2.355035605289929,
      "grad_norm": 4.446424961090088,
      "learning_rate": 4.764496439471007e-05,
      "loss": 3.6899,
      "step": 4630
    },
    {
      "epoch": 2.360122075279756,
      "grad_norm": 3.785942554473877,
      "learning_rate": 4.763987792472024e-05,
      "loss": 3.7736,
      "step": 4640
    },
    {
      "epoch": 2.365208545269583,
      "grad_norm": 5.180270195007324,
      "learning_rate": 4.763479145473042e-05,
      "loss": 3.6831,
      "step": 4650
    },
    {
      "epoch": 2.3702950152594098,
      "grad_norm": 4.773013114929199,
      "learning_rate": 4.762970498474059e-05,
      "loss": 3.8022,
      "step": 4660
    },
    {
      "epoch": 2.375381485249237,
      "grad_norm": 4.565668106079102,
      "learning_rate": 4.7624618514750766e-05,
      "loss": 3.7295,
      "step": 4670
    },
    {
      "epoch": 2.380467955239064,
      "grad_norm": 3.4002201557159424,
      "learning_rate": 4.761953204476094e-05,
      "loss": 3.7945,
      "step": 4680
    },
    {
      "epoch": 2.385554425228891,
      "grad_norm": 3.350419521331787,
      "learning_rate": 4.761444557477111e-05,
      "loss": 3.7091,
      "step": 4690
    },
    {
      "epoch": 2.390640895218718,
      "grad_norm": 4.265203475952148,
      "learning_rate": 4.760935910478128e-05,
      "loss": 3.7394,
      "step": 4700
    },
    {
      "epoch": 2.395727365208545,
      "grad_norm": 4.637005805969238,
      "learning_rate": 4.760427263479146e-05,
      "loss": 3.7654,
      "step": 4710
    },
    {
      "epoch": 2.400813835198372,
      "grad_norm": 3.4709601402282715,
      "learning_rate": 4.759918616480163e-05,
      "loss": 3.7143,
      "step": 4720
    },
    {
      "epoch": 2.4059003051881995,
      "grad_norm": 3.3729593753814697,
      "learning_rate": 4.7594099694811805e-05,
      "loss": 3.751,
      "step": 4730
    },
    {
      "epoch": 2.4109867751780265,
      "grad_norm": 4.194859027862549,
      "learning_rate": 4.7589013224821975e-05,
      "loss": 3.7014,
      "step": 4740
    },
    {
      "epoch": 2.4160732451678535,
      "grad_norm": 5.186005592346191,
      "learning_rate": 4.7583926754832145e-05,
      "loss": 3.7729,
      "step": 4750
    },
    {
      "epoch": 2.4211597151576805,
      "grad_norm": 3.5192174911499023,
      "learning_rate": 4.757884028484232e-05,
      "loss": 3.7645,
      "step": 4760
    },
    {
      "epoch": 2.4262461851475075,
      "grad_norm": 3.952028751373291,
      "learning_rate": 4.75737538148525e-05,
      "loss": 3.7307,
      "step": 4770
    },
    {
      "epoch": 2.431332655137335,
      "grad_norm": 4.758773326873779,
      "learning_rate": 4.756866734486267e-05,
      "loss": 3.7452,
      "step": 4780
    },
    {
      "epoch": 2.436419125127162,
      "grad_norm": 4.455031871795654,
      "learning_rate": 4.7563580874872845e-05,
      "loss": 3.7642,
      "step": 4790
    },
    {
      "epoch": 2.441505595116989,
      "grad_norm": 2.650963544845581,
      "learning_rate": 4.7558494404883015e-05,
      "loss": 3.8022,
      "step": 4800
    },
    {
      "epoch": 2.446592065106816,
      "grad_norm": 4.416297912597656,
      "learning_rate": 4.7553407934893185e-05,
      "loss": 3.7897,
      "step": 4810
    },
    {
      "epoch": 2.451678535096643,
      "grad_norm": 3.3899331092834473,
      "learning_rate": 4.754832146490336e-05,
      "loss": 3.7809,
      "step": 4820
    },
    {
      "epoch": 2.4567650050864698,
      "grad_norm": 4.730326175689697,
      "learning_rate": 4.754323499491353e-05,
      "loss": 3.6782,
      "step": 4830
    },
    {
      "epoch": 2.461851475076297,
      "grad_norm": 3.9171133041381836,
      "learning_rate": 4.75381485249237e-05,
      "loss": 3.664,
      "step": 4840
    },
    {
      "epoch": 2.466937945066124,
      "grad_norm": 3.900848627090454,
      "learning_rate": 4.753306205493388e-05,
      "loss": 3.6963,
      "step": 4850
    },
    {
      "epoch": 2.472024415055951,
      "grad_norm": 3.6779797077178955,
      "learning_rate": 4.752797558494405e-05,
      "loss": 3.7904,
      "step": 4860
    },
    {
      "epoch": 2.477110885045778,
      "grad_norm": 5.714904308319092,
      "learning_rate": 4.7522889114954225e-05,
      "loss": 3.7113,
      "step": 4870
    },
    {
      "epoch": 2.482197355035605,
      "grad_norm": 4.858906269073486,
      "learning_rate": 4.75178026449644e-05,
      "loss": 3.669,
      "step": 4880
    },
    {
      "epoch": 2.4872838250254325,
      "grad_norm": 3.5522751808166504,
      "learning_rate": 4.751271617497457e-05,
      "loss": 3.6697,
      "step": 4890
    },
    {
      "epoch": 2.4923702950152595,
      "grad_norm": 3.6095802783966064,
      "learning_rate": 4.750762970498474e-05,
      "loss": 3.7363,
      "step": 4900
    },
    {
      "epoch": 2.4974567650050865,
      "grad_norm": 5.125812530517578,
      "learning_rate": 4.750254323499492e-05,
      "loss": 3.7243,
      "step": 4910
    },
    {
      "epoch": 2.5025432349949135,
      "grad_norm": 5.69280481338501,
      "learning_rate": 4.749745676500509e-05,
      "loss": 3.7737,
      "step": 4920
    },
    {
      "epoch": 2.5076297049847405,
      "grad_norm": 2.7839348316192627,
      "learning_rate": 4.749237029501526e-05,
      "loss": 3.7156,
      "step": 4930
    },
    {
      "epoch": 2.5127161749745675,
      "grad_norm": 5.089852809906006,
      "learning_rate": 4.7487283825025434e-05,
      "loss": 3.7611,
      "step": 4940
    },
    {
      "epoch": 2.517802644964395,
      "grad_norm": 5.812798500061035,
      "learning_rate": 4.7482197355035604e-05,
      "loss": 3.7123,
      "step": 4950
    },
    {
      "epoch": 2.522889114954222,
      "grad_norm": 3.87550687789917,
      "learning_rate": 4.747711088504578e-05,
      "loss": 3.778,
      "step": 4960
    },
    {
      "epoch": 2.527975584944049,
      "grad_norm": 4.252837657928467,
      "learning_rate": 4.747202441505596e-05,
      "loss": 3.769,
      "step": 4970
    },
    {
      "epoch": 2.533062054933876,
      "grad_norm": 3.3828322887420654,
      "learning_rate": 4.746693794506613e-05,
      "loss": 3.7077,
      "step": 4980
    },
    {
      "epoch": 2.538148524923703,
      "grad_norm": 4.0107831954956055,
      "learning_rate": 4.74618514750763e-05,
      "loss": 3.6857,
      "step": 4990
    },
    {
      "epoch": 2.5432349949135302,
      "grad_norm": 5.351294040679932,
      "learning_rate": 4.7456765005086474e-05,
      "loss": 3.6802,
      "step": 5000
    },
    {
      "epoch": 2.548321464903357,
      "grad_norm": 4.689877033233643,
      "learning_rate": 4.7451678535096644e-05,
      "loss": 3.6626,
      "step": 5010
    },
    {
      "epoch": 2.553407934893184,
      "grad_norm": 3.8211734294891357,
      "learning_rate": 4.744659206510682e-05,
      "loss": 3.6476,
      "step": 5020
    },
    {
      "epoch": 2.558494404883011,
      "grad_norm": 4.359163761138916,
      "learning_rate": 4.744150559511699e-05,
      "loss": 3.7347,
      "step": 5030
    },
    {
      "epoch": 2.563580874872838,
      "grad_norm": 3.2256479263305664,
      "learning_rate": 4.743641912512716e-05,
      "loss": 3.6519,
      "step": 5040
    },
    {
      "epoch": 2.568667344862665,
      "grad_norm": 3.401954174041748,
      "learning_rate": 4.743133265513734e-05,
      "loss": 3.6615,
      "step": 5050
    },
    {
      "epoch": 2.573753814852492,
      "grad_norm": 3.7059149742126465,
      "learning_rate": 4.7426246185147514e-05,
      "loss": 3.7818,
      "step": 5060
    },
    {
      "epoch": 2.5788402848423195,
      "grad_norm": 5.809062957763672,
      "learning_rate": 4.7421159715157683e-05,
      "loss": 3.7481,
      "step": 5070
    },
    {
      "epoch": 2.5839267548321465,
      "grad_norm": 5.7823052406311035,
      "learning_rate": 4.741607324516786e-05,
      "loss": 3.6964,
      "step": 5080
    },
    {
      "epoch": 2.5890132248219735,
      "grad_norm": 3.965182304382324,
      "learning_rate": 4.741098677517803e-05,
      "loss": 3.6505,
      "step": 5090
    },
    {
      "epoch": 2.5940996948118005,
      "grad_norm": 3.550030469894409,
      "learning_rate": 4.74059003051882e-05,
      "loss": 3.7102,
      "step": 5100
    },
    {
      "epoch": 2.599186164801628,
      "grad_norm": 2.77231764793396,
      "learning_rate": 4.7400813835198377e-05,
      "loss": 3.6702,
      "step": 5110
    },
    {
      "epoch": 2.604272634791455,
      "grad_norm": 5.389247894287109,
      "learning_rate": 4.7395727365208546e-05,
      "loss": 3.673,
      "step": 5120
    },
    {
      "epoch": 2.609359104781282,
      "grad_norm": 4.422870635986328,
      "learning_rate": 4.7390640895218716e-05,
      "loss": 3.6684,
      "step": 5130
    },
    {
      "epoch": 2.614445574771109,
      "grad_norm": 5.599137306213379,
      "learning_rate": 4.738555442522889e-05,
      "loss": 3.7615,
      "step": 5140
    },
    {
      "epoch": 2.619532044760936,
      "grad_norm": 5.155229091644287,
      "learning_rate": 4.738046795523906e-05,
      "loss": 3.7898,
      "step": 5150
    },
    {
      "epoch": 2.624618514750763,
      "grad_norm": 4.242743492126465,
      "learning_rate": 4.737538148524924e-05,
      "loss": 3.7476,
      "step": 5160
    },
    {
      "epoch": 2.62970498474059,
      "grad_norm": 4.456991672515869,
      "learning_rate": 4.7370295015259416e-05,
      "loss": 3.6668,
      "step": 5170
    },
    {
      "epoch": 2.634791454730417,
      "grad_norm": 4.164248943328857,
      "learning_rate": 4.7365208545269586e-05,
      "loss": 3.7105,
      "step": 5180
    },
    {
      "epoch": 2.639877924720244,
      "grad_norm": 4.192049026489258,
      "learning_rate": 4.7360122075279756e-05,
      "loss": 3.6862,
      "step": 5190
    },
    {
      "epoch": 2.644964394710071,
      "grad_norm": 4.411014556884766,
      "learning_rate": 4.735503560528993e-05,
      "loss": 3.6542,
      "step": 5200
    },
    {
      "epoch": 2.650050864699898,
      "grad_norm": 6.183542728424072,
      "learning_rate": 4.73499491353001e-05,
      "loss": 3.7023,
      "step": 5210
    },
    {
      "epoch": 2.6551373346897256,
      "grad_norm": 5.223053455352783,
      "learning_rate": 4.734486266531027e-05,
      "loss": 3.6974,
      "step": 5220
    },
    {
      "epoch": 2.6602238046795526,
      "grad_norm": 6.425203800201416,
      "learning_rate": 4.733977619532045e-05,
      "loss": 3.6601,
      "step": 5230
    },
    {
      "epoch": 2.6653102746693795,
      "grad_norm": 3.5007314682006836,
      "learning_rate": 4.733468972533062e-05,
      "loss": 3.6727,
      "step": 5240
    },
    {
      "epoch": 2.6703967446592065,
      "grad_norm": 4.957117557525635,
      "learning_rate": 4.7329603255340796e-05,
      "loss": 3.693,
      "step": 5250
    },
    {
      "epoch": 2.6754832146490335,
      "grad_norm": 4.623505115509033,
      "learning_rate": 4.732451678535097e-05,
      "loss": 3.7079,
      "step": 5260
    },
    {
      "epoch": 2.6805696846388605,
      "grad_norm": 3.95219087600708,
      "learning_rate": 4.731943031536114e-05,
      "loss": 3.6843,
      "step": 5270
    },
    {
      "epoch": 2.6856561546286875,
      "grad_norm": 7.213279724121094,
      "learning_rate": 4.731434384537132e-05,
      "loss": 3.6372,
      "step": 5280
    },
    {
      "epoch": 2.690742624618515,
      "grad_norm": 4.484387397766113,
      "learning_rate": 4.730925737538149e-05,
      "loss": 3.6865,
      "step": 5290
    },
    {
      "epoch": 2.695829094608342,
      "grad_norm": 4.676220893859863,
      "learning_rate": 4.730417090539166e-05,
      "loss": 3.7287,
      "step": 5300
    },
    {
      "epoch": 2.700915564598169,
      "grad_norm": 5.956389427185059,
      "learning_rate": 4.7299084435401835e-05,
      "loss": 3.6351,
      "step": 5310
    },
    {
      "epoch": 2.706002034587996,
      "grad_norm": 3.696568489074707,
      "learning_rate": 4.7293997965412005e-05,
      "loss": 3.7946,
      "step": 5320
    },
    {
      "epoch": 2.7110885045778232,
      "grad_norm": 4.7207136154174805,
      "learning_rate": 4.7288911495422175e-05,
      "loss": 3.6459,
      "step": 5330
    },
    {
      "epoch": 2.7161749745676502,
      "grad_norm": 4.2314558029174805,
      "learning_rate": 4.728382502543235e-05,
      "loss": 3.691,
      "step": 5340
    },
    {
      "epoch": 2.721261444557477,
      "grad_norm": 3.279736042022705,
      "learning_rate": 4.727873855544253e-05,
      "loss": 3.7023,
      "step": 5350
    },
    {
      "epoch": 2.726347914547304,
      "grad_norm": 3.727085828781128,
      "learning_rate": 4.72736520854527e-05,
      "loss": 3.6622,
      "step": 5360
    },
    {
      "epoch": 2.731434384537131,
      "grad_norm": 5.499291896820068,
      "learning_rate": 4.7268565615462875e-05,
      "loss": 3.6241,
      "step": 5370
    },
    {
      "epoch": 2.736520854526958,
      "grad_norm": 3.705845355987549,
      "learning_rate": 4.7263479145473045e-05,
      "loss": 3.6156,
      "step": 5380
    },
    {
      "epoch": 2.741607324516785,
      "grad_norm": 3.802474021911621,
      "learning_rate": 4.7258392675483215e-05,
      "loss": 3.6701,
      "step": 5390
    },
    {
      "epoch": 2.7466937945066126,
      "grad_norm": 4.997459411621094,
      "learning_rate": 4.725330620549339e-05,
      "loss": 3.6839,
      "step": 5400
    },
    {
      "epoch": 2.7517802644964395,
      "grad_norm": 5.709918975830078,
      "learning_rate": 4.724821973550356e-05,
      "loss": 3.6577,
      "step": 5410
    },
    {
      "epoch": 2.7568667344862665,
      "grad_norm": 4.70246696472168,
      "learning_rate": 4.724313326551373e-05,
      "loss": 3.7086,
      "step": 5420
    },
    {
      "epoch": 2.7619532044760935,
      "grad_norm": 3.8968029022216797,
      "learning_rate": 4.723804679552391e-05,
      "loss": 3.7816,
      "step": 5430
    },
    {
      "epoch": 2.767039674465921,
      "grad_norm": 4.845094203948975,
      "learning_rate": 4.7232960325534085e-05,
      "loss": 3.7089,
      "step": 5440
    },
    {
      "epoch": 2.772126144455748,
      "grad_norm": 4.331329345703125,
      "learning_rate": 4.7227873855544255e-05,
      "loss": 3.7702,
      "step": 5450
    },
    {
      "epoch": 2.777212614445575,
      "grad_norm": 4.5414042472839355,
      "learning_rate": 4.722278738555443e-05,
      "loss": 3.7594,
      "step": 5460
    },
    {
      "epoch": 2.782299084435402,
      "grad_norm": 3.7642195224761963,
      "learning_rate": 4.72177009155646e-05,
      "loss": 3.7254,
      "step": 5470
    },
    {
      "epoch": 2.787385554425229,
      "grad_norm": 5.4843292236328125,
      "learning_rate": 4.721261444557477e-05,
      "loss": 3.62,
      "step": 5480
    },
    {
      "epoch": 2.792472024415056,
      "grad_norm": 5.650673866271973,
      "learning_rate": 4.720752797558495e-05,
      "loss": 3.6561,
      "step": 5490
    },
    {
      "epoch": 2.797558494404883,
      "grad_norm": 3.9296176433563232,
      "learning_rate": 4.720244150559512e-05,
      "loss": 3.6567,
      "step": 5500
    },
    {
      "epoch": 2.8026449643947102,
      "grad_norm": 5.057713985443115,
      "learning_rate": 4.719735503560529e-05,
      "loss": 3.6691,
      "step": 5510
    },
    {
      "epoch": 2.807731434384537,
      "grad_norm": 5.678650856018066,
      "learning_rate": 4.7192268565615464e-05,
      "loss": 3.6463,
      "step": 5520
    },
    {
      "epoch": 2.812817904374364,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.7187182095625634e-05,
      "loss": 3.6625,
      "step": 5530
    },
    {
      "epoch": 2.817904374364191,
      "grad_norm": 4.528802871704102,
      "learning_rate": 4.718209562563581e-05,
      "loss": 3.66,
      "step": 5540
    },
    {
      "epoch": 2.822990844354018,
      "grad_norm": 5.758062839508057,
      "learning_rate": 4.717700915564599e-05,
      "loss": 3.6774,
      "step": 5550
    },
    {
      "epoch": 2.8280773143438456,
      "grad_norm": 3.8056657314300537,
      "learning_rate": 4.717192268565616e-05,
      "loss": 3.656,
      "step": 5560
    },
    {
      "epoch": 2.8331637843336726,
      "grad_norm": 4.489880561828613,
      "learning_rate": 4.7166836215666334e-05,
      "loss": 3.6592,
      "step": 5570
    },
    {
      "epoch": 2.8382502543234995,
      "grad_norm": 5.6020355224609375,
      "learning_rate": 4.7161749745676504e-05,
      "loss": 3.6587,
      "step": 5580
    },
    {
      "epoch": 2.8433367243133265,
      "grad_norm": 4.388919830322266,
      "learning_rate": 4.7156663275686674e-05,
      "loss": 3.6172,
      "step": 5590
    },
    {
      "epoch": 2.8484231943031535,
      "grad_norm": 5.279253005981445,
      "learning_rate": 4.715157680569685e-05,
      "loss": 3.6641,
      "step": 5600
    },
    {
      "epoch": 2.8535096642929805,
      "grad_norm": 4.987951755523682,
      "learning_rate": 4.714649033570702e-05,
      "loss": 3.6629,
      "step": 5610
    },
    {
      "epoch": 2.8585961342828075,
      "grad_norm": 3.9382975101470947,
      "learning_rate": 4.714140386571719e-05,
      "loss": 3.6729,
      "step": 5620
    },
    {
      "epoch": 2.863682604272635,
      "grad_norm": 4.747213840484619,
      "learning_rate": 4.713631739572737e-05,
      "loss": 3.6258,
      "step": 5630
    },
    {
      "epoch": 2.868769074262462,
      "grad_norm": 5.04433012008667,
      "learning_rate": 4.7131230925737544e-05,
      "loss": 3.6859,
      "step": 5640
    },
    {
      "epoch": 2.873855544252289,
      "grad_norm": 4.856659412384033,
      "learning_rate": 4.7126144455747713e-05,
      "loss": 3.6226,
      "step": 5650
    },
    {
      "epoch": 2.878942014242116,
      "grad_norm": 4.189613342285156,
      "learning_rate": 4.712105798575789e-05,
      "loss": 3.6887,
      "step": 5660
    },
    {
      "epoch": 2.8840284842319432,
      "grad_norm": 4.049076557159424,
      "learning_rate": 4.711597151576806e-05,
      "loss": 3.6765,
      "step": 5670
    },
    {
      "epoch": 2.8891149542217702,
      "grad_norm": 3.7918357849121094,
      "learning_rate": 4.711088504577823e-05,
      "loss": 3.6691,
      "step": 5680
    },
    {
      "epoch": 2.894201424211597,
      "grad_norm": 4.2135162353515625,
      "learning_rate": 4.7105798575788407e-05,
      "loss": 3.6714,
      "step": 5690
    },
    {
      "epoch": 2.899287894201424,
      "grad_norm": 5.489110946655273,
      "learning_rate": 4.7100712105798576e-05,
      "loss": 3.6798,
      "step": 5700
    },
    {
      "epoch": 2.904374364191251,
      "grad_norm": 5.34439754486084,
      "learning_rate": 4.7095625635808746e-05,
      "loss": 3.6785,
      "step": 5710
    },
    {
      "epoch": 2.909460834181078,
      "grad_norm": 4.0993332862854,
      "learning_rate": 4.709053916581892e-05,
      "loss": 3.6565,
      "step": 5720
    },
    {
      "epoch": 2.914547304170905,
      "grad_norm": 4.631868839263916,
      "learning_rate": 4.70854526958291e-05,
      "loss": 3.7402,
      "step": 5730
    },
    {
      "epoch": 2.9196337741607326,
      "grad_norm": 3.4633841514587402,
      "learning_rate": 4.708036622583927e-05,
      "loss": 3.6674,
      "step": 5740
    },
    {
      "epoch": 2.9247202441505595,
      "grad_norm": 5.54067325592041,
      "learning_rate": 4.7075279755849446e-05,
      "loss": 3.6828,
      "step": 5750
    },
    {
      "epoch": 2.9298067141403865,
      "grad_norm": 3.4729666709899902,
      "learning_rate": 4.7070193285859616e-05,
      "loss": 3.6672,
      "step": 5760
    },
    {
      "epoch": 2.9348931841302135,
      "grad_norm": 4.912230491638184,
      "learning_rate": 4.7065106815869786e-05,
      "loss": 3.6515,
      "step": 5770
    },
    {
      "epoch": 2.939979654120041,
      "grad_norm": 3.7959399223327637,
      "learning_rate": 4.706002034587996e-05,
      "loss": 3.7472,
      "step": 5780
    },
    {
      "epoch": 2.945066124109868,
      "grad_norm": 4.85549259185791,
      "learning_rate": 4.705493387589013e-05,
      "loss": 3.6709,
      "step": 5790
    },
    {
      "epoch": 2.950152594099695,
      "grad_norm": 4.189378261566162,
      "learning_rate": 4.70498474059003e-05,
      "loss": 3.6489,
      "step": 5800
    },
    {
      "epoch": 2.955239064089522,
      "grad_norm": 3.500896453857422,
      "learning_rate": 4.704476093591048e-05,
      "loss": 3.615,
      "step": 5810
    },
    {
      "epoch": 2.960325534079349,
      "grad_norm": 4.711471080780029,
      "learning_rate": 4.703967446592065e-05,
      "loss": 3.6782,
      "step": 5820
    },
    {
      "epoch": 2.965412004069176,
      "grad_norm": 7.09427547454834,
      "learning_rate": 4.7034587995930826e-05,
      "loss": 3.6456,
      "step": 5830
    },
    {
      "epoch": 2.970498474059003,
      "grad_norm": 3.952336311340332,
      "learning_rate": 4.7029501525941e-05,
      "loss": 3.6552,
      "step": 5840
    },
    {
      "epoch": 2.9755849440488302,
      "grad_norm": 4.944598197937012,
      "learning_rate": 4.702441505595117e-05,
      "loss": 3.5345,
      "step": 5850
    },
    {
      "epoch": 2.980671414038657,
      "grad_norm": 6.228686332702637,
      "learning_rate": 4.701932858596135e-05,
      "loss": 3.5936,
      "step": 5860
    },
    {
      "epoch": 2.985757884028484,
      "grad_norm": 4.323083400726318,
      "learning_rate": 4.701424211597152e-05,
      "loss": 3.5843,
      "step": 5870
    },
    {
      "epoch": 2.990844354018311,
      "grad_norm": 5.371728897094727,
      "learning_rate": 4.700915564598169e-05,
      "loss": 3.7016,
      "step": 5880
    },
    {
      "epoch": 2.9959308240081386,
      "grad_norm": 4.7863569259643555,
      "learning_rate": 4.7004069175991865e-05,
      "loss": 3.6439,
      "step": 5890
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.7252442836761475,
      "eval_runtime": 2.8692,
      "eval_samples_per_second": 967.165,
      "eval_steps_per_second": 120.939,
      "step": 5898
    },
    {
      "epoch": 3.0010172939979656,
      "grad_norm": 7.583024501800537,
      "learning_rate": 4.6998982706002035e-05,
      "loss": 3.6023,
      "step": 5900
    },
    {
      "epoch": 3.0061037639877926,
      "grad_norm": 4.859519958496094,
      "learning_rate": 4.6993896236012205e-05,
      "loss": 3.5988,
      "step": 5910
    },
    {
      "epoch": 3.0111902339776195,
      "grad_norm": 3.942944288253784,
      "learning_rate": 4.698880976602238e-05,
      "loss": 3.6202,
      "step": 5920
    },
    {
      "epoch": 3.0162767039674465,
      "grad_norm": 4.281228542327881,
      "learning_rate": 4.698372329603256e-05,
      "loss": 3.6121,
      "step": 5930
    },
    {
      "epoch": 3.0213631739572735,
      "grad_norm": 3.435305595397949,
      "learning_rate": 4.697863682604273e-05,
      "loss": 3.6494,
      "step": 5940
    },
    {
      "epoch": 3.026449643947101,
      "grad_norm": 3.6954398155212402,
      "learning_rate": 4.6973550356052905e-05,
      "loss": 3.6734,
      "step": 5950
    },
    {
      "epoch": 3.031536113936928,
      "grad_norm": 4.779880046844482,
      "learning_rate": 4.6968463886063075e-05,
      "loss": 3.6766,
      "step": 5960
    },
    {
      "epoch": 3.036622583926755,
      "grad_norm": 4.524289131164551,
      "learning_rate": 4.6963377416073245e-05,
      "loss": 3.6474,
      "step": 5970
    },
    {
      "epoch": 3.041709053916582,
      "grad_norm": 5.124083995819092,
      "learning_rate": 4.695829094608342e-05,
      "loss": 3.5596,
      "step": 5980
    },
    {
      "epoch": 3.046795523906409,
      "grad_norm": 4.635921955108643,
      "learning_rate": 4.695320447609359e-05,
      "loss": 3.6808,
      "step": 5990
    },
    {
      "epoch": 3.051881993896236,
      "grad_norm": 4.853470325469971,
      "learning_rate": 4.694811800610376e-05,
      "loss": 3.5641,
      "step": 6000
    },
    {
      "epoch": 3.0569684638860632,
      "grad_norm": 4.713903427124023,
      "learning_rate": 4.694303153611394e-05,
      "loss": 3.6332,
      "step": 6010
    },
    {
      "epoch": 3.0620549338758902,
      "grad_norm": 5.001492023468018,
      "learning_rate": 4.6937945066124115e-05,
      "loss": 3.5625,
      "step": 6020
    },
    {
      "epoch": 3.067141403865717,
      "grad_norm": 6.610650539398193,
      "learning_rate": 4.6932858596134285e-05,
      "loss": 3.6483,
      "step": 6030
    },
    {
      "epoch": 3.072227873855544,
      "grad_norm": 4.805732250213623,
      "learning_rate": 4.692777212614446e-05,
      "loss": 3.604,
      "step": 6040
    },
    {
      "epoch": 3.077314343845371,
      "grad_norm": 6.504794597625732,
      "learning_rate": 4.692268565615463e-05,
      "loss": 3.6102,
      "step": 6050
    },
    {
      "epoch": 3.082400813835198,
      "grad_norm": 9.153931617736816,
      "learning_rate": 4.69175991861648e-05,
      "loss": 3.5449,
      "step": 6060
    },
    {
      "epoch": 3.0874872838250256,
      "grad_norm": 6.519296169281006,
      "learning_rate": 4.691251271617498e-05,
      "loss": 3.6389,
      "step": 6070
    },
    {
      "epoch": 3.0925737538148526,
      "grad_norm": 4.770105361938477,
      "learning_rate": 4.690742624618515e-05,
      "loss": 3.6886,
      "step": 6080
    },
    {
      "epoch": 3.0976602238046795,
      "grad_norm": 5.810426235198975,
      "learning_rate": 4.6902339776195324e-05,
      "loss": 3.6273,
      "step": 6090
    },
    {
      "epoch": 3.1027466937945065,
      "grad_norm": 4.306407451629639,
      "learning_rate": 4.6897253306205494e-05,
      "loss": 3.6181,
      "step": 6100
    },
    {
      "epoch": 3.1078331637843335,
      "grad_norm": 5.578598499298096,
      "learning_rate": 4.6892166836215664e-05,
      "loss": 3.6889,
      "step": 6110
    },
    {
      "epoch": 3.112919633774161,
      "grad_norm": 5.577609539031982,
      "learning_rate": 4.688708036622584e-05,
      "loss": 3.6188,
      "step": 6120
    },
    {
      "epoch": 3.118006103763988,
      "grad_norm": 4.331940174102783,
      "learning_rate": 4.688199389623602e-05,
      "loss": 3.5675,
      "step": 6130
    },
    {
      "epoch": 3.123092573753815,
      "grad_norm": 5.435168743133545,
      "learning_rate": 4.687690742624619e-05,
      "loss": 3.5775,
      "step": 6140
    },
    {
      "epoch": 3.128179043743642,
      "grad_norm": 4.688910484313965,
      "learning_rate": 4.6871820956256364e-05,
      "loss": 3.6808,
      "step": 6150
    },
    {
      "epoch": 3.133265513733469,
      "grad_norm": 5.937453269958496,
      "learning_rate": 4.6866734486266534e-05,
      "loss": 3.629,
      "step": 6160
    },
    {
      "epoch": 3.138351983723296,
      "grad_norm": 4.7829155921936035,
      "learning_rate": 4.6861648016276704e-05,
      "loss": 3.572,
      "step": 6170
    },
    {
      "epoch": 3.1434384537131232,
      "grad_norm": 4.767241954803467,
      "learning_rate": 4.685656154628688e-05,
      "loss": 3.6488,
      "step": 6180
    },
    {
      "epoch": 3.1485249237029502,
      "grad_norm": 4.491647243499756,
      "learning_rate": 4.685147507629705e-05,
      "loss": 3.6865,
      "step": 6190
    },
    {
      "epoch": 3.153611393692777,
      "grad_norm": 7.464101791381836,
      "learning_rate": 4.684638860630722e-05,
      "loss": 3.6062,
      "step": 6200
    },
    {
      "epoch": 3.158697863682604,
      "grad_norm": 4.401134967803955,
      "learning_rate": 4.68413021363174e-05,
      "loss": 3.6582,
      "step": 6210
    },
    {
      "epoch": 3.163784333672431,
      "grad_norm": 8.509295463562012,
      "learning_rate": 4.6836215666327574e-05,
      "loss": 3.6492,
      "step": 6220
    },
    {
      "epoch": 3.1688708036622586,
      "grad_norm": 5.979337215423584,
      "learning_rate": 4.6831129196337743e-05,
      "loss": 3.6675,
      "step": 6230
    },
    {
      "epoch": 3.1739572736520856,
      "grad_norm": 4.641373157501221,
      "learning_rate": 4.682604272634792e-05,
      "loss": 3.6731,
      "step": 6240
    },
    {
      "epoch": 3.1790437436419126,
      "grad_norm": 4.351202487945557,
      "learning_rate": 4.682095625635809e-05,
      "loss": 3.6228,
      "step": 6250
    },
    {
      "epoch": 3.1841302136317395,
      "grad_norm": 4.611045837402344,
      "learning_rate": 4.681586978636826e-05,
      "loss": 3.635,
      "step": 6260
    },
    {
      "epoch": 3.1892166836215665,
      "grad_norm": 3.8160417079925537,
      "learning_rate": 4.6810783316378437e-05,
      "loss": 3.6503,
      "step": 6270
    },
    {
      "epoch": 3.1943031536113935,
      "grad_norm": 6.671133518218994,
      "learning_rate": 4.6805696846388606e-05,
      "loss": 3.6312,
      "step": 6280
    },
    {
      "epoch": 3.199389623601221,
      "grad_norm": 4.400428771972656,
      "learning_rate": 4.6800610376398776e-05,
      "loss": 3.5836,
      "step": 6290
    },
    {
      "epoch": 3.204476093591048,
      "grad_norm": 6.711611270904541,
      "learning_rate": 4.679552390640895e-05,
      "loss": 3.6643,
      "step": 6300
    },
    {
      "epoch": 3.209562563580875,
      "grad_norm": 5.938518047332764,
      "learning_rate": 4.679043743641913e-05,
      "loss": 3.5879,
      "step": 6310
    },
    {
      "epoch": 3.214649033570702,
      "grad_norm": 5.224584102630615,
      "learning_rate": 4.67853509664293e-05,
      "loss": 3.6751,
      "step": 6320
    },
    {
      "epoch": 3.219735503560529,
      "grad_norm": 4.686142444610596,
      "learning_rate": 4.6780264496439476e-05,
      "loss": 3.5793,
      "step": 6330
    },
    {
      "epoch": 3.2248219735503563,
      "grad_norm": 5.041260719299316,
      "learning_rate": 4.6775178026449646e-05,
      "loss": 3.6131,
      "step": 6340
    },
    {
      "epoch": 3.2299084435401832,
      "grad_norm": 6.784351825714111,
      "learning_rate": 4.677009155645982e-05,
      "loss": 3.5302,
      "step": 6350
    },
    {
      "epoch": 3.2349949135300102,
      "grad_norm": 4.719609260559082,
      "learning_rate": 4.676500508646999e-05,
      "loss": 3.6024,
      "step": 6360
    },
    {
      "epoch": 3.240081383519837,
      "grad_norm": 4.837835788726807,
      "learning_rate": 4.675991861648016e-05,
      "loss": 3.6316,
      "step": 6370
    },
    {
      "epoch": 3.245167853509664,
      "grad_norm": 3.860846519470215,
      "learning_rate": 4.675483214649034e-05,
      "loss": 3.5562,
      "step": 6380
    },
    {
      "epoch": 3.250254323499491,
      "grad_norm": 4.719042778015137,
      "learning_rate": 4.674974567650051e-05,
      "loss": 3.6236,
      "step": 6390
    },
    {
      "epoch": 3.2553407934893186,
      "grad_norm": 5.3294572830200195,
      "learning_rate": 4.6744659206510686e-05,
      "loss": 3.5858,
      "step": 6400
    },
    {
      "epoch": 3.2604272634791456,
      "grad_norm": 4.688499450683594,
      "learning_rate": 4.673957273652086e-05,
      "loss": 3.6385,
      "step": 6410
    },
    {
      "epoch": 3.2655137334689726,
      "grad_norm": 5.017598628997803,
      "learning_rate": 4.673448626653103e-05,
      "loss": 3.5617,
      "step": 6420
    },
    {
      "epoch": 3.2706002034587995,
      "grad_norm": 4.725494384765625,
      "learning_rate": 4.67293997965412e-05,
      "loss": 3.5331,
      "step": 6430
    },
    {
      "epoch": 3.2756866734486265,
      "grad_norm": 4.814544200897217,
      "learning_rate": 4.672431332655138e-05,
      "loss": 3.6042,
      "step": 6440
    },
    {
      "epoch": 3.280773143438454,
      "grad_norm": 5.200475692749023,
      "learning_rate": 4.671922685656155e-05,
      "loss": 3.6681,
      "step": 6450
    },
    {
      "epoch": 3.285859613428281,
      "grad_norm": 6.304190635681152,
      "learning_rate": 4.671414038657172e-05,
      "loss": 3.6586,
      "step": 6460
    },
    {
      "epoch": 3.290946083418108,
      "grad_norm": 6.093272686004639,
      "learning_rate": 4.6709053916581895e-05,
      "loss": 3.5281,
      "step": 6470
    },
    {
      "epoch": 3.296032553407935,
      "grad_norm": 5.949427604675293,
      "learning_rate": 4.6703967446592065e-05,
      "loss": 3.6669,
      "step": 6480
    },
    {
      "epoch": 3.301119023397762,
      "grad_norm": 5.326529026031494,
      "learning_rate": 4.6698880976602235e-05,
      "loss": 3.5815,
      "step": 6490
    },
    {
      "epoch": 3.306205493387589,
      "grad_norm": 6.130629062652588,
      "learning_rate": 4.669379450661241e-05,
      "loss": 3.6712,
      "step": 6500
    },
    {
      "epoch": 3.311291963377416,
      "grad_norm": 7.084123134613037,
      "learning_rate": 4.668870803662259e-05,
      "loss": 3.6649,
      "step": 6510
    },
    {
      "epoch": 3.3163784333672433,
      "grad_norm": 4.31793737411499,
      "learning_rate": 4.668362156663276e-05,
      "loss": 3.6104,
      "step": 6520
    },
    {
      "epoch": 3.3214649033570702,
      "grad_norm": 6.770806789398193,
      "learning_rate": 4.6678535096642935e-05,
      "loss": 3.5508,
      "step": 6530
    },
    {
      "epoch": 3.326551373346897,
      "grad_norm": 7.9012956619262695,
      "learning_rate": 4.6673448626653105e-05,
      "loss": 3.5376,
      "step": 6540
    },
    {
      "epoch": 3.331637843336724,
      "grad_norm": 4.781438827514648,
      "learning_rate": 4.6668362156663275e-05,
      "loss": 3.5428,
      "step": 6550
    },
    {
      "epoch": 3.3367243133265516,
      "grad_norm": 4.8690314292907715,
      "learning_rate": 4.666327568667345e-05,
      "loss": 3.5954,
      "step": 6560
    },
    {
      "epoch": 3.3418107833163786,
      "grad_norm": 3.661860704421997,
      "learning_rate": 4.665818921668362e-05,
      "loss": 3.6351,
      "step": 6570
    },
    {
      "epoch": 3.3468972533062056,
      "grad_norm": 5.75223445892334,
      "learning_rate": 4.665310274669379e-05,
      "loss": 3.5385,
      "step": 6580
    },
    {
      "epoch": 3.3519837232960326,
      "grad_norm": 3.6822266578674316,
      "learning_rate": 4.664801627670397e-05,
      "loss": 3.6236,
      "step": 6590
    },
    {
      "epoch": 3.3570701932858595,
      "grad_norm": 7.664994239807129,
      "learning_rate": 4.6642929806714145e-05,
      "loss": 3.6336,
      "step": 6600
    },
    {
      "epoch": 3.3621566632756865,
      "grad_norm": 5.162015438079834,
      "learning_rate": 4.6637843336724315e-05,
      "loss": 3.5759,
      "step": 6610
    },
    {
      "epoch": 3.3672431332655135,
      "grad_norm": 5.551575660705566,
      "learning_rate": 4.663275686673449e-05,
      "loss": 3.6228,
      "step": 6620
    },
    {
      "epoch": 3.372329603255341,
      "grad_norm": 5.163883209228516,
      "learning_rate": 4.662767039674466e-05,
      "loss": 3.6341,
      "step": 6630
    },
    {
      "epoch": 3.377416073245168,
      "grad_norm": 6.469604969024658,
      "learning_rate": 4.662258392675484e-05,
      "loss": 3.5981,
      "step": 6640
    },
    {
      "epoch": 3.382502543234995,
      "grad_norm": 4.385519981384277,
      "learning_rate": 4.661749745676501e-05,
      "loss": 3.5927,
      "step": 6650
    },
    {
      "epoch": 3.387589013224822,
      "grad_norm": 5.208214282989502,
      "learning_rate": 4.661241098677518e-05,
      "loss": 3.5765,
      "step": 6660
    },
    {
      "epoch": 3.392675483214649,
      "grad_norm": 4.088645935058594,
      "learning_rate": 4.6607324516785354e-05,
      "loss": 3.5992,
      "step": 6670
    },
    {
      "epoch": 3.3977619532044763,
      "grad_norm": 5.166849613189697,
      "learning_rate": 4.6602238046795524e-05,
      "loss": 3.579,
      "step": 6680
    },
    {
      "epoch": 3.4028484231943033,
      "grad_norm": 6.5324506759643555,
      "learning_rate": 4.65971515768057e-05,
      "loss": 3.6316,
      "step": 6690
    },
    {
      "epoch": 3.4079348931841302,
      "grad_norm": 4.6760783195495605,
      "learning_rate": 4.659206510681588e-05,
      "loss": 3.6492,
      "step": 6700
    },
    {
      "epoch": 3.413021363173957,
      "grad_norm": 6.862030982971191,
      "learning_rate": 4.658697863682605e-05,
      "loss": 3.5725,
      "step": 6710
    },
    {
      "epoch": 3.418107833163784,
      "grad_norm": 5.874502658843994,
      "learning_rate": 4.658189216683622e-05,
      "loss": 3.5419,
      "step": 6720
    },
    {
      "epoch": 3.423194303153611,
      "grad_norm": 6.032994270324707,
      "learning_rate": 4.6576805696846394e-05,
      "loss": 3.6009,
      "step": 6730
    },
    {
      "epoch": 3.4282807731434386,
      "grad_norm": 4.199117183685303,
      "learning_rate": 4.6571719226856564e-05,
      "loss": 3.6195,
      "step": 6740
    },
    {
      "epoch": 3.4333672431332656,
      "grad_norm": 6.940876007080078,
      "learning_rate": 4.6566632756866734e-05,
      "loss": 3.6071,
      "step": 6750
    },
    {
      "epoch": 3.4384537131230926,
      "grad_norm": 6.1621012687683105,
      "learning_rate": 4.656154628687691e-05,
      "loss": 3.5854,
      "step": 6760
    },
    {
      "epoch": 3.4435401831129195,
      "grad_norm": 4.376574516296387,
      "learning_rate": 4.655645981688708e-05,
      "loss": 3.5615,
      "step": 6770
    },
    {
      "epoch": 3.4486266531027465,
      "grad_norm": 4.592933177947998,
      "learning_rate": 4.655137334689725e-05,
      "loss": 3.6003,
      "step": 6780
    },
    {
      "epoch": 3.453713123092574,
      "grad_norm": 4.879818916320801,
      "learning_rate": 4.654628687690743e-05,
      "loss": 3.6257,
      "step": 6790
    },
    {
      "epoch": 3.458799593082401,
      "grad_norm": 4.794998645782471,
      "learning_rate": 4.6541200406917604e-05,
      "loss": 3.5651,
      "step": 6800
    },
    {
      "epoch": 3.463886063072228,
      "grad_norm": 5.426793098449707,
      "learning_rate": 4.6536113936927773e-05,
      "loss": 3.6279,
      "step": 6810
    },
    {
      "epoch": 3.468972533062055,
      "grad_norm": 4.63775110244751,
      "learning_rate": 4.653102746693795e-05,
      "loss": 3.5838,
      "step": 6820
    },
    {
      "epoch": 3.474059003051882,
      "grad_norm": 6.2726240158081055,
      "learning_rate": 4.652594099694812e-05,
      "loss": 3.6179,
      "step": 6830
    },
    {
      "epoch": 3.479145473041709,
      "grad_norm": 4.428628921508789,
      "learning_rate": 4.652085452695829e-05,
      "loss": 3.5676,
      "step": 6840
    },
    {
      "epoch": 3.4842319430315363,
      "grad_norm": 4.96235466003418,
      "learning_rate": 4.6515768056968467e-05,
      "loss": 3.5855,
      "step": 6850
    },
    {
      "epoch": 3.4893184130213633,
      "grad_norm": 6.148820400238037,
      "learning_rate": 4.6510681586978636e-05,
      "loss": 3.5367,
      "step": 6860
    },
    {
      "epoch": 3.4944048830111902,
      "grad_norm": 7.199848175048828,
      "learning_rate": 4.6505595116988806e-05,
      "loss": 3.6036,
      "step": 6870
    },
    {
      "epoch": 3.499491353001017,
      "grad_norm": 6.699007987976074,
      "learning_rate": 4.650050864699898e-05,
      "loss": 3.6144,
      "step": 6880
    },
    {
      "epoch": 3.504577822990844,
      "grad_norm": 5.319265842437744,
      "learning_rate": 4.649542217700916e-05,
      "loss": 3.6197,
      "step": 6890
    },
    {
      "epoch": 3.5096642929806716,
      "grad_norm": 6.041663646697998,
      "learning_rate": 4.6490335707019336e-05,
      "loss": 3.503,
      "step": 6900
    },
    {
      "epoch": 3.5147507629704986,
      "grad_norm": 5.700667381286621,
      "learning_rate": 4.6485249237029506e-05,
      "loss": 3.5554,
      "step": 6910
    },
    {
      "epoch": 3.5198372329603256,
      "grad_norm": 5.902184009552002,
      "learning_rate": 4.6480162767039676e-05,
      "loss": 3.5574,
      "step": 6920
    },
    {
      "epoch": 3.5249237029501526,
      "grad_norm": 4.570537567138672,
      "learning_rate": 4.647507629704985e-05,
      "loss": 3.5995,
      "step": 6930
    },
    {
      "epoch": 3.5300101729399795,
      "grad_norm": 5.666985511779785,
      "learning_rate": 4.646998982706002e-05,
      "loss": 3.4969,
      "step": 6940
    },
    {
      "epoch": 3.5350966429298065,
      "grad_norm": 7.348017692565918,
      "learning_rate": 4.646490335707019e-05,
      "loss": 3.6009,
      "step": 6950
    },
    {
      "epoch": 3.5401831129196335,
      "grad_norm": 5.705157279968262,
      "learning_rate": 4.645981688708037e-05,
      "loss": 3.6524,
      "step": 6960
    },
    {
      "epoch": 3.545269582909461,
      "grad_norm": 6.622408866882324,
      "learning_rate": 4.645473041709054e-05,
      "loss": 3.6017,
      "step": 6970
    },
    {
      "epoch": 3.550356052899288,
      "grad_norm": 7.1577911376953125,
      "learning_rate": 4.6449643947100716e-05,
      "loss": 3.5671,
      "step": 6980
    },
    {
      "epoch": 3.555442522889115,
      "grad_norm": 7.3192901611328125,
      "learning_rate": 4.644455747711089e-05,
      "loss": 3.5647,
      "step": 6990
    },
    {
      "epoch": 3.560528992878942,
      "grad_norm": 5.097104072570801,
      "learning_rate": 4.643947100712106e-05,
      "loss": 3.4403,
      "step": 7000
    },
    {
      "epoch": 3.5656154628687693,
      "grad_norm": 5.256007671356201,
      "learning_rate": 4.643438453713123e-05,
      "loss": 3.5134,
      "step": 7010
    },
    {
      "epoch": 3.5707019328585963,
      "grad_norm": 8.457737922668457,
      "learning_rate": 4.642929806714141e-05,
      "loss": 3.5688,
      "step": 7020
    },
    {
      "epoch": 3.5757884028484233,
      "grad_norm": 6.196751117706299,
      "learning_rate": 4.642421159715158e-05,
      "loss": 3.5507,
      "step": 7030
    },
    {
      "epoch": 3.5808748728382502,
      "grad_norm": 7.598966121673584,
      "learning_rate": 4.641912512716175e-05,
      "loss": 3.5545,
      "step": 7040
    },
    {
      "epoch": 3.585961342828077,
      "grad_norm": 5.845785140991211,
      "learning_rate": 4.6414038657171925e-05,
      "loss": 3.5091,
      "step": 7050
    },
    {
      "epoch": 3.591047812817904,
      "grad_norm": 6.829281330108643,
      "learning_rate": 4.6408952187182095e-05,
      "loss": 3.5377,
      "step": 7060
    },
    {
      "epoch": 3.596134282807731,
      "grad_norm": 7.449661731719971,
      "learning_rate": 4.640386571719227e-05,
      "loss": 3.5916,
      "step": 7070
    },
    {
      "epoch": 3.6012207527975586,
      "grad_norm": 5.581085205078125,
      "learning_rate": 4.639877924720244e-05,
      "loss": 3.6195,
      "step": 7080
    },
    {
      "epoch": 3.6063072227873856,
      "grad_norm": 9.375107765197754,
      "learning_rate": 4.639369277721262e-05,
      "loss": 3.515,
      "step": 7090
    },
    {
      "epoch": 3.6113936927772126,
      "grad_norm": 6.638445854187012,
      "learning_rate": 4.638860630722279e-05,
      "loss": 3.5836,
      "step": 7100
    },
    {
      "epoch": 3.6164801627670395,
      "grad_norm": 5.276569366455078,
      "learning_rate": 4.6383519837232965e-05,
      "loss": 3.5954,
      "step": 7110
    },
    {
      "epoch": 3.621566632756867,
      "grad_norm": 6.055764198303223,
      "learning_rate": 4.6378433367243135e-05,
      "loss": 3.5648,
      "step": 7120
    },
    {
      "epoch": 3.626653102746694,
      "grad_norm": 4.181941509246826,
      "learning_rate": 4.6373346897253305e-05,
      "loss": 3.6137,
      "step": 7130
    },
    {
      "epoch": 3.631739572736521,
      "grad_norm": 5.305240631103516,
      "learning_rate": 4.636826042726348e-05,
      "loss": 3.4938,
      "step": 7140
    },
    {
      "epoch": 3.636826042726348,
      "grad_norm": 7.194119930267334,
      "learning_rate": 4.636317395727365e-05,
      "loss": 3.5962,
      "step": 7150
    },
    {
      "epoch": 3.641912512716175,
      "grad_norm": 5.363950729370117,
      "learning_rate": 4.635808748728383e-05,
      "loss": 3.4839,
      "step": 7160
    },
    {
      "epoch": 3.646998982706002,
      "grad_norm": 5.105096340179443,
      "learning_rate": 4.6353001017294e-05,
      "loss": 3.533,
      "step": 7170
    },
    {
      "epoch": 3.652085452695829,
      "grad_norm": 8.620826721191406,
      "learning_rate": 4.6347914547304175e-05,
      "loss": 3.5804,
      "step": 7180
    },
    {
      "epoch": 3.6571719226856563,
      "grad_norm": 7.782473087310791,
      "learning_rate": 4.634282807731435e-05,
      "loss": 3.5759,
      "step": 7190
    },
    {
      "epoch": 3.6622583926754833,
      "grad_norm": 6.057344436645508,
      "learning_rate": 4.633774160732452e-05,
      "loss": 3.5642,
      "step": 7200
    },
    {
      "epoch": 3.6673448626653102,
      "grad_norm": 6.224895000457764,
      "learning_rate": 4.633265513733469e-05,
      "loss": 3.5709,
      "step": 7210
    },
    {
      "epoch": 3.672431332655137,
      "grad_norm": 5.362300872802734,
      "learning_rate": 4.632756866734487e-05,
      "loss": 3.6043,
      "step": 7220
    },
    {
      "epoch": 3.6775178026449646,
      "grad_norm": 5.777322769165039,
      "learning_rate": 4.632248219735504e-05,
      "loss": 3.5394,
      "step": 7230
    },
    {
      "epoch": 3.6826042726347916,
      "grad_norm": 6.153487205505371,
      "learning_rate": 4.631739572736521e-05,
      "loss": 3.5048,
      "step": 7240
    },
    {
      "epoch": 3.6876907426246186,
      "grad_norm": 5.022377967834473,
      "learning_rate": 4.6312309257375384e-05,
      "loss": 3.5802,
      "step": 7250
    },
    {
      "epoch": 3.6927772126144456,
      "grad_norm": 6.604250907897949,
      "learning_rate": 4.6307222787385554e-05,
      "loss": 3.56,
      "step": 7260
    },
    {
      "epoch": 3.6978636826042726,
      "grad_norm": 6.2935662269592285,
      "learning_rate": 4.630213631739573e-05,
      "loss": 3.5526,
      "step": 7270
    },
    {
      "epoch": 3.7029501525940995,
      "grad_norm": 6.017805099487305,
      "learning_rate": 4.629704984740591e-05,
      "loss": 3.5665,
      "step": 7280
    },
    {
      "epoch": 3.7080366225839265,
      "grad_norm": 6.805716037750244,
      "learning_rate": 4.629196337741608e-05,
      "loss": 3.5369,
      "step": 7290
    },
    {
      "epoch": 3.713123092573754,
      "grad_norm": 5.785092830657959,
      "learning_rate": 4.628687690742625e-05,
      "loss": 3.5705,
      "step": 7300
    },
    {
      "epoch": 3.718209562563581,
      "grad_norm": 6.968968391418457,
      "learning_rate": 4.6281790437436424e-05,
      "loss": 3.5707,
      "step": 7310
    },
    {
      "epoch": 3.723296032553408,
      "grad_norm": 4.476573944091797,
      "learning_rate": 4.6276703967446594e-05,
      "loss": 3.6302,
      "step": 7320
    },
    {
      "epoch": 3.728382502543235,
      "grad_norm": 6.818967342376709,
      "learning_rate": 4.6271617497456764e-05,
      "loss": 3.5776,
      "step": 7330
    },
    {
      "epoch": 3.7334689725330623,
      "grad_norm": 4.902641773223877,
      "learning_rate": 4.626653102746694e-05,
      "loss": 3.542,
      "step": 7340
    },
    {
      "epoch": 3.7385554425228893,
      "grad_norm": 7.175673484802246,
      "learning_rate": 4.626144455747711e-05,
      "loss": 3.6294,
      "step": 7350
    },
    {
      "epoch": 3.7436419125127163,
      "grad_norm": 7.035593509674072,
      "learning_rate": 4.625635808748729e-05,
      "loss": 3.5111,
      "step": 7360
    },
    {
      "epoch": 3.7487283825025433,
      "grad_norm": 4.818260192871094,
      "learning_rate": 4.6251271617497464e-05,
      "loss": 3.5356,
      "step": 7370
    },
    {
      "epoch": 3.7538148524923702,
      "grad_norm": 4.316448211669922,
      "learning_rate": 4.6246185147507634e-05,
      "loss": 3.5158,
      "step": 7380
    },
    {
      "epoch": 3.758901322482197,
      "grad_norm": 5.205474853515625,
      "learning_rate": 4.6241098677517803e-05,
      "loss": 3.5076,
      "step": 7390
    },
    {
      "epoch": 3.763987792472024,
      "grad_norm": 4.981058597564697,
      "learning_rate": 4.623601220752798e-05,
      "loss": 3.5112,
      "step": 7400
    },
    {
      "epoch": 3.7690742624618516,
      "grad_norm": 9.11139965057373,
      "learning_rate": 4.623092573753815e-05,
      "loss": 3.5791,
      "step": 7410
    },
    {
      "epoch": 3.7741607324516786,
      "grad_norm": 6.497097492218018,
      "learning_rate": 4.622583926754832e-05,
      "loss": 3.5984,
      "step": 7420
    },
    {
      "epoch": 3.7792472024415056,
      "grad_norm": 5.455554962158203,
      "learning_rate": 4.6220752797558497e-05,
      "loss": 3.5153,
      "step": 7430
    },
    {
      "epoch": 3.7843336724313326,
      "grad_norm": 5.274377346038818,
      "learning_rate": 4.6215666327568666e-05,
      "loss": 3.6236,
      "step": 7440
    },
    {
      "epoch": 3.78942014242116,
      "grad_norm": 5.3269124031066895,
      "learning_rate": 4.621057985757884e-05,
      "loss": 3.5068,
      "step": 7450
    },
    {
      "epoch": 3.794506612410987,
      "grad_norm": 4.999887466430664,
      "learning_rate": 4.620549338758901e-05,
      "loss": 3.5454,
      "step": 7460
    },
    {
      "epoch": 3.799593082400814,
      "grad_norm": 5.569624900817871,
      "learning_rate": 4.620040691759919e-05,
      "loss": 3.5025,
      "step": 7470
    },
    {
      "epoch": 3.804679552390641,
      "grad_norm": 5.212758541107178,
      "learning_rate": 4.6195320447609366e-05,
      "loss": 3.5044,
      "step": 7480
    },
    {
      "epoch": 3.809766022380468,
      "grad_norm": 5.46865701675415,
      "learning_rate": 4.6190233977619536e-05,
      "loss": 3.5141,
      "step": 7490
    },
    {
      "epoch": 3.814852492370295,
      "grad_norm": 6.264723777770996,
      "learning_rate": 4.6185147507629706e-05,
      "loss": 3.5176,
      "step": 7500
    },
    {
      "epoch": 3.819938962360122,
      "grad_norm": 6.231154918670654,
      "learning_rate": 4.618006103763988e-05,
      "loss": 3.4819,
      "step": 7510
    },
    {
      "epoch": 3.8250254323499493,
      "grad_norm": 7.268845558166504,
      "learning_rate": 4.617497456765005e-05,
      "loss": 3.5521,
      "step": 7520
    },
    {
      "epoch": 3.8301119023397763,
      "grad_norm": 6.45893669128418,
      "learning_rate": 4.616988809766022e-05,
      "loss": 3.5357,
      "step": 7530
    },
    {
      "epoch": 3.8351983723296033,
      "grad_norm": 6.147562503814697,
      "learning_rate": 4.61648016276704e-05,
      "loss": 3.5249,
      "step": 7540
    },
    {
      "epoch": 3.8402848423194302,
      "grad_norm": 5.291461944580078,
      "learning_rate": 4.615971515768057e-05,
      "loss": 3.5391,
      "step": 7550
    },
    {
      "epoch": 3.845371312309257,
      "grad_norm": 6.942288398742676,
      "learning_rate": 4.6154628687690746e-05,
      "loss": 3.6022,
      "step": 7560
    },
    {
      "epoch": 3.8504577822990846,
      "grad_norm": 4.945714950561523,
      "learning_rate": 4.614954221770092e-05,
      "loss": 3.5635,
      "step": 7570
    },
    {
      "epoch": 3.8555442522889116,
      "grad_norm": 9.11294937133789,
      "learning_rate": 4.614445574771109e-05,
      "loss": 3.4485,
      "step": 7580
    },
    {
      "epoch": 3.8606307222787386,
      "grad_norm": 4.3927531242370605,
      "learning_rate": 4.613936927772126e-05,
      "loss": 3.5657,
      "step": 7590
    },
    {
      "epoch": 3.8657171922685656,
      "grad_norm": 6.62286901473999,
      "learning_rate": 4.613428280773144e-05,
      "loss": 3.5646,
      "step": 7600
    },
    {
      "epoch": 3.8708036622583926,
      "grad_norm": 7.678708076477051,
      "learning_rate": 4.612919633774161e-05,
      "loss": 3.549,
      "step": 7610
    },
    {
      "epoch": 3.8758901322482195,
      "grad_norm": 8.059137344360352,
      "learning_rate": 4.612410986775178e-05,
      "loss": 3.5159,
      "step": 7620
    },
    {
      "epoch": 3.8809766022380465,
      "grad_norm": 5.620709419250488,
      "learning_rate": 4.6119023397761955e-05,
      "loss": 3.5623,
      "step": 7630
    },
    {
      "epoch": 3.886063072227874,
      "grad_norm": 6.80388069152832,
      "learning_rate": 4.6113936927772125e-05,
      "loss": 3.5126,
      "step": 7640
    },
    {
      "epoch": 3.891149542217701,
      "grad_norm": 6.889127254486084,
      "learning_rate": 4.61088504577823e-05,
      "loss": 3.4742,
      "step": 7650
    },
    {
      "epoch": 3.896236012207528,
      "grad_norm": 5.844050884246826,
      "learning_rate": 4.610376398779248e-05,
      "loss": 3.6426,
      "step": 7660
    },
    {
      "epoch": 3.901322482197355,
      "grad_norm": 5.526522636413574,
      "learning_rate": 4.609867751780265e-05,
      "loss": 3.5746,
      "step": 7670
    },
    {
      "epoch": 3.9064089521871823,
      "grad_norm": 7.178666591644287,
      "learning_rate": 4.609359104781282e-05,
      "loss": 3.5512,
      "step": 7680
    },
    {
      "epoch": 3.9114954221770093,
      "grad_norm": 5.448190212249756,
      "learning_rate": 4.6088504577822995e-05,
      "loss": 3.5583,
      "step": 7690
    },
    {
      "epoch": 3.9165818921668363,
      "grad_norm": 7.872321605682373,
      "learning_rate": 4.6083418107833165e-05,
      "loss": 3.5339,
      "step": 7700
    },
    {
      "epoch": 3.9216683621566633,
      "grad_norm": 5.739564895629883,
      "learning_rate": 4.607833163784334e-05,
      "loss": 3.5863,
      "step": 7710
    },
    {
      "epoch": 3.9267548321464902,
      "grad_norm": 7.415482997894287,
      "learning_rate": 4.607324516785351e-05,
      "loss": 3.5802,
      "step": 7720
    },
    {
      "epoch": 3.931841302136317,
      "grad_norm": 6.35541296005249,
      "learning_rate": 4.606815869786368e-05,
      "loss": 3.4715,
      "step": 7730
    },
    {
      "epoch": 3.936927772126144,
      "grad_norm": 6.134989261627197,
      "learning_rate": 4.606307222787386e-05,
      "loss": 3.4953,
      "step": 7740
    },
    {
      "epoch": 3.9420142421159716,
      "grad_norm": 4.735561847686768,
      "learning_rate": 4.605798575788403e-05,
      "loss": 3.5409,
      "step": 7750
    },
    {
      "epoch": 3.9471007121057986,
      "grad_norm": 7.810882568359375,
      "learning_rate": 4.6052899287894205e-05,
      "loss": 3.4791,
      "step": 7760
    },
    {
      "epoch": 3.9521871820956256,
      "grad_norm": 8.222249984741211,
      "learning_rate": 4.604781281790438e-05,
      "loss": 3.4796,
      "step": 7770
    },
    {
      "epoch": 3.9572736520854526,
      "grad_norm": 4.569162845611572,
      "learning_rate": 4.604272634791455e-05,
      "loss": 3.5221,
      "step": 7780
    },
    {
      "epoch": 3.96236012207528,
      "grad_norm": 6.029212951660156,
      "learning_rate": 4.603763987792472e-05,
      "loss": 3.5498,
      "step": 7790
    },
    {
      "epoch": 3.967446592065107,
      "grad_norm": 6.914393901824951,
      "learning_rate": 4.60325534079349e-05,
      "loss": 3.4702,
      "step": 7800
    },
    {
      "epoch": 3.972533062054934,
      "grad_norm": 5.658518314361572,
      "learning_rate": 4.602746693794507e-05,
      "loss": 3.5559,
      "step": 7810
    },
    {
      "epoch": 3.977619532044761,
      "grad_norm": 4.597030162811279,
      "learning_rate": 4.602238046795524e-05,
      "loss": 3.5152,
      "step": 7820
    },
    {
      "epoch": 3.982706002034588,
      "grad_norm": 7.656993865966797,
      "learning_rate": 4.6017293997965414e-05,
      "loss": 3.5653,
      "step": 7830
    },
    {
      "epoch": 3.987792472024415,
      "grad_norm": 5.765600204467773,
      "learning_rate": 4.6012207527975584e-05,
      "loss": 3.5138,
      "step": 7840
    },
    {
      "epoch": 3.992878942014242,
      "grad_norm": 5.834529876708984,
      "learning_rate": 4.600712105798576e-05,
      "loss": 3.5151,
      "step": 7850
    },
    {
      "epoch": 3.9979654120040693,
      "grad_norm": 5.882931709289551,
      "learning_rate": 4.600203458799594e-05,
      "loss": 3.4387,
      "step": 7860
    },
    {
      "epoch": 4.0,
      "eval_loss": 3.682547092437744,
      "eval_runtime": 2.7269,
      "eval_samples_per_second": 1017.624,
      "eval_steps_per_second": 127.249,
      "step": 7864
    },
    {
      "epoch": 4.003051881993896,
      "grad_norm": 5.696679592132568,
      "learning_rate": 4.599694811800611e-05,
      "loss": 3.5395,
      "step": 7870
    },
    {
      "epoch": 4.008138351983724,
      "grad_norm": 7.439525127410889,
      "learning_rate": 4.599186164801628e-05,
      "loss": 3.5478,
      "step": 7880
    },
    {
      "epoch": 4.013224821973551,
      "grad_norm": 7.7172627449035645,
      "learning_rate": 4.5986775178026454e-05,
      "loss": 3.5519,
      "step": 7890
    },
    {
      "epoch": 4.018311291963378,
      "grad_norm": 6.432249546051025,
      "learning_rate": 4.5981688708036624e-05,
      "loss": 3.5054,
      "step": 7900
    },
    {
      "epoch": 4.023397761953205,
      "grad_norm": 6.983066082000732,
      "learning_rate": 4.5976602238046794e-05,
      "loss": 3.4648,
      "step": 7910
    },
    {
      "epoch": 4.028484231943032,
      "grad_norm": 6.731037616729736,
      "learning_rate": 4.597151576805697e-05,
      "loss": 3.5688,
      "step": 7920
    },
    {
      "epoch": 4.033570701932859,
      "grad_norm": 5.829944133758545,
      "learning_rate": 4.596642929806714e-05,
      "loss": 3.5034,
      "step": 7930
    },
    {
      "epoch": 4.038657171922686,
      "grad_norm": 6.856215953826904,
      "learning_rate": 4.596134282807732e-05,
      "loss": 3.4485,
      "step": 7940
    },
    {
      "epoch": 4.043743641912513,
      "grad_norm": 7.135995388031006,
      "learning_rate": 4.5956256358087494e-05,
      "loss": 3.5118,
      "step": 7950
    },
    {
      "epoch": 4.0488301119023395,
      "grad_norm": 6.787564277648926,
      "learning_rate": 4.5951169888097664e-05,
      "loss": 3.5648,
      "step": 7960
    },
    {
      "epoch": 4.0539165818921665,
      "grad_norm": 5.2697224617004395,
      "learning_rate": 4.594608341810784e-05,
      "loss": 3.5025,
      "step": 7970
    },
    {
      "epoch": 4.0590030518819935,
      "grad_norm": 8.408817291259766,
      "learning_rate": 4.594099694811801e-05,
      "loss": 3.4802,
      "step": 7980
    },
    {
      "epoch": 4.064089521871821,
      "grad_norm": 8.241003036499023,
      "learning_rate": 4.593591047812818e-05,
      "loss": 3.482,
      "step": 7990
    },
    {
      "epoch": 4.069175991861648,
      "grad_norm": 7.665555477142334,
      "learning_rate": 4.593082400813836e-05,
      "loss": 3.5327,
      "step": 8000
    },
    {
      "epoch": 4.074262461851475,
      "grad_norm": 7.697659969329834,
      "learning_rate": 4.5925737538148527e-05,
      "loss": 3.5564,
      "step": 8010
    },
    {
      "epoch": 4.079348931841302,
      "grad_norm": 8.690217971801758,
      "learning_rate": 4.5920651068158696e-05,
      "loss": 3.5187,
      "step": 8020
    },
    {
      "epoch": 4.084435401831129,
      "grad_norm": 6.476851940155029,
      "learning_rate": 4.591556459816887e-05,
      "loss": 3.5528,
      "step": 8030
    },
    {
      "epoch": 4.089521871820956,
      "grad_norm": 4.610379695892334,
      "learning_rate": 4.591047812817904e-05,
      "loss": 3.4914,
      "step": 8040
    },
    {
      "epoch": 4.094608341810783,
      "grad_norm": 7.771125316619873,
      "learning_rate": 4.590539165818922e-05,
      "loss": 3.5258,
      "step": 8050
    },
    {
      "epoch": 4.09969481180061,
      "grad_norm": 5.73425817489624,
      "learning_rate": 4.5900305188199396e-05,
      "loss": 3.5533,
      "step": 8060
    },
    {
      "epoch": 4.104781281790437,
      "grad_norm": 6.67058801651001,
      "learning_rate": 4.5895218718209566e-05,
      "loss": 3.5051,
      "step": 8070
    },
    {
      "epoch": 4.109867751780264,
      "grad_norm": 6.453938007354736,
      "learning_rate": 4.5890132248219736e-05,
      "loss": 3.468,
      "step": 8080
    },
    {
      "epoch": 4.114954221770091,
      "grad_norm": 5.88721227645874,
      "learning_rate": 4.588504577822991e-05,
      "loss": 3.5389,
      "step": 8090
    },
    {
      "epoch": 4.120040691759919,
      "grad_norm": 6.570307731628418,
      "learning_rate": 4.587995930824008e-05,
      "loss": 3.4452,
      "step": 8100
    },
    {
      "epoch": 4.125127161749746,
      "grad_norm": 5.611356735229492,
      "learning_rate": 4.587487283825025e-05,
      "loss": 3.4966,
      "step": 8110
    },
    {
      "epoch": 4.130213631739573,
      "grad_norm": 6.96566104888916,
      "learning_rate": 4.586978636826043e-05,
      "loss": 3.5291,
      "step": 8120
    },
    {
      "epoch": 4.1353001017294,
      "grad_norm": 7.933901786804199,
      "learning_rate": 4.58646998982706e-05,
      "loss": 3.4813,
      "step": 8130
    },
    {
      "epoch": 4.140386571719227,
      "grad_norm": 5.545637607574463,
      "learning_rate": 4.5859613428280776e-05,
      "loss": 3.552,
      "step": 8140
    },
    {
      "epoch": 4.145473041709054,
      "grad_norm": 6.902024269104004,
      "learning_rate": 4.585452695829095e-05,
      "loss": 3.4831,
      "step": 8150
    },
    {
      "epoch": 4.150559511698881,
      "grad_norm": 8.349004745483398,
      "learning_rate": 4.584944048830112e-05,
      "loss": 3.468,
      "step": 8160
    },
    {
      "epoch": 4.155645981688708,
      "grad_norm": 4.874841690063477,
      "learning_rate": 4.584435401831129e-05,
      "loss": 3.5194,
      "step": 8170
    },
    {
      "epoch": 4.160732451678535,
      "grad_norm": 5.534297943115234,
      "learning_rate": 4.583926754832147e-05,
      "loss": 3.5145,
      "step": 8180
    },
    {
      "epoch": 4.165818921668362,
      "grad_norm": 6.5342936515808105,
      "learning_rate": 4.583418107833164e-05,
      "loss": 3.5465,
      "step": 8190
    },
    {
      "epoch": 4.170905391658189,
      "grad_norm": 6.877661228179932,
      "learning_rate": 4.582909460834181e-05,
      "loss": 3.5069,
      "step": 8200
    },
    {
      "epoch": 4.175991861648017,
      "grad_norm": 5.838846683502197,
      "learning_rate": 4.5824008138351985e-05,
      "loss": 3.4646,
      "step": 8210
    },
    {
      "epoch": 4.181078331637844,
      "grad_norm": 5.677132606506348,
      "learning_rate": 4.5818921668362155e-05,
      "loss": 3.4656,
      "step": 8220
    },
    {
      "epoch": 4.186164801627671,
      "grad_norm": 7.908148765563965,
      "learning_rate": 4.581383519837233e-05,
      "loss": 3.4607,
      "step": 8230
    },
    {
      "epoch": 4.191251271617498,
      "grad_norm": 6.314956188201904,
      "learning_rate": 4.580874872838251e-05,
      "loss": 3.4616,
      "step": 8240
    },
    {
      "epoch": 4.196337741607325,
      "grad_norm": 7.86068058013916,
      "learning_rate": 4.580366225839268e-05,
      "loss": 3.5131,
      "step": 8250
    },
    {
      "epoch": 4.201424211597152,
      "grad_norm": 5.117482662200928,
      "learning_rate": 4.5798575788402855e-05,
      "loss": 3.4784,
      "step": 8260
    },
    {
      "epoch": 4.206510681586979,
      "grad_norm": 6.023205280303955,
      "learning_rate": 4.5793489318413025e-05,
      "loss": 3.4265,
      "step": 8270
    },
    {
      "epoch": 4.211597151576806,
      "grad_norm": 7.292799949645996,
      "learning_rate": 4.5788402848423195e-05,
      "loss": 3.4721,
      "step": 8280
    },
    {
      "epoch": 4.216683621566633,
      "grad_norm": 7.190707683563232,
      "learning_rate": 4.578331637843337e-05,
      "loss": 3.5505,
      "step": 8290
    },
    {
      "epoch": 4.2217700915564595,
      "grad_norm": 6.749729156494141,
      "learning_rate": 4.577822990844354e-05,
      "loss": 3.506,
      "step": 8300
    },
    {
      "epoch": 4.2268565615462865,
      "grad_norm": 7.345061302185059,
      "learning_rate": 4.577314343845371e-05,
      "loss": 3.4624,
      "step": 8310
    },
    {
      "epoch": 4.2319430315361135,
      "grad_norm": 8.302069664001465,
      "learning_rate": 4.576805696846389e-05,
      "loss": 3.4777,
      "step": 8320
    },
    {
      "epoch": 4.237029501525941,
      "grad_norm": 7.307460308074951,
      "learning_rate": 4.5762970498474065e-05,
      "loss": 3.4481,
      "step": 8330
    },
    {
      "epoch": 4.242115971515768,
      "grad_norm": 8.880420684814453,
      "learning_rate": 4.5757884028484235e-05,
      "loss": 3.4313,
      "step": 8340
    },
    {
      "epoch": 4.247202441505595,
      "grad_norm": 8.772892951965332,
      "learning_rate": 4.575279755849441e-05,
      "loss": 3.4719,
      "step": 8350
    },
    {
      "epoch": 4.252288911495422,
      "grad_norm": 7.044968128204346,
      "learning_rate": 4.574771108850458e-05,
      "loss": 3.5015,
      "step": 8360
    },
    {
      "epoch": 4.257375381485249,
      "grad_norm": 7.717684268951416,
      "learning_rate": 4.574262461851475e-05,
      "loss": 3.4443,
      "step": 8370
    },
    {
      "epoch": 4.262461851475076,
      "grad_norm": 5.569419860839844,
      "learning_rate": 4.573753814852493e-05,
      "loss": 3.5175,
      "step": 8380
    },
    {
      "epoch": 4.267548321464903,
      "grad_norm": 9.139665603637695,
      "learning_rate": 4.57324516785351e-05,
      "loss": 3.4793,
      "step": 8390
    },
    {
      "epoch": 4.27263479145473,
      "grad_norm": 6.840276718139648,
      "learning_rate": 4.572736520854527e-05,
      "loss": 3.4976,
      "step": 8400
    },
    {
      "epoch": 4.277721261444557,
      "grad_norm": 9.281371116638184,
      "learning_rate": 4.5722278738555444e-05,
      "loss": 3.4081,
      "step": 8410
    },
    {
      "epoch": 4.282807731434384,
      "grad_norm": 7.087295055389404,
      "learning_rate": 4.5717192268565614e-05,
      "loss": 3.557,
      "step": 8420
    },
    {
      "epoch": 4.287894201424211,
      "grad_norm": 6.432158470153809,
      "learning_rate": 4.571210579857579e-05,
      "loss": 3.4804,
      "step": 8430
    },
    {
      "epoch": 4.292980671414039,
      "grad_norm": 7.989634990692139,
      "learning_rate": 4.570701932858597e-05,
      "loss": 3.5354,
      "step": 8440
    },
    {
      "epoch": 4.298067141403866,
      "grad_norm": 8.7486572265625,
      "learning_rate": 4.570193285859614e-05,
      "loss": 3.3316,
      "step": 8450
    },
    {
      "epoch": 4.303153611393693,
      "grad_norm": 6.547328948974609,
      "learning_rate": 4.569684638860631e-05,
      "loss": 3.481,
      "step": 8460
    },
    {
      "epoch": 4.30824008138352,
      "grad_norm": 6.48618221282959,
      "learning_rate": 4.5691759918616484e-05,
      "loss": 3.4517,
      "step": 8470
    },
    {
      "epoch": 4.313326551373347,
      "grad_norm": 6.9151387214660645,
      "learning_rate": 4.5686673448626654e-05,
      "loss": 3.4846,
      "step": 8480
    },
    {
      "epoch": 4.318413021363174,
      "grad_norm": 8.77987289428711,
      "learning_rate": 4.5681586978636824e-05,
      "loss": 3.5076,
      "step": 8490
    },
    {
      "epoch": 4.323499491353001,
      "grad_norm": 7.875593662261963,
      "learning_rate": 4.5676500508647e-05,
      "loss": 3.4863,
      "step": 8500
    },
    {
      "epoch": 4.328585961342828,
      "grad_norm": 6.400660514831543,
      "learning_rate": 4.567141403865717e-05,
      "loss": 3.4977,
      "step": 8510
    },
    {
      "epoch": 4.333672431332655,
      "grad_norm": 5.12528657913208,
      "learning_rate": 4.566632756866735e-05,
      "loss": 3.4756,
      "step": 8520
    },
    {
      "epoch": 4.338758901322482,
      "grad_norm": 6.7897257804870605,
      "learning_rate": 4.5661241098677524e-05,
      "loss": 3.3882,
      "step": 8530
    },
    {
      "epoch": 4.343845371312309,
      "grad_norm": 7.742665767669678,
      "learning_rate": 4.5656154628687694e-05,
      "loss": 3.4606,
      "step": 8540
    },
    {
      "epoch": 4.348931841302137,
      "grad_norm": 6.026894569396973,
      "learning_rate": 4.565106815869787e-05,
      "loss": 3.5058,
      "step": 8550
    },
    {
      "epoch": 4.354018311291964,
      "grad_norm": 7.406733512878418,
      "learning_rate": 4.564598168870804e-05,
      "loss": 3.4474,
      "step": 8560
    },
    {
      "epoch": 4.359104781281791,
      "grad_norm": 5.7472124099731445,
      "learning_rate": 4.564089521871821e-05,
      "loss": 3.4928,
      "step": 8570
    },
    {
      "epoch": 4.364191251271618,
      "grad_norm": 6.1093645095825195,
      "learning_rate": 4.563580874872839e-05,
      "loss": 3.417,
      "step": 8580
    },
    {
      "epoch": 4.369277721261445,
      "grad_norm": 8.887754440307617,
      "learning_rate": 4.5630722278738557e-05,
      "loss": 3.4739,
      "step": 8590
    },
    {
      "epoch": 4.374364191251272,
      "grad_norm": 5.5056633949279785,
      "learning_rate": 4.5625635808748726e-05,
      "loss": 3.4902,
      "step": 8600
    },
    {
      "epoch": 4.379450661241099,
      "grad_norm": 8.776473999023438,
      "learning_rate": 4.56205493387589e-05,
      "loss": 3.4383,
      "step": 8610
    },
    {
      "epoch": 4.384537131230926,
      "grad_norm": 12.802504539489746,
      "learning_rate": 4.561546286876908e-05,
      "loss": 3.4436,
      "step": 8620
    },
    {
      "epoch": 4.389623601220753,
      "grad_norm": 5.422558784484863,
      "learning_rate": 4.561037639877925e-05,
      "loss": 3.4911,
      "step": 8630
    },
    {
      "epoch": 4.3947100712105795,
      "grad_norm": 8.648914337158203,
      "learning_rate": 4.5605289928789426e-05,
      "loss": 3.4686,
      "step": 8640
    },
    {
      "epoch": 4.3997965412004065,
      "grad_norm": 7.750622272491455,
      "learning_rate": 4.5600203458799596e-05,
      "loss": 3.4595,
      "step": 8650
    },
    {
      "epoch": 4.404883011190234,
      "grad_norm": 8.931674003601074,
      "learning_rate": 4.5595116988809766e-05,
      "loss": 3.4817,
      "step": 8660
    },
    {
      "epoch": 4.409969481180061,
      "grad_norm": 5.857328414916992,
      "learning_rate": 4.559003051881994e-05,
      "loss": 3.379,
      "step": 8670
    },
    {
      "epoch": 4.415055951169888,
      "grad_norm": 10.644730567932129,
      "learning_rate": 4.558494404883011e-05,
      "loss": 3.4553,
      "step": 8680
    },
    {
      "epoch": 4.420142421159715,
      "grad_norm": 7.72205114364624,
      "learning_rate": 4.557985757884028e-05,
      "loss": 3.423,
      "step": 8690
    },
    {
      "epoch": 4.425228891149542,
      "grad_norm": 7.861493110656738,
      "learning_rate": 4.557477110885046e-05,
      "loss": 3.5308,
      "step": 8700
    },
    {
      "epoch": 4.430315361139369,
      "grad_norm": 10.250804901123047,
      "learning_rate": 4.556968463886063e-05,
      "loss": 3.4332,
      "step": 8710
    },
    {
      "epoch": 4.435401831129196,
      "grad_norm": 9.085840225219727,
      "learning_rate": 4.5564598168870806e-05,
      "loss": 3.4776,
      "step": 8720
    },
    {
      "epoch": 4.440488301119023,
      "grad_norm": 8.356430053710938,
      "learning_rate": 4.555951169888098e-05,
      "loss": 3.4658,
      "step": 8730
    },
    {
      "epoch": 4.44557477110885,
      "grad_norm": 6.438593864440918,
      "learning_rate": 4.555442522889115e-05,
      "loss": 3.4122,
      "step": 8740
    },
    {
      "epoch": 4.450661241098677,
      "grad_norm": 6.882085800170898,
      "learning_rate": 4.554933875890132e-05,
      "loss": 3.3572,
      "step": 8750
    },
    {
      "epoch": 4.455747711088504,
      "grad_norm": 8.223237037658691,
      "learning_rate": 4.55442522889115e-05,
      "loss": 3.4867,
      "step": 8760
    },
    {
      "epoch": 4.460834181078331,
      "grad_norm": 10.363363265991211,
      "learning_rate": 4.553916581892167e-05,
      "loss": 3.4631,
      "step": 8770
    },
    {
      "epoch": 4.465920651068159,
      "grad_norm": 7.76474142074585,
      "learning_rate": 4.5534079348931846e-05,
      "loss": 3.4099,
      "step": 8780
    },
    {
      "epoch": 4.471007121057986,
      "grad_norm": 6.035660266876221,
      "learning_rate": 4.5528992878942015e-05,
      "loss": 3.5282,
      "step": 8790
    },
    {
      "epoch": 4.476093591047813,
      "grad_norm": 8.37148666381836,
      "learning_rate": 4.5523906408952185e-05,
      "loss": 3.4237,
      "step": 8800
    },
    {
      "epoch": 4.48118006103764,
      "grad_norm": 9.595413208007812,
      "learning_rate": 4.551881993896236e-05,
      "loss": 3.3798,
      "step": 8810
    },
    {
      "epoch": 4.486266531027467,
      "grad_norm": 9.972001075744629,
      "learning_rate": 4.551373346897254e-05,
      "loss": 3.3891,
      "step": 8820
    },
    {
      "epoch": 4.491353001017294,
      "grad_norm": 6.743781089782715,
      "learning_rate": 4.550864699898271e-05,
      "loss": 3.5214,
      "step": 8830
    },
    {
      "epoch": 4.496439471007121,
      "grad_norm": 7.265984535217285,
      "learning_rate": 4.5503560528992885e-05,
      "loss": 3.465,
      "step": 8840
    },
    {
      "epoch": 4.501525940996948,
      "grad_norm": 9.915414810180664,
      "learning_rate": 4.5498474059003055e-05,
      "loss": 3.4719,
      "step": 8850
    },
    {
      "epoch": 4.506612410986775,
      "grad_norm": 8.241029739379883,
      "learning_rate": 4.5493387589013225e-05,
      "loss": 3.4857,
      "step": 8860
    },
    {
      "epoch": 4.511698880976602,
      "grad_norm": 8.134593963623047,
      "learning_rate": 4.54883011190234e-05,
      "loss": 3.4377,
      "step": 8870
    },
    {
      "epoch": 4.51678535096643,
      "grad_norm": 8.605182647705078,
      "learning_rate": 4.548321464903357e-05,
      "loss": 3.4001,
      "step": 8880
    },
    {
      "epoch": 4.521871820956257,
      "grad_norm": 10.543033599853516,
      "learning_rate": 4.547812817904374e-05,
      "loss": 3.5066,
      "step": 8890
    },
    {
      "epoch": 4.526958290946084,
      "grad_norm": 8.078709602355957,
      "learning_rate": 4.547304170905392e-05,
      "loss": 3.4245,
      "step": 8900
    },
    {
      "epoch": 4.532044760935911,
      "grad_norm": 8.793431282043457,
      "learning_rate": 4.5467955239064095e-05,
      "loss": 3.4561,
      "step": 8910
    },
    {
      "epoch": 4.537131230925738,
      "grad_norm": 8.877737998962402,
      "learning_rate": 4.5462868769074265e-05,
      "loss": 3.4705,
      "step": 8920
    },
    {
      "epoch": 4.542217700915565,
      "grad_norm": 7.629286766052246,
      "learning_rate": 4.545778229908444e-05,
      "loss": 3.4751,
      "step": 8930
    },
    {
      "epoch": 4.547304170905392,
      "grad_norm": 7.7086076736450195,
      "learning_rate": 4.545269582909461e-05,
      "loss": 3.4642,
      "step": 8940
    },
    {
      "epoch": 4.552390640895219,
      "grad_norm": 7.560568332672119,
      "learning_rate": 4.544760935910478e-05,
      "loss": 3.4221,
      "step": 8950
    },
    {
      "epoch": 4.557477110885046,
      "grad_norm": 6.356943130493164,
      "learning_rate": 4.544252288911496e-05,
      "loss": 3.4515,
      "step": 8960
    },
    {
      "epoch": 4.562563580874873,
      "grad_norm": 7.457127094268799,
      "learning_rate": 4.543743641912513e-05,
      "loss": 3.4878,
      "step": 8970
    },
    {
      "epoch": 4.5676500508646996,
      "grad_norm": 6.367786884307861,
      "learning_rate": 4.54323499491353e-05,
      "loss": 3.4584,
      "step": 8980
    },
    {
      "epoch": 4.5727365208545265,
      "grad_norm": 10.294775009155273,
      "learning_rate": 4.5427263479145474e-05,
      "loss": 3.4518,
      "step": 8990
    },
    {
      "epoch": 4.577822990844354,
      "grad_norm": 6.925987720489502,
      "learning_rate": 4.5422177009155644e-05,
      "loss": 3.4573,
      "step": 9000
    },
    {
      "epoch": 4.582909460834181,
      "grad_norm": 9.72468090057373,
      "learning_rate": 4.541709053916582e-05,
      "loss": 3.4615,
      "step": 9010
    },
    {
      "epoch": 4.587995930824008,
      "grad_norm": 7.277684688568115,
      "learning_rate": 4.5412004069176e-05,
      "loss": 3.5261,
      "step": 9020
    },
    {
      "epoch": 4.593082400813835,
      "grad_norm": 8.789225578308105,
      "learning_rate": 4.540691759918617e-05,
      "loss": 3.4422,
      "step": 9030
    },
    {
      "epoch": 4.598168870803662,
      "grad_norm": 9.132124900817871,
      "learning_rate": 4.540183112919634e-05,
      "loss": 3.3697,
      "step": 9040
    },
    {
      "epoch": 4.603255340793489,
      "grad_norm": 9.38601303100586,
      "learning_rate": 4.5396744659206514e-05,
      "loss": 3.4617,
      "step": 9050
    },
    {
      "epoch": 4.608341810783316,
      "grad_norm": 7.389291286468506,
      "learning_rate": 4.5391658189216684e-05,
      "loss": 3.4407,
      "step": 9060
    },
    {
      "epoch": 4.613428280773143,
      "grad_norm": 7.182735443115234,
      "learning_rate": 4.538657171922686e-05,
      "loss": 3.4555,
      "step": 9070
    },
    {
      "epoch": 4.61851475076297,
      "grad_norm": 8.08858585357666,
      "learning_rate": 4.538148524923703e-05,
      "loss": 3.4242,
      "step": 9080
    },
    {
      "epoch": 4.623601220752797,
      "grad_norm": 9.837270736694336,
      "learning_rate": 4.53763987792472e-05,
      "loss": 3.4403,
      "step": 9090
    },
    {
      "epoch": 4.628687690742625,
      "grad_norm": 8.107197761535645,
      "learning_rate": 4.537131230925738e-05,
      "loss": 3.4998,
      "step": 9100
    },
    {
      "epoch": 4.633774160732452,
      "grad_norm": 8.040487289428711,
      "learning_rate": 4.5366225839267554e-05,
      "loss": 3.4844,
      "step": 9110
    },
    {
      "epoch": 4.638860630722279,
      "grad_norm": 8.661519050598145,
      "learning_rate": 4.5361139369277724e-05,
      "loss": 3.4773,
      "step": 9120
    },
    {
      "epoch": 4.643947100712106,
      "grad_norm": 7.067198276519775,
      "learning_rate": 4.53560528992879e-05,
      "loss": 3.4678,
      "step": 9130
    },
    {
      "epoch": 4.649033570701933,
      "grad_norm": 7.373317718505859,
      "learning_rate": 4.535096642929807e-05,
      "loss": 3.487,
      "step": 9140
    },
    {
      "epoch": 4.65412004069176,
      "grad_norm": 8.952664375305176,
      "learning_rate": 4.534587995930824e-05,
      "loss": 3.4,
      "step": 9150
    },
    {
      "epoch": 4.659206510681587,
      "grad_norm": 6.957043170928955,
      "learning_rate": 4.534079348931842e-05,
      "loss": 3.4165,
      "step": 9160
    },
    {
      "epoch": 4.664292980671414,
      "grad_norm": 7.385956764221191,
      "learning_rate": 4.5335707019328587e-05,
      "loss": 3.4986,
      "step": 9170
    },
    {
      "epoch": 4.669379450661241,
      "grad_norm": 8.847443580627441,
      "learning_rate": 4.5330620549338756e-05,
      "loss": 3.3991,
      "step": 9180
    },
    {
      "epoch": 4.674465920651068,
      "grad_norm": 8.921510696411133,
      "learning_rate": 4.532553407934893e-05,
      "loss": 3.387,
      "step": 9190
    },
    {
      "epoch": 4.679552390640895,
      "grad_norm": 5.965644359588623,
      "learning_rate": 4.532044760935911e-05,
      "loss": 3.5382,
      "step": 9200
    },
    {
      "epoch": 4.684638860630722,
      "grad_norm": 6.969622611999512,
      "learning_rate": 4.531536113936928e-05,
      "loss": 3.3679,
      "step": 9210
    },
    {
      "epoch": 4.689725330620549,
      "grad_norm": 7.57749080657959,
      "learning_rate": 4.5310274669379456e-05,
      "loss": 3.395,
      "step": 9220
    },
    {
      "epoch": 4.694811800610377,
      "grad_norm": 6.542710304260254,
      "learning_rate": 4.5305188199389626e-05,
      "loss": 3.4391,
      "step": 9230
    },
    {
      "epoch": 4.699898270600204,
      "grad_norm": 7.5784807205200195,
      "learning_rate": 4.5300101729399796e-05,
      "loss": 3.4806,
      "step": 9240
    },
    {
      "epoch": 4.704984740590031,
      "grad_norm": 6.456599712371826,
      "learning_rate": 4.529501525940997e-05,
      "loss": 3.5053,
      "step": 9250
    },
    {
      "epoch": 4.710071210579858,
      "grad_norm": 6.538114547729492,
      "learning_rate": 4.528992878942014e-05,
      "loss": 3.4178,
      "step": 9260
    },
    {
      "epoch": 4.715157680569685,
      "grad_norm": 8.753795623779297,
      "learning_rate": 4.528484231943031e-05,
      "loss": 3.4396,
      "step": 9270
    },
    {
      "epoch": 4.720244150559512,
      "grad_norm": 8.9151611328125,
      "learning_rate": 4.527975584944049e-05,
      "loss": 3.4548,
      "step": 9280
    },
    {
      "epoch": 4.725330620549339,
      "grad_norm": 6.75811243057251,
      "learning_rate": 4.5274669379450666e-05,
      "loss": 3.4676,
      "step": 9290
    },
    {
      "epoch": 4.730417090539166,
      "grad_norm": 5.821547031402588,
      "learning_rate": 4.5269582909460836e-05,
      "loss": 3.4346,
      "step": 9300
    },
    {
      "epoch": 4.735503560528993,
      "grad_norm": 5.982168197631836,
      "learning_rate": 4.526449643947101e-05,
      "loss": 3.4206,
      "step": 9310
    },
    {
      "epoch": 4.7405900305188196,
      "grad_norm": 8.34958267211914,
      "learning_rate": 4.525940996948118e-05,
      "loss": 3.4191,
      "step": 9320
    },
    {
      "epoch": 4.745676500508647,
      "grad_norm": 7.6805901527404785,
      "learning_rate": 4.525432349949136e-05,
      "loss": 3.3965,
      "step": 9330
    },
    {
      "epoch": 4.750762970498474,
      "grad_norm": 8.454001426696777,
      "learning_rate": 4.524923702950153e-05,
      "loss": 3.3419,
      "step": 9340
    },
    {
      "epoch": 4.755849440488301,
      "grad_norm": 8.188343048095703,
      "learning_rate": 4.52441505595117e-05,
      "loss": 3.5124,
      "step": 9350
    },
    {
      "epoch": 4.760935910478128,
      "grad_norm": 8.950751304626465,
      "learning_rate": 4.5239064089521876e-05,
      "loss": 3.4223,
      "step": 9360
    },
    {
      "epoch": 4.766022380467955,
      "grad_norm": 9.847806930541992,
      "learning_rate": 4.5233977619532045e-05,
      "loss": 3.3539,
      "step": 9370
    },
    {
      "epoch": 4.771108850457782,
      "grad_norm": 6.817256927490234,
      "learning_rate": 4.5228891149542215e-05,
      "loss": 3.4986,
      "step": 9380
    },
    {
      "epoch": 4.776195320447609,
      "grad_norm": 8.241292953491211,
      "learning_rate": 4.522380467955239e-05,
      "loss": 3.4582,
      "step": 9390
    },
    {
      "epoch": 4.781281790437436,
      "grad_norm": 7.211737155914307,
      "learning_rate": 4.521871820956257e-05,
      "loss": 3.398,
      "step": 9400
    },
    {
      "epoch": 4.786368260427263,
      "grad_norm": 10.099587440490723,
      "learning_rate": 4.521363173957274e-05,
      "loss": 3.4068,
      "step": 9410
    },
    {
      "epoch": 4.79145473041709,
      "grad_norm": 9.291696548461914,
      "learning_rate": 4.5208545269582915e-05,
      "loss": 3.3966,
      "step": 9420
    },
    {
      "epoch": 4.796541200406917,
      "grad_norm": 8.879063606262207,
      "learning_rate": 4.5203458799593085e-05,
      "loss": 3.3861,
      "step": 9430
    },
    {
      "epoch": 4.801627670396744,
      "grad_norm": 5.990861415863037,
      "learning_rate": 4.5198372329603255e-05,
      "loss": 3.3656,
      "step": 9440
    },
    {
      "epoch": 4.806714140386572,
      "grad_norm": 10.332392692565918,
      "learning_rate": 4.519328585961343e-05,
      "loss": 3.4341,
      "step": 9450
    },
    {
      "epoch": 4.811800610376399,
      "grad_norm": 9.404748916625977,
      "learning_rate": 4.51881993896236e-05,
      "loss": 3.3905,
      "step": 9460
    },
    {
      "epoch": 4.816887080366226,
      "grad_norm": 9.398710250854492,
      "learning_rate": 4.518311291963377e-05,
      "loss": 3.3496,
      "step": 9470
    },
    {
      "epoch": 4.821973550356053,
      "grad_norm": 10.12822151184082,
      "learning_rate": 4.517802644964395e-05,
      "loss": 3.4069,
      "step": 9480
    },
    {
      "epoch": 4.82706002034588,
      "grad_norm": 8.296046257019043,
      "learning_rate": 4.5172939979654125e-05,
      "loss": 3.4047,
      "step": 9490
    },
    {
      "epoch": 4.832146490335707,
      "grad_norm": 7.27692985534668,
      "learning_rate": 4.5167853509664295e-05,
      "loss": 3.4799,
      "step": 9500
    },
    {
      "epoch": 4.837232960325534,
      "grad_norm": 6.795954704284668,
      "learning_rate": 4.516276703967447e-05,
      "loss": 3.3804,
      "step": 9510
    },
    {
      "epoch": 4.842319430315361,
      "grad_norm": 7.342233657836914,
      "learning_rate": 4.515768056968464e-05,
      "loss": 3.471,
      "step": 9520
    },
    {
      "epoch": 4.847405900305188,
      "grad_norm": 5.689978122711182,
      "learning_rate": 4.515259409969481e-05,
      "loss": 3.458,
      "step": 9530
    },
    {
      "epoch": 4.852492370295015,
      "grad_norm": 9.231182098388672,
      "learning_rate": 4.514750762970499e-05,
      "loss": 3.3644,
      "step": 9540
    },
    {
      "epoch": 4.857578840284843,
      "grad_norm": 6.887257099151611,
      "learning_rate": 4.514242115971516e-05,
      "loss": 3.4188,
      "step": 9550
    },
    {
      "epoch": 4.86266531027467,
      "grad_norm": 9.625636100769043,
      "learning_rate": 4.513733468972533e-05,
      "loss": 3.3807,
      "step": 9560
    },
    {
      "epoch": 4.867751780264497,
      "grad_norm": 7.341774940490723,
      "learning_rate": 4.5132248219735504e-05,
      "loss": 3.4628,
      "step": 9570
    },
    {
      "epoch": 4.872838250254324,
      "grad_norm": 9.670441627502441,
      "learning_rate": 4.512716174974568e-05,
      "loss": 3.3221,
      "step": 9580
    },
    {
      "epoch": 4.877924720244151,
      "grad_norm": 8.147854804992676,
      "learning_rate": 4.512207527975586e-05,
      "loss": 3.4089,
      "step": 9590
    },
    {
      "epoch": 4.883011190233978,
      "grad_norm": 11.208895683288574,
      "learning_rate": 4.511698880976603e-05,
      "loss": 3.4554,
      "step": 9600
    },
    {
      "epoch": 4.888097660223805,
      "grad_norm": 7.076759338378906,
      "learning_rate": 4.51119023397762e-05,
      "loss": 3.4768,
      "step": 9610
    },
    {
      "epoch": 4.893184130213632,
      "grad_norm": 7.467263221740723,
      "learning_rate": 4.5106815869786374e-05,
      "loss": 3.4078,
      "step": 9620
    },
    {
      "epoch": 4.898270600203459,
      "grad_norm": 5.993905067443848,
      "learning_rate": 4.5101729399796544e-05,
      "loss": 3.3875,
      "step": 9630
    },
    {
      "epoch": 4.903357070193286,
      "grad_norm": 8.5457124710083,
      "learning_rate": 4.5096642929806714e-05,
      "loss": 3.4183,
      "step": 9640
    },
    {
      "epoch": 4.908443540183113,
      "grad_norm": 8.266600608825684,
      "learning_rate": 4.509155645981689e-05,
      "loss": 3.4442,
      "step": 9650
    },
    {
      "epoch": 4.9135300101729396,
      "grad_norm": 7.88046932220459,
      "learning_rate": 4.508646998982706e-05,
      "loss": 3.3544,
      "step": 9660
    },
    {
      "epoch": 4.918616480162767,
      "grad_norm": 9.515092849731445,
      "learning_rate": 4.508138351983723e-05,
      "loss": 3.5051,
      "step": 9670
    },
    {
      "epoch": 4.923702950152594,
      "grad_norm": 10.328898429870605,
      "learning_rate": 4.507629704984741e-05,
      "loss": 3.3576,
      "step": 9680
    },
    {
      "epoch": 4.928789420142421,
      "grad_norm": 8.191904067993164,
      "learning_rate": 4.5071210579857584e-05,
      "loss": 3.4015,
      "step": 9690
    },
    {
      "epoch": 4.933875890132248,
      "grad_norm": 9.719797134399414,
      "learning_rate": 4.5066124109867754e-05,
      "loss": 3.4232,
      "step": 9700
    },
    {
      "epoch": 4.938962360122075,
      "grad_norm": 8.77003002166748,
      "learning_rate": 4.506103763987793e-05,
      "loss": 3.316,
      "step": 9710
    },
    {
      "epoch": 4.944048830111902,
      "grad_norm": 7.335244178771973,
      "learning_rate": 4.50559511698881e-05,
      "loss": 3.4611,
      "step": 9720
    },
    {
      "epoch": 4.949135300101729,
      "grad_norm": 6.774808883666992,
      "learning_rate": 4.505086469989827e-05,
      "loss": 3.3914,
      "step": 9730
    },
    {
      "epoch": 4.954221770091556,
      "grad_norm": 6.645503044128418,
      "learning_rate": 4.504577822990845e-05,
      "loss": 3.3872,
      "step": 9740
    },
    {
      "epoch": 4.959308240081383,
      "grad_norm": 8.210997581481934,
      "learning_rate": 4.5040691759918617e-05,
      "loss": 3.3891,
      "step": 9750
    },
    {
      "epoch": 4.96439471007121,
      "grad_norm": 7.470862865447998,
      "learning_rate": 4.5035605289928786e-05,
      "loss": 3.4412,
      "step": 9760
    },
    {
      "epoch": 4.969481180061038,
      "grad_norm": 7.658910274505615,
      "learning_rate": 4.503051881993896e-05,
      "loss": 3.4044,
      "step": 9770
    },
    {
      "epoch": 4.974567650050865,
      "grad_norm": 9.376871109008789,
      "learning_rate": 4.502543234994914e-05,
      "loss": 3.2793,
      "step": 9780
    },
    {
      "epoch": 4.979654120040692,
      "grad_norm": 8.251885414123535,
      "learning_rate": 4.502034587995931e-05,
      "loss": 3.398,
      "step": 9790
    },
    {
      "epoch": 4.984740590030519,
      "grad_norm": 7.661507606506348,
      "learning_rate": 4.5015259409969486e-05,
      "loss": 3.3721,
      "step": 9800
    },
    {
      "epoch": 4.989827060020346,
      "grad_norm": 7.2751784324646,
      "learning_rate": 4.5010172939979656e-05,
      "loss": 3.4614,
      "step": 9810
    },
    {
      "epoch": 4.994913530010173,
      "grad_norm": 7.3902459144592285,
      "learning_rate": 4.5005086469989826e-05,
      "loss": 3.4271,
      "step": 9820
    },
    {
      "epoch": 5.0,
      "grad_norm": 13.9072847366333,
      "learning_rate": 4.5e-05,
      "loss": 3.3877,
      "step": 9830
    },
    {
      "epoch": 5.0,
      "eval_loss": 3.6719284057617188,
      "eval_runtime": 2.7431,
      "eval_samples_per_second": 1011.623,
      "eval_steps_per_second": 126.498,
      "step": 9830
    },
    {
      "epoch": 5.005086469989827,
      "grad_norm": 7.9567790031433105,
      "learning_rate": 4.499491353001017e-05,
      "loss": 3.3439,
      "step": 9840
    },
    {
      "epoch": 5.010172939979654,
      "grad_norm": 8.787237167358398,
      "learning_rate": 4.498982706002034e-05,
      "loss": 3.4455,
      "step": 9850
    },
    {
      "epoch": 5.015259409969481,
      "grad_norm": 8.89844036102295,
      "learning_rate": 4.498474059003052e-05,
      "loss": 3.3618,
      "step": 9860
    },
    {
      "epoch": 5.020345879959308,
      "grad_norm": 6.023423671722412,
      "learning_rate": 4.4979654120040696e-05,
      "loss": 3.3615,
      "step": 9870
    },
    {
      "epoch": 5.025432349949135,
      "grad_norm": 8.555155754089355,
      "learning_rate": 4.497456765005087e-05,
      "loss": 3.3757,
      "step": 9880
    },
    {
      "epoch": 5.030518819938963,
      "grad_norm": 8.894885063171387,
      "learning_rate": 4.496948118006104e-05,
      "loss": 3.4623,
      "step": 9890
    },
    {
      "epoch": 5.03560528992879,
      "grad_norm": 9.594351768493652,
      "learning_rate": 4.496439471007121e-05,
      "loss": 3.3697,
      "step": 9900
    },
    {
      "epoch": 5.040691759918617,
      "grad_norm": 14.147628784179688,
      "learning_rate": 4.495930824008139e-05,
      "loss": 3.292,
      "step": 9910
    },
    {
      "epoch": 5.045778229908444,
      "grad_norm": 7.516112327575684,
      "learning_rate": 4.495422177009156e-05,
      "loss": 3.4049,
      "step": 9920
    },
    {
      "epoch": 5.050864699898271,
      "grad_norm": 8.008264541625977,
      "learning_rate": 4.494913530010173e-05,
      "loss": 3.3821,
      "step": 9930
    },
    {
      "epoch": 5.055951169888098,
      "grad_norm": 9.882140159606934,
      "learning_rate": 4.4944048830111906e-05,
      "loss": 3.4062,
      "step": 9940
    },
    {
      "epoch": 5.061037639877925,
      "grad_norm": 13.335467338562012,
      "learning_rate": 4.4938962360122075e-05,
      "loss": 3.3017,
      "step": 9950
    },
    {
      "epoch": 5.066124109867752,
      "grad_norm": 11.549736022949219,
      "learning_rate": 4.493387589013225e-05,
      "loss": 3.4247,
      "step": 9960
    },
    {
      "epoch": 5.071210579857579,
      "grad_norm": 8.189862251281738,
      "learning_rate": 4.492878942014242e-05,
      "loss": 3.3668,
      "step": 9970
    },
    {
      "epoch": 5.076297049847406,
      "grad_norm": 8.225774765014648,
      "learning_rate": 4.49237029501526e-05,
      "loss": 3.3872,
      "step": 9980
    },
    {
      "epoch": 5.081383519837233,
      "grad_norm": 6.003359317779541,
      "learning_rate": 4.491861648016277e-05,
      "loss": 3.4136,
      "step": 9990
    },
    {
      "epoch": 5.0864699898270604,
      "grad_norm": 8.378362655639648,
      "learning_rate": 4.4913530010172945e-05,
      "loss": 3.3772,
      "step": 10000
    },
    {
      "epoch": 5.091556459816887,
      "grad_norm": 9.08553409576416,
      "learning_rate": 4.4908443540183115e-05,
      "loss": 3.4507,
      "step": 10010
    },
    {
      "epoch": 5.096642929806714,
      "grad_norm": 7.368302345275879,
      "learning_rate": 4.4903357070193285e-05,
      "loss": 3.3575,
      "step": 10020
    },
    {
      "epoch": 5.101729399796541,
      "grad_norm": 8.320717811584473,
      "learning_rate": 4.489827060020346e-05,
      "loss": 3.3418,
      "step": 10030
    },
    {
      "epoch": 5.106815869786368,
      "grad_norm": 8.783220291137695,
      "learning_rate": 4.489318413021363e-05,
      "loss": 3.4001,
      "step": 10040
    },
    {
      "epoch": 5.111902339776195,
      "grad_norm": 7.794065952301025,
      "learning_rate": 4.48880976602238e-05,
      "loss": 3.453,
      "step": 10050
    },
    {
      "epoch": 5.116988809766022,
      "grad_norm": 8.192094802856445,
      "learning_rate": 4.488301119023398e-05,
      "loss": 3.3681,
      "step": 10060
    },
    {
      "epoch": 5.122075279755849,
      "grad_norm": 7.488438606262207,
      "learning_rate": 4.4877924720244155e-05,
      "loss": 3.4124,
      "step": 10070
    },
    {
      "epoch": 5.127161749745676,
      "grad_norm": 9.813347816467285,
      "learning_rate": 4.4872838250254325e-05,
      "loss": 3.3748,
      "step": 10080
    },
    {
      "epoch": 5.132248219735503,
      "grad_norm": 7.967130184173584,
      "learning_rate": 4.48677517802645e-05,
      "loss": 3.41,
      "step": 10090
    },
    {
      "epoch": 5.13733468972533,
      "grad_norm": 6.782693386077881,
      "learning_rate": 4.486266531027467e-05,
      "loss": 3.3778,
      "step": 10100
    },
    {
      "epoch": 5.142421159715157,
      "grad_norm": 7.84092378616333,
      "learning_rate": 4.485757884028484e-05,
      "loss": 3.4061,
      "step": 10110
    },
    {
      "epoch": 5.147507629704985,
      "grad_norm": 5.829002380371094,
      "learning_rate": 4.485249237029502e-05,
      "loss": 3.2953,
      "step": 10120
    },
    {
      "epoch": 5.152594099694812,
      "grad_norm": 6.801539897918701,
      "learning_rate": 4.484740590030519e-05,
      "loss": 3.3162,
      "step": 10130
    },
    {
      "epoch": 5.157680569684639,
      "grad_norm": 8.075284004211426,
      "learning_rate": 4.4842319430315364e-05,
      "loss": 3.3222,
      "step": 10140
    },
    {
      "epoch": 5.162767039674466,
      "grad_norm": 9.009367942810059,
      "learning_rate": 4.4837232960325534e-05,
      "loss": 3.3519,
      "step": 10150
    },
    {
      "epoch": 5.167853509664293,
      "grad_norm": 9.120413780212402,
      "learning_rate": 4.483214649033571e-05,
      "loss": 3.3542,
      "step": 10160
    },
    {
      "epoch": 5.17293997965412,
      "grad_norm": 7.851102352142334,
      "learning_rate": 4.482706002034589e-05,
      "loss": 3.4059,
      "step": 10170
    },
    {
      "epoch": 5.178026449643947,
      "grad_norm": 8.88165283203125,
      "learning_rate": 4.482197355035606e-05,
      "loss": 3.344,
      "step": 10180
    },
    {
      "epoch": 5.183112919633774,
      "grad_norm": 8.889517784118652,
      "learning_rate": 4.481688708036623e-05,
      "loss": 3.4032,
      "step": 10190
    },
    {
      "epoch": 5.188199389623601,
      "grad_norm": 8.39539623260498,
      "learning_rate": 4.4811800610376404e-05,
      "loss": 3.3277,
      "step": 10200
    },
    {
      "epoch": 5.193285859613428,
      "grad_norm": 8.665796279907227,
      "learning_rate": 4.4806714140386574e-05,
      "loss": 3.4252,
      "step": 10210
    },
    {
      "epoch": 5.198372329603256,
      "grad_norm": 10.705954551696777,
      "learning_rate": 4.4801627670396744e-05,
      "loss": 3.3138,
      "step": 10220
    },
    {
      "epoch": 5.203458799593083,
      "grad_norm": 10.920796394348145,
      "learning_rate": 4.479654120040692e-05,
      "loss": 3.3161,
      "step": 10230
    },
    {
      "epoch": 5.20854526958291,
      "grad_norm": 9.953409194946289,
      "learning_rate": 4.479145473041709e-05,
      "loss": 3.3515,
      "step": 10240
    },
    {
      "epoch": 5.213631739572737,
      "grad_norm": 9.429926872253418,
      "learning_rate": 4.478636826042727e-05,
      "loss": 3.3914,
      "step": 10250
    },
    {
      "epoch": 5.218718209562564,
      "grad_norm": 10.035264015197754,
      "learning_rate": 4.4781281790437444e-05,
      "loss": 3.3721,
      "step": 10260
    },
    {
      "epoch": 5.223804679552391,
      "grad_norm": 10.723214149475098,
      "learning_rate": 4.4776195320447614e-05,
      "loss": 3.3597,
      "step": 10270
    },
    {
      "epoch": 5.228891149542218,
      "grad_norm": 9.610581398010254,
      "learning_rate": 4.4771108850457784e-05,
      "loss": 3.4669,
      "step": 10280
    },
    {
      "epoch": 5.233977619532045,
      "grad_norm": 9.203573226928711,
      "learning_rate": 4.476602238046796e-05,
      "loss": 3.3818,
      "step": 10290
    },
    {
      "epoch": 5.239064089521872,
      "grad_norm": 9.906189918518066,
      "learning_rate": 4.476093591047813e-05,
      "loss": 3.3805,
      "step": 10300
    },
    {
      "epoch": 5.244150559511699,
      "grad_norm": 7.550098896026611,
      "learning_rate": 4.47558494404883e-05,
      "loss": 3.3852,
      "step": 10310
    },
    {
      "epoch": 5.249237029501526,
      "grad_norm": 8.183079719543457,
      "learning_rate": 4.475076297049848e-05,
      "loss": 3.4084,
      "step": 10320
    },
    {
      "epoch": 5.254323499491353,
      "grad_norm": 10.722709655761719,
      "learning_rate": 4.4745676500508647e-05,
      "loss": 3.3396,
      "step": 10330
    },
    {
      "epoch": 5.2594099694811804,
      "grad_norm": 8.980326652526855,
      "learning_rate": 4.4740590030518816e-05,
      "loss": 3.4066,
      "step": 10340
    },
    {
      "epoch": 5.264496439471007,
      "grad_norm": 9.439671516418457,
      "learning_rate": 4.473550356052899e-05,
      "loss": 3.3253,
      "step": 10350
    },
    {
      "epoch": 5.269582909460834,
      "grad_norm": 5.883491039276123,
      "learning_rate": 4.473041709053917e-05,
      "loss": 3.3322,
      "step": 10360
    },
    {
      "epoch": 5.274669379450661,
      "grad_norm": 7.728847503662109,
      "learning_rate": 4.472533062054934e-05,
      "loss": 3.3758,
      "step": 10370
    },
    {
      "epoch": 5.279755849440488,
      "grad_norm": 7.2566938400268555,
      "learning_rate": 4.4720244150559516e-05,
      "loss": 3.3584,
      "step": 10380
    },
    {
      "epoch": 5.284842319430315,
      "grad_norm": 9.463056564331055,
      "learning_rate": 4.4715157680569686e-05,
      "loss": 3.3334,
      "step": 10390
    },
    {
      "epoch": 5.289928789420142,
      "grad_norm": 9.359456062316895,
      "learning_rate": 4.471007121057986e-05,
      "loss": 3.2811,
      "step": 10400
    },
    {
      "epoch": 5.295015259409969,
      "grad_norm": 8.876591682434082,
      "learning_rate": 4.470498474059003e-05,
      "loss": 3.4604,
      "step": 10410
    },
    {
      "epoch": 5.300101729399796,
      "grad_norm": 8.443236351013184,
      "learning_rate": 4.46998982706002e-05,
      "loss": 3.3761,
      "step": 10420
    },
    {
      "epoch": 5.305188199389623,
      "grad_norm": 7.779739856719971,
      "learning_rate": 4.469481180061038e-05,
      "loss": 3.4225,
      "step": 10430
    },
    {
      "epoch": 5.31027466937945,
      "grad_norm": 8.223272323608398,
      "learning_rate": 4.468972533062055e-05,
      "loss": 3.4115,
      "step": 10440
    },
    {
      "epoch": 5.315361139369278,
      "grad_norm": 10.142083168029785,
      "learning_rate": 4.4684638860630726e-05,
      "loss": 3.452,
      "step": 10450
    },
    {
      "epoch": 5.320447609359105,
      "grad_norm": 9.125947952270508,
      "learning_rate": 4.46795523906409e-05,
      "loss": 3.4257,
      "step": 10460
    },
    {
      "epoch": 5.325534079348932,
      "grad_norm": 7.132338523864746,
      "learning_rate": 4.467446592065107e-05,
      "loss": 3.3429,
      "step": 10470
    },
    {
      "epoch": 5.330620549338759,
      "grad_norm": 7.125019550323486,
      "learning_rate": 4.466937945066124e-05,
      "loss": 3.2622,
      "step": 10480
    },
    {
      "epoch": 5.335707019328586,
      "grad_norm": 11.581009864807129,
      "learning_rate": 4.466429298067142e-05,
      "loss": 3.351,
      "step": 10490
    },
    {
      "epoch": 5.340793489318413,
      "grad_norm": 6.582719326019287,
      "learning_rate": 4.465920651068159e-05,
      "loss": 3.2933,
      "step": 10500
    },
    {
      "epoch": 5.34587995930824,
      "grad_norm": 9.592324256896973,
      "learning_rate": 4.465412004069176e-05,
      "loss": 3.322,
      "step": 10510
    },
    {
      "epoch": 5.350966429298067,
      "grad_norm": 8.43247127532959,
      "learning_rate": 4.4649033570701936e-05,
      "loss": 3.4415,
      "step": 10520
    },
    {
      "epoch": 5.356052899287894,
      "grad_norm": 12.023277282714844,
      "learning_rate": 4.4643947100712105e-05,
      "loss": 3.3526,
      "step": 10530
    },
    {
      "epoch": 5.361139369277721,
      "grad_norm": 9.103813171386719,
      "learning_rate": 4.463886063072228e-05,
      "loss": 3.4279,
      "step": 10540
    },
    {
      "epoch": 5.366225839267548,
      "grad_norm": 6.361832618713379,
      "learning_rate": 4.463377416073246e-05,
      "loss": 3.3832,
      "step": 10550
    },
    {
      "epoch": 5.371312309257376,
      "grad_norm": 10.655050277709961,
      "learning_rate": 4.462868769074263e-05,
      "loss": 3.3426,
      "step": 10560
    },
    {
      "epoch": 5.376398779247203,
      "grad_norm": 10.224148750305176,
      "learning_rate": 4.46236012207528e-05,
      "loss": 3.3462,
      "step": 10570
    },
    {
      "epoch": 5.38148524923703,
      "grad_norm": 11.408224105834961,
      "learning_rate": 4.4618514750762975e-05,
      "loss": 3.2704,
      "step": 10580
    },
    {
      "epoch": 5.386571719226857,
      "grad_norm": 9.483844757080078,
      "learning_rate": 4.4613428280773145e-05,
      "loss": 3.3895,
      "step": 10590
    },
    {
      "epoch": 5.391658189216684,
      "grad_norm": 7.130020618438721,
      "learning_rate": 4.4608341810783315e-05,
      "loss": 3.4029,
      "step": 10600
    },
    {
      "epoch": 5.396744659206511,
      "grad_norm": 7.745676040649414,
      "learning_rate": 4.460325534079349e-05,
      "loss": 3.3698,
      "step": 10610
    },
    {
      "epoch": 5.401831129196338,
      "grad_norm": 10.487146377563477,
      "learning_rate": 4.459816887080366e-05,
      "loss": 3.2955,
      "step": 10620
    },
    {
      "epoch": 5.406917599186165,
      "grad_norm": 9.193432807922363,
      "learning_rate": 4.459308240081383e-05,
      "loss": 3.3925,
      "step": 10630
    },
    {
      "epoch": 5.412004069175992,
      "grad_norm": 8.540753364562988,
      "learning_rate": 4.458799593082401e-05,
      "loss": 3.3679,
      "step": 10640
    },
    {
      "epoch": 5.417090539165819,
      "grad_norm": 10.630615234375,
      "learning_rate": 4.4582909460834185e-05,
      "loss": 3.3183,
      "step": 10650
    },
    {
      "epoch": 5.422177009155646,
      "grad_norm": 8.636068344116211,
      "learning_rate": 4.4577822990844355e-05,
      "loss": 3.3556,
      "step": 10660
    },
    {
      "epoch": 5.4272634791454735,
      "grad_norm": 9.008451461791992,
      "learning_rate": 4.457273652085453e-05,
      "loss": 3.3082,
      "step": 10670
    },
    {
      "epoch": 5.4323499491353004,
      "grad_norm": 7.712475299835205,
      "learning_rate": 4.45676500508647e-05,
      "loss": 3.327,
      "step": 10680
    },
    {
      "epoch": 5.437436419125127,
      "grad_norm": 8.044414520263672,
      "learning_rate": 4.456256358087488e-05,
      "loss": 3.4002,
      "step": 10690
    },
    {
      "epoch": 5.442522889114954,
      "grad_norm": 8.008727073669434,
      "learning_rate": 4.455747711088505e-05,
      "loss": 3.333,
      "step": 10700
    },
    {
      "epoch": 5.447609359104781,
      "grad_norm": 6.735044002532959,
      "learning_rate": 4.455239064089522e-05,
      "loss": 3.3364,
      "step": 10710
    },
    {
      "epoch": 5.452695829094608,
      "grad_norm": 9.523913383483887,
      "learning_rate": 4.4547304170905394e-05,
      "loss": 3.3695,
      "step": 10720
    },
    {
      "epoch": 5.457782299084435,
      "grad_norm": 9.524142265319824,
      "learning_rate": 4.4542217700915564e-05,
      "loss": 3.2907,
      "step": 10730
    },
    {
      "epoch": 5.462868769074262,
      "grad_norm": 9.6463623046875,
      "learning_rate": 4.453713123092574e-05,
      "loss": 3.2932,
      "step": 10740
    },
    {
      "epoch": 5.467955239064089,
      "grad_norm": 9.36671257019043,
      "learning_rate": 4.453204476093592e-05,
      "loss": 3.3517,
      "step": 10750
    },
    {
      "epoch": 5.473041709053916,
      "grad_norm": 9.570650100708008,
      "learning_rate": 4.452695829094609e-05,
      "loss": 3.3564,
      "step": 10760
    },
    {
      "epoch": 5.478128179043743,
      "grad_norm": 10.900553703308105,
      "learning_rate": 4.452187182095626e-05,
      "loss": 3.395,
      "step": 10770
    },
    {
      "epoch": 5.48321464903357,
      "grad_norm": 8.267682075500488,
      "learning_rate": 4.4516785350966434e-05,
      "loss": 3.3565,
      "step": 10780
    },
    {
      "epoch": 5.488301119023398,
      "grad_norm": 6.798012733459473,
      "learning_rate": 4.4511698880976604e-05,
      "loss": 3.3898,
      "step": 10790
    },
    {
      "epoch": 5.493387589013225,
      "grad_norm": 9.496233940124512,
      "learning_rate": 4.4506612410986774e-05,
      "loss": 3.3927,
      "step": 10800
    },
    {
      "epoch": 5.498474059003052,
      "grad_norm": 9.266633987426758,
      "learning_rate": 4.450152594099695e-05,
      "loss": 3.2898,
      "step": 10810
    },
    {
      "epoch": 5.503560528992879,
      "grad_norm": 8.694616317749023,
      "learning_rate": 4.449643947100712e-05,
      "loss": 3.3541,
      "step": 10820
    },
    {
      "epoch": 5.508646998982706,
      "grad_norm": 10.764841079711914,
      "learning_rate": 4.44913530010173e-05,
      "loss": 3.3413,
      "step": 10830
    },
    {
      "epoch": 5.513733468972533,
      "grad_norm": 12.022740364074707,
      "learning_rate": 4.4486266531027474e-05,
      "loss": 3.291,
      "step": 10840
    },
    {
      "epoch": 5.51881993896236,
      "grad_norm": 7.828083038330078,
      "learning_rate": 4.4481180061037644e-05,
      "loss": 3.2283,
      "step": 10850
    },
    {
      "epoch": 5.523906408952187,
      "grad_norm": 9.341636657714844,
      "learning_rate": 4.4476093591047814e-05,
      "loss": 3.3576,
      "step": 10860
    },
    {
      "epoch": 5.528992878942014,
      "grad_norm": 9.401507377624512,
      "learning_rate": 4.447100712105799e-05,
      "loss": 3.3562,
      "step": 10870
    },
    {
      "epoch": 5.534079348931841,
      "grad_norm": 10.16762924194336,
      "learning_rate": 4.446592065106816e-05,
      "loss": 3.2606,
      "step": 10880
    },
    {
      "epoch": 5.539165818921669,
      "grad_norm": 9.584733963012695,
      "learning_rate": 4.446083418107833e-05,
      "loss": 3.3123,
      "step": 10890
    },
    {
      "epoch": 5.544252288911496,
      "grad_norm": 10.775392532348633,
      "learning_rate": 4.445574771108851e-05,
      "loss": 3.3168,
      "step": 10900
    },
    {
      "epoch": 5.549338758901323,
      "grad_norm": 10.052096366882324,
      "learning_rate": 4.4450661241098677e-05,
      "loss": 3.289,
      "step": 10910
    },
    {
      "epoch": 5.55442522889115,
      "grad_norm": 9.882708549499512,
      "learning_rate": 4.444557477110885e-05,
      "loss": 3.2876,
      "step": 10920
    },
    {
      "epoch": 5.559511698880977,
      "grad_norm": 12.903475761413574,
      "learning_rate": 4.444048830111902e-05,
      "loss": 3.2578,
      "step": 10930
    },
    {
      "epoch": 5.564598168870804,
      "grad_norm": 7.134469509124756,
      "learning_rate": 4.44354018311292e-05,
      "loss": 3.2762,
      "step": 10940
    },
    {
      "epoch": 5.569684638860631,
      "grad_norm": 10.457711219787598,
      "learning_rate": 4.4430315361139376e-05,
      "loss": 3.3346,
      "step": 10950
    },
    {
      "epoch": 5.574771108850458,
      "grad_norm": 11.453181266784668,
      "learning_rate": 4.4425228891149546e-05,
      "loss": 3.3907,
      "step": 10960
    },
    {
      "epoch": 5.579857578840285,
      "grad_norm": 8.242410659790039,
      "learning_rate": 4.4420142421159716e-05,
      "loss": 3.3826,
      "step": 10970
    },
    {
      "epoch": 5.584944048830112,
      "grad_norm": 10.822739601135254,
      "learning_rate": 4.441505595116989e-05,
      "loss": 3.3547,
      "step": 10980
    },
    {
      "epoch": 5.590030518819939,
      "grad_norm": 11.870401382446289,
      "learning_rate": 4.440996948118006e-05,
      "loss": 3.3461,
      "step": 10990
    },
    {
      "epoch": 5.595116988809766,
      "grad_norm": 7.754425525665283,
      "learning_rate": 4.440488301119023e-05,
      "loss": 3.3348,
      "step": 11000
    },
    {
      "epoch": 5.6002034587995935,
      "grad_norm": 10.82386302947998,
      "learning_rate": 4.439979654120041e-05,
      "loss": 3.3586,
      "step": 11010
    },
    {
      "epoch": 5.6052899287894205,
      "grad_norm": 8.672690391540527,
      "learning_rate": 4.439471007121058e-05,
      "loss": 3.2891,
      "step": 11020
    },
    {
      "epoch": 5.610376398779247,
      "grad_norm": 10.940115928649902,
      "learning_rate": 4.4389623601220756e-05,
      "loss": 3.3483,
      "step": 11030
    },
    {
      "epoch": 5.615462868769074,
      "grad_norm": 8.015664100646973,
      "learning_rate": 4.438453713123093e-05,
      "loss": 3.4545,
      "step": 11040
    },
    {
      "epoch": 5.620549338758901,
      "grad_norm": 8.609061241149902,
      "learning_rate": 4.43794506612411e-05,
      "loss": 3.3364,
      "step": 11050
    },
    {
      "epoch": 5.625635808748728,
      "grad_norm": 8.95185375213623,
      "learning_rate": 4.437436419125127e-05,
      "loss": 3.3531,
      "step": 11060
    },
    {
      "epoch": 5.630722278738555,
      "grad_norm": 7.477101802825928,
      "learning_rate": 4.436927772126145e-05,
      "loss": 3.3797,
      "step": 11070
    },
    {
      "epoch": 5.635808748728382,
      "grad_norm": 11.06296443939209,
      "learning_rate": 4.436419125127162e-05,
      "loss": 3.2687,
      "step": 11080
    },
    {
      "epoch": 5.640895218718209,
      "grad_norm": 7.0605597496032715,
      "learning_rate": 4.435910478128179e-05,
      "loss": 3.3735,
      "step": 11090
    },
    {
      "epoch": 5.645981688708036,
      "grad_norm": 9.048935890197754,
      "learning_rate": 4.4354018311291966e-05,
      "loss": 3.3826,
      "step": 11100
    },
    {
      "epoch": 5.651068158697864,
      "grad_norm": 9.549798011779785,
      "learning_rate": 4.4348931841302135e-05,
      "loss": 3.3,
      "step": 11110
    },
    {
      "epoch": 5.656154628687691,
      "grad_norm": 8.588309288024902,
      "learning_rate": 4.434384537131231e-05,
      "loss": 3.3629,
      "step": 11120
    },
    {
      "epoch": 5.661241098677518,
      "grad_norm": 8.058106422424316,
      "learning_rate": 4.433875890132249e-05,
      "loss": 3.4014,
      "step": 11130
    },
    {
      "epoch": 5.666327568667345,
      "grad_norm": 10.738755226135254,
      "learning_rate": 4.433367243133266e-05,
      "loss": 3.3039,
      "step": 11140
    },
    {
      "epoch": 5.671414038657172,
      "grad_norm": 10.400997161865234,
      "learning_rate": 4.432858596134283e-05,
      "loss": 3.2921,
      "step": 11150
    },
    {
      "epoch": 5.676500508646999,
      "grad_norm": 8.08258056640625,
      "learning_rate": 4.4323499491353005e-05,
      "loss": 3.2978,
      "step": 11160
    },
    {
      "epoch": 5.681586978636826,
      "grad_norm": 9.636003494262695,
      "learning_rate": 4.4318413021363175e-05,
      "loss": 3.3483,
      "step": 11170
    },
    {
      "epoch": 5.686673448626653,
      "grad_norm": 7.847623348236084,
      "learning_rate": 4.4313326551373345e-05,
      "loss": 3.3171,
      "step": 11180
    },
    {
      "epoch": 5.69175991861648,
      "grad_norm": 9.59643840789795,
      "learning_rate": 4.430824008138352e-05,
      "loss": 3.3215,
      "step": 11190
    },
    {
      "epoch": 5.696846388606307,
      "grad_norm": 9.010138511657715,
      "learning_rate": 4.430315361139369e-05,
      "loss": 3.3469,
      "step": 11200
    },
    {
      "epoch": 5.701932858596134,
      "grad_norm": 10.77082633972168,
      "learning_rate": 4.429806714140387e-05,
      "loss": 3.2313,
      "step": 11210
    },
    {
      "epoch": 5.707019328585961,
      "grad_norm": 8.151232719421387,
      "learning_rate": 4.4292980671414045e-05,
      "loss": 3.3291,
      "step": 11220
    },
    {
      "epoch": 5.712105798575788,
      "grad_norm": 9.16970443725586,
      "learning_rate": 4.4287894201424215e-05,
      "loss": 3.2572,
      "step": 11230
    },
    {
      "epoch": 5.717192268565616,
      "grad_norm": 11.333077430725098,
      "learning_rate": 4.428280773143439e-05,
      "loss": 3.3699,
      "step": 11240
    },
    {
      "epoch": 5.722278738555443,
      "grad_norm": 11.12392807006836,
      "learning_rate": 4.427772126144456e-05,
      "loss": 3.2718,
      "step": 11250
    },
    {
      "epoch": 5.72736520854527,
      "grad_norm": 9.172706604003906,
      "learning_rate": 4.427263479145473e-05,
      "loss": 3.359,
      "step": 11260
    },
    {
      "epoch": 5.732451678535097,
      "grad_norm": 11.577558517456055,
      "learning_rate": 4.426754832146491e-05,
      "loss": 3.2735,
      "step": 11270
    },
    {
      "epoch": 5.737538148524924,
      "grad_norm": 11.478280067443848,
      "learning_rate": 4.426246185147508e-05,
      "loss": 3.3528,
      "step": 11280
    },
    {
      "epoch": 5.742624618514751,
      "grad_norm": 13.171431541442871,
      "learning_rate": 4.425737538148525e-05,
      "loss": 3.2745,
      "step": 11290
    },
    {
      "epoch": 5.747711088504578,
      "grad_norm": 8.938261985778809,
      "learning_rate": 4.4252288911495424e-05,
      "loss": 3.3341,
      "step": 11300
    },
    {
      "epoch": 5.752797558494405,
      "grad_norm": 10.897587776184082,
      "learning_rate": 4.4247202441505594e-05,
      "loss": 3.2312,
      "step": 11310
    },
    {
      "epoch": 5.757884028484232,
      "grad_norm": 10.139737129211426,
      "learning_rate": 4.424211597151577e-05,
      "loss": 3.3423,
      "step": 11320
    },
    {
      "epoch": 5.762970498474059,
      "grad_norm": 10.771903991699219,
      "learning_rate": 4.423702950152595e-05,
      "loss": 3.3103,
      "step": 11330
    },
    {
      "epoch": 5.7680569684638865,
      "grad_norm": 9.907981872558594,
      "learning_rate": 4.423194303153612e-05,
      "loss": 3.3144,
      "step": 11340
    },
    {
      "epoch": 5.7731434384537135,
      "grad_norm": 8.606064796447754,
      "learning_rate": 4.422685656154629e-05,
      "loss": 3.288,
      "step": 11350
    },
    {
      "epoch": 5.7782299084435405,
      "grad_norm": 8.361233711242676,
      "learning_rate": 4.4221770091556464e-05,
      "loss": 3.3832,
      "step": 11360
    },
    {
      "epoch": 5.783316378433367,
      "grad_norm": 8.858541488647461,
      "learning_rate": 4.4216683621566634e-05,
      "loss": 3.2854,
      "step": 11370
    },
    {
      "epoch": 5.788402848423194,
      "grad_norm": 8.017171859741211,
      "learning_rate": 4.4211597151576804e-05,
      "loss": 3.2785,
      "step": 11380
    },
    {
      "epoch": 5.793489318413021,
      "grad_norm": 11.969258308410645,
      "learning_rate": 4.420651068158698e-05,
      "loss": 3.2457,
      "step": 11390
    },
    {
      "epoch": 5.798575788402848,
      "grad_norm": 10.202942848205566,
      "learning_rate": 4.420142421159715e-05,
      "loss": 3.3004,
      "step": 11400
    },
    {
      "epoch": 5.803662258392675,
      "grad_norm": 10.957554817199707,
      "learning_rate": 4.419633774160733e-05,
      "loss": 3.2722,
      "step": 11410
    },
    {
      "epoch": 5.808748728382502,
      "grad_norm": 8.270360946655273,
      "learning_rate": 4.4191251271617504e-05,
      "loss": 3.2521,
      "step": 11420
    },
    {
      "epoch": 5.813835198372329,
      "grad_norm": 8.591361999511719,
      "learning_rate": 4.4186164801627674e-05,
      "loss": 3.3187,
      "step": 11430
    },
    {
      "epoch": 5.818921668362156,
      "grad_norm": 13.481123924255371,
      "learning_rate": 4.4181078331637844e-05,
      "loss": 3.305,
      "step": 11440
    },
    {
      "epoch": 5.824008138351983,
      "grad_norm": 14.472496032714844,
      "learning_rate": 4.417599186164802e-05,
      "loss": 3.2616,
      "step": 11450
    },
    {
      "epoch": 5.829094608341811,
      "grad_norm": 9.587061882019043,
      "learning_rate": 4.417090539165819e-05,
      "loss": 3.3253,
      "step": 11460
    },
    {
      "epoch": 5.834181078331638,
      "grad_norm": 12.283282279968262,
      "learning_rate": 4.416581892166836e-05,
      "loss": 3.2938,
      "step": 11470
    },
    {
      "epoch": 5.839267548321465,
      "grad_norm": 9.572628021240234,
      "learning_rate": 4.416073245167854e-05,
      "loss": 3.2496,
      "step": 11480
    },
    {
      "epoch": 5.844354018311292,
      "grad_norm": 9.23408031463623,
      "learning_rate": 4.4155645981688707e-05,
      "loss": 3.3274,
      "step": 11490
    },
    {
      "epoch": 5.849440488301119,
      "grad_norm": 14.417545318603516,
      "learning_rate": 4.415055951169888e-05,
      "loss": 3.2937,
      "step": 11500
    },
    {
      "epoch": 5.854526958290946,
      "grad_norm": 8.926292419433594,
      "learning_rate": 4.414547304170906e-05,
      "loss": 3.294,
      "step": 11510
    },
    {
      "epoch": 5.859613428280773,
      "grad_norm": 12.539822578430176,
      "learning_rate": 4.414038657171923e-05,
      "loss": 3.2641,
      "step": 11520
    },
    {
      "epoch": 5.8646998982706,
      "grad_norm": 11.359343528747559,
      "learning_rate": 4.4135300101729407e-05,
      "loss": 3.2704,
      "step": 11530
    },
    {
      "epoch": 5.869786368260427,
      "grad_norm": 8.480549812316895,
      "learning_rate": 4.4130213631739576e-05,
      "loss": 3.3313,
      "step": 11540
    },
    {
      "epoch": 5.874872838250254,
      "grad_norm": 9.010926246643066,
      "learning_rate": 4.4125127161749746e-05,
      "loss": 3.3384,
      "step": 11550
    },
    {
      "epoch": 5.879959308240082,
      "grad_norm": 9.482748985290527,
      "learning_rate": 4.412004069175992e-05,
      "loss": 3.329,
      "step": 11560
    },
    {
      "epoch": 5.885045778229909,
      "grad_norm": 8.854823112487793,
      "learning_rate": 4.411495422177009e-05,
      "loss": 3.2712,
      "step": 11570
    },
    {
      "epoch": 5.890132248219736,
      "grad_norm": 11.604650497436523,
      "learning_rate": 4.410986775178026e-05,
      "loss": 3.3553,
      "step": 11580
    },
    {
      "epoch": 5.895218718209563,
      "grad_norm": 12.020246505737305,
      "learning_rate": 4.410478128179044e-05,
      "loss": 3.321,
      "step": 11590
    },
    {
      "epoch": 5.90030518819939,
      "grad_norm": 13.222394943237305,
      "learning_rate": 4.409969481180061e-05,
      "loss": 3.2785,
      "step": 11600
    },
    {
      "epoch": 5.905391658189217,
      "grad_norm": 8.09487247467041,
      "learning_rate": 4.4094608341810786e-05,
      "loss": 3.3072,
      "step": 11610
    },
    {
      "epoch": 5.910478128179044,
      "grad_norm": 10.938811302185059,
      "learning_rate": 4.408952187182096e-05,
      "loss": 3.2016,
      "step": 11620
    },
    {
      "epoch": 5.915564598168871,
      "grad_norm": 10.162993431091309,
      "learning_rate": 4.408443540183113e-05,
      "loss": 3.2129,
      "step": 11630
    },
    {
      "epoch": 5.920651068158698,
      "grad_norm": 9.318120002746582,
      "learning_rate": 4.40793489318413e-05,
      "loss": 3.1827,
      "step": 11640
    },
    {
      "epoch": 5.925737538148525,
      "grad_norm": 13.590348243713379,
      "learning_rate": 4.407426246185148e-05,
      "loss": 3.2526,
      "step": 11650
    },
    {
      "epoch": 5.930824008138352,
      "grad_norm": 8.646023750305176,
      "learning_rate": 4.406917599186165e-05,
      "loss": 3.1699,
      "step": 11660
    },
    {
      "epoch": 5.935910478128179,
      "grad_norm": 9.792922019958496,
      "learning_rate": 4.406408952187182e-05,
      "loss": 3.2753,
      "step": 11670
    },
    {
      "epoch": 5.940996948118006,
      "grad_norm": 10.818516731262207,
      "learning_rate": 4.4059003051881996e-05,
      "loss": 3.3989,
      "step": 11680
    },
    {
      "epoch": 5.9460834181078335,
      "grad_norm": 8.641620635986328,
      "learning_rate": 4.4053916581892165e-05,
      "loss": 3.3195,
      "step": 11690
    },
    {
      "epoch": 5.9511698880976605,
      "grad_norm": 10.680644035339355,
      "learning_rate": 4.404883011190234e-05,
      "loss": 3.2914,
      "step": 11700
    },
    {
      "epoch": 5.956256358087487,
      "grad_norm": 10.870576858520508,
      "learning_rate": 4.404374364191252e-05,
      "loss": 3.2912,
      "step": 11710
    },
    {
      "epoch": 5.961342828077314,
      "grad_norm": 9.983887672424316,
      "learning_rate": 4.403865717192269e-05,
      "loss": 3.2835,
      "step": 11720
    },
    {
      "epoch": 5.966429298067141,
      "grad_norm": 12.87192440032959,
      "learning_rate": 4.403357070193286e-05,
      "loss": 3.2574,
      "step": 11730
    },
    {
      "epoch": 5.971515768056968,
      "grad_norm": 8.533270835876465,
      "learning_rate": 4.4028484231943035e-05,
      "loss": 3.2379,
      "step": 11740
    },
    {
      "epoch": 5.976602238046795,
      "grad_norm": 9.024237632751465,
      "learning_rate": 4.4023397761953205e-05,
      "loss": 3.3316,
      "step": 11750
    },
    {
      "epoch": 5.981688708036622,
      "grad_norm": 10.567879676818848,
      "learning_rate": 4.401831129196338e-05,
      "loss": 3.2574,
      "step": 11760
    },
    {
      "epoch": 5.986775178026449,
      "grad_norm": 10.429017066955566,
      "learning_rate": 4.401322482197355e-05,
      "loss": 3.222,
      "step": 11770
    },
    {
      "epoch": 5.991861648016277,
      "grad_norm": 10.473714828491211,
      "learning_rate": 4.400813835198372e-05,
      "loss": 3.288,
      "step": 11780
    },
    {
      "epoch": 5.996948118006104,
      "grad_norm": 7.6641693115234375,
      "learning_rate": 4.40030518819939e-05,
      "loss": 3.2724,
      "step": 11790
    },
    {
      "epoch": 6.0,
      "eval_loss": 3.6637704372406006,
      "eval_runtime": 2.6533,
      "eval_samples_per_second": 1045.881,
      "eval_steps_per_second": 130.782,
      "step": 11796
    },
    {
      "epoch": 6.002034587995931,
      "grad_norm": 10.321154594421387,
      "learning_rate": 4.3997965412004075e-05,
      "loss": 3.2956,
      "step": 11800
    },
    {
      "epoch": 6.007121057985758,
      "grad_norm": 11.66472053527832,
      "learning_rate": 4.3992878942014245e-05,
      "loss": 3.3096,
      "step": 11810
    },
    {
      "epoch": 6.012207527975585,
      "grad_norm": 10.45529556274414,
      "learning_rate": 4.398779247202442e-05,
      "loss": 3.2932,
      "step": 11820
    },
    {
      "epoch": 6.017293997965412,
      "grad_norm": 10.264235496520996,
      "learning_rate": 4.398270600203459e-05,
      "loss": 3.2458,
      "step": 11830
    },
    {
      "epoch": 6.022380467955239,
      "grad_norm": 14.35147762298584,
      "learning_rate": 4.397761953204476e-05,
      "loss": 3.2788,
      "step": 11840
    },
    {
      "epoch": 6.027466937945066,
      "grad_norm": 15.582086563110352,
      "learning_rate": 4.397253306205494e-05,
      "loss": 3.2331,
      "step": 11850
    },
    {
      "epoch": 6.032553407934893,
      "grad_norm": 11.903886795043945,
      "learning_rate": 4.396744659206511e-05,
      "loss": 3.2726,
      "step": 11860
    },
    {
      "epoch": 6.03763987792472,
      "grad_norm": 9.484766960144043,
      "learning_rate": 4.396236012207528e-05,
      "loss": 3.272,
      "step": 11870
    },
    {
      "epoch": 6.042726347914547,
      "grad_norm": 10.379478454589844,
      "learning_rate": 4.3957273652085454e-05,
      "loss": 3.2954,
      "step": 11880
    },
    {
      "epoch": 6.047812817904374,
      "grad_norm": 10.991541862487793,
      "learning_rate": 4.3952187182095624e-05,
      "loss": 3.2578,
      "step": 11890
    },
    {
      "epoch": 6.052899287894202,
      "grad_norm": 8.954492568969727,
      "learning_rate": 4.39471007121058e-05,
      "loss": 3.2568,
      "step": 11900
    },
    {
      "epoch": 6.057985757884029,
      "grad_norm": 9.687455177307129,
      "learning_rate": 4.394201424211598e-05,
      "loss": 3.2565,
      "step": 11910
    },
    {
      "epoch": 6.063072227873856,
      "grad_norm": 14.133340835571289,
      "learning_rate": 4.393692777212615e-05,
      "loss": 3.3482,
      "step": 11920
    },
    {
      "epoch": 6.068158697863683,
      "grad_norm": 10.29002571105957,
      "learning_rate": 4.393184130213632e-05,
      "loss": 3.2378,
      "step": 11930
    },
    {
      "epoch": 6.07324516785351,
      "grad_norm": 8.566374778747559,
      "learning_rate": 4.3926754832146494e-05,
      "loss": 3.2614,
      "step": 11940
    },
    {
      "epoch": 6.078331637843337,
      "grad_norm": 13.991579055786133,
      "learning_rate": 4.3921668362156664e-05,
      "loss": 3.2539,
      "step": 11950
    },
    {
      "epoch": 6.083418107833164,
      "grad_norm": 10.802800178527832,
      "learning_rate": 4.3916581892166834e-05,
      "loss": 3.2131,
      "step": 11960
    },
    {
      "epoch": 6.088504577822991,
      "grad_norm": 10.449579238891602,
      "learning_rate": 4.391149542217701e-05,
      "loss": 3.2507,
      "step": 11970
    },
    {
      "epoch": 6.093591047812818,
      "grad_norm": 9.865748405456543,
      "learning_rate": 4.390640895218718e-05,
      "loss": 3.2515,
      "step": 11980
    },
    {
      "epoch": 6.098677517802645,
      "grad_norm": 11.40145492553711,
      "learning_rate": 4.390132248219736e-05,
      "loss": 3.2907,
      "step": 11990
    },
    {
      "epoch": 6.103763987792472,
      "grad_norm": 9.285175323486328,
      "learning_rate": 4.3896236012207534e-05,
      "loss": 3.2842,
      "step": 12000
    },
    {
      "epoch": 6.1088504577822995,
      "grad_norm": 9.865772247314453,
      "learning_rate": 4.3891149542217704e-05,
      "loss": 3.2518,
      "step": 12010
    },
    {
      "epoch": 6.1139369277721265,
      "grad_norm": 8.89970588684082,
      "learning_rate": 4.388606307222788e-05,
      "loss": 3.2889,
      "step": 12020
    },
    {
      "epoch": 6.1190233977619535,
      "grad_norm": 11.525833129882812,
      "learning_rate": 4.388097660223805e-05,
      "loss": 3.2806,
      "step": 12030
    },
    {
      "epoch": 6.1241098677517805,
      "grad_norm": 12.991072654724121,
      "learning_rate": 4.387589013224822e-05,
      "loss": 3.3201,
      "step": 12040
    },
    {
      "epoch": 6.129196337741607,
      "grad_norm": 12.112812995910645,
      "learning_rate": 4.38708036622584e-05,
      "loss": 3.3306,
      "step": 12050
    },
    {
      "epoch": 6.134282807731434,
      "grad_norm": 11.45815372467041,
      "learning_rate": 4.386571719226857e-05,
      "loss": 3.2541,
      "step": 12060
    },
    {
      "epoch": 6.139369277721261,
      "grad_norm": 8.017121315002441,
      "learning_rate": 4.3860630722278737e-05,
      "loss": 3.277,
      "step": 12070
    },
    {
      "epoch": 6.144455747711088,
      "grad_norm": 9.718277931213379,
      "learning_rate": 4.385554425228891e-05,
      "loss": 3.1811,
      "step": 12080
    },
    {
      "epoch": 6.149542217700915,
      "grad_norm": 13.993668556213379,
      "learning_rate": 4.385045778229909e-05,
      "loss": 3.2282,
      "step": 12090
    },
    {
      "epoch": 6.154628687690742,
      "grad_norm": 11.539910316467285,
      "learning_rate": 4.384537131230926e-05,
      "loss": 3.2476,
      "step": 12100
    },
    {
      "epoch": 6.159715157680569,
      "grad_norm": 11.749434471130371,
      "learning_rate": 4.3840284842319437e-05,
      "loss": 3.213,
      "step": 12110
    },
    {
      "epoch": 6.164801627670396,
      "grad_norm": 9.77609920501709,
      "learning_rate": 4.3835198372329606e-05,
      "loss": 3.2155,
      "step": 12120
    },
    {
      "epoch": 6.169888097660224,
      "grad_norm": 9.506766319274902,
      "learning_rate": 4.3830111902339776e-05,
      "loss": 3.2272,
      "step": 12130
    },
    {
      "epoch": 6.174974567650051,
      "grad_norm": 11.262831687927246,
      "learning_rate": 4.382502543234995e-05,
      "loss": 3.2988,
      "step": 12140
    },
    {
      "epoch": 6.180061037639878,
      "grad_norm": 11.318602561950684,
      "learning_rate": 4.381993896236012e-05,
      "loss": 3.2822,
      "step": 12150
    },
    {
      "epoch": 6.185147507629705,
      "grad_norm": 11.836808204650879,
      "learning_rate": 4.381485249237029e-05,
      "loss": 3.2697,
      "step": 12160
    },
    {
      "epoch": 6.190233977619532,
      "grad_norm": 11.729042053222656,
      "learning_rate": 4.380976602238047e-05,
      "loss": 3.2733,
      "step": 12170
    },
    {
      "epoch": 6.195320447609359,
      "grad_norm": 12.708338737487793,
      "learning_rate": 4.3804679552390646e-05,
      "loss": 3.1653,
      "step": 12180
    },
    {
      "epoch": 6.200406917599186,
      "grad_norm": 8.255293846130371,
      "learning_rate": 4.3799593082400816e-05,
      "loss": 3.2832,
      "step": 12190
    },
    {
      "epoch": 6.205493387589013,
      "grad_norm": 10.413613319396973,
      "learning_rate": 4.379450661241099e-05,
      "loss": 3.2147,
      "step": 12200
    },
    {
      "epoch": 6.21057985757884,
      "grad_norm": 9.32861614227295,
      "learning_rate": 4.378942014242116e-05,
      "loss": 3.3121,
      "step": 12210
    },
    {
      "epoch": 6.215666327568667,
      "grad_norm": 7.6213274002075195,
      "learning_rate": 4.378433367243133e-05,
      "loss": 3.3546,
      "step": 12220
    },
    {
      "epoch": 6.220752797558495,
      "grad_norm": 12.637046813964844,
      "learning_rate": 4.377924720244151e-05,
      "loss": 3.2685,
      "step": 12230
    },
    {
      "epoch": 6.225839267548322,
      "grad_norm": 9.392398834228516,
      "learning_rate": 4.377416073245168e-05,
      "loss": 3.2708,
      "step": 12240
    },
    {
      "epoch": 6.230925737538149,
      "grad_norm": 12.580951690673828,
      "learning_rate": 4.376907426246185e-05,
      "loss": 3.3171,
      "step": 12250
    },
    {
      "epoch": 6.236012207527976,
      "grad_norm": 11.83179759979248,
      "learning_rate": 4.3763987792472026e-05,
      "loss": 3.2393,
      "step": 12260
    },
    {
      "epoch": 6.241098677517803,
      "grad_norm": 14.688858032226562,
      "learning_rate": 4.3758901322482195e-05,
      "loss": 3.2679,
      "step": 12270
    },
    {
      "epoch": 6.24618514750763,
      "grad_norm": 9.04433822631836,
      "learning_rate": 4.375381485249237e-05,
      "loss": 3.1769,
      "step": 12280
    },
    {
      "epoch": 6.251271617497457,
      "grad_norm": 17.40118980407715,
      "learning_rate": 4.374872838250255e-05,
      "loss": 3.1394,
      "step": 12290
    },
    {
      "epoch": 6.256358087487284,
      "grad_norm": 9.697709083557129,
      "learning_rate": 4.374364191251272e-05,
      "loss": 3.2937,
      "step": 12300
    },
    {
      "epoch": 6.261444557477111,
      "grad_norm": 8.016406059265137,
      "learning_rate": 4.3738555442522895e-05,
      "loss": 3.2296,
      "step": 12310
    },
    {
      "epoch": 6.266531027466938,
      "grad_norm": 9.189093589782715,
      "learning_rate": 4.3733468972533065e-05,
      "loss": 3.2485,
      "step": 12320
    },
    {
      "epoch": 6.271617497456765,
      "grad_norm": 11.598220825195312,
      "learning_rate": 4.3728382502543235e-05,
      "loss": 3.2793,
      "step": 12330
    },
    {
      "epoch": 6.276703967446592,
      "grad_norm": 9.918442726135254,
      "learning_rate": 4.372329603255341e-05,
      "loss": 3.2943,
      "step": 12340
    },
    {
      "epoch": 6.2817904374364195,
      "grad_norm": 9.345632553100586,
      "learning_rate": 4.371820956256358e-05,
      "loss": 3.2528,
      "step": 12350
    },
    {
      "epoch": 6.2868769074262465,
      "grad_norm": 10.641983032226562,
      "learning_rate": 4.371312309257375e-05,
      "loss": 3.2424,
      "step": 12360
    },
    {
      "epoch": 6.2919633774160735,
      "grad_norm": 11.368219375610352,
      "learning_rate": 4.370803662258393e-05,
      "loss": 3.2857,
      "step": 12370
    },
    {
      "epoch": 6.2970498474059005,
      "grad_norm": 11.845643043518066,
      "learning_rate": 4.3702950152594105e-05,
      "loss": 3.2977,
      "step": 12380
    },
    {
      "epoch": 6.302136317395727,
      "grad_norm": 9.470362663269043,
      "learning_rate": 4.3697863682604275e-05,
      "loss": 3.2194,
      "step": 12390
    },
    {
      "epoch": 6.307222787385554,
      "grad_norm": 12.201417922973633,
      "learning_rate": 4.369277721261445e-05,
      "loss": 3.2114,
      "step": 12400
    },
    {
      "epoch": 6.312309257375381,
      "grad_norm": 10.631743431091309,
      "learning_rate": 4.368769074262462e-05,
      "loss": 3.3156,
      "step": 12410
    },
    {
      "epoch": 6.317395727365208,
      "grad_norm": 9.965656280517578,
      "learning_rate": 4.368260427263479e-05,
      "loss": 3.2043,
      "step": 12420
    },
    {
      "epoch": 6.322482197355035,
      "grad_norm": 10.944114685058594,
      "learning_rate": 4.367751780264497e-05,
      "loss": 3.2998,
      "step": 12430
    },
    {
      "epoch": 6.327568667344862,
      "grad_norm": 9.99160385131836,
      "learning_rate": 4.367243133265514e-05,
      "loss": 3.2468,
      "step": 12440
    },
    {
      "epoch": 6.332655137334689,
      "grad_norm": 13.582206726074219,
      "learning_rate": 4.366734486266531e-05,
      "loss": 3.2438,
      "step": 12450
    },
    {
      "epoch": 6.337741607324517,
      "grad_norm": 9.67580795288086,
      "learning_rate": 4.3662258392675484e-05,
      "loss": 3.1804,
      "step": 12460
    },
    {
      "epoch": 6.342828077314344,
      "grad_norm": 9.666092872619629,
      "learning_rate": 4.365717192268566e-05,
      "loss": 3.2117,
      "step": 12470
    },
    {
      "epoch": 6.347914547304171,
      "grad_norm": 9.113028526306152,
      "learning_rate": 4.365208545269583e-05,
      "loss": 3.2186,
      "step": 12480
    },
    {
      "epoch": 6.353001017293998,
      "grad_norm": 10.728819847106934,
      "learning_rate": 4.364699898270601e-05,
      "loss": 3.2088,
      "step": 12490
    },
    {
      "epoch": 6.358087487283825,
      "grad_norm": 9.439756393432617,
      "learning_rate": 4.364191251271618e-05,
      "loss": 3.2442,
      "step": 12500
    },
    {
      "epoch": 6.363173957273652,
      "grad_norm": 11.986948013305664,
      "learning_rate": 4.363682604272635e-05,
      "loss": 3.2328,
      "step": 12510
    },
    {
      "epoch": 6.368260427263479,
      "grad_norm": 11.900724411010742,
      "learning_rate": 4.3631739572736524e-05,
      "loss": 3.3693,
      "step": 12520
    },
    {
      "epoch": 6.373346897253306,
      "grad_norm": 9.327332496643066,
      "learning_rate": 4.3626653102746694e-05,
      "loss": 3.1949,
      "step": 12530
    },
    {
      "epoch": 6.378433367243133,
      "grad_norm": 12.742366790771484,
      "learning_rate": 4.3621566632756864e-05,
      "loss": 3.1994,
      "step": 12540
    },
    {
      "epoch": 6.38351983723296,
      "grad_norm": 11.644450187683105,
      "learning_rate": 4.361648016276704e-05,
      "loss": 3.2054,
      "step": 12550
    },
    {
      "epoch": 6.388606307222787,
      "grad_norm": 12.152549743652344,
      "learning_rate": 4.361139369277721e-05,
      "loss": 3.2915,
      "step": 12560
    },
    {
      "epoch": 6.393692777212614,
      "grad_norm": 13.813855171203613,
      "learning_rate": 4.360630722278739e-05,
      "loss": 3.2399,
      "step": 12570
    },
    {
      "epoch": 6.398779247202442,
      "grad_norm": 10.363560676574707,
      "learning_rate": 4.3601220752797564e-05,
      "loss": 3.2051,
      "step": 12580
    },
    {
      "epoch": 6.403865717192269,
      "grad_norm": 14.333500862121582,
      "learning_rate": 4.3596134282807734e-05,
      "loss": 3.2273,
      "step": 12590
    },
    {
      "epoch": 6.408952187182096,
      "grad_norm": 10.041943550109863,
      "learning_rate": 4.359104781281791e-05,
      "loss": 3.1964,
      "step": 12600
    },
    {
      "epoch": 6.414038657171923,
      "grad_norm": 10.672920227050781,
      "learning_rate": 4.358596134282808e-05,
      "loss": 3.185,
      "step": 12610
    },
    {
      "epoch": 6.41912512716175,
      "grad_norm": 12.653586387634277,
      "learning_rate": 4.358087487283825e-05,
      "loss": 3.2546,
      "step": 12620
    },
    {
      "epoch": 6.424211597151577,
      "grad_norm": 10.195255279541016,
      "learning_rate": 4.357578840284843e-05,
      "loss": 3.2866,
      "step": 12630
    },
    {
      "epoch": 6.429298067141404,
      "grad_norm": 15.661551475524902,
      "learning_rate": 4.35707019328586e-05,
      "loss": 3.2556,
      "step": 12640
    },
    {
      "epoch": 6.434384537131231,
      "grad_norm": 10.90810775756836,
      "learning_rate": 4.356561546286877e-05,
      "loss": 3.2203,
      "step": 12650
    },
    {
      "epoch": 6.439471007121058,
      "grad_norm": 11.408138275146484,
      "learning_rate": 4.356052899287894e-05,
      "loss": 3.2925,
      "step": 12660
    },
    {
      "epoch": 6.444557477110885,
      "grad_norm": 12.793128967285156,
      "learning_rate": 4.355544252288912e-05,
      "loss": 3.296,
      "step": 12670
    },
    {
      "epoch": 6.4496439471007125,
      "grad_norm": 10.968443870544434,
      "learning_rate": 4.355035605289929e-05,
      "loss": 3.2581,
      "step": 12680
    },
    {
      "epoch": 6.4547304170905395,
      "grad_norm": 10.757184028625488,
      "learning_rate": 4.3545269582909467e-05,
      "loss": 3.3232,
      "step": 12690
    },
    {
      "epoch": 6.4598168870803665,
      "grad_norm": 8.678820610046387,
      "learning_rate": 4.3540183112919636e-05,
      "loss": 3.2757,
      "step": 12700
    },
    {
      "epoch": 6.4649033570701935,
      "grad_norm": 14.799894332885742,
      "learning_rate": 4.3535096642929806e-05,
      "loss": 3.2901,
      "step": 12710
    },
    {
      "epoch": 6.4699898270600205,
      "grad_norm": 10.032944679260254,
      "learning_rate": 4.353001017293998e-05,
      "loss": 3.2963,
      "step": 12720
    },
    {
      "epoch": 6.475076297049847,
      "grad_norm": 13.40778923034668,
      "learning_rate": 4.352492370295015e-05,
      "loss": 3.2068,
      "step": 12730
    },
    {
      "epoch": 6.480162767039674,
      "grad_norm": 9.726919174194336,
      "learning_rate": 4.351983723296032e-05,
      "loss": 3.2773,
      "step": 12740
    },
    {
      "epoch": 6.485249237029501,
      "grad_norm": 13.39201545715332,
      "learning_rate": 4.35147507629705e-05,
      "loss": 3.2166,
      "step": 12750
    },
    {
      "epoch": 6.490335707019328,
      "grad_norm": 9.157268524169922,
      "learning_rate": 4.3509664292980676e-05,
      "loss": 3.2764,
      "step": 12760
    },
    {
      "epoch": 6.495422177009155,
      "grad_norm": 11.771903991699219,
      "learning_rate": 4.3504577822990846e-05,
      "loss": 3.2539,
      "step": 12770
    },
    {
      "epoch": 6.500508646998982,
      "grad_norm": 13.204174995422363,
      "learning_rate": 4.349949135300102e-05,
      "loss": 3.2557,
      "step": 12780
    },
    {
      "epoch": 6.505595116988809,
      "grad_norm": 13.495856285095215,
      "learning_rate": 4.349440488301119e-05,
      "loss": 3.236,
      "step": 12790
    },
    {
      "epoch": 6.510681586978637,
      "grad_norm": 10.073770523071289,
      "learning_rate": 4.348931841302136e-05,
      "loss": 3.2274,
      "step": 12800
    },
    {
      "epoch": 6.515768056968464,
      "grad_norm": 12.489601135253906,
      "learning_rate": 4.348423194303154e-05,
      "loss": 3.2227,
      "step": 12810
    },
    {
      "epoch": 6.520854526958291,
      "grad_norm": 10.239514350891113,
      "learning_rate": 4.347914547304171e-05,
      "loss": 3.2058,
      "step": 12820
    },
    {
      "epoch": 6.525940996948118,
      "grad_norm": 10.744050979614258,
      "learning_rate": 4.3474059003051886e-05,
      "loss": 3.2196,
      "step": 12830
    },
    {
      "epoch": 6.531027466937945,
      "grad_norm": 13.954591751098633,
      "learning_rate": 4.3468972533062056e-05,
      "loss": 3.2675,
      "step": 12840
    },
    {
      "epoch": 6.536113936927772,
      "grad_norm": 12.813207626342773,
      "learning_rate": 4.346388606307223e-05,
      "loss": 3.059,
      "step": 12850
    },
    {
      "epoch": 6.541200406917599,
      "grad_norm": 10.649134635925293,
      "learning_rate": 4.34587995930824e-05,
      "loss": 3.2497,
      "step": 12860
    },
    {
      "epoch": 6.546286876907426,
      "grad_norm": 12.22921371459961,
      "learning_rate": 4.345371312309258e-05,
      "loss": 3.1965,
      "step": 12870
    },
    {
      "epoch": 6.551373346897253,
      "grad_norm": 13.119014739990234,
      "learning_rate": 4.344862665310275e-05,
      "loss": 3.2605,
      "step": 12880
    },
    {
      "epoch": 6.55645981688708,
      "grad_norm": 9.97067928314209,
      "learning_rate": 4.3443540183112925e-05,
      "loss": 3.1715,
      "step": 12890
    },
    {
      "epoch": 6.561546286876908,
      "grad_norm": 10.690508842468262,
      "learning_rate": 4.3438453713123095e-05,
      "loss": 3.2088,
      "step": 12900
    },
    {
      "epoch": 6.566632756866735,
      "grad_norm": 10.881477355957031,
      "learning_rate": 4.3433367243133265e-05,
      "loss": 3.2244,
      "step": 12910
    },
    {
      "epoch": 6.571719226856562,
      "grad_norm": 11.270980834960938,
      "learning_rate": 4.342828077314344e-05,
      "loss": 3.163,
      "step": 12920
    },
    {
      "epoch": 6.576805696846389,
      "grad_norm": 13.24986743927002,
      "learning_rate": 4.342319430315361e-05,
      "loss": 3.175,
      "step": 12930
    },
    {
      "epoch": 6.581892166836216,
      "grad_norm": 8.316305160522461,
      "learning_rate": 4.341810783316378e-05,
      "loss": 3.2499,
      "step": 12940
    },
    {
      "epoch": 6.586978636826043,
      "grad_norm": 12.652242660522461,
      "learning_rate": 4.341302136317396e-05,
      "loss": 3.2104,
      "step": 12950
    },
    {
      "epoch": 6.59206510681587,
      "grad_norm": 11.31636905670166,
      "learning_rate": 4.3407934893184135e-05,
      "loss": 3.1912,
      "step": 12960
    },
    {
      "epoch": 6.597151576805697,
      "grad_norm": 14.508254051208496,
      "learning_rate": 4.3402848423194305e-05,
      "loss": 3.2508,
      "step": 12970
    },
    {
      "epoch": 6.602238046795524,
      "grad_norm": 12.235015869140625,
      "learning_rate": 4.339776195320448e-05,
      "loss": 3.2799,
      "step": 12980
    },
    {
      "epoch": 6.607324516785351,
      "grad_norm": 12.649606704711914,
      "learning_rate": 4.339267548321465e-05,
      "loss": 3.1434,
      "step": 12990
    },
    {
      "epoch": 6.612410986775178,
      "grad_norm": 11.523170471191406,
      "learning_rate": 4.338758901322482e-05,
      "loss": 3.194,
      "step": 13000
    },
    {
      "epoch": 6.617497456765005,
      "grad_norm": 8.370020866394043,
      "learning_rate": 4.3382502543235e-05,
      "loss": 3.2238,
      "step": 13010
    },
    {
      "epoch": 6.622583926754832,
      "grad_norm": 11.630651473999023,
      "learning_rate": 4.337741607324517e-05,
      "loss": 3.2333,
      "step": 13020
    },
    {
      "epoch": 6.6276703967446595,
      "grad_norm": 10.014694213867188,
      "learning_rate": 4.337232960325534e-05,
      "loss": 3.16,
      "step": 13030
    },
    {
      "epoch": 6.6327568667344865,
      "grad_norm": 14.04801082611084,
      "learning_rate": 4.3367243133265514e-05,
      "loss": 3.1436,
      "step": 13040
    },
    {
      "epoch": 6.6378433367243135,
      "grad_norm": 11.986512184143066,
      "learning_rate": 4.336215666327569e-05,
      "loss": 3.2103,
      "step": 13050
    },
    {
      "epoch": 6.6429298067141405,
      "grad_norm": 10.435846328735352,
      "learning_rate": 4.335707019328586e-05,
      "loss": 3.2419,
      "step": 13060
    },
    {
      "epoch": 6.648016276703967,
      "grad_norm": 10.736136436462402,
      "learning_rate": 4.335198372329604e-05,
      "loss": 3.1607,
      "step": 13070
    },
    {
      "epoch": 6.653102746693794,
      "grad_norm": 12.121850967407227,
      "learning_rate": 4.334689725330621e-05,
      "loss": 3.1977,
      "step": 13080
    },
    {
      "epoch": 6.658189216683621,
      "grad_norm": 12.989575386047363,
      "learning_rate": 4.334181078331638e-05,
      "loss": 3.2144,
      "step": 13090
    },
    {
      "epoch": 6.663275686673448,
      "grad_norm": 11.619843482971191,
      "learning_rate": 4.3336724313326554e-05,
      "loss": 3.2204,
      "step": 13100
    },
    {
      "epoch": 6.668362156663275,
      "grad_norm": 11.178447723388672,
      "learning_rate": 4.3331637843336724e-05,
      "loss": 3.2746,
      "step": 13110
    },
    {
      "epoch": 6.673448626653103,
      "grad_norm": 12.512187957763672,
      "learning_rate": 4.33265513733469e-05,
      "loss": 3.1951,
      "step": 13120
    },
    {
      "epoch": 6.67853509664293,
      "grad_norm": 11.354557991027832,
      "learning_rate": 4.332146490335707e-05,
      "loss": 3.252,
      "step": 13130
    },
    {
      "epoch": 6.683621566632757,
      "grad_norm": 10.032448768615723,
      "learning_rate": 4.331637843336725e-05,
      "loss": 3.2204,
      "step": 13140
    },
    {
      "epoch": 6.688708036622584,
      "grad_norm": 11.649613380432129,
      "learning_rate": 4.3311291963377424e-05,
      "loss": 3.2595,
      "step": 13150
    },
    {
      "epoch": 6.693794506612411,
      "grad_norm": 12.93228816986084,
      "learning_rate": 4.3306205493387594e-05,
      "loss": 3.1594,
      "step": 13160
    },
    {
      "epoch": 6.698880976602238,
      "grad_norm": 12.875904083251953,
      "learning_rate": 4.3301119023397764e-05,
      "loss": 3.2391,
      "step": 13170
    },
    {
      "epoch": 6.703967446592065,
      "grad_norm": 10.05705451965332,
      "learning_rate": 4.329603255340794e-05,
      "loss": 3.2218,
      "step": 13180
    },
    {
      "epoch": 6.709053916581892,
      "grad_norm": 10.930212020874023,
      "learning_rate": 4.329094608341811e-05,
      "loss": 3.217,
      "step": 13190
    },
    {
      "epoch": 6.714140386571719,
      "grad_norm": 11.344766616821289,
      "learning_rate": 4.328585961342828e-05,
      "loss": 3.1942,
      "step": 13200
    },
    {
      "epoch": 6.719226856561546,
      "grad_norm": 11.154949188232422,
      "learning_rate": 4.328077314343846e-05,
      "loss": 3.1724,
      "step": 13210
    },
    {
      "epoch": 6.724313326551373,
      "grad_norm": 13.205628395080566,
      "learning_rate": 4.327568667344863e-05,
      "loss": 3.1691,
      "step": 13220
    },
    {
      "epoch": 6.7293997965412,
      "grad_norm": 14.286026000976562,
      "learning_rate": 4.32706002034588e-05,
      "loss": 3.2411,
      "step": 13230
    },
    {
      "epoch": 6.734486266531027,
      "grad_norm": 14.378456115722656,
      "learning_rate": 4.326551373346897e-05,
      "loss": 3.1806,
      "step": 13240
    },
    {
      "epoch": 6.739572736520855,
      "grad_norm": 10.716500282287598,
      "learning_rate": 4.326042726347915e-05,
      "loss": 3.2419,
      "step": 13250
    },
    {
      "epoch": 6.744659206510682,
      "grad_norm": 12.551521301269531,
      "learning_rate": 4.325534079348932e-05,
      "loss": 3.2212,
      "step": 13260
    },
    {
      "epoch": 6.749745676500509,
      "grad_norm": 9.260600090026855,
      "learning_rate": 4.3250254323499497e-05,
      "loss": 3.1934,
      "step": 13270
    },
    {
      "epoch": 6.754832146490336,
      "grad_norm": 13.325748443603516,
      "learning_rate": 4.3245167853509666e-05,
      "loss": 3.2576,
      "step": 13280
    },
    {
      "epoch": 6.759918616480163,
      "grad_norm": 15.46668529510498,
      "learning_rate": 4.3240081383519836e-05,
      "loss": 3.1851,
      "step": 13290
    },
    {
      "epoch": 6.76500508646999,
      "grad_norm": 15.04997730255127,
      "learning_rate": 4.323499491353001e-05,
      "loss": 3.3016,
      "step": 13300
    },
    {
      "epoch": 6.770091556459817,
      "grad_norm": 12.005414009094238,
      "learning_rate": 4.322990844354018e-05,
      "loss": 3.2148,
      "step": 13310
    },
    {
      "epoch": 6.775178026449644,
      "grad_norm": 14.2921724319458,
      "learning_rate": 4.322482197355035e-05,
      "loss": 3.0956,
      "step": 13320
    },
    {
      "epoch": 6.780264496439471,
      "grad_norm": 10.710622787475586,
      "learning_rate": 4.321973550356053e-05,
      "loss": 3.2437,
      "step": 13330
    },
    {
      "epoch": 6.785350966429298,
      "grad_norm": 7.9996209144592285,
      "learning_rate": 4.3214649033570706e-05,
      "loss": 3.2293,
      "step": 13340
    },
    {
      "epoch": 6.790437436419126,
      "grad_norm": 12.140299797058105,
      "learning_rate": 4.3209562563580876e-05,
      "loss": 3.1489,
      "step": 13350
    },
    {
      "epoch": 6.7955239064089525,
      "grad_norm": 12.35049057006836,
      "learning_rate": 4.320447609359105e-05,
      "loss": 3.1709,
      "step": 13360
    },
    {
      "epoch": 6.8006103763987795,
      "grad_norm": 12.051545143127441,
      "learning_rate": 4.319938962360122e-05,
      "loss": 3.2423,
      "step": 13370
    },
    {
      "epoch": 6.8056968463886065,
      "grad_norm": 10.954889297485352,
      "learning_rate": 4.31943031536114e-05,
      "loss": 3.1714,
      "step": 13380
    },
    {
      "epoch": 6.8107833163784335,
      "grad_norm": 19.3961181640625,
      "learning_rate": 4.318921668362157e-05,
      "loss": 3.0959,
      "step": 13390
    },
    {
      "epoch": 6.8158697863682605,
      "grad_norm": 10.931567192077637,
      "learning_rate": 4.318413021363174e-05,
      "loss": 3.2132,
      "step": 13400
    },
    {
      "epoch": 6.820956256358087,
      "grad_norm": 13.261393547058105,
      "learning_rate": 4.3179043743641916e-05,
      "loss": 3.1546,
      "step": 13410
    },
    {
      "epoch": 6.826042726347914,
      "grad_norm": 9.935296058654785,
      "learning_rate": 4.3173957273652086e-05,
      "loss": 3.1956,
      "step": 13420
    },
    {
      "epoch": 6.831129196337741,
      "grad_norm": 12.821161270141602,
      "learning_rate": 4.316887080366226e-05,
      "loss": 3.1993,
      "step": 13430
    },
    {
      "epoch": 6.836215666327568,
      "grad_norm": 11.978597640991211,
      "learning_rate": 4.316378433367244e-05,
      "loss": 3.1445,
      "step": 13440
    },
    {
      "epoch": 6.841302136317395,
      "grad_norm": 12.49182415008545,
      "learning_rate": 4.315869786368261e-05,
      "loss": 3.2034,
      "step": 13450
    },
    {
      "epoch": 6.846388606307222,
      "grad_norm": 14.693467140197754,
      "learning_rate": 4.315361139369278e-05,
      "loss": 3.2202,
      "step": 13460
    },
    {
      "epoch": 6.85147507629705,
      "grad_norm": 10.535002708435059,
      "learning_rate": 4.3148524923702955e-05,
      "loss": 3.1653,
      "step": 13470
    },
    {
      "epoch": 6.856561546286877,
      "grad_norm": 12.487319946289062,
      "learning_rate": 4.3143438453713125e-05,
      "loss": 3.2003,
      "step": 13480
    },
    {
      "epoch": 6.861648016276704,
      "grad_norm": 14.141436576843262,
      "learning_rate": 4.3138351983723295e-05,
      "loss": 3.1275,
      "step": 13490
    },
    {
      "epoch": 6.866734486266531,
      "grad_norm": 18.355438232421875,
      "learning_rate": 4.313326551373347e-05,
      "loss": 3.2035,
      "step": 13500
    },
    {
      "epoch": 6.871820956256358,
      "grad_norm": 16.005586624145508,
      "learning_rate": 4.312817904374364e-05,
      "loss": 3.217,
      "step": 13510
    },
    {
      "epoch": 6.876907426246185,
      "grad_norm": 11.108264923095703,
      "learning_rate": 4.312309257375381e-05,
      "loss": 3.1158,
      "step": 13520
    },
    {
      "epoch": 6.881993896236012,
      "grad_norm": 11.893735885620117,
      "learning_rate": 4.311800610376399e-05,
      "loss": 3.0873,
      "step": 13530
    },
    {
      "epoch": 6.887080366225839,
      "grad_norm": 12.533491134643555,
      "learning_rate": 4.3112919633774165e-05,
      "loss": 3.218,
      "step": 13540
    },
    {
      "epoch": 6.892166836215666,
      "grad_norm": 9.502979278564453,
      "learning_rate": 4.3107833163784335e-05,
      "loss": 3.1552,
      "step": 13550
    },
    {
      "epoch": 6.897253306205493,
      "grad_norm": 13.223057746887207,
      "learning_rate": 4.310274669379451e-05,
      "loss": 3.1817,
      "step": 13560
    },
    {
      "epoch": 6.902339776195321,
      "grad_norm": 11.536028861999512,
      "learning_rate": 4.309766022380468e-05,
      "loss": 3.1795,
      "step": 13570
    },
    {
      "epoch": 6.907426246185148,
      "grad_norm": 16.273283004760742,
      "learning_rate": 4.309257375381485e-05,
      "loss": 3.1935,
      "step": 13580
    },
    {
      "epoch": 6.912512716174975,
      "grad_norm": 15.69054889678955,
      "learning_rate": 4.308748728382503e-05,
      "loss": 3.2156,
      "step": 13590
    },
    {
      "epoch": 6.917599186164802,
      "grad_norm": 11.104248046875,
      "learning_rate": 4.30824008138352e-05,
      "loss": 3.2676,
      "step": 13600
    },
    {
      "epoch": 6.922685656154629,
      "grad_norm": 10.940813064575195,
      "learning_rate": 4.307731434384537e-05,
      "loss": 3.214,
      "step": 13610
    },
    {
      "epoch": 6.927772126144456,
      "grad_norm": 12.108895301818848,
      "learning_rate": 4.3072227873855544e-05,
      "loss": 3.1756,
      "step": 13620
    },
    {
      "epoch": 6.932858596134283,
      "grad_norm": 10.68118667602539,
      "learning_rate": 4.306714140386572e-05,
      "loss": 3.1892,
      "step": 13630
    },
    {
      "epoch": 6.93794506612411,
      "grad_norm": 12.384748458862305,
      "learning_rate": 4.30620549338759e-05,
      "loss": 3.1572,
      "step": 13640
    },
    {
      "epoch": 6.943031536113937,
      "grad_norm": 8.849796295166016,
      "learning_rate": 4.305696846388607e-05,
      "loss": 3.2611,
      "step": 13650
    },
    {
      "epoch": 6.948118006103764,
      "grad_norm": 11.645842552185059,
      "learning_rate": 4.305188199389624e-05,
      "loss": 3.1861,
      "step": 13660
    },
    {
      "epoch": 6.953204476093591,
      "grad_norm": 13.525778770446777,
      "learning_rate": 4.3046795523906414e-05,
      "loss": 3.1645,
      "step": 13670
    },
    {
      "epoch": 6.958290946083418,
      "grad_norm": 14.071260452270508,
      "learning_rate": 4.3041709053916584e-05,
      "loss": 3.1992,
      "step": 13680
    },
    {
      "epoch": 6.963377416073245,
      "grad_norm": 11.111673355102539,
      "learning_rate": 4.3036622583926754e-05,
      "loss": 3.1705,
      "step": 13690
    },
    {
      "epoch": 6.9684638860630725,
      "grad_norm": 9.188492774963379,
      "learning_rate": 4.303153611393693e-05,
      "loss": 3.2778,
      "step": 13700
    },
    {
      "epoch": 6.9735503560528995,
      "grad_norm": 12.945023536682129,
      "learning_rate": 4.30264496439471e-05,
      "loss": 3.2087,
      "step": 13710
    },
    {
      "epoch": 6.9786368260427265,
      "grad_norm": 10.833484649658203,
      "learning_rate": 4.302136317395728e-05,
      "loss": 3.2751,
      "step": 13720
    },
    {
      "epoch": 6.9837232960325535,
      "grad_norm": 14.694769859313965,
      "learning_rate": 4.3016276703967454e-05,
      "loss": 3.2338,
      "step": 13730
    },
    {
      "epoch": 6.9888097660223805,
      "grad_norm": 13.234106063842773,
      "learning_rate": 4.3011190233977624e-05,
      "loss": 3.1321,
      "step": 13740
    },
    {
      "epoch": 6.9938962360122074,
      "grad_norm": 10.053802490234375,
      "learning_rate": 4.3006103763987794e-05,
      "loss": 3.1413,
      "step": 13750
    },
    {
      "epoch": 6.998982706002034,
      "grad_norm": 8.430957794189453,
      "learning_rate": 4.300101729399797e-05,
      "loss": 3.1525,
      "step": 13760
    },
    {
      "epoch": 7.0,
      "eval_loss": 3.6769213676452637,
      "eval_runtime": 2.7325,
      "eval_samples_per_second": 1015.572,
      "eval_steps_per_second": 126.992,
      "step": 13762
    },
    {
      "epoch": 7.004069175991861,
      "grad_norm": 10.273591995239258,
      "learning_rate": 4.299593082400814e-05,
      "loss": 3.1452,
      "step": 13770
    },
    {
      "epoch": 7.009155645981688,
      "grad_norm": 16.875816345214844,
      "learning_rate": 4.299084435401831e-05,
      "loss": 3.1482,
      "step": 13780
    },
    {
      "epoch": 7.014242115971515,
      "grad_norm": 12.53924560546875,
      "learning_rate": 4.298575788402849e-05,
      "loss": 3.2078,
      "step": 13790
    },
    {
      "epoch": 7.019328585961343,
      "grad_norm": 10.827129364013672,
      "learning_rate": 4.298067141403866e-05,
      "loss": 3.1239,
      "step": 13800
    },
    {
      "epoch": 7.02441505595117,
      "grad_norm": 12.341171264648438,
      "learning_rate": 4.2975584944048833e-05,
      "loss": 3.1725,
      "step": 13810
    },
    {
      "epoch": 7.029501525940997,
      "grad_norm": 16.116308212280273,
      "learning_rate": 4.2970498474059e-05,
      "loss": 3.1662,
      "step": 13820
    },
    {
      "epoch": 7.034587995930824,
      "grad_norm": 12.5940580368042,
      "learning_rate": 4.296541200406918e-05,
      "loss": 3.1504,
      "step": 13830
    },
    {
      "epoch": 7.039674465920651,
      "grad_norm": 10.270939826965332,
      "learning_rate": 4.296032553407935e-05,
      "loss": 3.1542,
      "step": 13840
    },
    {
      "epoch": 7.044760935910478,
      "grad_norm": 13.675044059753418,
      "learning_rate": 4.2955239064089527e-05,
      "loss": 3.1778,
      "step": 13850
    },
    {
      "epoch": 7.049847405900305,
      "grad_norm": 12.641772270202637,
      "learning_rate": 4.2950152594099696e-05,
      "loss": 3.0791,
      "step": 13860
    },
    {
      "epoch": 7.054933875890132,
      "grad_norm": 11.683900833129883,
      "learning_rate": 4.2945066124109866e-05,
      "loss": 3.1666,
      "step": 13870
    },
    {
      "epoch": 7.060020345879959,
      "grad_norm": 10.862201690673828,
      "learning_rate": 4.293997965412004e-05,
      "loss": 3.0777,
      "step": 13880
    },
    {
      "epoch": 7.065106815869786,
      "grad_norm": 13.655694961547852,
      "learning_rate": 4.293489318413021e-05,
      "loss": 3.161,
      "step": 13890
    },
    {
      "epoch": 7.070193285859613,
      "grad_norm": 8.200600624084473,
      "learning_rate": 4.292980671414038e-05,
      "loss": 3.1453,
      "step": 13900
    },
    {
      "epoch": 7.075279755849441,
      "grad_norm": 15.026144981384277,
      "learning_rate": 4.292472024415056e-05,
      "loss": 3.1585,
      "step": 13910
    },
    {
      "epoch": 7.080366225839268,
      "grad_norm": 11.750396728515625,
      "learning_rate": 4.2919633774160736e-05,
      "loss": 3.1313,
      "step": 13920
    },
    {
      "epoch": 7.085452695829095,
      "grad_norm": 12.38849925994873,
      "learning_rate": 4.291454730417091e-05,
      "loss": 3.177,
      "step": 13930
    },
    {
      "epoch": 7.090539165818922,
      "grad_norm": 12.021245956420898,
      "learning_rate": 4.290946083418108e-05,
      "loss": 3.2122,
      "step": 13940
    },
    {
      "epoch": 7.095625635808749,
      "grad_norm": 12.161373138427734,
      "learning_rate": 4.290437436419125e-05,
      "loss": 3.1708,
      "step": 13950
    },
    {
      "epoch": 7.100712105798576,
      "grad_norm": 13.129401206970215,
      "learning_rate": 4.289928789420143e-05,
      "loss": 3.073,
      "step": 13960
    },
    {
      "epoch": 7.105798575788403,
      "grad_norm": 10.828367233276367,
      "learning_rate": 4.28942014242116e-05,
      "loss": 3.1155,
      "step": 13970
    },
    {
      "epoch": 7.11088504577823,
      "grad_norm": 10.841264724731445,
      "learning_rate": 4.288911495422177e-05,
      "loss": 3.1287,
      "step": 13980
    },
    {
      "epoch": 7.115971515768057,
      "grad_norm": 15.200733184814453,
      "learning_rate": 4.2884028484231946e-05,
      "loss": 3.2465,
      "step": 13990
    },
    {
      "epoch": 7.121057985757884,
      "grad_norm": 9.717218399047852,
      "learning_rate": 4.2878942014242116e-05,
      "loss": 3.1542,
      "step": 14000
    },
    {
      "epoch": 7.126144455747711,
      "grad_norm": 10.921928405761719,
      "learning_rate": 4.287385554425229e-05,
      "loss": 3.1075,
      "step": 14010
    },
    {
      "epoch": 7.131230925737539,
      "grad_norm": 14.983436584472656,
      "learning_rate": 4.286876907426247e-05,
      "loss": 3.1438,
      "step": 14020
    },
    {
      "epoch": 7.136317395727366,
      "grad_norm": 11.756205558776855,
      "learning_rate": 4.286368260427264e-05,
      "loss": 3.162,
      "step": 14030
    },
    {
      "epoch": 7.1414038657171925,
      "grad_norm": 11.468505859375,
      "learning_rate": 4.285859613428281e-05,
      "loss": 3.1048,
      "step": 14040
    },
    {
      "epoch": 7.1464903357070195,
      "grad_norm": 14.1563720703125,
      "learning_rate": 4.2853509664292985e-05,
      "loss": 3.0829,
      "step": 14050
    },
    {
      "epoch": 7.1515768056968465,
      "grad_norm": 10.17048454284668,
      "learning_rate": 4.2848423194303155e-05,
      "loss": 3.1338,
      "step": 14060
    },
    {
      "epoch": 7.1566632756866735,
      "grad_norm": 12.64617919921875,
      "learning_rate": 4.2843336724313325e-05,
      "loss": 3.1603,
      "step": 14070
    },
    {
      "epoch": 7.1617497456765005,
      "grad_norm": 11.992523193359375,
      "learning_rate": 4.28382502543235e-05,
      "loss": 3.0474,
      "step": 14080
    },
    {
      "epoch": 7.1668362156663274,
      "grad_norm": 12.396697998046875,
      "learning_rate": 4.283316378433367e-05,
      "loss": 3.1842,
      "step": 14090
    },
    {
      "epoch": 7.171922685656154,
      "grad_norm": 20.31121253967285,
      "learning_rate": 4.282807731434385e-05,
      "loss": 3.0549,
      "step": 14100
    },
    {
      "epoch": 7.177009155645981,
      "grad_norm": 14.751421928405762,
      "learning_rate": 4.2822990844354025e-05,
      "loss": 3.1789,
      "step": 14110
    },
    {
      "epoch": 7.182095625635808,
      "grad_norm": 12.409894943237305,
      "learning_rate": 4.2817904374364195e-05,
      "loss": 3.1746,
      "step": 14120
    },
    {
      "epoch": 7.187182095625635,
      "grad_norm": 13.433791160583496,
      "learning_rate": 4.2812817904374365e-05,
      "loss": 3.2143,
      "step": 14130
    },
    {
      "epoch": 7.192268565615463,
      "grad_norm": 12.743548393249512,
      "learning_rate": 4.280773143438454e-05,
      "loss": 3.1745,
      "step": 14140
    },
    {
      "epoch": 7.19735503560529,
      "grad_norm": 13.934086799621582,
      "learning_rate": 4.280264496439471e-05,
      "loss": 3.2603,
      "step": 14150
    },
    {
      "epoch": 7.202441505595117,
      "grad_norm": 11.211115837097168,
      "learning_rate": 4.279755849440488e-05,
      "loss": 3.1376,
      "step": 14160
    },
    {
      "epoch": 7.207527975584944,
      "grad_norm": 12.362502098083496,
      "learning_rate": 4.279247202441506e-05,
      "loss": 3.1554,
      "step": 14170
    },
    {
      "epoch": 7.212614445574771,
      "grad_norm": 13.903292655944824,
      "learning_rate": 4.278738555442523e-05,
      "loss": 3.0682,
      "step": 14180
    },
    {
      "epoch": 7.217700915564598,
      "grad_norm": 11.02169418334961,
      "learning_rate": 4.2782299084435405e-05,
      "loss": 3.0991,
      "step": 14190
    },
    {
      "epoch": 7.222787385554425,
      "grad_norm": 10.840696334838867,
      "learning_rate": 4.2777212614445574e-05,
      "loss": 3.1028,
      "step": 14200
    },
    {
      "epoch": 7.227873855544252,
      "grad_norm": 12.94845199584961,
      "learning_rate": 4.277212614445575e-05,
      "loss": 3.1271,
      "step": 14210
    },
    {
      "epoch": 7.232960325534079,
      "grad_norm": 15.978728294372559,
      "learning_rate": 4.276703967446593e-05,
      "loss": 3.1043,
      "step": 14220
    },
    {
      "epoch": 7.238046795523906,
      "grad_norm": 13.682154655456543,
      "learning_rate": 4.27619532044761e-05,
      "loss": 3.1041,
      "step": 14230
    },
    {
      "epoch": 7.243133265513733,
      "grad_norm": 12.878096580505371,
      "learning_rate": 4.275686673448627e-05,
      "loss": 3.2179,
      "step": 14240
    },
    {
      "epoch": 7.248219735503561,
      "grad_norm": 11.207656860351562,
      "learning_rate": 4.2751780264496444e-05,
      "loss": 3.1071,
      "step": 14250
    },
    {
      "epoch": 7.253306205493388,
      "grad_norm": 11.72025203704834,
      "learning_rate": 4.2746693794506614e-05,
      "loss": 3.1531,
      "step": 14260
    },
    {
      "epoch": 7.258392675483215,
      "grad_norm": 18.842105865478516,
      "learning_rate": 4.2741607324516784e-05,
      "loss": 3.1879,
      "step": 14270
    },
    {
      "epoch": 7.263479145473042,
      "grad_norm": 14.7766752243042,
      "learning_rate": 4.273652085452696e-05,
      "loss": 3.1552,
      "step": 14280
    },
    {
      "epoch": 7.268565615462869,
      "grad_norm": 17.4432315826416,
      "learning_rate": 4.273143438453713e-05,
      "loss": 3.18,
      "step": 14290
    },
    {
      "epoch": 7.273652085452696,
      "grad_norm": 19.059328079223633,
      "learning_rate": 4.272634791454731e-05,
      "loss": 3.1934,
      "step": 14300
    },
    {
      "epoch": 7.278738555442523,
      "grad_norm": 20.17289161682129,
      "learning_rate": 4.2721261444557484e-05,
      "loss": 3.0622,
      "step": 14310
    },
    {
      "epoch": 7.28382502543235,
      "grad_norm": 16.343456268310547,
      "learning_rate": 4.2716174974567654e-05,
      "loss": 3.1558,
      "step": 14320
    },
    {
      "epoch": 7.288911495422177,
      "grad_norm": 13.172005653381348,
      "learning_rate": 4.2711088504577824e-05,
      "loss": 3.0537,
      "step": 14330
    },
    {
      "epoch": 7.293997965412004,
      "grad_norm": 16.60810089111328,
      "learning_rate": 4.2706002034588e-05,
      "loss": 3.1135,
      "step": 14340
    },
    {
      "epoch": 7.299084435401831,
      "grad_norm": 16.020545959472656,
      "learning_rate": 4.270091556459817e-05,
      "loss": 3.1602,
      "step": 14350
    },
    {
      "epoch": 7.304170905391659,
      "grad_norm": 11.636219024658203,
      "learning_rate": 4.269582909460834e-05,
      "loss": 3.1384,
      "step": 14360
    },
    {
      "epoch": 7.309257375381486,
      "grad_norm": 20.152603149414062,
      "learning_rate": 4.269074262461852e-05,
      "loss": 3.14,
      "step": 14370
    },
    {
      "epoch": 7.3143438453713125,
      "grad_norm": 10.907771110534668,
      "learning_rate": 4.268565615462869e-05,
      "loss": 3.197,
      "step": 14380
    },
    {
      "epoch": 7.3194303153611395,
      "grad_norm": 14.668522834777832,
      "learning_rate": 4.2680569684638863e-05,
      "loss": 3.0659,
      "step": 14390
    },
    {
      "epoch": 7.3245167853509665,
      "grad_norm": 13.445975303649902,
      "learning_rate": 4.267548321464904e-05,
      "loss": 3.0688,
      "step": 14400
    },
    {
      "epoch": 7.3296032553407935,
      "grad_norm": 11.791834831237793,
      "learning_rate": 4.267039674465921e-05,
      "loss": 3.1671,
      "step": 14410
    },
    {
      "epoch": 7.3346897253306205,
      "grad_norm": 16.061546325683594,
      "learning_rate": 4.266531027466938e-05,
      "loss": 3.1514,
      "step": 14420
    },
    {
      "epoch": 7.3397761953204474,
      "grad_norm": 13.313654899597168,
      "learning_rate": 4.2660223804679557e-05,
      "loss": 3.1496,
      "step": 14430
    },
    {
      "epoch": 7.344862665310274,
      "grad_norm": 12.607399940490723,
      "learning_rate": 4.2655137334689726e-05,
      "loss": 3.1192,
      "step": 14440
    },
    {
      "epoch": 7.349949135300101,
      "grad_norm": 13.569948196411133,
      "learning_rate": 4.26500508646999e-05,
      "loss": 3.1463,
      "step": 14450
    },
    {
      "epoch": 7.355035605289928,
      "grad_norm": 14.261526107788086,
      "learning_rate": 4.264496439471007e-05,
      "loss": 3.1239,
      "step": 14460
    },
    {
      "epoch": 7.360122075279756,
      "grad_norm": 12.560382843017578,
      "learning_rate": 4.263987792472024e-05,
      "loss": 3.0617,
      "step": 14470
    },
    {
      "epoch": 7.365208545269583,
      "grad_norm": 16.32639503479004,
      "learning_rate": 4.263479145473042e-05,
      "loss": 3.0966,
      "step": 14480
    },
    {
      "epoch": 7.37029501525941,
      "grad_norm": 14.776453018188477,
      "learning_rate": 4.262970498474059e-05,
      "loss": 3.1112,
      "step": 14490
    },
    {
      "epoch": 7.375381485249237,
      "grad_norm": 12.066682815551758,
      "learning_rate": 4.2624618514750766e-05,
      "loss": 3.1113,
      "step": 14500
    },
    {
      "epoch": 7.380467955239064,
      "grad_norm": 12.517330169677734,
      "learning_rate": 4.261953204476094e-05,
      "loss": 3.1233,
      "step": 14510
    },
    {
      "epoch": 7.385554425228891,
      "grad_norm": 13.851790428161621,
      "learning_rate": 4.261444557477111e-05,
      "loss": 3.141,
      "step": 14520
    },
    {
      "epoch": 7.390640895218718,
      "grad_norm": 13.756128311157227,
      "learning_rate": 4.260935910478128e-05,
      "loss": 3.1469,
      "step": 14530
    },
    {
      "epoch": 7.395727365208545,
      "grad_norm": 14.324167251586914,
      "learning_rate": 4.260427263479146e-05,
      "loss": 3.1193,
      "step": 14540
    },
    {
      "epoch": 7.400813835198372,
      "grad_norm": 15.719882011413574,
      "learning_rate": 4.259918616480163e-05,
      "loss": 3.1657,
      "step": 14550
    },
    {
      "epoch": 7.405900305188199,
      "grad_norm": 16.051488876342773,
      "learning_rate": 4.25940996948118e-05,
      "loss": 3.0825,
      "step": 14560
    },
    {
      "epoch": 7.410986775178026,
      "grad_norm": 12.990646362304688,
      "learning_rate": 4.2589013224821976e-05,
      "loss": 3.0969,
      "step": 14570
    },
    {
      "epoch": 7.416073245167853,
      "grad_norm": 13.806253433227539,
      "learning_rate": 4.2583926754832146e-05,
      "loss": 3.1618,
      "step": 14580
    },
    {
      "epoch": 7.421159715157681,
      "grad_norm": 14.143521308898926,
      "learning_rate": 4.257884028484232e-05,
      "loss": 3.1213,
      "step": 14590
    },
    {
      "epoch": 7.426246185147508,
      "grad_norm": 10.488945007324219,
      "learning_rate": 4.25737538148525e-05,
      "loss": 3.094,
      "step": 14600
    },
    {
      "epoch": 7.431332655137335,
      "grad_norm": 14.12580394744873,
      "learning_rate": 4.256866734486267e-05,
      "loss": 3.0819,
      "step": 14610
    },
    {
      "epoch": 7.436419125127162,
      "grad_norm": 13.350830078125,
      "learning_rate": 4.256358087487284e-05,
      "loss": 3.1071,
      "step": 14620
    },
    {
      "epoch": 7.441505595116989,
      "grad_norm": 14.03148078918457,
      "learning_rate": 4.2558494404883015e-05,
      "loss": 3.107,
      "step": 14630
    },
    {
      "epoch": 7.446592065106816,
      "grad_norm": 12.300223350524902,
      "learning_rate": 4.2553407934893185e-05,
      "loss": 3.1764,
      "step": 14640
    },
    {
      "epoch": 7.451678535096643,
      "grad_norm": 12.85336971282959,
      "learning_rate": 4.2548321464903355e-05,
      "loss": 3.0464,
      "step": 14650
    },
    {
      "epoch": 7.45676500508647,
      "grad_norm": 12.15282917022705,
      "learning_rate": 4.254323499491353e-05,
      "loss": 3.1255,
      "step": 14660
    },
    {
      "epoch": 7.461851475076297,
      "grad_norm": 12.284222602844238,
      "learning_rate": 4.25381485249237e-05,
      "loss": 3.211,
      "step": 14670
    },
    {
      "epoch": 7.466937945066124,
      "grad_norm": 14.121687889099121,
      "learning_rate": 4.253306205493388e-05,
      "loss": 3.1216,
      "step": 14680
    },
    {
      "epoch": 7.472024415055952,
      "grad_norm": 16.524276733398438,
      "learning_rate": 4.2527975584944055e-05,
      "loss": 3.1521,
      "step": 14690
    },
    {
      "epoch": 7.477110885045779,
      "grad_norm": 19.722383499145508,
      "learning_rate": 4.2522889114954225e-05,
      "loss": 3.1311,
      "step": 14700
    },
    {
      "epoch": 7.482197355035606,
      "grad_norm": 12.820029258728027,
      "learning_rate": 4.2517802644964395e-05,
      "loss": 3.1059,
      "step": 14710
    },
    {
      "epoch": 7.4872838250254325,
      "grad_norm": 13.001785278320312,
      "learning_rate": 4.251271617497457e-05,
      "loss": 3.1925,
      "step": 14720
    },
    {
      "epoch": 7.4923702950152595,
      "grad_norm": 13.155878067016602,
      "learning_rate": 4.250762970498474e-05,
      "loss": 3.1147,
      "step": 14730
    },
    {
      "epoch": 7.4974567650050865,
      "grad_norm": 15.996042251586914,
      "learning_rate": 4.250254323499492e-05,
      "loss": 3.1141,
      "step": 14740
    },
    {
      "epoch": 7.5025432349949135,
      "grad_norm": 11.898548126220703,
      "learning_rate": 4.249745676500509e-05,
      "loss": 3.1785,
      "step": 14750
    },
    {
      "epoch": 7.5076297049847405,
      "grad_norm": 13.852673530578613,
      "learning_rate": 4.249237029501526e-05,
      "loss": 3.1382,
      "step": 14760
    },
    {
      "epoch": 7.5127161749745675,
      "grad_norm": 14.574823379516602,
      "learning_rate": 4.2487283825025435e-05,
      "loss": 3.0871,
      "step": 14770
    },
    {
      "epoch": 7.517802644964394,
      "grad_norm": 10.835354804992676,
      "learning_rate": 4.2482197355035604e-05,
      "loss": 3.1315,
      "step": 14780
    },
    {
      "epoch": 7.522889114954221,
      "grad_norm": 11.193902015686035,
      "learning_rate": 4.247711088504578e-05,
      "loss": 3.0229,
      "step": 14790
    },
    {
      "epoch": 7.527975584944048,
      "grad_norm": 14.579432487487793,
      "learning_rate": 4.247202441505596e-05,
      "loss": 3.1399,
      "step": 14800
    },
    {
      "epoch": 7.533062054933876,
      "grad_norm": 12.744514465332031,
      "learning_rate": 4.246693794506613e-05,
      "loss": 3.0398,
      "step": 14810
    },
    {
      "epoch": 7.538148524923703,
      "grad_norm": 12.99458122253418,
      "learning_rate": 4.24618514750763e-05,
      "loss": 3.1385,
      "step": 14820
    },
    {
      "epoch": 7.54323499491353,
      "grad_norm": 17.42848014831543,
      "learning_rate": 4.2456765005086474e-05,
      "loss": 3.1555,
      "step": 14830
    },
    {
      "epoch": 7.548321464903357,
      "grad_norm": 15.132218360900879,
      "learning_rate": 4.2451678535096644e-05,
      "loss": 3.1332,
      "step": 14840
    },
    {
      "epoch": 7.553407934893184,
      "grad_norm": 15.804088592529297,
      "learning_rate": 4.2446592065106814e-05,
      "loss": 3.1074,
      "step": 14850
    },
    {
      "epoch": 7.558494404883011,
      "grad_norm": 16.974454879760742,
      "learning_rate": 4.244150559511699e-05,
      "loss": 3.1388,
      "step": 14860
    },
    {
      "epoch": 7.563580874872838,
      "grad_norm": 15.404154777526855,
      "learning_rate": 4.243641912512716e-05,
      "loss": 3.1569,
      "step": 14870
    },
    {
      "epoch": 7.568667344862665,
      "grad_norm": 14.716658592224121,
      "learning_rate": 4.243133265513734e-05,
      "loss": 3.1215,
      "step": 14880
    },
    {
      "epoch": 7.573753814852492,
      "grad_norm": 16.49824333190918,
      "learning_rate": 4.2426246185147514e-05,
      "loss": 3.1136,
      "step": 14890
    },
    {
      "epoch": 7.578840284842319,
      "grad_norm": 10.719346046447754,
      "learning_rate": 4.2421159715157684e-05,
      "loss": 3.0679,
      "step": 14900
    },
    {
      "epoch": 7.583926754832147,
      "grad_norm": 12.473305702209473,
      "learning_rate": 4.2416073245167854e-05,
      "loss": 3.1261,
      "step": 14910
    },
    {
      "epoch": 7.589013224821974,
      "grad_norm": 14.338983535766602,
      "learning_rate": 4.241098677517803e-05,
      "loss": 3.1794,
      "step": 14920
    },
    {
      "epoch": 7.594099694811801,
      "grad_norm": 16.877290725708008,
      "learning_rate": 4.24059003051882e-05,
      "loss": 3.1705,
      "step": 14930
    },
    {
      "epoch": 7.599186164801628,
      "grad_norm": 13.139010429382324,
      "learning_rate": 4.240081383519837e-05,
      "loss": 3.0979,
      "step": 14940
    },
    {
      "epoch": 7.604272634791455,
      "grad_norm": 11.532674789428711,
      "learning_rate": 4.239572736520855e-05,
      "loss": 3.0955,
      "step": 14950
    },
    {
      "epoch": 7.609359104781282,
      "grad_norm": 14.043411254882812,
      "learning_rate": 4.239064089521872e-05,
      "loss": 3.0287,
      "step": 14960
    },
    {
      "epoch": 7.614445574771109,
      "grad_norm": 13.63361930847168,
      "learning_rate": 4.2385554425228893e-05,
      "loss": 3.1438,
      "step": 14970
    },
    {
      "epoch": 7.619532044760936,
      "grad_norm": 13.212944030761719,
      "learning_rate": 4.238046795523907e-05,
      "loss": 3.0442,
      "step": 14980
    },
    {
      "epoch": 7.624618514750763,
      "grad_norm": 15.190218925476074,
      "learning_rate": 4.237538148524924e-05,
      "loss": 3.1295,
      "step": 14990
    },
    {
      "epoch": 7.62970498474059,
      "grad_norm": 11.597360610961914,
      "learning_rate": 4.237029501525942e-05,
      "loss": 3.1138,
      "step": 15000
    },
    {
      "epoch": 7.634791454730417,
      "grad_norm": 12.543248176574707,
      "learning_rate": 4.2365208545269587e-05,
      "loss": 3.1447,
      "step": 15010
    },
    {
      "epoch": 7.639877924720244,
      "grad_norm": 16.22816276550293,
      "learning_rate": 4.2360122075279756e-05,
      "loss": 3.1042,
      "step": 15020
    },
    {
      "epoch": 7.644964394710071,
      "grad_norm": 11.093331336975098,
      "learning_rate": 4.235503560528993e-05,
      "loss": 3.0394,
      "step": 15030
    },
    {
      "epoch": 7.650050864699899,
      "grad_norm": 12.495610237121582,
      "learning_rate": 4.23499491353001e-05,
      "loss": 3.1838,
      "step": 15040
    },
    {
      "epoch": 7.655137334689726,
      "grad_norm": 11.908525466918945,
      "learning_rate": 4.234486266531027e-05,
      "loss": 3.1888,
      "step": 15050
    },
    {
      "epoch": 7.6602238046795526,
      "grad_norm": 15.471025466918945,
      "learning_rate": 4.233977619532045e-05,
      "loss": 3.0813,
      "step": 15060
    },
    {
      "epoch": 7.6653102746693795,
      "grad_norm": 13.863896369934082,
      "learning_rate": 4.2334689725330626e-05,
      "loss": 3.0078,
      "step": 15070
    },
    {
      "epoch": 7.6703967446592065,
      "grad_norm": 14.345905303955078,
      "learning_rate": 4.2329603255340796e-05,
      "loss": 3.0546,
      "step": 15080
    },
    {
      "epoch": 7.6754832146490335,
      "grad_norm": 11.831214904785156,
      "learning_rate": 4.232451678535097e-05,
      "loss": 3.0776,
      "step": 15090
    },
    {
      "epoch": 7.6805696846388605,
      "grad_norm": 12.968852043151855,
      "learning_rate": 4.231943031536114e-05,
      "loss": 3.1703,
      "step": 15100
    },
    {
      "epoch": 7.6856561546286875,
      "grad_norm": 15.242877960205078,
      "learning_rate": 4.231434384537131e-05,
      "loss": 3.1459,
      "step": 15110
    },
    {
      "epoch": 7.690742624618514,
      "grad_norm": 22.34627342224121,
      "learning_rate": 4.230925737538149e-05,
      "loss": 3.1387,
      "step": 15120
    },
    {
      "epoch": 7.695829094608342,
      "grad_norm": 14.133163452148438,
      "learning_rate": 4.230417090539166e-05,
      "loss": 3.1142,
      "step": 15130
    },
    {
      "epoch": 7.700915564598169,
      "grad_norm": 13.027560234069824,
      "learning_rate": 4.229908443540183e-05,
      "loss": 3.1234,
      "step": 15140
    },
    {
      "epoch": 7.706002034587996,
      "grad_norm": 11.375372886657715,
      "learning_rate": 4.2293997965412006e-05,
      "loss": 3.175,
      "step": 15150
    },
    {
      "epoch": 7.711088504577823,
      "grad_norm": 11.36523723602295,
      "learning_rate": 4.2288911495422176e-05,
      "loss": 3.0121,
      "step": 15160
    },
    {
      "epoch": 7.71617497456765,
      "grad_norm": 17.384368896484375,
      "learning_rate": 4.228382502543235e-05,
      "loss": 3.1465,
      "step": 15170
    },
    {
      "epoch": 7.721261444557477,
      "grad_norm": 13.962657928466797,
      "learning_rate": 4.227873855544253e-05,
      "loss": 3.1597,
      "step": 15180
    },
    {
      "epoch": 7.726347914547304,
      "grad_norm": 11.474245071411133,
      "learning_rate": 4.22736520854527e-05,
      "loss": 3.1306,
      "step": 15190
    },
    {
      "epoch": 7.731434384537131,
      "grad_norm": 15.07315731048584,
      "learning_rate": 4.226856561546287e-05,
      "loss": 3.0696,
      "step": 15200
    },
    {
      "epoch": 7.736520854526958,
      "grad_norm": 14.428342819213867,
      "learning_rate": 4.2263479145473045e-05,
      "loss": 3.0782,
      "step": 15210
    },
    {
      "epoch": 7.741607324516785,
      "grad_norm": 15.720989227294922,
      "learning_rate": 4.2258392675483215e-05,
      "loss": 3.1217,
      "step": 15220
    },
    {
      "epoch": 7.746693794506612,
      "grad_norm": 18.848772048950195,
      "learning_rate": 4.2253306205493385e-05,
      "loss": 3.0633,
      "step": 15230
    },
    {
      "epoch": 7.751780264496439,
      "grad_norm": 10.711638450622559,
      "learning_rate": 4.224821973550356e-05,
      "loss": 3.0461,
      "step": 15240
    },
    {
      "epoch": 7.756866734486266,
      "grad_norm": 12.17372989654541,
      "learning_rate": 4.224313326551373e-05,
      "loss": 3.1125,
      "step": 15250
    },
    {
      "epoch": 7.761953204476094,
      "grad_norm": 13.191296577453613,
      "learning_rate": 4.223804679552391e-05,
      "loss": 3.0668,
      "step": 15260
    },
    {
      "epoch": 7.767039674465921,
      "grad_norm": 13.211410522460938,
      "learning_rate": 4.2232960325534085e-05,
      "loss": 3.0303,
      "step": 15270
    },
    {
      "epoch": 7.772126144455748,
      "grad_norm": 12.40515422821045,
      "learning_rate": 4.2227873855544255e-05,
      "loss": 3.1485,
      "step": 15280
    },
    {
      "epoch": 7.777212614445575,
      "grad_norm": 12.181644439697266,
      "learning_rate": 4.222278738555443e-05,
      "loss": 3.0834,
      "step": 15290
    },
    {
      "epoch": 7.782299084435402,
      "grad_norm": 13.572179794311523,
      "learning_rate": 4.22177009155646e-05,
      "loss": 3.1004,
      "step": 15300
    },
    {
      "epoch": 7.787385554425229,
      "grad_norm": 13.06275749206543,
      "learning_rate": 4.221261444557477e-05,
      "loss": 3.0173,
      "step": 15310
    },
    {
      "epoch": 7.792472024415056,
      "grad_norm": 14.570067405700684,
      "learning_rate": 4.220752797558495e-05,
      "loss": 3.0428,
      "step": 15320
    },
    {
      "epoch": 7.797558494404883,
      "grad_norm": 14.011626243591309,
      "learning_rate": 4.220244150559512e-05,
      "loss": 3.1313,
      "step": 15330
    },
    {
      "epoch": 7.80264496439471,
      "grad_norm": 15.654574394226074,
      "learning_rate": 4.219735503560529e-05,
      "loss": 3.0749,
      "step": 15340
    },
    {
      "epoch": 7.807731434384537,
      "grad_norm": 17.508468627929688,
      "learning_rate": 4.2192268565615465e-05,
      "loss": 3.0906,
      "step": 15350
    },
    {
      "epoch": 7.812817904374365,
      "grad_norm": 13.286454200744629,
      "learning_rate": 4.218718209562564e-05,
      "loss": 3.125,
      "step": 15360
    },
    {
      "epoch": 7.817904374364192,
      "grad_norm": 13.360023498535156,
      "learning_rate": 4.218209562563581e-05,
      "loss": 3.0951,
      "step": 15370
    },
    {
      "epoch": 7.822990844354019,
      "grad_norm": 11.309256553649902,
      "learning_rate": 4.217700915564599e-05,
      "loss": 3.0487,
      "step": 15380
    },
    {
      "epoch": 7.828077314343846,
      "grad_norm": 13.12709903717041,
      "learning_rate": 4.217192268565616e-05,
      "loss": 3.1617,
      "step": 15390
    },
    {
      "epoch": 7.8331637843336726,
      "grad_norm": 14.071491241455078,
      "learning_rate": 4.216683621566633e-05,
      "loss": 3.1844,
      "step": 15400
    },
    {
      "epoch": 7.8382502543234995,
      "grad_norm": 12.364241600036621,
      "learning_rate": 4.2161749745676504e-05,
      "loss": 3.1696,
      "step": 15410
    },
    {
      "epoch": 7.8433367243133265,
      "grad_norm": 19.79291343688965,
      "learning_rate": 4.2156663275686674e-05,
      "loss": 3.0735,
      "step": 15420
    },
    {
      "epoch": 7.8484231943031535,
      "grad_norm": 14.562897682189941,
      "learning_rate": 4.2151576805696844e-05,
      "loss": 3.0242,
      "step": 15430
    },
    {
      "epoch": 7.8535096642929805,
      "grad_norm": 14.977615356445312,
      "learning_rate": 4.214649033570702e-05,
      "loss": 3.1287,
      "step": 15440
    },
    {
      "epoch": 7.8585961342828075,
      "grad_norm": 11.758264541625977,
      "learning_rate": 4.214140386571719e-05,
      "loss": 3.0398,
      "step": 15450
    },
    {
      "epoch": 7.863682604272634,
      "grad_norm": 17.808387756347656,
      "learning_rate": 4.213631739572737e-05,
      "loss": 3.1225,
      "step": 15460
    },
    {
      "epoch": 7.868769074262461,
      "grad_norm": 13.108689308166504,
      "learning_rate": 4.2131230925737544e-05,
      "loss": 3.1365,
      "step": 15470
    },
    {
      "epoch": 7.873855544252289,
      "grad_norm": 12.008440971374512,
      "learning_rate": 4.2126144455747714e-05,
      "loss": 3.0901,
      "step": 15480
    },
    {
      "epoch": 7.878942014242116,
      "grad_norm": 17.53647804260254,
      "learning_rate": 4.2121057985757884e-05,
      "loss": 3.0432,
      "step": 15490
    },
    {
      "epoch": 7.884028484231943,
      "grad_norm": 13.808286666870117,
      "learning_rate": 4.211597151576806e-05,
      "loss": 3.1026,
      "step": 15500
    },
    {
      "epoch": 7.88911495422177,
      "grad_norm": 13.618459701538086,
      "learning_rate": 4.211088504577823e-05,
      "loss": 3.1036,
      "step": 15510
    },
    {
      "epoch": 7.894201424211597,
      "grad_norm": 12.215971946716309,
      "learning_rate": 4.21057985757884e-05,
      "loss": 3.0799,
      "step": 15520
    },
    {
      "epoch": 7.899287894201424,
      "grad_norm": 12.885077476501465,
      "learning_rate": 4.210071210579858e-05,
      "loss": 3.0807,
      "step": 15530
    },
    {
      "epoch": 7.904374364191251,
      "grad_norm": 18.56186866760254,
      "learning_rate": 4.209562563580875e-05,
      "loss": 3.1557,
      "step": 15540
    },
    {
      "epoch": 7.909460834181078,
      "grad_norm": 14.582984924316406,
      "learning_rate": 4.2090539165818923e-05,
      "loss": 3.1582,
      "step": 15550
    },
    {
      "epoch": 7.914547304170905,
      "grad_norm": 16.465837478637695,
      "learning_rate": 4.20854526958291e-05,
      "loss": 3.0601,
      "step": 15560
    },
    {
      "epoch": 7.919633774160732,
      "grad_norm": 14.270831108093262,
      "learning_rate": 4.208036622583927e-05,
      "loss": 3.1212,
      "step": 15570
    },
    {
      "epoch": 7.92472024415056,
      "grad_norm": 12.586968421936035,
      "learning_rate": 4.207527975584945e-05,
      "loss": 3.1045,
      "step": 15580
    },
    {
      "epoch": 7.929806714140387,
      "grad_norm": 15.558255195617676,
      "learning_rate": 4.2070193285859617e-05,
      "loss": 3.0311,
      "step": 15590
    },
    {
      "epoch": 7.934893184130214,
      "grad_norm": 13.376678466796875,
      "learning_rate": 4.2065106815869786e-05,
      "loss": 2.9757,
      "step": 15600
    },
    {
      "epoch": 7.939979654120041,
      "grad_norm": 16.242046356201172,
      "learning_rate": 4.206002034587996e-05,
      "loss": 3.0942,
      "step": 15610
    },
    {
      "epoch": 7.945066124109868,
      "grad_norm": 18.457134246826172,
      "learning_rate": 4.205493387589013e-05,
      "loss": 3.0879,
      "step": 15620
    },
    {
      "epoch": 7.950152594099695,
      "grad_norm": 15.074601173400879,
      "learning_rate": 4.20498474059003e-05,
      "loss": 3.1301,
      "step": 15630
    },
    {
      "epoch": 7.955239064089522,
      "grad_norm": 13.659849166870117,
      "learning_rate": 4.204476093591048e-05,
      "loss": 3.0986,
      "step": 15640
    },
    {
      "epoch": 7.960325534079349,
      "grad_norm": 15.776952743530273,
      "learning_rate": 4.2039674465920656e-05,
      "loss": 3.0368,
      "step": 15650
    },
    {
      "epoch": 7.965412004069176,
      "grad_norm": 13.78429889678955,
      "learning_rate": 4.2034587995930826e-05,
      "loss": 3.0502,
      "step": 15660
    },
    {
      "epoch": 7.970498474059003,
      "grad_norm": 15.760326385498047,
      "learning_rate": 4.2029501525941e-05,
      "loss": 3.1004,
      "step": 15670
    },
    {
      "epoch": 7.97558494404883,
      "grad_norm": 14.86307430267334,
      "learning_rate": 4.202441505595117e-05,
      "loss": 3.0484,
      "step": 15680
    },
    {
      "epoch": 7.980671414038657,
      "grad_norm": 14.491249084472656,
      "learning_rate": 4.201932858596134e-05,
      "loss": 3.1425,
      "step": 15690
    },
    {
      "epoch": 7.985757884028484,
      "grad_norm": 10.053237915039062,
      "learning_rate": 4.201424211597152e-05,
      "loss": 3.042,
      "step": 15700
    },
    {
      "epoch": 7.990844354018312,
      "grad_norm": 14.69250774383545,
      "learning_rate": 4.200915564598169e-05,
      "loss": 3.151,
      "step": 15710
    },
    {
      "epoch": 7.995930824008139,
      "grad_norm": 20.6966495513916,
      "learning_rate": 4.200406917599186e-05,
      "loss": 3.1464,
      "step": 15720
    },
    {
      "epoch": 8.0,
      "eval_loss": 3.713327407836914,
      "eval_runtime": 2.7573,
      "eval_samples_per_second": 1006.431,
      "eval_steps_per_second": 125.849,
      "step": 15728
    },
    {
      "epoch": 8.001017293997965,
      "grad_norm": 12.642679214477539,
      "learning_rate": 4.1998982706002036e-05,
      "loss": 3.0924,
      "step": 15730
    },
    {
      "epoch": 8.006103763987792,
      "grad_norm": 12.817683219909668,
      "learning_rate": 4.199389623601221e-05,
      "loss": 2.9862,
      "step": 15740
    },
    {
      "epoch": 8.011190233977619,
      "grad_norm": 14.11485767364502,
      "learning_rate": 4.198880976602238e-05,
      "loss": 3.0411,
      "step": 15750
    },
    {
      "epoch": 8.016276703967447,
      "grad_norm": 15.573882102966309,
      "learning_rate": 4.198372329603256e-05,
      "loss": 3.091,
      "step": 15760
    },
    {
      "epoch": 8.021363173957274,
      "grad_norm": 15.250882148742676,
      "learning_rate": 4.197863682604273e-05,
      "loss": 3.046,
      "step": 15770
    },
    {
      "epoch": 8.026449643947101,
      "grad_norm": 16.279945373535156,
      "learning_rate": 4.19735503560529e-05,
      "loss": 3.0686,
      "step": 15780
    },
    {
      "epoch": 8.031536113936928,
      "grad_norm": 12.608466148376465,
      "learning_rate": 4.1968463886063075e-05,
      "loss": 3.0287,
      "step": 15790
    },
    {
      "epoch": 8.036622583926755,
      "grad_norm": 13.92084789276123,
      "learning_rate": 4.1963377416073245e-05,
      "loss": 3.0574,
      "step": 15800
    },
    {
      "epoch": 8.041709053916582,
      "grad_norm": 12.6139497756958,
      "learning_rate": 4.195829094608342e-05,
      "loss": 3.0438,
      "step": 15810
    },
    {
      "epoch": 8.04679552390641,
      "grad_norm": 16.893686294555664,
      "learning_rate": 4.195320447609359e-05,
      "loss": 2.9795,
      "step": 15820
    },
    {
      "epoch": 8.051881993896236,
      "grad_norm": 13.840221405029297,
      "learning_rate": 4.194811800610376e-05,
      "loss": 3.0664,
      "step": 15830
    },
    {
      "epoch": 8.056968463886063,
      "grad_norm": 12.243568420410156,
      "learning_rate": 4.194303153611394e-05,
      "loss": 3.1259,
      "step": 15840
    },
    {
      "epoch": 8.06205493387589,
      "grad_norm": 14.656675338745117,
      "learning_rate": 4.1937945066124115e-05,
      "loss": 3.0602,
      "step": 15850
    },
    {
      "epoch": 8.067141403865717,
      "grad_norm": 12.570331573486328,
      "learning_rate": 4.1932858596134285e-05,
      "loss": 3.0551,
      "step": 15860
    },
    {
      "epoch": 8.072227873855544,
      "grad_norm": 13.719451904296875,
      "learning_rate": 4.192777212614446e-05,
      "loss": 3.04,
      "step": 15870
    },
    {
      "epoch": 8.077314343845371,
      "grad_norm": 13.92348575592041,
      "learning_rate": 4.192268565615463e-05,
      "loss": 2.9709,
      "step": 15880
    },
    {
      "epoch": 8.082400813835198,
      "grad_norm": 14.639581680297852,
      "learning_rate": 4.19175991861648e-05,
      "loss": 3.0527,
      "step": 15890
    },
    {
      "epoch": 8.087487283825025,
      "grad_norm": 16.026445388793945,
      "learning_rate": 4.191251271617498e-05,
      "loss": 3.0881,
      "step": 15900
    },
    {
      "epoch": 8.092573753814852,
      "grad_norm": 11.959795951843262,
      "learning_rate": 4.190742624618515e-05,
      "loss": 3.0274,
      "step": 15910
    },
    {
      "epoch": 8.097660223804679,
      "grad_norm": 11.719841957092285,
      "learning_rate": 4.190233977619532e-05,
      "loss": 3.0842,
      "step": 15920
    },
    {
      "epoch": 8.102746693794506,
      "grad_norm": 21.434667587280273,
      "learning_rate": 4.1897253306205495e-05,
      "loss": 2.9966,
      "step": 15930
    },
    {
      "epoch": 8.107833163784333,
      "grad_norm": 13.645977020263672,
      "learning_rate": 4.189216683621567e-05,
      "loss": 3.0334,
      "step": 15940
    },
    {
      "epoch": 8.11291963377416,
      "grad_norm": 14.345834732055664,
      "learning_rate": 4.188708036622584e-05,
      "loss": 3.156,
      "step": 15950
    },
    {
      "epoch": 8.118006103763987,
      "grad_norm": 13.172213554382324,
      "learning_rate": 4.188199389623602e-05,
      "loss": 3.0196,
      "step": 15960
    },
    {
      "epoch": 8.123092573753814,
      "grad_norm": 15.184868812561035,
      "learning_rate": 4.187690742624619e-05,
      "loss": 3.0977,
      "step": 15970
    },
    {
      "epoch": 8.128179043743643,
      "grad_norm": 15.078577995300293,
      "learning_rate": 4.187182095625636e-05,
      "loss": 2.9609,
      "step": 15980
    },
    {
      "epoch": 8.13326551373347,
      "grad_norm": 11.646635055541992,
      "learning_rate": 4.1866734486266534e-05,
      "loss": 3.0419,
      "step": 15990
    },
    {
      "epoch": 8.138351983723297,
      "grad_norm": 14.089622497558594,
      "learning_rate": 4.1861648016276704e-05,
      "loss": 3.0983,
      "step": 16000
    },
    {
      "epoch": 8.143438453713124,
      "grad_norm": 18.178194046020508,
      "learning_rate": 4.1856561546286874e-05,
      "loss": 2.9844,
      "step": 16010
    },
    {
      "epoch": 8.14852492370295,
      "grad_norm": 14.809123039245605,
      "learning_rate": 4.185147507629705e-05,
      "loss": 3.0006,
      "step": 16020
    },
    {
      "epoch": 8.153611393692778,
      "grad_norm": 13.862286567687988,
      "learning_rate": 4.184638860630723e-05,
      "loss": 3.1002,
      "step": 16030
    },
    {
      "epoch": 8.158697863682605,
      "grad_norm": 13.46859073638916,
      "learning_rate": 4.18413021363174e-05,
      "loss": 3.0432,
      "step": 16040
    },
    {
      "epoch": 8.163784333672432,
      "grad_norm": 14.257820129394531,
      "learning_rate": 4.1836215666327574e-05,
      "loss": 3.02,
      "step": 16050
    },
    {
      "epoch": 8.168870803662259,
      "grad_norm": 16.28763198852539,
      "learning_rate": 4.1831129196337744e-05,
      "loss": 3.1642,
      "step": 16060
    },
    {
      "epoch": 8.173957273652086,
      "grad_norm": 17.6334285736084,
      "learning_rate": 4.182604272634792e-05,
      "loss": 3.0077,
      "step": 16070
    },
    {
      "epoch": 8.179043743641913,
      "grad_norm": 14.004481315612793,
      "learning_rate": 4.182095625635809e-05,
      "loss": 3.0259,
      "step": 16080
    },
    {
      "epoch": 8.18413021363174,
      "grad_norm": 16.46037483215332,
      "learning_rate": 4.181586978636826e-05,
      "loss": 3.052,
      "step": 16090
    },
    {
      "epoch": 8.189216683621567,
      "grad_norm": 17.226348876953125,
      "learning_rate": 4.181078331637844e-05,
      "loss": 2.9504,
      "step": 16100
    },
    {
      "epoch": 8.194303153611393,
      "grad_norm": 17.895912170410156,
      "learning_rate": 4.180569684638861e-05,
      "loss": 3.0303,
      "step": 16110
    },
    {
      "epoch": 8.19938962360122,
      "grad_norm": 16.361204147338867,
      "learning_rate": 4.180061037639878e-05,
      "loss": 2.9849,
      "step": 16120
    },
    {
      "epoch": 8.204476093591047,
      "grad_norm": 16.49782371520996,
      "learning_rate": 4.1795523906408953e-05,
      "loss": 3.0349,
      "step": 16130
    },
    {
      "epoch": 8.209562563580874,
      "grad_norm": 14.566994667053223,
      "learning_rate": 4.179043743641913e-05,
      "loss": 3.0569,
      "step": 16140
    },
    {
      "epoch": 8.214649033570701,
      "grad_norm": 14.57111930847168,
      "learning_rate": 4.17853509664293e-05,
      "loss": 3.0793,
      "step": 16150
    },
    {
      "epoch": 8.219735503560528,
      "grad_norm": 14.283585548400879,
      "learning_rate": 4.178026449643948e-05,
      "loss": 3.0465,
      "step": 16160
    },
    {
      "epoch": 8.224821973550355,
      "grad_norm": 17.444656372070312,
      "learning_rate": 4.1775178026449647e-05,
      "loss": 3.0705,
      "step": 16170
    },
    {
      "epoch": 8.229908443540182,
      "grad_norm": 14.9514741897583,
      "learning_rate": 4.1770091556459816e-05,
      "loss": 2.9771,
      "step": 16180
    },
    {
      "epoch": 8.23499491353001,
      "grad_norm": 15.936408042907715,
      "learning_rate": 4.176500508646999e-05,
      "loss": 3.0914,
      "step": 16190
    },
    {
      "epoch": 8.240081383519838,
      "grad_norm": 13.253631591796875,
      "learning_rate": 4.175991861648016e-05,
      "loss": 3.0853,
      "step": 16200
    },
    {
      "epoch": 8.245167853509665,
      "grad_norm": 17.19363784790039,
      "learning_rate": 4.175483214649033e-05,
      "loss": 3.0722,
      "step": 16210
    },
    {
      "epoch": 8.250254323499492,
      "grad_norm": 19.427589416503906,
      "learning_rate": 4.174974567650051e-05,
      "loss": 3.064,
      "step": 16220
    },
    {
      "epoch": 8.255340793489319,
      "grad_norm": 16.279085159301758,
      "learning_rate": 4.1744659206510686e-05,
      "loss": 3.0061,
      "step": 16230
    },
    {
      "epoch": 8.260427263479146,
      "grad_norm": 17.342815399169922,
      "learning_rate": 4.1739572736520856e-05,
      "loss": 3.0399,
      "step": 16240
    },
    {
      "epoch": 8.265513733468973,
      "grad_norm": 17.31095314025879,
      "learning_rate": 4.173448626653103e-05,
      "loss": 3.0469,
      "step": 16250
    },
    {
      "epoch": 8.2706002034588,
      "grad_norm": 13.056520462036133,
      "learning_rate": 4.17293997965412e-05,
      "loss": 3.0648,
      "step": 16260
    },
    {
      "epoch": 8.275686673448627,
      "grad_norm": 14.867215156555176,
      "learning_rate": 4.172431332655137e-05,
      "loss": 3.1455,
      "step": 16270
    },
    {
      "epoch": 8.280773143438454,
      "grad_norm": 14.298624992370605,
      "learning_rate": 4.171922685656155e-05,
      "loss": 3.0469,
      "step": 16280
    },
    {
      "epoch": 8.285859613428281,
      "grad_norm": 16.771358489990234,
      "learning_rate": 4.171414038657172e-05,
      "loss": 3.0395,
      "step": 16290
    },
    {
      "epoch": 8.290946083418108,
      "grad_norm": 15.219084739685059,
      "learning_rate": 4.170905391658189e-05,
      "loss": 3.1545,
      "step": 16300
    },
    {
      "epoch": 8.296032553407935,
      "grad_norm": 11.508055686950684,
      "learning_rate": 4.1703967446592066e-05,
      "loss": 3.0153,
      "step": 16310
    },
    {
      "epoch": 8.301119023397762,
      "grad_norm": 13.855613708496094,
      "learning_rate": 4.169888097660224e-05,
      "loss": 3.0494,
      "step": 16320
    },
    {
      "epoch": 8.306205493387589,
      "grad_norm": 15.576948165893555,
      "learning_rate": 4.169379450661241e-05,
      "loss": 3.0138,
      "step": 16330
    },
    {
      "epoch": 8.311291963377416,
      "grad_norm": 13.996167182922363,
      "learning_rate": 4.168870803662259e-05,
      "loss": 3.0082,
      "step": 16340
    },
    {
      "epoch": 8.316378433367243,
      "grad_norm": 16.40753746032715,
      "learning_rate": 4.168362156663276e-05,
      "loss": 3.043,
      "step": 16350
    },
    {
      "epoch": 8.32146490335707,
      "grad_norm": 19.71021842956543,
      "learning_rate": 4.1678535096642936e-05,
      "loss": 3.0542,
      "step": 16360
    },
    {
      "epoch": 8.326551373346897,
      "grad_norm": 12.649968147277832,
      "learning_rate": 4.1673448626653105e-05,
      "loss": 2.9874,
      "step": 16370
    },
    {
      "epoch": 8.331637843336724,
      "grad_norm": 14.527009963989258,
      "learning_rate": 4.1668362156663275e-05,
      "loss": 2.9228,
      "step": 16380
    },
    {
      "epoch": 8.33672431332655,
      "grad_norm": 12.72978401184082,
      "learning_rate": 4.166327568667345e-05,
      "loss": 3.0293,
      "step": 16390
    },
    {
      "epoch": 8.341810783316378,
      "grad_norm": 14.178592681884766,
      "learning_rate": 4.165818921668362e-05,
      "loss": 2.9301,
      "step": 16400
    },
    {
      "epoch": 8.346897253306205,
      "grad_norm": 12.609213829040527,
      "learning_rate": 4.165310274669379e-05,
      "loss": 2.9904,
      "step": 16410
    },
    {
      "epoch": 8.351983723296033,
      "grad_norm": 15.157535552978516,
      "learning_rate": 4.164801627670397e-05,
      "loss": 3.0524,
      "step": 16420
    },
    {
      "epoch": 8.35707019328586,
      "grad_norm": 15.748165130615234,
      "learning_rate": 4.1642929806714145e-05,
      "loss": 3.0421,
      "step": 16430
    },
    {
      "epoch": 8.362156663275687,
      "grad_norm": 14.729609489440918,
      "learning_rate": 4.1637843336724315e-05,
      "loss": 3.0077,
      "step": 16440
    },
    {
      "epoch": 8.367243133265514,
      "grad_norm": 18.995594024658203,
      "learning_rate": 4.163275686673449e-05,
      "loss": 3.004,
      "step": 16450
    },
    {
      "epoch": 8.372329603255341,
      "grad_norm": 13.411614418029785,
      "learning_rate": 4.162767039674466e-05,
      "loss": 3.0745,
      "step": 16460
    },
    {
      "epoch": 8.377416073245168,
      "grad_norm": 12.055179595947266,
      "learning_rate": 4.162258392675483e-05,
      "loss": 3.0207,
      "step": 16470
    },
    {
      "epoch": 8.382502543234995,
      "grad_norm": 16.617124557495117,
      "learning_rate": 4.161749745676501e-05,
      "loss": 3.0873,
      "step": 16480
    },
    {
      "epoch": 8.387589013224822,
      "grad_norm": 14.540626525878906,
      "learning_rate": 4.161241098677518e-05,
      "loss": 3.1158,
      "step": 16490
    },
    {
      "epoch": 8.39267548321465,
      "grad_norm": 14.954989433288574,
      "learning_rate": 4.160732451678535e-05,
      "loss": 3.1259,
      "step": 16500
    },
    {
      "epoch": 8.397761953204476,
      "grad_norm": 18.768104553222656,
      "learning_rate": 4.1602238046795525e-05,
      "loss": 2.9916,
      "step": 16510
    },
    {
      "epoch": 8.402848423194303,
      "grad_norm": 12.995193481445312,
      "learning_rate": 4.15971515768057e-05,
      "loss": 3.0448,
      "step": 16520
    },
    {
      "epoch": 8.40793489318413,
      "grad_norm": 17.57463264465332,
      "learning_rate": 4.159206510681587e-05,
      "loss": 2.9723,
      "step": 16530
    },
    {
      "epoch": 8.413021363173957,
      "grad_norm": 16.133718490600586,
      "learning_rate": 4.158697863682605e-05,
      "loss": 3.0489,
      "step": 16540
    },
    {
      "epoch": 8.418107833163784,
      "grad_norm": 13.860818862915039,
      "learning_rate": 4.158189216683622e-05,
      "loss": 3.0366,
      "step": 16550
    },
    {
      "epoch": 8.423194303153611,
      "grad_norm": 16.154043197631836,
      "learning_rate": 4.157680569684639e-05,
      "loss": 3.1236,
      "step": 16560
    },
    {
      "epoch": 8.428280773143438,
      "grad_norm": 17.254810333251953,
      "learning_rate": 4.1571719226856564e-05,
      "loss": 3.0631,
      "step": 16570
    },
    {
      "epoch": 8.433367243133265,
      "grad_norm": 16.39348602294922,
      "learning_rate": 4.1566632756866734e-05,
      "loss": 3.0426,
      "step": 16580
    },
    {
      "epoch": 8.438453713123092,
      "grad_norm": 18.45505714416504,
      "learning_rate": 4.1561546286876904e-05,
      "loss": 3.0546,
      "step": 16590
    },
    {
      "epoch": 8.443540183112919,
      "grad_norm": 12.306584358215332,
      "learning_rate": 4.155645981688708e-05,
      "loss": 2.9916,
      "step": 16600
    },
    {
      "epoch": 8.448626653102746,
      "grad_norm": 17.18433952331543,
      "learning_rate": 4.155137334689726e-05,
      "loss": 3.0231,
      "step": 16610
    },
    {
      "epoch": 8.453713123092573,
      "grad_norm": 14.220017433166504,
      "learning_rate": 4.1546286876907434e-05,
      "loss": 2.9899,
      "step": 16620
    },
    {
      "epoch": 8.4587995930824,
      "grad_norm": 15.66584587097168,
      "learning_rate": 4.1541200406917604e-05,
      "loss": 3.0457,
      "step": 16630
    },
    {
      "epoch": 8.463886063072227,
      "grad_norm": 13.766162872314453,
      "learning_rate": 4.1536113936927774e-05,
      "loss": 3.0027,
      "step": 16640
    },
    {
      "epoch": 8.468972533062056,
      "grad_norm": 15.919840812683105,
      "learning_rate": 4.153102746693795e-05,
      "loss": 3.0014,
      "step": 16650
    },
    {
      "epoch": 8.474059003051883,
      "grad_norm": 15.541712760925293,
      "learning_rate": 4.152594099694812e-05,
      "loss": 3.0988,
      "step": 16660
    },
    {
      "epoch": 8.47914547304171,
      "grad_norm": 12.615327835083008,
      "learning_rate": 4.152085452695829e-05,
      "loss": 3.0228,
      "step": 16670
    },
    {
      "epoch": 8.484231943031537,
      "grad_norm": 13.184610366821289,
      "learning_rate": 4.151576805696847e-05,
      "loss": 3.0776,
      "step": 16680
    },
    {
      "epoch": 8.489318413021364,
      "grad_norm": 16.35489845275879,
      "learning_rate": 4.151068158697864e-05,
      "loss": 2.9874,
      "step": 16690
    },
    {
      "epoch": 8.49440488301119,
      "grad_norm": 12.653728485107422,
      "learning_rate": 4.1505595116988814e-05,
      "loss": 3.0937,
      "step": 16700
    },
    {
      "epoch": 8.499491353001018,
      "grad_norm": 20.02229881286621,
      "learning_rate": 4.1500508646998983e-05,
      "loss": 3.1151,
      "step": 16710
    },
    {
      "epoch": 8.504577822990845,
      "grad_norm": 12.676877975463867,
      "learning_rate": 4.149542217700916e-05,
      "loss": 2.9142,
      "step": 16720
    },
    {
      "epoch": 8.509664292980672,
      "grad_norm": 12.319001197814941,
      "learning_rate": 4.149033570701933e-05,
      "loss": 3.0722,
      "step": 16730
    },
    {
      "epoch": 8.514750762970499,
      "grad_norm": 15.592782020568848,
      "learning_rate": 4.148524923702951e-05,
      "loss": 3.1184,
      "step": 16740
    },
    {
      "epoch": 8.519837232960326,
      "grad_norm": 18.743465423583984,
      "learning_rate": 4.1480162767039677e-05,
      "loss": 2.9618,
      "step": 16750
    },
    {
      "epoch": 8.524923702950153,
      "grad_norm": 17.721641540527344,
      "learning_rate": 4.1475076297049846e-05,
      "loss": 3.0926,
      "step": 16760
    },
    {
      "epoch": 8.53001017293998,
      "grad_norm": 17.021787643432617,
      "learning_rate": 4.146998982706002e-05,
      "loss": 3.1025,
      "step": 16770
    },
    {
      "epoch": 8.535096642929807,
      "grad_norm": 14.084356307983398,
      "learning_rate": 4.146490335707019e-05,
      "loss": 3.0083,
      "step": 16780
    },
    {
      "epoch": 8.540183112919634,
      "grad_norm": 16.171903610229492,
      "learning_rate": 4.145981688708036e-05,
      "loss": 3.0492,
      "step": 16790
    },
    {
      "epoch": 8.54526958290946,
      "grad_norm": 14.904053688049316,
      "learning_rate": 4.145473041709054e-05,
      "loss": 3.0582,
      "step": 16800
    },
    {
      "epoch": 8.550356052899287,
      "grad_norm": 13.989822387695312,
      "learning_rate": 4.1449643947100716e-05,
      "loss": 3.0326,
      "step": 16810
    },
    {
      "epoch": 8.555442522889114,
      "grad_norm": 16.354724884033203,
      "learning_rate": 4.1444557477110886e-05,
      "loss": 2.9541,
      "step": 16820
    },
    {
      "epoch": 8.560528992878941,
      "grad_norm": 17.048316955566406,
      "learning_rate": 4.143947100712106e-05,
      "loss": 2.9776,
      "step": 16830
    },
    {
      "epoch": 8.565615462868768,
      "grad_norm": 12.72722339630127,
      "learning_rate": 4.143438453713123e-05,
      "loss": 2.9851,
      "step": 16840
    },
    {
      "epoch": 8.570701932858595,
      "grad_norm": 17.762733459472656,
      "learning_rate": 4.14292980671414e-05,
      "loss": 2.9403,
      "step": 16850
    },
    {
      "epoch": 8.575788402848422,
      "grad_norm": 14.546319961547852,
      "learning_rate": 4.142421159715158e-05,
      "loss": 3.0608,
      "step": 16860
    },
    {
      "epoch": 8.580874872838251,
      "grad_norm": 14.069843292236328,
      "learning_rate": 4.141912512716175e-05,
      "loss": 3.0253,
      "step": 16870
    },
    {
      "epoch": 8.585961342828078,
      "grad_norm": 13.212080001831055,
      "learning_rate": 4.1414038657171926e-05,
      "loss": 2.9136,
      "step": 16880
    },
    {
      "epoch": 8.591047812817905,
      "grad_norm": 14.664051055908203,
      "learning_rate": 4.1408952187182096e-05,
      "loss": 2.9791,
      "step": 16890
    },
    {
      "epoch": 8.596134282807732,
      "grad_norm": 20.06330680847168,
      "learning_rate": 4.140386571719227e-05,
      "loss": 2.9958,
      "step": 16900
    },
    {
      "epoch": 8.601220752797559,
      "grad_norm": 17.220691680908203,
      "learning_rate": 4.139877924720245e-05,
      "loss": 3.0157,
      "step": 16910
    },
    {
      "epoch": 8.606307222787386,
      "grad_norm": 13.51904296875,
      "learning_rate": 4.139369277721262e-05,
      "loss": 2.9758,
      "step": 16920
    },
    {
      "epoch": 8.611393692777213,
      "grad_norm": 15.547951698303223,
      "learning_rate": 4.138860630722279e-05,
      "loss": 2.9708,
      "step": 16930
    },
    {
      "epoch": 8.61648016276704,
      "grad_norm": 19.8306941986084,
      "learning_rate": 4.1383519837232966e-05,
      "loss": 3.0131,
      "step": 16940
    },
    {
      "epoch": 8.621566632756867,
      "grad_norm": 13.331671714782715,
      "learning_rate": 4.1378433367243135e-05,
      "loss": 2.9293,
      "step": 16950
    },
    {
      "epoch": 8.626653102746694,
      "grad_norm": 14.50127124786377,
      "learning_rate": 4.1373346897253305e-05,
      "loss": 2.9191,
      "step": 16960
    },
    {
      "epoch": 8.631739572736521,
      "grad_norm": 17.7882137298584,
      "learning_rate": 4.136826042726348e-05,
      "loss": 2.9778,
      "step": 16970
    },
    {
      "epoch": 8.636826042726348,
      "grad_norm": 19.910341262817383,
      "learning_rate": 4.136317395727365e-05,
      "loss": 2.9557,
      "step": 16980
    },
    {
      "epoch": 8.641912512716175,
      "grad_norm": 14.46020793914795,
      "learning_rate": 4.135808748728383e-05,
      "loss": 2.9815,
      "step": 16990
    },
    {
      "epoch": 8.646998982706002,
      "grad_norm": 23.579545974731445,
      "learning_rate": 4.1353001017294005e-05,
      "loss": 3.0085,
      "step": 17000
    },
    {
      "epoch": 8.652085452695829,
      "grad_norm": 20.15991973876953,
      "learning_rate": 4.1347914547304175e-05,
      "loss": 3.0104,
      "step": 17010
    },
    {
      "epoch": 8.657171922685656,
      "grad_norm": 14.355549812316895,
      "learning_rate": 4.1342828077314345e-05,
      "loss": 2.9644,
      "step": 17020
    },
    {
      "epoch": 8.662258392675483,
      "grad_norm": 14.034157752990723,
      "learning_rate": 4.133774160732452e-05,
      "loss": 2.9565,
      "step": 17030
    },
    {
      "epoch": 8.66734486266531,
      "grad_norm": 17.430397033691406,
      "learning_rate": 4.133265513733469e-05,
      "loss": 2.9868,
      "step": 17040
    },
    {
      "epoch": 8.672431332655137,
      "grad_norm": 12.348099708557129,
      "learning_rate": 4.132756866734486e-05,
      "loss": 3.0487,
      "step": 17050
    },
    {
      "epoch": 8.677517802644964,
      "grad_norm": 16.873287200927734,
      "learning_rate": 4.132248219735504e-05,
      "loss": 2.9188,
      "step": 17060
    },
    {
      "epoch": 8.68260427263479,
      "grad_norm": 15.565933227539062,
      "learning_rate": 4.131739572736521e-05,
      "loss": 3.0655,
      "step": 17070
    },
    {
      "epoch": 8.687690742624618,
      "grad_norm": 13.667984008789062,
      "learning_rate": 4.131230925737538e-05,
      "loss": 2.8695,
      "step": 17080
    },
    {
      "epoch": 8.692777212614445,
      "grad_norm": 15.636825561523438,
      "learning_rate": 4.1307222787385555e-05,
      "loss": 3.0126,
      "step": 17090
    },
    {
      "epoch": 8.697863682604273,
      "grad_norm": 15.596190452575684,
      "learning_rate": 4.130213631739573e-05,
      "loss": 2.9783,
      "step": 17100
    },
    {
      "epoch": 8.7029501525941,
      "grad_norm": 19.0146427154541,
      "learning_rate": 4.12970498474059e-05,
      "loss": 3.009,
      "step": 17110
    },
    {
      "epoch": 8.708036622583927,
      "grad_norm": 15.99878215789795,
      "learning_rate": 4.129196337741608e-05,
      "loss": 2.935,
      "step": 17120
    },
    {
      "epoch": 8.713123092573754,
      "grad_norm": 13.142971992492676,
      "learning_rate": 4.128687690742625e-05,
      "loss": 3.0086,
      "step": 17130
    },
    {
      "epoch": 8.718209562563581,
      "grad_norm": 16.88128662109375,
      "learning_rate": 4.128179043743642e-05,
      "loss": 2.9527,
      "step": 17140
    },
    {
      "epoch": 8.723296032553408,
      "grad_norm": 15.058663368225098,
      "learning_rate": 4.1276703967446594e-05,
      "loss": 2.998,
      "step": 17150
    },
    {
      "epoch": 8.728382502543235,
      "grad_norm": 17.070297241210938,
      "learning_rate": 4.1271617497456764e-05,
      "loss": 2.9453,
      "step": 17160
    },
    {
      "epoch": 8.733468972533062,
      "grad_norm": 15.97370719909668,
      "learning_rate": 4.126653102746694e-05,
      "loss": 3.0144,
      "step": 17170
    },
    {
      "epoch": 8.73855544252289,
      "grad_norm": 17.567724227905273,
      "learning_rate": 4.126144455747711e-05,
      "loss": 2.9867,
      "step": 17180
    },
    {
      "epoch": 8.743641912512716,
      "grad_norm": 12.011130332946777,
      "learning_rate": 4.125635808748729e-05,
      "loss": 3.0717,
      "step": 17190
    },
    {
      "epoch": 8.748728382502543,
      "grad_norm": 20.481313705444336,
      "learning_rate": 4.1251271617497464e-05,
      "loss": 2.9278,
      "step": 17200
    },
    {
      "epoch": 8.75381485249237,
      "grad_norm": 15.998044967651367,
      "learning_rate": 4.1246185147507634e-05,
      "loss": 2.9947,
      "step": 17210
    },
    {
      "epoch": 8.758901322482197,
      "grad_norm": 16.966455459594727,
      "learning_rate": 4.1241098677517804e-05,
      "loss": 3.0374,
      "step": 17220
    },
    {
      "epoch": 8.763987792472024,
      "grad_norm": 16.570022583007812,
      "learning_rate": 4.123601220752798e-05,
      "loss": 3.0693,
      "step": 17230
    },
    {
      "epoch": 8.769074262461851,
      "grad_norm": 22.078248977661133,
      "learning_rate": 4.123092573753815e-05,
      "loss": 3.0299,
      "step": 17240
    },
    {
      "epoch": 8.774160732451678,
      "grad_norm": 15.423684120178223,
      "learning_rate": 4.122583926754832e-05,
      "loss": 3.0593,
      "step": 17250
    },
    {
      "epoch": 8.779247202441505,
      "grad_norm": 15.940496444702148,
      "learning_rate": 4.12207527975585e-05,
      "loss": 2.9046,
      "step": 17260
    },
    {
      "epoch": 8.784333672431332,
      "grad_norm": 20.720308303833008,
      "learning_rate": 4.121566632756867e-05,
      "loss": 3.0042,
      "step": 17270
    },
    {
      "epoch": 8.789420142421159,
      "grad_norm": 15.317667961120605,
      "learning_rate": 4.1210579857578844e-05,
      "loss": 3.0101,
      "step": 17280
    },
    {
      "epoch": 8.794506612410986,
      "grad_norm": 17.565820693969727,
      "learning_rate": 4.120549338758902e-05,
      "loss": 2.9321,
      "step": 17290
    },
    {
      "epoch": 8.799593082400813,
      "grad_norm": 19.79435920715332,
      "learning_rate": 4.120040691759919e-05,
      "loss": 2.9684,
      "step": 17300
    },
    {
      "epoch": 8.804679552390642,
      "grad_norm": 12.858826637268066,
      "learning_rate": 4.119532044760936e-05,
      "loss": 3.0448,
      "step": 17310
    },
    {
      "epoch": 8.809766022380469,
      "grad_norm": 20.351491928100586,
      "learning_rate": 4.119023397761954e-05,
      "loss": 3.0066,
      "step": 17320
    },
    {
      "epoch": 8.814852492370296,
      "grad_norm": 15.441603660583496,
      "learning_rate": 4.1185147507629707e-05,
      "loss": 2.9326,
      "step": 17330
    },
    {
      "epoch": 8.819938962360123,
      "grad_norm": 18.070323944091797,
      "learning_rate": 4.1180061037639876e-05,
      "loss": 3.0019,
      "step": 17340
    },
    {
      "epoch": 8.82502543234995,
      "grad_norm": 17.06037712097168,
      "learning_rate": 4.117497456765005e-05,
      "loss": 3.0146,
      "step": 17350
    },
    {
      "epoch": 8.830111902339777,
      "grad_norm": 16.17291831970215,
      "learning_rate": 4.116988809766022e-05,
      "loss": 2.9394,
      "step": 17360
    },
    {
      "epoch": 8.835198372329604,
      "grad_norm": 15.980441093444824,
      "learning_rate": 4.116480162767039e-05,
      "loss": 3.05,
      "step": 17370
    },
    {
      "epoch": 8.84028484231943,
      "grad_norm": 19.29876708984375,
      "learning_rate": 4.115971515768057e-05,
      "loss": 3.0292,
      "step": 17380
    },
    {
      "epoch": 8.845371312309258,
      "grad_norm": 23.342819213867188,
      "learning_rate": 4.1154628687690746e-05,
      "loss": 3.1063,
      "step": 17390
    },
    {
      "epoch": 8.850457782299085,
      "grad_norm": 18.4923038482666,
      "learning_rate": 4.1149542217700916e-05,
      "loss": 3.0361,
      "step": 17400
    },
    {
      "epoch": 8.855544252288912,
      "grad_norm": 14.132847785949707,
      "learning_rate": 4.114445574771109e-05,
      "loss": 2.9616,
      "step": 17410
    },
    {
      "epoch": 8.860630722278739,
      "grad_norm": 17.88593864440918,
      "learning_rate": 4.113936927772126e-05,
      "loss": 2.9095,
      "step": 17420
    },
    {
      "epoch": 8.865717192268566,
      "grad_norm": 18.4537353515625,
      "learning_rate": 4.113428280773144e-05,
      "loss": 3.0038,
      "step": 17430
    },
    {
      "epoch": 8.870803662258393,
      "grad_norm": 14.300372123718262,
      "learning_rate": 4.112919633774161e-05,
      "loss": 2.9878,
      "step": 17440
    },
    {
      "epoch": 8.87589013224822,
      "grad_norm": 18.620115280151367,
      "learning_rate": 4.112410986775178e-05,
      "loss": 2.8998,
      "step": 17450
    },
    {
      "epoch": 8.880976602238047,
      "grad_norm": 15.606851577758789,
      "learning_rate": 4.1119023397761956e-05,
      "loss": 2.9733,
      "step": 17460
    },
    {
      "epoch": 8.886063072227874,
      "grad_norm": 15.800910949707031,
      "learning_rate": 4.1113936927772126e-05,
      "loss": 3.0212,
      "step": 17470
    },
    {
      "epoch": 8.8911495422177,
      "grad_norm": 20.806673049926758,
      "learning_rate": 4.11088504577823e-05,
      "loss": 3.0577,
      "step": 17480
    },
    {
      "epoch": 8.896236012207527,
      "grad_norm": 13.35545825958252,
      "learning_rate": 4.110376398779248e-05,
      "loss": 3.0331,
      "step": 17490
    },
    {
      "epoch": 8.901322482197354,
      "grad_norm": 16.289955139160156,
      "learning_rate": 4.109867751780265e-05,
      "loss": 2.9766,
      "step": 17500
    },
    {
      "epoch": 8.906408952187181,
      "grad_norm": 15.72870922088623,
      "learning_rate": 4.109359104781282e-05,
      "loss": 2.9024,
      "step": 17510
    },
    {
      "epoch": 8.911495422177008,
      "grad_norm": 20.54204750061035,
      "learning_rate": 4.1088504577822996e-05,
      "loss": 2.9701,
      "step": 17520
    },
    {
      "epoch": 8.916581892166835,
      "grad_norm": 16.220869064331055,
      "learning_rate": 4.1083418107833165e-05,
      "loss": 2.9644,
      "step": 17530
    },
    {
      "epoch": 8.921668362156662,
      "grad_norm": 14.77046012878418,
      "learning_rate": 4.1078331637843335e-05,
      "loss": 3.0602,
      "step": 17540
    },
    {
      "epoch": 8.926754832146491,
      "grad_norm": 19.673912048339844,
      "learning_rate": 4.107324516785351e-05,
      "loss": 3.0911,
      "step": 17550
    },
    {
      "epoch": 8.931841302136318,
      "grad_norm": 18.117366790771484,
      "learning_rate": 4.106815869786368e-05,
      "loss": 2.9252,
      "step": 17560
    },
    {
      "epoch": 8.936927772126145,
      "grad_norm": 16.33839225769043,
      "learning_rate": 4.106307222787386e-05,
      "loss": 2.8998,
      "step": 17570
    },
    {
      "epoch": 8.942014242115972,
      "grad_norm": 20.71544647216797,
      "learning_rate": 4.1057985757884035e-05,
      "loss": 2.9542,
      "step": 17580
    },
    {
      "epoch": 8.947100712105799,
      "grad_norm": 16.96527671813965,
      "learning_rate": 4.1052899287894205e-05,
      "loss": 3.0082,
      "step": 17590
    },
    {
      "epoch": 8.952187182095626,
      "grad_norm": 13.522089958190918,
      "learning_rate": 4.1047812817904375e-05,
      "loss": 2.993,
      "step": 17600
    },
    {
      "epoch": 8.957273652085453,
      "grad_norm": 21.904705047607422,
      "learning_rate": 4.104272634791455e-05,
      "loss": 2.9838,
      "step": 17610
    },
    {
      "epoch": 8.96236012207528,
      "grad_norm": 15.635845184326172,
      "learning_rate": 4.103763987792472e-05,
      "loss": 3.005,
      "step": 17620
    },
    {
      "epoch": 8.967446592065107,
      "grad_norm": 13.1416597366333,
      "learning_rate": 4.103255340793489e-05,
      "loss": 3.0006,
      "step": 17630
    },
    {
      "epoch": 8.972533062054934,
      "grad_norm": 17.022314071655273,
      "learning_rate": 4.102746693794507e-05,
      "loss": 2.9426,
      "step": 17640
    },
    {
      "epoch": 8.977619532044761,
      "grad_norm": 18.365243911743164,
      "learning_rate": 4.102238046795524e-05,
      "loss": 2.9256,
      "step": 17650
    },
    {
      "epoch": 8.982706002034588,
      "grad_norm": 14.82380485534668,
      "learning_rate": 4.1017293997965415e-05,
      "loss": 3.0053,
      "step": 17660
    },
    {
      "epoch": 8.987792472024415,
      "grad_norm": 14.04443359375,
      "learning_rate": 4.1012207527975585e-05,
      "loss": 2.9872,
      "step": 17670
    },
    {
      "epoch": 8.992878942014242,
      "grad_norm": 17.036373138427734,
      "learning_rate": 4.100712105798576e-05,
      "loss": 3.0809,
      "step": 17680
    },
    {
      "epoch": 8.997965412004069,
      "grad_norm": 15.37876033782959,
      "learning_rate": 4.100203458799594e-05,
      "loss": 2.943,
      "step": 17690
    },
    {
      "epoch": 9.0,
      "eval_loss": 3.7648251056671143,
      "eval_runtime": 2.7349,
      "eval_samples_per_second": 1014.679,
      "eval_steps_per_second": 126.881,
      "step": 17694
    },
    {
      "epoch": 9.003051881993896,
      "grad_norm": 17.7829647064209,
      "learning_rate": 4.099694811800611e-05,
      "loss": 2.8911,
      "step": 17700
    },
    {
      "epoch": 9.008138351983723,
      "grad_norm": 14.267013549804688,
      "learning_rate": 4.099186164801628e-05,
      "loss": 2.9277,
      "step": 17710
    },
    {
      "epoch": 9.01322482197355,
      "grad_norm": 14.123200416564941,
      "learning_rate": 4.0986775178026454e-05,
      "loss": 2.9864,
      "step": 17720
    },
    {
      "epoch": 9.018311291963377,
      "grad_norm": 15.370793342590332,
      "learning_rate": 4.0981688708036624e-05,
      "loss": 2.9413,
      "step": 17730
    },
    {
      "epoch": 9.023397761953204,
      "grad_norm": 15.06745719909668,
      "learning_rate": 4.0976602238046794e-05,
      "loss": 2.931,
      "step": 17740
    },
    {
      "epoch": 9.02848423194303,
      "grad_norm": 15.101973533630371,
      "learning_rate": 4.097151576805697e-05,
      "loss": 2.9934,
      "step": 17750
    },
    {
      "epoch": 9.033570701932858,
      "grad_norm": 20.866804122924805,
      "learning_rate": 4.096642929806714e-05,
      "loss": 2.8912,
      "step": 17760
    },
    {
      "epoch": 9.038657171922686,
      "grad_norm": 19.9517879486084,
      "learning_rate": 4.096134282807732e-05,
      "loss": 2.9963,
      "step": 17770
    },
    {
      "epoch": 9.043743641912513,
      "grad_norm": 13.274077415466309,
      "learning_rate": 4.0956256358087494e-05,
      "loss": 2.9767,
      "step": 17780
    },
    {
      "epoch": 9.04883011190234,
      "grad_norm": 16.494768142700195,
      "learning_rate": 4.0951169888097664e-05,
      "loss": 2.8859,
      "step": 17790
    },
    {
      "epoch": 9.053916581892167,
      "grad_norm": 16.354982376098633,
      "learning_rate": 4.0946083418107834e-05,
      "loss": 2.9448,
      "step": 17800
    },
    {
      "epoch": 9.059003051881994,
      "grad_norm": 17.369049072265625,
      "learning_rate": 4.094099694811801e-05,
      "loss": 2.9271,
      "step": 17810
    },
    {
      "epoch": 9.064089521871821,
      "grad_norm": 16.241147994995117,
      "learning_rate": 4.093591047812818e-05,
      "loss": 2.974,
      "step": 17820
    },
    {
      "epoch": 9.069175991861648,
      "grad_norm": 15.53996467590332,
      "learning_rate": 4.093082400813835e-05,
      "loss": 2.8884,
      "step": 17830
    },
    {
      "epoch": 9.074262461851475,
      "grad_norm": 12.90694808959961,
      "learning_rate": 4.092573753814853e-05,
      "loss": 3.0033,
      "step": 17840
    },
    {
      "epoch": 9.079348931841302,
      "grad_norm": 14.670012474060059,
      "learning_rate": 4.09206510681587e-05,
      "loss": 2.9918,
      "step": 17850
    },
    {
      "epoch": 9.08443540183113,
      "grad_norm": 14.221627235412598,
      "learning_rate": 4.0915564598168874e-05,
      "loss": 2.9122,
      "step": 17860
    },
    {
      "epoch": 9.089521871820956,
      "grad_norm": 15.872110366821289,
      "learning_rate": 4.091047812817905e-05,
      "loss": 3.0225,
      "step": 17870
    },
    {
      "epoch": 9.094608341810783,
      "grad_norm": 15.102648735046387,
      "learning_rate": 4.090539165818922e-05,
      "loss": 3.0213,
      "step": 17880
    },
    {
      "epoch": 9.09969481180061,
      "grad_norm": 13.407626152038574,
      "learning_rate": 4.090030518819939e-05,
      "loss": 3.0386,
      "step": 17890
    },
    {
      "epoch": 9.104781281790437,
      "grad_norm": 16.45392417907715,
      "learning_rate": 4.089521871820957e-05,
      "loss": 2.9963,
      "step": 17900
    },
    {
      "epoch": 9.109867751780264,
      "grad_norm": 16.120441436767578,
      "learning_rate": 4.0890132248219737e-05,
      "loss": 2.9238,
      "step": 17910
    },
    {
      "epoch": 9.114954221770091,
      "grad_norm": 15.291790008544922,
      "learning_rate": 4.0885045778229906e-05,
      "loss": 2.9467,
      "step": 17920
    },
    {
      "epoch": 9.120040691759918,
      "grad_norm": 14.26624584197998,
      "learning_rate": 4.087995930824008e-05,
      "loss": 2.9571,
      "step": 17930
    },
    {
      "epoch": 9.125127161749745,
      "grad_norm": 14.542678833007812,
      "learning_rate": 4.087487283825025e-05,
      "loss": 2.9465,
      "step": 17940
    },
    {
      "epoch": 9.130213631739572,
      "grad_norm": 16.622230529785156,
      "learning_rate": 4.086978636826043e-05,
      "loss": 2.9489,
      "step": 17950
    },
    {
      "epoch": 9.135300101729399,
      "grad_norm": 25.576419830322266,
      "learning_rate": 4.0864699898270606e-05,
      "loss": 3.0124,
      "step": 17960
    },
    {
      "epoch": 9.140386571719226,
      "grad_norm": 17.73505210876465,
      "learning_rate": 4.0859613428280776e-05,
      "loss": 2.9261,
      "step": 17970
    },
    {
      "epoch": 9.145473041709053,
      "grad_norm": 16.22924041748047,
      "learning_rate": 4.085452695829095e-05,
      "loss": 2.9122,
      "step": 17980
    },
    {
      "epoch": 9.150559511698882,
      "grad_norm": 21.836210250854492,
      "learning_rate": 4.084944048830112e-05,
      "loss": 2.9186,
      "step": 17990
    },
    {
      "epoch": 9.155645981688709,
      "grad_norm": 19.82320213317871,
      "learning_rate": 4.084435401831129e-05,
      "loss": 2.951,
      "step": 18000
    },
    {
      "epoch": 9.160732451678536,
      "grad_norm": 16.145103454589844,
      "learning_rate": 4.083926754832147e-05,
      "loss": 2.9757,
      "step": 18010
    },
    {
      "epoch": 9.165818921668363,
      "grad_norm": 16.55875015258789,
      "learning_rate": 4.083418107833164e-05,
      "loss": 2.9271,
      "step": 18020
    },
    {
      "epoch": 9.17090539165819,
      "grad_norm": 15.430570602416992,
      "learning_rate": 4.082909460834181e-05,
      "loss": 2.9215,
      "step": 18030
    },
    {
      "epoch": 9.175991861648017,
      "grad_norm": 15.887337684631348,
      "learning_rate": 4.0824008138351986e-05,
      "loss": 2.8998,
      "step": 18040
    },
    {
      "epoch": 9.181078331637844,
      "grad_norm": 20.951013565063477,
      "learning_rate": 4.0818921668362156e-05,
      "loss": 2.9931,
      "step": 18050
    },
    {
      "epoch": 9.18616480162767,
      "grad_norm": 19.15635108947754,
      "learning_rate": 4.081383519837233e-05,
      "loss": 2.9493,
      "step": 18060
    },
    {
      "epoch": 9.191251271617498,
      "grad_norm": 16.67058753967285,
      "learning_rate": 4.080874872838251e-05,
      "loss": 2.9227,
      "step": 18070
    },
    {
      "epoch": 9.196337741607325,
      "grad_norm": 15.965582847595215,
      "learning_rate": 4.080366225839268e-05,
      "loss": 2.9348,
      "step": 18080
    },
    {
      "epoch": 9.201424211597152,
      "grad_norm": 15.072225570678711,
      "learning_rate": 4.079857578840285e-05,
      "loss": 2.8882,
      "step": 18090
    },
    {
      "epoch": 9.206510681586979,
      "grad_norm": 15.978683471679688,
      "learning_rate": 4.0793489318413026e-05,
      "loss": 2.9415,
      "step": 18100
    },
    {
      "epoch": 9.211597151576806,
      "grad_norm": 16.63453483581543,
      "learning_rate": 4.0788402848423195e-05,
      "loss": 2.9039,
      "step": 18110
    },
    {
      "epoch": 9.216683621566633,
      "grad_norm": 22.186405181884766,
      "learning_rate": 4.0783316378433365e-05,
      "loss": 2.996,
      "step": 18120
    },
    {
      "epoch": 9.22177009155646,
      "grad_norm": 13.71848201751709,
      "learning_rate": 4.077822990844354e-05,
      "loss": 2.9792,
      "step": 18130
    },
    {
      "epoch": 9.226856561546287,
      "grad_norm": 16.610483169555664,
      "learning_rate": 4.077314343845371e-05,
      "loss": 3.0283,
      "step": 18140
    },
    {
      "epoch": 9.231943031536114,
      "grad_norm": 18.550838470458984,
      "learning_rate": 4.076805696846389e-05,
      "loss": 2.9293,
      "step": 18150
    },
    {
      "epoch": 9.23702950152594,
      "grad_norm": 17.03409194946289,
      "learning_rate": 4.0762970498474065e-05,
      "loss": 2.8837,
      "step": 18160
    },
    {
      "epoch": 9.242115971515767,
      "grad_norm": 13.36514663696289,
      "learning_rate": 4.0757884028484235e-05,
      "loss": 2.9408,
      "step": 18170
    },
    {
      "epoch": 9.247202441505594,
      "grad_norm": 17.290061950683594,
      "learning_rate": 4.0752797558494405e-05,
      "loss": 2.9595,
      "step": 18180
    },
    {
      "epoch": 9.252288911495421,
      "grad_norm": 20.916759490966797,
      "learning_rate": 4.074771108850458e-05,
      "loss": 2.9405,
      "step": 18190
    },
    {
      "epoch": 9.257375381485248,
      "grad_norm": 20.62480926513672,
      "learning_rate": 4.074262461851475e-05,
      "loss": 2.9768,
      "step": 18200
    },
    {
      "epoch": 9.262461851475077,
      "grad_norm": 17.7508544921875,
      "learning_rate": 4.073753814852492e-05,
      "loss": 2.9749,
      "step": 18210
    },
    {
      "epoch": 9.267548321464904,
      "grad_norm": 19.074325561523438,
      "learning_rate": 4.07324516785351e-05,
      "loss": 2.8463,
      "step": 18220
    },
    {
      "epoch": 9.272634791454731,
      "grad_norm": 17.712228775024414,
      "learning_rate": 4.072736520854527e-05,
      "loss": 2.9797,
      "step": 18230
    },
    {
      "epoch": 9.277721261444558,
      "grad_norm": 19.865915298461914,
      "learning_rate": 4.0722278738555445e-05,
      "loss": 2.9089,
      "step": 18240
    },
    {
      "epoch": 9.282807731434385,
      "grad_norm": 17.875886917114258,
      "learning_rate": 4.071719226856562e-05,
      "loss": 2.9654,
      "step": 18250
    },
    {
      "epoch": 9.287894201424212,
      "grad_norm": 19.825864791870117,
      "learning_rate": 4.071210579857579e-05,
      "loss": 2.9573,
      "step": 18260
    },
    {
      "epoch": 9.292980671414039,
      "grad_norm": 19.232755661010742,
      "learning_rate": 4.070701932858597e-05,
      "loss": 2.9365,
      "step": 18270
    },
    {
      "epoch": 9.298067141403866,
      "grad_norm": 14.7161226272583,
      "learning_rate": 4.070193285859614e-05,
      "loss": 2.9525,
      "step": 18280
    },
    {
      "epoch": 9.303153611393693,
      "grad_norm": 18.262592315673828,
      "learning_rate": 4.069684638860631e-05,
      "loss": 2.94,
      "step": 18290
    },
    {
      "epoch": 9.30824008138352,
      "grad_norm": 23.084165573120117,
      "learning_rate": 4.0691759918616484e-05,
      "loss": 3.0065,
      "step": 18300
    },
    {
      "epoch": 9.313326551373347,
      "grad_norm": 17.025537490844727,
      "learning_rate": 4.0686673448626654e-05,
      "loss": 2.9314,
      "step": 18310
    },
    {
      "epoch": 9.318413021363174,
      "grad_norm": 21.51336097717285,
      "learning_rate": 4.0681586978636824e-05,
      "loss": 2.8992,
      "step": 18320
    },
    {
      "epoch": 9.323499491353001,
      "grad_norm": 19.373451232910156,
      "learning_rate": 4.0676500508647e-05,
      "loss": 2.9642,
      "step": 18330
    },
    {
      "epoch": 9.328585961342828,
      "grad_norm": 20.07758903503418,
      "learning_rate": 4.067141403865717e-05,
      "loss": 2.9567,
      "step": 18340
    },
    {
      "epoch": 9.333672431332655,
      "grad_norm": 14.597077369689941,
      "learning_rate": 4.066632756866735e-05,
      "loss": 2.9577,
      "step": 18350
    },
    {
      "epoch": 9.338758901322482,
      "grad_norm": 14.759299278259277,
      "learning_rate": 4.0661241098677524e-05,
      "loss": 2.9505,
      "step": 18360
    },
    {
      "epoch": 9.343845371312309,
      "grad_norm": 17.109458923339844,
      "learning_rate": 4.0656154628687694e-05,
      "loss": 2.9699,
      "step": 18370
    },
    {
      "epoch": 9.348931841302136,
      "grad_norm": 13.980874061584473,
      "learning_rate": 4.0651068158697864e-05,
      "loss": 3.0017,
      "step": 18380
    },
    {
      "epoch": 9.354018311291963,
      "grad_norm": 16.561677932739258,
      "learning_rate": 4.064598168870804e-05,
      "loss": 2.8903,
      "step": 18390
    },
    {
      "epoch": 9.35910478128179,
      "grad_norm": 17.990833282470703,
      "learning_rate": 4.064089521871821e-05,
      "loss": 2.9016,
      "step": 18400
    },
    {
      "epoch": 9.364191251271617,
      "grad_norm": 21.87836265563965,
      "learning_rate": 4.063580874872838e-05,
      "loss": 2.8519,
      "step": 18410
    },
    {
      "epoch": 9.369277721261444,
      "grad_norm": 18.470766067504883,
      "learning_rate": 4.063072227873856e-05,
      "loss": 2.9446,
      "step": 18420
    },
    {
      "epoch": 9.37436419125127,
      "grad_norm": 17.204219818115234,
      "learning_rate": 4.062563580874873e-05,
      "loss": 2.9099,
      "step": 18430
    },
    {
      "epoch": 9.3794506612411,
      "grad_norm": 13.91237735748291,
      "learning_rate": 4.0620549338758904e-05,
      "loss": 2.9076,
      "step": 18440
    },
    {
      "epoch": 9.384537131230926,
      "grad_norm": 18.96249008178711,
      "learning_rate": 4.061546286876908e-05,
      "loss": 2.8732,
      "step": 18450
    },
    {
      "epoch": 9.389623601220753,
      "grad_norm": 17.932819366455078,
      "learning_rate": 4.061037639877925e-05,
      "loss": 2.9077,
      "step": 18460
    },
    {
      "epoch": 9.39471007121058,
      "grad_norm": 16.82056999206543,
      "learning_rate": 4.060528992878942e-05,
      "loss": 2.9714,
      "step": 18470
    },
    {
      "epoch": 9.399796541200407,
      "grad_norm": 16.496986389160156,
      "learning_rate": 4.06002034587996e-05,
      "loss": 2.8816,
      "step": 18480
    },
    {
      "epoch": 9.404883011190234,
      "grad_norm": 19.639240264892578,
      "learning_rate": 4.0595116988809767e-05,
      "loss": 2.9472,
      "step": 18490
    },
    {
      "epoch": 9.409969481180061,
      "grad_norm": 21.24271011352539,
      "learning_rate": 4.059003051881994e-05,
      "loss": 2.9969,
      "step": 18500
    },
    {
      "epoch": 9.415055951169888,
      "grad_norm": 18.974445343017578,
      "learning_rate": 4.058494404883011e-05,
      "loss": 2.9489,
      "step": 18510
    },
    {
      "epoch": 9.420142421159715,
      "grad_norm": 16.626230239868164,
      "learning_rate": 4.057985757884028e-05,
      "loss": 2.8616,
      "step": 18520
    },
    {
      "epoch": 9.425228891149542,
      "grad_norm": 13.693256378173828,
      "learning_rate": 4.057477110885046e-05,
      "loss": 2.9272,
      "step": 18530
    },
    {
      "epoch": 9.43031536113937,
      "grad_norm": 15.685395240783691,
      "learning_rate": 4.0569684638860636e-05,
      "loss": 2.9391,
      "step": 18540
    },
    {
      "epoch": 9.435401831129196,
      "grad_norm": 20.713993072509766,
      "learning_rate": 4.0564598168870806e-05,
      "loss": 2.8977,
      "step": 18550
    },
    {
      "epoch": 9.440488301119023,
      "grad_norm": 19.910724639892578,
      "learning_rate": 4.055951169888098e-05,
      "loss": 2.9742,
      "step": 18560
    },
    {
      "epoch": 9.44557477110885,
      "grad_norm": 16.381898880004883,
      "learning_rate": 4.055442522889115e-05,
      "loss": 2.8858,
      "step": 18570
    },
    {
      "epoch": 9.450661241098677,
      "grad_norm": 15.660425186157227,
      "learning_rate": 4.054933875890132e-05,
      "loss": 2.8886,
      "step": 18580
    },
    {
      "epoch": 9.455747711088504,
      "grad_norm": 17.879806518554688,
      "learning_rate": 4.05442522889115e-05,
      "loss": 2.8393,
      "step": 18590
    },
    {
      "epoch": 9.460834181078331,
      "grad_norm": 21.1641902923584,
      "learning_rate": 4.053916581892167e-05,
      "loss": 2.9119,
      "step": 18600
    },
    {
      "epoch": 9.465920651068158,
      "grad_norm": 22.572519302368164,
      "learning_rate": 4.053407934893184e-05,
      "loss": 2.9116,
      "step": 18610
    },
    {
      "epoch": 9.471007121057985,
      "grad_norm": 14.557098388671875,
      "learning_rate": 4.0528992878942016e-05,
      "loss": 2.957,
      "step": 18620
    },
    {
      "epoch": 9.476093591047812,
      "grad_norm": 19.768850326538086,
      "learning_rate": 4.052390640895219e-05,
      "loss": 2.8919,
      "step": 18630
    },
    {
      "epoch": 9.481180061037639,
      "grad_norm": 22.795856475830078,
      "learning_rate": 4.051881993896236e-05,
      "loss": 2.909,
      "step": 18640
    },
    {
      "epoch": 9.486266531027466,
      "grad_norm": 15.900467872619629,
      "learning_rate": 4.051373346897254e-05,
      "loss": 2.9448,
      "step": 18650
    },
    {
      "epoch": 9.491353001017295,
      "grad_norm": 16.397192001342773,
      "learning_rate": 4.050864699898271e-05,
      "loss": 2.9492,
      "step": 18660
    },
    {
      "epoch": 9.496439471007122,
      "grad_norm": 15.828567504882812,
      "learning_rate": 4.050356052899288e-05,
      "loss": 2.8706,
      "step": 18670
    },
    {
      "epoch": 9.501525940996949,
      "grad_norm": 15.133334159851074,
      "learning_rate": 4.0498474059003056e-05,
      "loss": 2.8621,
      "step": 18680
    },
    {
      "epoch": 9.506612410986776,
      "grad_norm": 15.994793891906738,
      "learning_rate": 4.0493387589013225e-05,
      "loss": 2.9012,
      "step": 18690
    },
    {
      "epoch": 9.511698880976603,
      "grad_norm": 17.67922592163086,
      "learning_rate": 4.0488301119023395e-05,
      "loss": 2.85,
      "step": 18700
    },
    {
      "epoch": 9.51678535096643,
      "grad_norm": 18.004226684570312,
      "learning_rate": 4.048321464903357e-05,
      "loss": 2.9333,
      "step": 18710
    },
    {
      "epoch": 9.521871820956257,
      "grad_norm": 15.1765718460083,
      "learning_rate": 4.047812817904374e-05,
      "loss": 2.9358,
      "step": 18720
    },
    {
      "epoch": 9.526958290946084,
      "grad_norm": 18.849733352661133,
      "learning_rate": 4.047304170905392e-05,
      "loss": 2.9112,
      "step": 18730
    },
    {
      "epoch": 9.53204476093591,
      "grad_norm": 17.988147735595703,
      "learning_rate": 4.0467955239064095e-05,
      "loss": 2.9333,
      "step": 18740
    },
    {
      "epoch": 9.537131230925738,
      "grad_norm": 20.227283477783203,
      "learning_rate": 4.0462868769074265e-05,
      "loss": 2.9691,
      "step": 18750
    },
    {
      "epoch": 9.542217700915565,
      "grad_norm": 16.03424835205078,
      "learning_rate": 4.0457782299084435e-05,
      "loss": 2.9168,
      "step": 18760
    },
    {
      "epoch": 9.547304170905392,
      "grad_norm": 20.843975067138672,
      "learning_rate": 4.045269582909461e-05,
      "loss": 2.8453,
      "step": 18770
    },
    {
      "epoch": 9.552390640895219,
      "grad_norm": 18.158153533935547,
      "learning_rate": 4.044760935910478e-05,
      "loss": 2.8931,
      "step": 18780
    },
    {
      "epoch": 9.557477110885046,
      "grad_norm": 16.399871826171875,
      "learning_rate": 4.044252288911496e-05,
      "loss": 2.9022,
      "step": 18790
    },
    {
      "epoch": 9.562563580874873,
      "grad_norm": 18.12413787841797,
      "learning_rate": 4.043743641912513e-05,
      "loss": 2.8654,
      "step": 18800
    },
    {
      "epoch": 9.5676500508647,
      "grad_norm": 20.36684799194336,
      "learning_rate": 4.04323499491353e-05,
      "loss": 2.9182,
      "step": 18810
    },
    {
      "epoch": 9.572736520854527,
      "grad_norm": 18.4255428314209,
      "learning_rate": 4.0427263479145475e-05,
      "loss": 2.9655,
      "step": 18820
    },
    {
      "epoch": 9.577822990844354,
      "grad_norm": 18.328353881835938,
      "learning_rate": 4.042217700915565e-05,
      "loss": 2.8809,
      "step": 18830
    },
    {
      "epoch": 9.58290946083418,
      "grad_norm": 18.094026565551758,
      "learning_rate": 4.041709053916582e-05,
      "loss": 2.897,
      "step": 18840
    },
    {
      "epoch": 9.587995930824007,
      "grad_norm": 14.818376541137695,
      "learning_rate": 4.0412004069176e-05,
      "loss": 2.979,
      "step": 18850
    },
    {
      "epoch": 9.593082400813834,
      "grad_norm": 20.03533172607422,
      "learning_rate": 4.040691759918617e-05,
      "loss": 2.88,
      "step": 18860
    },
    {
      "epoch": 9.598168870803661,
      "grad_norm": 19.287385940551758,
      "learning_rate": 4.040183112919634e-05,
      "loss": 2.9024,
      "step": 18870
    },
    {
      "epoch": 9.603255340793488,
      "grad_norm": 19.48023223876953,
      "learning_rate": 4.0396744659206514e-05,
      "loss": 2.9663,
      "step": 18880
    },
    {
      "epoch": 9.608341810783317,
      "grad_norm": 23.856060028076172,
      "learning_rate": 4.0391658189216684e-05,
      "loss": 2.7745,
      "step": 18890
    },
    {
      "epoch": 9.613428280773144,
      "grad_norm": 14.836688995361328,
      "learning_rate": 4.0386571719226854e-05,
      "loss": 2.9302,
      "step": 18900
    },
    {
      "epoch": 9.618514750762971,
      "grad_norm": 17.42951774597168,
      "learning_rate": 4.038148524923703e-05,
      "loss": 2.8918,
      "step": 18910
    },
    {
      "epoch": 9.623601220752798,
      "grad_norm": 17.56458282470703,
      "learning_rate": 4.037639877924721e-05,
      "loss": 2.8783,
      "step": 18920
    },
    {
      "epoch": 9.628687690742625,
      "grad_norm": 18.88770866394043,
      "learning_rate": 4.037131230925738e-05,
      "loss": 2.9138,
      "step": 18930
    },
    {
      "epoch": 9.633774160732452,
      "grad_norm": 18.5414981842041,
      "learning_rate": 4.0366225839267554e-05,
      "loss": 2.9108,
      "step": 18940
    },
    {
      "epoch": 9.638860630722279,
      "grad_norm": 16.51805305480957,
      "learning_rate": 4.0361139369277724e-05,
      "loss": 2.9671,
      "step": 18950
    },
    {
      "epoch": 9.643947100712106,
      "grad_norm": 16.78903579711914,
      "learning_rate": 4.0356052899287894e-05,
      "loss": 2.8928,
      "step": 18960
    },
    {
      "epoch": 9.649033570701933,
      "grad_norm": 20.07527732849121,
      "learning_rate": 4.035096642929807e-05,
      "loss": 2.8939,
      "step": 18970
    },
    {
      "epoch": 9.65412004069176,
      "grad_norm": 15.766014099121094,
      "learning_rate": 4.034587995930824e-05,
      "loss": 3.0198,
      "step": 18980
    },
    {
      "epoch": 9.659206510681587,
      "grad_norm": 16.06906509399414,
      "learning_rate": 4.034079348931841e-05,
      "loss": 3.0623,
      "step": 18990
    },
    {
      "epoch": 9.664292980671414,
      "grad_norm": 19.547216415405273,
      "learning_rate": 4.033570701932859e-05,
      "loss": 2.9399,
      "step": 19000
    },
    {
      "epoch": 9.669379450661241,
      "grad_norm": 15.647756576538086,
      "learning_rate": 4.033062054933876e-05,
      "loss": 2.8637,
      "step": 19010
    },
    {
      "epoch": 9.674465920651068,
      "grad_norm": 16.04351043701172,
      "learning_rate": 4.0325534079348934e-05,
      "loss": 2.9283,
      "step": 19020
    },
    {
      "epoch": 9.679552390640895,
      "grad_norm": 16.542448043823242,
      "learning_rate": 4.032044760935911e-05,
      "loss": 2.9209,
      "step": 19030
    },
    {
      "epoch": 9.684638860630722,
      "grad_norm": 23.417131423950195,
      "learning_rate": 4.031536113936928e-05,
      "loss": 2.9471,
      "step": 19040
    },
    {
      "epoch": 9.689725330620549,
      "grad_norm": 15.269722938537598,
      "learning_rate": 4.031027466937946e-05,
      "loss": 2.8426,
      "step": 19050
    },
    {
      "epoch": 9.694811800610376,
      "grad_norm": 16.304943084716797,
      "learning_rate": 4.030518819938963e-05,
      "loss": 2.908,
      "step": 19060
    },
    {
      "epoch": 9.699898270600203,
      "grad_norm": 14.486788749694824,
      "learning_rate": 4.0300101729399797e-05,
      "loss": 2.9658,
      "step": 19070
    },
    {
      "epoch": 9.70498474059003,
      "grad_norm": 15.492481231689453,
      "learning_rate": 4.029501525940997e-05,
      "loss": 2.9172,
      "step": 19080
    },
    {
      "epoch": 9.710071210579857,
      "grad_norm": 23.821533203125,
      "learning_rate": 4.028992878942014e-05,
      "loss": 2.8839,
      "step": 19090
    },
    {
      "epoch": 9.715157680569686,
      "grad_norm": 19.341514587402344,
      "learning_rate": 4.028484231943031e-05,
      "loss": 2.8269,
      "step": 19100
    },
    {
      "epoch": 9.720244150559513,
      "grad_norm": 17.275068283081055,
      "learning_rate": 4.027975584944049e-05,
      "loss": 2.8709,
      "step": 19110
    },
    {
      "epoch": 9.72533062054934,
      "grad_norm": 17.284805297851562,
      "learning_rate": 4.0274669379450666e-05,
      "loss": 2.9329,
      "step": 19120
    },
    {
      "epoch": 9.730417090539166,
      "grad_norm": 21.433448791503906,
      "learning_rate": 4.0269582909460836e-05,
      "loss": 2.9256,
      "step": 19130
    },
    {
      "epoch": 9.735503560528993,
      "grad_norm": 23.505966186523438,
      "learning_rate": 4.026449643947101e-05,
      "loss": 2.8499,
      "step": 19140
    },
    {
      "epoch": 9.74059003051882,
      "grad_norm": 23.582569122314453,
      "learning_rate": 4.025940996948118e-05,
      "loss": 2.8649,
      "step": 19150
    },
    {
      "epoch": 9.745676500508647,
      "grad_norm": 15.823878288269043,
      "learning_rate": 4.025432349949135e-05,
      "loss": 2.9181,
      "step": 19160
    },
    {
      "epoch": 9.750762970498474,
      "grad_norm": 17.5471134185791,
      "learning_rate": 4.024923702950153e-05,
      "loss": 2.9557,
      "step": 19170
    },
    {
      "epoch": 9.755849440488301,
      "grad_norm": 19.34697914123535,
      "learning_rate": 4.02441505595117e-05,
      "loss": 2.9433,
      "step": 19180
    },
    {
      "epoch": 9.760935910478128,
      "grad_norm": 19.890663146972656,
      "learning_rate": 4.023906408952187e-05,
      "loss": 2.9729,
      "step": 19190
    },
    {
      "epoch": 9.766022380467955,
      "grad_norm": 15.35732364654541,
      "learning_rate": 4.0233977619532046e-05,
      "loss": 2.9766,
      "step": 19200
    },
    {
      "epoch": 9.771108850457782,
      "grad_norm": 17.771154403686523,
      "learning_rate": 4.022889114954222e-05,
      "loss": 3.0014,
      "step": 19210
    },
    {
      "epoch": 9.77619532044761,
      "grad_norm": 16.714736938476562,
      "learning_rate": 4.022380467955239e-05,
      "loss": 2.9135,
      "step": 19220
    },
    {
      "epoch": 9.781281790437436,
      "grad_norm": 18.65943145751953,
      "learning_rate": 4.021871820956257e-05,
      "loss": 2.8911,
      "step": 19230
    },
    {
      "epoch": 9.786368260427263,
      "grad_norm": 17.12548828125,
      "learning_rate": 4.021363173957274e-05,
      "loss": 2.9347,
      "step": 19240
    },
    {
      "epoch": 9.79145473041709,
      "grad_norm": 18.969369888305664,
      "learning_rate": 4.020854526958291e-05,
      "loss": 2.7925,
      "step": 19250
    },
    {
      "epoch": 9.796541200406917,
      "grad_norm": 16.713645935058594,
      "learning_rate": 4.0203458799593086e-05,
      "loss": 2.9007,
      "step": 19260
    },
    {
      "epoch": 9.801627670396744,
      "grad_norm": 18.210094451904297,
      "learning_rate": 4.0198372329603255e-05,
      "loss": 2.9584,
      "step": 19270
    },
    {
      "epoch": 9.806714140386571,
      "grad_norm": 19.17723274230957,
      "learning_rate": 4.0193285859613425e-05,
      "loss": 2.8625,
      "step": 19280
    },
    {
      "epoch": 9.811800610376398,
      "grad_norm": 15.605119705200195,
      "learning_rate": 4.01881993896236e-05,
      "loss": 2.9544,
      "step": 19290
    },
    {
      "epoch": 9.816887080366225,
      "grad_norm": 14.944156646728516,
      "learning_rate": 4.018311291963377e-05,
      "loss": 2.8618,
      "step": 19300
    },
    {
      "epoch": 9.821973550356052,
      "grad_norm": 17.956571578979492,
      "learning_rate": 4.017802644964395e-05,
      "loss": 2.8982,
      "step": 19310
    },
    {
      "epoch": 9.827060020345879,
      "grad_norm": 19.705768585205078,
      "learning_rate": 4.0172939979654125e-05,
      "loss": 2.886,
      "step": 19320
    },
    {
      "epoch": 9.832146490335706,
      "grad_norm": 18.049150466918945,
      "learning_rate": 4.0167853509664295e-05,
      "loss": 2.8783,
      "step": 19330
    },
    {
      "epoch": 9.837232960325535,
      "grad_norm": 15.291770935058594,
      "learning_rate": 4.016276703967447e-05,
      "loss": 2.8572,
      "step": 19340
    },
    {
      "epoch": 9.842319430315362,
      "grad_norm": 19.429704666137695,
      "learning_rate": 4.015768056968464e-05,
      "loss": 2.8467,
      "step": 19350
    },
    {
      "epoch": 9.847405900305189,
      "grad_norm": 17.98078155517578,
      "learning_rate": 4.015259409969481e-05,
      "loss": 2.8437,
      "step": 19360
    },
    {
      "epoch": 9.852492370295016,
      "grad_norm": 16.333297729492188,
      "learning_rate": 4.014750762970499e-05,
      "loss": 2.9691,
      "step": 19370
    },
    {
      "epoch": 9.857578840284843,
      "grad_norm": 20.244421005249023,
      "learning_rate": 4.014242115971516e-05,
      "loss": 2.8495,
      "step": 19380
    },
    {
      "epoch": 9.86266531027467,
      "grad_norm": 16.279775619506836,
      "learning_rate": 4.013733468972533e-05,
      "loss": 2.8749,
      "step": 19390
    },
    {
      "epoch": 9.867751780264497,
      "grad_norm": 20.40589141845703,
      "learning_rate": 4.0132248219735505e-05,
      "loss": 2.9081,
      "step": 19400
    },
    {
      "epoch": 9.872838250254324,
      "grad_norm": 21.523237228393555,
      "learning_rate": 4.012716174974568e-05,
      "loss": 2.9018,
      "step": 19410
    },
    {
      "epoch": 9.87792472024415,
      "grad_norm": 18.060590744018555,
      "learning_rate": 4.012207527975585e-05,
      "loss": 2.9258,
      "step": 19420
    },
    {
      "epoch": 9.883011190233978,
      "grad_norm": 19.82276725769043,
      "learning_rate": 4.011698880976603e-05,
      "loss": 2.8221,
      "step": 19430
    },
    {
      "epoch": 9.888097660223805,
      "grad_norm": 17.23100471496582,
      "learning_rate": 4.01119023397762e-05,
      "loss": 2.9528,
      "step": 19440
    },
    {
      "epoch": 9.893184130213632,
      "grad_norm": 20.672901153564453,
      "learning_rate": 4.010681586978637e-05,
      "loss": 2.8631,
      "step": 19450
    },
    {
      "epoch": 9.898270600203459,
      "grad_norm": 14.893900871276855,
      "learning_rate": 4.0101729399796544e-05,
      "loss": 2.9462,
      "step": 19460
    },
    {
      "epoch": 9.903357070193286,
      "grad_norm": 17.53611183166504,
      "learning_rate": 4.0096642929806714e-05,
      "loss": 2.8737,
      "step": 19470
    },
    {
      "epoch": 9.908443540183113,
      "grad_norm": 20.690471649169922,
      "learning_rate": 4.0091556459816884e-05,
      "loss": 2.8683,
      "step": 19480
    },
    {
      "epoch": 9.91353001017294,
      "grad_norm": 17.922636032104492,
      "learning_rate": 4.008646998982706e-05,
      "loss": 2.7745,
      "step": 19490
    },
    {
      "epoch": 9.918616480162767,
      "grad_norm": 17.35273551940918,
      "learning_rate": 4.008138351983724e-05,
      "loss": 2.8785,
      "step": 19500
    },
    {
      "epoch": 9.923702950152594,
      "grad_norm": 18.170894622802734,
      "learning_rate": 4.007629704984741e-05,
      "loss": 2.9042,
      "step": 19510
    },
    {
      "epoch": 9.92878942014242,
      "grad_norm": 21.72920799255371,
      "learning_rate": 4.0071210579857584e-05,
      "loss": 2.8669,
      "step": 19520
    },
    {
      "epoch": 9.933875890132247,
      "grad_norm": 22.597183227539062,
      "learning_rate": 4.0066124109867754e-05,
      "loss": 2.9343,
      "step": 19530
    },
    {
      "epoch": 9.938962360122074,
      "grad_norm": 14.545188903808594,
      "learning_rate": 4.0061037639877924e-05,
      "loss": 2.9024,
      "step": 19540
    },
    {
      "epoch": 9.944048830111903,
      "grad_norm": 19.40158462524414,
      "learning_rate": 4.00559511698881e-05,
      "loss": 2.8203,
      "step": 19550
    },
    {
      "epoch": 9.94913530010173,
      "grad_norm": 12.941417694091797,
      "learning_rate": 4.005086469989827e-05,
      "loss": 2.8359,
      "step": 19560
    },
    {
      "epoch": 9.954221770091557,
      "grad_norm": 16.517892837524414,
      "learning_rate": 4.004577822990844e-05,
      "loss": 2.8931,
      "step": 19570
    },
    {
      "epoch": 9.959308240081384,
      "grad_norm": 22.427669525146484,
      "learning_rate": 4.004069175991862e-05,
      "loss": 2.8977,
      "step": 19580
    },
    {
      "epoch": 9.964394710071211,
      "grad_norm": 18.209959030151367,
      "learning_rate": 4.0035605289928794e-05,
      "loss": 2.8841,
      "step": 19590
    },
    {
      "epoch": 9.969481180061038,
      "grad_norm": 23.3698787689209,
      "learning_rate": 4.0030518819938964e-05,
      "loss": 2.8993,
      "step": 19600
    },
    {
      "epoch": 9.974567650050865,
      "grad_norm": 15.755956649780273,
      "learning_rate": 4.002543234994914e-05,
      "loss": 2.8419,
      "step": 19610
    },
    {
      "epoch": 9.979654120040692,
      "grad_norm": 23.806447982788086,
      "learning_rate": 4.002034587995931e-05,
      "loss": 2.8617,
      "step": 19620
    },
    {
      "epoch": 9.984740590030519,
      "grad_norm": 18.111289978027344,
      "learning_rate": 4.001525940996949e-05,
      "loss": 2.9129,
      "step": 19630
    },
    {
      "epoch": 9.989827060020346,
      "grad_norm": 18.014062881469727,
      "learning_rate": 4.001017293997966e-05,
      "loss": 2.8218,
      "step": 19640
    },
    {
      "epoch": 9.994913530010173,
      "grad_norm": 25.961999893188477,
      "learning_rate": 4.0005086469989827e-05,
      "loss": 2.8961,
      "step": 19650
    },
    {
      "epoch": 10.0,
      "grad_norm": 24.441627502441406,
      "learning_rate": 4e-05,
      "loss": 2.7776,
      "step": 19660
    },
    {
      "epoch": 10.0,
      "eval_loss": 3.8048148155212402,
      "eval_runtime": 2.7324,
      "eval_samples_per_second": 1015.593,
      "eval_steps_per_second": 126.995,
      "step": 19660
    },
    {
      "epoch": 10.005086469989827,
      "grad_norm": 17.97553825378418,
      "learning_rate": 3.999491353001017e-05,
      "loss": 2.9099,
      "step": 19670
    },
    {
      "epoch": 10.010172939979654,
      "grad_norm": 16.034500122070312,
      "learning_rate": 3.998982706002034e-05,
      "loss": 2.9315,
      "step": 19680
    },
    {
      "epoch": 10.015259409969481,
      "grad_norm": 18.339160919189453,
      "learning_rate": 3.998474059003052e-05,
      "loss": 2.7623,
      "step": 19690
    },
    {
      "epoch": 10.020345879959308,
      "grad_norm": 14.788800239562988,
      "learning_rate": 3.9979654120040696e-05,
      "loss": 2.8955,
      "step": 19700
    },
    {
      "epoch": 10.025432349949135,
      "grad_norm": 18.892671585083008,
      "learning_rate": 3.9974567650050866e-05,
      "loss": 2.9123,
      "step": 19710
    },
    {
      "epoch": 10.030518819938962,
      "grad_norm": 21.633420944213867,
      "learning_rate": 3.996948118006104e-05,
      "loss": 2.9161,
      "step": 19720
    },
    {
      "epoch": 10.035605289928789,
      "grad_norm": 16.357648849487305,
      "learning_rate": 3.996439471007121e-05,
      "loss": 2.8712,
      "step": 19730
    },
    {
      "epoch": 10.040691759918616,
      "grad_norm": 18.027427673339844,
      "learning_rate": 3.995930824008138e-05,
      "loss": 2.9667,
      "step": 19740
    },
    {
      "epoch": 10.045778229908443,
      "grad_norm": 18.335418701171875,
      "learning_rate": 3.995422177009156e-05,
      "loss": 2.8541,
      "step": 19750
    },
    {
      "epoch": 10.05086469989827,
      "grad_norm": 18.87486457824707,
      "learning_rate": 3.994913530010173e-05,
      "loss": 2.8292,
      "step": 19760
    },
    {
      "epoch": 10.055951169888097,
      "grad_norm": 15.084955215454102,
      "learning_rate": 3.99440488301119e-05,
      "loss": 2.8338,
      "step": 19770
    },
    {
      "epoch": 10.061037639877926,
      "grad_norm": 15.963970184326172,
      "learning_rate": 3.9938962360122076e-05,
      "loss": 2.8525,
      "step": 19780
    },
    {
      "epoch": 10.066124109867753,
      "grad_norm": 25.474454879760742,
      "learning_rate": 3.993387589013225e-05,
      "loss": 2.8658,
      "step": 19790
    },
    {
      "epoch": 10.07121057985758,
      "grad_norm": 17.62731170654297,
      "learning_rate": 3.992878942014242e-05,
      "loss": 2.8296,
      "step": 19800
    },
    {
      "epoch": 10.076297049847406,
      "grad_norm": 19.3278865814209,
      "learning_rate": 3.99237029501526e-05,
      "loss": 2.8028,
      "step": 19810
    },
    {
      "epoch": 10.081383519837233,
      "grad_norm": 21.4920711517334,
      "learning_rate": 3.991861648016277e-05,
      "loss": 2.7898,
      "step": 19820
    },
    {
      "epoch": 10.08646998982706,
      "grad_norm": 17.512685775756836,
      "learning_rate": 3.991353001017294e-05,
      "loss": 2.8351,
      "step": 19830
    },
    {
      "epoch": 10.091556459816887,
      "grad_norm": 18.747817993164062,
      "learning_rate": 3.9908443540183116e-05,
      "loss": 2.8411,
      "step": 19840
    },
    {
      "epoch": 10.096642929806714,
      "grad_norm": 21.47553253173828,
      "learning_rate": 3.9903357070193285e-05,
      "loss": 2.8899,
      "step": 19850
    },
    {
      "epoch": 10.101729399796541,
      "grad_norm": 15.880910873413086,
      "learning_rate": 3.989827060020346e-05,
      "loss": 2.8282,
      "step": 19860
    },
    {
      "epoch": 10.106815869786368,
      "grad_norm": 19.011518478393555,
      "learning_rate": 3.989318413021363e-05,
      "loss": 2.8296,
      "step": 19870
    },
    {
      "epoch": 10.111902339776195,
      "grad_norm": 16.30070686340332,
      "learning_rate": 3.988809766022381e-05,
      "loss": 2.8278,
      "step": 19880
    },
    {
      "epoch": 10.116988809766022,
      "grad_norm": 23.726184844970703,
      "learning_rate": 3.9883011190233985e-05,
      "loss": 2.8887,
      "step": 19890
    },
    {
      "epoch": 10.12207527975585,
      "grad_norm": 18.595115661621094,
      "learning_rate": 3.9877924720244155e-05,
      "loss": 2.8714,
      "step": 19900
    },
    {
      "epoch": 10.127161749745676,
      "grad_norm": 17.200090408325195,
      "learning_rate": 3.9872838250254325e-05,
      "loss": 2.9112,
      "step": 19910
    },
    {
      "epoch": 10.132248219735503,
      "grad_norm": 24.191652297973633,
      "learning_rate": 3.98677517802645e-05,
      "loss": 2.7862,
      "step": 19920
    },
    {
      "epoch": 10.13733468972533,
      "grad_norm": 20.271947860717773,
      "learning_rate": 3.986266531027467e-05,
      "loss": 2.8573,
      "step": 19930
    },
    {
      "epoch": 10.142421159715157,
      "grad_norm": 19.36347198486328,
      "learning_rate": 3.985757884028484e-05,
      "loss": 2.8352,
      "step": 19940
    },
    {
      "epoch": 10.147507629704984,
      "grad_norm": 14.480460166931152,
      "learning_rate": 3.985249237029502e-05,
      "loss": 2.8716,
      "step": 19950
    },
    {
      "epoch": 10.152594099694811,
      "grad_norm": 21.922988891601562,
      "learning_rate": 3.984740590030519e-05,
      "loss": 2.8151,
      "step": 19960
    },
    {
      "epoch": 10.157680569684638,
      "grad_norm": 18.908416748046875,
      "learning_rate": 3.984231943031536e-05,
      "loss": 2.9281,
      "step": 19970
    },
    {
      "epoch": 10.162767039674465,
      "grad_norm": 17.121356964111328,
      "learning_rate": 3.9837232960325535e-05,
      "loss": 2.8558,
      "step": 19980
    },
    {
      "epoch": 10.167853509664292,
      "grad_norm": 17.770063400268555,
      "learning_rate": 3.983214649033571e-05,
      "loss": 2.7254,
      "step": 19990
    },
    {
      "epoch": 10.172939979654121,
      "grad_norm": 21.041337966918945,
      "learning_rate": 3.982706002034588e-05,
      "loss": 2.8518,
      "step": 20000
    },
    {
      "epoch": 10.178026449643948,
      "grad_norm": 21.688365936279297,
      "learning_rate": 3.982197355035606e-05,
      "loss": 2.8076,
      "step": 20010
    },
    {
      "epoch": 10.183112919633775,
      "grad_norm": 16.29541778564453,
      "learning_rate": 3.981688708036623e-05,
      "loss": 2.8636,
      "step": 20020
    },
    {
      "epoch": 10.188199389623602,
      "grad_norm": 18.090023040771484,
      "learning_rate": 3.98118006103764e-05,
      "loss": 2.8615,
      "step": 20030
    },
    {
      "epoch": 10.193285859613429,
      "grad_norm": 25.233112335205078,
      "learning_rate": 3.9806714140386574e-05,
      "loss": 2.8851,
      "step": 20040
    },
    {
      "epoch": 10.198372329603256,
      "grad_norm": 22.055322647094727,
      "learning_rate": 3.9801627670396744e-05,
      "loss": 2.8758,
      "step": 20050
    },
    {
      "epoch": 10.203458799593083,
      "grad_norm": 18.963781356811523,
      "learning_rate": 3.9796541200406914e-05,
      "loss": 2.868,
      "step": 20060
    },
    {
      "epoch": 10.20854526958291,
      "grad_norm": 25.54207420349121,
      "learning_rate": 3.979145473041709e-05,
      "loss": 2.7946,
      "step": 20070
    },
    {
      "epoch": 10.213631739572737,
      "grad_norm": 20.241071701049805,
      "learning_rate": 3.978636826042727e-05,
      "loss": 2.9014,
      "step": 20080
    },
    {
      "epoch": 10.218718209562564,
      "grad_norm": 16.975605010986328,
      "learning_rate": 3.978128179043744e-05,
      "loss": 2.8972,
      "step": 20090
    },
    {
      "epoch": 10.22380467955239,
      "grad_norm": 23.76844024658203,
      "learning_rate": 3.9776195320447614e-05,
      "loss": 2.9329,
      "step": 20100
    },
    {
      "epoch": 10.228891149542218,
      "grad_norm": 20.965652465820312,
      "learning_rate": 3.9771108850457784e-05,
      "loss": 2.8583,
      "step": 20110
    },
    {
      "epoch": 10.233977619532045,
      "grad_norm": 19.733009338378906,
      "learning_rate": 3.976602238046796e-05,
      "loss": 2.9419,
      "step": 20120
    },
    {
      "epoch": 10.239064089521872,
      "grad_norm": 17.210676193237305,
      "learning_rate": 3.976093591047813e-05,
      "loss": 2.924,
      "step": 20130
    },
    {
      "epoch": 10.244150559511699,
      "grad_norm": 26.316801071166992,
      "learning_rate": 3.97558494404883e-05,
      "loss": 2.8797,
      "step": 20140
    },
    {
      "epoch": 10.249237029501526,
      "grad_norm": 16.686317443847656,
      "learning_rate": 3.975076297049848e-05,
      "loss": 2.8376,
      "step": 20150
    },
    {
      "epoch": 10.254323499491353,
      "grad_norm": 20.092348098754883,
      "learning_rate": 3.974567650050865e-05,
      "loss": 2.7857,
      "step": 20160
    },
    {
      "epoch": 10.25940996948118,
      "grad_norm": 18.387022018432617,
      "learning_rate": 3.9740590030518824e-05,
      "loss": 2.836,
      "step": 20170
    },
    {
      "epoch": 10.264496439471007,
      "grad_norm": 18.325698852539062,
      "learning_rate": 3.9735503560529e-05,
      "loss": 2.9006,
      "step": 20180
    },
    {
      "epoch": 10.269582909460834,
      "grad_norm": 20.269428253173828,
      "learning_rate": 3.973041709053917e-05,
      "loss": 2.7522,
      "step": 20190
    },
    {
      "epoch": 10.27466937945066,
      "grad_norm": 20.578266143798828,
      "learning_rate": 3.972533062054934e-05,
      "loss": 2.9064,
      "step": 20200
    },
    {
      "epoch": 10.279755849440487,
      "grad_norm": 22.148958206176758,
      "learning_rate": 3.972024415055952e-05,
      "loss": 2.8106,
      "step": 20210
    },
    {
      "epoch": 10.284842319430314,
      "grad_norm": 23.440567016601562,
      "learning_rate": 3.971515768056969e-05,
      "loss": 2.8779,
      "step": 20220
    },
    {
      "epoch": 10.289928789420143,
      "grad_norm": 22.284181594848633,
      "learning_rate": 3.9710071210579857e-05,
      "loss": 2.9011,
      "step": 20230
    },
    {
      "epoch": 10.29501525940997,
      "grad_norm": 19.956039428710938,
      "learning_rate": 3.970498474059003e-05,
      "loss": 2.922,
      "step": 20240
    },
    {
      "epoch": 10.300101729399797,
      "grad_norm": 15.315617561340332,
      "learning_rate": 3.96998982706002e-05,
      "loss": 2.8658,
      "step": 20250
    },
    {
      "epoch": 10.305188199389624,
      "grad_norm": 24.5156307220459,
      "learning_rate": 3.969481180061037e-05,
      "loss": 2.8956,
      "step": 20260
    },
    {
      "epoch": 10.310274669379451,
      "grad_norm": 28.648313522338867,
      "learning_rate": 3.968972533062055e-05,
      "loss": 2.8257,
      "step": 20270
    },
    {
      "epoch": 10.315361139369278,
      "grad_norm": 20.42894172668457,
      "learning_rate": 3.9684638860630726e-05,
      "loss": 2.9114,
      "step": 20280
    },
    {
      "epoch": 10.320447609359105,
      "grad_norm": 19.212800979614258,
      "learning_rate": 3.9679552390640896e-05,
      "loss": 2.8659,
      "step": 20290
    },
    {
      "epoch": 10.325534079348932,
      "grad_norm": 16.828899383544922,
      "learning_rate": 3.967446592065107e-05,
      "loss": 2.8782,
      "step": 20300
    },
    {
      "epoch": 10.330620549338759,
      "grad_norm": 16.975645065307617,
      "learning_rate": 3.966937945066124e-05,
      "loss": 2.8442,
      "step": 20310
    },
    {
      "epoch": 10.335707019328586,
      "grad_norm": 22.668428421020508,
      "learning_rate": 3.966429298067141e-05,
      "loss": 2.7902,
      "step": 20320
    },
    {
      "epoch": 10.340793489318413,
      "grad_norm": 18.527118682861328,
      "learning_rate": 3.965920651068159e-05,
      "loss": 2.7902,
      "step": 20330
    },
    {
      "epoch": 10.34587995930824,
      "grad_norm": 20.37275505065918,
      "learning_rate": 3.965412004069176e-05,
      "loss": 2.7714,
      "step": 20340
    },
    {
      "epoch": 10.350966429298067,
      "grad_norm": 18.69345474243164,
      "learning_rate": 3.964903357070193e-05,
      "loss": 2.9245,
      "step": 20350
    },
    {
      "epoch": 10.356052899287894,
      "grad_norm": 19.964691162109375,
      "learning_rate": 3.9643947100712106e-05,
      "loss": 2.9247,
      "step": 20360
    },
    {
      "epoch": 10.361139369277721,
      "grad_norm": 18.223392486572266,
      "learning_rate": 3.963886063072228e-05,
      "loss": 2.8222,
      "step": 20370
    },
    {
      "epoch": 10.366225839267548,
      "grad_norm": 23.788291931152344,
      "learning_rate": 3.963377416073245e-05,
      "loss": 2.8357,
      "step": 20380
    },
    {
      "epoch": 10.371312309257375,
      "grad_norm": 19.008268356323242,
      "learning_rate": 3.962868769074263e-05,
      "loss": 2.9361,
      "step": 20390
    },
    {
      "epoch": 10.376398779247202,
      "grad_norm": 16.22800064086914,
      "learning_rate": 3.96236012207528e-05,
      "loss": 2.9076,
      "step": 20400
    },
    {
      "epoch": 10.381485249237029,
      "grad_norm": 19.12531280517578,
      "learning_rate": 3.9618514750762976e-05,
      "loss": 2.8403,
      "step": 20410
    },
    {
      "epoch": 10.386571719226856,
      "grad_norm": 18.20454216003418,
      "learning_rate": 3.9613428280773146e-05,
      "loss": 2.754,
      "step": 20420
    },
    {
      "epoch": 10.391658189216683,
      "grad_norm": 22.460124969482422,
      "learning_rate": 3.9608341810783315e-05,
      "loss": 2.7168,
      "step": 20430
    },
    {
      "epoch": 10.396744659206512,
      "grad_norm": 16.619733810424805,
      "learning_rate": 3.960325534079349e-05,
      "loss": 2.8417,
      "step": 20440
    },
    {
      "epoch": 10.401831129196339,
      "grad_norm": 23.547718048095703,
      "learning_rate": 3.959816887080366e-05,
      "loss": 2.8582,
      "step": 20450
    },
    {
      "epoch": 10.406917599186166,
      "grad_norm": 14.932708740234375,
      "learning_rate": 3.959308240081384e-05,
      "loss": 2.8958,
      "step": 20460
    },
    {
      "epoch": 10.412004069175993,
      "grad_norm": 19.274229049682617,
      "learning_rate": 3.9587995930824015e-05,
      "loss": 2.8211,
      "step": 20470
    },
    {
      "epoch": 10.41709053916582,
      "grad_norm": 19.458770751953125,
      "learning_rate": 3.9582909460834185e-05,
      "loss": 2.7713,
      "step": 20480
    },
    {
      "epoch": 10.422177009155646,
      "grad_norm": 19.846471786499023,
      "learning_rate": 3.9577822990844355e-05,
      "loss": 2.8225,
      "step": 20490
    },
    {
      "epoch": 10.427263479145473,
      "grad_norm": 19.22979164123535,
      "learning_rate": 3.957273652085453e-05,
      "loss": 2.8178,
      "step": 20500
    },
    {
      "epoch": 10.4323499491353,
      "grad_norm": 22.797666549682617,
      "learning_rate": 3.95676500508647e-05,
      "loss": 2.8822,
      "step": 20510
    },
    {
      "epoch": 10.437436419125127,
      "grad_norm": 15.426003456115723,
      "learning_rate": 3.956256358087487e-05,
      "loss": 2.8717,
      "step": 20520
    },
    {
      "epoch": 10.442522889114954,
      "grad_norm": 18.764904022216797,
      "learning_rate": 3.955747711088505e-05,
      "loss": 2.7087,
      "step": 20530
    },
    {
      "epoch": 10.447609359104781,
      "grad_norm": 16.247772216796875,
      "learning_rate": 3.955239064089522e-05,
      "loss": 2.8674,
      "step": 20540
    },
    {
      "epoch": 10.452695829094608,
      "grad_norm": 23.711244583129883,
      "learning_rate": 3.9547304170905395e-05,
      "loss": 2.8329,
      "step": 20550
    },
    {
      "epoch": 10.457782299084435,
      "grad_norm": 21.13958168029785,
      "learning_rate": 3.9542217700915565e-05,
      "loss": 2.8147,
      "step": 20560
    },
    {
      "epoch": 10.462868769074262,
      "grad_norm": 18.72568702697754,
      "learning_rate": 3.953713123092574e-05,
      "loss": 2.7407,
      "step": 20570
    },
    {
      "epoch": 10.46795523906409,
      "grad_norm": 18.554174423217773,
      "learning_rate": 3.953204476093591e-05,
      "loss": 2.8078,
      "step": 20580
    },
    {
      "epoch": 10.473041709053916,
      "grad_norm": 19.319551467895508,
      "learning_rate": 3.952695829094609e-05,
      "loss": 2.7688,
      "step": 20590
    },
    {
      "epoch": 10.478128179043743,
      "grad_norm": 24.6843204498291,
      "learning_rate": 3.952187182095626e-05,
      "loss": 2.796,
      "step": 20600
    },
    {
      "epoch": 10.48321464903357,
      "grad_norm": 17.441293716430664,
      "learning_rate": 3.951678535096643e-05,
      "loss": 2.7956,
      "step": 20610
    },
    {
      "epoch": 10.488301119023397,
      "grad_norm": 19.32904624938965,
      "learning_rate": 3.9511698880976604e-05,
      "loss": 2.7899,
      "step": 20620
    },
    {
      "epoch": 10.493387589013224,
      "grad_norm": 23.680370330810547,
      "learning_rate": 3.9506612410986774e-05,
      "loss": 2.8381,
      "step": 20630
    },
    {
      "epoch": 10.498474059003051,
      "grad_norm": 20.98632049560547,
      "learning_rate": 3.9501525940996944e-05,
      "loss": 2.8726,
      "step": 20640
    },
    {
      "epoch": 10.503560528992878,
      "grad_norm": 17.82634925842285,
      "learning_rate": 3.949643947100712e-05,
      "loss": 2.8464,
      "step": 20650
    },
    {
      "epoch": 10.508646998982705,
      "grad_norm": 16.094528198242188,
      "learning_rate": 3.94913530010173e-05,
      "loss": 2.8662,
      "step": 20660
    },
    {
      "epoch": 10.513733468972532,
      "grad_norm": 23.72955322265625,
      "learning_rate": 3.9486266531027474e-05,
      "loss": 2.8891,
      "step": 20670
    },
    {
      "epoch": 10.518819938962361,
      "grad_norm": 16.08864974975586,
      "learning_rate": 3.9481180061037644e-05,
      "loss": 2.9445,
      "step": 20680
    },
    {
      "epoch": 10.523906408952188,
      "grad_norm": 17.546674728393555,
      "learning_rate": 3.9476093591047814e-05,
      "loss": 2.8098,
      "step": 20690
    },
    {
      "epoch": 10.528992878942015,
      "grad_norm": 22.123781204223633,
      "learning_rate": 3.947100712105799e-05,
      "loss": 2.8793,
      "step": 20700
    },
    {
      "epoch": 10.534079348931842,
      "grad_norm": 20.12065887451172,
      "learning_rate": 3.946592065106816e-05,
      "loss": 2.7324,
      "step": 20710
    },
    {
      "epoch": 10.539165818921669,
      "grad_norm": 16.489498138427734,
      "learning_rate": 3.946083418107833e-05,
      "loss": 2.8414,
      "step": 20720
    },
    {
      "epoch": 10.544252288911496,
      "grad_norm": 17.874170303344727,
      "learning_rate": 3.945574771108851e-05,
      "loss": 2.8723,
      "step": 20730
    },
    {
      "epoch": 10.549338758901323,
      "grad_norm": 22.720792770385742,
      "learning_rate": 3.945066124109868e-05,
      "loss": 2.7852,
      "step": 20740
    },
    {
      "epoch": 10.55442522889115,
      "grad_norm": 19.65569496154785,
      "learning_rate": 3.9445574771108854e-05,
      "loss": 2.8571,
      "step": 20750
    },
    {
      "epoch": 10.559511698880977,
      "grad_norm": 19.314987182617188,
      "learning_rate": 3.944048830111903e-05,
      "loss": 2.8547,
      "step": 20760
    },
    {
      "epoch": 10.564598168870804,
      "grad_norm": 20.008955001831055,
      "learning_rate": 3.94354018311292e-05,
      "loss": 2.7938,
      "step": 20770
    },
    {
      "epoch": 10.56968463886063,
      "grad_norm": 20.753843307495117,
      "learning_rate": 3.943031536113937e-05,
      "loss": 2.7453,
      "step": 20780
    },
    {
      "epoch": 10.574771108850458,
      "grad_norm": 16.32398796081543,
      "learning_rate": 3.942522889114955e-05,
      "loss": 2.8582,
      "step": 20790
    },
    {
      "epoch": 10.579857578840285,
      "grad_norm": 19.97404670715332,
      "learning_rate": 3.942014242115972e-05,
      "loss": 2.8561,
      "step": 20800
    },
    {
      "epoch": 10.584944048830112,
      "grad_norm": 20.519264221191406,
      "learning_rate": 3.941505595116989e-05,
      "loss": 2.7172,
      "step": 20810
    },
    {
      "epoch": 10.590030518819939,
      "grad_norm": 16.936498641967773,
      "learning_rate": 3.940996948118006e-05,
      "loss": 2.7841,
      "step": 20820
    },
    {
      "epoch": 10.595116988809766,
      "grad_norm": 19.11488914489746,
      "learning_rate": 3.940488301119023e-05,
      "loss": 2.8402,
      "step": 20830
    },
    {
      "epoch": 10.600203458799593,
      "grad_norm": 21.679258346557617,
      "learning_rate": 3.939979654120041e-05,
      "loss": 2.8794,
      "step": 20840
    },
    {
      "epoch": 10.60528992878942,
      "grad_norm": 21.082317352294922,
      "learning_rate": 3.9394710071210587e-05,
      "loss": 2.7358,
      "step": 20850
    },
    {
      "epoch": 10.610376398779247,
      "grad_norm": 15.373753547668457,
      "learning_rate": 3.9389623601220756e-05,
      "loss": 2.8133,
      "step": 20860
    },
    {
      "epoch": 10.615462868769074,
      "grad_norm": 20.560884475708008,
      "learning_rate": 3.9384537131230926e-05,
      "loss": 2.783,
      "step": 20870
    },
    {
      "epoch": 10.6205493387589,
      "grad_norm": 20.972585678100586,
      "learning_rate": 3.93794506612411e-05,
      "loss": 2.7858,
      "step": 20880
    },
    {
      "epoch": 10.62563580874873,
      "grad_norm": 24.862140655517578,
      "learning_rate": 3.937436419125127e-05,
      "loss": 2.8658,
      "step": 20890
    },
    {
      "epoch": 10.630722278738556,
      "grad_norm": 20.60081672668457,
      "learning_rate": 3.936927772126144e-05,
      "loss": 2.7404,
      "step": 20900
    },
    {
      "epoch": 10.635808748728383,
      "grad_norm": 21.336414337158203,
      "learning_rate": 3.936419125127162e-05,
      "loss": 2.8724,
      "step": 20910
    },
    {
      "epoch": 10.64089521871821,
      "grad_norm": 22.219186782836914,
      "learning_rate": 3.935910478128179e-05,
      "loss": 2.7763,
      "step": 20920
    },
    {
      "epoch": 10.645981688708037,
      "grad_norm": 21.75295639038086,
      "learning_rate": 3.9354018311291966e-05,
      "loss": 2.8453,
      "step": 20930
    },
    {
      "epoch": 10.651068158697864,
      "grad_norm": 14.921614646911621,
      "learning_rate": 3.9348931841302136e-05,
      "loss": 2.8514,
      "step": 20940
    },
    {
      "epoch": 10.656154628687691,
      "grad_norm": 17.20330810546875,
      "learning_rate": 3.934384537131231e-05,
      "loss": 2.8454,
      "step": 20950
    },
    {
      "epoch": 10.661241098677518,
      "grad_norm": 22.994794845581055,
      "learning_rate": 3.933875890132249e-05,
      "loss": 2.8165,
      "step": 20960
    },
    {
      "epoch": 10.666327568667345,
      "grad_norm": 17.06945037841797,
      "learning_rate": 3.933367243133266e-05,
      "loss": 2.7289,
      "step": 20970
    },
    {
      "epoch": 10.671414038657172,
      "grad_norm": 16.680139541625977,
      "learning_rate": 3.932858596134283e-05,
      "loss": 2.7264,
      "step": 20980
    },
    {
      "epoch": 10.676500508646999,
      "grad_norm": 16.677087783813477,
      "learning_rate": 3.9323499491353006e-05,
      "loss": 2.8041,
      "step": 20990
    },
    {
      "epoch": 10.681586978636826,
      "grad_norm": 23.789335250854492,
      "learning_rate": 3.9318413021363176e-05,
      "loss": 2.8487,
      "step": 21000
    },
    {
      "epoch": 10.686673448626653,
      "grad_norm": 17.83799934387207,
      "learning_rate": 3.9313326551373345e-05,
      "loss": 2.7281,
      "step": 21010
    },
    {
      "epoch": 10.69175991861648,
      "grad_norm": 18.34488296508789,
      "learning_rate": 3.930824008138352e-05,
      "loss": 2.7561,
      "step": 21020
    },
    {
      "epoch": 10.696846388606307,
      "grad_norm": 33.51326370239258,
      "learning_rate": 3.930315361139369e-05,
      "loss": 2.8033,
      "step": 21030
    },
    {
      "epoch": 10.701932858596134,
      "grad_norm": 18.438947677612305,
      "learning_rate": 3.929806714140387e-05,
      "loss": 2.7655,
      "step": 21040
    },
    {
      "epoch": 10.707019328585961,
      "grad_norm": 22.738759994506836,
      "learning_rate": 3.9292980671414045e-05,
      "loss": 2.8818,
      "step": 21050
    },
    {
      "epoch": 10.712105798575788,
      "grad_norm": 18.727323532104492,
      "learning_rate": 3.9287894201424215e-05,
      "loss": 2.8129,
      "step": 21060
    },
    {
      "epoch": 10.717192268565615,
      "grad_norm": 21.782819747924805,
      "learning_rate": 3.9282807731434385e-05,
      "loss": 2.832,
      "step": 21070
    },
    {
      "epoch": 10.722278738555442,
      "grad_norm": 18.03968620300293,
      "learning_rate": 3.927772126144456e-05,
      "loss": 2.8348,
      "step": 21080
    },
    {
      "epoch": 10.727365208545269,
      "grad_norm": 19.254167556762695,
      "learning_rate": 3.927263479145473e-05,
      "loss": 2.8451,
      "step": 21090
    },
    {
      "epoch": 10.732451678535096,
      "grad_norm": 23.215892791748047,
      "learning_rate": 3.92675483214649e-05,
      "loss": 2.7598,
      "step": 21100
    },
    {
      "epoch": 10.737538148524923,
      "grad_norm": 21.39461898803711,
      "learning_rate": 3.926246185147508e-05,
      "loss": 2.8451,
      "step": 21110
    },
    {
      "epoch": 10.742624618514752,
      "grad_norm": 24.691055297851562,
      "learning_rate": 3.925737538148525e-05,
      "loss": 2.8709,
      "step": 21120
    },
    {
      "epoch": 10.747711088504579,
      "grad_norm": 23.46249771118164,
      "learning_rate": 3.9252288911495425e-05,
      "loss": 2.8444,
      "step": 21130
    },
    {
      "epoch": 10.752797558494406,
      "grad_norm": 17.91111183166504,
      "learning_rate": 3.92472024415056e-05,
      "loss": 2.7675,
      "step": 21140
    },
    {
      "epoch": 10.757884028484233,
      "grad_norm": 24.246009826660156,
      "learning_rate": 3.924211597151577e-05,
      "loss": 2.7838,
      "step": 21150
    },
    {
      "epoch": 10.76297049847406,
      "grad_norm": 16.05024528503418,
      "learning_rate": 3.923702950152594e-05,
      "loss": 2.7357,
      "step": 21160
    },
    {
      "epoch": 10.768056968463886,
      "grad_norm": 20.217241287231445,
      "learning_rate": 3.923194303153612e-05,
      "loss": 2.7824,
      "step": 21170
    },
    {
      "epoch": 10.773143438453713,
      "grad_norm": 22.006511688232422,
      "learning_rate": 3.922685656154629e-05,
      "loss": 2.7639,
      "step": 21180
    },
    {
      "epoch": 10.77822990844354,
      "grad_norm": 29.391122817993164,
      "learning_rate": 3.922177009155646e-05,
      "loss": 2.7561,
      "step": 21190
    },
    {
      "epoch": 10.783316378433367,
      "grad_norm": 23.163352966308594,
      "learning_rate": 3.9216683621566634e-05,
      "loss": 2.764,
      "step": 21200
    },
    {
      "epoch": 10.788402848423194,
      "grad_norm": 16.481229782104492,
      "learning_rate": 3.9211597151576804e-05,
      "loss": 2.8147,
      "step": 21210
    },
    {
      "epoch": 10.793489318413021,
      "grad_norm": 16.619140625,
      "learning_rate": 3.920651068158698e-05,
      "loss": 2.8071,
      "step": 21220
    },
    {
      "epoch": 10.798575788402848,
      "grad_norm": 22.170087814331055,
      "learning_rate": 3.920142421159715e-05,
      "loss": 2.8176,
      "step": 21230
    },
    {
      "epoch": 10.803662258392675,
      "grad_norm": 21.338464736938477,
      "learning_rate": 3.919633774160733e-05,
      "loss": 2.8019,
      "step": 21240
    },
    {
      "epoch": 10.808748728382502,
      "grad_norm": 17.67259407043457,
      "learning_rate": 3.9191251271617504e-05,
      "loss": 2.8505,
      "step": 21250
    },
    {
      "epoch": 10.81383519837233,
      "grad_norm": 20.928674697875977,
      "learning_rate": 3.9186164801627674e-05,
      "loss": 2.7868,
      "step": 21260
    },
    {
      "epoch": 10.818921668362156,
      "grad_norm": 25.50274085998535,
      "learning_rate": 3.9181078331637844e-05,
      "loss": 2.7628,
      "step": 21270
    },
    {
      "epoch": 10.824008138351983,
      "grad_norm": 19.506458282470703,
      "learning_rate": 3.917599186164802e-05,
      "loss": 2.8219,
      "step": 21280
    },
    {
      "epoch": 10.82909460834181,
      "grad_norm": 21.378355026245117,
      "learning_rate": 3.917090539165819e-05,
      "loss": 2.772,
      "step": 21290
    },
    {
      "epoch": 10.834181078331637,
      "grad_norm": 16.53775978088379,
      "learning_rate": 3.916581892166836e-05,
      "loss": 2.7836,
      "step": 21300
    },
    {
      "epoch": 10.839267548321464,
      "grad_norm": 18.782217025756836,
      "learning_rate": 3.916073245167854e-05,
      "loss": 2.812,
      "step": 21310
    },
    {
      "epoch": 10.844354018311291,
      "grad_norm": 19.13958740234375,
      "learning_rate": 3.915564598168871e-05,
      "loss": 2.8868,
      "step": 21320
    },
    {
      "epoch": 10.84944048830112,
      "grad_norm": 20.693437576293945,
      "learning_rate": 3.9150559511698884e-05,
      "loss": 2.8223,
      "step": 21330
    },
    {
      "epoch": 10.854526958290947,
      "grad_norm": 20.145832061767578,
      "learning_rate": 3.914547304170906e-05,
      "loss": 2.7783,
      "step": 21340
    },
    {
      "epoch": 10.859613428280774,
      "grad_norm": 18.475576400756836,
      "learning_rate": 3.914038657171923e-05,
      "loss": 2.8103,
      "step": 21350
    },
    {
      "epoch": 10.864699898270601,
      "grad_norm": 16.657917022705078,
      "learning_rate": 3.91353001017294e-05,
      "loss": 2.9346,
      "step": 21360
    },
    {
      "epoch": 10.869786368260428,
      "grad_norm": 17.731103897094727,
      "learning_rate": 3.913021363173958e-05,
      "loss": 2.8503,
      "step": 21370
    },
    {
      "epoch": 10.874872838250255,
      "grad_norm": 17.938013076782227,
      "learning_rate": 3.912512716174975e-05,
      "loss": 2.7409,
      "step": 21380
    },
    {
      "epoch": 10.879959308240082,
      "grad_norm": 18.93351936340332,
      "learning_rate": 3.912004069175992e-05,
      "loss": 2.8962,
      "step": 21390
    },
    {
      "epoch": 10.885045778229909,
      "grad_norm": 21.552879333496094,
      "learning_rate": 3.911495422177009e-05,
      "loss": 2.8016,
      "step": 21400
    },
    {
      "epoch": 10.890132248219736,
      "grad_norm": 16.44453239440918,
      "learning_rate": 3.910986775178026e-05,
      "loss": 2.7347,
      "step": 21410
    },
    {
      "epoch": 10.895218718209563,
      "grad_norm": 21.78315544128418,
      "learning_rate": 3.910478128179044e-05,
      "loss": 2.7577,
      "step": 21420
    },
    {
      "epoch": 10.90030518819939,
      "grad_norm": 20.13141632080078,
      "learning_rate": 3.9099694811800617e-05,
      "loss": 2.8644,
      "step": 21430
    },
    {
      "epoch": 10.905391658189217,
      "grad_norm": 17.422992706298828,
      "learning_rate": 3.9094608341810786e-05,
      "loss": 2.7233,
      "step": 21440
    },
    {
      "epoch": 10.910478128179044,
      "grad_norm": 22.147226333618164,
      "learning_rate": 3.9089521871820956e-05,
      "loss": 2.7209,
      "step": 21450
    },
    {
      "epoch": 10.91556459816887,
      "grad_norm": 20.462215423583984,
      "learning_rate": 3.908443540183113e-05,
      "loss": 2.7737,
      "step": 21460
    },
    {
      "epoch": 10.920651068158698,
      "grad_norm": 26.48049545288086,
      "learning_rate": 3.90793489318413e-05,
      "loss": 2.7627,
      "step": 21470
    },
    {
      "epoch": 10.925737538148525,
      "grad_norm": 21.702207565307617,
      "learning_rate": 3.907426246185148e-05,
      "loss": 2.7698,
      "step": 21480
    },
    {
      "epoch": 10.930824008138352,
      "grad_norm": 20.987043380737305,
      "learning_rate": 3.906917599186165e-05,
      "loss": 2.8126,
      "step": 21490
    },
    {
      "epoch": 10.935910478128179,
      "grad_norm": 21.102252960205078,
      "learning_rate": 3.906408952187182e-05,
      "loss": 2.801,
      "step": 21500
    },
    {
      "epoch": 10.940996948118006,
      "grad_norm": 22.36076545715332,
      "learning_rate": 3.9059003051881996e-05,
      "loss": 2.7448,
      "step": 21510
    },
    {
      "epoch": 10.946083418107833,
      "grad_norm": 22.34707260131836,
      "learning_rate": 3.905391658189217e-05,
      "loss": 2.86,
      "step": 21520
    },
    {
      "epoch": 10.95116988809766,
      "grad_norm": 18.433979034423828,
      "learning_rate": 3.904883011190234e-05,
      "loss": 2.8694,
      "step": 21530
    },
    {
      "epoch": 10.956256358087487,
      "grad_norm": 24.158451080322266,
      "learning_rate": 3.904374364191252e-05,
      "loss": 2.8364,
      "step": 21540
    },
    {
      "epoch": 10.961342828077314,
      "grad_norm": 18.663480758666992,
      "learning_rate": 3.903865717192269e-05,
      "loss": 2.7502,
      "step": 21550
    },
    {
      "epoch": 10.96642929806714,
      "grad_norm": 19.248319625854492,
      "learning_rate": 3.903357070193286e-05,
      "loss": 2.7868,
      "step": 21560
    },
    {
      "epoch": 10.97151576805697,
      "grad_norm": 23.461467742919922,
      "learning_rate": 3.9028484231943036e-05,
      "loss": 2.765,
      "step": 21570
    },
    {
      "epoch": 10.976602238046796,
      "grad_norm": 20.302154541015625,
      "learning_rate": 3.9023397761953206e-05,
      "loss": 2.8419,
      "step": 21580
    },
    {
      "epoch": 10.981688708036623,
      "grad_norm": 22.218088150024414,
      "learning_rate": 3.9018311291963375e-05,
      "loss": 2.8195,
      "step": 21590
    },
    {
      "epoch": 10.98677517802645,
      "grad_norm": 16.417924880981445,
      "learning_rate": 3.901322482197355e-05,
      "loss": 2.7864,
      "step": 21600
    },
    {
      "epoch": 10.991861648016277,
      "grad_norm": 21.00762176513672,
      "learning_rate": 3.900813835198372e-05,
      "loss": 2.7154,
      "step": 21610
    },
    {
      "epoch": 10.996948118006104,
      "grad_norm": 19.464935302734375,
      "learning_rate": 3.90030518819939e-05,
      "loss": 2.7722,
      "step": 21620
    },
    {
      "epoch": 11.0,
      "eval_loss": 3.8491148948669434,
      "eval_runtime": 2.7312,
      "eval_samples_per_second": 1016.053,
      "eval_steps_per_second": 127.052,
      "step": 21626
    },
    {
      "epoch": 11.002034587995931,
      "grad_norm": 23.075164794921875,
      "learning_rate": 3.8997965412004075e-05,
      "loss": 2.8406,
      "step": 21630
    },
    {
      "epoch": 11.007121057985758,
      "grad_norm": 17.952857971191406,
      "learning_rate": 3.8992878942014245e-05,
      "loss": 2.7008,
      "step": 21640
    },
    {
      "epoch": 11.012207527975585,
      "grad_norm": 18.47662353515625,
      "learning_rate": 3.8987792472024415e-05,
      "loss": 2.7796,
      "step": 21650
    },
    {
      "epoch": 11.017293997965412,
      "grad_norm": 19.323137283325195,
      "learning_rate": 3.898270600203459e-05,
      "loss": 2.7731,
      "step": 21660
    },
    {
      "epoch": 11.022380467955239,
      "grad_norm": 17.69808578491211,
      "learning_rate": 3.897761953204476e-05,
      "loss": 2.6807,
      "step": 21670
    },
    {
      "epoch": 11.027466937945066,
      "grad_norm": 26.31926918029785,
      "learning_rate": 3.897253306205493e-05,
      "loss": 2.7996,
      "step": 21680
    },
    {
      "epoch": 11.032553407934893,
      "grad_norm": 26.005794525146484,
      "learning_rate": 3.896744659206511e-05,
      "loss": 2.8248,
      "step": 21690
    },
    {
      "epoch": 11.03763987792472,
      "grad_norm": 16.22664451599121,
      "learning_rate": 3.896236012207528e-05,
      "loss": 2.8261,
      "step": 21700
    },
    {
      "epoch": 11.042726347914547,
      "grad_norm": 27.13120460510254,
      "learning_rate": 3.8957273652085455e-05,
      "loss": 2.7299,
      "step": 21710
    },
    {
      "epoch": 11.047812817904374,
      "grad_norm": 21.210908889770508,
      "learning_rate": 3.895218718209563e-05,
      "loss": 2.769,
      "step": 21720
    },
    {
      "epoch": 11.052899287894201,
      "grad_norm": 26.432069778442383,
      "learning_rate": 3.89471007121058e-05,
      "loss": 2.8093,
      "step": 21730
    },
    {
      "epoch": 11.057985757884028,
      "grad_norm": 16.84189796447754,
      "learning_rate": 3.894201424211598e-05,
      "loss": 2.8055,
      "step": 21740
    },
    {
      "epoch": 11.063072227873855,
      "grad_norm": 14.380887031555176,
      "learning_rate": 3.893692777212615e-05,
      "loss": 2.8313,
      "step": 21750
    },
    {
      "epoch": 11.068158697863682,
      "grad_norm": 23.92730140686035,
      "learning_rate": 3.893184130213632e-05,
      "loss": 2.7328,
      "step": 21760
    },
    {
      "epoch": 11.073245167853509,
      "grad_norm": 18.691513061523438,
      "learning_rate": 3.8926754832146495e-05,
      "loss": 2.764,
      "step": 21770
    },
    {
      "epoch": 11.078331637843336,
      "grad_norm": 25.47438621520996,
      "learning_rate": 3.8921668362156664e-05,
      "loss": 2.8062,
      "step": 21780
    },
    {
      "epoch": 11.083418107833165,
      "grad_norm": 22.067705154418945,
      "learning_rate": 3.8916581892166834e-05,
      "loss": 2.7651,
      "step": 21790
    },
    {
      "epoch": 11.088504577822992,
      "grad_norm": 20.305500030517578,
      "learning_rate": 3.891149542217701e-05,
      "loss": 2.7199,
      "step": 21800
    },
    {
      "epoch": 11.093591047812819,
      "grad_norm": 19.842361450195312,
      "learning_rate": 3.890640895218719e-05,
      "loss": 2.7623,
      "step": 21810
    },
    {
      "epoch": 11.098677517802646,
      "grad_norm": 23.353748321533203,
      "learning_rate": 3.890132248219736e-05,
      "loss": 2.7289,
      "step": 21820
    },
    {
      "epoch": 11.103763987792473,
      "grad_norm": 21.1817684173584,
      "learning_rate": 3.8896236012207534e-05,
      "loss": 2.7351,
      "step": 21830
    },
    {
      "epoch": 11.1088504577823,
      "grad_norm": 17.99818229675293,
      "learning_rate": 3.8891149542217704e-05,
      "loss": 2.8365,
      "step": 21840
    },
    {
      "epoch": 11.113936927772126,
      "grad_norm": 21.601125717163086,
      "learning_rate": 3.8886063072227874e-05,
      "loss": 2.7161,
      "step": 21850
    },
    {
      "epoch": 11.119023397761953,
      "grad_norm": 18.27482032775879,
      "learning_rate": 3.888097660223805e-05,
      "loss": 2.8301,
      "step": 21860
    },
    {
      "epoch": 11.12410986775178,
      "grad_norm": 19.89629364013672,
      "learning_rate": 3.887589013224822e-05,
      "loss": 2.7244,
      "step": 21870
    },
    {
      "epoch": 11.129196337741607,
      "grad_norm": 18.130035400390625,
      "learning_rate": 3.887080366225839e-05,
      "loss": 2.8209,
      "step": 21880
    },
    {
      "epoch": 11.134282807731434,
      "grad_norm": 18.009620666503906,
      "learning_rate": 3.886571719226857e-05,
      "loss": 2.6977,
      "step": 21890
    },
    {
      "epoch": 11.139369277721261,
      "grad_norm": 21.809709548950195,
      "learning_rate": 3.886063072227874e-05,
      "loss": 2.8246,
      "step": 21900
    },
    {
      "epoch": 11.144455747711088,
      "grad_norm": 20.883235931396484,
      "learning_rate": 3.8855544252288914e-05,
      "loss": 2.8049,
      "step": 21910
    },
    {
      "epoch": 11.149542217700915,
      "grad_norm": 22.49576759338379,
      "learning_rate": 3.885045778229909e-05,
      "loss": 2.7067,
      "step": 21920
    },
    {
      "epoch": 11.154628687690742,
      "grad_norm": 17.50847816467285,
      "learning_rate": 3.884537131230926e-05,
      "loss": 2.7952,
      "step": 21930
    },
    {
      "epoch": 11.15971515768057,
      "grad_norm": 20.284120559692383,
      "learning_rate": 3.884028484231943e-05,
      "loss": 2.7651,
      "step": 21940
    },
    {
      "epoch": 11.164801627670396,
      "grad_norm": 19.648216247558594,
      "learning_rate": 3.883519837232961e-05,
      "loss": 2.7815,
      "step": 21950
    },
    {
      "epoch": 11.169888097660223,
      "grad_norm": 23.495031356811523,
      "learning_rate": 3.883011190233978e-05,
      "loss": 2.78,
      "step": 21960
    },
    {
      "epoch": 11.17497456765005,
      "grad_norm": 21.935138702392578,
      "learning_rate": 3.882502543234995e-05,
      "loss": 2.6901,
      "step": 21970
    },
    {
      "epoch": 11.180061037639877,
      "grad_norm": 22.16670036315918,
      "learning_rate": 3.881993896236012e-05,
      "loss": 2.7766,
      "step": 21980
    },
    {
      "epoch": 11.185147507629704,
      "grad_norm": 20.330371856689453,
      "learning_rate": 3.881485249237029e-05,
      "loss": 2.7464,
      "step": 21990
    },
    {
      "epoch": 11.190233977619531,
      "grad_norm": 19.362348556518555,
      "learning_rate": 3.880976602238047e-05,
      "loss": 2.7776,
      "step": 22000
    },
    {
      "epoch": 11.19532044760936,
      "grad_norm": 23.59122657775879,
      "learning_rate": 3.8804679552390647e-05,
      "loss": 2.8058,
      "step": 22010
    },
    {
      "epoch": 11.200406917599187,
      "grad_norm": 16.935983657836914,
      "learning_rate": 3.8799593082400816e-05,
      "loss": 2.7273,
      "step": 22020
    },
    {
      "epoch": 11.205493387589014,
      "grad_norm": 22.875896453857422,
      "learning_rate": 3.879450661241099e-05,
      "loss": 2.7446,
      "step": 22030
    },
    {
      "epoch": 11.210579857578841,
      "grad_norm": 20.28771209716797,
      "learning_rate": 3.878942014242116e-05,
      "loss": 2.7643,
      "step": 22040
    },
    {
      "epoch": 11.215666327568668,
      "grad_norm": 19.050264358520508,
      "learning_rate": 3.878433367243133e-05,
      "loss": 2.7525,
      "step": 22050
    },
    {
      "epoch": 11.220752797558495,
      "grad_norm": 20.540882110595703,
      "learning_rate": 3.877924720244151e-05,
      "loss": 2.699,
      "step": 22060
    },
    {
      "epoch": 11.225839267548322,
      "grad_norm": 17.459394454956055,
      "learning_rate": 3.877416073245168e-05,
      "loss": 2.7787,
      "step": 22070
    },
    {
      "epoch": 11.230925737538149,
      "grad_norm": 22.125869750976562,
      "learning_rate": 3.876907426246185e-05,
      "loss": 2.6883,
      "step": 22080
    },
    {
      "epoch": 11.236012207527976,
      "grad_norm": 20.926450729370117,
      "learning_rate": 3.8763987792472026e-05,
      "loss": 2.7848,
      "step": 22090
    },
    {
      "epoch": 11.241098677517803,
      "grad_norm": 19.29798126220703,
      "learning_rate": 3.87589013224822e-05,
      "loss": 2.6718,
      "step": 22100
    },
    {
      "epoch": 11.24618514750763,
      "grad_norm": 23.598087310791016,
      "learning_rate": 3.875381485249237e-05,
      "loss": 2.8116,
      "step": 22110
    },
    {
      "epoch": 11.251271617497457,
      "grad_norm": 18.177759170532227,
      "learning_rate": 3.874872838250255e-05,
      "loss": 2.8334,
      "step": 22120
    },
    {
      "epoch": 11.256358087487284,
      "grad_norm": 25.02385902404785,
      "learning_rate": 3.874364191251272e-05,
      "loss": 2.7129,
      "step": 22130
    },
    {
      "epoch": 11.26144455747711,
      "grad_norm": 25.077608108520508,
      "learning_rate": 3.873855544252289e-05,
      "loss": 2.7562,
      "step": 22140
    },
    {
      "epoch": 11.266531027466938,
      "grad_norm": 17.59563636779785,
      "learning_rate": 3.8733468972533066e-05,
      "loss": 2.8279,
      "step": 22150
    },
    {
      "epoch": 11.271617497456765,
      "grad_norm": 19.548038482666016,
      "learning_rate": 3.8728382502543236e-05,
      "loss": 2.7975,
      "step": 22160
    },
    {
      "epoch": 11.276703967446592,
      "grad_norm": 22.712125778198242,
      "learning_rate": 3.8723296032553405e-05,
      "loss": 2.6774,
      "step": 22170
    },
    {
      "epoch": 11.281790437436419,
      "grad_norm": 18.938879013061523,
      "learning_rate": 3.871820956256358e-05,
      "loss": 2.7574,
      "step": 22180
    },
    {
      "epoch": 11.286876907426246,
      "grad_norm": 22.769567489624023,
      "learning_rate": 3.871312309257375e-05,
      "loss": 2.7463,
      "step": 22190
    },
    {
      "epoch": 11.291963377416073,
      "grad_norm": 25.31669044494629,
      "learning_rate": 3.870803662258393e-05,
      "loss": 2.8361,
      "step": 22200
    },
    {
      "epoch": 11.2970498474059,
      "grad_norm": 17.42578125,
      "learning_rate": 3.8702950152594105e-05,
      "loss": 2.6502,
      "step": 22210
    },
    {
      "epoch": 11.302136317395727,
      "grad_norm": 21.840221405029297,
      "learning_rate": 3.8697863682604275e-05,
      "loss": 2.7144,
      "step": 22220
    },
    {
      "epoch": 11.307222787385555,
      "grad_norm": 17.296655654907227,
      "learning_rate": 3.8692777212614445e-05,
      "loss": 2.7414,
      "step": 22230
    },
    {
      "epoch": 11.312309257375382,
      "grad_norm": 27.0892276763916,
      "learning_rate": 3.868769074262462e-05,
      "loss": 2.6644,
      "step": 22240
    },
    {
      "epoch": 11.31739572736521,
      "grad_norm": 19.582462310791016,
      "learning_rate": 3.868260427263479e-05,
      "loss": 2.7764,
      "step": 22250
    },
    {
      "epoch": 11.322482197355036,
      "grad_norm": 21.026226043701172,
      "learning_rate": 3.867751780264496e-05,
      "loss": 2.8143,
      "step": 22260
    },
    {
      "epoch": 11.327568667344863,
      "grad_norm": 17.93621063232422,
      "learning_rate": 3.867243133265514e-05,
      "loss": 2.7692,
      "step": 22270
    },
    {
      "epoch": 11.33265513733469,
      "grad_norm": 20.652612686157227,
      "learning_rate": 3.866734486266531e-05,
      "loss": 2.7247,
      "step": 22280
    },
    {
      "epoch": 11.337741607324517,
      "grad_norm": 18.95530128479004,
      "learning_rate": 3.8662258392675485e-05,
      "loss": 2.7337,
      "step": 22290
    },
    {
      "epoch": 11.342828077314344,
      "grad_norm": 18.615957260131836,
      "learning_rate": 3.865717192268566e-05,
      "loss": 2.7624,
      "step": 22300
    },
    {
      "epoch": 11.347914547304171,
      "grad_norm": 23.713415145874023,
      "learning_rate": 3.865208545269583e-05,
      "loss": 2.7878,
      "step": 22310
    },
    {
      "epoch": 11.353001017293998,
      "grad_norm": 20.62632942199707,
      "learning_rate": 3.864699898270601e-05,
      "loss": 2.7039,
      "step": 22320
    },
    {
      "epoch": 11.358087487283825,
      "grad_norm": 21.4996280670166,
      "learning_rate": 3.864191251271618e-05,
      "loss": 2.7062,
      "step": 22330
    },
    {
      "epoch": 11.363173957273652,
      "grad_norm": 22.506004333496094,
      "learning_rate": 3.863682604272635e-05,
      "loss": 2.6431,
      "step": 22340
    },
    {
      "epoch": 11.368260427263479,
      "grad_norm": 24.230009078979492,
      "learning_rate": 3.8631739572736525e-05,
      "loss": 2.7219,
      "step": 22350
    },
    {
      "epoch": 11.373346897253306,
      "grad_norm": 27.39864730834961,
      "learning_rate": 3.8626653102746694e-05,
      "loss": 2.7361,
      "step": 22360
    },
    {
      "epoch": 11.378433367243133,
      "grad_norm": 28.71054458618164,
      "learning_rate": 3.8621566632756864e-05,
      "loss": 2.7715,
      "step": 22370
    },
    {
      "epoch": 11.38351983723296,
      "grad_norm": 19.783987045288086,
      "learning_rate": 3.861648016276704e-05,
      "loss": 2.767,
      "step": 22380
    },
    {
      "epoch": 11.388606307222787,
      "grad_norm": 19.646028518676758,
      "learning_rate": 3.861139369277722e-05,
      "loss": 2.7099,
      "step": 22390
    },
    {
      "epoch": 11.393692777212614,
      "grad_norm": 23.611141204833984,
      "learning_rate": 3.860630722278739e-05,
      "loss": 2.764,
      "step": 22400
    },
    {
      "epoch": 11.398779247202441,
      "grad_norm": 20.933557510375977,
      "learning_rate": 3.8601220752797564e-05,
      "loss": 2.728,
      "step": 22410
    },
    {
      "epoch": 11.403865717192268,
      "grad_norm": 21.95313835144043,
      "learning_rate": 3.8596134282807734e-05,
      "loss": 2.7731,
      "step": 22420
    },
    {
      "epoch": 11.408952187182095,
      "grad_norm": 20.134170532226562,
      "learning_rate": 3.8591047812817904e-05,
      "loss": 2.765,
      "step": 22430
    },
    {
      "epoch": 11.414038657171922,
      "grad_norm": 18.295637130737305,
      "learning_rate": 3.858596134282808e-05,
      "loss": 2.8092,
      "step": 22440
    },
    {
      "epoch": 11.419125127161749,
      "grad_norm": 24.829530715942383,
      "learning_rate": 3.858087487283825e-05,
      "loss": 2.6908,
      "step": 22450
    },
    {
      "epoch": 11.424211597151578,
      "grad_norm": 17.903717041015625,
      "learning_rate": 3.857578840284842e-05,
      "loss": 2.7469,
      "step": 22460
    },
    {
      "epoch": 11.429298067141405,
      "grad_norm": 20.671693801879883,
      "learning_rate": 3.85707019328586e-05,
      "loss": 2.7626,
      "step": 22470
    },
    {
      "epoch": 11.434384537131232,
      "grad_norm": 18.389856338500977,
      "learning_rate": 3.8565615462868774e-05,
      "loss": 2.7413,
      "step": 22480
    },
    {
      "epoch": 11.439471007121059,
      "grad_norm": 18.76555633544922,
      "learning_rate": 3.8560528992878944e-05,
      "loss": 2.6928,
      "step": 22490
    },
    {
      "epoch": 11.444557477110886,
      "grad_norm": 21.06430435180664,
      "learning_rate": 3.855544252288912e-05,
      "loss": 2.8367,
      "step": 22500
    },
    {
      "epoch": 11.449643947100713,
      "grad_norm": 21.72516632080078,
      "learning_rate": 3.855035605289929e-05,
      "loss": 2.7559,
      "step": 22510
    },
    {
      "epoch": 11.45473041709054,
      "grad_norm": 23.216697692871094,
      "learning_rate": 3.854526958290946e-05,
      "loss": 2.7641,
      "step": 22520
    },
    {
      "epoch": 11.459816887080366,
      "grad_norm": 20.101455688476562,
      "learning_rate": 3.854018311291964e-05,
      "loss": 2.7606,
      "step": 22530
    },
    {
      "epoch": 11.464903357070193,
      "grad_norm": 18.242259979248047,
      "learning_rate": 3.853509664292981e-05,
      "loss": 2.7812,
      "step": 22540
    },
    {
      "epoch": 11.46998982706002,
      "grad_norm": 24.263858795166016,
      "learning_rate": 3.8530010172939983e-05,
      "loss": 2.6527,
      "step": 22550
    },
    {
      "epoch": 11.475076297049847,
      "grad_norm": 17.589601516723633,
      "learning_rate": 3.852492370295015e-05,
      "loss": 2.7722,
      "step": 22560
    },
    {
      "epoch": 11.480162767039674,
      "grad_norm": 20.782367706298828,
      "learning_rate": 3.851983723296032e-05,
      "loss": 2.7217,
      "step": 22570
    },
    {
      "epoch": 11.485249237029501,
      "grad_norm": 18.259265899658203,
      "learning_rate": 3.85147507629705e-05,
      "loss": 2.7177,
      "step": 22580
    },
    {
      "epoch": 11.490335707019328,
      "grad_norm": 17.78154182434082,
      "learning_rate": 3.8509664292980677e-05,
      "loss": 2.6845,
      "step": 22590
    },
    {
      "epoch": 11.495422177009155,
      "grad_norm": 22.478017807006836,
      "learning_rate": 3.8504577822990846e-05,
      "loss": 2.7473,
      "step": 22600
    },
    {
      "epoch": 11.500508646998982,
      "grad_norm": 16.650049209594727,
      "learning_rate": 3.849949135300102e-05,
      "loss": 2.7313,
      "step": 22610
    },
    {
      "epoch": 11.50559511698881,
      "grad_norm": 23.858585357666016,
      "learning_rate": 3.849440488301119e-05,
      "loss": 2.7598,
      "step": 22620
    },
    {
      "epoch": 11.510681586978636,
      "grad_norm": 22.192401885986328,
      "learning_rate": 3.848931841302136e-05,
      "loss": 2.7194,
      "step": 22630
    },
    {
      "epoch": 11.515768056968463,
      "grad_norm": 22.126386642456055,
      "learning_rate": 3.848423194303154e-05,
      "loss": 2.793,
      "step": 22640
    },
    {
      "epoch": 11.52085452695829,
      "grad_norm": 25.98366928100586,
      "learning_rate": 3.847914547304171e-05,
      "loss": 2.7051,
      "step": 22650
    },
    {
      "epoch": 11.525940996948117,
      "grad_norm": 19.739225387573242,
      "learning_rate": 3.847405900305188e-05,
      "loss": 2.8538,
      "step": 22660
    },
    {
      "epoch": 11.531027466937946,
      "grad_norm": 20.940166473388672,
      "learning_rate": 3.8468972533062056e-05,
      "loss": 2.8082,
      "step": 22670
    },
    {
      "epoch": 11.536113936927773,
      "grad_norm": 29.916135787963867,
      "learning_rate": 3.846388606307223e-05,
      "loss": 2.7299,
      "step": 22680
    },
    {
      "epoch": 11.5412004069176,
      "grad_norm": 21.456947326660156,
      "learning_rate": 3.84587995930824e-05,
      "loss": 2.7166,
      "step": 22690
    },
    {
      "epoch": 11.546286876907427,
      "grad_norm": 27.4039306640625,
      "learning_rate": 3.845371312309258e-05,
      "loss": 2.8003,
      "step": 22700
    },
    {
      "epoch": 11.551373346897254,
      "grad_norm": 29.057659149169922,
      "learning_rate": 3.844862665310275e-05,
      "loss": 2.7034,
      "step": 22710
    },
    {
      "epoch": 11.556459816887081,
      "grad_norm": 20.60205078125,
      "learning_rate": 3.844354018311292e-05,
      "loss": 2.7352,
      "step": 22720
    },
    {
      "epoch": 11.561546286876908,
      "grad_norm": 17.344371795654297,
      "learning_rate": 3.8438453713123096e-05,
      "loss": 2.712,
      "step": 22730
    },
    {
      "epoch": 11.566632756866735,
      "grad_norm": 21.179920196533203,
      "learning_rate": 3.8433367243133266e-05,
      "loss": 2.7395,
      "step": 22740
    },
    {
      "epoch": 11.571719226856562,
      "grad_norm": 18.53753662109375,
      "learning_rate": 3.8428280773143435e-05,
      "loss": 2.765,
      "step": 22750
    },
    {
      "epoch": 11.576805696846389,
      "grad_norm": 22.25364112854004,
      "learning_rate": 3.842319430315361e-05,
      "loss": 2.7905,
      "step": 22760
    },
    {
      "epoch": 11.581892166836216,
      "grad_norm": 16.675872802734375,
      "learning_rate": 3.841810783316379e-05,
      "loss": 2.7523,
      "step": 22770
    },
    {
      "epoch": 11.586978636826043,
      "grad_norm": 26.369140625,
      "learning_rate": 3.841302136317396e-05,
      "loss": 2.8226,
      "step": 22780
    },
    {
      "epoch": 11.59206510681587,
      "grad_norm": 22.263925552368164,
      "learning_rate": 3.8407934893184135e-05,
      "loss": 2.7052,
      "step": 22790
    },
    {
      "epoch": 11.597151576805697,
      "grad_norm": 19.93617820739746,
      "learning_rate": 3.8402848423194305e-05,
      "loss": 2.7857,
      "step": 22800
    },
    {
      "epoch": 11.602238046795524,
      "grad_norm": 22.288599014282227,
      "learning_rate": 3.8397761953204475e-05,
      "loss": 2.8105,
      "step": 22810
    },
    {
      "epoch": 11.60732451678535,
      "grad_norm": 16.88165283203125,
      "learning_rate": 3.839267548321465e-05,
      "loss": 2.7894,
      "step": 22820
    },
    {
      "epoch": 11.612410986775178,
      "grad_norm": 21.28264808654785,
      "learning_rate": 3.838758901322482e-05,
      "loss": 2.7655,
      "step": 22830
    },
    {
      "epoch": 11.617497456765005,
      "grad_norm": 24.211139678955078,
      "learning_rate": 3.8382502543235e-05,
      "loss": 2.8007,
      "step": 22840
    },
    {
      "epoch": 11.622583926754832,
      "grad_norm": 21.572608947753906,
      "learning_rate": 3.837741607324517e-05,
      "loss": 2.6878,
      "step": 22850
    },
    {
      "epoch": 11.627670396744659,
      "grad_norm": 17.74789810180664,
      "learning_rate": 3.837232960325534e-05,
      "loss": 2.8582,
      "step": 22860
    },
    {
      "epoch": 11.632756866734486,
      "grad_norm": 22.900728225708008,
      "learning_rate": 3.8367243133265515e-05,
      "loss": 2.684,
      "step": 22870
    },
    {
      "epoch": 11.637843336724313,
      "grad_norm": 18.13514518737793,
      "learning_rate": 3.836215666327569e-05,
      "loss": 2.7193,
      "step": 22880
    },
    {
      "epoch": 11.64292980671414,
      "grad_norm": 19.483444213867188,
      "learning_rate": 3.835707019328586e-05,
      "loss": 2.7045,
      "step": 22890
    },
    {
      "epoch": 11.648016276703967,
      "grad_norm": 20.16979217529297,
      "learning_rate": 3.835198372329604e-05,
      "loss": 2.7679,
      "step": 22900
    },
    {
      "epoch": 11.653102746693795,
      "grad_norm": 17.827043533325195,
      "learning_rate": 3.834689725330621e-05,
      "loss": 2.7582,
      "step": 22910
    },
    {
      "epoch": 11.658189216683622,
      "grad_norm": 20.238149642944336,
      "learning_rate": 3.834181078331638e-05,
      "loss": 2.8037,
      "step": 22920
    },
    {
      "epoch": 11.66327568667345,
      "grad_norm": 22.087657928466797,
      "learning_rate": 3.8336724313326555e-05,
      "loss": 2.7453,
      "step": 22930
    },
    {
      "epoch": 11.668362156663276,
      "grad_norm": 26.165386199951172,
      "learning_rate": 3.8331637843336724e-05,
      "loss": 2.74,
      "step": 22940
    },
    {
      "epoch": 11.673448626653103,
      "grad_norm": 27.711084365844727,
      "learning_rate": 3.8326551373346894e-05,
      "loss": 2.702,
      "step": 22950
    },
    {
      "epoch": 11.67853509664293,
      "grad_norm": 25.10924530029297,
      "learning_rate": 3.832146490335707e-05,
      "loss": 2.7585,
      "step": 22960
    },
    {
      "epoch": 11.683621566632757,
      "grad_norm": 26.3699893951416,
      "learning_rate": 3.831637843336725e-05,
      "loss": 2.7129,
      "step": 22970
    },
    {
      "epoch": 11.688708036622584,
      "grad_norm": 18.448970794677734,
      "learning_rate": 3.831129196337742e-05,
      "loss": 2.7261,
      "step": 22980
    },
    {
      "epoch": 11.693794506612411,
      "grad_norm": 20.940092086791992,
      "learning_rate": 3.8306205493387594e-05,
      "loss": 2.7522,
      "step": 22990
    },
    {
      "epoch": 11.698880976602238,
      "grad_norm": 18.03622817993164,
      "learning_rate": 3.8301119023397764e-05,
      "loss": 2.6714,
      "step": 23000
    },
    {
      "epoch": 11.703967446592065,
      "grad_norm": 20.920135498046875,
      "learning_rate": 3.8296032553407934e-05,
      "loss": 2.6858,
      "step": 23010
    },
    {
      "epoch": 11.709053916581892,
      "grad_norm": 21.365331649780273,
      "learning_rate": 3.829094608341811e-05,
      "loss": 2.7621,
      "step": 23020
    },
    {
      "epoch": 11.714140386571719,
      "grad_norm": 23.307693481445312,
      "learning_rate": 3.828585961342828e-05,
      "loss": 2.7697,
      "step": 23030
    },
    {
      "epoch": 11.719226856561546,
      "grad_norm": 16.703868865966797,
      "learning_rate": 3.828077314343845e-05,
      "loss": 2.767,
      "step": 23040
    },
    {
      "epoch": 11.724313326551373,
      "grad_norm": 17.773300170898438,
      "learning_rate": 3.827568667344863e-05,
      "loss": 2.7954,
      "step": 23050
    },
    {
      "epoch": 11.7293997965412,
      "grad_norm": 15.35882568359375,
      "learning_rate": 3.8270600203458804e-05,
      "loss": 2.8252,
      "step": 23060
    },
    {
      "epoch": 11.734486266531027,
      "grad_norm": 25.92255973815918,
      "learning_rate": 3.8265513733468974e-05,
      "loss": 2.7157,
      "step": 23070
    },
    {
      "epoch": 11.739572736520854,
      "grad_norm": 21.81801986694336,
      "learning_rate": 3.826042726347915e-05,
      "loss": 2.7648,
      "step": 23080
    },
    {
      "epoch": 11.744659206510681,
      "grad_norm": 25.16291046142578,
      "learning_rate": 3.825534079348932e-05,
      "loss": 2.681,
      "step": 23090
    },
    {
      "epoch": 11.749745676500508,
      "grad_norm": 21.363759994506836,
      "learning_rate": 3.82502543234995e-05,
      "loss": 2.7215,
      "step": 23100
    },
    {
      "epoch": 11.754832146490335,
      "grad_norm": 22.65402603149414,
      "learning_rate": 3.824516785350967e-05,
      "loss": 2.7144,
      "step": 23110
    },
    {
      "epoch": 11.759918616480164,
      "grad_norm": 20.596435546875,
      "learning_rate": 3.824008138351984e-05,
      "loss": 2.7762,
      "step": 23120
    },
    {
      "epoch": 11.76500508646999,
      "grad_norm": 24.69519805908203,
      "learning_rate": 3.8234994913530013e-05,
      "loss": 2.7469,
      "step": 23130
    },
    {
      "epoch": 11.770091556459818,
      "grad_norm": 18.40545082092285,
      "learning_rate": 3.822990844354018e-05,
      "loss": 2.7366,
      "step": 23140
    },
    {
      "epoch": 11.775178026449645,
      "grad_norm": 23.516925811767578,
      "learning_rate": 3.822482197355035e-05,
      "loss": 2.8107,
      "step": 23150
    },
    {
      "epoch": 11.780264496439472,
      "grad_norm": 20.792421340942383,
      "learning_rate": 3.821973550356053e-05,
      "loss": 2.7109,
      "step": 23160
    },
    {
      "epoch": 11.785350966429299,
      "grad_norm": 19.997913360595703,
      "learning_rate": 3.8214649033570707e-05,
      "loss": 2.8083,
      "step": 23170
    },
    {
      "epoch": 11.790437436419126,
      "grad_norm": 20.980770111083984,
      "learning_rate": 3.8209562563580876e-05,
      "loss": 2.677,
      "step": 23180
    },
    {
      "epoch": 11.795523906408953,
      "grad_norm": 21.941648483276367,
      "learning_rate": 3.820447609359105e-05,
      "loss": 2.6774,
      "step": 23190
    },
    {
      "epoch": 11.80061037639878,
      "grad_norm": 22.13533592224121,
      "learning_rate": 3.819938962360122e-05,
      "loss": 2.7454,
      "step": 23200
    },
    {
      "epoch": 11.805696846388607,
      "grad_norm": 17.897220611572266,
      "learning_rate": 3.819430315361139e-05,
      "loss": 2.6526,
      "step": 23210
    },
    {
      "epoch": 11.810783316378433,
      "grad_norm": 22.154001235961914,
      "learning_rate": 3.818921668362157e-05,
      "loss": 2.762,
      "step": 23220
    },
    {
      "epoch": 11.81586978636826,
      "grad_norm": 19.805526733398438,
      "learning_rate": 3.818413021363174e-05,
      "loss": 2.7614,
      "step": 23230
    },
    {
      "epoch": 11.820956256358087,
      "grad_norm": 23.51304817199707,
      "learning_rate": 3.817904374364191e-05,
      "loss": 2.71,
      "step": 23240
    },
    {
      "epoch": 11.826042726347914,
      "grad_norm": 22.89546775817871,
      "learning_rate": 3.8173957273652086e-05,
      "loss": 2.679,
      "step": 23250
    },
    {
      "epoch": 11.831129196337741,
      "grad_norm": 22.817970275878906,
      "learning_rate": 3.816887080366226e-05,
      "loss": 2.6394,
      "step": 23260
    },
    {
      "epoch": 11.836215666327568,
      "grad_norm": 21.458045959472656,
      "learning_rate": 3.816378433367243e-05,
      "loss": 2.7169,
      "step": 23270
    },
    {
      "epoch": 11.841302136317395,
      "grad_norm": 20.261425018310547,
      "learning_rate": 3.815869786368261e-05,
      "loss": 2.6307,
      "step": 23280
    },
    {
      "epoch": 11.846388606307222,
      "grad_norm": 23.497644424438477,
      "learning_rate": 3.815361139369278e-05,
      "loss": 2.8021,
      "step": 23290
    },
    {
      "epoch": 11.85147507629705,
      "grad_norm": 30.9713191986084,
      "learning_rate": 3.814852492370295e-05,
      "loss": 2.6513,
      "step": 23300
    },
    {
      "epoch": 11.856561546286876,
      "grad_norm": 21.62212371826172,
      "learning_rate": 3.8143438453713126e-05,
      "loss": 2.7771,
      "step": 23310
    },
    {
      "epoch": 11.861648016276703,
      "grad_norm": 20.88262367248535,
      "learning_rate": 3.8138351983723296e-05,
      "loss": 2.6782,
      "step": 23320
    },
    {
      "epoch": 11.86673448626653,
      "grad_norm": 22.68789291381836,
      "learning_rate": 3.8133265513733466e-05,
      "loss": 2.7256,
      "step": 23330
    },
    {
      "epoch": 11.871820956256357,
      "grad_norm": 21.609699249267578,
      "learning_rate": 3.812817904374364e-05,
      "loss": 2.7379,
      "step": 23340
    },
    {
      "epoch": 11.876907426246184,
      "grad_norm": 21.238685607910156,
      "learning_rate": 3.812309257375382e-05,
      "loss": 2.81,
      "step": 23350
    },
    {
      "epoch": 11.881993896236013,
      "grad_norm": 19.249977111816406,
      "learning_rate": 3.8118006103763996e-05,
      "loss": 2.748,
      "step": 23360
    },
    {
      "epoch": 11.88708036622584,
      "grad_norm": 22.776628494262695,
      "learning_rate": 3.8112919633774165e-05,
      "loss": 2.6818,
      "step": 23370
    },
    {
      "epoch": 11.892166836215667,
      "grad_norm": 22.226898193359375,
      "learning_rate": 3.8107833163784335e-05,
      "loss": 2.7615,
      "step": 23380
    },
    {
      "epoch": 11.897253306205494,
      "grad_norm": 18.71812629699707,
      "learning_rate": 3.810274669379451e-05,
      "loss": 2.6861,
      "step": 23390
    },
    {
      "epoch": 11.902339776195321,
      "grad_norm": 21.156885147094727,
      "learning_rate": 3.809766022380468e-05,
      "loss": 2.7868,
      "step": 23400
    },
    {
      "epoch": 11.907426246185148,
      "grad_norm": 17.988422393798828,
      "learning_rate": 3.809257375381485e-05,
      "loss": 2.6769,
      "step": 23410
    },
    {
      "epoch": 11.912512716174975,
      "grad_norm": 18.4997615814209,
      "learning_rate": 3.808748728382503e-05,
      "loss": 2.6796,
      "step": 23420
    },
    {
      "epoch": 11.917599186164802,
      "grad_norm": 25.454391479492188,
      "learning_rate": 3.80824008138352e-05,
      "loss": 2.683,
      "step": 23430
    },
    {
      "epoch": 11.922685656154629,
      "grad_norm": 19.7291202545166,
      "learning_rate": 3.8077314343845375e-05,
      "loss": 2.7556,
      "step": 23440
    },
    {
      "epoch": 11.927772126144456,
      "grad_norm": 21.255508422851562,
      "learning_rate": 3.8072227873855545e-05,
      "loss": 2.7014,
      "step": 23450
    },
    {
      "epoch": 11.932858596134283,
      "grad_norm": 22.745594024658203,
      "learning_rate": 3.806714140386572e-05,
      "loss": 2.6764,
      "step": 23460
    },
    {
      "epoch": 11.93794506612411,
      "grad_norm": 20.982160568237305,
      "learning_rate": 3.806205493387589e-05,
      "loss": 2.6843,
      "step": 23470
    },
    {
      "epoch": 11.943031536113937,
      "grad_norm": 18.96265411376953,
      "learning_rate": 3.805696846388607e-05,
      "loss": 2.6503,
      "step": 23480
    },
    {
      "epoch": 11.948118006103764,
      "grad_norm": 23.764846801757812,
      "learning_rate": 3.805188199389624e-05,
      "loss": 2.7424,
      "step": 23490
    },
    {
      "epoch": 11.95320447609359,
      "grad_norm": 19.742849349975586,
      "learning_rate": 3.804679552390641e-05,
      "loss": 2.6703,
      "step": 23500
    },
    {
      "epoch": 11.958290946083418,
      "grad_norm": 25.065935134887695,
      "learning_rate": 3.8041709053916585e-05,
      "loss": 2.7362,
      "step": 23510
    },
    {
      "epoch": 11.963377416073245,
      "grad_norm": 24.199199676513672,
      "learning_rate": 3.8036622583926754e-05,
      "loss": 2.6342,
      "step": 23520
    },
    {
      "epoch": 11.968463886063072,
      "grad_norm": 18.500469207763672,
      "learning_rate": 3.8031536113936924e-05,
      "loss": 2.7023,
      "step": 23530
    },
    {
      "epoch": 11.973550356052899,
      "grad_norm": 24.840778350830078,
      "learning_rate": 3.80264496439471e-05,
      "loss": 2.6243,
      "step": 23540
    },
    {
      "epoch": 11.978636826042726,
      "grad_norm": 21.492219924926758,
      "learning_rate": 3.802136317395728e-05,
      "loss": 2.6947,
      "step": 23550
    },
    {
      "epoch": 11.983723296032553,
      "grad_norm": 23.950246810913086,
      "learning_rate": 3.801627670396745e-05,
      "loss": 2.7558,
      "step": 23560
    },
    {
      "epoch": 11.988809766022381,
      "grad_norm": 25.359594345092773,
      "learning_rate": 3.8011190233977624e-05,
      "loss": 2.7564,
      "step": 23570
    },
    {
      "epoch": 11.993896236012208,
      "grad_norm": 21.786489486694336,
      "learning_rate": 3.8006103763987794e-05,
      "loss": 2.669,
      "step": 23580
    },
    {
      "epoch": 11.998982706002035,
      "grad_norm": 22.791088104248047,
      "learning_rate": 3.8001017293997964e-05,
      "loss": 2.6293,
      "step": 23590
    },
    {
      "epoch": 12.0,
      "eval_loss": 3.915283203125,
      "eval_runtime": 2.7509,
      "eval_samples_per_second": 1008.768,
      "eval_steps_per_second": 126.141,
      "step": 23592
    },
    {
      "epoch": 12.004069175991862,
      "grad_norm": 23.64306640625,
      "learning_rate": 3.799593082400814e-05,
      "loss": 2.6117,
      "step": 23600
    },
    {
      "epoch": 12.00915564598169,
      "grad_norm": 19.931962966918945,
      "learning_rate": 3.799084435401831e-05,
      "loss": 2.7023,
      "step": 23610
    },
    {
      "epoch": 12.014242115971516,
      "grad_norm": 30.26859474182129,
      "learning_rate": 3.798575788402848e-05,
      "loss": 2.757,
      "step": 23620
    },
    {
      "epoch": 12.019328585961343,
      "grad_norm": 18.531997680664062,
      "learning_rate": 3.798067141403866e-05,
      "loss": 2.6647,
      "step": 23630
    },
    {
      "epoch": 12.02441505595117,
      "grad_norm": 18.512592315673828,
      "learning_rate": 3.7975584944048834e-05,
      "loss": 2.7332,
      "step": 23640
    },
    {
      "epoch": 12.029501525940997,
      "grad_norm": 19.526838302612305,
      "learning_rate": 3.797049847405901e-05,
      "loss": 2.6707,
      "step": 23650
    },
    {
      "epoch": 12.034587995930824,
      "grad_norm": 21.532014846801758,
      "learning_rate": 3.796541200406918e-05,
      "loss": 2.6885,
      "step": 23660
    },
    {
      "epoch": 12.039674465920651,
      "grad_norm": 22.844684600830078,
      "learning_rate": 3.796032553407935e-05,
      "loss": 2.7357,
      "step": 23670
    },
    {
      "epoch": 12.044760935910478,
      "grad_norm": 22.71563148498535,
      "learning_rate": 3.795523906408953e-05,
      "loss": 2.683,
      "step": 23680
    },
    {
      "epoch": 12.049847405900305,
      "grad_norm": 21.753137588500977,
      "learning_rate": 3.79501525940997e-05,
      "loss": 2.634,
      "step": 23690
    },
    {
      "epoch": 12.054933875890132,
      "grad_norm": 21.144393920898438,
      "learning_rate": 3.794506612410987e-05,
      "loss": 2.6952,
      "step": 23700
    },
    {
      "epoch": 12.060020345879959,
      "grad_norm": 23.900447845458984,
      "learning_rate": 3.7939979654120043e-05,
      "loss": 2.703,
      "step": 23710
    },
    {
      "epoch": 12.065106815869786,
      "grad_norm": 22.491506576538086,
      "learning_rate": 3.793489318413021e-05,
      "loss": 2.7753,
      "step": 23720
    },
    {
      "epoch": 12.070193285859613,
      "grad_norm": 22.788833618164062,
      "learning_rate": 3.792980671414039e-05,
      "loss": 2.6322,
      "step": 23730
    },
    {
      "epoch": 12.07527975584944,
      "grad_norm": 25.78053092956543,
      "learning_rate": 3.792472024415057e-05,
      "loss": 2.627,
      "step": 23740
    },
    {
      "epoch": 12.080366225839267,
      "grad_norm": 22.091840744018555,
      "learning_rate": 3.7919633774160737e-05,
      "loss": 2.7568,
      "step": 23750
    },
    {
      "epoch": 12.085452695829094,
      "grad_norm": 19.088794708251953,
      "learning_rate": 3.7914547304170906e-05,
      "loss": 2.7138,
      "step": 23760
    },
    {
      "epoch": 12.090539165818921,
      "grad_norm": 24.958053588867188,
      "learning_rate": 3.790946083418108e-05,
      "loss": 2.6703,
      "step": 23770
    },
    {
      "epoch": 12.095625635808748,
      "grad_norm": 23.14921760559082,
      "learning_rate": 3.790437436419125e-05,
      "loss": 2.6239,
      "step": 23780
    },
    {
      "epoch": 12.100712105798575,
      "grad_norm": 21.411001205444336,
      "learning_rate": 3.789928789420142e-05,
      "loss": 2.7236,
      "step": 23790
    },
    {
      "epoch": 12.105798575788404,
      "grad_norm": 24.313465118408203,
      "learning_rate": 3.78942014242116e-05,
      "loss": 2.6964,
      "step": 23800
    },
    {
      "epoch": 12.11088504577823,
      "grad_norm": 24.152976989746094,
      "learning_rate": 3.788911495422177e-05,
      "loss": 2.6817,
      "step": 23810
    },
    {
      "epoch": 12.115971515768058,
      "grad_norm": 32.859519958496094,
      "learning_rate": 3.788402848423194e-05,
      "loss": 2.6668,
      "step": 23820
    },
    {
      "epoch": 12.121057985757885,
      "grad_norm": 23.752243041992188,
      "learning_rate": 3.7878942014242116e-05,
      "loss": 2.7015,
      "step": 23830
    },
    {
      "epoch": 12.126144455747712,
      "grad_norm": 23.250837326049805,
      "learning_rate": 3.787385554425229e-05,
      "loss": 2.6177,
      "step": 23840
    },
    {
      "epoch": 12.131230925737539,
      "grad_norm": 19.695091247558594,
      "learning_rate": 3.786876907426246e-05,
      "loss": 2.6807,
      "step": 23850
    },
    {
      "epoch": 12.136317395727366,
      "grad_norm": 23.83995819091797,
      "learning_rate": 3.786368260427264e-05,
      "loss": 2.6718,
      "step": 23860
    },
    {
      "epoch": 12.141403865717193,
      "grad_norm": 26.3787841796875,
      "learning_rate": 3.785859613428281e-05,
      "loss": 2.5109,
      "step": 23870
    },
    {
      "epoch": 12.14649033570702,
      "grad_norm": 27.574064254760742,
      "learning_rate": 3.785350966429298e-05,
      "loss": 2.7356,
      "step": 23880
    },
    {
      "epoch": 12.151576805696847,
      "grad_norm": 21.5371036529541,
      "learning_rate": 3.7848423194303156e-05,
      "loss": 2.7082,
      "step": 23890
    },
    {
      "epoch": 12.156663275686673,
      "grad_norm": 25.367061614990234,
      "learning_rate": 3.7843336724313326e-05,
      "loss": 2.6922,
      "step": 23900
    },
    {
      "epoch": 12.1617497456765,
      "grad_norm": 20.630523681640625,
      "learning_rate": 3.78382502543235e-05,
      "loss": 2.613,
      "step": 23910
    },
    {
      "epoch": 12.166836215666327,
      "grad_norm": 27.875856399536133,
      "learning_rate": 3.783316378433367e-05,
      "loss": 2.6544,
      "step": 23920
    },
    {
      "epoch": 12.171922685656154,
      "grad_norm": 24.72232437133789,
      "learning_rate": 3.782807731434385e-05,
      "loss": 2.6223,
      "step": 23930
    },
    {
      "epoch": 12.177009155645981,
      "grad_norm": 22.183271408081055,
      "learning_rate": 3.7822990844354026e-05,
      "loss": 2.6477,
      "step": 23940
    },
    {
      "epoch": 12.182095625635808,
      "grad_norm": 27.143115997314453,
      "learning_rate": 3.7817904374364195e-05,
      "loss": 2.7223,
      "step": 23950
    },
    {
      "epoch": 12.187182095625635,
      "grad_norm": 25.694103240966797,
      "learning_rate": 3.7812817904374365e-05,
      "loss": 2.6747,
      "step": 23960
    },
    {
      "epoch": 12.192268565615462,
      "grad_norm": 18.88129997253418,
      "learning_rate": 3.780773143438454e-05,
      "loss": 2.6861,
      "step": 23970
    },
    {
      "epoch": 12.19735503560529,
      "grad_norm": 29.18071937561035,
      "learning_rate": 3.780264496439471e-05,
      "loss": 2.6744,
      "step": 23980
    },
    {
      "epoch": 12.202441505595116,
      "grad_norm": 27.141380310058594,
      "learning_rate": 3.779755849440488e-05,
      "loss": 2.6126,
      "step": 23990
    },
    {
      "epoch": 12.207527975584943,
      "grad_norm": 22.978729248046875,
      "learning_rate": 3.779247202441506e-05,
      "loss": 2.6795,
      "step": 24000
    },
    {
      "epoch": 12.21261444557477,
      "grad_norm": 21.36687469482422,
      "learning_rate": 3.778738555442523e-05,
      "loss": 2.6681,
      "step": 24010
    },
    {
      "epoch": 12.217700915564599,
      "grad_norm": 17.419103622436523,
      "learning_rate": 3.7782299084435405e-05,
      "loss": 2.5826,
      "step": 24020
    },
    {
      "epoch": 12.222787385554426,
      "grad_norm": 22.839284896850586,
      "learning_rate": 3.777721261444558e-05,
      "loss": 2.7144,
      "step": 24030
    },
    {
      "epoch": 12.227873855544253,
      "grad_norm": 21.660572052001953,
      "learning_rate": 3.777212614445575e-05,
      "loss": 2.7287,
      "step": 24040
    },
    {
      "epoch": 12.23296032553408,
      "grad_norm": 28.513032913208008,
      "learning_rate": 3.776703967446592e-05,
      "loss": 2.65,
      "step": 24050
    },
    {
      "epoch": 12.238046795523907,
      "grad_norm": 18.49991798400879,
      "learning_rate": 3.77619532044761e-05,
      "loss": 2.6233,
      "step": 24060
    },
    {
      "epoch": 12.243133265513734,
      "grad_norm": 22.79060173034668,
      "learning_rate": 3.775686673448627e-05,
      "loss": 2.7496,
      "step": 24070
    },
    {
      "epoch": 12.248219735503561,
      "grad_norm": 25.163925170898438,
      "learning_rate": 3.775178026449644e-05,
      "loss": 2.6773,
      "step": 24080
    },
    {
      "epoch": 12.253306205493388,
      "grad_norm": 30.001605987548828,
      "learning_rate": 3.7746693794506615e-05,
      "loss": 2.6553,
      "step": 24090
    },
    {
      "epoch": 12.258392675483215,
      "grad_norm": 23.574676513671875,
      "learning_rate": 3.7741607324516784e-05,
      "loss": 2.7245,
      "step": 24100
    },
    {
      "epoch": 12.263479145473042,
      "grad_norm": 22.940324783325195,
      "learning_rate": 3.7736520854526954e-05,
      "loss": 2.673,
      "step": 24110
    },
    {
      "epoch": 12.268565615462869,
      "grad_norm": 26.217790603637695,
      "learning_rate": 3.773143438453713e-05,
      "loss": 2.614,
      "step": 24120
    },
    {
      "epoch": 12.273652085452696,
      "grad_norm": 18.99417495727539,
      "learning_rate": 3.772634791454731e-05,
      "loss": 2.6861,
      "step": 24130
    },
    {
      "epoch": 12.278738555442523,
      "grad_norm": 22.727205276489258,
      "learning_rate": 3.772126144455748e-05,
      "loss": 2.6612,
      "step": 24140
    },
    {
      "epoch": 12.28382502543235,
      "grad_norm": 23.92072868347168,
      "learning_rate": 3.7716174974567654e-05,
      "loss": 2.7464,
      "step": 24150
    },
    {
      "epoch": 12.288911495422177,
      "grad_norm": 26.392030715942383,
      "learning_rate": 3.7711088504577824e-05,
      "loss": 2.6486,
      "step": 24160
    },
    {
      "epoch": 12.293997965412004,
      "grad_norm": 19.62209129333496,
      "learning_rate": 3.7706002034588e-05,
      "loss": 2.7106,
      "step": 24170
    },
    {
      "epoch": 12.29908443540183,
      "grad_norm": 24.04806900024414,
      "learning_rate": 3.770091556459817e-05,
      "loss": 2.7174,
      "step": 24180
    },
    {
      "epoch": 12.304170905391658,
      "grad_norm": 21.87887191772461,
      "learning_rate": 3.769582909460834e-05,
      "loss": 2.6209,
      "step": 24190
    },
    {
      "epoch": 12.309257375381485,
      "grad_norm": 22.95487403869629,
      "learning_rate": 3.769074262461852e-05,
      "loss": 2.6907,
      "step": 24200
    },
    {
      "epoch": 12.314343845371312,
      "grad_norm": 21.286191940307617,
      "learning_rate": 3.768565615462869e-05,
      "loss": 2.6687,
      "step": 24210
    },
    {
      "epoch": 12.319430315361139,
      "grad_norm": 25.627927780151367,
      "learning_rate": 3.7680569684638864e-05,
      "loss": 2.6743,
      "step": 24220
    },
    {
      "epoch": 12.324516785350966,
      "grad_norm": 25.10276985168457,
      "learning_rate": 3.767548321464904e-05,
      "loss": 2.6774,
      "step": 24230
    },
    {
      "epoch": 12.329603255340793,
      "grad_norm": 25.0637149810791,
      "learning_rate": 3.767039674465921e-05,
      "loss": 2.6631,
      "step": 24240
    },
    {
      "epoch": 12.334689725330621,
      "grad_norm": 26.309511184692383,
      "learning_rate": 3.766531027466938e-05,
      "loss": 2.6886,
      "step": 24250
    },
    {
      "epoch": 12.339776195320448,
      "grad_norm": 28.397687911987305,
      "learning_rate": 3.766022380467956e-05,
      "loss": 2.6496,
      "step": 24260
    },
    {
      "epoch": 12.344862665310275,
      "grad_norm": 25.42385482788086,
      "learning_rate": 3.765513733468973e-05,
      "loss": 2.6694,
      "step": 24270
    },
    {
      "epoch": 12.349949135300102,
      "grad_norm": 24.64238166809082,
      "learning_rate": 3.76500508646999e-05,
      "loss": 2.6303,
      "step": 24280
    },
    {
      "epoch": 12.35503560528993,
      "grad_norm": 29.019683837890625,
      "learning_rate": 3.7644964394710073e-05,
      "loss": 2.629,
      "step": 24290
    },
    {
      "epoch": 12.360122075279756,
      "grad_norm": 23.278120040893555,
      "learning_rate": 3.763987792472024e-05,
      "loss": 2.6768,
      "step": 24300
    },
    {
      "epoch": 12.365208545269583,
      "grad_norm": 26.246950149536133,
      "learning_rate": 3.763479145473042e-05,
      "loss": 2.5796,
      "step": 24310
    },
    {
      "epoch": 12.37029501525941,
      "grad_norm": 24.53049087524414,
      "learning_rate": 3.76297049847406e-05,
      "loss": 2.6833,
      "step": 24320
    },
    {
      "epoch": 12.375381485249237,
      "grad_norm": 22.054405212402344,
      "learning_rate": 3.7624618514750767e-05,
      "loss": 2.6804,
      "step": 24330
    },
    {
      "epoch": 12.380467955239064,
      "grad_norm": 22.453510284423828,
      "learning_rate": 3.7619532044760936e-05,
      "loss": 2.6725,
      "step": 24340
    },
    {
      "epoch": 12.385554425228891,
      "grad_norm": 22.134361267089844,
      "learning_rate": 3.761444557477111e-05,
      "loss": 2.648,
      "step": 24350
    },
    {
      "epoch": 12.390640895218718,
      "grad_norm": 20.150243759155273,
      "learning_rate": 3.760935910478128e-05,
      "loss": 2.7037,
      "step": 24360
    },
    {
      "epoch": 12.395727365208545,
      "grad_norm": 20.64900779724121,
      "learning_rate": 3.760427263479145e-05,
      "loss": 2.6317,
      "step": 24370
    },
    {
      "epoch": 12.400813835198372,
      "grad_norm": 26.719112396240234,
      "learning_rate": 3.759918616480163e-05,
      "loss": 2.7019,
      "step": 24380
    },
    {
      "epoch": 12.405900305188199,
      "grad_norm": 22.205528259277344,
      "learning_rate": 3.75940996948118e-05,
      "loss": 2.67,
      "step": 24390
    },
    {
      "epoch": 12.410986775178026,
      "grad_norm": 23.64457893371582,
      "learning_rate": 3.7589013224821976e-05,
      "loss": 2.6492,
      "step": 24400
    },
    {
      "epoch": 12.416073245167853,
      "grad_norm": 29.13590431213379,
      "learning_rate": 3.7583926754832146e-05,
      "loss": 2.6193,
      "step": 24410
    },
    {
      "epoch": 12.42115971515768,
      "grad_norm": 22.770246505737305,
      "learning_rate": 3.757884028484232e-05,
      "loss": 2.6589,
      "step": 24420
    },
    {
      "epoch": 12.426246185147507,
      "grad_norm": 22.590476989746094,
      "learning_rate": 3.757375381485249e-05,
      "loss": 2.8078,
      "step": 24430
    },
    {
      "epoch": 12.431332655137334,
      "grad_norm": 22.8802433013916,
      "learning_rate": 3.756866734486267e-05,
      "loss": 2.6683,
      "step": 24440
    },
    {
      "epoch": 12.436419125127161,
      "grad_norm": 23.214719772338867,
      "learning_rate": 3.756358087487284e-05,
      "loss": 2.5925,
      "step": 24450
    },
    {
      "epoch": 12.44150559511699,
      "grad_norm": 19.09304428100586,
      "learning_rate": 3.7558494404883016e-05,
      "loss": 2.6113,
      "step": 24460
    },
    {
      "epoch": 12.446592065106817,
      "grad_norm": 22.999521255493164,
      "learning_rate": 3.7553407934893186e-05,
      "loss": 2.5691,
      "step": 24470
    },
    {
      "epoch": 12.451678535096644,
      "grad_norm": 22.7724666595459,
      "learning_rate": 3.7548321464903356e-05,
      "loss": 2.667,
      "step": 24480
    },
    {
      "epoch": 12.45676500508647,
      "grad_norm": 22.329551696777344,
      "learning_rate": 3.754323499491353e-05,
      "loss": 2.6858,
      "step": 24490
    },
    {
      "epoch": 12.461851475076298,
      "grad_norm": 24.041242599487305,
      "learning_rate": 3.75381485249237e-05,
      "loss": 2.6395,
      "step": 24500
    },
    {
      "epoch": 12.466937945066125,
      "grad_norm": 29.129491806030273,
      "learning_rate": 3.753306205493388e-05,
      "loss": 2.6121,
      "step": 24510
    },
    {
      "epoch": 12.472024415055952,
      "grad_norm": 20.64234161376953,
      "learning_rate": 3.7527975584944056e-05,
      "loss": 2.6109,
      "step": 24520
    },
    {
      "epoch": 12.477110885045779,
      "grad_norm": 18.709579467773438,
      "learning_rate": 3.7522889114954225e-05,
      "loss": 2.6522,
      "step": 24530
    },
    {
      "epoch": 12.482197355035606,
      "grad_norm": 24.620994567871094,
      "learning_rate": 3.7517802644964395e-05,
      "loss": 2.6628,
      "step": 24540
    },
    {
      "epoch": 12.487283825025433,
      "grad_norm": 24.81568717956543,
      "learning_rate": 3.751271617497457e-05,
      "loss": 2.6676,
      "step": 24550
    },
    {
      "epoch": 12.49237029501526,
      "grad_norm": 23.727046966552734,
      "learning_rate": 3.750762970498474e-05,
      "loss": 2.6139,
      "step": 24560
    },
    {
      "epoch": 12.497456765005087,
      "grad_norm": 25.173946380615234,
      "learning_rate": 3.750254323499491e-05,
      "loss": 2.6568,
      "step": 24570
    },
    {
      "epoch": 12.502543234994913,
      "grad_norm": 22.17749786376953,
      "learning_rate": 3.749745676500509e-05,
      "loss": 2.5746,
      "step": 24580
    },
    {
      "epoch": 12.50762970498474,
      "grad_norm": 19.85502052307129,
      "learning_rate": 3.749237029501526e-05,
      "loss": 2.6181,
      "step": 24590
    },
    {
      "epoch": 12.512716174974567,
      "grad_norm": 23.281299591064453,
      "learning_rate": 3.7487283825025435e-05,
      "loss": 2.6416,
      "step": 24600
    },
    {
      "epoch": 12.517802644964394,
      "grad_norm": 23.665136337280273,
      "learning_rate": 3.748219735503561e-05,
      "loss": 2.7965,
      "step": 24610
    },
    {
      "epoch": 12.522889114954221,
      "grad_norm": 16.697763442993164,
      "learning_rate": 3.747711088504578e-05,
      "loss": 2.6322,
      "step": 24620
    },
    {
      "epoch": 12.527975584944048,
      "grad_norm": 26.98272705078125,
      "learning_rate": 3.747202441505595e-05,
      "loss": 2.6006,
      "step": 24630
    },
    {
      "epoch": 12.533062054933875,
      "grad_norm": 21.299137115478516,
      "learning_rate": 3.746693794506613e-05,
      "loss": 2.617,
      "step": 24640
    },
    {
      "epoch": 12.538148524923702,
      "grad_norm": 25.521987915039062,
      "learning_rate": 3.74618514750763e-05,
      "loss": 2.7772,
      "step": 24650
    },
    {
      "epoch": 12.54323499491353,
      "grad_norm": 22.82430076599121,
      "learning_rate": 3.745676500508647e-05,
      "loss": 2.6085,
      "step": 24660
    },
    {
      "epoch": 12.548321464903356,
      "grad_norm": 23.829130172729492,
      "learning_rate": 3.7451678535096645e-05,
      "loss": 2.7067,
      "step": 24670
    },
    {
      "epoch": 12.553407934893183,
      "grad_norm": 24.59111976623535,
      "learning_rate": 3.7446592065106814e-05,
      "loss": 2.6331,
      "step": 24680
    },
    {
      "epoch": 12.55849440488301,
      "grad_norm": 20.397449493408203,
      "learning_rate": 3.744150559511699e-05,
      "loss": 2.6365,
      "step": 24690
    },
    {
      "epoch": 12.563580874872839,
      "grad_norm": 24.856948852539062,
      "learning_rate": 3.743641912512717e-05,
      "loss": 2.5927,
      "step": 24700
    },
    {
      "epoch": 12.568667344862666,
      "grad_norm": 26.8404541015625,
      "learning_rate": 3.743133265513734e-05,
      "loss": 2.6184,
      "step": 24710
    },
    {
      "epoch": 12.573753814852493,
      "grad_norm": 20.381698608398438,
      "learning_rate": 3.7426246185147514e-05,
      "loss": 2.6518,
      "step": 24720
    },
    {
      "epoch": 12.57884028484232,
      "grad_norm": 25.34033203125,
      "learning_rate": 3.7421159715157684e-05,
      "loss": 2.7099,
      "step": 24730
    },
    {
      "epoch": 12.583926754832147,
      "grad_norm": 25.07046127319336,
      "learning_rate": 3.7416073245167854e-05,
      "loss": 2.5822,
      "step": 24740
    },
    {
      "epoch": 12.589013224821974,
      "grad_norm": 20.365373611450195,
      "learning_rate": 3.741098677517803e-05,
      "loss": 2.616,
      "step": 24750
    },
    {
      "epoch": 12.594099694811801,
      "grad_norm": 20.281707763671875,
      "learning_rate": 3.74059003051882e-05,
      "loss": 2.6773,
      "step": 24760
    },
    {
      "epoch": 12.599186164801628,
      "grad_norm": 21.000219345092773,
      "learning_rate": 3.740081383519837e-05,
      "loss": 2.6654,
      "step": 24770
    },
    {
      "epoch": 12.604272634791455,
      "grad_norm": 26.108470916748047,
      "learning_rate": 3.739572736520855e-05,
      "loss": 2.601,
      "step": 24780
    },
    {
      "epoch": 12.609359104781282,
      "grad_norm": 20.331636428833008,
      "learning_rate": 3.739064089521872e-05,
      "loss": 2.7703,
      "step": 24790
    },
    {
      "epoch": 12.614445574771109,
      "grad_norm": 23.13714599609375,
      "learning_rate": 3.7385554425228894e-05,
      "loss": 2.637,
      "step": 24800
    },
    {
      "epoch": 12.619532044760936,
      "grad_norm": 21.670225143432617,
      "learning_rate": 3.738046795523907e-05,
      "loss": 2.6555,
      "step": 24810
    },
    {
      "epoch": 12.624618514750763,
      "grad_norm": 25.27489471435547,
      "learning_rate": 3.737538148524924e-05,
      "loss": 2.6991,
      "step": 24820
    },
    {
      "epoch": 12.62970498474059,
      "grad_norm": 21.03879165649414,
      "learning_rate": 3.737029501525941e-05,
      "loss": 2.6282,
      "step": 24830
    },
    {
      "epoch": 12.634791454730417,
      "grad_norm": 22.461997985839844,
      "learning_rate": 3.736520854526959e-05,
      "loss": 2.7065,
      "step": 24840
    },
    {
      "epoch": 12.639877924720244,
      "grad_norm": 24.259281158447266,
      "learning_rate": 3.736012207527976e-05,
      "loss": 2.6217,
      "step": 24850
    },
    {
      "epoch": 12.64496439471007,
      "grad_norm": 21.37872314453125,
      "learning_rate": 3.735503560528993e-05,
      "loss": 2.6059,
      "step": 24860
    },
    {
      "epoch": 12.650050864699898,
      "grad_norm": 28.720571517944336,
      "learning_rate": 3.7349949135300103e-05,
      "loss": 2.7046,
      "step": 24870
    },
    {
      "epoch": 12.655137334689725,
      "grad_norm": 20.703716278076172,
      "learning_rate": 3.734486266531027e-05,
      "loss": 2.6871,
      "step": 24880
    },
    {
      "epoch": 12.660223804679552,
      "grad_norm": 25.741971969604492,
      "learning_rate": 3.733977619532045e-05,
      "loss": 2.7614,
      "step": 24890
    },
    {
      "epoch": 12.665310274669379,
      "grad_norm": 25.515579223632812,
      "learning_rate": 3.733468972533063e-05,
      "loss": 2.6265,
      "step": 24900
    },
    {
      "epoch": 12.670396744659207,
      "grad_norm": 20.342376708984375,
      "learning_rate": 3.7329603255340797e-05,
      "loss": 2.6713,
      "step": 24910
    },
    {
      "epoch": 12.675483214649034,
      "grad_norm": 22.646787643432617,
      "learning_rate": 3.7324516785350966e-05,
      "loss": 2.689,
      "step": 24920
    },
    {
      "epoch": 12.680569684638861,
      "grad_norm": 25.27349853515625,
      "learning_rate": 3.731943031536114e-05,
      "loss": 2.6759,
      "step": 24930
    },
    {
      "epoch": 12.685656154628688,
      "grad_norm": 17.46593475341797,
      "learning_rate": 3.731434384537131e-05,
      "loss": 2.6835,
      "step": 24940
    },
    {
      "epoch": 12.690742624618515,
      "grad_norm": 27.641538619995117,
      "learning_rate": 3.730925737538148e-05,
      "loss": 2.6769,
      "step": 24950
    },
    {
      "epoch": 12.695829094608342,
      "grad_norm": 20.852712631225586,
      "learning_rate": 3.730417090539166e-05,
      "loss": 2.5545,
      "step": 24960
    },
    {
      "epoch": 12.70091556459817,
      "grad_norm": 24.107152938842773,
      "learning_rate": 3.729908443540183e-05,
      "loss": 2.728,
      "step": 24970
    },
    {
      "epoch": 12.706002034587996,
      "grad_norm": 18.95564842224121,
      "learning_rate": 3.7293997965412006e-05,
      "loss": 2.7771,
      "step": 24980
    },
    {
      "epoch": 12.711088504577823,
      "grad_norm": 21.15164566040039,
      "learning_rate": 3.728891149542218e-05,
      "loss": 2.6616,
      "step": 24990
    },
    {
      "epoch": 12.71617497456765,
      "grad_norm": 26.289684295654297,
      "learning_rate": 3.728382502543235e-05,
      "loss": 2.651,
      "step": 25000
    },
    {
      "epoch": 12.721261444557477,
      "grad_norm": 22.947885513305664,
      "learning_rate": 3.727873855544253e-05,
      "loss": 2.6345,
      "step": 25010
    },
    {
      "epoch": 12.726347914547304,
      "grad_norm": 26.52816390991211,
      "learning_rate": 3.72736520854527e-05,
      "loss": 2.6621,
      "step": 25020
    },
    {
      "epoch": 12.731434384537131,
      "grad_norm": 25.879823684692383,
      "learning_rate": 3.726856561546287e-05,
      "loss": 2.7211,
      "step": 25030
    },
    {
      "epoch": 12.736520854526958,
      "grad_norm": 18.024311065673828,
      "learning_rate": 3.7263479145473046e-05,
      "loss": 2.6343,
      "step": 25040
    },
    {
      "epoch": 12.741607324516785,
      "grad_norm": 24.377016067504883,
      "learning_rate": 3.7258392675483216e-05,
      "loss": 2.5919,
      "step": 25050
    },
    {
      "epoch": 12.746693794506612,
      "grad_norm": 23.441349029541016,
      "learning_rate": 3.7253306205493386e-05,
      "loss": 2.706,
      "step": 25060
    },
    {
      "epoch": 12.751780264496439,
      "grad_norm": 24.82020378112793,
      "learning_rate": 3.724821973550356e-05,
      "loss": 2.6795,
      "step": 25070
    },
    {
      "epoch": 12.756866734486266,
      "grad_norm": 30.208160400390625,
      "learning_rate": 3.724313326551373e-05,
      "loss": 2.6616,
      "step": 25080
    },
    {
      "epoch": 12.761953204476093,
      "grad_norm": 28.232303619384766,
      "learning_rate": 3.723804679552391e-05,
      "loss": 2.5564,
      "step": 25090
    },
    {
      "epoch": 12.76703967446592,
      "grad_norm": 22.49513053894043,
      "learning_rate": 3.7232960325534086e-05,
      "loss": 2.6353,
      "step": 25100
    },
    {
      "epoch": 12.772126144455747,
      "grad_norm": 25.741018295288086,
      "learning_rate": 3.7227873855544255e-05,
      "loss": 2.6337,
      "step": 25110
    },
    {
      "epoch": 12.777212614445574,
      "grad_norm": 21.702804565429688,
      "learning_rate": 3.7222787385554425e-05,
      "loss": 2.5902,
      "step": 25120
    },
    {
      "epoch": 12.782299084435401,
      "grad_norm": 24.270315170288086,
      "learning_rate": 3.72177009155646e-05,
      "loss": 2.6485,
      "step": 25130
    },
    {
      "epoch": 12.787385554425228,
      "grad_norm": 18.696683883666992,
      "learning_rate": 3.721261444557477e-05,
      "loss": 2.5629,
      "step": 25140
    },
    {
      "epoch": 12.792472024415057,
      "grad_norm": 22.789051055908203,
      "learning_rate": 3.720752797558494e-05,
      "loss": 2.6811,
      "step": 25150
    },
    {
      "epoch": 12.797558494404884,
      "grad_norm": 21.847126007080078,
      "learning_rate": 3.720244150559512e-05,
      "loss": 2.7004,
      "step": 25160
    },
    {
      "epoch": 12.80264496439471,
      "grad_norm": 22.11095428466797,
      "learning_rate": 3.719735503560529e-05,
      "loss": 2.6105,
      "step": 25170
    },
    {
      "epoch": 12.807731434384538,
      "grad_norm": 27.716686248779297,
      "learning_rate": 3.7192268565615465e-05,
      "loss": 2.5375,
      "step": 25180
    },
    {
      "epoch": 12.812817904374365,
      "grad_norm": 21.50916290283203,
      "learning_rate": 3.718718209562564e-05,
      "loss": 2.6653,
      "step": 25190
    },
    {
      "epoch": 12.817904374364192,
      "grad_norm": 20.754194259643555,
      "learning_rate": 3.718209562563581e-05,
      "loss": 2.5844,
      "step": 25200
    },
    {
      "epoch": 12.822990844354019,
      "grad_norm": 24.255456924438477,
      "learning_rate": 3.717700915564598e-05,
      "loss": 2.6581,
      "step": 25210
    },
    {
      "epoch": 12.828077314343846,
      "grad_norm": 24.86035919189453,
      "learning_rate": 3.717192268565616e-05,
      "loss": 2.6122,
      "step": 25220
    },
    {
      "epoch": 12.833163784333673,
      "grad_norm": 24.0318603515625,
      "learning_rate": 3.716683621566633e-05,
      "loss": 2.6455,
      "step": 25230
    },
    {
      "epoch": 12.8382502543235,
      "grad_norm": 23.484495162963867,
      "learning_rate": 3.71617497456765e-05,
      "loss": 2.667,
      "step": 25240
    },
    {
      "epoch": 12.843336724313327,
      "grad_norm": 22.153091430664062,
      "learning_rate": 3.7156663275686675e-05,
      "loss": 2.7093,
      "step": 25250
    },
    {
      "epoch": 12.848423194303153,
      "grad_norm": 17.767080307006836,
      "learning_rate": 3.7151576805696844e-05,
      "loss": 2.5262,
      "step": 25260
    },
    {
      "epoch": 12.85350966429298,
      "grad_norm": 20.115394592285156,
      "learning_rate": 3.714649033570702e-05,
      "loss": 2.6445,
      "step": 25270
    },
    {
      "epoch": 12.858596134282807,
      "grad_norm": 25.745288848876953,
      "learning_rate": 3.71414038657172e-05,
      "loss": 2.6918,
      "step": 25280
    },
    {
      "epoch": 12.863682604272634,
      "grad_norm": 18.321964263916016,
      "learning_rate": 3.713631739572737e-05,
      "loss": 2.6483,
      "step": 25290
    },
    {
      "epoch": 12.868769074262461,
      "grad_norm": 24.110013961791992,
      "learning_rate": 3.7131230925737544e-05,
      "loss": 2.6892,
      "step": 25300
    },
    {
      "epoch": 12.873855544252288,
      "grad_norm": 21.488346099853516,
      "learning_rate": 3.7126144455747714e-05,
      "loss": 2.6499,
      "step": 25310
    },
    {
      "epoch": 12.878942014242115,
      "grad_norm": 30.41213607788086,
      "learning_rate": 3.7121057985757884e-05,
      "loss": 2.6701,
      "step": 25320
    },
    {
      "epoch": 12.884028484231942,
      "grad_norm": 25.04864501953125,
      "learning_rate": 3.711597151576806e-05,
      "loss": 2.6296,
      "step": 25330
    },
    {
      "epoch": 12.88911495422177,
      "grad_norm": 28.309682846069336,
      "learning_rate": 3.711088504577823e-05,
      "loss": 2.7313,
      "step": 25340
    },
    {
      "epoch": 12.894201424211598,
      "grad_norm": 26.92045783996582,
      "learning_rate": 3.71057985757884e-05,
      "loss": 2.6218,
      "step": 25350
    },
    {
      "epoch": 12.899287894201425,
      "grad_norm": 22.4550724029541,
      "learning_rate": 3.710071210579858e-05,
      "loss": 2.6513,
      "step": 25360
    },
    {
      "epoch": 12.904374364191252,
      "grad_norm": 21.376102447509766,
      "learning_rate": 3.7095625635808754e-05,
      "loss": 2.6228,
      "step": 25370
    },
    {
      "epoch": 12.909460834181079,
      "grad_norm": 25.08220100402832,
      "learning_rate": 3.7090539165818924e-05,
      "loss": 2.6126,
      "step": 25380
    },
    {
      "epoch": 12.914547304170906,
      "grad_norm": 25.61432456970215,
      "learning_rate": 3.70854526958291e-05,
      "loss": 2.6769,
      "step": 25390
    },
    {
      "epoch": 12.919633774160733,
      "grad_norm": 24.643712997436523,
      "learning_rate": 3.708036622583927e-05,
      "loss": 2.6045,
      "step": 25400
    },
    {
      "epoch": 12.92472024415056,
      "grad_norm": 20.161754608154297,
      "learning_rate": 3.707527975584944e-05,
      "loss": 2.6189,
      "step": 25410
    },
    {
      "epoch": 12.929806714140387,
      "grad_norm": 22.420312881469727,
      "learning_rate": 3.707019328585962e-05,
      "loss": 2.6323,
      "step": 25420
    },
    {
      "epoch": 12.934893184130214,
      "grad_norm": 23.029294967651367,
      "learning_rate": 3.706510681586979e-05,
      "loss": 2.7269,
      "step": 25430
    },
    {
      "epoch": 12.939979654120041,
      "grad_norm": 31.372644424438477,
      "learning_rate": 3.706002034587996e-05,
      "loss": 2.6361,
      "step": 25440
    },
    {
      "epoch": 12.945066124109868,
      "grad_norm": 26.217594146728516,
      "learning_rate": 3.7054933875890133e-05,
      "loss": 2.6523,
      "step": 25450
    },
    {
      "epoch": 12.950152594099695,
      "grad_norm": 19.894271850585938,
      "learning_rate": 3.70498474059003e-05,
      "loss": 2.6141,
      "step": 25460
    },
    {
      "epoch": 12.955239064089522,
      "grad_norm": 24.677568435668945,
      "learning_rate": 3.704476093591048e-05,
      "loss": 2.6607,
      "step": 25470
    },
    {
      "epoch": 12.960325534079349,
      "grad_norm": 27.46201515197754,
      "learning_rate": 3.703967446592066e-05,
      "loss": 2.644,
      "step": 25480
    },
    {
      "epoch": 12.965412004069176,
      "grad_norm": 30.91033935546875,
      "learning_rate": 3.7034587995930827e-05,
      "loss": 2.6521,
      "step": 25490
    },
    {
      "epoch": 12.970498474059003,
      "grad_norm": 29.229106903076172,
      "learning_rate": 3.7029501525940996e-05,
      "loss": 2.615,
      "step": 25500
    },
    {
      "epoch": 12.97558494404883,
      "grad_norm": 23.19025421142578,
      "learning_rate": 3.702441505595117e-05,
      "loss": 2.6531,
      "step": 25510
    },
    {
      "epoch": 12.980671414038657,
      "grad_norm": 25.03069496154785,
      "learning_rate": 3.701932858596134e-05,
      "loss": 2.683,
      "step": 25520
    },
    {
      "epoch": 12.985757884028484,
      "grad_norm": 23.41371726989746,
      "learning_rate": 3.701424211597152e-05,
      "loss": 2.654,
      "step": 25530
    },
    {
      "epoch": 12.99084435401831,
      "grad_norm": 22.041921615600586,
      "learning_rate": 3.700915564598169e-05,
      "loss": 2.5768,
      "step": 25540
    },
    {
      "epoch": 12.995930824008138,
      "grad_norm": 19.989988327026367,
      "learning_rate": 3.700406917599186e-05,
      "loss": 2.5806,
      "step": 25550
    },
    {
      "epoch": 13.0,
      "eval_loss": 3.9575958251953125,
      "eval_runtime": 2.7358,
      "eval_samples_per_second": 1014.342,
      "eval_steps_per_second": 126.838,
      "step": 25558
    },
    {
      "epoch": 13.001017293997965,
      "grad_norm": 28.365955352783203,
      "learning_rate": 3.6998982706002036e-05,
      "loss": 2.5825,
      "step": 25560
    },
    {
      "epoch": 13.006103763987792,
      "grad_norm": 22.925718307495117,
      "learning_rate": 3.699389623601221e-05,
      "loss": 2.5888,
      "step": 25570
    },
    {
      "epoch": 13.011190233977619,
      "grad_norm": 26.188068389892578,
      "learning_rate": 3.698880976602238e-05,
      "loss": 2.619,
      "step": 25580
    },
    {
      "epoch": 13.016276703967447,
      "grad_norm": 25.892566680908203,
      "learning_rate": 3.698372329603256e-05,
      "loss": 2.673,
      "step": 25590
    },
    {
      "epoch": 13.021363173957274,
      "grad_norm": 24.962467193603516,
      "learning_rate": 3.697863682604273e-05,
      "loss": 2.5145,
      "step": 25600
    },
    {
      "epoch": 13.026449643947101,
      "grad_norm": 20.235015869140625,
      "learning_rate": 3.69735503560529e-05,
      "loss": 2.6112,
      "step": 25610
    },
    {
      "epoch": 13.031536113936928,
      "grad_norm": 34.26694869995117,
      "learning_rate": 3.6968463886063076e-05,
      "loss": 2.625,
      "step": 25620
    },
    {
      "epoch": 13.036622583926755,
      "grad_norm": 18.7071475982666,
      "learning_rate": 3.6963377416073246e-05,
      "loss": 2.5943,
      "step": 25630
    },
    {
      "epoch": 13.041709053916582,
      "grad_norm": 27.20894432067871,
      "learning_rate": 3.6958290946083416e-05,
      "loss": 2.6415,
      "step": 25640
    },
    {
      "epoch": 13.04679552390641,
      "grad_norm": 23.666370391845703,
      "learning_rate": 3.695320447609359e-05,
      "loss": 2.6167,
      "step": 25650
    },
    {
      "epoch": 13.051881993896236,
      "grad_norm": 24.398900985717773,
      "learning_rate": 3.694811800610377e-05,
      "loss": 2.5683,
      "step": 25660
    },
    {
      "epoch": 13.056968463886063,
      "grad_norm": 22.231670379638672,
      "learning_rate": 3.694303153611394e-05,
      "loss": 2.5009,
      "step": 25670
    },
    {
      "epoch": 13.06205493387589,
      "grad_norm": 24.02815818786621,
      "learning_rate": 3.6937945066124116e-05,
      "loss": 2.5916,
      "step": 25680
    },
    {
      "epoch": 13.067141403865717,
      "grad_norm": 25.950775146484375,
      "learning_rate": 3.6932858596134285e-05,
      "loss": 2.5145,
      "step": 25690
    },
    {
      "epoch": 13.072227873855544,
      "grad_norm": 23.06424331665039,
      "learning_rate": 3.6927772126144455e-05,
      "loss": 2.6267,
      "step": 25700
    },
    {
      "epoch": 13.077314343845371,
      "grad_norm": 22.849334716796875,
      "learning_rate": 3.692268565615463e-05,
      "loss": 2.5736,
      "step": 25710
    },
    {
      "epoch": 13.082400813835198,
      "grad_norm": 24.209156036376953,
      "learning_rate": 3.69175991861648e-05,
      "loss": 2.5961,
      "step": 25720
    },
    {
      "epoch": 13.087487283825025,
      "grad_norm": 20.496482849121094,
      "learning_rate": 3.691251271617497e-05,
      "loss": 2.5671,
      "step": 25730
    },
    {
      "epoch": 13.092573753814852,
      "grad_norm": 25.861454010009766,
      "learning_rate": 3.690742624618515e-05,
      "loss": 2.7157,
      "step": 25740
    },
    {
      "epoch": 13.097660223804679,
      "grad_norm": 19.703824996948242,
      "learning_rate": 3.690233977619532e-05,
      "loss": 2.6219,
      "step": 25750
    },
    {
      "epoch": 13.102746693794506,
      "grad_norm": 24.731008529663086,
      "learning_rate": 3.6897253306205495e-05,
      "loss": 2.6325,
      "step": 25760
    },
    {
      "epoch": 13.107833163784333,
      "grad_norm": 16.621976852416992,
      "learning_rate": 3.689216683621567e-05,
      "loss": 2.6297,
      "step": 25770
    },
    {
      "epoch": 13.11291963377416,
      "grad_norm": 26.23912811279297,
      "learning_rate": 3.688708036622584e-05,
      "loss": 2.6148,
      "step": 25780
    },
    {
      "epoch": 13.118006103763987,
      "grad_norm": 26.627443313598633,
      "learning_rate": 3.688199389623601e-05,
      "loss": 2.5629,
      "step": 25790
    },
    {
      "epoch": 13.123092573753814,
      "grad_norm": 22.534452438354492,
      "learning_rate": 3.687690742624619e-05,
      "loss": 2.6201,
      "step": 25800
    },
    {
      "epoch": 13.128179043743643,
      "grad_norm": 25.305789947509766,
      "learning_rate": 3.687182095625636e-05,
      "loss": 2.6411,
      "step": 25810
    },
    {
      "epoch": 13.13326551373347,
      "grad_norm": 23.58902359008789,
      "learning_rate": 3.6866734486266535e-05,
      "loss": 2.6579,
      "step": 25820
    },
    {
      "epoch": 13.138351983723297,
      "grad_norm": 20.768102645874023,
      "learning_rate": 3.6861648016276705e-05,
      "loss": 2.5972,
      "step": 25830
    },
    {
      "epoch": 13.143438453713124,
      "grad_norm": 24.192028045654297,
      "learning_rate": 3.6856561546286874e-05,
      "loss": 2.4948,
      "step": 25840
    },
    {
      "epoch": 13.14852492370295,
      "grad_norm": 22.786243438720703,
      "learning_rate": 3.685147507629705e-05,
      "loss": 2.6071,
      "step": 25850
    },
    {
      "epoch": 13.153611393692778,
      "grad_norm": 24.714523315429688,
      "learning_rate": 3.684638860630723e-05,
      "loss": 2.6343,
      "step": 25860
    },
    {
      "epoch": 13.158697863682605,
      "grad_norm": 20.725421905517578,
      "learning_rate": 3.68413021363174e-05,
      "loss": 2.6263,
      "step": 25870
    },
    {
      "epoch": 13.163784333672432,
      "grad_norm": 26.486669540405273,
      "learning_rate": 3.6836215666327574e-05,
      "loss": 2.6058,
      "step": 25880
    },
    {
      "epoch": 13.168870803662259,
      "grad_norm": 22.097522735595703,
      "learning_rate": 3.6831129196337744e-05,
      "loss": 2.554,
      "step": 25890
    },
    {
      "epoch": 13.173957273652086,
      "grad_norm": 26.240217208862305,
      "learning_rate": 3.6826042726347914e-05,
      "loss": 2.6104,
      "step": 25900
    },
    {
      "epoch": 13.179043743641913,
      "grad_norm": 31.772045135498047,
      "learning_rate": 3.682095625635809e-05,
      "loss": 2.6117,
      "step": 25910
    },
    {
      "epoch": 13.18413021363174,
      "grad_norm": 17.256988525390625,
      "learning_rate": 3.681586978636826e-05,
      "loss": 2.687,
      "step": 25920
    },
    {
      "epoch": 13.189216683621567,
      "grad_norm": 29.151878356933594,
      "learning_rate": 3.681078331637843e-05,
      "loss": 2.5534,
      "step": 25930
    },
    {
      "epoch": 13.194303153611393,
      "grad_norm": 23.528371810913086,
      "learning_rate": 3.680569684638861e-05,
      "loss": 2.6486,
      "step": 25940
    },
    {
      "epoch": 13.19938962360122,
      "grad_norm": 25.864707946777344,
      "learning_rate": 3.6800610376398784e-05,
      "loss": 2.5673,
      "step": 25950
    },
    {
      "epoch": 13.204476093591047,
      "grad_norm": 22.267452239990234,
      "learning_rate": 3.6795523906408954e-05,
      "loss": 2.5812,
      "step": 25960
    },
    {
      "epoch": 13.209562563580874,
      "grad_norm": 22.684415817260742,
      "learning_rate": 3.679043743641913e-05,
      "loss": 2.6028,
      "step": 25970
    },
    {
      "epoch": 13.214649033570701,
      "grad_norm": 23.444047927856445,
      "learning_rate": 3.67853509664293e-05,
      "loss": 2.6114,
      "step": 25980
    },
    {
      "epoch": 13.219735503560528,
      "grad_norm": 23.024242401123047,
      "learning_rate": 3.678026449643947e-05,
      "loss": 2.681,
      "step": 25990
    },
    {
      "epoch": 13.224821973550355,
      "grad_norm": 26.02151107788086,
      "learning_rate": 3.677517802644965e-05,
      "loss": 2.5159,
      "step": 26000
    },
    {
      "epoch": 13.229908443540182,
      "grad_norm": 25.6826229095459,
      "learning_rate": 3.677009155645982e-05,
      "loss": 2.5854,
      "step": 26010
    },
    {
      "epoch": 13.23499491353001,
      "grad_norm": 22.612932205200195,
      "learning_rate": 3.676500508646999e-05,
      "loss": 2.611,
      "step": 26020
    },
    {
      "epoch": 13.240081383519838,
      "grad_norm": 25.230998992919922,
      "learning_rate": 3.6759918616480163e-05,
      "loss": 2.5586,
      "step": 26030
    },
    {
      "epoch": 13.245167853509665,
      "grad_norm": 27.087995529174805,
      "learning_rate": 3.675483214649033e-05,
      "loss": 2.5559,
      "step": 26040
    },
    {
      "epoch": 13.250254323499492,
      "grad_norm": 24.01997947692871,
      "learning_rate": 3.674974567650051e-05,
      "loss": 2.6276,
      "step": 26050
    },
    {
      "epoch": 13.255340793489319,
      "grad_norm": 24.490453720092773,
      "learning_rate": 3.674465920651069e-05,
      "loss": 2.6294,
      "step": 26060
    },
    {
      "epoch": 13.260427263479146,
      "grad_norm": 21.904773712158203,
      "learning_rate": 3.6739572736520857e-05,
      "loss": 2.6664,
      "step": 26070
    },
    {
      "epoch": 13.265513733468973,
      "grad_norm": 22.039981842041016,
      "learning_rate": 3.673448626653103e-05,
      "loss": 2.6034,
      "step": 26080
    },
    {
      "epoch": 13.2706002034588,
      "grad_norm": 24.502073287963867,
      "learning_rate": 3.67293997965412e-05,
      "loss": 2.5961,
      "step": 26090
    },
    {
      "epoch": 13.275686673448627,
      "grad_norm": 22.517101287841797,
      "learning_rate": 3.672431332655137e-05,
      "loss": 2.5673,
      "step": 26100
    },
    {
      "epoch": 13.280773143438454,
      "grad_norm": 27.776952743530273,
      "learning_rate": 3.671922685656155e-05,
      "loss": 2.5548,
      "step": 26110
    },
    {
      "epoch": 13.285859613428281,
      "grad_norm": 21.831817626953125,
      "learning_rate": 3.671414038657172e-05,
      "loss": 2.5196,
      "step": 26120
    },
    {
      "epoch": 13.290946083418108,
      "grad_norm": 24.374135971069336,
      "learning_rate": 3.670905391658189e-05,
      "loss": 2.6028,
      "step": 26130
    },
    {
      "epoch": 13.296032553407935,
      "grad_norm": 25.145591735839844,
      "learning_rate": 3.6703967446592066e-05,
      "loss": 2.6354,
      "step": 26140
    },
    {
      "epoch": 13.301119023397762,
      "grad_norm": 32.43538284301758,
      "learning_rate": 3.669888097660224e-05,
      "loss": 2.5617,
      "step": 26150
    },
    {
      "epoch": 13.306205493387589,
      "grad_norm": 21.981231689453125,
      "learning_rate": 3.669379450661241e-05,
      "loss": 2.664,
      "step": 26160
    },
    {
      "epoch": 13.311291963377416,
      "grad_norm": 28.695693969726562,
      "learning_rate": 3.668870803662259e-05,
      "loss": 2.6036,
      "step": 26170
    },
    {
      "epoch": 13.316378433367243,
      "grad_norm": 27.192161560058594,
      "learning_rate": 3.668362156663276e-05,
      "loss": 2.5887,
      "step": 26180
    },
    {
      "epoch": 13.32146490335707,
      "grad_norm": 22.53290367126465,
      "learning_rate": 3.667853509664293e-05,
      "loss": 2.5814,
      "step": 26190
    },
    {
      "epoch": 13.326551373346897,
      "grad_norm": 21.15979766845703,
      "learning_rate": 3.6673448626653106e-05,
      "loss": 2.6589,
      "step": 26200
    },
    {
      "epoch": 13.331637843336724,
      "grad_norm": 24.858335494995117,
      "learning_rate": 3.6668362156663276e-05,
      "loss": 2.4975,
      "step": 26210
    },
    {
      "epoch": 13.33672431332655,
      "grad_norm": 26.981653213500977,
      "learning_rate": 3.6663275686673446e-05,
      "loss": 2.5607,
      "step": 26220
    },
    {
      "epoch": 13.341810783316378,
      "grad_norm": 21.59932518005371,
      "learning_rate": 3.665818921668362e-05,
      "loss": 2.6015,
      "step": 26230
    },
    {
      "epoch": 13.346897253306205,
      "grad_norm": 30.131385803222656,
      "learning_rate": 3.66531027466938e-05,
      "loss": 2.5586,
      "step": 26240
    },
    {
      "epoch": 13.351983723296033,
      "grad_norm": 29.517471313476562,
      "learning_rate": 3.664801627670397e-05,
      "loss": 2.5746,
      "step": 26250
    },
    {
      "epoch": 13.35707019328586,
      "grad_norm": 20.563922882080078,
      "learning_rate": 3.6642929806714146e-05,
      "loss": 2.5247,
      "step": 26260
    },
    {
      "epoch": 13.362156663275687,
      "grad_norm": 22.54793357849121,
      "learning_rate": 3.6637843336724315e-05,
      "loss": 2.5029,
      "step": 26270
    },
    {
      "epoch": 13.367243133265514,
      "grad_norm": 22.016395568847656,
      "learning_rate": 3.6632756866734485e-05,
      "loss": 2.6041,
      "step": 26280
    },
    {
      "epoch": 13.372329603255341,
      "grad_norm": 22.64702796936035,
      "learning_rate": 3.662767039674466e-05,
      "loss": 2.5165,
      "step": 26290
    },
    {
      "epoch": 13.377416073245168,
      "grad_norm": 17.346467971801758,
      "learning_rate": 3.662258392675483e-05,
      "loss": 2.5606,
      "step": 26300
    },
    {
      "epoch": 13.382502543234995,
      "grad_norm": 27.446636199951172,
      "learning_rate": 3.6617497456765e-05,
      "loss": 2.5564,
      "step": 26310
    },
    {
      "epoch": 13.387589013224822,
      "grad_norm": 32.5144157409668,
      "learning_rate": 3.661241098677518e-05,
      "loss": 2.4992,
      "step": 26320
    },
    {
      "epoch": 13.39267548321465,
      "grad_norm": 21.387054443359375,
      "learning_rate": 3.6607324516785355e-05,
      "loss": 2.6559,
      "step": 26330
    },
    {
      "epoch": 13.397761953204476,
      "grad_norm": 25.139812469482422,
      "learning_rate": 3.6602238046795525e-05,
      "loss": 2.628,
      "step": 26340
    },
    {
      "epoch": 13.402848423194303,
      "grad_norm": 21.636072158813477,
      "learning_rate": 3.65971515768057e-05,
      "loss": 2.6384,
      "step": 26350
    },
    {
      "epoch": 13.40793489318413,
      "grad_norm": 29.31821632385254,
      "learning_rate": 3.659206510681587e-05,
      "loss": 2.5736,
      "step": 26360
    },
    {
      "epoch": 13.413021363173957,
      "grad_norm": 22.42988395690918,
      "learning_rate": 3.658697863682605e-05,
      "loss": 2.5811,
      "step": 26370
    },
    {
      "epoch": 13.418107833163784,
      "grad_norm": 23.605457305908203,
      "learning_rate": 3.658189216683622e-05,
      "loss": 2.6094,
      "step": 26380
    },
    {
      "epoch": 13.423194303153611,
      "grad_norm": 23.221805572509766,
      "learning_rate": 3.657680569684639e-05,
      "loss": 2.6362,
      "step": 26390
    },
    {
      "epoch": 13.428280773143438,
      "grad_norm": 24.308380126953125,
      "learning_rate": 3.6571719226856565e-05,
      "loss": 2.5753,
      "step": 26400
    },
    {
      "epoch": 13.433367243133265,
      "grad_norm": 23.94118881225586,
      "learning_rate": 3.6566632756866735e-05,
      "loss": 2.6578,
      "step": 26410
    },
    {
      "epoch": 13.438453713123092,
      "grad_norm": 27.30632209777832,
      "learning_rate": 3.6561546286876904e-05,
      "loss": 2.5515,
      "step": 26420
    },
    {
      "epoch": 13.443540183112919,
      "grad_norm": 28.925094604492188,
      "learning_rate": 3.655645981688708e-05,
      "loss": 2.6406,
      "step": 26430
    },
    {
      "epoch": 13.448626653102746,
      "grad_norm": 29.784343719482422,
      "learning_rate": 3.655137334689726e-05,
      "loss": 2.6496,
      "step": 26440
    },
    {
      "epoch": 13.453713123092573,
      "grad_norm": 26.541603088378906,
      "learning_rate": 3.654628687690743e-05,
      "loss": 2.5626,
      "step": 26450
    },
    {
      "epoch": 13.4587995930824,
      "grad_norm": 24.783954620361328,
      "learning_rate": 3.6541200406917604e-05,
      "loss": 2.6031,
      "step": 26460
    },
    {
      "epoch": 13.463886063072227,
      "grad_norm": 23.412302017211914,
      "learning_rate": 3.6536113936927774e-05,
      "loss": 2.6003,
      "step": 26470
    },
    {
      "epoch": 13.468972533062056,
      "grad_norm": 24.568458557128906,
      "learning_rate": 3.6531027466937944e-05,
      "loss": 2.5834,
      "step": 26480
    },
    {
      "epoch": 13.474059003051883,
      "grad_norm": 21.332448959350586,
      "learning_rate": 3.652594099694812e-05,
      "loss": 2.6893,
      "step": 26490
    },
    {
      "epoch": 13.47914547304171,
      "grad_norm": 22.818511962890625,
      "learning_rate": 3.652085452695829e-05,
      "loss": 2.5998,
      "step": 26500
    },
    {
      "epoch": 13.484231943031537,
      "grad_norm": 20.977218627929688,
      "learning_rate": 3.651576805696846e-05,
      "loss": 2.5652,
      "step": 26510
    },
    {
      "epoch": 13.489318413021364,
      "grad_norm": 27.77196502685547,
      "learning_rate": 3.651068158697864e-05,
      "loss": 2.5954,
      "step": 26520
    },
    {
      "epoch": 13.49440488301119,
      "grad_norm": 29.426998138427734,
      "learning_rate": 3.6505595116988814e-05,
      "loss": 2.5196,
      "step": 26530
    },
    {
      "epoch": 13.499491353001018,
      "grad_norm": 20.942502975463867,
      "learning_rate": 3.6500508646998984e-05,
      "loss": 2.5802,
      "step": 26540
    },
    {
      "epoch": 13.504577822990845,
      "grad_norm": 32.94001388549805,
      "learning_rate": 3.649542217700916e-05,
      "loss": 2.5319,
      "step": 26550
    },
    {
      "epoch": 13.509664292980672,
      "grad_norm": 24.39183235168457,
      "learning_rate": 3.649033570701933e-05,
      "loss": 2.6435,
      "step": 26560
    },
    {
      "epoch": 13.514750762970499,
      "grad_norm": 25.483577728271484,
      "learning_rate": 3.64852492370295e-05,
      "loss": 2.6038,
      "step": 26570
    },
    {
      "epoch": 13.519837232960326,
      "grad_norm": 25.728849411010742,
      "learning_rate": 3.648016276703968e-05,
      "loss": 2.477,
      "step": 26580
    },
    {
      "epoch": 13.524923702950153,
      "grad_norm": 21.445358276367188,
      "learning_rate": 3.647507629704985e-05,
      "loss": 2.5453,
      "step": 26590
    },
    {
      "epoch": 13.53001017293998,
      "grad_norm": 29.755672454833984,
      "learning_rate": 3.646998982706002e-05,
      "loss": 2.5531,
      "step": 26600
    },
    {
      "epoch": 13.535096642929807,
      "grad_norm": 24.988468170166016,
      "learning_rate": 3.6464903357070193e-05,
      "loss": 2.5409,
      "step": 26610
    },
    {
      "epoch": 13.540183112919634,
      "grad_norm": 22.57501983642578,
      "learning_rate": 3.645981688708037e-05,
      "loss": 2.6614,
      "step": 26620
    },
    {
      "epoch": 13.54526958290946,
      "grad_norm": 26.210519790649414,
      "learning_rate": 3.645473041709055e-05,
      "loss": 2.5504,
      "step": 26630
    },
    {
      "epoch": 13.550356052899287,
      "grad_norm": 25.39409637451172,
      "learning_rate": 3.644964394710072e-05,
      "loss": 2.6157,
      "step": 26640
    },
    {
      "epoch": 13.555442522889114,
      "grad_norm": 26.705209732055664,
      "learning_rate": 3.6444557477110887e-05,
      "loss": 2.6532,
      "step": 26650
    },
    {
      "epoch": 13.560528992878941,
      "grad_norm": 22.937889099121094,
      "learning_rate": 3.643947100712106e-05,
      "loss": 2.5494,
      "step": 26660
    },
    {
      "epoch": 13.565615462868768,
      "grad_norm": 29.537986755371094,
      "learning_rate": 3.643438453713123e-05,
      "loss": 2.5644,
      "step": 26670
    },
    {
      "epoch": 13.570701932858595,
      "grad_norm": 34.536956787109375,
      "learning_rate": 3.64292980671414e-05,
      "loss": 2.5467,
      "step": 26680
    },
    {
      "epoch": 13.575788402848422,
      "grad_norm": 27.037960052490234,
      "learning_rate": 3.642421159715158e-05,
      "loss": 2.6194,
      "step": 26690
    },
    {
      "epoch": 13.580874872838251,
      "grad_norm": 24.429288864135742,
      "learning_rate": 3.641912512716175e-05,
      "loss": 2.6053,
      "step": 26700
    },
    {
      "epoch": 13.585961342828078,
      "grad_norm": 35.474971771240234,
      "learning_rate": 3.641403865717192e-05,
      "loss": 2.5674,
      "step": 26710
    },
    {
      "epoch": 13.591047812817905,
      "grad_norm": 23.53052520751953,
      "learning_rate": 3.6408952187182096e-05,
      "loss": 2.597,
      "step": 26720
    },
    {
      "epoch": 13.596134282807732,
      "grad_norm": 21.93185806274414,
      "learning_rate": 3.640386571719227e-05,
      "loss": 2.5494,
      "step": 26730
    },
    {
      "epoch": 13.601220752797559,
      "grad_norm": 26.168031692504883,
      "learning_rate": 3.639877924720244e-05,
      "loss": 2.5343,
      "step": 26740
    },
    {
      "epoch": 13.606307222787386,
      "grad_norm": 33.68019104003906,
      "learning_rate": 3.639369277721262e-05,
      "loss": 2.5507,
      "step": 26750
    },
    {
      "epoch": 13.611393692777213,
      "grad_norm": 28.498153686523438,
      "learning_rate": 3.638860630722279e-05,
      "loss": 2.4761,
      "step": 26760
    },
    {
      "epoch": 13.61648016276704,
      "grad_norm": 24.02202606201172,
      "learning_rate": 3.638351983723296e-05,
      "loss": 2.5482,
      "step": 26770
    },
    {
      "epoch": 13.621566632756867,
      "grad_norm": 21.981098175048828,
      "learning_rate": 3.6378433367243136e-05,
      "loss": 2.5202,
      "step": 26780
    },
    {
      "epoch": 13.626653102746694,
      "grad_norm": 29.105920791625977,
      "learning_rate": 3.6373346897253306e-05,
      "loss": 2.6386,
      "step": 26790
    },
    {
      "epoch": 13.631739572736521,
      "grad_norm": 26.42009735107422,
      "learning_rate": 3.6368260427263476e-05,
      "loss": 2.6391,
      "step": 26800
    },
    {
      "epoch": 13.636826042726348,
      "grad_norm": 22.49962043762207,
      "learning_rate": 3.636317395727365e-05,
      "loss": 2.6419,
      "step": 26810
    },
    {
      "epoch": 13.641912512716175,
      "grad_norm": 33.6543083190918,
      "learning_rate": 3.635808748728383e-05,
      "loss": 2.5865,
      "step": 26820
    },
    {
      "epoch": 13.646998982706002,
      "grad_norm": 23.552701950073242,
      "learning_rate": 3.6353001017294e-05,
      "loss": 2.605,
      "step": 26830
    },
    {
      "epoch": 13.652085452695829,
      "grad_norm": 28.826366424560547,
      "learning_rate": 3.6347914547304176e-05,
      "loss": 2.5328,
      "step": 26840
    },
    {
      "epoch": 13.657171922685656,
      "grad_norm": 28.44046401977539,
      "learning_rate": 3.6342828077314345e-05,
      "loss": 2.5736,
      "step": 26850
    },
    {
      "epoch": 13.662258392675483,
      "grad_norm": 24.691364288330078,
      "learning_rate": 3.6337741607324515e-05,
      "loss": 2.5357,
      "step": 26860
    },
    {
      "epoch": 13.66734486266531,
      "grad_norm": 18.516916275024414,
      "learning_rate": 3.633265513733469e-05,
      "loss": 2.5745,
      "step": 26870
    },
    {
      "epoch": 13.672431332655137,
      "grad_norm": 25.317249298095703,
      "learning_rate": 3.632756866734486e-05,
      "loss": 2.5336,
      "step": 26880
    },
    {
      "epoch": 13.677517802644964,
      "grad_norm": 19.32080841064453,
      "learning_rate": 3.632248219735504e-05,
      "loss": 2.5887,
      "step": 26890
    },
    {
      "epoch": 13.68260427263479,
      "grad_norm": 21.5327091217041,
      "learning_rate": 3.631739572736521e-05,
      "loss": 2.4393,
      "step": 26900
    },
    {
      "epoch": 13.687690742624618,
      "grad_norm": 22.00644874572754,
      "learning_rate": 3.6312309257375385e-05,
      "loss": 2.6332,
      "step": 26910
    },
    {
      "epoch": 13.692777212614445,
      "grad_norm": 25.99074935913086,
      "learning_rate": 3.630722278738556e-05,
      "loss": 2.6561,
      "step": 26920
    },
    {
      "epoch": 13.697863682604273,
      "grad_norm": 24.168493270874023,
      "learning_rate": 3.630213631739573e-05,
      "loss": 2.5304,
      "step": 26930
    },
    {
      "epoch": 13.7029501525941,
      "grad_norm": 22.083608627319336,
      "learning_rate": 3.62970498474059e-05,
      "loss": 2.5938,
      "step": 26940
    },
    {
      "epoch": 13.708036622583927,
      "grad_norm": 26.5695743560791,
      "learning_rate": 3.629196337741608e-05,
      "loss": 2.6206,
      "step": 26950
    },
    {
      "epoch": 13.713123092573754,
      "grad_norm": 23.523147583007812,
      "learning_rate": 3.628687690742625e-05,
      "loss": 2.5947,
      "step": 26960
    },
    {
      "epoch": 13.718209562563581,
      "grad_norm": 24.82472801208496,
      "learning_rate": 3.628179043743642e-05,
      "loss": 2.6062,
      "step": 26970
    },
    {
      "epoch": 13.723296032553408,
      "grad_norm": 27.588600158691406,
      "learning_rate": 3.6276703967446595e-05,
      "loss": 2.5261,
      "step": 26980
    },
    {
      "epoch": 13.728382502543235,
      "grad_norm": 20.96311378479004,
      "learning_rate": 3.6271617497456765e-05,
      "loss": 2.7239,
      "step": 26990
    },
    {
      "epoch": 13.733468972533062,
      "grad_norm": 17.59089469909668,
      "learning_rate": 3.6266531027466935e-05,
      "loss": 2.5686,
      "step": 27000
    },
    {
      "epoch": 13.73855544252289,
      "grad_norm": 21.5617733001709,
      "learning_rate": 3.626144455747711e-05,
      "loss": 2.5194,
      "step": 27010
    },
    {
      "epoch": 13.743641912512716,
      "grad_norm": 20.28849983215332,
      "learning_rate": 3.625635808748729e-05,
      "loss": 2.5542,
      "step": 27020
    },
    {
      "epoch": 13.748728382502543,
      "grad_norm": 25.27450180053711,
      "learning_rate": 3.625127161749746e-05,
      "loss": 2.5452,
      "step": 27030
    },
    {
      "epoch": 13.75381485249237,
      "grad_norm": 26.367286682128906,
      "learning_rate": 3.6246185147507634e-05,
      "loss": 2.4804,
      "step": 27040
    },
    {
      "epoch": 13.758901322482197,
      "grad_norm": 24.19088363647461,
      "learning_rate": 3.6241098677517804e-05,
      "loss": 2.6504,
      "step": 27050
    },
    {
      "epoch": 13.763987792472024,
      "grad_norm": 26.049924850463867,
      "learning_rate": 3.6236012207527974e-05,
      "loss": 2.5721,
      "step": 27060
    },
    {
      "epoch": 13.769074262461851,
      "grad_norm": 25.57564926147461,
      "learning_rate": 3.623092573753815e-05,
      "loss": 2.5787,
      "step": 27070
    },
    {
      "epoch": 13.774160732451678,
      "grad_norm": 22.545377731323242,
      "learning_rate": 3.622583926754832e-05,
      "loss": 2.5507,
      "step": 27080
    },
    {
      "epoch": 13.779247202441505,
      "grad_norm": 24.359657287597656,
      "learning_rate": 3.622075279755849e-05,
      "loss": 2.5428,
      "step": 27090
    },
    {
      "epoch": 13.784333672431332,
      "grad_norm": 28.095521926879883,
      "learning_rate": 3.621566632756867e-05,
      "loss": 2.5749,
      "step": 27100
    },
    {
      "epoch": 13.789420142421159,
      "grad_norm": 24.757518768310547,
      "learning_rate": 3.6210579857578844e-05,
      "loss": 2.5609,
      "step": 27110
    },
    {
      "epoch": 13.794506612410986,
      "grad_norm": 28.370254516601562,
      "learning_rate": 3.6205493387589014e-05,
      "loss": 2.5937,
      "step": 27120
    },
    {
      "epoch": 13.799593082400813,
      "grad_norm": 24.03608512878418,
      "learning_rate": 3.620040691759919e-05,
      "loss": 2.4786,
      "step": 27130
    },
    {
      "epoch": 13.804679552390642,
      "grad_norm": 24.770414352416992,
      "learning_rate": 3.619532044760936e-05,
      "loss": 2.5819,
      "step": 27140
    },
    {
      "epoch": 13.809766022380469,
      "grad_norm": 20.758325576782227,
      "learning_rate": 3.619023397761954e-05,
      "loss": 2.5681,
      "step": 27150
    },
    {
      "epoch": 13.814852492370296,
      "grad_norm": 24.458045959472656,
      "learning_rate": 3.618514750762971e-05,
      "loss": 2.5519,
      "step": 27160
    },
    {
      "epoch": 13.819938962360123,
      "grad_norm": 25.47452735900879,
      "learning_rate": 3.618006103763988e-05,
      "loss": 2.5357,
      "step": 27170
    },
    {
      "epoch": 13.82502543234995,
      "grad_norm": 22.326101303100586,
      "learning_rate": 3.6174974567650054e-05,
      "loss": 2.5043,
      "step": 27180
    },
    {
      "epoch": 13.830111902339777,
      "grad_norm": 26.49034881591797,
      "learning_rate": 3.6169888097660223e-05,
      "loss": 2.5346,
      "step": 27190
    },
    {
      "epoch": 13.835198372329604,
      "grad_norm": 23.79900550842285,
      "learning_rate": 3.61648016276704e-05,
      "loss": 2.5267,
      "step": 27200
    },
    {
      "epoch": 13.84028484231943,
      "grad_norm": 28.70361328125,
      "learning_rate": 3.615971515768058e-05,
      "loss": 2.5605,
      "step": 27210
    },
    {
      "epoch": 13.845371312309258,
      "grad_norm": 22.442724227905273,
      "learning_rate": 3.615462868769075e-05,
      "loss": 2.6388,
      "step": 27220
    },
    {
      "epoch": 13.850457782299085,
      "grad_norm": 22.486051559448242,
      "learning_rate": 3.6149542217700917e-05,
      "loss": 2.5005,
      "step": 27230
    },
    {
      "epoch": 13.855544252288912,
      "grad_norm": 28.153364181518555,
      "learning_rate": 3.614445574771109e-05,
      "loss": 2.546,
      "step": 27240
    },
    {
      "epoch": 13.860630722278739,
      "grad_norm": 29.812231063842773,
      "learning_rate": 3.613936927772126e-05,
      "loss": 2.4994,
      "step": 27250
    },
    {
      "epoch": 13.865717192268566,
      "grad_norm": 24.985698699951172,
      "learning_rate": 3.613428280773143e-05,
      "loss": 2.5449,
      "step": 27260
    },
    {
      "epoch": 13.870803662258393,
      "grad_norm": 24.14445686340332,
      "learning_rate": 3.612919633774161e-05,
      "loss": 2.5783,
      "step": 27270
    },
    {
      "epoch": 13.87589013224822,
      "grad_norm": 21.392314910888672,
      "learning_rate": 3.612410986775178e-05,
      "loss": 2.6089,
      "step": 27280
    },
    {
      "epoch": 13.880976602238047,
      "grad_norm": 32.452850341796875,
      "learning_rate": 3.6119023397761956e-05,
      "loss": 2.5623,
      "step": 27290
    },
    {
      "epoch": 13.886063072227874,
      "grad_norm": 21.14097023010254,
      "learning_rate": 3.6113936927772126e-05,
      "loss": 2.5451,
      "step": 27300
    },
    {
      "epoch": 13.8911495422177,
      "grad_norm": 28.92094612121582,
      "learning_rate": 3.61088504577823e-05,
      "loss": 2.5657,
      "step": 27310
    },
    {
      "epoch": 13.896236012207527,
      "grad_norm": 24.841888427734375,
      "learning_rate": 3.610376398779247e-05,
      "loss": 2.528,
      "step": 27320
    },
    {
      "epoch": 13.901322482197354,
      "grad_norm": 20.21637725830078,
      "learning_rate": 3.609867751780265e-05,
      "loss": 2.5454,
      "step": 27330
    },
    {
      "epoch": 13.906408952187181,
      "grad_norm": 27.20342445373535,
      "learning_rate": 3.609359104781282e-05,
      "loss": 2.55,
      "step": 27340
    },
    {
      "epoch": 13.911495422177008,
      "grad_norm": 22.187732696533203,
      "learning_rate": 3.608850457782299e-05,
      "loss": 2.4306,
      "step": 27350
    },
    {
      "epoch": 13.916581892166835,
      "grad_norm": 22.059219360351562,
      "learning_rate": 3.6083418107833166e-05,
      "loss": 2.6405,
      "step": 27360
    },
    {
      "epoch": 13.921668362156662,
      "grad_norm": 35.64411544799805,
      "learning_rate": 3.6078331637843336e-05,
      "loss": 2.5885,
      "step": 27370
    },
    {
      "epoch": 13.926754832146491,
      "grad_norm": 24.946033477783203,
      "learning_rate": 3.6073245167853506e-05,
      "loss": 2.632,
      "step": 27380
    },
    {
      "epoch": 13.931841302136318,
      "grad_norm": 25.545747756958008,
      "learning_rate": 3.606815869786368e-05,
      "loss": 2.517,
      "step": 27390
    },
    {
      "epoch": 13.936927772126145,
      "grad_norm": 28.263317108154297,
      "learning_rate": 3.606307222787386e-05,
      "loss": 2.6274,
      "step": 27400
    },
    {
      "epoch": 13.942014242115972,
      "grad_norm": 24.972307205200195,
      "learning_rate": 3.605798575788403e-05,
      "loss": 2.6149,
      "step": 27410
    },
    {
      "epoch": 13.947100712105799,
      "grad_norm": 24.960025787353516,
      "learning_rate": 3.6052899287894206e-05,
      "loss": 2.593,
      "step": 27420
    },
    {
      "epoch": 13.952187182095626,
      "grad_norm": 23.796689987182617,
      "learning_rate": 3.6047812817904375e-05,
      "loss": 2.5025,
      "step": 27430
    },
    {
      "epoch": 13.957273652085453,
      "grad_norm": 21.601106643676758,
      "learning_rate": 3.604272634791455e-05,
      "loss": 2.5162,
      "step": 27440
    },
    {
      "epoch": 13.96236012207528,
      "grad_norm": 28.733631134033203,
      "learning_rate": 3.603763987792472e-05,
      "loss": 2.5729,
      "step": 27450
    },
    {
      "epoch": 13.967446592065107,
      "grad_norm": 28.748634338378906,
      "learning_rate": 3.603255340793489e-05,
      "loss": 2.5044,
      "step": 27460
    },
    {
      "epoch": 13.972533062054934,
      "grad_norm": 21.197071075439453,
      "learning_rate": 3.602746693794507e-05,
      "loss": 2.5618,
      "step": 27470
    },
    {
      "epoch": 13.977619532044761,
      "grad_norm": 25.073305130004883,
      "learning_rate": 3.602238046795524e-05,
      "loss": 2.5322,
      "step": 27480
    },
    {
      "epoch": 13.982706002034588,
      "grad_norm": 22.10771369934082,
      "learning_rate": 3.6017293997965415e-05,
      "loss": 2.5228,
      "step": 27490
    },
    {
      "epoch": 13.987792472024415,
      "grad_norm": 23.761953353881836,
      "learning_rate": 3.601220752797559e-05,
      "loss": 2.598,
      "step": 27500
    },
    {
      "epoch": 13.992878942014242,
      "grad_norm": 28.778226852416992,
      "learning_rate": 3.600712105798576e-05,
      "loss": 2.5364,
      "step": 27510
    },
    {
      "epoch": 13.997965412004069,
      "grad_norm": 22.39048957824707,
      "learning_rate": 3.600203458799593e-05,
      "loss": 2.5928,
      "step": 27520
    },
    {
      "epoch": 14.0,
      "eval_loss": 3.999718427658081,
      "eval_runtime": 2.7351,
      "eval_samples_per_second": 1014.57,
      "eval_steps_per_second": 126.867,
      "step": 27524
    },
    {
      "epoch": 14.003051881993896,
      "grad_norm": 23.142223358154297,
      "learning_rate": 3.599694811800611e-05,
      "loss": 2.5132,
      "step": 27530
    },
    {
      "epoch": 14.008138351983723,
      "grad_norm": 32.49002456665039,
      "learning_rate": 3.599186164801628e-05,
      "loss": 2.4784,
      "step": 27540
    },
    {
      "epoch": 14.01322482197355,
      "grad_norm": 24.993547439575195,
      "learning_rate": 3.598677517802645e-05,
      "loss": 2.4576,
      "step": 27550
    },
    {
      "epoch": 14.018311291963377,
      "grad_norm": 27.214996337890625,
      "learning_rate": 3.5981688708036625e-05,
      "loss": 2.497,
      "step": 27560
    },
    {
      "epoch": 14.023397761953204,
      "grad_norm": 25.93816375732422,
      "learning_rate": 3.5976602238046795e-05,
      "loss": 2.5295,
      "step": 27570
    },
    {
      "epoch": 14.02848423194303,
      "grad_norm": 29.219207763671875,
      "learning_rate": 3.597151576805697e-05,
      "loss": 2.6124,
      "step": 27580
    },
    {
      "epoch": 14.033570701932858,
      "grad_norm": 26.4349365234375,
      "learning_rate": 3.596642929806715e-05,
      "loss": 2.4838,
      "step": 27590
    },
    {
      "epoch": 14.038657171922686,
      "grad_norm": 37.600467681884766,
      "learning_rate": 3.596134282807732e-05,
      "loss": 2.5527,
      "step": 27600
    },
    {
      "epoch": 14.043743641912513,
      "grad_norm": 24.772937774658203,
      "learning_rate": 3.595625635808749e-05,
      "loss": 2.5219,
      "step": 27610
    },
    {
      "epoch": 14.04883011190234,
      "grad_norm": 33.65708541870117,
      "learning_rate": 3.5951169888097664e-05,
      "loss": 2.5428,
      "step": 27620
    },
    {
      "epoch": 14.053916581892167,
      "grad_norm": 23.54153060913086,
      "learning_rate": 3.5946083418107834e-05,
      "loss": 2.5644,
      "step": 27630
    },
    {
      "epoch": 14.059003051881994,
      "grad_norm": 35.09742736816406,
      "learning_rate": 3.5940996948118004e-05,
      "loss": 2.5459,
      "step": 27640
    },
    {
      "epoch": 14.064089521871821,
      "grad_norm": 20.0224666595459,
      "learning_rate": 3.593591047812818e-05,
      "loss": 2.534,
      "step": 27650
    },
    {
      "epoch": 14.069175991861648,
      "grad_norm": 29.533206939697266,
      "learning_rate": 3.593082400813835e-05,
      "loss": 2.5771,
      "step": 27660
    },
    {
      "epoch": 14.074262461851475,
      "grad_norm": 28.003828048706055,
      "learning_rate": 3.592573753814852e-05,
      "loss": 2.5677,
      "step": 27670
    },
    {
      "epoch": 14.079348931841302,
      "grad_norm": 33.285003662109375,
      "learning_rate": 3.59206510681587e-05,
      "loss": 2.5284,
      "step": 27680
    },
    {
      "epoch": 14.08443540183113,
      "grad_norm": 24.354618072509766,
      "learning_rate": 3.5915564598168874e-05,
      "loss": 2.526,
      "step": 27690
    },
    {
      "epoch": 14.089521871820956,
      "grad_norm": 29.604564666748047,
      "learning_rate": 3.591047812817905e-05,
      "loss": 2.4392,
      "step": 27700
    },
    {
      "epoch": 14.094608341810783,
      "grad_norm": 24.25470733642578,
      "learning_rate": 3.590539165818922e-05,
      "loss": 2.5457,
      "step": 27710
    },
    {
      "epoch": 14.09969481180061,
      "grad_norm": 25.8465633392334,
      "learning_rate": 3.590030518819939e-05,
      "loss": 2.5484,
      "step": 27720
    },
    {
      "epoch": 14.104781281790437,
      "grad_norm": 27.554738998413086,
      "learning_rate": 3.589521871820957e-05,
      "loss": 2.505,
      "step": 27730
    },
    {
      "epoch": 14.109867751780264,
      "grad_norm": 38.062286376953125,
      "learning_rate": 3.589013224821974e-05,
      "loss": 2.4656,
      "step": 27740
    },
    {
      "epoch": 14.114954221770091,
      "grad_norm": 25.982934951782227,
      "learning_rate": 3.588504577822991e-05,
      "loss": 2.4642,
      "step": 27750
    },
    {
      "epoch": 14.120040691759918,
      "grad_norm": 27.0142765045166,
      "learning_rate": 3.5879959308240084e-05,
      "loss": 2.5597,
      "step": 27760
    },
    {
      "epoch": 14.125127161749745,
      "grad_norm": 24.281532287597656,
      "learning_rate": 3.5874872838250253e-05,
      "loss": 2.6064,
      "step": 27770
    },
    {
      "epoch": 14.130213631739572,
      "grad_norm": 22.168603897094727,
      "learning_rate": 3.586978636826043e-05,
      "loss": 2.4154,
      "step": 27780
    },
    {
      "epoch": 14.135300101729399,
      "grad_norm": 24.452484130859375,
      "learning_rate": 3.586469989827061e-05,
      "loss": 2.563,
      "step": 27790
    },
    {
      "epoch": 14.140386571719226,
      "grad_norm": 25.925769805908203,
      "learning_rate": 3.585961342828078e-05,
      "loss": 2.5079,
      "step": 27800
    },
    {
      "epoch": 14.145473041709053,
      "grad_norm": 18.498056411743164,
      "learning_rate": 3.5854526958290947e-05,
      "loss": 2.589,
      "step": 27810
    },
    {
      "epoch": 14.150559511698882,
      "grad_norm": 27.72418785095215,
      "learning_rate": 3.584944048830112e-05,
      "loss": 2.5971,
      "step": 27820
    },
    {
      "epoch": 14.155645981688709,
      "grad_norm": 24.27602767944336,
      "learning_rate": 3.584435401831129e-05,
      "loss": 2.557,
      "step": 27830
    },
    {
      "epoch": 14.160732451678536,
      "grad_norm": 22.593149185180664,
      "learning_rate": 3.583926754832146e-05,
      "loss": 2.4907,
      "step": 27840
    },
    {
      "epoch": 14.165818921668363,
      "grad_norm": 25.167299270629883,
      "learning_rate": 3.583418107833164e-05,
      "loss": 2.5264,
      "step": 27850
    },
    {
      "epoch": 14.17090539165819,
      "grad_norm": 34.902061462402344,
      "learning_rate": 3.582909460834181e-05,
      "loss": 2.5932,
      "step": 27860
    },
    {
      "epoch": 14.175991861648017,
      "grad_norm": 28.075292587280273,
      "learning_rate": 3.5824008138351986e-05,
      "loss": 2.5728,
      "step": 27870
    },
    {
      "epoch": 14.181078331637844,
      "grad_norm": 25.366825103759766,
      "learning_rate": 3.581892166836216e-05,
      "loss": 2.5632,
      "step": 27880
    },
    {
      "epoch": 14.18616480162767,
      "grad_norm": 22.329309463500977,
      "learning_rate": 3.581383519837233e-05,
      "loss": 2.5314,
      "step": 27890
    },
    {
      "epoch": 14.191251271617498,
      "grad_norm": 21.871524810791016,
      "learning_rate": 3.58087487283825e-05,
      "loss": 2.5602,
      "step": 27900
    },
    {
      "epoch": 14.196337741607325,
      "grad_norm": 24.156932830810547,
      "learning_rate": 3.580366225839268e-05,
      "loss": 2.5337,
      "step": 27910
    },
    {
      "epoch": 14.201424211597152,
      "grad_norm": 30.055950164794922,
      "learning_rate": 3.579857578840285e-05,
      "loss": 2.5196,
      "step": 27920
    },
    {
      "epoch": 14.206510681586979,
      "grad_norm": 26.874204635620117,
      "learning_rate": 3.579348931841302e-05,
      "loss": 2.51,
      "step": 27930
    },
    {
      "epoch": 14.211597151576806,
      "grad_norm": 26.662187576293945,
      "learning_rate": 3.5788402848423196e-05,
      "loss": 2.5486,
      "step": 27940
    },
    {
      "epoch": 14.216683621566633,
      "grad_norm": 27.278730392456055,
      "learning_rate": 3.5783316378433366e-05,
      "loss": 2.481,
      "step": 27950
    },
    {
      "epoch": 14.22177009155646,
      "grad_norm": 26.166574478149414,
      "learning_rate": 3.577822990844354e-05,
      "loss": 2.558,
      "step": 27960
    },
    {
      "epoch": 14.226856561546287,
      "grad_norm": 24.934110641479492,
      "learning_rate": 3.577314343845371e-05,
      "loss": 2.5064,
      "step": 27970
    },
    {
      "epoch": 14.231943031536114,
      "grad_norm": 25.893936157226562,
      "learning_rate": 3.576805696846389e-05,
      "loss": 2.4778,
      "step": 27980
    },
    {
      "epoch": 14.23702950152594,
      "grad_norm": 26.635562896728516,
      "learning_rate": 3.5762970498474066e-05,
      "loss": 2.5307,
      "step": 27990
    },
    {
      "epoch": 14.242115971515767,
      "grad_norm": 32.32164001464844,
      "learning_rate": 3.5757884028484236e-05,
      "loss": 2.5718,
      "step": 28000
    },
    {
      "epoch": 14.247202441505594,
      "grad_norm": 27.25312614440918,
      "learning_rate": 3.5752797558494405e-05,
      "loss": 2.5811,
      "step": 28010
    },
    {
      "epoch": 14.252288911495421,
      "grad_norm": 22.605091094970703,
      "learning_rate": 3.574771108850458e-05,
      "loss": 2.4682,
      "step": 28020
    },
    {
      "epoch": 14.257375381485248,
      "grad_norm": 24.051801681518555,
      "learning_rate": 3.574262461851475e-05,
      "loss": 2.5032,
      "step": 28030
    },
    {
      "epoch": 14.262461851475077,
      "grad_norm": 28.65964698791504,
      "learning_rate": 3.573753814852492e-05,
      "loss": 2.5452,
      "step": 28040
    },
    {
      "epoch": 14.267548321464904,
      "grad_norm": 25.025484085083008,
      "learning_rate": 3.57324516785351e-05,
      "loss": 2.5243,
      "step": 28050
    },
    {
      "epoch": 14.272634791454731,
      "grad_norm": 24.46236801147461,
      "learning_rate": 3.572736520854527e-05,
      "loss": 2.5708,
      "step": 28060
    },
    {
      "epoch": 14.277721261444558,
      "grad_norm": 25.966169357299805,
      "learning_rate": 3.5722278738555445e-05,
      "loss": 2.4654,
      "step": 28070
    },
    {
      "epoch": 14.282807731434385,
      "grad_norm": 29.549278259277344,
      "learning_rate": 3.571719226856562e-05,
      "loss": 2.5476,
      "step": 28080
    },
    {
      "epoch": 14.287894201424212,
      "grad_norm": 26.98677635192871,
      "learning_rate": 3.571210579857579e-05,
      "loss": 2.4957,
      "step": 28090
    },
    {
      "epoch": 14.292980671414039,
      "grad_norm": 23.625707626342773,
      "learning_rate": 3.570701932858596e-05,
      "loss": 2.4811,
      "step": 28100
    },
    {
      "epoch": 14.298067141403866,
      "grad_norm": 24.516382217407227,
      "learning_rate": 3.570193285859614e-05,
      "loss": 2.4438,
      "step": 28110
    },
    {
      "epoch": 14.303153611393693,
      "grad_norm": 20.809310913085938,
      "learning_rate": 3.569684638860631e-05,
      "loss": 2.6032,
      "step": 28120
    },
    {
      "epoch": 14.30824008138352,
      "grad_norm": 24.52149200439453,
      "learning_rate": 3.569175991861648e-05,
      "loss": 2.5227,
      "step": 28130
    },
    {
      "epoch": 14.313326551373347,
      "grad_norm": 21.064966201782227,
      "learning_rate": 3.5686673448626655e-05,
      "loss": 2.4443,
      "step": 28140
    },
    {
      "epoch": 14.318413021363174,
      "grad_norm": 26.48155403137207,
      "learning_rate": 3.5681586978636825e-05,
      "loss": 2.475,
      "step": 28150
    },
    {
      "epoch": 14.323499491353001,
      "grad_norm": 25.540998458862305,
      "learning_rate": 3.5676500508647e-05,
      "loss": 2.5041,
      "step": 28160
    },
    {
      "epoch": 14.328585961342828,
      "grad_norm": 22.04833221435547,
      "learning_rate": 3.567141403865718e-05,
      "loss": 2.5472,
      "step": 28170
    },
    {
      "epoch": 14.333672431332655,
      "grad_norm": 26.430030822753906,
      "learning_rate": 3.566632756866735e-05,
      "loss": 2.5347,
      "step": 28180
    },
    {
      "epoch": 14.338758901322482,
      "grad_norm": 26.93621253967285,
      "learning_rate": 3.566124109867752e-05,
      "loss": 2.5385,
      "step": 28190
    },
    {
      "epoch": 14.343845371312309,
      "grad_norm": 35.655548095703125,
      "learning_rate": 3.5656154628687694e-05,
      "loss": 2.471,
      "step": 28200
    },
    {
      "epoch": 14.348931841302136,
      "grad_norm": 36.976280212402344,
      "learning_rate": 3.5651068158697864e-05,
      "loss": 2.5493,
      "step": 28210
    },
    {
      "epoch": 14.354018311291963,
      "grad_norm": 29.695566177368164,
      "learning_rate": 3.5645981688708034e-05,
      "loss": 2.4986,
      "step": 28220
    },
    {
      "epoch": 14.35910478128179,
      "grad_norm": 27.65077781677246,
      "learning_rate": 3.564089521871821e-05,
      "loss": 2.6045,
      "step": 28230
    },
    {
      "epoch": 14.364191251271617,
      "grad_norm": 23.982418060302734,
      "learning_rate": 3.563580874872838e-05,
      "loss": 2.5202,
      "step": 28240
    },
    {
      "epoch": 14.369277721261444,
      "grad_norm": 23.105234146118164,
      "learning_rate": 3.563072227873856e-05,
      "loss": 2.455,
      "step": 28250
    },
    {
      "epoch": 14.37436419125127,
      "grad_norm": 28.6563777923584,
      "learning_rate": 3.5625635808748734e-05,
      "loss": 2.477,
      "step": 28260
    },
    {
      "epoch": 14.3794506612411,
      "grad_norm": 30.019298553466797,
      "learning_rate": 3.5620549338758904e-05,
      "loss": 2.5249,
      "step": 28270
    },
    {
      "epoch": 14.384537131230926,
      "grad_norm": 31.269145965576172,
      "learning_rate": 3.561546286876908e-05,
      "loss": 2.542,
      "step": 28280
    },
    {
      "epoch": 14.389623601220753,
      "grad_norm": 33.26258850097656,
      "learning_rate": 3.561037639877925e-05,
      "loss": 2.5769,
      "step": 28290
    },
    {
      "epoch": 14.39471007121058,
      "grad_norm": 31.868532180786133,
      "learning_rate": 3.560528992878942e-05,
      "loss": 2.5812,
      "step": 28300
    },
    {
      "epoch": 14.399796541200407,
      "grad_norm": 44.18618392944336,
      "learning_rate": 3.56002034587996e-05,
      "loss": 2.482,
      "step": 28310
    },
    {
      "epoch": 14.404883011190234,
      "grad_norm": 29.01361656188965,
      "learning_rate": 3.559511698880977e-05,
      "loss": 2.5066,
      "step": 28320
    },
    {
      "epoch": 14.409969481180061,
      "grad_norm": 23.482681274414062,
      "learning_rate": 3.559003051881994e-05,
      "loss": 2.5264,
      "step": 28330
    },
    {
      "epoch": 14.415055951169888,
      "grad_norm": 19.453426361083984,
      "learning_rate": 3.5584944048830114e-05,
      "loss": 2.5717,
      "step": 28340
    },
    {
      "epoch": 14.420142421159715,
      "grad_norm": 30.613576889038086,
      "learning_rate": 3.5579857578840283e-05,
      "loss": 2.4495,
      "step": 28350
    },
    {
      "epoch": 14.425228891149542,
      "grad_norm": 33.82867431640625,
      "learning_rate": 3.557477110885046e-05,
      "loss": 2.4783,
      "step": 28360
    },
    {
      "epoch": 14.43031536113937,
      "grad_norm": 25.421091079711914,
      "learning_rate": 3.556968463886064e-05,
      "loss": 2.5358,
      "step": 28370
    },
    {
      "epoch": 14.435401831129196,
      "grad_norm": 32.592166900634766,
      "learning_rate": 3.556459816887081e-05,
      "loss": 2.5,
      "step": 28380
    },
    {
      "epoch": 14.440488301119023,
      "grad_norm": 22.62225914001465,
      "learning_rate": 3.5559511698880977e-05,
      "loss": 2.4905,
      "step": 28390
    },
    {
      "epoch": 14.44557477110885,
      "grad_norm": 27.44552230834961,
      "learning_rate": 3.555442522889115e-05,
      "loss": 2.5015,
      "step": 28400
    },
    {
      "epoch": 14.450661241098677,
      "grad_norm": 24.341142654418945,
      "learning_rate": 3.554933875890132e-05,
      "loss": 2.3717,
      "step": 28410
    },
    {
      "epoch": 14.455747711088504,
      "grad_norm": 23.511192321777344,
      "learning_rate": 3.554425228891149e-05,
      "loss": 2.5156,
      "step": 28420
    },
    {
      "epoch": 14.460834181078331,
      "grad_norm": 24.681974411010742,
      "learning_rate": 3.553916581892167e-05,
      "loss": 2.4857,
      "step": 28430
    },
    {
      "epoch": 14.465920651068158,
      "grad_norm": 27.96304702758789,
      "learning_rate": 3.553407934893184e-05,
      "loss": 2.5606,
      "step": 28440
    },
    {
      "epoch": 14.471007121057985,
      "grad_norm": 27.917444229125977,
      "learning_rate": 3.5528992878942016e-05,
      "loss": 2.4695,
      "step": 28450
    },
    {
      "epoch": 14.476093591047812,
      "grad_norm": 23.645614624023438,
      "learning_rate": 3.552390640895219e-05,
      "loss": 2.4265,
      "step": 28460
    },
    {
      "epoch": 14.481180061037639,
      "grad_norm": 27.023649215698242,
      "learning_rate": 3.551881993896236e-05,
      "loss": 2.4655,
      "step": 28470
    },
    {
      "epoch": 14.486266531027466,
      "grad_norm": 27.478273391723633,
      "learning_rate": 3.551373346897253e-05,
      "loss": 2.5346,
      "step": 28480
    },
    {
      "epoch": 14.491353001017295,
      "grad_norm": 25.791589736938477,
      "learning_rate": 3.550864699898271e-05,
      "loss": 2.5218,
      "step": 28490
    },
    {
      "epoch": 14.496439471007122,
      "grad_norm": 25.809158325195312,
      "learning_rate": 3.550356052899288e-05,
      "loss": 2.499,
      "step": 28500
    },
    {
      "epoch": 14.501525940996949,
      "grad_norm": 21.509794235229492,
      "learning_rate": 3.5498474059003056e-05,
      "loss": 2.5323,
      "step": 28510
    },
    {
      "epoch": 14.506612410986776,
      "grad_norm": 25.64480972290039,
      "learning_rate": 3.5493387589013226e-05,
      "loss": 2.4826,
      "step": 28520
    },
    {
      "epoch": 14.511698880976603,
      "grad_norm": 28.06730079650879,
      "learning_rate": 3.5488301119023396e-05,
      "loss": 2.4524,
      "step": 28530
    },
    {
      "epoch": 14.51678535096643,
      "grad_norm": 20.785310745239258,
      "learning_rate": 3.548321464903357e-05,
      "loss": 2.5008,
      "step": 28540
    },
    {
      "epoch": 14.521871820956257,
      "grad_norm": 28.982257843017578,
      "learning_rate": 3.547812817904375e-05,
      "loss": 2.5067,
      "step": 28550
    },
    {
      "epoch": 14.526958290946084,
      "grad_norm": 24.543474197387695,
      "learning_rate": 3.547304170905392e-05,
      "loss": 2.4461,
      "step": 28560
    },
    {
      "epoch": 14.53204476093591,
      "grad_norm": 31.000085830688477,
      "learning_rate": 3.5467955239064096e-05,
      "loss": 2.4622,
      "step": 28570
    },
    {
      "epoch": 14.537131230925738,
      "grad_norm": 31.66912078857422,
      "learning_rate": 3.5462868769074266e-05,
      "loss": 2.4761,
      "step": 28580
    },
    {
      "epoch": 14.542217700915565,
      "grad_norm": 32.08290481567383,
      "learning_rate": 3.5457782299084435e-05,
      "loss": 2.4939,
      "step": 28590
    },
    {
      "epoch": 14.547304170905392,
      "grad_norm": 30.776456832885742,
      "learning_rate": 3.545269582909461e-05,
      "loss": 2.524,
      "step": 28600
    },
    {
      "epoch": 14.552390640895219,
      "grad_norm": 23.31394386291504,
      "learning_rate": 3.544760935910478e-05,
      "loss": 2.4817,
      "step": 28610
    },
    {
      "epoch": 14.557477110885046,
      "grad_norm": 39.334625244140625,
      "learning_rate": 3.544252288911495e-05,
      "loss": 2.4205,
      "step": 28620
    },
    {
      "epoch": 14.562563580874873,
      "grad_norm": 25.68551254272461,
      "learning_rate": 3.543743641912513e-05,
      "loss": 2.4219,
      "step": 28630
    },
    {
      "epoch": 14.5676500508647,
      "grad_norm": 29.479795455932617,
      "learning_rate": 3.54323499491353e-05,
      "loss": 2.5326,
      "step": 28640
    },
    {
      "epoch": 14.572736520854527,
      "grad_norm": 31.0422306060791,
      "learning_rate": 3.5427263479145475e-05,
      "loss": 2.5135,
      "step": 28650
    },
    {
      "epoch": 14.577822990844354,
      "grad_norm": 23.2525634765625,
      "learning_rate": 3.542217700915565e-05,
      "loss": 2.4756,
      "step": 28660
    },
    {
      "epoch": 14.58290946083418,
      "grad_norm": 31.545726776123047,
      "learning_rate": 3.541709053916582e-05,
      "loss": 2.4439,
      "step": 28670
    },
    {
      "epoch": 14.587995930824007,
      "grad_norm": 29.991943359375,
      "learning_rate": 3.541200406917599e-05,
      "loss": 2.4845,
      "step": 28680
    },
    {
      "epoch": 14.593082400813834,
      "grad_norm": 28.80584716796875,
      "learning_rate": 3.540691759918617e-05,
      "loss": 2.4562,
      "step": 28690
    },
    {
      "epoch": 14.598168870803661,
      "grad_norm": 27.434131622314453,
      "learning_rate": 3.540183112919634e-05,
      "loss": 2.4066,
      "step": 28700
    },
    {
      "epoch": 14.603255340793488,
      "grad_norm": 36.6966438293457,
      "learning_rate": 3.539674465920651e-05,
      "loss": 2.5242,
      "step": 28710
    },
    {
      "epoch": 14.608341810783317,
      "grad_norm": 28.178258895874023,
      "learning_rate": 3.5391658189216685e-05,
      "loss": 2.5057,
      "step": 28720
    },
    {
      "epoch": 14.613428280773144,
      "grad_norm": 33.548545837402344,
      "learning_rate": 3.5386571719226855e-05,
      "loss": 2.5645,
      "step": 28730
    },
    {
      "epoch": 14.618514750762971,
      "grad_norm": 29.89643669128418,
      "learning_rate": 3.538148524923703e-05,
      "loss": 2.5782,
      "step": 28740
    },
    {
      "epoch": 14.623601220752798,
      "grad_norm": 27.520727157592773,
      "learning_rate": 3.537639877924721e-05,
      "loss": 2.5815,
      "step": 28750
    },
    {
      "epoch": 14.628687690742625,
      "grad_norm": 30.146493911743164,
      "learning_rate": 3.537131230925738e-05,
      "loss": 2.3823,
      "step": 28760
    },
    {
      "epoch": 14.633774160732452,
      "grad_norm": 33.64444351196289,
      "learning_rate": 3.5366225839267555e-05,
      "loss": 2.5138,
      "step": 28770
    },
    {
      "epoch": 14.638860630722279,
      "grad_norm": 23.42426300048828,
      "learning_rate": 3.5361139369277724e-05,
      "loss": 2.6003,
      "step": 28780
    },
    {
      "epoch": 14.643947100712106,
      "grad_norm": 26.733243942260742,
      "learning_rate": 3.5356052899287894e-05,
      "loss": 2.5415,
      "step": 28790
    },
    {
      "epoch": 14.649033570701933,
      "grad_norm": 27.510047912597656,
      "learning_rate": 3.535096642929807e-05,
      "loss": 2.4342,
      "step": 28800
    },
    {
      "epoch": 14.65412004069176,
      "grad_norm": 25.475221633911133,
      "learning_rate": 3.534587995930824e-05,
      "loss": 2.4816,
      "step": 28810
    },
    {
      "epoch": 14.659206510681587,
      "grad_norm": 25.642004013061523,
      "learning_rate": 3.534079348931841e-05,
      "loss": 2.4865,
      "step": 28820
    },
    {
      "epoch": 14.664292980671414,
      "grad_norm": 25.614965438842773,
      "learning_rate": 3.533570701932859e-05,
      "loss": 2.5034,
      "step": 28830
    },
    {
      "epoch": 14.669379450661241,
      "grad_norm": 22.05394172668457,
      "learning_rate": 3.5330620549338764e-05,
      "loss": 2.4763,
      "step": 28840
    },
    {
      "epoch": 14.674465920651068,
      "grad_norm": 32.367515563964844,
      "learning_rate": 3.5325534079348934e-05,
      "loss": 2.5587,
      "step": 28850
    },
    {
      "epoch": 14.679552390640895,
      "grad_norm": 24.26045036315918,
      "learning_rate": 3.532044760935911e-05,
      "loss": 2.4238,
      "step": 28860
    },
    {
      "epoch": 14.684638860630722,
      "grad_norm": 24.381820678710938,
      "learning_rate": 3.531536113936928e-05,
      "loss": 2.5882,
      "step": 28870
    },
    {
      "epoch": 14.689725330620549,
      "grad_norm": 24.071683883666992,
      "learning_rate": 3.531027466937945e-05,
      "loss": 2.6108,
      "step": 28880
    },
    {
      "epoch": 14.694811800610376,
      "grad_norm": 31.218351364135742,
      "learning_rate": 3.530518819938963e-05,
      "loss": 2.5237,
      "step": 28890
    },
    {
      "epoch": 14.699898270600203,
      "grad_norm": 30.621644973754883,
      "learning_rate": 3.53001017293998e-05,
      "loss": 2.4849,
      "step": 28900
    },
    {
      "epoch": 14.70498474059003,
      "grad_norm": 24.479551315307617,
      "learning_rate": 3.529501525940997e-05,
      "loss": 2.4709,
      "step": 28910
    },
    {
      "epoch": 14.710071210579857,
      "grad_norm": 29.72401237487793,
      "learning_rate": 3.5289928789420144e-05,
      "loss": 2.4477,
      "step": 28920
    },
    {
      "epoch": 14.715157680569686,
      "grad_norm": 25.956737518310547,
      "learning_rate": 3.5284842319430313e-05,
      "loss": 2.5275,
      "step": 28930
    },
    {
      "epoch": 14.720244150559513,
      "grad_norm": 27.843263626098633,
      "learning_rate": 3.527975584944049e-05,
      "loss": 2.5121,
      "step": 28940
    },
    {
      "epoch": 14.72533062054934,
      "grad_norm": 26.06340789794922,
      "learning_rate": 3.527466937945067e-05,
      "loss": 2.5107,
      "step": 28950
    },
    {
      "epoch": 14.730417090539166,
      "grad_norm": 21.569896697998047,
      "learning_rate": 3.526958290946084e-05,
      "loss": 2.4541,
      "step": 28960
    },
    {
      "epoch": 14.735503560528993,
      "grad_norm": 24.8012638092041,
      "learning_rate": 3.526449643947101e-05,
      "loss": 2.4472,
      "step": 28970
    },
    {
      "epoch": 14.74059003051882,
      "grad_norm": 24.87848472595215,
      "learning_rate": 3.525940996948118e-05,
      "loss": 2.4718,
      "step": 28980
    },
    {
      "epoch": 14.745676500508647,
      "grad_norm": 29.518707275390625,
      "learning_rate": 3.525432349949135e-05,
      "loss": 2.517,
      "step": 28990
    },
    {
      "epoch": 14.750762970498474,
      "grad_norm": 29.14219093322754,
      "learning_rate": 3.524923702950152e-05,
      "loss": 2.4453,
      "step": 29000
    },
    {
      "epoch": 14.755849440488301,
      "grad_norm": 25.51660919189453,
      "learning_rate": 3.52441505595117e-05,
      "loss": 2.5445,
      "step": 29010
    },
    {
      "epoch": 14.760935910478128,
      "grad_norm": 24.08369255065918,
      "learning_rate": 3.523906408952187e-05,
      "loss": 2.508,
      "step": 29020
    },
    {
      "epoch": 14.766022380467955,
      "grad_norm": 20.63327407836914,
      "learning_rate": 3.5233977619532046e-05,
      "loss": 2.4472,
      "step": 29030
    },
    {
      "epoch": 14.771108850457782,
      "grad_norm": 28.94316864013672,
      "learning_rate": 3.522889114954222e-05,
      "loss": 2.5696,
      "step": 29040
    },
    {
      "epoch": 14.77619532044761,
      "grad_norm": 25.026851654052734,
      "learning_rate": 3.522380467955239e-05,
      "loss": 2.5502,
      "step": 29050
    },
    {
      "epoch": 14.781281790437436,
      "grad_norm": 26.172178268432617,
      "learning_rate": 3.521871820956257e-05,
      "loss": 2.503,
      "step": 29060
    },
    {
      "epoch": 14.786368260427263,
      "grad_norm": 28.003511428833008,
      "learning_rate": 3.521363173957274e-05,
      "loss": 2.5851,
      "step": 29070
    },
    {
      "epoch": 14.79145473041709,
      "grad_norm": 22.13345718383789,
      "learning_rate": 3.520854526958291e-05,
      "loss": 2.4922,
      "step": 29080
    },
    {
      "epoch": 14.796541200406917,
      "grad_norm": 36.20461654663086,
      "learning_rate": 3.5203458799593086e-05,
      "loss": 2.569,
      "step": 29090
    },
    {
      "epoch": 14.801627670396744,
      "grad_norm": 22.237110137939453,
      "learning_rate": 3.5198372329603256e-05,
      "loss": 2.4491,
      "step": 29100
    },
    {
      "epoch": 14.806714140386571,
      "grad_norm": 23.72537612915039,
      "learning_rate": 3.5193285859613426e-05,
      "loss": 2.3898,
      "step": 29110
    },
    {
      "epoch": 14.811800610376398,
      "grad_norm": 21.329490661621094,
      "learning_rate": 3.51881993896236e-05,
      "loss": 2.4964,
      "step": 29120
    },
    {
      "epoch": 14.816887080366225,
      "grad_norm": 26.61436653137207,
      "learning_rate": 3.518311291963378e-05,
      "loss": 2.4605,
      "step": 29130
    },
    {
      "epoch": 14.821973550356052,
      "grad_norm": 25.70069122314453,
      "learning_rate": 3.517802644964395e-05,
      "loss": 2.4605,
      "step": 29140
    },
    {
      "epoch": 14.827060020345879,
      "grad_norm": 23.838354110717773,
      "learning_rate": 3.5172939979654126e-05,
      "loss": 2.4692,
      "step": 29150
    },
    {
      "epoch": 14.832146490335706,
      "grad_norm": 30.8393497467041,
      "learning_rate": 3.5167853509664296e-05,
      "loss": 2.4994,
      "step": 29160
    },
    {
      "epoch": 14.837232960325535,
      "grad_norm": 33.79541015625,
      "learning_rate": 3.5162767039674465e-05,
      "loss": 2.453,
      "step": 29170
    },
    {
      "epoch": 14.842319430315362,
      "grad_norm": 27.872535705566406,
      "learning_rate": 3.515768056968464e-05,
      "loss": 2.5883,
      "step": 29180
    },
    {
      "epoch": 14.847405900305189,
      "grad_norm": 25.10430335998535,
      "learning_rate": 3.515259409969481e-05,
      "loss": 2.4493,
      "step": 29190
    },
    {
      "epoch": 14.852492370295016,
      "grad_norm": 24.549728393554688,
      "learning_rate": 3.514750762970498e-05,
      "loss": 2.4684,
      "step": 29200
    },
    {
      "epoch": 14.857578840284843,
      "grad_norm": 32.73617935180664,
      "learning_rate": 3.514242115971516e-05,
      "loss": 2.4156,
      "step": 29210
    },
    {
      "epoch": 14.86266531027467,
      "grad_norm": 35.3087043762207,
      "learning_rate": 3.5137334689725335e-05,
      "loss": 2.4364,
      "step": 29220
    },
    {
      "epoch": 14.867751780264497,
      "grad_norm": 25.671249389648438,
      "learning_rate": 3.5132248219735505e-05,
      "loss": 2.4358,
      "step": 29230
    },
    {
      "epoch": 14.872838250254324,
      "grad_norm": 29.555150985717773,
      "learning_rate": 3.512716174974568e-05,
      "loss": 2.4409,
      "step": 29240
    },
    {
      "epoch": 14.87792472024415,
      "grad_norm": 28.42913818359375,
      "learning_rate": 3.512207527975585e-05,
      "loss": 2.5179,
      "step": 29250
    },
    {
      "epoch": 14.883011190233978,
      "grad_norm": 27.1180477142334,
      "learning_rate": 3.511698880976602e-05,
      "loss": 2.45,
      "step": 29260
    },
    {
      "epoch": 14.888097660223805,
      "grad_norm": 27.64202880859375,
      "learning_rate": 3.51119023397762e-05,
      "loss": 2.5131,
      "step": 29270
    },
    {
      "epoch": 14.893184130213632,
      "grad_norm": 34.08301544189453,
      "learning_rate": 3.510681586978637e-05,
      "loss": 2.419,
      "step": 29280
    },
    {
      "epoch": 14.898270600203459,
      "grad_norm": 21.978418350219727,
      "learning_rate": 3.510172939979654e-05,
      "loss": 2.4871,
      "step": 29290
    },
    {
      "epoch": 14.903357070193286,
      "grad_norm": 25.86213493347168,
      "learning_rate": 3.5096642929806715e-05,
      "loss": 2.4365,
      "step": 29300
    },
    {
      "epoch": 14.908443540183113,
      "grad_norm": 21.82318115234375,
      "learning_rate": 3.5091556459816885e-05,
      "loss": 2.6168,
      "step": 29310
    },
    {
      "epoch": 14.91353001017294,
      "grad_norm": 31.59905433654785,
      "learning_rate": 3.508646998982706e-05,
      "loss": 2.4539,
      "step": 29320
    },
    {
      "epoch": 14.918616480162767,
      "grad_norm": 32.03992462158203,
      "learning_rate": 3.508138351983724e-05,
      "loss": 2.4616,
      "step": 29330
    },
    {
      "epoch": 14.923702950152594,
      "grad_norm": 28.46120834350586,
      "learning_rate": 3.507629704984741e-05,
      "loss": 2.4764,
      "step": 29340
    },
    {
      "epoch": 14.92878942014242,
      "grad_norm": 25.27185821533203,
      "learning_rate": 3.5071210579857585e-05,
      "loss": 2.439,
      "step": 29350
    },
    {
      "epoch": 14.933875890132247,
      "grad_norm": 28.794878005981445,
      "learning_rate": 3.5066124109867754e-05,
      "loss": 2.4372,
      "step": 29360
    },
    {
      "epoch": 14.938962360122074,
      "grad_norm": 27.286048889160156,
      "learning_rate": 3.5061037639877924e-05,
      "loss": 2.5388,
      "step": 29370
    },
    {
      "epoch": 14.944048830111903,
      "grad_norm": 28.40847396850586,
      "learning_rate": 3.50559511698881e-05,
      "loss": 2.503,
      "step": 29380
    },
    {
      "epoch": 14.94913530010173,
      "grad_norm": 35.282875061035156,
      "learning_rate": 3.505086469989827e-05,
      "loss": 2.5781,
      "step": 29390
    },
    {
      "epoch": 14.954221770091557,
      "grad_norm": 30.918441772460938,
      "learning_rate": 3.504577822990844e-05,
      "loss": 2.4431,
      "step": 29400
    },
    {
      "epoch": 14.959308240081384,
      "grad_norm": 30.576406478881836,
      "learning_rate": 3.504069175991862e-05,
      "loss": 2.5049,
      "step": 29410
    },
    {
      "epoch": 14.964394710071211,
      "grad_norm": 28.838468551635742,
      "learning_rate": 3.5035605289928794e-05,
      "loss": 2.5018,
      "step": 29420
    },
    {
      "epoch": 14.969481180061038,
      "grad_norm": 33.75078201293945,
      "learning_rate": 3.5030518819938964e-05,
      "loss": 2.4953,
      "step": 29430
    },
    {
      "epoch": 14.974567650050865,
      "grad_norm": 25.712038040161133,
      "learning_rate": 3.502543234994914e-05,
      "loss": 2.5319,
      "step": 29440
    },
    {
      "epoch": 14.979654120040692,
      "grad_norm": 25.411813735961914,
      "learning_rate": 3.502034587995931e-05,
      "loss": 2.4824,
      "step": 29450
    },
    {
      "epoch": 14.984740590030519,
      "grad_norm": 25.02060890197754,
      "learning_rate": 3.501525940996948e-05,
      "loss": 2.4496,
      "step": 29460
    },
    {
      "epoch": 14.989827060020346,
      "grad_norm": 22.805389404296875,
      "learning_rate": 3.501017293997966e-05,
      "loss": 2.4774,
      "step": 29470
    },
    {
      "epoch": 14.994913530010173,
      "grad_norm": 29.88399314880371,
      "learning_rate": 3.500508646998983e-05,
      "loss": 2.4262,
      "step": 29480
    },
    {
      "epoch": 15.0,
      "grad_norm": 30.999717712402344,
      "learning_rate": 3.5e-05,
      "loss": 2.5055,
      "step": 29490
    },
    {
      "epoch": 15.0,
      "eval_loss": 4.0656304359436035,
      "eval_runtime": 2.7593,
      "eval_samples_per_second": 1005.674,
      "eval_steps_per_second": 125.755,
      "step": 29490
    },
    {
      "epoch": 15.005086469989827,
      "grad_norm": 31.1119384765625,
      "learning_rate": 3.4994913530010174e-05,
      "loss": 2.4137,
      "step": 29500
    },
    {
      "epoch": 15.010172939979654,
      "grad_norm": 27.285097122192383,
      "learning_rate": 3.498982706002035e-05,
      "loss": 2.5165,
      "step": 29510
    },
    {
      "epoch": 15.015259409969481,
      "grad_norm": 25.163257598876953,
      "learning_rate": 3.498474059003052e-05,
      "loss": 2.44,
      "step": 29520
    },
    {
      "epoch": 15.020345879959308,
      "grad_norm": 27.80599594116211,
      "learning_rate": 3.49796541200407e-05,
      "loss": 2.4895,
      "step": 29530
    },
    {
      "epoch": 15.025432349949135,
      "grad_norm": 26.265695571899414,
      "learning_rate": 3.497456765005087e-05,
      "loss": 2.4872,
      "step": 29540
    },
    {
      "epoch": 15.030518819938962,
      "grad_norm": 24.028051376342773,
      "learning_rate": 3.496948118006104e-05,
      "loss": 2.5043,
      "step": 29550
    },
    {
      "epoch": 15.035605289928789,
      "grad_norm": 23.567787170410156,
      "learning_rate": 3.496439471007121e-05,
      "loss": 2.4781,
      "step": 29560
    },
    {
      "epoch": 15.040691759918616,
      "grad_norm": 33.500823974609375,
      "learning_rate": 3.495930824008138e-05,
      "loss": 2.4168,
      "step": 29570
    },
    {
      "epoch": 15.045778229908443,
      "grad_norm": 25.171037673950195,
      "learning_rate": 3.495422177009156e-05,
      "loss": 2.4114,
      "step": 29580
    },
    {
      "epoch": 15.05086469989827,
      "grad_norm": 30.108015060424805,
      "learning_rate": 3.494913530010173e-05,
      "loss": 2.4265,
      "step": 29590
    },
    {
      "epoch": 15.055951169888097,
      "grad_norm": 27.33765983581543,
      "learning_rate": 3.49440488301119e-05,
      "loss": 2.4053,
      "step": 29600
    },
    {
      "epoch": 15.061037639877926,
      "grad_norm": 21.56207847595215,
      "learning_rate": 3.4938962360122076e-05,
      "loss": 2.4341,
      "step": 29610
    },
    {
      "epoch": 15.066124109867753,
      "grad_norm": 29.367277145385742,
      "learning_rate": 3.493387589013225e-05,
      "loss": 2.4261,
      "step": 29620
    },
    {
      "epoch": 15.07121057985758,
      "grad_norm": 26.990394592285156,
      "learning_rate": 3.492878942014242e-05,
      "loss": 2.4991,
      "step": 29630
    },
    {
      "epoch": 15.076297049847406,
      "grad_norm": 24.615041732788086,
      "learning_rate": 3.49237029501526e-05,
      "loss": 2.4938,
      "step": 29640
    },
    {
      "epoch": 15.081383519837233,
      "grad_norm": 24.650894165039062,
      "learning_rate": 3.491861648016277e-05,
      "loss": 2.5341,
      "step": 29650
    },
    {
      "epoch": 15.08646998982706,
      "grad_norm": 29.820640563964844,
      "learning_rate": 3.491353001017294e-05,
      "loss": 2.4641,
      "step": 29660
    },
    {
      "epoch": 15.091556459816887,
      "grad_norm": 27.987930297851562,
      "learning_rate": 3.4908443540183116e-05,
      "loss": 2.3961,
      "step": 29670
    },
    {
      "epoch": 15.096642929806714,
      "grad_norm": 23.469881057739258,
      "learning_rate": 3.4903357070193286e-05,
      "loss": 2.4431,
      "step": 29680
    },
    {
      "epoch": 15.101729399796541,
      "grad_norm": 24.477792739868164,
      "learning_rate": 3.4898270600203456e-05,
      "loss": 2.4827,
      "step": 29690
    },
    {
      "epoch": 15.106815869786368,
      "grad_norm": 29.33049201965332,
      "learning_rate": 3.489318413021363e-05,
      "loss": 2.5226,
      "step": 29700
    },
    {
      "epoch": 15.111902339776195,
      "grad_norm": 24.79323387145996,
      "learning_rate": 3.488809766022381e-05,
      "loss": 2.442,
      "step": 29710
    },
    {
      "epoch": 15.116988809766022,
      "grad_norm": 27.59617042541504,
      "learning_rate": 3.488301119023398e-05,
      "loss": 2.4949,
      "step": 29720
    },
    {
      "epoch": 15.12207527975585,
      "grad_norm": 25.20314598083496,
      "learning_rate": 3.4877924720244156e-05,
      "loss": 2.4502,
      "step": 29730
    },
    {
      "epoch": 15.127161749745676,
      "grad_norm": 31.08364486694336,
      "learning_rate": 3.4872838250254326e-05,
      "loss": 2.402,
      "step": 29740
    },
    {
      "epoch": 15.132248219735503,
      "grad_norm": 28.358827590942383,
      "learning_rate": 3.4867751780264495e-05,
      "loss": 2.4032,
      "step": 29750
    },
    {
      "epoch": 15.13733468972533,
      "grad_norm": 27.538860321044922,
      "learning_rate": 3.486266531027467e-05,
      "loss": 2.5251,
      "step": 29760
    },
    {
      "epoch": 15.142421159715157,
      "grad_norm": 30.83287239074707,
      "learning_rate": 3.485757884028484e-05,
      "loss": 2.4636,
      "step": 29770
    },
    {
      "epoch": 15.147507629704984,
      "grad_norm": 22.34522819519043,
      "learning_rate": 3.485249237029501e-05,
      "loss": 2.4318,
      "step": 29780
    },
    {
      "epoch": 15.152594099694811,
      "grad_norm": 22.85285758972168,
      "learning_rate": 3.484740590030519e-05,
      "loss": 2.4055,
      "step": 29790
    },
    {
      "epoch": 15.157680569684638,
      "grad_norm": 28.979393005371094,
      "learning_rate": 3.4842319430315365e-05,
      "loss": 2.3812,
      "step": 29800
    },
    {
      "epoch": 15.162767039674465,
      "grad_norm": 27.665550231933594,
      "learning_rate": 3.4837232960325535e-05,
      "loss": 2.4435,
      "step": 29810
    },
    {
      "epoch": 15.167853509664292,
      "grad_norm": 23.262338638305664,
      "learning_rate": 3.483214649033571e-05,
      "loss": 2.4156,
      "step": 29820
    },
    {
      "epoch": 15.172939979654121,
      "grad_norm": 31.584936141967773,
      "learning_rate": 3.482706002034588e-05,
      "loss": 2.4767,
      "step": 29830
    },
    {
      "epoch": 15.178026449643948,
      "grad_norm": 25.08302116394043,
      "learning_rate": 3.482197355035605e-05,
      "loss": 2.5689,
      "step": 29840
    },
    {
      "epoch": 15.183112919633775,
      "grad_norm": 28.327898025512695,
      "learning_rate": 3.481688708036623e-05,
      "loss": 2.4018,
      "step": 29850
    },
    {
      "epoch": 15.188199389623602,
      "grad_norm": 29.296316146850586,
      "learning_rate": 3.48118006103764e-05,
      "loss": 2.4361,
      "step": 29860
    },
    {
      "epoch": 15.193285859613429,
      "grad_norm": 38.8968505859375,
      "learning_rate": 3.4806714140386575e-05,
      "loss": 2.4742,
      "step": 29870
    },
    {
      "epoch": 15.198372329603256,
      "grad_norm": 25.84625244140625,
      "learning_rate": 3.4801627670396745e-05,
      "loss": 2.4014,
      "step": 29880
    },
    {
      "epoch": 15.203458799593083,
      "grad_norm": 34.767765045166016,
      "learning_rate": 3.4796541200406915e-05,
      "loss": 2.5697,
      "step": 29890
    },
    {
      "epoch": 15.20854526958291,
      "grad_norm": 22.194965362548828,
      "learning_rate": 3.479145473041709e-05,
      "loss": 2.4439,
      "step": 29900
    },
    {
      "epoch": 15.213631739572737,
      "grad_norm": 33.04604721069336,
      "learning_rate": 3.478636826042727e-05,
      "loss": 2.4825,
      "step": 29910
    },
    {
      "epoch": 15.218718209562564,
      "grad_norm": 30.870620727539062,
      "learning_rate": 3.478128179043744e-05,
      "loss": 2.4956,
      "step": 29920
    },
    {
      "epoch": 15.22380467955239,
      "grad_norm": 26.455141067504883,
      "learning_rate": 3.4776195320447615e-05,
      "loss": 2.4401,
      "step": 29930
    },
    {
      "epoch": 15.228891149542218,
      "grad_norm": 28.131433486938477,
      "learning_rate": 3.4771108850457784e-05,
      "loss": 2.4766,
      "step": 29940
    },
    {
      "epoch": 15.233977619532045,
      "grad_norm": 29.691905975341797,
      "learning_rate": 3.4766022380467954e-05,
      "loss": 2.4143,
      "step": 29950
    },
    {
      "epoch": 15.239064089521872,
      "grad_norm": 26.630542755126953,
      "learning_rate": 3.476093591047813e-05,
      "loss": 2.4742,
      "step": 29960
    },
    {
      "epoch": 15.244150559511699,
      "grad_norm": 45.87558364868164,
      "learning_rate": 3.47558494404883e-05,
      "loss": 2.4194,
      "step": 29970
    },
    {
      "epoch": 15.249237029501526,
      "grad_norm": 30.18748664855957,
      "learning_rate": 3.475076297049847e-05,
      "loss": 2.4174,
      "step": 29980
    },
    {
      "epoch": 15.254323499491353,
      "grad_norm": 31.09859848022461,
      "learning_rate": 3.474567650050865e-05,
      "loss": 2.4304,
      "step": 29990
    },
    {
      "epoch": 15.25940996948118,
      "grad_norm": 24.804344177246094,
      "learning_rate": 3.4740590030518824e-05,
      "loss": 2.4206,
      "step": 30000
    },
    {
      "epoch": 15.264496439471007,
      "grad_norm": 25.82041358947754,
      "learning_rate": 3.4735503560528994e-05,
      "loss": 2.3887,
      "step": 30010
    },
    {
      "epoch": 15.269582909460834,
      "grad_norm": 23.858184814453125,
      "learning_rate": 3.473041709053917e-05,
      "loss": 2.4845,
      "step": 30020
    },
    {
      "epoch": 15.27466937945066,
      "grad_norm": 27.092567443847656,
      "learning_rate": 3.472533062054934e-05,
      "loss": 2.4366,
      "step": 30030
    },
    {
      "epoch": 15.279755849440487,
      "grad_norm": 26.233867645263672,
      "learning_rate": 3.472024415055951e-05,
      "loss": 2.4222,
      "step": 30040
    },
    {
      "epoch": 15.284842319430314,
      "grad_norm": 26.52509880065918,
      "learning_rate": 3.471515768056969e-05,
      "loss": 2.3434,
      "step": 30050
    },
    {
      "epoch": 15.289928789420143,
      "grad_norm": 29.431846618652344,
      "learning_rate": 3.471007121057986e-05,
      "loss": 2.4583,
      "step": 30060
    },
    {
      "epoch": 15.29501525940997,
      "grad_norm": 26.173757553100586,
      "learning_rate": 3.470498474059003e-05,
      "loss": 2.5326,
      "step": 30070
    },
    {
      "epoch": 15.300101729399797,
      "grad_norm": 22.93539047241211,
      "learning_rate": 3.4699898270600204e-05,
      "loss": 2.4029,
      "step": 30080
    },
    {
      "epoch": 15.305188199389624,
      "grad_norm": 25.342960357666016,
      "learning_rate": 3.469481180061038e-05,
      "loss": 2.401,
      "step": 30090
    },
    {
      "epoch": 15.310274669379451,
      "grad_norm": 26.109451293945312,
      "learning_rate": 3.468972533062055e-05,
      "loss": 2.4581,
      "step": 30100
    },
    {
      "epoch": 15.315361139369278,
      "grad_norm": 23.969552993774414,
      "learning_rate": 3.468463886063073e-05,
      "loss": 2.4407,
      "step": 30110
    },
    {
      "epoch": 15.320447609359105,
      "grad_norm": 30.006803512573242,
      "learning_rate": 3.46795523906409e-05,
      "loss": 2.4455,
      "step": 30120
    },
    {
      "epoch": 15.325534079348932,
      "grad_norm": 22.375587463378906,
      "learning_rate": 3.4674465920651073e-05,
      "loss": 2.4157,
      "step": 30130
    },
    {
      "epoch": 15.330620549338759,
      "grad_norm": 29.430221557617188,
      "learning_rate": 3.466937945066124e-05,
      "loss": 2.4558,
      "step": 30140
    },
    {
      "epoch": 15.335707019328586,
      "grad_norm": 24.571510314941406,
      "learning_rate": 3.466429298067141e-05,
      "loss": 2.4375,
      "step": 30150
    },
    {
      "epoch": 15.340793489318413,
      "grad_norm": 27.05306625366211,
      "learning_rate": 3.465920651068159e-05,
      "loss": 2.3659,
      "step": 30160
    },
    {
      "epoch": 15.34587995930824,
      "grad_norm": 32.67467498779297,
      "learning_rate": 3.465412004069176e-05,
      "loss": 2.4604,
      "step": 30170
    },
    {
      "epoch": 15.350966429298067,
      "grad_norm": 29.183378219604492,
      "learning_rate": 3.4649033570701936e-05,
      "loss": 2.4104,
      "step": 30180
    },
    {
      "epoch": 15.356052899287894,
      "grad_norm": 23.227663040161133,
      "learning_rate": 3.4643947100712106e-05,
      "loss": 2.4286,
      "step": 30190
    },
    {
      "epoch": 15.361139369277721,
      "grad_norm": 37.79901885986328,
      "learning_rate": 3.463886063072228e-05,
      "loss": 2.4552,
      "step": 30200
    },
    {
      "epoch": 15.366225839267548,
      "grad_norm": 26.265817642211914,
      "learning_rate": 3.463377416073245e-05,
      "loss": 2.4131,
      "step": 30210
    },
    {
      "epoch": 15.371312309257375,
      "grad_norm": 34.53628158569336,
      "learning_rate": 3.462868769074263e-05,
      "loss": 2.4012,
      "step": 30220
    },
    {
      "epoch": 15.376398779247202,
      "grad_norm": 32.18581008911133,
      "learning_rate": 3.46236012207528e-05,
      "loss": 2.4732,
      "step": 30230
    },
    {
      "epoch": 15.381485249237029,
      "grad_norm": 27.648622512817383,
      "learning_rate": 3.461851475076297e-05,
      "loss": 2.4475,
      "step": 30240
    },
    {
      "epoch": 15.386571719226856,
      "grad_norm": 26.82105827331543,
      "learning_rate": 3.4613428280773146e-05,
      "loss": 2.4145,
      "step": 30250
    },
    {
      "epoch": 15.391658189216683,
      "grad_norm": 22.05324363708496,
      "learning_rate": 3.4608341810783316e-05,
      "loss": 2.4347,
      "step": 30260
    },
    {
      "epoch": 15.396744659206512,
      "grad_norm": 23.83780860900879,
      "learning_rate": 3.4603255340793486e-05,
      "loss": 2.4671,
      "step": 30270
    },
    {
      "epoch": 15.401831129196339,
      "grad_norm": 28.343536376953125,
      "learning_rate": 3.459816887080366e-05,
      "loss": 2.3568,
      "step": 30280
    },
    {
      "epoch": 15.406917599186166,
      "grad_norm": 24.080596923828125,
      "learning_rate": 3.459308240081384e-05,
      "loss": 2.4644,
      "step": 30290
    },
    {
      "epoch": 15.412004069175993,
      "grad_norm": 30.4708309173584,
      "learning_rate": 3.458799593082401e-05,
      "loss": 2.3768,
      "step": 30300
    },
    {
      "epoch": 15.41709053916582,
      "grad_norm": 29.191261291503906,
      "learning_rate": 3.4582909460834186e-05,
      "loss": 2.4897,
      "step": 30310
    },
    {
      "epoch": 15.422177009155646,
      "grad_norm": 27.531414031982422,
      "learning_rate": 3.4577822990844356e-05,
      "loss": 2.4005,
      "step": 30320
    },
    {
      "epoch": 15.427263479145473,
      "grad_norm": 29.089906692504883,
      "learning_rate": 3.4572736520854525e-05,
      "loss": 2.4283,
      "step": 30330
    },
    {
      "epoch": 15.4323499491353,
      "grad_norm": 29.64827537536621,
      "learning_rate": 3.45676500508647e-05,
      "loss": 2.3988,
      "step": 30340
    },
    {
      "epoch": 15.437436419125127,
      "grad_norm": 30.484766006469727,
      "learning_rate": 3.456256358087487e-05,
      "loss": 2.4929,
      "step": 30350
    },
    {
      "epoch": 15.442522889114954,
      "grad_norm": 29.26978302001953,
      "learning_rate": 3.455747711088504e-05,
      "loss": 2.4818,
      "step": 30360
    },
    {
      "epoch": 15.447609359104781,
      "grad_norm": 25.824228286743164,
      "learning_rate": 3.455239064089522e-05,
      "loss": 2.4518,
      "step": 30370
    },
    {
      "epoch": 15.452695829094608,
      "grad_norm": 23.790678024291992,
      "learning_rate": 3.4547304170905395e-05,
      "loss": 2.3936,
      "step": 30380
    },
    {
      "epoch": 15.457782299084435,
      "grad_norm": 29.53590202331543,
      "learning_rate": 3.454221770091557e-05,
      "loss": 2.4168,
      "step": 30390
    },
    {
      "epoch": 15.462868769074262,
      "grad_norm": 23.242572784423828,
      "learning_rate": 3.453713123092574e-05,
      "loss": 2.3915,
      "step": 30400
    },
    {
      "epoch": 15.46795523906409,
      "grad_norm": 29.49665641784668,
      "learning_rate": 3.453204476093591e-05,
      "loss": 2.4736,
      "step": 30410
    },
    {
      "epoch": 15.473041709053916,
      "grad_norm": 29.778234481811523,
      "learning_rate": 3.452695829094609e-05,
      "loss": 2.495,
      "step": 30420
    },
    {
      "epoch": 15.478128179043743,
      "grad_norm": 30.53461265563965,
      "learning_rate": 3.452187182095626e-05,
      "loss": 2.3701,
      "step": 30430
    },
    {
      "epoch": 15.48321464903357,
      "grad_norm": 32.29304885864258,
      "learning_rate": 3.451678535096643e-05,
      "loss": 2.3676,
      "step": 30440
    },
    {
      "epoch": 15.488301119023397,
      "grad_norm": 27.0892391204834,
      "learning_rate": 3.4511698880976605e-05,
      "loss": 2.4103,
      "step": 30450
    },
    {
      "epoch": 15.493387589013224,
      "grad_norm": 26.524824142456055,
      "learning_rate": 3.4506612410986775e-05,
      "loss": 2.4253,
      "step": 30460
    },
    {
      "epoch": 15.498474059003051,
      "grad_norm": 24.900352478027344,
      "learning_rate": 3.450152594099695e-05,
      "loss": 2.4088,
      "step": 30470
    },
    {
      "epoch": 15.503560528992878,
      "grad_norm": 35.13578414916992,
      "learning_rate": 3.449643947100713e-05,
      "loss": 2.4576,
      "step": 30480
    },
    {
      "epoch": 15.508646998982705,
      "grad_norm": 26.39599609375,
      "learning_rate": 3.44913530010173e-05,
      "loss": 2.5428,
      "step": 30490
    },
    {
      "epoch": 15.513733468972532,
      "grad_norm": 30.03797721862793,
      "learning_rate": 3.448626653102747e-05,
      "loss": 2.4506,
      "step": 30500
    },
    {
      "epoch": 15.518819938962361,
      "grad_norm": 26.09008026123047,
      "learning_rate": 3.4481180061037645e-05,
      "loss": 2.4334,
      "step": 30510
    },
    {
      "epoch": 15.523906408952188,
      "grad_norm": 30.77952766418457,
      "learning_rate": 3.4476093591047814e-05,
      "loss": 2.4898,
      "step": 30520
    },
    {
      "epoch": 15.528992878942015,
      "grad_norm": 24.90667724609375,
      "learning_rate": 3.4471007121057984e-05,
      "loss": 2.41,
      "step": 30530
    },
    {
      "epoch": 15.534079348931842,
      "grad_norm": 29.0460147857666,
      "learning_rate": 3.446592065106816e-05,
      "loss": 2.5431,
      "step": 30540
    },
    {
      "epoch": 15.539165818921669,
      "grad_norm": 35.211753845214844,
      "learning_rate": 3.446083418107833e-05,
      "loss": 2.3912,
      "step": 30550
    },
    {
      "epoch": 15.544252288911496,
      "grad_norm": 33.573036193847656,
      "learning_rate": 3.44557477110885e-05,
      "loss": 2.4389,
      "step": 30560
    },
    {
      "epoch": 15.549338758901323,
      "grad_norm": 32.19215774536133,
      "learning_rate": 3.445066124109868e-05,
      "loss": 2.4703,
      "step": 30570
    },
    {
      "epoch": 15.55442522889115,
      "grad_norm": 27.264110565185547,
      "learning_rate": 3.4445574771108854e-05,
      "loss": 2.4571,
      "step": 30580
    },
    {
      "epoch": 15.559511698880977,
      "grad_norm": 29.07216453552246,
      "learning_rate": 3.4440488301119024e-05,
      "loss": 2.4165,
      "step": 30590
    },
    {
      "epoch": 15.564598168870804,
      "grad_norm": 30.110103607177734,
      "learning_rate": 3.44354018311292e-05,
      "loss": 2.386,
      "step": 30600
    },
    {
      "epoch": 15.56968463886063,
      "grad_norm": 30.568546295166016,
      "learning_rate": 3.443031536113937e-05,
      "loss": 2.3867,
      "step": 30610
    },
    {
      "epoch": 15.574771108850458,
      "grad_norm": 28.37567138671875,
      "learning_rate": 3.442522889114954e-05,
      "loss": 2.4626,
      "step": 30620
    },
    {
      "epoch": 15.579857578840285,
      "grad_norm": 33.26612854003906,
      "learning_rate": 3.442014242115972e-05,
      "loss": 2.4862,
      "step": 30630
    },
    {
      "epoch": 15.584944048830112,
      "grad_norm": 25.304903030395508,
      "learning_rate": 3.441505595116989e-05,
      "loss": 2.3503,
      "step": 30640
    },
    {
      "epoch": 15.590030518819939,
      "grad_norm": 31.03887939453125,
      "learning_rate": 3.440996948118006e-05,
      "loss": 2.5219,
      "step": 30650
    },
    {
      "epoch": 15.595116988809766,
      "grad_norm": 24.418779373168945,
      "learning_rate": 3.4404883011190234e-05,
      "loss": 2.4102,
      "step": 30660
    },
    {
      "epoch": 15.600203458799593,
      "grad_norm": 29.22603988647461,
      "learning_rate": 3.439979654120041e-05,
      "loss": 2.3927,
      "step": 30670
    },
    {
      "epoch": 15.60528992878942,
      "grad_norm": 26.531341552734375,
      "learning_rate": 3.439471007121059e-05,
      "loss": 2.4012,
      "step": 30680
    },
    {
      "epoch": 15.610376398779247,
      "grad_norm": 31.16507911682129,
      "learning_rate": 3.438962360122076e-05,
      "loss": 2.3856,
      "step": 30690
    },
    {
      "epoch": 15.615462868769074,
      "grad_norm": 25.83399772644043,
      "learning_rate": 3.438453713123093e-05,
      "loss": 2.3645,
      "step": 30700
    },
    {
      "epoch": 15.6205493387589,
      "grad_norm": 27.90756607055664,
      "learning_rate": 3.4379450661241103e-05,
      "loss": 2.4198,
      "step": 30710
    },
    {
      "epoch": 15.62563580874873,
      "grad_norm": 27.437475204467773,
      "learning_rate": 3.437436419125127e-05,
      "loss": 2.4014,
      "step": 30720
    },
    {
      "epoch": 15.630722278738556,
      "grad_norm": 23.415794372558594,
      "learning_rate": 3.436927772126144e-05,
      "loss": 2.428,
      "step": 30730
    },
    {
      "epoch": 15.635808748728383,
      "grad_norm": 25.982181549072266,
      "learning_rate": 3.436419125127162e-05,
      "loss": 2.4631,
      "step": 30740
    },
    {
      "epoch": 15.64089521871821,
      "grad_norm": 37.32500076293945,
      "learning_rate": 3.435910478128179e-05,
      "loss": 2.3124,
      "step": 30750
    },
    {
      "epoch": 15.645981688708037,
      "grad_norm": 25.20018768310547,
      "learning_rate": 3.4354018311291966e-05,
      "loss": 2.2915,
      "step": 30760
    },
    {
      "epoch": 15.651068158697864,
      "grad_norm": 30.41108512878418,
      "learning_rate": 3.434893184130214e-05,
      "loss": 2.3031,
      "step": 30770
    },
    {
      "epoch": 15.656154628687691,
      "grad_norm": 31.929975509643555,
      "learning_rate": 3.434384537131231e-05,
      "loss": 2.4092,
      "step": 30780
    },
    {
      "epoch": 15.661241098677518,
      "grad_norm": 28.850475311279297,
      "learning_rate": 3.433875890132248e-05,
      "loss": 2.4151,
      "step": 30790
    },
    {
      "epoch": 15.666327568667345,
      "grad_norm": 24.296274185180664,
      "learning_rate": 3.433367243133266e-05,
      "loss": 2.4295,
      "step": 30800
    },
    {
      "epoch": 15.671414038657172,
      "grad_norm": 38.114986419677734,
      "learning_rate": 3.432858596134283e-05,
      "loss": 2.3825,
      "step": 30810
    },
    {
      "epoch": 15.676500508646999,
      "grad_norm": 22.401029586791992,
      "learning_rate": 3.4323499491353e-05,
      "loss": 2.4897,
      "step": 30820
    },
    {
      "epoch": 15.681586978636826,
      "grad_norm": 33.06532287597656,
      "learning_rate": 3.4318413021363176e-05,
      "loss": 2.4543,
      "step": 30830
    },
    {
      "epoch": 15.686673448626653,
      "grad_norm": 22.92716407775879,
      "learning_rate": 3.4313326551373346e-05,
      "loss": 2.4975,
      "step": 30840
    },
    {
      "epoch": 15.69175991861648,
      "grad_norm": 26.450340270996094,
      "learning_rate": 3.430824008138352e-05,
      "loss": 2.3157,
      "step": 30850
    },
    {
      "epoch": 15.696846388606307,
      "grad_norm": 24.819982528686523,
      "learning_rate": 3.430315361139369e-05,
      "loss": 2.4595,
      "step": 30860
    },
    {
      "epoch": 15.701932858596134,
      "grad_norm": 28.940004348754883,
      "learning_rate": 3.429806714140387e-05,
      "loss": 2.3528,
      "step": 30870
    },
    {
      "epoch": 15.707019328585961,
      "grad_norm": 30.753219604492188,
      "learning_rate": 3.429298067141404e-05,
      "loss": 2.4419,
      "step": 30880
    },
    {
      "epoch": 15.712105798575788,
      "grad_norm": 29.216259002685547,
      "learning_rate": 3.4287894201424216e-05,
      "loss": 2.4122,
      "step": 30890
    },
    {
      "epoch": 15.717192268565615,
      "grad_norm": 30.27107048034668,
      "learning_rate": 3.4282807731434386e-05,
      "loss": 2.435,
      "step": 30900
    },
    {
      "epoch": 15.722278738555442,
      "grad_norm": 32.58068084716797,
      "learning_rate": 3.4277721261444555e-05,
      "loss": 2.2824,
      "step": 30910
    },
    {
      "epoch": 15.727365208545269,
      "grad_norm": 33.86595916748047,
      "learning_rate": 3.427263479145473e-05,
      "loss": 2.4212,
      "step": 30920
    },
    {
      "epoch": 15.732451678535096,
      "grad_norm": 28.050472259521484,
      "learning_rate": 3.42675483214649e-05,
      "loss": 2.381,
      "step": 30930
    },
    {
      "epoch": 15.737538148524923,
      "grad_norm": 29.662546157836914,
      "learning_rate": 3.426246185147508e-05,
      "loss": 2.4748,
      "step": 30940
    },
    {
      "epoch": 15.742624618514752,
      "grad_norm": 28.2562255859375,
      "learning_rate": 3.425737538148525e-05,
      "loss": 2.4579,
      "step": 30950
    },
    {
      "epoch": 15.747711088504579,
      "grad_norm": 26.767406463623047,
      "learning_rate": 3.4252288911495425e-05,
      "loss": 2.4582,
      "step": 30960
    },
    {
      "epoch": 15.752797558494406,
      "grad_norm": 26.769195556640625,
      "learning_rate": 3.42472024415056e-05,
      "loss": 2.3523,
      "step": 30970
    },
    {
      "epoch": 15.757884028484233,
      "grad_norm": 30.587478637695312,
      "learning_rate": 3.424211597151577e-05,
      "loss": 2.5143,
      "step": 30980
    },
    {
      "epoch": 15.76297049847406,
      "grad_norm": 23.928022384643555,
      "learning_rate": 3.423702950152594e-05,
      "loss": 2.4742,
      "step": 30990
    },
    {
      "epoch": 15.768056968463886,
      "grad_norm": 26.375259399414062,
      "learning_rate": 3.423194303153612e-05,
      "loss": 2.4586,
      "step": 31000
    },
    {
      "epoch": 15.773143438453713,
      "grad_norm": 23.305253982543945,
      "learning_rate": 3.422685656154629e-05,
      "loss": 2.4491,
      "step": 31010
    },
    {
      "epoch": 15.77822990844354,
      "grad_norm": 28.818376541137695,
      "learning_rate": 3.422177009155646e-05,
      "loss": 2.3768,
      "step": 31020
    },
    {
      "epoch": 15.783316378433367,
      "grad_norm": 24.27251625061035,
      "learning_rate": 3.4216683621566635e-05,
      "loss": 2.3999,
      "step": 31030
    },
    {
      "epoch": 15.788402848423194,
      "grad_norm": 31.44338607788086,
      "learning_rate": 3.4211597151576805e-05,
      "loss": 2.4298,
      "step": 31040
    },
    {
      "epoch": 15.793489318413021,
      "grad_norm": 30.051898956298828,
      "learning_rate": 3.420651068158698e-05,
      "loss": 2.4453,
      "step": 31050
    },
    {
      "epoch": 15.798575788402848,
      "grad_norm": 38.547279357910156,
      "learning_rate": 3.420142421159716e-05,
      "loss": 2.3851,
      "step": 31060
    },
    {
      "epoch": 15.803662258392675,
      "grad_norm": 27.47001075744629,
      "learning_rate": 3.419633774160733e-05,
      "loss": 2.4789,
      "step": 31070
    },
    {
      "epoch": 15.808748728382502,
      "grad_norm": 27.186321258544922,
      "learning_rate": 3.41912512716175e-05,
      "loss": 2.3888,
      "step": 31080
    },
    {
      "epoch": 15.81383519837233,
      "grad_norm": 32.439937591552734,
      "learning_rate": 3.4186164801627675e-05,
      "loss": 2.3584,
      "step": 31090
    },
    {
      "epoch": 15.818921668362156,
      "grad_norm": 29.352611541748047,
      "learning_rate": 3.4181078331637844e-05,
      "loss": 2.4908,
      "step": 31100
    },
    {
      "epoch": 15.824008138351983,
      "grad_norm": 34.6015739440918,
      "learning_rate": 3.4175991861648014e-05,
      "loss": 2.4205,
      "step": 31110
    },
    {
      "epoch": 15.82909460834181,
      "grad_norm": 27.835731506347656,
      "learning_rate": 3.417090539165819e-05,
      "loss": 2.415,
      "step": 31120
    },
    {
      "epoch": 15.834181078331637,
      "grad_norm": 26.03523826599121,
      "learning_rate": 3.416581892166836e-05,
      "loss": 2.4459,
      "step": 31130
    },
    {
      "epoch": 15.839267548321464,
      "grad_norm": 28.88224983215332,
      "learning_rate": 3.416073245167854e-05,
      "loss": 2.4196,
      "step": 31140
    },
    {
      "epoch": 15.844354018311291,
      "grad_norm": 23.436874389648438,
      "learning_rate": 3.4155645981688714e-05,
      "loss": 2.5722,
      "step": 31150
    },
    {
      "epoch": 15.84944048830112,
      "grad_norm": 34.84876251220703,
      "learning_rate": 3.4150559511698884e-05,
      "loss": 2.4233,
      "step": 31160
    },
    {
      "epoch": 15.854526958290947,
      "grad_norm": 30.592866897583008,
      "learning_rate": 3.4145473041709054e-05,
      "loss": 2.399,
      "step": 31170
    },
    {
      "epoch": 15.859613428280774,
      "grad_norm": 33.272891998291016,
      "learning_rate": 3.414038657171923e-05,
      "loss": 2.4301,
      "step": 31180
    },
    {
      "epoch": 15.864699898270601,
      "grad_norm": 28.590930938720703,
      "learning_rate": 3.41353001017294e-05,
      "loss": 2.4121,
      "step": 31190
    },
    {
      "epoch": 15.869786368260428,
      "grad_norm": 32.228572845458984,
      "learning_rate": 3.413021363173958e-05,
      "loss": 2.344,
      "step": 31200
    },
    {
      "epoch": 15.874872838250255,
      "grad_norm": 31.404300689697266,
      "learning_rate": 3.412512716174975e-05,
      "loss": 2.4631,
      "step": 31210
    },
    {
      "epoch": 15.879959308240082,
      "grad_norm": 28.464115142822266,
      "learning_rate": 3.412004069175992e-05,
      "loss": 2.4213,
      "step": 31220
    },
    {
      "epoch": 15.885045778229909,
      "grad_norm": 32.526065826416016,
      "learning_rate": 3.4114954221770094e-05,
      "loss": 2.4465,
      "step": 31230
    },
    {
      "epoch": 15.890132248219736,
      "grad_norm": 30.089574813842773,
      "learning_rate": 3.4109867751780264e-05,
      "loss": 2.3609,
      "step": 31240
    },
    {
      "epoch": 15.895218718209563,
      "grad_norm": 23.682910919189453,
      "learning_rate": 3.410478128179044e-05,
      "loss": 2.4689,
      "step": 31250
    },
    {
      "epoch": 15.90030518819939,
      "grad_norm": 44.10221862792969,
      "learning_rate": 3.409969481180062e-05,
      "loss": 2.3288,
      "step": 31260
    },
    {
      "epoch": 15.905391658189217,
      "grad_norm": 24.30685043334961,
      "learning_rate": 3.409460834181079e-05,
      "loss": 2.4064,
      "step": 31270
    },
    {
      "epoch": 15.910478128179044,
      "grad_norm": 27.962512969970703,
      "learning_rate": 3.408952187182096e-05,
      "loss": 2.3503,
      "step": 31280
    },
    {
      "epoch": 15.91556459816887,
      "grad_norm": 26.868844985961914,
      "learning_rate": 3.4084435401831133e-05,
      "loss": 2.3721,
      "step": 31290
    },
    {
      "epoch": 15.920651068158698,
      "grad_norm": 26.046432495117188,
      "learning_rate": 3.40793489318413e-05,
      "loss": 2.4623,
      "step": 31300
    },
    {
      "epoch": 15.925737538148525,
      "grad_norm": 27.54381561279297,
      "learning_rate": 3.407426246185147e-05,
      "loss": 2.4968,
      "step": 31310
    },
    {
      "epoch": 15.930824008138352,
      "grad_norm": 31.049455642700195,
      "learning_rate": 3.406917599186165e-05,
      "loss": 2.3686,
      "step": 31320
    },
    {
      "epoch": 15.935910478128179,
      "grad_norm": 29.436620712280273,
      "learning_rate": 3.406408952187182e-05,
      "loss": 2.4184,
      "step": 31330
    },
    {
      "epoch": 15.940996948118006,
      "grad_norm": 21.23460578918457,
      "learning_rate": 3.4059003051881996e-05,
      "loss": 2.359,
      "step": 31340
    },
    {
      "epoch": 15.946083418107833,
      "grad_norm": 23.112382888793945,
      "learning_rate": 3.405391658189217e-05,
      "loss": 2.4799,
      "step": 31350
    },
    {
      "epoch": 15.95116988809766,
      "grad_norm": 29.1729679107666,
      "learning_rate": 3.404883011190234e-05,
      "loss": 2.4543,
      "step": 31360
    },
    {
      "epoch": 15.956256358087487,
      "grad_norm": 33.65755081176758,
      "learning_rate": 3.404374364191251e-05,
      "loss": 2.4355,
      "step": 31370
    },
    {
      "epoch": 15.961342828077314,
      "grad_norm": 23.561235427856445,
      "learning_rate": 3.403865717192269e-05,
      "loss": 2.4345,
      "step": 31380
    },
    {
      "epoch": 15.96642929806714,
      "grad_norm": 27.37887954711914,
      "learning_rate": 3.403357070193286e-05,
      "loss": 2.4484,
      "step": 31390
    },
    {
      "epoch": 15.97151576805697,
      "grad_norm": 32.03459167480469,
      "learning_rate": 3.402848423194303e-05,
      "loss": 2.4155,
      "step": 31400
    },
    {
      "epoch": 15.976602238046796,
      "grad_norm": 25.052480697631836,
      "learning_rate": 3.4023397761953206e-05,
      "loss": 2.429,
      "step": 31410
    },
    {
      "epoch": 15.981688708036623,
      "grad_norm": 30.258712768554688,
      "learning_rate": 3.4018311291963376e-05,
      "loss": 2.3638,
      "step": 31420
    },
    {
      "epoch": 15.98677517802645,
      "grad_norm": 28.074312210083008,
      "learning_rate": 3.401322482197355e-05,
      "loss": 2.3971,
      "step": 31430
    },
    {
      "epoch": 15.991861648016277,
      "grad_norm": 28.846323013305664,
      "learning_rate": 3.400813835198373e-05,
      "loss": 2.3925,
      "step": 31440
    },
    {
      "epoch": 15.996948118006104,
      "grad_norm": 27.256181716918945,
      "learning_rate": 3.40030518819939e-05,
      "loss": 2.3654,
      "step": 31450
    },
    {
      "epoch": 16.0,
      "eval_loss": 4.1159749031066895,
      "eval_runtime": 2.7507,
      "eval_samples_per_second": 1008.822,
      "eval_steps_per_second": 126.148,
      "step": 31456
    },
    {
      "epoch": 16.00203458799593,
      "grad_norm": 23.490816116333008,
      "learning_rate": 3.399796541200407e-05,
      "loss": 2.4361,
      "step": 31460
    },
    {
      "epoch": 16.007121057985756,
      "grad_norm": 29.566211700439453,
      "learning_rate": 3.3992878942014246e-05,
      "loss": 2.365,
      "step": 31470
    },
    {
      "epoch": 16.012207527975583,
      "grad_norm": 27.869770050048828,
      "learning_rate": 3.3987792472024416e-05,
      "loss": 2.4495,
      "step": 31480
    },
    {
      "epoch": 16.01729399796541,
      "grad_norm": 30.988155364990234,
      "learning_rate": 3.398270600203459e-05,
      "loss": 2.4715,
      "step": 31490
    },
    {
      "epoch": 16.022380467955237,
      "grad_norm": 24.733678817749023,
      "learning_rate": 3.397761953204476e-05,
      "loss": 2.3813,
      "step": 31500
    },
    {
      "epoch": 16.027466937945068,
      "grad_norm": 31.451393127441406,
      "learning_rate": 3.397253306205493e-05,
      "loss": 2.405,
      "step": 31510
    },
    {
      "epoch": 16.032553407934895,
      "grad_norm": 27.067258834838867,
      "learning_rate": 3.396744659206511e-05,
      "loss": 2.3927,
      "step": 31520
    },
    {
      "epoch": 16.037639877924722,
      "grad_norm": 24.857393264770508,
      "learning_rate": 3.396236012207528e-05,
      "loss": 2.4322,
      "step": 31530
    },
    {
      "epoch": 16.04272634791455,
      "grad_norm": 26.162403106689453,
      "learning_rate": 3.3957273652085455e-05,
      "loss": 2.3706,
      "step": 31540
    },
    {
      "epoch": 16.047812817904376,
      "grad_norm": 28.590129852294922,
      "learning_rate": 3.395218718209563e-05,
      "loss": 2.4444,
      "step": 31550
    },
    {
      "epoch": 16.052899287894203,
      "grad_norm": 26.048927307128906,
      "learning_rate": 3.39471007121058e-05,
      "loss": 2.3894,
      "step": 31560
    },
    {
      "epoch": 16.05798575788403,
      "grad_norm": 24.188467025756836,
      "learning_rate": 3.394201424211597e-05,
      "loss": 2.414,
      "step": 31570
    },
    {
      "epoch": 16.063072227873857,
      "grad_norm": 28.443204879760742,
      "learning_rate": 3.393692777212615e-05,
      "loss": 2.3326,
      "step": 31580
    },
    {
      "epoch": 16.068158697863684,
      "grad_norm": 18.755041122436523,
      "learning_rate": 3.393184130213632e-05,
      "loss": 2.4386,
      "step": 31590
    },
    {
      "epoch": 16.07324516785351,
      "grad_norm": 27.56755828857422,
      "learning_rate": 3.392675483214649e-05,
      "loss": 2.3262,
      "step": 31600
    },
    {
      "epoch": 16.078331637843338,
      "grad_norm": 32.09584045410156,
      "learning_rate": 3.3921668362156665e-05,
      "loss": 2.3893,
      "step": 31610
    },
    {
      "epoch": 16.083418107833165,
      "grad_norm": 27.84404182434082,
      "learning_rate": 3.3916581892166835e-05,
      "loss": 2.3997,
      "step": 31620
    },
    {
      "epoch": 16.08850457782299,
      "grad_norm": 30.983633041381836,
      "learning_rate": 3.391149542217701e-05,
      "loss": 2.3645,
      "step": 31630
    },
    {
      "epoch": 16.09359104781282,
      "grad_norm": 35.18622970581055,
      "learning_rate": 3.390640895218719e-05,
      "loss": 2.341,
      "step": 31640
    },
    {
      "epoch": 16.098677517802646,
      "grad_norm": 24.96799659729004,
      "learning_rate": 3.390132248219736e-05,
      "loss": 2.3777,
      "step": 31650
    },
    {
      "epoch": 16.103763987792473,
      "grad_norm": 39.0202522277832,
      "learning_rate": 3.389623601220753e-05,
      "loss": 2.3622,
      "step": 31660
    },
    {
      "epoch": 16.1088504577823,
      "grad_norm": 28.09933853149414,
      "learning_rate": 3.3891149542217705e-05,
      "loss": 2.336,
      "step": 31670
    },
    {
      "epoch": 16.113936927772126,
      "grad_norm": 34.910648345947266,
      "learning_rate": 3.3886063072227874e-05,
      "loss": 2.467,
      "step": 31680
    },
    {
      "epoch": 16.119023397761953,
      "grad_norm": 36.972740173339844,
      "learning_rate": 3.3880976602238044e-05,
      "loss": 2.2923,
      "step": 31690
    },
    {
      "epoch": 16.12410986775178,
      "grad_norm": 26.35561752319336,
      "learning_rate": 3.387589013224822e-05,
      "loss": 2.4889,
      "step": 31700
    },
    {
      "epoch": 16.129196337741607,
      "grad_norm": 26.10279655456543,
      "learning_rate": 3.387080366225839e-05,
      "loss": 2.3911,
      "step": 31710
    },
    {
      "epoch": 16.134282807731434,
      "grad_norm": 31.728649139404297,
      "learning_rate": 3.386571719226857e-05,
      "loss": 2.333,
      "step": 31720
    },
    {
      "epoch": 16.13936927772126,
      "grad_norm": 28.975738525390625,
      "learning_rate": 3.3860630722278744e-05,
      "loss": 2.3494,
      "step": 31730
    },
    {
      "epoch": 16.14445574771109,
      "grad_norm": 29.469301223754883,
      "learning_rate": 3.3855544252288914e-05,
      "loss": 2.4078,
      "step": 31740
    },
    {
      "epoch": 16.149542217700915,
      "grad_norm": 23.393041610717773,
      "learning_rate": 3.385045778229909e-05,
      "loss": 2.3721,
      "step": 31750
    },
    {
      "epoch": 16.154628687690742,
      "grad_norm": 26.023971557617188,
      "learning_rate": 3.384537131230926e-05,
      "loss": 2.3439,
      "step": 31760
    },
    {
      "epoch": 16.15971515768057,
      "grad_norm": 30.57189178466797,
      "learning_rate": 3.384028484231943e-05,
      "loss": 2.4677,
      "step": 31770
    },
    {
      "epoch": 16.164801627670396,
      "grad_norm": 24.336353302001953,
      "learning_rate": 3.383519837232961e-05,
      "loss": 2.4427,
      "step": 31780
    },
    {
      "epoch": 16.169888097660223,
      "grad_norm": 31.421329498291016,
      "learning_rate": 3.383011190233978e-05,
      "loss": 2.3992,
      "step": 31790
    },
    {
      "epoch": 16.17497456765005,
      "grad_norm": 23.134923934936523,
      "learning_rate": 3.382502543234995e-05,
      "loss": 2.4038,
      "step": 31800
    },
    {
      "epoch": 16.180061037639877,
      "grad_norm": 27.894229888916016,
      "learning_rate": 3.3819938962360124e-05,
      "loss": 2.3768,
      "step": 31810
    },
    {
      "epoch": 16.185147507629704,
      "grad_norm": 26.71550178527832,
      "learning_rate": 3.3814852492370294e-05,
      "loss": 2.4316,
      "step": 31820
    },
    {
      "epoch": 16.19023397761953,
      "grad_norm": 28.839994430541992,
      "learning_rate": 3.380976602238047e-05,
      "loss": 2.3585,
      "step": 31830
    },
    {
      "epoch": 16.195320447609358,
      "grad_norm": 29.288501739501953,
      "learning_rate": 3.380467955239065e-05,
      "loss": 2.3134,
      "step": 31840
    },
    {
      "epoch": 16.200406917599185,
      "grad_norm": 33.99223327636719,
      "learning_rate": 3.379959308240082e-05,
      "loss": 2.3459,
      "step": 31850
    },
    {
      "epoch": 16.205493387589012,
      "grad_norm": 28.246015548706055,
      "learning_rate": 3.379450661241099e-05,
      "loss": 2.305,
      "step": 31860
    },
    {
      "epoch": 16.21057985757884,
      "grad_norm": 25.544574737548828,
      "learning_rate": 3.3789420142421163e-05,
      "loss": 2.3838,
      "step": 31870
    },
    {
      "epoch": 16.215666327568666,
      "grad_norm": 33.828548431396484,
      "learning_rate": 3.378433367243133e-05,
      "loss": 2.3536,
      "step": 31880
    },
    {
      "epoch": 16.220752797558493,
      "grad_norm": 34.01821517944336,
      "learning_rate": 3.37792472024415e-05,
      "loss": 2.356,
      "step": 31890
    },
    {
      "epoch": 16.22583926754832,
      "grad_norm": 28.22464370727539,
      "learning_rate": 3.377416073245168e-05,
      "loss": 2.3808,
      "step": 31900
    },
    {
      "epoch": 16.230925737538147,
      "grad_norm": 27.029565811157227,
      "learning_rate": 3.376907426246185e-05,
      "loss": 2.4091,
      "step": 31910
    },
    {
      "epoch": 16.236012207527974,
      "grad_norm": 32.504371643066406,
      "learning_rate": 3.3763987792472026e-05,
      "loss": 2.3826,
      "step": 31920
    },
    {
      "epoch": 16.2410986775178,
      "grad_norm": 30.67244529724121,
      "learning_rate": 3.37589013224822e-05,
      "loss": 2.381,
      "step": 31930
    },
    {
      "epoch": 16.246185147507628,
      "grad_norm": 24.54241180419922,
      "learning_rate": 3.375381485249237e-05,
      "loss": 2.2935,
      "step": 31940
    },
    {
      "epoch": 16.25127161749746,
      "grad_norm": 23.807384490966797,
      "learning_rate": 3.374872838250254e-05,
      "loss": 2.348,
      "step": 31950
    },
    {
      "epoch": 16.256358087487286,
      "grad_norm": 30.516469955444336,
      "learning_rate": 3.374364191251272e-05,
      "loss": 2.3694,
      "step": 31960
    },
    {
      "epoch": 16.261444557477112,
      "grad_norm": 27.803693771362305,
      "learning_rate": 3.373855544252289e-05,
      "loss": 2.2754,
      "step": 31970
    },
    {
      "epoch": 16.26653102746694,
      "grad_norm": 26.559810638427734,
      "learning_rate": 3.373346897253306e-05,
      "loss": 2.3682,
      "step": 31980
    },
    {
      "epoch": 16.271617497456766,
      "grad_norm": 25.536052703857422,
      "learning_rate": 3.3728382502543236e-05,
      "loss": 2.3009,
      "step": 31990
    },
    {
      "epoch": 16.276703967446593,
      "grad_norm": 32.54351043701172,
      "learning_rate": 3.3723296032553406e-05,
      "loss": 2.3667,
      "step": 32000
    },
    {
      "epoch": 16.28179043743642,
      "grad_norm": 29.386262893676758,
      "learning_rate": 3.371820956256358e-05,
      "loss": 2.4011,
      "step": 32010
    },
    {
      "epoch": 16.286876907426247,
      "grad_norm": 33.29239273071289,
      "learning_rate": 3.371312309257376e-05,
      "loss": 2.3409,
      "step": 32020
    },
    {
      "epoch": 16.291963377416074,
      "grad_norm": 21.835174560546875,
      "learning_rate": 3.370803662258393e-05,
      "loss": 2.2818,
      "step": 32030
    },
    {
      "epoch": 16.2970498474059,
      "grad_norm": 38.16487503051758,
      "learning_rate": 3.3702950152594106e-05,
      "loss": 2.2851,
      "step": 32040
    },
    {
      "epoch": 16.30213631739573,
      "grad_norm": 27.72134017944336,
      "learning_rate": 3.3697863682604276e-05,
      "loss": 2.345,
      "step": 32050
    },
    {
      "epoch": 16.307222787385555,
      "grad_norm": 25.056415557861328,
      "learning_rate": 3.3692777212614446e-05,
      "loss": 2.4063,
      "step": 32060
    },
    {
      "epoch": 16.312309257375382,
      "grad_norm": 33.63447570800781,
      "learning_rate": 3.368769074262462e-05,
      "loss": 2.3886,
      "step": 32070
    },
    {
      "epoch": 16.31739572736521,
      "grad_norm": 32.44703674316406,
      "learning_rate": 3.368260427263479e-05,
      "loss": 2.3712,
      "step": 32080
    },
    {
      "epoch": 16.322482197355036,
      "grad_norm": 26.633419036865234,
      "learning_rate": 3.367751780264496e-05,
      "loss": 2.3953,
      "step": 32090
    },
    {
      "epoch": 16.327568667344863,
      "grad_norm": 29.758991241455078,
      "learning_rate": 3.367243133265514e-05,
      "loss": 2.3481,
      "step": 32100
    },
    {
      "epoch": 16.33265513733469,
      "grad_norm": 30.32774543762207,
      "learning_rate": 3.3667344862665315e-05,
      "loss": 2.3034,
      "step": 32110
    },
    {
      "epoch": 16.337741607324517,
      "grad_norm": 26.976181030273438,
      "learning_rate": 3.3662258392675485e-05,
      "loss": 2.388,
      "step": 32120
    },
    {
      "epoch": 16.342828077314344,
      "grad_norm": 23.389347076416016,
      "learning_rate": 3.365717192268566e-05,
      "loss": 2.3682,
      "step": 32130
    },
    {
      "epoch": 16.34791454730417,
      "grad_norm": 35.31315612792969,
      "learning_rate": 3.365208545269583e-05,
      "loss": 2.3643,
      "step": 32140
    },
    {
      "epoch": 16.353001017293998,
      "grad_norm": 30.54989242553711,
      "learning_rate": 3.3646998982706e-05,
      "loss": 2.3977,
      "step": 32150
    },
    {
      "epoch": 16.358087487283825,
      "grad_norm": 29.814645767211914,
      "learning_rate": 3.364191251271618e-05,
      "loss": 2.3481,
      "step": 32160
    },
    {
      "epoch": 16.363173957273652,
      "grad_norm": 27.392732620239258,
      "learning_rate": 3.363682604272635e-05,
      "loss": 2.4356,
      "step": 32170
    },
    {
      "epoch": 16.36826042726348,
      "grad_norm": 25.54619598388672,
      "learning_rate": 3.363173957273652e-05,
      "loss": 2.4933,
      "step": 32180
    },
    {
      "epoch": 16.373346897253306,
      "grad_norm": 23.38511848449707,
      "learning_rate": 3.3626653102746695e-05,
      "loss": 2.2549,
      "step": 32190
    },
    {
      "epoch": 16.378433367243133,
      "grad_norm": 30.621410369873047,
      "learning_rate": 3.3621566632756865e-05,
      "loss": 2.3062,
      "step": 32200
    },
    {
      "epoch": 16.38351983723296,
      "grad_norm": 31.244274139404297,
      "learning_rate": 3.361648016276704e-05,
      "loss": 2.3456,
      "step": 32210
    },
    {
      "epoch": 16.388606307222787,
      "grad_norm": 29.475635528564453,
      "learning_rate": 3.361139369277722e-05,
      "loss": 2.4037,
      "step": 32220
    },
    {
      "epoch": 16.393692777212614,
      "grad_norm": 35.06958770751953,
      "learning_rate": 3.360630722278739e-05,
      "loss": 2.3512,
      "step": 32230
    },
    {
      "epoch": 16.39877924720244,
      "grad_norm": 27.498762130737305,
      "learning_rate": 3.360122075279756e-05,
      "loss": 2.3206,
      "step": 32240
    },
    {
      "epoch": 16.403865717192268,
      "grad_norm": 26.849035263061523,
      "learning_rate": 3.3596134282807735e-05,
      "loss": 2.3499,
      "step": 32250
    },
    {
      "epoch": 16.408952187182095,
      "grad_norm": 26.714111328125,
      "learning_rate": 3.3591047812817904e-05,
      "loss": 2.4079,
      "step": 32260
    },
    {
      "epoch": 16.414038657171922,
      "grad_norm": 26.61793327331543,
      "learning_rate": 3.3585961342828074e-05,
      "loss": 2.4116,
      "step": 32270
    },
    {
      "epoch": 16.41912512716175,
      "grad_norm": 26.18073844909668,
      "learning_rate": 3.358087487283825e-05,
      "loss": 2.2421,
      "step": 32280
    },
    {
      "epoch": 16.424211597151576,
      "grad_norm": 33.38957214355469,
      "learning_rate": 3.357578840284842e-05,
      "loss": 2.3531,
      "step": 32290
    },
    {
      "epoch": 16.429298067141403,
      "grad_norm": 30.237667083740234,
      "learning_rate": 3.35707019328586e-05,
      "loss": 2.3762,
      "step": 32300
    },
    {
      "epoch": 16.43438453713123,
      "grad_norm": 29.680208206176758,
      "learning_rate": 3.3565615462868774e-05,
      "loss": 2.3804,
      "step": 32310
    },
    {
      "epoch": 16.439471007121057,
      "grad_norm": 32.35532760620117,
      "learning_rate": 3.3560528992878944e-05,
      "loss": 2.3597,
      "step": 32320
    },
    {
      "epoch": 16.444557477110884,
      "grad_norm": 23.201425552368164,
      "learning_rate": 3.355544252288912e-05,
      "loss": 2.3071,
      "step": 32330
    },
    {
      "epoch": 16.44964394710071,
      "grad_norm": 34.57734298706055,
      "learning_rate": 3.355035605289929e-05,
      "loss": 2.3529,
      "step": 32340
    },
    {
      "epoch": 16.454730417090538,
      "grad_norm": 32.46772766113281,
      "learning_rate": 3.354526958290946e-05,
      "loss": 2.3283,
      "step": 32350
    },
    {
      "epoch": 16.459816887080365,
      "grad_norm": 25.83855438232422,
      "learning_rate": 3.354018311291964e-05,
      "loss": 2.3308,
      "step": 32360
    },
    {
      "epoch": 16.46490335707019,
      "grad_norm": 28.510780334472656,
      "learning_rate": 3.353509664292981e-05,
      "loss": 2.2867,
      "step": 32370
    },
    {
      "epoch": 16.46998982706002,
      "grad_norm": 34.2449836730957,
      "learning_rate": 3.353001017293998e-05,
      "loss": 2.5139,
      "step": 32380
    },
    {
      "epoch": 16.475076297049846,
      "grad_norm": 24.0694637298584,
      "learning_rate": 3.3524923702950154e-05,
      "loss": 2.4807,
      "step": 32390
    },
    {
      "epoch": 16.480162767039676,
      "grad_norm": 22.627628326416016,
      "learning_rate": 3.351983723296033e-05,
      "loss": 2.442,
      "step": 32400
    },
    {
      "epoch": 16.485249237029503,
      "grad_norm": 27.534093856811523,
      "learning_rate": 3.35147507629705e-05,
      "loss": 2.3345,
      "step": 32410
    },
    {
      "epoch": 16.49033570701933,
      "grad_norm": 28.45012092590332,
      "learning_rate": 3.350966429298068e-05,
      "loss": 2.3686,
      "step": 32420
    },
    {
      "epoch": 16.495422177009157,
      "grad_norm": 25.949726104736328,
      "learning_rate": 3.350457782299085e-05,
      "loss": 2.3835,
      "step": 32430
    },
    {
      "epoch": 16.500508646998984,
      "grad_norm": 25.319473266601562,
      "learning_rate": 3.349949135300102e-05,
      "loss": 2.4386,
      "step": 32440
    },
    {
      "epoch": 16.50559511698881,
      "grad_norm": 29.783004760742188,
      "learning_rate": 3.3494404883011193e-05,
      "loss": 2.3325,
      "step": 32450
    },
    {
      "epoch": 16.510681586978638,
      "grad_norm": 26.090999603271484,
      "learning_rate": 3.348931841302136e-05,
      "loss": 2.3744,
      "step": 32460
    },
    {
      "epoch": 16.515768056968465,
      "grad_norm": 26.60296630859375,
      "learning_rate": 3.348423194303153e-05,
      "loss": 2.3814,
      "step": 32470
    },
    {
      "epoch": 16.520854526958292,
      "grad_norm": 30.87086296081543,
      "learning_rate": 3.347914547304171e-05,
      "loss": 2.3993,
      "step": 32480
    },
    {
      "epoch": 16.52594099694812,
      "grad_norm": 31.433185577392578,
      "learning_rate": 3.347405900305188e-05,
      "loss": 2.3751,
      "step": 32490
    },
    {
      "epoch": 16.531027466937946,
      "grad_norm": 26.744369506835938,
      "learning_rate": 3.3468972533062056e-05,
      "loss": 2.3819,
      "step": 32500
    },
    {
      "epoch": 16.536113936927773,
      "grad_norm": 29.236204147338867,
      "learning_rate": 3.346388606307223e-05,
      "loss": 2.3026,
      "step": 32510
    },
    {
      "epoch": 16.5412004069176,
      "grad_norm": 25.616411209106445,
      "learning_rate": 3.34587995930824e-05,
      "loss": 2.3601,
      "step": 32520
    },
    {
      "epoch": 16.546286876907427,
      "grad_norm": 31.30562973022461,
      "learning_rate": 3.345371312309257e-05,
      "loss": 2.3634,
      "step": 32530
    },
    {
      "epoch": 16.551373346897254,
      "grad_norm": 23.983177185058594,
      "learning_rate": 3.344862665310275e-05,
      "loss": 2.2855,
      "step": 32540
    },
    {
      "epoch": 16.55645981688708,
      "grad_norm": 27.435163497924805,
      "learning_rate": 3.344354018311292e-05,
      "loss": 2.3611,
      "step": 32550
    },
    {
      "epoch": 16.561546286876908,
      "grad_norm": 32.481651306152344,
      "learning_rate": 3.3438453713123096e-05,
      "loss": 2.3292,
      "step": 32560
    },
    {
      "epoch": 16.566632756866735,
      "grad_norm": 27.705034255981445,
      "learning_rate": 3.3433367243133266e-05,
      "loss": 2.3822,
      "step": 32570
    },
    {
      "epoch": 16.571719226856562,
      "grad_norm": 27.235645294189453,
      "learning_rate": 3.3428280773143436e-05,
      "loss": 2.4611,
      "step": 32580
    },
    {
      "epoch": 16.57680569684639,
      "grad_norm": 27.208314895629883,
      "learning_rate": 3.342319430315361e-05,
      "loss": 2.3492,
      "step": 32590
    },
    {
      "epoch": 16.581892166836216,
      "grad_norm": 28.892620086669922,
      "learning_rate": 3.341810783316379e-05,
      "loss": 2.407,
      "step": 32600
    },
    {
      "epoch": 16.586978636826043,
      "grad_norm": 25.266483306884766,
      "learning_rate": 3.341302136317396e-05,
      "loss": 2.4196,
      "step": 32610
    },
    {
      "epoch": 16.59206510681587,
      "grad_norm": 25.820701599121094,
      "learning_rate": 3.3407934893184136e-05,
      "loss": 2.3457,
      "step": 32620
    },
    {
      "epoch": 16.597151576805697,
      "grad_norm": 26.79338836669922,
      "learning_rate": 3.3402848423194306e-05,
      "loss": 2.4505,
      "step": 32630
    },
    {
      "epoch": 16.602238046795524,
      "grad_norm": 28.041624069213867,
      "learning_rate": 3.3397761953204476e-05,
      "loss": 2.3648,
      "step": 32640
    },
    {
      "epoch": 16.60732451678535,
      "grad_norm": 28.12540054321289,
      "learning_rate": 3.339267548321465e-05,
      "loss": 2.3511,
      "step": 32650
    },
    {
      "epoch": 16.612410986775178,
      "grad_norm": 30.25236701965332,
      "learning_rate": 3.338758901322482e-05,
      "loss": 2.3695,
      "step": 32660
    },
    {
      "epoch": 16.617497456765005,
      "grad_norm": 38.220703125,
      "learning_rate": 3.338250254323499e-05,
      "loss": 2.2998,
      "step": 32670
    },
    {
      "epoch": 16.62258392675483,
      "grad_norm": 30.08182716369629,
      "learning_rate": 3.337741607324517e-05,
      "loss": 2.3818,
      "step": 32680
    },
    {
      "epoch": 16.62767039674466,
      "grad_norm": 32.78841018676758,
      "learning_rate": 3.3372329603255345e-05,
      "loss": 2.3235,
      "step": 32690
    },
    {
      "epoch": 16.632756866734486,
      "grad_norm": 28.198331832885742,
      "learning_rate": 3.3367243133265515e-05,
      "loss": 2.416,
      "step": 32700
    },
    {
      "epoch": 16.637843336724313,
      "grad_norm": 25.396808624267578,
      "learning_rate": 3.336215666327569e-05,
      "loss": 2.4742,
      "step": 32710
    },
    {
      "epoch": 16.64292980671414,
      "grad_norm": 32.33207321166992,
      "learning_rate": 3.335707019328586e-05,
      "loss": 2.3092,
      "step": 32720
    },
    {
      "epoch": 16.648016276703967,
      "grad_norm": 33.459346771240234,
      "learning_rate": 3.335198372329603e-05,
      "loss": 2.2422,
      "step": 32730
    },
    {
      "epoch": 16.653102746693794,
      "grad_norm": 24.00994300842285,
      "learning_rate": 3.334689725330621e-05,
      "loss": 2.3597,
      "step": 32740
    },
    {
      "epoch": 16.65818921668362,
      "grad_norm": 39.3448371887207,
      "learning_rate": 3.334181078331638e-05,
      "loss": 2.4174,
      "step": 32750
    },
    {
      "epoch": 16.663275686673447,
      "grad_norm": 26.049091339111328,
      "learning_rate": 3.333672431332655e-05,
      "loss": 2.3249,
      "step": 32760
    },
    {
      "epoch": 16.668362156663274,
      "grad_norm": 32.24024200439453,
      "learning_rate": 3.3331637843336725e-05,
      "loss": 2.3201,
      "step": 32770
    },
    {
      "epoch": 16.6734486266531,
      "grad_norm": 34.032676696777344,
      "learning_rate": 3.3326551373346895e-05,
      "loss": 2.4046,
      "step": 32780
    },
    {
      "epoch": 16.67853509664293,
      "grad_norm": 32.76310348510742,
      "learning_rate": 3.332146490335707e-05,
      "loss": 2.3852,
      "step": 32790
    },
    {
      "epoch": 16.683621566632755,
      "grad_norm": 34.209693908691406,
      "learning_rate": 3.331637843336725e-05,
      "loss": 2.3683,
      "step": 32800
    },
    {
      "epoch": 16.688708036622582,
      "grad_norm": 22.699201583862305,
      "learning_rate": 3.331129196337742e-05,
      "loss": 2.2955,
      "step": 32810
    },
    {
      "epoch": 16.69379450661241,
      "grad_norm": 25.891401290893555,
      "learning_rate": 3.3306205493387595e-05,
      "loss": 2.347,
      "step": 32820
    },
    {
      "epoch": 16.698880976602236,
      "grad_norm": 24.86750030517578,
      "learning_rate": 3.3301119023397765e-05,
      "loss": 2.3744,
      "step": 32830
    },
    {
      "epoch": 16.703967446592067,
      "grad_norm": 22.687108993530273,
      "learning_rate": 3.3296032553407934e-05,
      "loss": 2.3074,
      "step": 32840
    },
    {
      "epoch": 16.709053916581894,
      "grad_norm": 26.96078872680664,
      "learning_rate": 3.329094608341811e-05,
      "loss": 2.3684,
      "step": 32850
    },
    {
      "epoch": 16.71414038657172,
      "grad_norm": 27.411170959472656,
      "learning_rate": 3.328585961342828e-05,
      "loss": 2.35,
      "step": 32860
    },
    {
      "epoch": 16.719226856561548,
      "grad_norm": 31.12860870361328,
      "learning_rate": 3.328077314343845e-05,
      "loss": 2.294,
      "step": 32870
    },
    {
      "epoch": 16.724313326551375,
      "grad_norm": 29.291492462158203,
      "learning_rate": 3.327568667344863e-05,
      "loss": 2.3336,
      "step": 32880
    },
    {
      "epoch": 16.729399796541202,
      "grad_norm": 26.924163818359375,
      "learning_rate": 3.3270600203458804e-05,
      "loss": 2.352,
      "step": 32890
    },
    {
      "epoch": 16.73448626653103,
      "grad_norm": 43.2659912109375,
      "learning_rate": 3.3265513733468974e-05,
      "loss": 2.3514,
      "step": 32900
    },
    {
      "epoch": 16.739572736520856,
      "grad_norm": 31.062894821166992,
      "learning_rate": 3.326042726347915e-05,
      "loss": 2.3279,
      "step": 32910
    },
    {
      "epoch": 16.744659206510683,
      "grad_norm": 34.14672088623047,
      "learning_rate": 3.325534079348932e-05,
      "loss": 2.3058,
      "step": 32920
    },
    {
      "epoch": 16.74974567650051,
      "grad_norm": 31.70940589904785,
      "learning_rate": 3.325025432349949e-05,
      "loss": 2.3454,
      "step": 32930
    },
    {
      "epoch": 16.754832146490337,
      "grad_norm": 31.435640335083008,
      "learning_rate": 3.324516785350967e-05,
      "loss": 2.3726,
      "step": 32940
    },
    {
      "epoch": 16.759918616480164,
      "grad_norm": 28.926559448242188,
      "learning_rate": 3.324008138351984e-05,
      "loss": 2.3218,
      "step": 32950
    },
    {
      "epoch": 16.76500508646999,
      "grad_norm": 23.42789077758789,
      "learning_rate": 3.323499491353001e-05,
      "loss": 2.3858,
      "step": 32960
    },
    {
      "epoch": 16.770091556459818,
      "grad_norm": 29.057769775390625,
      "learning_rate": 3.3229908443540184e-05,
      "loss": 2.3626,
      "step": 32970
    },
    {
      "epoch": 16.775178026449645,
      "grad_norm": 34.20114517211914,
      "learning_rate": 3.322482197355036e-05,
      "loss": 2.3408,
      "step": 32980
    },
    {
      "epoch": 16.78026449643947,
      "grad_norm": 32.73015213012695,
      "learning_rate": 3.321973550356053e-05,
      "loss": 2.2636,
      "step": 32990
    },
    {
      "epoch": 16.7853509664293,
      "grad_norm": 25.841144561767578,
      "learning_rate": 3.321464903357071e-05,
      "loss": 2.3198,
      "step": 33000
    },
    {
      "epoch": 16.790437436419126,
      "grad_norm": 32.88256072998047,
      "learning_rate": 3.320956256358088e-05,
      "loss": 2.4278,
      "step": 33010
    },
    {
      "epoch": 16.795523906408953,
      "grad_norm": 27.485305786132812,
      "learning_rate": 3.320447609359105e-05,
      "loss": 2.3964,
      "step": 33020
    },
    {
      "epoch": 16.80061037639878,
      "grad_norm": 27.73738670349121,
      "learning_rate": 3.3199389623601223e-05,
      "loss": 2.3741,
      "step": 33030
    },
    {
      "epoch": 16.805696846388607,
      "grad_norm": 24.21678924560547,
      "learning_rate": 3.319430315361139e-05,
      "loss": 2.3655,
      "step": 33040
    },
    {
      "epoch": 16.810783316378433,
      "grad_norm": 27.104482650756836,
      "learning_rate": 3.318921668362156e-05,
      "loss": 2.305,
      "step": 33050
    },
    {
      "epoch": 16.81586978636826,
      "grad_norm": 31.60257339477539,
      "learning_rate": 3.318413021363174e-05,
      "loss": 2.3911,
      "step": 33060
    },
    {
      "epoch": 16.820956256358087,
      "grad_norm": 27.09715461730957,
      "learning_rate": 3.3179043743641917e-05,
      "loss": 2.3767,
      "step": 33070
    },
    {
      "epoch": 16.826042726347914,
      "grad_norm": 28.983137130737305,
      "learning_rate": 3.3173957273652086e-05,
      "loss": 2.3204,
      "step": 33080
    },
    {
      "epoch": 16.83112919633774,
      "grad_norm": 31.647151947021484,
      "learning_rate": 3.316887080366226e-05,
      "loss": 2.3601,
      "step": 33090
    },
    {
      "epoch": 16.83621566632757,
      "grad_norm": 27.583280563354492,
      "learning_rate": 3.316378433367243e-05,
      "loss": 2.3144,
      "step": 33100
    },
    {
      "epoch": 16.841302136317395,
      "grad_norm": 28.194698333740234,
      "learning_rate": 3.315869786368261e-05,
      "loss": 2.3771,
      "step": 33110
    },
    {
      "epoch": 16.846388606307222,
      "grad_norm": 25.997045516967773,
      "learning_rate": 3.315361139369278e-05,
      "loss": 2.3539,
      "step": 33120
    },
    {
      "epoch": 16.85147507629705,
      "grad_norm": 33.72493362426758,
      "learning_rate": 3.314852492370295e-05,
      "loss": 2.2828,
      "step": 33130
    },
    {
      "epoch": 16.856561546286876,
      "grad_norm": 27.44940185546875,
      "learning_rate": 3.3143438453713126e-05,
      "loss": 2.4294,
      "step": 33140
    },
    {
      "epoch": 16.861648016276703,
      "grad_norm": 30.162439346313477,
      "learning_rate": 3.3138351983723296e-05,
      "loss": 2.3615,
      "step": 33150
    },
    {
      "epoch": 16.86673448626653,
      "grad_norm": 26.487321853637695,
      "learning_rate": 3.3133265513733466e-05,
      "loss": 2.4026,
      "step": 33160
    },
    {
      "epoch": 16.871820956256357,
      "grad_norm": 33.76603317260742,
      "learning_rate": 3.312817904374364e-05,
      "loss": 2.3362,
      "step": 33170
    },
    {
      "epoch": 16.876907426246184,
      "grad_norm": 28.60513687133789,
      "learning_rate": 3.312309257375382e-05,
      "loss": 2.3279,
      "step": 33180
    },
    {
      "epoch": 16.88199389623601,
      "grad_norm": 27.460033416748047,
      "learning_rate": 3.311800610376399e-05,
      "loss": 2.4085,
      "step": 33190
    },
    {
      "epoch": 16.887080366225838,
      "grad_norm": 38.101558685302734,
      "learning_rate": 3.3112919633774166e-05,
      "loss": 2.3113,
      "step": 33200
    },
    {
      "epoch": 16.892166836215665,
      "grad_norm": 29.85700035095215,
      "learning_rate": 3.3107833163784336e-05,
      "loss": 2.4245,
      "step": 33210
    },
    {
      "epoch": 16.897253306205492,
      "grad_norm": 23.468036651611328,
      "learning_rate": 3.3102746693794506e-05,
      "loss": 2.3635,
      "step": 33220
    },
    {
      "epoch": 16.90233977619532,
      "grad_norm": 25.743783950805664,
      "learning_rate": 3.309766022380468e-05,
      "loss": 2.4254,
      "step": 33230
    },
    {
      "epoch": 16.907426246185146,
      "grad_norm": 28.383316040039062,
      "learning_rate": 3.309257375381485e-05,
      "loss": 2.3244,
      "step": 33240
    },
    {
      "epoch": 16.912512716174973,
      "grad_norm": 30.26325225830078,
      "learning_rate": 3.308748728382502e-05,
      "loss": 2.4134,
      "step": 33250
    },
    {
      "epoch": 16.9175991861648,
      "grad_norm": 27.777084350585938,
      "learning_rate": 3.30824008138352e-05,
      "loss": 2.3299,
      "step": 33260
    },
    {
      "epoch": 16.922685656154627,
      "grad_norm": 27.712108612060547,
      "learning_rate": 3.3077314343845375e-05,
      "loss": 2.3846,
      "step": 33270
    },
    {
      "epoch": 16.927772126144454,
      "grad_norm": 32.915836334228516,
      "learning_rate": 3.3072227873855545e-05,
      "loss": 2.2612,
      "step": 33280
    },
    {
      "epoch": 16.93285859613428,
      "grad_norm": 28.56911849975586,
      "learning_rate": 3.306714140386572e-05,
      "loss": 2.3584,
      "step": 33290
    },
    {
      "epoch": 16.93794506612411,
      "grad_norm": 35.58906936645508,
      "learning_rate": 3.306205493387589e-05,
      "loss": 2.388,
      "step": 33300
    },
    {
      "epoch": 16.94303153611394,
      "grad_norm": 34.55402755737305,
      "learning_rate": 3.305696846388606e-05,
      "loss": 2.3004,
      "step": 33310
    },
    {
      "epoch": 16.948118006103766,
      "grad_norm": 31.639482498168945,
      "learning_rate": 3.305188199389624e-05,
      "loss": 2.3804,
      "step": 33320
    },
    {
      "epoch": 16.953204476093592,
      "grad_norm": 29.953853607177734,
      "learning_rate": 3.304679552390641e-05,
      "loss": 2.4033,
      "step": 33330
    },
    {
      "epoch": 16.95829094608342,
      "grad_norm": 39.070899963378906,
      "learning_rate": 3.304170905391658e-05,
      "loss": 2.3078,
      "step": 33340
    },
    {
      "epoch": 16.963377416073246,
      "grad_norm": 28.386533737182617,
      "learning_rate": 3.3036622583926755e-05,
      "loss": 2.3468,
      "step": 33350
    },
    {
      "epoch": 16.968463886063073,
      "grad_norm": 26.63092803955078,
      "learning_rate": 3.303153611393693e-05,
      "loss": 2.3309,
      "step": 33360
    },
    {
      "epoch": 16.9735503560529,
      "grad_norm": 29.09409523010254,
      "learning_rate": 3.302644964394711e-05,
      "loss": 2.385,
      "step": 33370
    },
    {
      "epoch": 16.978636826042727,
      "grad_norm": 27.219762802124023,
      "learning_rate": 3.302136317395728e-05,
      "loss": 2.3628,
      "step": 33380
    },
    {
      "epoch": 16.983723296032554,
      "grad_norm": 30.537261962890625,
      "learning_rate": 3.301627670396745e-05,
      "loss": 2.3958,
      "step": 33390
    },
    {
      "epoch": 16.98880976602238,
      "grad_norm": 26.574384689331055,
      "learning_rate": 3.3011190233977625e-05,
      "loss": 2.3412,
      "step": 33400
    },
    {
      "epoch": 16.99389623601221,
      "grad_norm": 32.750492095947266,
      "learning_rate": 3.3006103763987795e-05,
      "loss": 2.3436,
      "step": 33410
    },
    {
      "epoch": 16.998982706002035,
      "grad_norm": 32.16349411010742,
      "learning_rate": 3.3001017293997964e-05,
      "loss": 2.3163,
      "step": 33420
    },
    {
      "epoch": 17.0,
      "eval_loss": 4.184648036956787,
      "eval_runtime": 2.7246,
      "eval_samples_per_second": 1018.48,
      "eval_steps_per_second": 127.356,
      "step": 33422
    },
    {
      "epoch": 17.004069175991862,
      "grad_norm": 31.90349006652832,
      "learning_rate": 3.299593082400814e-05,
      "loss": 2.3364,
      "step": 33430
    },
    {
      "epoch": 17.00915564598169,
      "grad_norm": 31.385732650756836,
      "learning_rate": 3.299084435401831e-05,
      "loss": 2.2189,
      "step": 33440
    },
    {
      "epoch": 17.014242115971516,
      "grad_norm": 30.400402069091797,
      "learning_rate": 3.298575788402848e-05,
      "loss": 2.2819,
      "step": 33450
    },
    {
      "epoch": 17.019328585961343,
      "grad_norm": 28.334901809692383,
      "learning_rate": 3.298067141403866e-05,
      "loss": 2.3759,
      "step": 33460
    },
    {
      "epoch": 17.02441505595117,
      "grad_norm": 42.93083572387695,
      "learning_rate": 3.2975584944048834e-05,
      "loss": 2.302,
      "step": 33470
    },
    {
      "epoch": 17.029501525940997,
      "grad_norm": 34.52485275268555,
      "learning_rate": 3.2970498474059004e-05,
      "loss": 2.3135,
      "step": 33480
    },
    {
      "epoch": 17.034587995930824,
      "grad_norm": 26.950471878051758,
      "learning_rate": 3.296541200406918e-05,
      "loss": 2.3203,
      "step": 33490
    },
    {
      "epoch": 17.03967446592065,
      "grad_norm": 30.994277954101562,
      "learning_rate": 3.296032553407935e-05,
      "loss": 2.3143,
      "step": 33500
    },
    {
      "epoch": 17.044760935910478,
      "grad_norm": 30.199735641479492,
      "learning_rate": 3.295523906408952e-05,
      "loss": 2.2053,
      "step": 33510
    },
    {
      "epoch": 17.049847405900305,
      "grad_norm": 28.395877838134766,
      "learning_rate": 3.29501525940997e-05,
      "loss": 2.2677,
      "step": 33520
    },
    {
      "epoch": 17.054933875890132,
      "grad_norm": 28.92222023010254,
      "learning_rate": 3.294506612410987e-05,
      "loss": 2.3763,
      "step": 33530
    },
    {
      "epoch": 17.06002034587996,
      "grad_norm": 31.751453399658203,
      "learning_rate": 3.293997965412004e-05,
      "loss": 2.3328,
      "step": 33540
    },
    {
      "epoch": 17.065106815869786,
      "grad_norm": 32.9730339050293,
      "learning_rate": 3.2934893184130214e-05,
      "loss": 2.28,
      "step": 33550
    },
    {
      "epoch": 17.070193285859613,
      "grad_norm": 30.60175323486328,
      "learning_rate": 3.292980671414039e-05,
      "loss": 2.2422,
      "step": 33560
    },
    {
      "epoch": 17.07527975584944,
      "grad_norm": 43.403507232666016,
      "learning_rate": 3.292472024415056e-05,
      "loss": 2.2656,
      "step": 33570
    },
    {
      "epoch": 17.080366225839267,
      "grad_norm": 24.93985366821289,
      "learning_rate": 3.291963377416074e-05,
      "loss": 2.2279,
      "step": 33580
    },
    {
      "epoch": 17.085452695829094,
      "grad_norm": 31.129167556762695,
      "learning_rate": 3.291454730417091e-05,
      "loss": 2.3251,
      "step": 33590
    },
    {
      "epoch": 17.09053916581892,
      "grad_norm": 32.23210906982422,
      "learning_rate": 3.290946083418108e-05,
      "loss": 2.3696,
      "step": 33600
    },
    {
      "epoch": 17.095625635808748,
      "grad_norm": 26.053993225097656,
      "learning_rate": 3.2904374364191253e-05,
      "loss": 2.3159,
      "step": 33610
    },
    {
      "epoch": 17.100712105798575,
      "grad_norm": 27.286779403686523,
      "learning_rate": 3.289928789420142e-05,
      "loss": 2.3084,
      "step": 33620
    },
    {
      "epoch": 17.105798575788402,
      "grad_norm": 30.351783752441406,
      "learning_rate": 3.28942014242116e-05,
      "loss": 2.2824,
      "step": 33630
    },
    {
      "epoch": 17.11088504577823,
      "grad_norm": 29.576902389526367,
      "learning_rate": 3.288911495422177e-05,
      "loss": 2.3844,
      "step": 33640
    },
    {
      "epoch": 17.115971515768056,
      "grad_norm": 26.5921573638916,
      "learning_rate": 3.2884028484231947e-05,
      "loss": 2.2665,
      "step": 33650
    },
    {
      "epoch": 17.121057985757883,
      "grad_norm": 45.100128173828125,
      "learning_rate": 3.287894201424212e-05,
      "loss": 2.3762,
      "step": 33660
    },
    {
      "epoch": 17.12614445574771,
      "grad_norm": 26.286523818969727,
      "learning_rate": 3.287385554425229e-05,
      "loss": 2.3427,
      "step": 33670
    },
    {
      "epoch": 17.131230925737537,
      "grad_norm": 27.91238784790039,
      "learning_rate": 3.286876907426246e-05,
      "loss": 2.3321,
      "step": 33680
    },
    {
      "epoch": 17.136317395727364,
      "grad_norm": 40.29723358154297,
      "learning_rate": 3.286368260427264e-05,
      "loss": 2.3709,
      "step": 33690
    },
    {
      "epoch": 17.14140386571719,
      "grad_norm": 33.08500671386719,
      "learning_rate": 3.285859613428281e-05,
      "loss": 2.3294,
      "step": 33700
    },
    {
      "epoch": 17.146490335707018,
      "grad_norm": 28.897430419921875,
      "learning_rate": 3.285350966429298e-05,
      "loss": 2.262,
      "step": 33710
    },
    {
      "epoch": 17.151576805696845,
      "grad_norm": 36.736141204833984,
      "learning_rate": 3.2848423194303156e-05,
      "loss": 2.312,
      "step": 33720
    },
    {
      "epoch": 17.15666327568667,
      "grad_norm": 33.11087417602539,
      "learning_rate": 3.2843336724313326e-05,
      "loss": 2.3705,
      "step": 33730
    },
    {
      "epoch": 17.161749745676502,
      "grad_norm": 32.28327560424805,
      "learning_rate": 3.2838250254323496e-05,
      "loss": 2.2423,
      "step": 33740
    },
    {
      "epoch": 17.16683621566633,
      "grad_norm": 25.557939529418945,
      "learning_rate": 3.283316378433367e-05,
      "loss": 2.3404,
      "step": 33750
    },
    {
      "epoch": 17.171922685656156,
      "grad_norm": 31.42392349243164,
      "learning_rate": 3.282807731434385e-05,
      "loss": 2.2694,
      "step": 33760
    },
    {
      "epoch": 17.177009155645983,
      "grad_norm": 28.50890350341797,
      "learning_rate": 3.282299084435402e-05,
      "loss": 2.2909,
      "step": 33770
    },
    {
      "epoch": 17.18209562563581,
      "grad_norm": 27.982166290283203,
      "learning_rate": 3.2817904374364196e-05,
      "loss": 2.3412,
      "step": 33780
    },
    {
      "epoch": 17.187182095625637,
      "grad_norm": 30.452999114990234,
      "learning_rate": 3.2812817904374366e-05,
      "loss": 2.2391,
      "step": 33790
    },
    {
      "epoch": 17.192268565615464,
      "grad_norm": 32.31938552856445,
      "learning_rate": 3.2807731434384536e-05,
      "loss": 2.3497,
      "step": 33800
    },
    {
      "epoch": 17.19735503560529,
      "grad_norm": 28.463245391845703,
      "learning_rate": 3.280264496439471e-05,
      "loss": 2.3096,
      "step": 33810
    },
    {
      "epoch": 17.202441505595118,
      "grad_norm": 29.043434143066406,
      "learning_rate": 3.279755849440488e-05,
      "loss": 2.3309,
      "step": 33820
    },
    {
      "epoch": 17.207527975584945,
      "grad_norm": 27.868162155151367,
      "learning_rate": 3.279247202441505e-05,
      "loss": 2.282,
      "step": 33830
    },
    {
      "epoch": 17.212614445574772,
      "grad_norm": 27.685871124267578,
      "learning_rate": 3.278738555442523e-05,
      "loss": 2.308,
      "step": 33840
    },
    {
      "epoch": 17.2177009155646,
      "grad_norm": 26.40473175048828,
      "learning_rate": 3.2782299084435405e-05,
      "loss": 2.3135,
      "step": 33850
    },
    {
      "epoch": 17.222787385554426,
      "grad_norm": 26.931406021118164,
      "learning_rate": 3.2777212614445575e-05,
      "loss": 2.3137,
      "step": 33860
    },
    {
      "epoch": 17.227873855544253,
      "grad_norm": 26.647541046142578,
      "learning_rate": 3.277212614445575e-05,
      "loss": 2.3704,
      "step": 33870
    },
    {
      "epoch": 17.23296032553408,
      "grad_norm": 30.54054832458496,
      "learning_rate": 3.276703967446592e-05,
      "loss": 2.3123,
      "step": 33880
    },
    {
      "epoch": 17.238046795523907,
      "grad_norm": 34.57909393310547,
      "learning_rate": 3.276195320447609e-05,
      "loss": 2.2914,
      "step": 33890
    },
    {
      "epoch": 17.243133265513734,
      "grad_norm": 30.980876922607422,
      "learning_rate": 3.275686673448627e-05,
      "loss": 2.4637,
      "step": 33900
    },
    {
      "epoch": 17.24821973550356,
      "grad_norm": 24.376611709594727,
      "learning_rate": 3.275178026449644e-05,
      "loss": 2.2381,
      "step": 33910
    },
    {
      "epoch": 17.253306205493388,
      "grad_norm": 28.41327476501465,
      "learning_rate": 3.2746693794506615e-05,
      "loss": 2.2865,
      "step": 33920
    },
    {
      "epoch": 17.258392675483215,
      "grad_norm": 32.89426803588867,
      "learning_rate": 3.2741607324516785e-05,
      "loss": 2.2419,
      "step": 33930
    },
    {
      "epoch": 17.263479145473042,
      "grad_norm": 28.144773483276367,
      "learning_rate": 3.273652085452696e-05,
      "loss": 2.3409,
      "step": 33940
    },
    {
      "epoch": 17.26856561546287,
      "grad_norm": 33.532352447509766,
      "learning_rate": 3.273143438453714e-05,
      "loss": 2.2933,
      "step": 33950
    },
    {
      "epoch": 17.273652085452696,
      "grad_norm": 26.240690231323242,
      "learning_rate": 3.272634791454731e-05,
      "loss": 2.3026,
      "step": 33960
    },
    {
      "epoch": 17.278738555442523,
      "grad_norm": 26.46723175048828,
      "learning_rate": 3.272126144455748e-05,
      "loss": 2.3171,
      "step": 33970
    },
    {
      "epoch": 17.28382502543235,
      "grad_norm": 27.78815269470215,
      "learning_rate": 3.2716174974567655e-05,
      "loss": 2.3342,
      "step": 33980
    },
    {
      "epoch": 17.288911495422177,
      "grad_norm": 28.873735427856445,
      "learning_rate": 3.2711088504577825e-05,
      "loss": 2.3313,
      "step": 33990
    },
    {
      "epoch": 17.293997965412004,
      "grad_norm": 25.695329666137695,
      "learning_rate": 3.2706002034587994e-05,
      "loss": 2.2936,
      "step": 34000
    },
    {
      "epoch": 17.29908443540183,
      "grad_norm": 31.28225326538086,
      "learning_rate": 3.270091556459817e-05,
      "loss": 2.2856,
      "step": 34010
    },
    {
      "epoch": 17.304170905391658,
      "grad_norm": 37.68531036376953,
      "learning_rate": 3.269582909460834e-05,
      "loss": 2.3516,
      "step": 34020
    },
    {
      "epoch": 17.309257375381485,
      "grad_norm": 30.29933738708496,
      "learning_rate": 3.269074262461852e-05,
      "loss": 2.2262,
      "step": 34030
    },
    {
      "epoch": 17.31434384537131,
      "grad_norm": 26.081073760986328,
      "learning_rate": 3.2685656154628694e-05,
      "loss": 2.3227,
      "step": 34040
    },
    {
      "epoch": 17.31943031536114,
      "grad_norm": 22.856746673583984,
      "learning_rate": 3.2680569684638864e-05,
      "loss": 2.4038,
      "step": 34050
    },
    {
      "epoch": 17.324516785350966,
      "grad_norm": 34.631900787353516,
      "learning_rate": 3.2675483214649034e-05,
      "loss": 2.2994,
      "step": 34060
    },
    {
      "epoch": 17.329603255340793,
      "grad_norm": 26.849821090698242,
      "learning_rate": 3.267039674465921e-05,
      "loss": 2.3277,
      "step": 34070
    },
    {
      "epoch": 17.33468972533062,
      "grad_norm": 29.155712127685547,
      "learning_rate": 3.266531027466938e-05,
      "loss": 2.2265,
      "step": 34080
    },
    {
      "epoch": 17.339776195320447,
      "grad_norm": 26.114356994628906,
      "learning_rate": 3.266022380467955e-05,
      "loss": 2.2501,
      "step": 34090
    },
    {
      "epoch": 17.344862665310274,
      "grad_norm": 28.20772933959961,
      "learning_rate": 3.265513733468973e-05,
      "loss": 2.3328,
      "step": 34100
    },
    {
      "epoch": 17.3499491353001,
      "grad_norm": 37.06182861328125,
      "learning_rate": 3.26500508646999e-05,
      "loss": 2.2612,
      "step": 34110
    },
    {
      "epoch": 17.355035605289928,
      "grad_norm": 26.889080047607422,
      "learning_rate": 3.264496439471007e-05,
      "loss": 2.371,
      "step": 34120
    },
    {
      "epoch": 17.360122075279754,
      "grad_norm": 35.21895980834961,
      "learning_rate": 3.2639877924720244e-05,
      "loss": 2.3469,
      "step": 34130
    },
    {
      "epoch": 17.36520854526958,
      "grad_norm": 23.385364532470703,
      "learning_rate": 3.263479145473042e-05,
      "loss": 2.3107,
      "step": 34140
    },
    {
      "epoch": 17.37029501525941,
      "grad_norm": 28.949466705322266,
      "learning_rate": 3.262970498474059e-05,
      "loss": 2.2502,
      "step": 34150
    },
    {
      "epoch": 17.375381485249235,
      "grad_norm": 29.303491592407227,
      "learning_rate": 3.262461851475077e-05,
      "loss": 2.3297,
      "step": 34160
    },
    {
      "epoch": 17.380467955239062,
      "grad_norm": 28.48514747619629,
      "learning_rate": 3.261953204476094e-05,
      "loss": 2.2932,
      "step": 34170
    },
    {
      "epoch": 17.38555442522889,
      "grad_norm": 33.426513671875,
      "learning_rate": 3.2614445574771114e-05,
      "loss": 2.2699,
      "step": 34180
    },
    {
      "epoch": 17.39064089521872,
      "grad_norm": 42.775665283203125,
      "learning_rate": 3.2609359104781283e-05,
      "loss": 2.3122,
      "step": 34190
    },
    {
      "epoch": 17.395727365208547,
      "grad_norm": 27.664112091064453,
      "learning_rate": 3.260427263479145e-05,
      "loss": 2.3806,
      "step": 34200
    },
    {
      "epoch": 17.400813835198374,
      "grad_norm": 30.01534080505371,
      "learning_rate": 3.259918616480163e-05,
      "loss": 2.2737,
      "step": 34210
    },
    {
      "epoch": 17.4059003051882,
      "grad_norm": 31.584453582763672,
      "learning_rate": 3.25940996948118e-05,
      "loss": 2.345,
      "step": 34220
    },
    {
      "epoch": 17.410986775178028,
      "grad_norm": 28.506101608276367,
      "learning_rate": 3.2589013224821977e-05,
      "loss": 2.2819,
      "step": 34230
    },
    {
      "epoch": 17.416073245167855,
      "grad_norm": 26.105571746826172,
      "learning_rate": 3.258392675483215e-05,
      "loss": 2.2363,
      "step": 34240
    },
    {
      "epoch": 17.421159715157682,
      "grad_norm": 31.86705780029297,
      "learning_rate": 3.257884028484232e-05,
      "loss": 2.158,
      "step": 34250
    },
    {
      "epoch": 17.42624618514751,
      "grad_norm": 30.019481658935547,
      "learning_rate": 3.257375381485249e-05,
      "loss": 2.3006,
      "step": 34260
    },
    {
      "epoch": 17.431332655137336,
      "grad_norm": 26.03209114074707,
      "learning_rate": 3.256866734486267e-05,
      "loss": 2.3683,
      "step": 34270
    },
    {
      "epoch": 17.436419125127163,
      "grad_norm": 34.742347717285156,
      "learning_rate": 3.256358087487284e-05,
      "loss": 2.311,
      "step": 34280
    },
    {
      "epoch": 17.44150559511699,
      "grad_norm": 27.873058319091797,
      "learning_rate": 3.255849440488301e-05,
      "loss": 2.2702,
      "step": 34290
    },
    {
      "epoch": 17.446592065106817,
      "grad_norm": 31.895421981811523,
      "learning_rate": 3.2553407934893186e-05,
      "loss": 2.3275,
      "step": 34300
    },
    {
      "epoch": 17.451678535096644,
      "grad_norm": 31.170207977294922,
      "learning_rate": 3.2548321464903356e-05,
      "loss": 2.3057,
      "step": 34310
    },
    {
      "epoch": 17.45676500508647,
      "grad_norm": 28.39800262451172,
      "learning_rate": 3.254323499491353e-05,
      "loss": 2.261,
      "step": 34320
    },
    {
      "epoch": 17.461851475076298,
      "grad_norm": 30.990753173828125,
      "learning_rate": 3.253814852492371e-05,
      "loss": 2.3572,
      "step": 34330
    },
    {
      "epoch": 17.466937945066125,
      "grad_norm": 24.836896896362305,
      "learning_rate": 3.253306205493388e-05,
      "loss": 2.2015,
      "step": 34340
    },
    {
      "epoch": 17.47202441505595,
      "grad_norm": 30.953548431396484,
      "learning_rate": 3.252797558494405e-05,
      "loss": 2.3318,
      "step": 34350
    },
    {
      "epoch": 17.47711088504578,
      "grad_norm": 29.238067626953125,
      "learning_rate": 3.2522889114954226e-05,
      "loss": 2.3236,
      "step": 34360
    },
    {
      "epoch": 17.482197355035606,
      "grad_norm": 32.014915466308594,
      "learning_rate": 3.2517802644964396e-05,
      "loss": 2.2775,
      "step": 34370
    },
    {
      "epoch": 17.487283825025433,
      "grad_norm": 25.795299530029297,
      "learning_rate": 3.2512716174974566e-05,
      "loss": 2.198,
      "step": 34380
    },
    {
      "epoch": 17.49237029501526,
      "grad_norm": 29.81844711303711,
      "learning_rate": 3.250762970498474e-05,
      "loss": 2.275,
      "step": 34390
    },
    {
      "epoch": 17.497456765005087,
      "grad_norm": 29.837671279907227,
      "learning_rate": 3.250254323499491e-05,
      "loss": 2.2941,
      "step": 34400
    },
    {
      "epoch": 17.502543234994913,
      "grad_norm": 38.19585037231445,
      "learning_rate": 3.249745676500508e-05,
      "loss": 2.3138,
      "step": 34410
    },
    {
      "epoch": 17.50762970498474,
      "grad_norm": 36.3369140625,
      "learning_rate": 3.249237029501526e-05,
      "loss": 2.3306,
      "step": 34420
    },
    {
      "epoch": 17.512716174974567,
      "grad_norm": 28.426042556762695,
      "learning_rate": 3.2487283825025435e-05,
      "loss": 2.3467,
      "step": 34430
    },
    {
      "epoch": 17.517802644964394,
      "grad_norm": 34.968387603759766,
      "learning_rate": 3.248219735503561e-05,
      "loss": 2.3416,
      "step": 34440
    },
    {
      "epoch": 17.52288911495422,
      "grad_norm": 33.4559326171875,
      "learning_rate": 3.247711088504578e-05,
      "loss": 2.2829,
      "step": 34450
    },
    {
      "epoch": 17.52797558494405,
      "grad_norm": 30.512187957763672,
      "learning_rate": 3.247202441505595e-05,
      "loss": 2.2732,
      "step": 34460
    },
    {
      "epoch": 17.533062054933875,
      "grad_norm": 28.41303253173828,
      "learning_rate": 3.246693794506613e-05,
      "loss": 2.3082,
      "step": 34470
    },
    {
      "epoch": 17.538148524923702,
      "grad_norm": 32.77104568481445,
      "learning_rate": 3.24618514750763e-05,
      "loss": 2.3561,
      "step": 34480
    },
    {
      "epoch": 17.54323499491353,
      "grad_norm": 30.342967987060547,
      "learning_rate": 3.245676500508647e-05,
      "loss": 2.3465,
      "step": 34490
    },
    {
      "epoch": 17.548321464903356,
      "grad_norm": 27.28266143798828,
      "learning_rate": 3.2451678535096645e-05,
      "loss": 2.2945,
      "step": 34500
    },
    {
      "epoch": 17.553407934893183,
      "grad_norm": 32.62879943847656,
      "learning_rate": 3.2446592065106815e-05,
      "loss": 2.1932,
      "step": 34510
    },
    {
      "epoch": 17.55849440488301,
      "grad_norm": 31.658727645874023,
      "learning_rate": 3.244150559511699e-05,
      "loss": 2.278,
      "step": 34520
    },
    {
      "epoch": 17.563580874872837,
      "grad_norm": 26.174768447875977,
      "learning_rate": 3.243641912512717e-05,
      "loss": 2.3365,
      "step": 34530
    },
    {
      "epoch": 17.568667344862664,
      "grad_norm": 34.86494827270508,
      "learning_rate": 3.243133265513734e-05,
      "loss": 2.412,
      "step": 34540
    },
    {
      "epoch": 17.57375381485249,
      "grad_norm": 29.311582565307617,
      "learning_rate": 3.242624618514751e-05,
      "loss": 2.2785,
      "step": 34550
    },
    {
      "epoch": 17.578840284842318,
      "grad_norm": 28.886507034301758,
      "learning_rate": 3.2421159715157685e-05,
      "loss": 2.3106,
      "step": 34560
    },
    {
      "epoch": 17.583926754832145,
      "grad_norm": 34.2018928527832,
      "learning_rate": 3.2416073245167855e-05,
      "loss": 2.3715,
      "step": 34570
    },
    {
      "epoch": 17.589013224821972,
      "grad_norm": 27.987234115600586,
      "learning_rate": 3.2410986775178024e-05,
      "loss": 2.2056,
      "step": 34580
    },
    {
      "epoch": 17.5940996948118,
      "grad_norm": 33.27973556518555,
      "learning_rate": 3.24059003051882e-05,
      "loss": 2.3524,
      "step": 34590
    },
    {
      "epoch": 17.599186164801626,
      "grad_norm": 38.547176361083984,
      "learning_rate": 3.240081383519837e-05,
      "loss": 2.3126,
      "step": 34600
    },
    {
      "epoch": 17.604272634791453,
      "grad_norm": 34.79167938232422,
      "learning_rate": 3.239572736520855e-05,
      "loss": 2.2251,
      "step": 34610
    },
    {
      "epoch": 17.60935910478128,
      "grad_norm": 31.655860900878906,
      "learning_rate": 3.2390640895218724e-05,
      "loss": 2.2519,
      "step": 34620
    },
    {
      "epoch": 17.61444557477111,
      "grad_norm": 26.92909812927246,
      "learning_rate": 3.2385554425228894e-05,
      "loss": 2.2905,
      "step": 34630
    },
    {
      "epoch": 17.619532044760938,
      "grad_norm": 29.466657638549805,
      "learning_rate": 3.2380467955239064e-05,
      "loss": 2.2678,
      "step": 34640
    },
    {
      "epoch": 17.624618514750765,
      "grad_norm": 27.504899978637695,
      "learning_rate": 3.237538148524924e-05,
      "loss": 2.3423,
      "step": 34650
    },
    {
      "epoch": 17.62970498474059,
      "grad_norm": 27.153018951416016,
      "learning_rate": 3.237029501525941e-05,
      "loss": 2.3014,
      "step": 34660
    },
    {
      "epoch": 17.63479145473042,
      "grad_norm": 26.62380599975586,
      "learning_rate": 3.236520854526958e-05,
      "loss": 2.4081,
      "step": 34670
    },
    {
      "epoch": 17.639877924720246,
      "grad_norm": 26.2889404296875,
      "learning_rate": 3.236012207527976e-05,
      "loss": 2.232,
      "step": 34680
    },
    {
      "epoch": 17.644964394710072,
      "grad_norm": 29.979909896850586,
      "learning_rate": 3.235503560528993e-05,
      "loss": 2.2761,
      "step": 34690
    },
    {
      "epoch": 17.6500508646999,
      "grad_norm": 32.96697235107422,
      "learning_rate": 3.2349949135300104e-05,
      "loss": 2.2133,
      "step": 34700
    },
    {
      "epoch": 17.655137334689726,
      "grad_norm": 39.075069427490234,
      "learning_rate": 3.2344862665310274e-05,
      "loss": 2.2244,
      "step": 34710
    },
    {
      "epoch": 17.660223804679553,
      "grad_norm": 31.181100845336914,
      "learning_rate": 3.233977619532045e-05,
      "loss": 2.2914,
      "step": 34720
    },
    {
      "epoch": 17.66531027466938,
      "grad_norm": 32.04267120361328,
      "learning_rate": 3.233468972533063e-05,
      "loss": 2.3363,
      "step": 34730
    },
    {
      "epoch": 17.670396744659207,
      "grad_norm": 32.83686828613281,
      "learning_rate": 3.23296032553408e-05,
      "loss": 2.2741,
      "step": 34740
    },
    {
      "epoch": 17.675483214649034,
      "grad_norm": 29.531343460083008,
      "learning_rate": 3.232451678535097e-05,
      "loss": 2.3232,
      "step": 34750
    },
    {
      "epoch": 17.68056968463886,
      "grad_norm": 30.424800872802734,
      "learning_rate": 3.2319430315361144e-05,
      "loss": 2.2613,
      "step": 34760
    },
    {
      "epoch": 17.68565615462869,
      "grad_norm": 27.981304168701172,
      "learning_rate": 3.2314343845371313e-05,
      "loss": 2.2377,
      "step": 34770
    },
    {
      "epoch": 17.690742624618515,
      "grad_norm": 37.64270782470703,
      "learning_rate": 3.230925737538148e-05,
      "loss": 2.1924,
      "step": 34780
    },
    {
      "epoch": 17.695829094608342,
      "grad_norm": 30.102386474609375,
      "learning_rate": 3.230417090539166e-05,
      "loss": 2.341,
      "step": 34790
    },
    {
      "epoch": 17.70091556459817,
      "grad_norm": 29.48631477355957,
      "learning_rate": 3.229908443540183e-05,
      "loss": 2.2337,
      "step": 34800
    },
    {
      "epoch": 17.706002034587996,
      "grad_norm": 27.974950790405273,
      "learning_rate": 3.2293997965412007e-05,
      "loss": 2.3807,
      "step": 34810
    },
    {
      "epoch": 17.711088504577823,
      "grad_norm": 29.179357528686523,
      "learning_rate": 3.228891149542218e-05,
      "loss": 2.3243,
      "step": 34820
    },
    {
      "epoch": 17.71617497456765,
      "grad_norm": 29.036001205444336,
      "learning_rate": 3.228382502543235e-05,
      "loss": 2.3552,
      "step": 34830
    },
    {
      "epoch": 17.721261444557477,
      "grad_norm": 24.887977600097656,
      "learning_rate": 3.227873855544252e-05,
      "loss": 2.3052,
      "step": 34840
    },
    {
      "epoch": 17.726347914547304,
      "grad_norm": 32.522666931152344,
      "learning_rate": 3.22736520854527e-05,
      "loss": 2.3301,
      "step": 34850
    },
    {
      "epoch": 17.73143438453713,
      "grad_norm": 35.050594329833984,
      "learning_rate": 3.226856561546287e-05,
      "loss": 2.2909,
      "step": 34860
    },
    {
      "epoch": 17.736520854526958,
      "grad_norm": 26.329320907592773,
      "learning_rate": 3.226347914547304e-05,
      "loss": 2.3148,
      "step": 34870
    },
    {
      "epoch": 17.741607324516785,
      "grad_norm": 30.694034576416016,
      "learning_rate": 3.2258392675483216e-05,
      "loss": 2.3606,
      "step": 34880
    },
    {
      "epoch": 17.746693794506612,
      "grad_norm": 29.733675003051758,
      "learning_rate": 3.2253306205493386e-05,
      "loss": 2.2358,
      "step": 34890
    },
    {
      "epoch": 17.75178026449644,
      "grad_norm": 38.34771728515625,
      "learning_rate": 3.224821973550356e-05,
      "loss": 2.3037,
      "step": 34900
    },
    {
      "epoch": 17.756866734486266,
      "grad_norm": 34.758121490478516,
      "learning_rate": 3.224313326551374e-05,
      "loss": 2.176,
      "step": 34910
    },
    {
      "epoch": 17.761953204476093,
      "grad_norm": 30.193828582763672,
      "learning_rate": 3.223804679552391e-05,
      "loss": 2.2821,
      "step": 34920
    },
    {
      "epoch": 17.76703967446592,
      "grad_norm": 34.831764221191406,
      "learning_rate": 3.223296032553408e-05,
      "loss": 2.2776,
      "step": 34930
    },
    {
      "epoch": 17.772126144455747,
      "grad_norm": 36.8391227722168,
      "learning_rate": 3.2227873855544256e-05,
      "loss": 2.2976,
      "step": 34940
    },
    {
      "epoch": 17.777212614445574,
      "grad_norm": 32.552677154541016,
      "learning_rate": 3.2222787385554426e-05,
      "loss": 2.2587,
      "step": 34950
    },
    {
      "epoch": 17.7822990844354,
      "grad_norm": 29.026897430419922,
      "learning_rate": 3.2217700915564596e-05,
      "loss": 2.3631,
      "step": 34960
    },
    {
      "epoch": 17.787385554425228,
      "grad_norm": 38.22381591796875,
      "learning_rate": 3.221261444557477e-05,
      "loss": 2.2814,
      "step": 34970
    },
    {
      "epoch": 17.792472024415055,
      "grad_norm": 30.378555297851562,
      "learning_rate": 3.220752797558494e-05,
      "loss": 2.2716,
      "step": 34980
    },
    {
      "epoch": 17.797558494404882,
      "grad_norm": 33.708885192871094,
      "learning_rate": 3.220244150559512e-05,
      "loss": 2.2685,
      "step": 34990
    },
    {
      "epoch": 17.80264496439471,
      "grad_norm": 28.741483688354492,
      "learning_rate": 3.2197355035605296e-05,
      "loss": 2.3101,
      "step": 35000
    },
    {
      "epoch": 17.807731434384536,
      "grad_norm": 28.323936462402344,
      "learning_rate": 3.2192268565615465e-05,
      "loss": 2.3314,
      "step": 35010
    },
    {
      "epoch": 17.812817904374363,
      "grad_norm": 36.481536865234375,
      "learning_rate": 3.218718209562564e-05,
      "loss": 2.3485,
      "step": 35020
    },
    {
      "epoch": 17.81790437436419,
      "grad_norm": 39.16683578491211,
      "learning_rate": 3.218209562563581e-05,
      "loss": 2.321,
      "step": 35030
    },
    {
      "epoch": 17.822990844354017,
      "grad_norm": 36.50544357299805,
      "learning_rate": 3.217700915564598e-05,
      "loss": 2.2938,
      "step": 35040
    },
    {
      "epoch": 17.828077314343844,
      "grad_norm": 34.822078704833984,
      "learning_rate": 3.217192268565616e-05,
      "loss": 2.2612,
      "step": 35050
    },
    {
      "epoch": 17.83316378433367,
      "grad_norm": 29.847293853759766,
      "learning_rate": 3.216683621566633e-05,
      "loss": 2.2584,
      "step": 35060
    },
    {
      "epoch": 17.838250254323498,
      "grad_norm": 24.855438232421875,
      "learning_rate": 3.21617497456765e-05,
      "loss": 2.3499,
      "step": 35070
    },
    {
      "epoch": 17.843336724313325,
      "grad_norm": 24.441097259521484,
      "learning_rate": 3.2156663275686675e-05,
      "loss": 2.2277,
      "step": 35080
    },
    {
      "epoch": 17.848423194303155,
      "grad_norm": 40.812862396240234,
      "learning_rate": 3.2151576805696845e-05,
      "loss": 2.3533,
      "step": 35090
    },
    {
      "epoch": 17.853509664292982,
      "grad_norm": 29.0773983001709,
      "learning_rate": 3.214649033570702e-05,
      "loss": 2.3376,
      "step": 35100
    },
    {
      "epoch": 17.85859613428281,
      "grad_norm": 32.803253173828125,
      "learning_rate": 3.21414038657172e-05,
      "loss": 2.3422,
      "step": 35110
    },
    {
      "epoch": 17.863682604272636,
      "grad_norm": 39.93585205078125,
      "learning_rate": 3.213631739572737e-05,
      "loss": 2.2719,
      "step": 35120
    },
    {
      "epoch": 17.868769074262463,
      "grad_norm": 28.390636444091797,
      "learning_rate": 3.213123092573754e-05,
      "loss": 2.3065,
      "step": 35130
    },
    {
      "epoch": 17.87385554425229,
      "grad_norm": 29.759607315063477,
      "learning_rate": 3.2126144455747715e-05,
      "loss": 2.2745,
      "step": 35140
    },
    {
      "epoch": 17.878942014242117,
      "grad_norm": 28.641618728637695,
      "learning_rate": 3.2121057985757885e-05,
      "loss": 2.1682,
      "step": 35150
    },
    {
      "epoch": 17.884028484231944,
      "grad_norm": 34.950199127197266,
      "learning_rate": 3.2115971515768055e-05,
      "loss": 2.3217,
      "step": 35160
    },
    {
      "epoch": 17.88911495422177,
      "grad_norm": 29.465757369995117,
      "learning_rate": 3.211088504577823e-05,
      "loss": 2.2838,
      "step": 35170
    },
    {
      "epoch": 17.894201424211598,
      "grad_norm": 31.440454483032227,
      "learning_rate": 3.21057985757884e-05,
      "loss": 2.2959,
      "step": 35180
    },
    {
      "epoch": 17.899287894201425,
      "grad_norm": 28.792863845825195,
      "learning_rate": 3.210071210579858e-05,
      "loss": 2.3953,
      "step": 35190
    },
    {
      "epoch": 17.904374364191252,
      "grad_norm": 30.923694610595703,
      "learning_rate": 3.2095625635808754e-05,
      "loss": 2.3092,
      "step": 35200
    },
    {
      "epoch": 17.90946083418108,
      "grad_norm": 33.11262130737305,
      "learning_rate": 3.2090539165818924e-05,
      "loss": 2.2764,
      "step": 35210
    },
    {
      "epoch": 17.914547304170906,
      "grad_norm": 38.56220245361328,
      "learning_rate": 3.2085452695829094e-05,
      "loss": 2.3075,
      "step": 35220
    },
    {
      "epoch": 17.919633774160733,
      "grad_norm": 24.55397605895996,
      "learning_rate": 3.208036622583927e-05,
      "loss": 2.2859,
      "step": 35230
    },
    {
      "epoch": 17.92472024415056,
      "grad_norm": 28.263565063476562,
      "learning_rate": 3.207527975584944e-05,
      "loss": 2.3665,
      "step": 35240
    },
    {
      "epoch": 17.929806714140387,
      "grad_norm": 37.94634246826172,
      "learning_rate": 3.207019328585962e-05,
      "loss": 2.3193,
      "step": 35250
    },
    {
      "epoch": 17.934893184130214,
      "grad_norm": 29.949939727783203,
      "learning_rate": 3.206510681586979e-05,
      "loss": 2.2634,
      "step": 35260
    },
    {
      "epoch": 17.93997965412004,
      "grad_norm": 28.06138801574707,
      "learning_rate": 3.206002034587996e-05,
      "loss": 2.2649,
      "step": 35270
    },
    {
      "epoch": 17.945066124109868,
      "grad_norm": 31.203060150146484,
      "learning_rate": 3.2054933875890134e-05,
      "loss": 2.2992,
      "step": 35280
    },
    {
      "epoch": 17.950152594099695,
      "grad_norm": 43.49785614013672,
      "learning_rate": 3.204984740590031e-05,
      "loss": 2.3014,
      "step": 35290
    },
    {
      "epoch": 17.955239064089522,
      "grad_norm": 28.106000900268555,
      "learning_rate": 3.204476093591048e-05,
      "loss": 2.2902,
      "step": 35300
    },
    {
      "epoch": 17.96032553407935,
      "grad_norm": 31.339506149291992,
      "learning_rate": 3.203967446592066e-05,
      "loss": 2.2035,
      "step": 35310
    },
    {
      "epoch": 17.965412004069176,
      "grad_norm": 30.506717681884766,
      "learning_rate": 3.203458799593083e-05,
      "loss": 2.3163,
      "step": 35320
    },
    {
      "epoch": 17.970498474059003,
      "grad_norm": 28.766828536987305,
      "learning_rate": 3.2029501525941e-05,
      "loss": 2.3361,
      "step": 35330
    },
    {
      "epoch": 17.97558494404883,
      "grad_norm": 35.331642150878906,
      "learning_rate": 3.2024415055951174e-05,
      "loss": 2.2599,
      "step": 35340
    },
    {
      "epoch": 17.980671414038657,
      "grad_norm": 43.67035675048828,
      "learning_rate": 3.2019328585961343e-05,
      "loss": 2.3239,
      "step": 35350
    },
    {
      "epoch": 17.985757884028484,
      "grad_norm": 31.67609214782715,
      "learning_rate": 3.201424211597151e-05,
      "loss": 2.4034,
      "step": 35360
    },
    {
      "epoch": 17.99084435401831,
      "grad_norm": 36.70155334472656,
      "learning_rate": 3.200915564598169e-05,
      "loss": 2.323,
      "step": 35370
    },
    {
      "epoch": 17.995930824008138,
      "grad_norm": 35.87823486328125,
      "learning_rate": 3.200406917599186e-05,
      "loss": 2.3686,
      "step": 35380
    },
    {
      "epoch": 18.0,
      "eval_loss": 4.210157871246338,
      "eval_runtime": 2.737,
      "eval_samples_per_second": 1013.89,
      "eval_steps_per_second": 126.782,
      "step": 35388
    },
    {
      "epoch": 18.001017293997965,
      "grad_norm": 26.129674911499023,
      "learning_rate": 3.1998982706002037e-05,
      "loss": 2.3489,
      "step": 35390
    },
    {
      "epoch": 18.00610376398779,
      "grad_norm": 36.073020935058594,
      "learning_rate": 3.199389623601221e-05,
      "loss": 2.2216,
      "step": 35400
    },
    {
      "epoch": 18.01119023397762,
      "grad_norm": 37.92762756347656,
      "learning_rate": 3.198880976602238e-05,
      "loss": 2.2786,
      "step": 35410
    },
    {
      "epoch": 18.016276703967446,
      "grad_norm": 33.005592346191406,
      "learning_rate": 3.198372329603255e-05,
      "loss": 2.2422,
      "step": 35420
    },
    {
      "epoch": 18.021363173957273,
      "grad_norm": 33.24058532714844,
      "learning_rate": 3.197863682604273e-05,
      "loss": 2.1821,
      "step": 35430
    },
    {
      "epoch": 18.0264496439471,
      "grad_norm": 29.057960510253906,
      "learning_rate": 3.19735503560529e-05,
      "loss": 2.2278,
      "step": 35440
    },
    {
      "epoch": 18.031536113936927,
      "grad_norm": 25.27603530883789,
      "learning_rate": 3.196846388606307e-05,
      "loss": 2.1496,
      "step": 35450
    },
    {
      "epoch": 18.036622583926754,
      "grad_norm": 32.43949890136719,
      "learning_rate": 3.1963377416073246e-05,
      "loss": 2.3326,
      "step": 35460
    },
    {
      "epoch": 18.04170905391658,
      "grad_norm": 23.277616500854492,
      "learning_rate": 3.1958290946083416e-05,
      "loss": 2.2378,
      "step": 35470
    },
    {
      "epoch": 18.046795523906408,
      "grad_norm": 29.573396682739258,
      "learning_rate": 3.195320447609359e-05,
      "loss": 2.2436,
      "step": 35480
    },
    {
      "epoch": 18.051881993896234,
      "grad_norm": 36.61152267456055,
      "learning_rate": 3.194811800610377e-05,
      "loss": 2.3091,
      "step": 35490
    },
    {
      "epoch": 18.05696846388606,
      "grad_norm": 24.794565200805664,
      "learning_rate": 3.194303153611394e-05,
      "loss": 2.325,
      "step": 35500
    },
    {
      "epoch": 18.06205493387589,
      "grad_norm": 37.17064666748047,
      "learning_rate": 3.193794506612411e-05,
      "loss": 2.1609,
      "step": 35510
    },
    {
      "epoch": 18.067141403865715,
      "grad_norm": 35.87251663208008,
      "learning_rate": 3.1932858596134286e-05,
      "loss": 2.3226,
      "step": 35520
    },
    {
      "epoch": 18.072227873855546,
      "grad_norm": 24.955245971679688,
      "learning_rate": 3.1927772126144456e-05,
      "loss": 2.2282,
      "step": 35530
    },
    {
      "epoch": 18.077314343845373,
      "grad_norm": 30.798887252807617,
      "learning_rate": 3.192268565615463e-05,
      "loss": 2.2973,
      "step": 35540
    },
    {
      "epoch": 18.0824008138352,
      "grad_norm": 26.198442459106445,
      "learning_rate": 3.19175991861648e-05,
      "loss": 2.3552,
      "step": 35550
    },
    {
      "epoch": 18.087487283825027,
      "grad_norm": 24.1673526763916,
      "learning_rate": 3.191251271617497e-05,
      "loss": 2.2796,
      "step": 35560
    },
    {
      "epoch": 18.092573753814854,
      "grad_norm": 31.318870544433594,
      "learning_rate": 3.190742624618515e-05,
      "loss": 2.1783,
      "step": 35570
    },
    {
      "epoch": 18.09766022380468,
      "grad_norm": 30.4610652923584,
      "learning_rate": 3.1902339776195326e-05,
      "loss": 2.2212,
      "step": 35580
    },
    {
      "epoch": 18.102746693794508,
      "grad_norm": 39.143978118896484,
      "learning_rate": 3.1897253306205495e-05,
      "loss": 2.2879,
      "step": 35590
    },
    {
      "epoch": 18.107833163784335,
      "grad_norm": 35.36964797973633,
      "learning_rate": 3.189216683621567e-05,
      "loss": 2.2463,
      "step": 35600
    },
    {
      "epoch": 18.112919633774162,
      "grad_norm": 29.288057327270508,
      "learning_rate": 3.188708036622584e-05,
      "loss": 2.255,
      "step": 35610
    },
    {
      "epoch": 18.11800610376399,
      "grad_norm": 33.4626579284668,
      "learning_rate": 3.188199389623601e-05,
      "loss": 2.2681,
      "step": 35620
    },
    {
      "epoch": 18.123092573753816,
      "grad_norm": 34.23853302001953,
      "learning_rate": 3.187690742624619e-05,
      "loss": 2.2287,
      "step": 35630
    },
    {
      "epoch": 18.128179043743643,
      "grad_norm": 31.084362030029297,
      "learning_rate": 3.187182095625636e-05,
      "loss": 2.3506,
      "step": 35640
    },
    {
      "epoch": 18.13326551373347,
      "grad_norm": 27.414630889892578,
      "learning_rate": 3.186673448626653e-05,
      "loss": 2.2167,
      "step": 35650
    },
    {
      "epoch": 18.138351983723297,
      "grad_norm": 28.37221336364746,
      "learning_rate": 3.1861648016276705e-05,
      "loss": 2.3139,
      "step": 35660
    },
    {
      "epoch": 18.143438453713124,
      "grad_norm": 28.55242347717285,
      "learning_rate": 3.1856561546286875e-05,
      "loss": 2.2414,
      "step": 35670
    },
    {
      "epoch": 18.14852492370295,
      "grad_norm": 35.629791259765625,
      "learning_rate": 3.185147507629705e-05,
      "loss": 2.2231,
      "step": 35680
    },
    {
      "epoch": 18.153611393692778,
      "grad_norm": 30.710533142089844,
      "learning_rate": 3.184638860630723e-05,
      "loss": 2.1646,
      "step": 35690
    },
    {
      "epoch": 18.158697863682605,
      "grad_norm": 22.225696563720703,
      "learning_rate": 3.18413021363174e-05,
      "loss": 2.2906,
      "step": 35700
    },
    {
      "epoch": 18.16378433367243,
      "grad_norm": 32.189369201660156,
      "learning_rate": 3.183621566632757e-05,
      "loss": 2.2023,
      "step": 35710
    },
    {
      "epoch": 18.16887080366226,
      "grad_norm": 36.27117156982422,
      "learning_rate": 3.1831129196337745e-05,
      "loss": 2.1874,
      "step": 35720
    },
    {
      "epoch": 18.173957273652086,
      "grad_norm": 31.783048629760742,
      "learning_rate": 3.1826042726347915e-05,
      "loss": 2.2498,
      "step": 35730
    },
    {
      "epoch": 18.179043743641913,
      "grad_norm": 34.53597640991211,
      "learning_rate": 3.1820956256358085e-05,
      "loss": 2.2335,
      "step": 35740
    },
    {
      "epoch": 18.18413021363174,
      "grad_norm": 29.329631805419922,
      "learning_rate": 3.181586978636826e-05,
      "loss": 2.1934,
      "step": 35750
    },
    {
      "epoch": 18.189216683621567,
      "grad_norm": 45.11614227294922,
      "learning_rate": 3.181078331637843e-05,
      "loss": 2.3222,
      "step": 35760
    },
    {
      "epoch": 18.194303153611393,
      "grad_norm": 32.28059005737305,
      "learning_rate": 3.180569684638861e-05,
      "loss": 2.2645,
      "step": 35770
    },
    {
      "epoch": 18.19938962360122,
      "grad_norm": 31.035552978515625,
      "learning_rate": 3.1800610376398784e-05,
      "loss": 2.2628,
      "step": 35780
    },
    {
      "epoch": 18.204476093591047,
      "grad_norm": 30.94345474243164,
      "learning_rate": 3.1795523906408954e-05,
      "loss": 2.2423,
      "step": 35790
    },
    {
      "epoch": 18.209562563580874,
      "grad_norm": 28.821491241455078,
      "learning_rate": 3.179043743641913e-05,
      "loss": 2.3014,
      "step": 35800
    },
    {
      "epoch": 18.2146490335707,
      "grad_norm": 26.429546356201172,
      "learning_rate": 3.17853509664293e-05,
      "loss": 2.3364,
      "step": 35810
    },
    {
      "epoch": 18.21973550356053,
      "grad_norm": 29.516088485717773,
      "learning_rate": 3.178026449643947e-05,
      "loss": 2.285,
      "step": 35820
    },
    {
      "epoch": 18.224821973550355,
      "grad_norm": 30.78920555114746,
      "learning_rate": 3.177517802644965e-05,
      "loss": 2.2423,
      "step": 35830
    },
    {
      "epoch": 18.229908443540182,
      "grad_norm": 37.02214431762695,
      "learning_rate": 3.177009155645982e-05,
      "loss": 2.1824,
      "step": 35840
    },
    {
      "epoch": 18.23499491353001,
      "grad_norm": 34.21462631225586,
      "learning_rate": 3.176500508646999e-05,
      "loss": 2.2218,
      "step": 35850
    },
    {
      "epoch": 18.240081383519836,
      "grad_norm": 27.349000930786133,
      "learning_rate": 3.1759918616480164e-05,
      "loss": 2.3015,
      "step": 35860
    },
    {
      "epoch": 18.245167853509663,
      "grad_norm": 25.765888214111328,
      "learning_rate": 3.175483214649034e-05,
      "loss": 2.2901,
      "step": 35870
    },
    {
      "epoch": 18.25025432349949,
      "grad_norm": 29.7802791595459,
      "learning_rate": 3.174974567650051e-05,
      "loss": 2.242,
      "step": 35880
    },
    {
      "epoch": 18.255340793489317,
      "grad_norm": 27.15618133544922,
      "learning_rate": 3.174465920651069e-05,
      "loss": 2.2467,
      "step": 35890
    },
    {
      "epoch": 18.260427263479144,
      "grad_norm": 31.21234130859375,
      "learning_rate": 3.173957273652086e-05,
      "loss": 2.2762,
      "step": 35900
    },
    {
      "epoch": 18.26551373346897,
      "grad_norm": 36.64937210083008,
      "learning_rate": 3.173448626653103e-05,
      "loss": 2.2571,
      "step": 35910
    },
    {
      "epoch": 18.270600203458798,
      "grad_norm": 37.799190521240234,
      "learning_rate": 3.1729399796541204e-05,
      "loss": 2.1857,
      "step": 35920
    },
    {
      "epoch": 18.275686673448625,
      "grad_norm": 31.390975952148438,
      "learning_rate": 3.1724313326551373e-05,
      "loss": 2.2481,
      "step": 35930
    },
    {
      "epoch": 18.280773143438452,
      "grad_norm": 36.29449462890625,
      "learning_rate": 3.171922685656154e-05,
      "loss": 2.3079,
      "step": 35940
    },
    {
      "epoch": 18.28585961342828,
      "grad_norm": 33.78176498413086,
      "learning_rate": 3.171414038657172e-05,
      "loss": 2.2528,
      "step": 35950
    },
    {
      "epoch": 18.290946083418106,
      "grad_norm": 27.059247970581055,
      "learning_rate": 3.17090539165819e-05,
      "loss": 2.2521,
      "step": 35960
    },
    {
      "epoch": 18.296032553407933,
      "grad_norm": 25.172204971313477,
      "learning_rate": 3.1703967446592067e-05,
      "loss": 2.2725,
      "step": 35970
    },
    {
      "epoch": 18.301119023397764,
      "grad_norm": 29.120058059692383,
      "learning_rate": 3.169888097660224e-05,
      "loss": 2.3019,
      "step": 35980
    },
    {
      "epoch": 18.30620549338759,
      "grad_norm": 33.33546447753906,
      "learning_rate": 3.169379450661241e-05,
      "loss": 2.2222,
      "step": 35990
    },
    {
      "epoch": 18.311291963377418,
      "grad_norm": 35.800907135009766,
      "learning_rate": 3.168870803662258e-05,
      "loss": 2.3048,
      "step": 36000
    },
    {
      "epoch": 18.316378433367245,
      "grad_norm": 26.24353790283203,
      "learning_rate": 3.168362156663276e-05,
      "loss": 2.2378,
      "step": 36010
    },
    {
      "epoch": 18.32146490335707,
      "grad_norm": 36.56911087036133,
      "learning_rate": 3.167853509664293e-05,
      "loss": 2.2736,
      "step": 36020
    },
    {
      "epoch": 18.3265513733469,
      "grad_norm": 42.10227584838867,
      "learning_rate": 3.16734486266531e-05,
      "loss": 2.2345,
      "step": 36030
    },
    {
      "epoch": 18.331637843336726,
      "grad_norm": 31.682815551757812,
      "learning_rate": 3.1668362156663276e-05,
      "loss": 2.2457,
      "step": 36040
    },
    {
      "epoch": 18.336724313326553,
      "grad_norm": 30.798171997070312,
      "learning_rate": 3.1663275686673446e-05,
      "loss": 2.2,
      "step": 36050
    },
    {
      "epoch": 18.34181078331638,
      "grad_norm": 26.631969451904297,
      "learning_rate": 3.165818921668362e-05,
      "loss": 2.2346,
      "step": 36060
    },
    {
      "epoch": 18.346897253306206,
      "grad_norm": 32.8615608215332,
      "learning_rate": 3.16531027466938e-05,
      "loss": 2.184,
      "step": 36070
    },
    {
      "epoch": 18.351983723296033,
      "grad_norm": 35.81764602661133,
      "learning_rate": 3.164801627670397e-05,
      "loss": 2.2539,
      "step": 36080
    },
    {
      "epoch": 18.35707019328586,
      "grad_norm": 23.826221466064453,
      "learning_rate": 3.1642929806714146e-05,
      "loss": 2.2698,
      "step": 36090
    },
    {
      "epoch": 18.362156663275687,
      "grad_norm": 34.92192459106445,
      "learning_rate": 3.1637843336724316e-05,
      "loss": 2.2138,
      "step": 36100
    },
    {
      "epoch": 18.367243133265514,
      "grad_norm": 39.24163055419922,
      "learning_rate": 3.1632756866734486e-05,
      "loss": 2.1799,
      "step": 36110
    },
    {
      "epoch": 18.37232960325534,
      "grad_norm": 34.25535583496094,
      "learning_rate": 3.162767039674466e-05,
      "loss": 2.2568,
      "step": 36120
    },
    {
      "epoch": 18.37741607324517,
      "grad_norm": 36.526031494140625,
      "learning_rate": 3.162258392675483e-05,
      "loss": 2.3026,
      "step": 36130
    },
    {
      "epoch": 18.382502543234995,
      "grad_norm": 30.077247619628906,
      "learning_rate": 3.1617497456765e-05,
      "loss": 2.3354,
      "step": 36140
    },
    {
      "epoch": 18.387589013224822,
      "grad_norm": 29.11313247680664,
      "learning_rate": 3.161241098677518e-05,
      "loss": 2.2463,
      "step": 36150
    },
    {
      "epoch": 18.39267548321465,
      "grad_norm": 29.2873477935791,
      "learning_rate": 3.1607324516785356e-05,
      "loss": 2.2097,
      "step": 36160
    },
    {
      "epoch": 18.397761953204476,
      "grad_norm": 31.294679641723633,
      "learning_rate": 3.1602238046795525e-05,
      "loss": 2.321,
      "step": 36170
    },
    {
      "epoch": 18.402848423194303,
      "grad_norm": 32.971927642822266,
      "learning_rate": 3.15971515768057e-05,
      "loss": 2.2136,
      "step": 36180
    },
    {
      "epoch": 18.40793489318413,
      "grad_norm": 26.628944396972656,
      "learning_rate": 3.159206510681587e-05,
      "loss": 2.2424,
      "step": 36190
    },
    {
      "epoch": 18.413021363173957,
      "grad_norm": 38.70140838623047,
      "learning_rate": 3.158697863682604e-05,
      "loss": 2.0957,
      "step": 36200
    },
    {
      "epoch": 18.418107833163784,
      "grad_norm": 30.680408477783203,
      "learning_rate": 3.158189216683622e-05,
      "loss": 2.1536,
      "step": 36210
    },
    {
      "epoch": 18.42319430315361,
      "grad_norm": 29.023767471313477,
      "learning_rate": 3.157680569684639e-05,
      "loss": 2.2792,
      "step": 36220
    },
    {
      "epoch": 18.428280773143438,
      "grad_norm": 36.4109992980957,
      "learning_rate": 3.157171922685656e-05,
      "loss": 2.2233,
      "step": 36230
    },
    {
      "epoch": 18.433367243133265,
      "grad_norm": 33.435298919677734,
      "learning_rate": 3.1566632756866735e-05,
      "loss": 2.2065,
      "step": 36240
    },
    {
      "epoch": 18.438453713123092,
      "grad_norm": 29.72176742553711,
      "learning_rate": 3.156154628687691e-05,
      "loss": 2.2662,
      "step": 36250
    },
    {
      "epoch": 18.44354018311292,
      "grad_norm": 27.14434051513672,
      "learning_rate": 3.155645981688708e-05,
      "loss": 2.2689,
      "step": 36260
    },
    {
      "epoch": 18.448626653102746,
      "grad_norm": 29.228769302368164,
      "learning_rate": 3.155137334689726e-05,
      "loss": 2.2149,
      "step": 36270
    },
    {
      "epoch": 18.453713123092573,
      "grad_norm": 31.20172882080078,
      "learning_rate": 3.154628687690743e-05,
      "loss": 2.2566,
      "step": 36280
    },
    {
      "epoch": 18.4587995930824,
      "grad_norm": 35.50203323364258,
      "learning_rate": 3.15412004069176e-05,
      "loss": 2.2204,
      "step": 36290
    },
    {
      "epoch": 18.463886063072227,
      "grad_norm": 30.660018920898438,
      "learning_rate": 3.1536113936927775e-05,
      "loss": 2.3563,
      "step": 36300
    },
    {
      "epoch": 18.468972533062054,
      "grad_norm": 25.792442321777344,
      "learning_rate": 3.1531027466937945e-05,
      "loss": 2.3907,
      "step": 36310
    },
    {
      "epoch": 18.47405900305188,
      "grad_norm": 27.48418617248535,
      "learning_rate": 3.1525940996948115e-05,
      "loss": 2.2431,
      "step": 36320
    },
    {
      "epoch": 18.479145473041708,
      "grad_norm": 39.462039947509766,
      "learning_rate": 3.152085452695829e-05,
      "loss": 2.3447,
      "step": 36330
    },
    {
      "epoch": 18.484231943031535,
      "grad_norm": 26.47381019592285,
      "learning_rate": 3.151576805696846e-05,
      "loss": 2.2729,
      "step": 36340
    },
    {
      "epoch": 18.489318413021362,
      "grad_norm": 29.427316665649414,
      "learning_rate": 3.151068158697864e-05,
      "loss": 2.2472,
      "step": 36350
    },
    {
      "epoch": 18.49440488301119,
      "grad_norm": 35.93485641479492,
      "learning_rate": 3.1505595116988814e-05,
      "loss": 2.2824,
      "step": 36360
    },
    {
      "epoch": 18.499491353001016,
      "grad_norm": 33.85837936401367,
      "learning_rate": 3.1500508646998984e-05,
      "loss": 2.3316,
      "step": 36370
    },
    {
      "epoch": 18.504577822990843,
      "grad_norm": 35.79813003540039,
      "learning_rate": 3.149542217700916e-05,
      "loss": 2.2925,
      "step": 36380
    },
    {
      "epoch": 18.50966429298067,
      "grad_norm": 28.085418701171875,
      "learning_rate": 3.149033570701933e-05,
      "loss": 2.2157,
      "step": 36390
    },
    {
      "epoch": 18.514750762970497,
      "grad_norm": 29.212209701538086,
      "learning_rate": 3.14852492370295e-05,
      "loss": 2.2942,
      "step": 36400
    },
    {
      "epoch": 18.519837232960327,
      "grad_norm": 33.14263153076172,
      "learning_rate": 3.148016276703968e-05,
      "loss": 2.1976,
      "step": 36410
    },
    {
      "epoch": 18.524923702950154,
      "grad_norm": 30.42007064819336,
      "learning_rate": 3.147507629704985e-05,
      "loss": 2.1755,
      "step": 36420
    },
    {
      "epoch": 18.53001017293998,
      "grad_norm": 26.530025482177734,
      "learning_rate": 3.146998982706002e-05,
      "loss": 2.2296,
      "step": 36430
    },
    {
      "epoch": 18.53509664292981,
      "grad_norm": 34.17891311645508,
      "learning_rate": 3.1464903357070194e-05,
      "loss": 2.2402,
      "step": 36440
    },
    {
      "epoch": 18.540183112919635,
      "grad_norm": 33.135101318359375,
      "learning_rate": 3.145981688708037e-05,
      "loss": 2.278,
      "step": 36450
    },
    {
      "epoch": 18.545269582909462,
      "grad_norm": 27.240495681762695,
      "learning_rate": 3.145473041709054e-05,
      "loss": 2.3376,
      "step": 36460
    },
    {
      "epoch": 18.55035605289929,
      "grad_norm": 31.967018127441406,
      "learning_rate": 3.144964394710072e-05,
      "loss": 2.2696,
      "step": 36470
    },
    {
      "epoch": 18.555442522889116,
      "grad_norm": 37.10983657836914,
      "learning_rate": 3.144455747711089e-05,
      "loss": 2.2256,
      "step": 36480
    },
    {
      "epoch": 18.560528992878943,
      "grad_norm": 39.755523681640625,
      "learning_rate": 3.143947100712106e-05,
      "loss": 2.2534,
      "step": 36490
    },
    {
      "epoch": 18.56561546286877,
      "grad_norm": 32.187705993652344,
      "learning_rate": 3.1434384537131234e-05,
      "loss": 2.2627,
      "step": 36500
    },
    {
      "epoch": 18.570701932858597,
      "grad_norm": 28.01772689819336,
      "learning_rate": 3.1429298067141403e-05,
      "loss": 2.2322,
      "step": 36510
    },
    {
      "epoch": 18.575788402848424,
      "grad_norm": 31.599313735961914,
      "learning_rate": 3.142421159715157e-05,
      "loss": 2.2198,
      "step": 36520
    },
    {
      "epoch": 18.58087487283825,
      "grad_norm": 32.441802978515625,
      "learning_rate": 3.141912512716175e-05,
      "loss": 2.2595,
      "step": 36530
    },
    {
      "epoch": 18.585961342828078,
      "grad_norm": 28.19712257385254,
      "learning_rate": 3.141403865717193e-05,
      "loss": 2.1746,
      "step": 36540
    },
    {
      "epoch": 18.591047812817905,
      "grad_norm": 31.28108787536621,
      "learning_rate": 3.1408952187182097e-05,
      "loss": 2.2639,
      "step": 36550
    },
    {
      "epoch": 18.596134282807732,
      "grad_norm": 31.53400230407715,
      "learning_rate": 3.140386571719227e-05,
      "loss": 2.2875,
      "step": 36560
    },
    {
      "epoch": 18.60122075279756,
      "grad_norm": 27.014657974243164,
      "learning_rate": 3.139877924720244e-05,
      "loss": 2.2496,
      "step": 36570
    },
    {
      "epoch": 18.606307222787386,
      "grad_norm": 31.28441047668457,
      "learning_rate": 3.139369277721261e-05,
      "loss": 2.2565,
      "step": 36580
    },
    {
      "epoch": 18.611393692777213,
      "grad_norm": 33.473670959472656,
      "learning_rate": 3.138860630722279e-05,
      "loss": 2.2384,
      "step": 36590
    },
    {
      "epoch": 18.61648016276704,
      "grad_norm": 25.94448471069336,
      "learning_rate": 3.138351983723296e-05,
      "loss": 2.1754,
      "step": 36600
    },
    {
      "epoch": 18.621566632756867,
      "grad_norm": 32.10581588745117,
      "learning_rate": 3.1378433367243136e-05,
      "loss": 2.1493,
      "step": 36610
    },
    {
      "epoch": 18.626653102746694,
      "grad_norm": 31.13042449951172,
      "learning_rate": 3.1373346897253306e-05,
      "loss": 2.2943,
      "step": 36620
    },
    {
      "epoch": 18.63173957273652,
      "grad_norm": 34.017478942871094,
      "learning_rate": 3.1368260427263476e-05,
      "loss": 2.1942,
      "step": 36630
    },
    {
      "epoch": 18.636826042726348,
      "grad_norm": 25.875295639038086,
      "learning_rate": 3.136317395727365e-05,
      "loss": 2.2007,
      "step": 36640
    },
    {
      "epoch": 18.641912512716175,
      "grad_norm": 37.40751266479492,
      "learning_rate": 3.135808748728383e-05,
      "loss": 2.1675,
      "step": 36650
    },
    {
      "epoch": 18.646998982706002,
      "grad_norm": 27.773977279663086,
      "learning_rate": 3.1353001017294e-05,
      "loss": 2.3251,
      "step": 36660
    },
    {
      "epoch": 18.65208545269583,
      "grad_norm": 34.50885009765625,
      "learning_rate": 3.1347914547304176e-05,
      "loss": 2.2371,
      "step": 36670
    },
    {
      "epoch": 18.657171922685656,
      "grad_norm": 39.90315246582031,
      "learning_rate": 3.1342828077314346e-05,
      "loss": 2.2495,
      "step": 36680
    },
    {
      "epoch": 18.662258392675483,
      "grad_norm": 28.17784309387207,
      "learning_rate": 3.1337741607324516e-05,
      "loss": 2.2384,
      "step": 36690
    },
    {
      "epoch": 18.66734486266531,
      "grad_norm": 33.7894287109375,
      "learning_rate": 3.133265513733469e-05,
      "loss": 2.2002,
      "step": 36700
    },
    {
      "epoch": 18.672431332655137,
      "grad_norm": 32.549835205078125,
      "learning_rate": 3.132756866734486e-05,
      "loss": 2.2668,
      "step": 36710
    },
    {
      "epoch": 18.677517802644964,
      "grad_norm": 38.09974670410156,
      "learning_rate": 3.132248219735503e-05,
      "loss": 2.23,
      "step": 36720
    },
    {
      "epoch": 18.68260427263479,
      "grad_norm": 25.484607696533203,
      "learning_rate": 3.131739572736521e-05,
      "loss": 2.2567,
      "step": 36730
    },
    {
      "epoch": 18.687690742624618,
      "grad_norm": 28.452552795410156,
      "learning_rate": 3.1312309257375386e-05,
      "loss": 2.187,
      "step": 36740
    },
    {
      "epoch": 18.692777212614445,
      "grad_norm": 38.21186828613281,
      "learning_rate": 3.1307222787385555e-05,
      "loss": 2.242,
      "step": 36750
    },
    {
      "epoch": 18.69786368260427,
      "grad_norm": 32.85657501220703,
      "learning_rate": 3.130213631739573e-05,
      "loss": 2.2194,
      "step": 36760
    },
    {
      "epoch": 18.7029501525941,
      "grad_norm": 34.605918884277344,
      "learning_rate": 3.12970498474059e-05,
      "loss": 2.2747,
      "step": 36770
    },
    {
      "epoch": 18.708036622583926,
      "grad_norm": 28.697128295898438,
      "learning_rate": 3.129196337741607e-05,
      "loss": 2.1784,
      "step": 36780
    },
    {
      "epoch": 18.713123092573753,
      "grad_norm": 26.608747482299805,
      "learning_rate": 3.128687690742625e-05,
      "loss": 2.2286,
      "step": 36790
    },
    {
      "epoch": 18.71820956256358,
      "grad_norm": 44.43345642089844,
      "learning_rate": 3.128179043743642e-05,
      "loss": 2.2659,
      "step": 36800
    },
    {
      "epoch": 18.723296032553407,
      "grad_norm": 32.94095993041992,
      "learning_rate": 3.127670396744659e-05,
      "loss": 2.1518,
      "step": 36810
    },
    {
      "epoch": 18.728382502543234,
      "grad_norm": 36.0985107421875,
      "learning_rate": 3.1271617497456765e-05,
      "loss": 2.1961,
      "step": 36820
    },
    {
      "epoch": 18.73346897253306,
      "grad_norm": 26.859817504882812,
      "learning_rate": 3.126653102746694e-05,
      "loss": 2.2524,
      "step": 36830
    },
    {
      "epoch": 18.738555442522888,
      "grad_norm": 30.253103256225586,
      "learning_rate": 3.126144455747711e-05,
      "loss": 2.2,
      "step": 36840
    },
    {
      "epoch": 18.743641912512714,
      "grad_norm": 28.5596981048584,
      "learning_rate": 3.125635808748729e-05,
      "loss": 2.2298,
      "step": 36850
    },
    {
      "epoch": 18.74872838250254,
      "grad_norm": 37.204532623291016,
      "learning_rate": 3.125127161749746e-05,
      "loss": 2.2365,
      "step": 36860
    },
    {
      "epoch": 18.753814852492372,
      "grad_norm": 25.232999801635742,
      "learning_rate": 3.1246185147507635e-05,
      "loss": 2.2693,
      "step": 36870
    },
    {
      "epoch": 18.7589013224822,
      "grad_norm": 25.306171417236328,
      "learning_rate": 3.1241098677517805e-05,
      "loss": 2.1979,
      "step": 36880
    },
    {
      "epoch": 18.763987792472026,
      "grad_norm": 30.578453063964844,
      "learning_rate": 3.1236012207527975e-05,
      "loss": 2.2736,
      "step": 36890
    },
    {
      "epoch": 18.769074262461853,
      "grad_norm": 35.65650177001953,
      "learning_rate": 3.123092573753815e-05,
      "loss": 2.2784,
      "step": 36900
    },
    {
      "epoch": 18.77416073245168,
      "grad_norm": 28.65064811706543,
      "learning_rate": 3.122583926754832e-05,
      "loss": 2.337,
      "step": 36910
    },
    {
      "epoch": 18.779247202441507,
      "grad_norm": 29.361061096191406,
      "learning_rate": 3.12207527975585e-05,
      "loss": 2.2679,
      "step": 36920
    },
    {
      "epoch": 18.784333672431334,
      "grad_norm": 35.037803649902344,
      "learning_rate": 3.1215666327568675e-05,
      "loss": 2.1715,
      "step": 36930
    },
    {
      "epoch": 18.78942014242116,
      "grad_norm": 38.99393081665039,
      "learning_rate": 3.1210579857578844e-05,
      "loss": 2.1493,
      "step": 36940
    },
    {
      "epoch": 18.794506612410988,
      "grad_norm": 30.918296813964844,
      "learning_rate": 3.1205493387589014e-05,
      "loss": 2.3061,
      "step": 36950
    },
    {
      "epoch": 18.799593082400815,
      "grad_norm": 32.04536437988281,
      "learning_rate": 3.120040691759919e-05,
      "loss": 2.2235,
      "step": 36960
    },
    {
      "epoch": 18.804679552390642,
      "grad_norm": 30.917360305786133,
      "learning_rate": 3.119532044760936e-05,
      "loss": 2.1736,
      "step": 36970
    },
    {
      "epoch": 18.80976602238047,
      "grad_norm": 34.18051528930664,
      "learning_rate": 3.119023397761953e-05,
      "loss": 2.1229,
      "step": 36980
    },
    {
      "epoch": 18.814852492370296,
      "grad_norm": 34.462554931640625,
      "learning_rate": 3.118514750762971e-05,
      "loss": 2.2339,
      "step": 36990
    },
    {
      "epoch": 18.819938962360123,
      "grad_norm": 28.544952392578125,
      "learning_rate": 3.118006103763988e-05,
      "loss": 2.2057,
      "step": 37000
    },
    {
      "epoch": 18.82502543234995,
      "grad_norm": 28.362102508544922,
      "learning_rate": 3.117497456765005e-05,
      "loss": 2.1865,
      "step": 37010
    },
    {
      "epoch": 18.830111902339777,
      "grad_norm": 32.56584548950195,
      "learning_rate": 3.1169888097660224e-05,
      "loss": 2.2037,
      "step": 37020
    },
    {
      "epoch": 18.835198372329604,
      "grad_norm": 36.149635314941406,
      "learning_rate": 3.11648016276704e-05,
      "loss": 2.3314,
      "step": 37030
    },
    {
      "epoch": 18.84028484231943,
      "grad_norm": 30.591556549072266,
      "learning_rate": 3.115971515768057e-05,
      "loss": 2.2325,
      "step": 37040
    },
    {
      "epoch": 18.845371312309258,
      "grad_norm": 29.388042449951172,
      "learning_rate": 3.115462868769075e-05,
      "loss": 2.2481,
      "step": 37050
    },
    {
      "epoch": 18.850457782299085,
      "grad_norm": 22.799646377563477,
      "learning_rate": 3.114954221770092e-05,
      "loss": 2.2365,
      "step": 37060
    },
    {
      "epoch": 18.85554425228891,
      "grad_norm": 38.22380447387695,
      "learning_rate": 3.114445574771109e-05,
      "loss": 2.2828,
      "step": 37070
    },
    {
      "epoch": 18.86063072227874,
      "grad_norm": 27.75473976135254,
      "learning_rate": 3.1139369277721264e-05,
      "loss": 2.1937,
      "step": 37080
    },
    {
      "epoch": 18.865717192268566,
      "grad_norm": 29.853132247924805,
      "learning_rate": 3.1134282807731433e-05,
      "loss": 2.2751,
      "step": 37090
    },
    {
      "epoch": 18.870803662258393,
      "grad_norm": 37.16269302368164,
      "learning_rate": 3.11291963377416e-05,
      "loss": 2.2064,
      "step": 37100
    },
    {
      "epoch": 18.87589013224822,
      "grad_norm": 31.378665924072266,
      "learning_rate": 3.112410986775178e-05,
      "loss": 2.1906,
      "step": 37110
    },
    {
      "epoch": 18.880976602238047,
      "grad_norm": 30.472702026367188,
      "learning_rate": 3.111902339776196e-05,
      "loss": 2.244,
      "step": 37120
    },
    {
      "epoch": 18.886063072227874,
      "grad_norm": 32.30229949951172,
      "learning_rate": 3.111393692777213e-05,
      "loss": 2.2389,
      "step": 37130
    },
    {
      "epoch": 18.8911495422177,
      "grad_norm": 29.590591430664062,
      "learning_rate": 3.11088504577823e-05,
      "loss": 2.2295,
      "step": 37140
    },
    {
      "epoch": 18.896236012207527,
      "grad_norm": 29.976293563842773,
      "learning_rate": 3.110376398779247e-05,
      "loss": 2.2739,
      "step": 37150
    },
    {
      "epoch": 18.901322482197354,
      "grad_norm": 31.97064208984375,
      "learning_rate": 3.109867751780265e-05,
      "loss": 2.1766,
      "step": 37160
    },
    {
      "epoch": 18.90640895218718,
      "grad_norm": 26.99700927734375,
      "learning_rate": 3.109359104781282e-05,
      "loss": 2.1344,
      "step": 37170
    },
    {
      "epoch": 18.91149542217701,
      "grad_norm": 26.45164680480957,
      "learning_rate": 3.108850457782299e-05,
      "loss": 2.2197,
      "step": 37180
    },
    {
      "epoch": 18.916581892166835,
      "grad_norm": 29.182392120361328,
      "learning_rate": 3.1083418107833166e-05,
      "loss": 2.2232,
      "step": 37190
    },
    {
      "epoch": 18.921668362156662,
      "grad_norm": 33.493743896484375,
      "learning_rate": 3.1078331637843336e-05,
      "loss": 2.2319,
      "step": 37200
    },
    {
      "epoch": 18.92675483214649,
      "grad_norm": 31.71807289123535,
      "learning_rate": 3.107324516785351e-05,
      "loss": 2.2022,
      "step": 37210
    },
    {
      "epoch": 18.931841302136316,
      "grad_norm": 32.5358772277832,
      "learning_rate": 3.106815869786369e-05,
      "loss": 2.1711,
      "step": 37220
    },
    {
      "epoch": 18.936927772126143,
      "grad_norm": 36.877479553222656,
      "learning_rate": 3.106307222787386e-05,
      "loss": 2.2163,
      "step": 37230
    },
    {
      "epoch": 18.94201424211597,
      "grad_norm": 34.39481735229492,
      "learning_rate": 3.105798575788403e-05,
      "loss": 2.2053,
      "step": 37240
    },
    {
      "epoch": 18.947100712105797,
      "grad_norm": 31.67207145690918,
      "learning_rate": 3.1052899287894206e-05,
      "loss": 2.3242,
      "step": 37250
    },
    {
      "epoch": 18.952187182095624,
      "grad_norm": 35.813560485839844,
      "learning_rate": 3.1047812817904376e-05,
      "loss": 2.1994,
      "step": 37260
    },
    {
      "epoch": 18.95727365208545,
      "grad_norm": 36.0975341796875,
      "learning_rate": 3.1042726347914546e-05,
      "loss": 2.2116,
      "step": 37270
    },
    {
      "epoch": 18.962360122075278,
      "grad_norm": 28.741592407226562,
      "learning_rate": 3.103763987792472e-05,
      "loss": 2.1252,
      "step": 37280
    },
    {
      "epoch": 18.967446592065105,
      "grad_norm": 30.687950134277344,
      "learning_rate": 3.103255340793489e-05,
      "loss": 2.2579,
      "step": 37290
    },
    {
      "epoch": 18.972533062054932,
      "grad_norm": 31.58506965637207,
      "learning_rate": 3.102746693794506e-05,
      "loss": 2.2264,
      "step": 37300
    },
    {
      "epoch": 18.977619532044763,
      "grad_norm": 30.07839012145996,
      "learning_rate": 3.102238046795524e-05,
      "loss": 2.1965,
      "step": 37310
    },
    {
      "epoch": 18.98270600203459,
      "grad_norm": 28.0903377532959,
      "learning_rate": 3.1017293997965416e-05,
      "loss": 2.3033,
      "step": 37320
    },
    {
      "epoch": 18.987792472024417,
      "grad_norm": 26.612821578979492,
      "learning_rate": 3.1012207527975585e-05,
      "loss": 2.2943,
      "step": 37330
    },
    {
      "epoch": 18.992878942014244,
      "grad_norm": 30.896991729736328,
      "learning_rate": 3.100712105798576e-05,
      "loss": 2.1585,
      "step": 37340
    },
    {
      "epoch": 18.99796541200407,
      "grad_norm": 28.79961585998535,
      "learning_rate": 3.100203458799593e-05,
      "loss": 2.2171,
      "step": 37350
    },
    {
      "epoch": 19.0,
      "eval_loss": 4.284923076629639,
      "eval_runtime": 2.7311,
      "eval_samples_per_second": 1016.057,
      "eval_steps_per_second": 127.053,
      "step": 37354
    },
    {
      "epoch": 19.003051881993898,
      "grad_norm": 36.33942413330078,
      "learning_rate": 3.09969481180061e-05,
      "loss": 2.2106,
      "step": 37360
    },
    {
      "epoch": 19.008138351983725,
      "grad_norm": 28.87208366394043,
      "learning_rate": 3.099186164801628e-05,
      "loss": 2.1862,
      "step": 37370
    },
    {
      "epoch": 19.01322482197355,
      "grad_norm": 32.75699996948242,
      "learning_rate": 3.098677517802645e-05,
      "loss": 2.218,
      "step": 37380
    },
    {
      "epoch": 19.01831129196338,
      "grad_norm": 31.221742630004883,
      "learning_rate": 3.098168870803662e-05,
      "loss": 2.1434,
      "step": 37390
    },
    {
      "epoch": 19.023397761953206,
      "grad_norm": 32.02903366088867,
      "learning_rate": 3.0976602238046795e-05,
      "loss": 2.141,
      "step": 37400
    },
    {
      "epoch": 19.028484231943033,
      "grad_norm": 31.346643447875977,
      "learning_rate": 3.097151576805697e-05,
      "loss": 2.159,
      "step": 37410
    },
    {
      "epoch": 19.03357070193286,
      "grad_norm": 30.275747299194336,
      "learning_rate": 3.096642929806715e-05,
      "loss": 2.1734,
      "step": 37420
    },
    {
      "epoch": 19.038657171922686,
      "grad_norm": 30.320362091064453,
      "learning_rate": 3.096134282807732e-05,
      "loss": 2.1907,
      "step": 37430
    },
    {
      "epoch": 19.043743641912513,
      "grad_norm": 27.27620506286621,
      "learning_rate": 3.095625635808749e-05,
      "loss": 2.1149,
      "step": 37440
    },
    {
      "epoch": 19.04883011190234,
      "grad_norm": 41.916690826416016,
      "learning_rate": 3.0951169888097665e-05,
      "loss": 2.2527,
      "step": 37450
    },
    {
      "epoch": 19.053916581892167,
      "grad_norm": 32.165061950683594,
      "learning_rate": 3.0946083418107835e-05,
      "loss": 2.1436,
      "step": 37460
    },
    {
      "epoch": 19.059003051881994,
      "grad_norm": 34.973106384277344,
      "learning_rate": 3.0940996948118005e-05,
      "loss": 2.1586,
      "step": 37470
    },
    {
      "epoch": 19.06408952187182,
      "grad_norm": 30.12891387939453,
      "learning_rate": 3.093591047812818e-05,
      "loss": 2.1693,
      "step": 37480
    },
    {
      "epoch": 19.06917599186165,
      "grad_norm": 40.39470672607422,
      "learning_rate": 3.093082400813835e-05,
      "loss": 2.2416,
      "step": 37490
    },
    {
      "epoch": 19.074262461851475,
      "grad_norm": 31.337682723999023,
      "learning_rate": 3.092573753814853e-05,
      "loss": 2.1177,
      "step": 37500
    },
    {
      "epoch": 19.079348931841302,
      "grad_norm": 27.994373321533203,
      "learning_rate": 3.0920651068158705e-05,
      "loss": 2.1835,
      "step": 37510
    },
    {
      "epoch": 19.08443540183113,
      "grad_norm": 38.41002655029297,
      "learning_rate": 3.0915564598168874e-05,
      "loss": 2.1114,
      "step": 37520
    },
    {
      "epoch": 19.089521871820956,
      "grad_norm": 31.115041732788086,
      "learning_rate": 3.0910478128179044e-05,
      "loss": 2.1104,
      "step": 37530
    },
    {
      "epoch": 19.094608341810783,
      "grad_norm": 30.14217185974121,
      "learning_rate": 3.090539165818922e-05,
      "loss": 2.1457,
      "step": 37540
    },
    {
      "epoch": 19.09969481180061,
      "grad_norm": 32.31148910522461,
      "learning_rate": 3.090030518819939e-05,
      "loss": 2.185,
      "step": 37550
    },
    {
      "epoch": 19.104781281790437,
      "grad_norm": 33.8387565612793,
      "learning_rate": 3.089521871820956e-05,
      "loss": 2.228,
      "step": 37560
    },
    {
      "epoch": 19.109867751780264,
      "grad_norm": 35.78815460205078,
      "learning_rate": 3.089013224821974e-05,
      "loss": 2.1766,
      "step": 37570
    },
    {
      "epoch": 19.11495422177009,
      "grad_norm": 28.73051643371582,
      "learning_rate": 3.088504577822991e-05,
      "loss": 2.2235,
      "step": 37580
    },
    {
      "epoch": 19.120040691759918,
      "grad_norm": 29.792827606201172,
      "learning_rate": 3.0879959308240084e-05,
      "loss": 2.1533,
      "step": 37590
    },
    {
      "epoch": 19.125127161749745,
      "grad_norm": 24.86192512512207,
      "learning_rate": 3.0874872838250254e-05,
      "loss": 2.1805,
      "step": 37600
    },
    {
      "epoch": 19.130213631739572,
      "grad_norm": 27.555788040161133,
      "learning_rate": 3.086978636826043e-05,
      "loss": 2.2119,
      "step": 37610
    },
    {
      "epoch": 19.1353001017294,
      "grad_norm": 43.98219299316406,
      "learning_rate": 3.08646998982706e-05,
      "loss": 2.163,
      "step": 37620
    },
    {
      "epoch": 19.140386571719226,
      "grad_norm": 36.90776824951172,
      "learning_rate": 3.085961342828078e-05,
      "loss": 2.2461,
      "step": 37630
    },
    {
      "epoch": 19.145473041709053,
      "grad_norm": 31.285926818847656,
      "learning_rate": 3.085452695829095e-05,
      "loss": 2.1503,
      "step": 37640
    },
    {
      "epoch": 19.15055951169888,
      "grad_norm": 32.849876403808594,
      "learning_rate": 3.084944048830112e-05,
      "loss": 2.1317,
      "step": 37650
    },
    {
      "epoch": 19.155645981688707,
      "grad_norm": 42.16718292236328,
      "learning_rate": 3.0844354018311294e-05,
      "loss": 2.1975,
      "step": 37660
    },
    {
      "epoch": 19.160732451678534,
      "grad_norm": 35.35713577270508,
      "learning_rate": 3.0839267548321463e-05,
      "loss": 2.1784,
      "step": 37670
    },
    {
      "epoch": 19.16581892166836,
      "grad_norm": 32.503990173339844,
      "learning_rate": 3.083418107833164e-05,
      "loss": 2.1767,
      "step": 37680
    },
    {
      "epoch": 19.170905391658188,
      "grad_norm": 32.52722930908203,
      "learning_rate": 3.082909460834181e-05,
      "loss": 2.1787,
      "step": 37690
    },
    {
      "epoch": 19.175991861648015,
      "grad_norm": 31.875089645385742,
      "learning_rate": 3.082400813835199e-05,
      "loss": 2.2197,
      "step": 37700
    },
    {
      "epoch": 19.181078331637842,
      "grad_norm": 33.55437088012695,
      "learning_rate": 3.0818921668362163e-05,
      "loss": 2.1663,
      "step": 37710
    },
    {
      "epoch": 19.18616480162767,
      "grad_norm": 30.22682762145996,
      "learning_rate": 3.081383519837233e-05,
      "loss": 2.2179,
      "step": 37720
    },
    {
      "epoch": 19.191251271617496,
      "grad_norm": 30.505285263061523,
      "learning_rate": 3.08087487283825e-05,
      "loss": 2.3063,
      "step": 37730
    },
    {
      "epoch": 19.196337741607323,
      "grad_norm": 30.71319580078125,
      "learning_rate": 3.080366225839268e-05,
      "loss": 2.1491,
      "step": 37740
    },
    {
      "epoch": 19.20142421159715,
      "grad_norm": 34.211544036865234,
      "learning_rate": 3.079857578840285e-05,
      "loss": 2.1752,
      "step": 37750
    },
    {
      "epoch": 19.20651068158698,
      "grad_norm": 34.00466537475586,
      "learning_rate": 3.079348931841302e-05,
      "loss": 2.2168,
      "step": 37760
    },
    {
      "epoch": 19.211597151576807,
      "grad_norm": 40.436824798583984,
      "learning_rate": 3.0788402848423196e-05,
      "loss": 2.1407,
      "step": 37770
    },
    {
      "epoch": 19.216683621566634,
      "grad_norm": 30.447242736816406,
      "learning_rate": 3.0783316378433366e-05,
      "loss": 2.2031,
      "step": 37780
    },
    {
      "epoch": 19.22177009155646,
      "grad_norm": 38.12314224243164,
      "learning_rate": 3.077822990844354e-05,
      "loss": 2.3189,
      "step": 37790
    },
    {
      "epoch": 19.22685656154629,
      "grad_norm": 28.531877517700195,
      "learning_rate": 3.077314343845372e-05,
      "loss": 2.2972,
      "step": 37800
    },
    {
      "epoch": 19.231943031536115,
      "grad_norm": 28.852582931518555,
      "learning_rate": 3.076805696846389e-05,
      "loss": 2.1877,
      "step": 37810
    },
    {
      "epoch": 19.237029501525942,
      "grad_norm": 29.322200775146484,
      "learning_rate": 3.076297049847406e-05,
      "loss": 2.2937,
      "step": 37820
    },
    {
      "epoch": 19.24211597151577,
      "grad_norm": 35.13043975830078,
      "learning_rate": 3.0757884028484236e-05,
      "loss": 2.1515,
      "step": 37830
    },
    {
      "epoch": 19.247202441505596,
      "grad_norm": 30.613508224487305,
      "learning_rate": 3.0752797558494406e-05,
      "loss": 2.1727,
      "step": 37840
    },
    {
      "epoch": 19.252288911495423,
      "grad_norm": 27.043052673339844,
      "learning_rate": 3.0747711088504576e-05,
      "loss": 2.2105,
      "step": 37850
    },
    {
      "epoch": 19.25737538148525,
      "grad_norm": 28.89836311340332,
      "learning_rate": 3.074262461851475e-05,
      "loss": 2.2149,
      "step": 37860
    },
    {
      "epoch": 19.262461851475077,
      "grad_norm": 44.067745208740234,
      "learning_rate": 3.073753814852492e-05,
      "loss": 2.0835,
      "step": 37870
    },
    {
      "epoch": 19.267548321464904,
      "grad_norm": 27.9044189453125,
      "learning_rate": 3.07324516785351e-05,
      "loss": 2.2057,
      "step": 37880
    },
    {
      "epoch": 19.27263479145473,
      "grad_norm": 30.861984252929688,
      "learning_rate": 3.0727365208545276e-05,
      "loss": 2.0948,
      "step": 37890
    },
    {
      "epoch": 19.277721261444558,
      "grad_norm": 33.496788024902344,
      "learning_rate": 3.0722278738555446e-05,
      "loss": 2.1902,
      "step": 37900
    },
    {
      "epoch": 19.282807731434385,
      "grad_norm": 31.624446868896484,
      "learning_rate": 3.0717192268565615e-05,
      "loss": 2.1348,
      "step": 37910
    },
    {
      "epoch": 19.287894201424212,
      "grad_norm": 38.350807189941406,
      "learning_rate": 3.071210579857579e-05,
      "loss": 2.1465,
      "step": 37920
    },
    {
      "epoch": 19.29298067141404,
      "grad_norm": 26.611560821533203,
      "learning_rate": 3.070701932858596e-05,
      "loss": 2.1923,
      "step": 37930
    },
    {
      "epoch": 19.298067141403866,
      "grad_norm": 28.517627716064453,
      "learning_rate": 3.070193285859613e-05,
      "loss": 2.288,
      "step": 37940
    },
    {
      "epoch": 19.303153611393693,
      "grad_norm": 30.494569778442383,
      "learning_rate": 3.069684638860631e-05,
      "loss": 2.2041,
      "step": 37950
    },
    {
      "epoch": 19.30824008138352,
      "grad_norm": 32.843326568603516,
      "learning_rate": 3.069175991861648e-05,
      "loss": 2.1125,
      "step": 37960
    },
    {
      "epoch": 19.313326551373347,
      "grad_norm": 26.897520065307617,
      "learning_rate": 3.0686673448626655e-05,
      "loss": 2.2241,
      "step": 37970
    },
    {
      "epoch": 19.318413021363174,
      "grad_norm": 39.35092544555664,
      "learning_rate": 3.0681586978636825e-05,
      "loss": 2.2746,
      "step": 37980
    },
    {
      "epoch": 19.323499491353,
      "grad_norm": 35.166080474853516,
      "learning_rate": 3.0676500508647e-05,
      "loss": 2.2035,
      "step": 37990
    },
    {
      "epoch": 19.328585961342828,
      "grad_norm": 30.08974266052246,
      "learning_rate": 3.067141403865718e-05,
      "loss": 2.108,
      "step": 38000
    },
    {
      "epoch": 19.333672431332655,
      "grad_norm": 32.494380950927734,
      "learning_rate": 3.066632756866735e-05,
      "loss": 2.2441,
      "step": 38010
    },
    {
      "epoch": 19.338758901322482,
      "grad_norm": 28.71830177307129,
      "learning_rate": 3.066124109867752e-05,
      "loss": 2.2319,
      "step": 38020
    },
    {
      "epoch": 19.34384537131231,
      "grad_norm": 29.12831687927246,
      "learning_rate": 3.0656154628687695e-05,
      "loss": 2.1405,
      "step": 38030
    },
    {
      "epoch": 19.348931841302136,
      "grad_norm": 42.32304000854492,
      "learning_rate": 3.0651068158697865e-05,
      "loss": 2.1663,
      "step": 38040
    },
    {
      "epoch": 19.354018311291963,
      "grad_norm": 40.833431243896484,
      "learning_rate": 3.0645981688708035e-05,
      "loss": 2.2283,
      "step": 38050
    },
    {
      "epoch": 19.35910478128179,
      "grad_norm": 30.76321029663086,
      "learning_rate": 3.064089521871821e-05,
      "loss": 2.2135,
      "step": 38060
    },
    {
      "epoch": 19.364191251271617,
      "grad_norm": 35.285133361816406,
      "learning_rate": 3.063580874872838e-05,
      "loss": 2.2698,
      "step": 38070
    },
    {
      "epoch": 19.369277721261444,
      "grad_norm": 29.57953643798828,
      "learning_rate": 3.063072227873856e-05,
      "loss": 2.216,
      "step": 38080
    },
    {
      "epoch": 19.37436419125127,
      "grad_norm": 33.00984573364258,
      "learning_rate": 3.0625635808748735e-05,
      "loss": 2.1843,
      "step": 38090
    },
    {
      "epoch": 19.379450661241098,
      "grad_norm": 28.987895965576172,
      "learning_rate": 3.0620549338758904e-05,
      "loss": 2.3066,
      "step": 38100
    },
    {
      "epoch": 19.384537131230925,
      "grad_norm": 30.45783233642578,
      "learning_rate": 3.0615462868769074e-05,
      "loss": 2.2292,
      "step": 38110
    },
    {
      "epoch": 19.38962360122075,
      "grad_norm": 34.86447525024414,
      "learning_rate": 3.061037639877925e-05,
      "loss": 2.1702,
      "step": 38120
    },
    {
      "epoch": 19.39471007121058,
      "grad_norm": 36.250946044921875,
      "learning_rate": 3.060528992878942e-05,
      "loss": 2.1535,
      "step": 38130
    },
    {
      "epoch": 19.399796541200406,
      "grad_norm": 37.67980194091797,
      "learning_rate": 3.060020345879959e-05,
      "loss": 2.2126,
      "step": 38140
    },
    {
      "epoch": 19.404883011190233,
      "grad_norm": 30.961166381835938,
      "learning_rate": 3.059511698880977e-05,
      "loss": 2.2084,
      "step": 38150
    },
    {
      "epoch": 19.40996948118006,
      "grad_norm": 30.97203826904297,
      "learning_rate": 3.059003051881994e-05,
      "loss": 2.2252,
      "step": 38160
    },
    {
      "epoch": 19.415055951169887,
      "grad_norm": 33.633575439453125,
      "learning_rate": 3.0584944048830114e-05,
      "loss": 2.2249,
      "step": 38170
    },
    {
      "epoch": 19.420142421159714,
      "grad_norm": 34.366371154785156,
      "learning_rate": 3.057985757884029e-05,
      "loss": 2.1598,
      "step": 38180
    },
    {
      "epoch": 19.42522889114954,
      "grad_norm": 34.30484390258789,
      "learning_rate": 3.057477110885046e-05,
      "loss": 2.1551,
      "step": 38190
    },
    {
      "epoch": 19.43031536113937,
      "grad_norm": 44.13010787963867,
      "learning_rate": 3.056968463886063e-05,
      "loss": 2.1709,
      "step": 38200
    },
    {
      "epoch": 19.435401831129198,
      "grad_norm": 36.42731475830078,
      "learning_rate": 3.056459816887081e-05,
      "loss": 2.1801,
      "step": 38210
    },
    {
      "epoch": 19.440488301119025,
      "grad_norm": 34.09759521484375,
      "learning_rate": 3.055951169888098e-05,
      "loss": 2.2046,
      "step": 38220
    },
    {
      "epoch": 19.445574771108852,
      "grad_norm": 35.504085540771484,
      "learning_rate": 3.0554425228891154e-05,
      "loss": 2.1228,
      "step": 38230
    },
    {
      "epoch": 19.45066124109868,
      "grad_norm": 28.959428787231445,
      "learning_rate": 3.0549338758901324e-05,
      "loss": 2.214,
      "step": 38240
    },
    {
      "epoch": 19.455747711088506,
      "grad_norm": 30.45506477355957,
      "learning_rate": 3.0544252288911494e-05,
      "loss": 2.1501,
      "step": 38250
    },
    {
      "epoch": 19.460834181078333,
      "grad_norm": 34.367713928222656,
      "learning_rate": 3.053916581892167e-05,
      "loss": 2.1916,
      "step": 38260
    },
    {
      "epoch": 19.46592065106816,
      "grad_norm": 38.61252975463867,
      "learning_rate": 3.053407934893184e-05,
      "loss": 2.2096,
      "step": 38270
    },
    {
      "epoch": 19.471007121057987,
      "grad_norm": 31.665010452270508,
      "learning_rate": 3.052899287894202e-05,
      "loss": 2.2093,
      "step": 38280
    },
    {
      "epoch": 19.476093591047814,
      "grad_norm": 27.14371109008789,
      "learning_rate": 3.0523906408952193e-05,
      "loss": 2.1668,
      "step": 38290
    },
    {
      "epoch": 19.48118006103764,
      "grad_norm": 34.87406539916992,
      "learning_rate": 3.051881993896236e-05,
      "loss": 2.243,
      "step": 38300
    },
    {
      "epoch": 19.486266531027468,
      "grad_norm": 31.48822021484375,
      "learning_rate": 3.0513733468972533e-05,
      "loss": 2.1792,
      "step": 38310
    },
    {
      "epoch": 19.491353001017295,
      "grad_norm": 32.064720153808594,
      "learning_rate": 3.050864699898271e-05,
      "loss": 2.2034,
      "step": 38320
    },
    {
      "epoch": 19.496439471007122,
      "grad_norm": 32.06694412231445,
      "learning_rate": 3.050356052899288e-05,
      "loss": 2.1074,
      "step": 38330
    },
    {
      "epoch": 19.50152594099695,
      "grad_norm": 28.776079177856445,
      "learning_rate": 3.0498474059003053e-05,
      "loss": 2.147,
      "step": 38340
    },
    {
      "epoch": 19.506612410986776,
      "grad_norm": 29.159271240234375,
      "learning_rate": 3.049338758901323e-05,
      "loss": 2.1016,
      "step": 38350
    },
    {
      "epoch": 19.511698880976603,
      "grad_norm": 33.2237663269043,
      "learning_rate": 3.04883011190234e-05,
      "loss": 2.2175,
      "step": 38360
    },
    {
      "epoch": 19.51678535096643,
      "grad_norm": 35.35186767578125,
      "learning_rate": 3.048321464903357e-05,
      "loss": 2.1421,
      "step": 38370
    },
    {
      "epoch": 19.521871820956257,
      "grad_norm": 36.62859344482422,
      "learning_rate": 3.0478128179043746e-05,
      "loss": 2.2537,
      "step": 38380
    },
    {
      "epoch": 19.526958290946084,
      "grad_norm": 29.36382293701172,
      "learning_rate": 3.0473041709053916e-05,
      "loss": 2.1026,
      "step": 38390
    },
    {
      "epoch": 19.53204476093591,
      "grad_norm": 20.964412689208984,
      "learning_rate": 3.046795523906409e-05,
      "loss": 2.1601,
      "step": 38400
    },
    {
      "epoch": 19.537131230925738,
      "grad_norm": 34.98125457763672,
      "learning_rate": 3.0462868769074266e-05,
      "loss": 2.2741,
      "step": 38410
    },
    {
      "epoch": 19.542217700915565,
      "grad_norm": 31.215877532958984,
      "learning_rate": 3.0457782299084436e-05,
      "loss": 2.2418,
      "step": 38420
    },
    {
      "epoch": 19.54730417090539,
      "grad_norm": 33.68611145019531,
      "learning_rate": 3.0452695829094606e-05,
      "loss": 2.07,
      "step": 38430
    },
    {
      "epoch": 19.55239064089522,
      "grad_norm": 32.615657806396484,
      "learning_rate": 3.0447609359104782e-05,
      "loss": 2.1531,
      "step": 38440
    },
    {
      "epoch": 19.557477110885046,
      "grad_norm": 30.647188186645508,
      "learning_rate": 3.0442522889114956e-05,
      "loss": 2.2128,
      "step": 38450
    },
    {
      "epoch": 19.562563580874873,
      "grad_norm": 37.27157211303711,
      "learning_rate": 3.0437436419125126e-05,
      "loss": 2.2432,
      "step": 38460
    },
    {
      "epoch": 19.5676500508647,
      "grad_norm": 32.06634521484375,
      "learning_rate": 3.0432349949135302e-05,
      "loss": 2.1857,
      "step": 38470
    },
    {
      "epoch": 19.572736520854527,
      "grad_norm": 38.33047866821289,
      "learning_rate": 3.0427263479145472e-05,
      "loss": 2.2503,
      "step": 38480
    },
    {
      "epoch": 19.577822990844354,
      "grad_norm": 32.59617233276367,
      "learning_rate": 3.042217700915565e-05,
      "loss": 2.1441,
      "step": 38490
    },
    {
      "epoch": 19.58290946083418,
      "grad_norm": 35.43669509887695,
      "learning_rate": 3.0417090539165822e-05,
      "loss": 2.2299,
      "step": 38500
    },
    {
      "epoch": 19.587995930824007,
      "grad_norm": 31.20355796813965,
      "learning_rate": 3.0412004069175992e-05,
      "loss": 2.2045,
      "step": 38510
    },
    {
      "epoch": 19.593082400813834,
      "grad_norm": 32.755558013916016,
      "learning_rate": 3.040691759918617e-05,
      "loss": 2.0909,
      "step": 38520
    },
    {
      "epoch": 19.59816887080366,
      "grad_norm": 34.567771911621094,
      "learning_rate": 3.040183112919634e-05,
      "loss": 2.1301,
      "step": 38530
    },
    {
      "epoch": 19.60325534079349,
      "grad_norm": 37.11247634887695,
      "learning_rate": 3.0396744659206512e-05,
      "loss": 2.1738,
      "step": 38540
    },
    {
      "epoch": 19.608341810783315,
      "grad_norm": 35.9420051574707,
      "learning_rate": 3.039165818921669e-05,
      "loss": 2.1107,
      "step": 38550
    },
    {
      "epoch": 19.613428280773142,
      "grad_norm": 34.50056076049805,
      "learning_rate": 3.038657171922686e-05,
      "loss": 2.2224,
      "step": 38560
    },
    {
      "epoch": 19.61851475076297,
      "grad_norm": 36.08980941772461,
      "learning_rate": 3.038148524923703e-05,
      "loss": 2.2343,
      "step": 38570
    },
    {
      "epoch": 19.623601220752796,
      "grad_norm": 33.30758285522461,
      "learning_rate": 3.0376398779247205e-05,
      "loss": 2.1894,
      "step": 38580
    },
    {
      "epoch": 19.628687690742623,
      "grad_norm": 36.51847457885742,
      "learning_rate": 3.0371312309257378e-05,
      "loss": 2.3042,
      "step": 38590
    },
    {
      "epoch": 19.63377416073245,
      "grad_norm": 40.8339958190918,
      "learning_rate": 3.0366225839267548e-05,
      "loss": 2.1823,
      "step": 38600
    },
    {
      "epoch": 19.638860630722277,
      "grad_norm": 44.48150634765625,
      "learning_rate": 3.0361139369277725e-05,
      "loss": 2.2661,
      "step": 38610
    },
    {
      "epoch": 19.643947100712104,
      "grad_norm": 34.37873458862305,
      "learning_rate": 3.0356052899287895e-05,
      "loss": 2.1725,
      "step": 38620
    },
    {
      "epoch": 19.64903357070193,
      "grad_norm": 34.6496467590332,
      "learning_rate": 3.0350966429298068e-05,
      "loss": 2.2072,
      "step": 38630
    },
    {
      "epoch": 19.654120040691758,
      "grad_norm": 46.90670394897461,
      "learning_rate": 3.0345879959308245e-05,
      "loss": 2.205,
      "step": 38640
    },
    {
      "epoch": 19.659206510681585,
      "grad_norm": 36.50227737426758,
      "learning_rate": 3.0340793489318415e-05,
      "loss": 2.1742,
      "step": 38650
    },
    {
      "epoch": 19.664292980671416,
      "grad_norm": 34.799041748046875,
      "learning_rate": 3.0335707019328584e-05,
      "loss": 2.2277,
      "step": 38660
    },
    {
      "epoch": 19.669379450661243,
      "grad_norm": 33.27241897583008,
      "learning_rate": 3.033062054933876e-05,
      "loss": 2.1592,
      "step": 38670
    },
    {
      "epoch": 19.67446592065107,
      "grad_norm": 32.703086853027344,
      "learning_rate": 3.0325534079348934e-05,
      "loss": 2.2378,
      "step": 38680
    },
    {
      "epoch": 19.679552390640897,
      "grad_norm": 32.936920166015625,
      "learning_rate": 3.0320447609359104e-05,
      "loss": 2.148,
      "step": 38690
    },
    {
      "epoch": 19.684638860630724,
      "grad_norm": 29.70079231262207,
      "learning_rate": 3.031536113936928e-05,
      "loss": 2.2004,
      "step": 38700
    },
    {
      "epoch": 19.68972533062055,
      "grad_norm": 38.3963737487793,
      "learning_rate": 3.031027466937945e-05,
      "loss": 2.2052,
      "step": 38710
    },
    {
      "epoch": 19.694811800610378,
      "grad_norm": 32.98628234863281,
      "learning_rate": 3.0305188199389624e-05,
      "loss": 2.1671,
      "step": 38720
    },
    {
      "epoch": 19.699898270600205,
      "grad_norm": 31.233539581298828,
      "learning_rate": 3.0300101729399797e-05,
      "loss": 2.1794,
      "step": 38730
    },
    {
      "epoch": 19.70498474059003,
      "grad_norm": 32.112579345703125,
      "learning_rate": 3.029501525940997e-05,
      "loss": 2.1725,
      "step": 38740
    },
    {
      "epoch": 19.71007121057986,
      "grad_norm": 28.403425216674805,
      "learning_rate": 3.028992878942014e-05,
      "loss": 2.2295,
      "step": 38750
    },
    {
      "epoch": 19.715157680569686,
      "grad_norm": 34.02827453613281,
      "learning_rate": 3.0284842319430317e-05,
      "loss": 2.1984,
      "step": 38760
    },
    {
      "epoch": 19.720244150559513,
      "grad_norm": 34.32587814331055,
      "learning_rate": 3.0279755849440487e-05,
      "loss": 2.1946,
      "step": 38770
    },
    {
      "epoch": 19.72533062054934,
      "grad_norm": 26.69603157043457,
      "learning_rate": 3.0274669379450664e-05,
      "loss": 2.2346,
      "step": 38780
    },
    {
      "epoch": 19.730417090539166,
      "grad_norm": 36.536685943603516,
      "learning_rate": 3.0269582909460837e-05,
      "loss": 2.2224,
      "step": 38790
    },
    {
      "epoch": 19.735503560528993,
      "grad_norm": 23.992921829223633,
      "learning_rate": 3.0264496439471007e-05,
      "loss": 2.2476,
      "step": 38800
    },
    {
      "epoch": 19.74059003051882,
      "grad_norm": 35.88223648071289,
      "learning_rate": 3.0259409969481184e-05,
      "loss": 2.1376,
      "step": 38810
    },
    {
      "epoch": 19.745676500508647,
      "grad_norm": 29.459436416625977,
      "learning_rate": 3.0254323499491354e-05,
      "loss": 2.1635,
      "step": 38820
    },
    {
      "epoch": 19.750762970498474,
      "grad_norm": 28.780410766601562,
      "learning_rate": 3.0249237029501527e-05,
      "loss": 2.2052,
      "step": 38830
    },
    {
      "epoch": 19.7558494404883,
      "grad_norm": 31.98296356201172,
      "learning_rate": 3.0244150559511704e-05,
      "loss": 2.1532,
      "step": 38840
    },
    {
      "epoch": 19.76093591047813,
      "grad_norm": 29.308794021606445,
      "learning_rate": 3.0239064089521873e-05,
      "loss": 2.1295,
      "step": 38850
    },
    {
      "epoch": 19.766022380467955,
      "grad_norm": 37.574188232421875,
      "learning_rate": 3.0233977619532043e-05,
      "loss": 2.1425,
      "step": 38860
    },
    {
      "epoch": 19.771108850457782,
      "grad_norm": 30.372018814086914,
      "learning_rate": 3.022889114954222e-05,
      "loss": 2.1517,
      "step": 38870
    },
    {
      "epoch": 19.77619532044761,
      "grad_norm": 36.824981689453125,
      "learning_rate": 3.0223804679552393e-05,
      "loss": 2.1898,
      "step": 38880
    },
    {
      "epoch": 19.781281790437436,
      "grad_norm": 36.229549407958984,
      "learning_rate": 3.0218718209562563e-05,
      "loss": 2.1168,
      "step": 38890
    },
    {
      "epoch": 19.786368260427263,
      "grad_norm": 33.20064926147461,
      "learning_rate": 3.021363173957274e-05,
      "loss": 2.1319,
      "step": 38900
    },
    {
      "epoch": 19.79145473041709,
      "grad_norm": 29.287818908691406,
      "learning_rate": 3.020854526958291e-05,
      "loss": 2.2468,
      "step": 38910
    },
    {
      "epoch": 19.796541200406917,
      "grad_norm": 31.961383819580078,
      "learning_rate": 3.0203458799593083e-05,
      "loss": 2.2025,
      "step": 38920
    },
    {
      "epoch": 19.801627670396744,
      "grad_norm": 28.90103530883789,
      "learning_rate": 3.019837232960326e-05,
      "loss": 2.169,
      "step": 38930
    },
    {
      "epoch": 19.80671414038657,
      "grad_norm": 33.10467529296875,
      "learning_rate": 3.019328585961343e-05,
      "loss": 2.2174,
      "step": 38940
    },
    {
      "epoch": 19.811800610376398,
      "grad_norm": 27.63223648071289,
      "learning_rate": 3.01881993896236e-05,
      "loss": 2.1615,
      "step": 38950
    },
    {
      "epoch": 19.816887080366225,
      "grad_norm": 37.65415573120117,
      "learning_rate": 3.0183112919633776e-05,
      "loss": 2.0622,
      "step": 38960
    },
    {
      "epoch": 19.821973550356052,
      "grad_norm": 33.92082595825195,
      "learning_rate": 3.017802644964395e-05,
      "loss": 2.1768,
      "step": 38970
    },
    {
      "epoch": 19.82706002034588,
      "grad_norm": 34.727176666259766,
      "learning_rate": 3.017293997965412e-05,
      "loss": 2.2181,
      "step": 38980
    },
    {
      "epoch": 19.832146490335706,
      "grad_norm": 33.59999465942383,
      "learning_rate": 3.0167853509664296e-05,
      "loss": 2.1202,
      "step": 38990
    },
    {
      "epoch": 19.837232960325533,
      "grad_norm": 41.964229583740234,
      "learning_rate": 3.0162767039674466e-05,
      "loss": 2.1935,
      "step": 39000
    },
    {
      "epoch": 19.84231943031536,
      "grad_norm": 29.931737899780273,
      "learning_rate": 3.015768056968464e-05,
      "loss": 2.134,
      "step": 39010
    },
    {
      "epoch": 19.847405900305187,
      "grad_norm": 34.599517822265625,
      "learning_rate": 3.0152594099694816e-05,
      "loss": 2.18,
      "step": 39020
    },
    {
      "epoch": 19.852492370295014,
      "grad_norm": 28.96318817138672,
      "learning_rate": 3.0147507629704986e-05,
      "loss": 2.0342,
      "step": 39030
    },
    {
      "epoch": 19.85757884028484,
      "grad_norm": 36.28819274902344,
      "learning_rate": 3.0142421159715162e-05,
      "loss": 2.1449,
      "step": 39040
    },
    {
      "epoch": 19.862665310274668,
      "grad_norm": 37.05697250366211,
      "learning_rate": 3.0137334689725332e-05,
      "loss": 2.1696,
      "step": 39050
    },
    {
      "epoch": 19.867751780264495,
      "grad_norm": 52.15974807739258,
      "learning_rate": 3.0132248219735502e-05,
      "loss": 2.1075,
      "step": 39060
    },
    {
      "epoch": 19.872838250254322,
      "grad_norm": 31.75588607788086,
      "learning_rate": 3.012716174974568e-05,
      "loss": 2.1739,
      "step": 39070
    },
    {
      "epoch": 19.87792472024415,
      "grad_norm": 26.7390193939209,
      "learning_rate": 3.0122075279755852e-05,
      "loss": 2.2776,
      "step": 39080
    },
    {
      "epoch": 19.88301119023398,
      "grad_norm": 41.517852783203125,
      "learning_rate": 3.0116988809766022e-05,
      "loss": 2.2124,
      "step": 39090
    },
    {
      "epoch": 19.888097660223806,
      "grad_norm": 31.497217178344727,
      "learning_rate": 3.01119023397762e-05,
      "loss": 2.2135,
      "step": 39100
    },
    {
      "epoch": 19.893184130213633,
      "grad_norm": 31.365283966064453,
      "learning_rate": 3.010681586978637e-05,
      "loss": 2.0475,
      "step": 39110
    },
    {
      "epoch": 19.89827060020346,
      "grad_norm": 32.19587707519531,
      "learning_rate": 3.0101729399796542e-05,
      "loss": 2.1412,
      "step": 39120
    },
    {
      "epoch": 19.903357070193287,
      "grad_norm": 36.49952697753906,
      "learning_rate": 3.009664292980672e-05,
      "loss": 2.1275,
      "step": 39130
    },
    {
      "epoch": 19.908443540183114,
      "grad_norm": 33.959564208984375,
      "learning_rate": 3.009155645981689e-05,
      "loss": 2.2329,
      "step": 39140
    },
    {
      "epoch": 19.91353001017294,
      "grad_norm": 30.613658905029297,
      "learning_rate": 3.008646998982706e-05,
      "loss": 2.2685,
      "step": 39150
    },
    {
      "epoch": 19.91861648016277,
      "grad_norm": 34.14879608154297,
      "learning_rate": 3.0081383519837235e-05,
      "loss": 2.1484,
      "step": 39160
    },
    {
      "epoch": 19.923702950152595,
      "grad_norm": 31.967628479003906,
      "learning_rate": 3.007629704984741e-05,
      "loss": 2.1406,
      "step": 39170
    },
    {
      "epoch": 19.928789420142422,
      "grad_norm": 29.85579490661621,
      "learning_rate": 3.0071210579857578e-05,
      "loss": 2.2279,
      "step": 39180
    },
    {
      "epoch": 19.93387589013225,
      "grad_norm": 28.44858169555664,
      "learning_rate": 3.0066124109867755e-05,
      "loss": 1.9972,
      "step": 39190
    },
    {
      "epoch": 19.938962360122076,
      "grad_norm": 40.1317138671875,
      "learning_rate": 3.0061037639877925e-05,
      "loss": 2.2773,
      "step": 39200
    },
    {
      "epoch": 19.944048830111903,
      "grad_norm": 29.07370376586914,
      "learning_rate": 3.0055951169888098e-05,
      "loss": 2.1117,
      "step": 39210
    },
    {
      "epoch": 19.94913530010173,
      "grad_norm": 36.05318832397461,
      "learning_rate": 3.0050864699898275e-05,
      "loss": 2.1202,
      "step": 39220
    },
    {
      "epoch": 19.954221770091557,
      "grad_norm": 35.674537658691406,
      "learning_rate": 3.0045778229908445e-05,
      "loss": 2.1716,
      "step": 39230
    },
    {
      "epoch": 19.959308240081384,
      "grad_norm": 27.072179794311523,
      "learning_rate": 3.0040691759918615e-05,
      "loss": 2.1579,
      "step": 39240
    },
    {
      "epoch": 19.96439471007121,
      "grad_norm": 34.40131378173828,
      "learning_rate": 3.003560528992879e-05,
      "loss": 2.1348,
      "step": 39250
    },
    {
      "epoch": 19.969481180061038,
      "grad_norm": 29.196575164794922,
      "learning_rate": 3.0030518819938964e-05,
      "loss": 2.1694,
      "step": 39260
    },
    {
      "epoch": 19.974567650050865,
      "grad_norm": 26.46864891052246,
      "learning_rate": 3.0025432349949134e-05,
      "loss": 2.1289,
      "step": 39270
    },
    {
      "epoch": 19.979654120040692,
      "grad_norm": 27.99962615966797,
      "learning_rate": 3.002034587995931e-05,
      "loss": 2.1755,
      "step": 39280
    },
    {
      "epoch": 19.98474059003052,
      "grad_norm": 26.55296516418457,
      "learning_rate": 3.001525940996948e-05,
      "loss": 2.2494,
      "step": 39290
    },
    {
      "epoch": 19.989827060020346,
      "grad_norm": 35.62255859375,
      "learning_rate": 3.0010172939979658e-05,
      "loss": 2.1758,
      "step": 39300
    },
    {
      "epoch": 19.994913530010173,
      "grad_norm": 26.016660690307617,
      "learning_rate": 3.000508646998983e-05,
      "loss": 2.2343,
      "step": 39310
    },
    {
      "epoch": 20.0,
      "grad_norm": 42.14768981933594,
      "learning_rate": 3e-05,
      "loss": 2.1634,
      "step": 39320
    },
    {
      "epoch": 20.0,
      "eval_loss": 4.3328166007995605,
      "eval_runtime": 2.7299,
      "eval_samples_per_second": 1016.533,
      "eval_steps_per_second": 127.112,
      "step": 39320
    },
    {
      "epoch": 20.005086469989827,
      "grad_norm": 30.194360733032227,
      "learning_rate": 2.9994913530010177e-05,
      "loss": 2.0884,
      "step": 39330
    },
    {
      "epoch": 20.010172939979654,
      "grad_norm": 33.6904296875,
      "learning_rate": 2.9989827060020347e-05,
      "loss": 2.2037,
      "step": 39340
    },
    {
      "epoch": 20.01525940996948,
      "grad_norm": 27.541316986083984,
      "learning_rate": 2.998474059003052e-05,
      "loss": 2.1389,
      "step": 39350
    },
    {
      "epoch": 20.020345879959308,
      "grad_norm": 33.40470504760742,
      "learning_rate": 2.9979654120040694e-05,
      "loss": 2.0788,
      "step": 39360
    },
    {
      "epoch": 20.025432349949135,
      "grad_norm": 32.728248596191406,
      "learning_rate": 2.9974567650050867e-05,
      "loss": 2.1212,
      "step": 39370
    },
    {
      "epoch": 20.030518819938962,
      "grad_norm": 32.377174377441406,
      "learning_rate": 2.9969481180061037e-05,
      "loss": 2.186,
      "step": 39380
    },
    {
      "epoch": 20.03560528992879,
      "grad_norm": 30.43524932861328,
      "learning_rate": 2.9964394710071214e-05,
      "loss": 2.1614,
      "step": 39390
    },
    {
      "epoch": 20.040691759918616,
      "grad_norm": 34.79432678222656,
      "learning_rate": 2.9959308240081384e-05,
      "loss": 2.2241,
      "step": 39400
    },
    {
      "epoch": 20.045778229908443,
      "grad_norm": 38.190391540527344,
      "learning_rate": 2.9954221770091557e-05,
      "loss": 2.1063,
      "step": 39410
    },
    {
      "epoch": 20.05086469989827,
      "grad_norm": 39.0728759765625,
      "learning_rate": 2.9949135300101734e-05,
      "loss": 2.1816,
      "step": 39420
    },
    {
      "epoch": 20.055951169888097,
      "grad_norm": 29.9769287109375,
      "learning_rate": 2.9944048830111903e-05,
      "loss": 2.1422,
      "step": 39430
    },
    {
      "epoch": 20.061037639877924,
      "grad_norm": 38.12486267089844,
      "learning_rate": 2.9938962360122073e-05,
      "loss": 2.1164,
      "step": 39440
    },
    {
      "epoch": 20.06612410986775,
      "grad_norm": 35.011077880859375,
      "learning_rate": 2.993387589013225e-05,
      "loss": 2.1094,
      "step": 39450
    },
    {
      "epoch": 20.071210579857578,
      "grad_norm": 29.02411460876465,
      "learning_rate": 2.9928789420142423e-05,
      "loss": 2.1094,
      "step": 39460
    },
    {
      "epoch": 20.076297049847405,
      "grad_norm": 30.924606323242188,
      "learning_rate": 2.9923702950152593e-05,
      "loss": 2.1155,
      "step": 39470
    },
    {
      "epoch": 20.08138351983723,
      "grad_norm": 30.19492530822754,
      "learning_rate": 2.991861648016277e-05,
      "loss": 2.1758,
      "step": 39480
    },
    {
      "epoch": 20.08646998982706,
      "grad_norm": 31.42143440246582,
      "learning_rate": 2.991353001017294e-05,
      "loss": 2.136,
      "step": 39490
    },
    {
      "epoch": 20.091556459816886,
      "grad_norm": 31.885414123535156,
      "learning_rate": 2.9908443540183113e-05,
      "loss": 2.0865,
      "step": 39500
    },
    {
      "epoch": 20.096642929806713,
      "grad_norm": 35.27762222290039,
      "learning_rate": 2.990335707019329e-05,
      "loss": 2.122,
      "step": 39510
    },
    {
      "epoch": 20.10172939979654,
      "grad_norm": 32.78470993041992,
      "learning_rate": 2.989827060020346e-05,
      "loss": 2.139,
      "step": 39520
    },
    {
      "epoch": 20.106815869786367,
      "grad_norm": 33.79820251464844,
      "learning_rate": 2.989318413021363e-05,
      "loss": 2.1597,
      "step": 39530
    },
    {
      "epoch": 20.111902339776194,
      "grad_norm": 34.2274055480957,
      "learning_rate": 2.9888097660223806e-05,
      "loss": 2.2398,
      "step": 39540
    },
    {
      "epoch": 20.116988809766024,
      "grad_norm": 37.335777282714844,
      "learning_rate": 2.988301119023398e-05,
      "loss": 2.14,
      "step": 39550
    },
    {
      "epoch": 20.12207527975585,
      "grad_norm": 28.763647079467773,
      "learning_rate": 2.987792472024415e-05,
      "loss": 1.9888,
      "step": 39560
    },
    {
      "epoch": 20.127161749745678,
      "grad_norm": 29.971805572509766,
      "learning_rate": 2.9872838250254326e-05,
      "loss": 2.1504,
      "step": 39570
    },
    {
      "epoch": 20.132248219735505,
      "grad_norm": 36.96919631958008,
      "learning_rate": 2.9867751780264496e-05,
      "loss": 2.1838,
      "step": 39580
    },
    {
      "epoch": 20.137334689725332,
      "grad_norm": 35.47605514526367,
      "learning_rate": 2.9862665310274673e-05,
      "loss": 2.1378,
      "step": 39590
    },
    {
      "epoch": 20.14242115971516,
      "grad_norm": 41.929317474365234,
      "learning_rate": 2.9857578840284846e-05,
      "loss": 2.1336,
      "step": 39600
    },
    {
      "epoch": 20.147507629704986,
      "grad_norm": 32.66023635864258,
      "learning_rate": 2.9852492370295016e-05,
      "loss": 2.1271,
      "step": 39610
    },
    {
      "epoch": 20.152594099694813,
      "grad_norm": 39.42974090576172,
      "learning_rate": 2.9847405900305192e-05,
      "loss": 2.0682,
      "step": 39620
    },
    {
      "epoch": 20.15768056968464,
      "grad_norm": 37.43015670776367,
      "learning_rate": 2.9842319430315362e-05,
      "loss": 2.0489,
      "step": 39630
    },
    {
      "epoch": 20.162767039674467,
      "grad_norm": 32.65678024291992,
      "learning_rate": 2.9837232960325536e-05,
      "loss": 2.1942,
      "step": 39640
    },
    {
      "epoch": 20.167853509664294,
      "grad_norm": 33.43653106689453,
      "learning_rate": 2.9832146490335712e-05,
      "loss": 2.0772,
      "step": 39650
    },
    {
      "epoch": 20.17293997965412,
      "grad_norm": 27.612499237060547,
      "learning_rate": 2.9827060020345882e-05,
      "loss": 2.1709,
      "step": 39660
    },
    {
      "epoch": 20.178026449643948,
      "grad_norm": 31.721820831298828,
      "learning_rate": 2.9821973550356052e-05,
      "loss": 2.1871,
      "step": 39670
    },
    {
      "epoch": 20.183112919633775,
      "grad_norm": 28.687700271606445,
      "learning_rate": 2.981688708036623e-05,
      "loss": 2.1704,
      "step": 39680
    },
    {
      "epoch": 20.188199389623602,
      "grad_norm": 30.421274185180664,
      "learning_rate": 2.98118006103764e-05,
      "loss": 2.2529,
      "step": 39690
    },
    {
      "epoch": 20.19328585961343,
      "grad_norm": 29.78961944580078,
      "learning_rate": 2.9806714140386572e-05,
      "loss": 2.1818,
      "step": 39700
    },
    {
      "epoch": 20.198372329603256,
      "grad_norm": 32.71185302734375,
      "learning_rate": 2.980162767039675e-05,
      "loss": 2.1657,
      "step": 39710
    },
    {
      "epoch": 20.203458799593083,
      "grad_norm": 42.21668243408203,
      "learning_rate": 2.979654120040692e-05,
      "loss": 2.1036,
      "step": 39720
    },
    {
      "epoch": 20.20854526958291,
      "grad_norm": 36.805442810058594,
      "learning_rate": 2.979145473041709e-05,
      "loss": 2.1958,
      "step": 39730
    },
    {
      "epoch": 20.213631739572737,
      "grad_norm": 30.0430908203125,
      "learning_rate": 2.9786368260427265e-05,
      "loss": 2.2376,
      "step": 39740
    },
    {
      "epoch": 20.218718209562564,
      "grad_norm": 31.336471557617188,
      "learning_rate": 2.978128179043744e-05,
      "loss": 2.1492,
      "step": 39750
    },
    {
      "epoch": 20.22380467955239,
      "grad_norm": 39.781959533691406,
      "learning_rate": 2.9776195320447608e-05,
      "loss": 2.1918,
      "step": 39760
    },
    {
      "epoch": 20.228891149542218,
      "grad_norm": 37.134559631347656,
      "learning_rate": 2.9771108850457785e-05,
      "loss": 2.1478,
      "step": 39770
    },
    {
      "epoch": 20.233977619532045,
      "grad_norm": 33.95537185668945,
      "learning_rate": 2.9766022380467955e-05,
      "loss": 2.1235,
      "step": 39780
    },
    {
      "epoch": 20.23906408952187,
      "grad_norm": 39.27839279174805,
      "learning_rate": 2.9760935910478128e-05,
      "loss": 2.1442,
      "step": 39790
    },
    {
      "epoch": 20.2441505595117,
      "grad_norm": 30.811485290527344,
      "learning_rate": 2.9755849440488305e-05,
      "loss": 2.1613,
      "step": 39800
    },
    {
      "epoch": 20.249237029501526,
      "grad_norm": 50.58206558227539,
      "learning_rate": 2.9750762970498475e-05,
      "loss": 2.224,
      "step": 39810
    },
    {
      "epoch": 20.254323499491353,
      "grad_norm": 42.413021087646484,
      "learning_rate": 2.9745676500508645e-05,
      "loss": 2.1222,
      "step": 39820
    },
    {
      "epoch": 20.25940996948118,
      "grad_norm": 37.39176559448242,
      "learning_rate": 2.974059003051882e-05,
      "loss": 2.104,
      "step": 39830
    },
    {
      "epoch": 20.264496439471007,
      "grad_norm": 35.034481048583984,
      "learning_rate": 2.9735503560528994e-05,
      "loss": 2.184,
      "step": 39840
    },
    {
      "epoch": 20.269582909460834,
      "grad_norm": 29.54119300842285,
      "learning_rate": 2.973041709053917e-05,
      "loss": 2.1325,
      "step": 39850
    },
    {
      "epoch": 20.27466937945066,
      "grad_norm": 46.86356735229492,
      "learning_rate": 2.972533062054934e-05,
      "loss": 2.1031,
      "step": 39860
    },
    {
      "epoch": 20.279755849440487,
      "grad_norm": 36.16828536987305,
      "learning_rate": 2.972024415055951e-05,
      "loss": 2.1249,
      "step": 39870
    },
    {
      "epoch": 20.284842319430314,
      "grad_norm": 33.42139434814453,
      "learning_rate": 2.9715157680569688e-05,
      "loss": 2.0204,
      "step": 39880
    },
    {
      "epoch": 20.28992878942014,
      "grad_norm": 29.250513076782227,
      "learning_rate": 2.971007121057986e-05,
      "loss": 2.1288,
      "step": 39890
    },
    {
      "epoch": 20.29501525940997,
      "grad_norm": 33.9542121887207,
      "learning_rate": 2.970498474059003e-05,
      "loss": 2.0978,
      "step": 39900
    },
    {
      "epoch": 20.300101729399795,
      "grad_norm": 31.597883224487305,
      "learning_rate": 2.9699898270600207e-05,
      "loss": 2.1523,
      "step": 39910
    },
    {
      "epoch": 20.305188199389622,
      "grad_norm": 38.08816909790039,
      "learning_rate": 2.9694811800610377e-05,
      "loss": 2.1328,
      "step": 39920
    },
    {
      "epoch": 20.31027466937945,
      "grad_norm": 35.72411346435547,
      "learning_rate": 2.968972533062055e-05,
      "loss": 2.2058,
      "step": 39930
    },
    {
      "epoch": 20.315361139369276,
      "grad_norm": 32.79176712036133,
      "learning_rate": 2.9684638860630727e-05,
      "loss": 2.1193,
      "step": 39940
    },
    {
      "epoch": 20.320447609359103,
      "grad_norm": 28.643604278564453,
      "learning_rate": 2.9679552390640897e-05,
      "loss": 2.0888,
      "step": 39950
    },
    {
      "epoch": 20.32553407934893,
      "grad_norm": 35.04496383666992,
      "learning_rate": 2.9674465920651067e-05,
      "loss": 2.0921,
      "step": 39960
    },
    {
      "epoch": 20.330620549338757,
      "grad_norm": 41.39378356933594,
      "learning_rate": 2.9669379450661244e-05,
      "loss": 2.2299,
      "step": 39970
    },
    {
      "epoch": 20.335707019328584,
      "grad_norm": 31.305734634399414,
      "learning_rate": 2.9664292980671417e-05,
      "loss": 2.0855,
      "step": 39980
    },
    {
      "epoch": 20.340793489318415,
      "grad_norm": 39.71306610107422,
      "learning_rate": 2.9659206510681587e-05,
      "loss": 2.1092,
      "step": 39990
    },
    {
      "epoch": 20.345879959308242,
      "grad_norm": 36.76088333129883,
      "learning_rate": 2.9654120040691764e-05,
      "loss": 2.121,
      "step": 40000
    },
    {
      "epoch": 20.35096642929807,
      "grad_norm": 33.42138671875,
      "learning_rate": 2.9649033570701933e-05,
      "loss": 2.1807,
      "step": 40010
    },
    {
      "epoch": 20.356052899287896,
      "grad_norm": 29.840818405151367,
      "learning_rate": 2.9643947100712103e-05,
      "loss": 2.182,
      "step": 40020
    },
    {
      "epoch": 20.361139369277723,
      "grad_norm": 33.24943542480469,
      "learning_rate": 2.963886063072228e-05,
      "loss": 2.1264,
      "step": 40030
    },
    {
      "epoch": 20.36622583926755,
      "grad_norm": 33.827415466308594,
      "learning_rate": 2.9633774160732453e-05,
      "loss": 2.2408,
      "step": 40040
    },
    {
      "epoch": 20.371312309257377,
      "grad_norm": 32.96857452392578,
      "learning_rate": 2.9628687690742623e-05,
      "loss": 2.07,
      "step": 40050
    },
    {
      "epoch": 20.376398779247204,
      "grad_norm": 34.2088623046875,
      "learning_rate": 2.96236012207528e-05,
      "loss": 2.1363,
      "step": 40060
    },
    {
      "epoch": 20.38148524923703,
      "grad_norm": 32.17557907104492,
      "learning_rate": 2.961851475076297e-05,
      "loss": 2.1608,
      "step": 40070
    },
    {
      "epoch": 20.386571719226858,
      "grad_norm": 31.058908462524414,
      "learning_rate": 2.9613428280773143e-05,
      "loss": 2.0846,
      "step": 40080
    },
    {
      "epoch": 20.391658189216685,
      "grad_norm": 29.5502872467041,
      "learning_rate": 2.960834181078332e-05,
      "loss": 2.2079,
      "step": 40090
    },
    {
      "epoch": 20.39674465920651,
      "grad_norm": 27.962238311767578,
      "learning_rate": 2.960325534079349e-05,
      "loss": 2.2203,
      "step": 40100
    },
    {
      "epoch": 20.40183112919634,
      "grad_norm": 27.640180587768555,
      "learning_rate": 2.9598168870803666e-05,
      "loss": 2.1769,
      "step": 40110
    },
    {
      "epoch": 20.406917599186166,
      "grad_norm": 27.557897567749023,
      "learning_rate": 2.9593082400813836e-05,
      "loss": 2.1566,
      "step": 40120
    },
    {
      "epoch": 20.412004069175993,
      "grad_norm": 35.56635284423828,
      "learning_rate": 2.958799593082401e-05,
      "loss": 2.1856,
      "step": 40130
    },
    {
      "epoch": 20.41709053916582,
      "grad_norm": 37.196109771728516,
      "learning_rate": 2.9582909460834186e-05,
      "loss": 2.1344,
      "step": 40140
    },
    {
      "epoch": 20.422177009155646,
      "grad_norm": 31.213319778442383,
      "learning_rate": 2.9577822990844356e-05,
      "loss": 2.1318,
      "step": 40150
    },
    {
      "epoch": 20.427263479145473,
      "grad_norm": 31.880651473999023,
      "learning_rate": 2.9572736520854526e-05,
      "loss": 2.114,
      "step": 40160
    },
    {
      "epoch": 20.4323499491353,
      "grad_norm": 24.002073287963867,
      "learning_rate": 2.9567650050864703e-05,
      "loss": 2.0836,
      "step": 40170
    },
    {
      "epoch": 20.437436419125127,
      "grad_norm": 30.853303909301758,
      "learning_rate": 2.9562563580874876e-05,
      "loss": 2.1407,
      "step": 40180
    },
    {
      "epoch": 20.442522889114954,
      "grad_norm": 34.34222412109375,
      "learning_rate": 2.9557477110885046e-05,
      "loss": 2.0786,
      "step": 40190
    },
    {
      "epoch": 20.44760935910478,
      "grad_norm": 30.977855682373047,
      "learning_rate": 2.9552390640895222e-05,
      "loss": 2.1774,
      "step": 40200
    },
    {
      "epoch": 20.45269582909461,
      "grad_norm": 35.01749038696289,
      "learning_rate": 2.9547304170905392e-05,
      "loss": 2.1123,
      "step": 40210
    },
    {
      "epoch": 20.457782299084435,
      "grad_norm": 42.38917922973633,
      "learning_rate": 2.9542217700915566e-05,
      "loss": 2.0974,
      "step": 40220
    },
    {
      "epoch": 20.462868769074262,
      "grad_norm": 29.306995391845703,
      "learning_rate": 2.9537131230925742e-05,
      "loss": 2.075,
      "step": 40230
    },
    {
      "epoch": 20.46795523906409,
      "grad_norm": 35.72737503051758,
      "learning_rate": 2.9532044760935912e-05,
      "loss": 2.1069,
      "step": 40240
    },
    {
      "epoch": 20.473041709053916,
      "grad_norm": 35.718162536621094,
      "learning_rate": 2.9526958290946082e-05,
      "loss": 2.1116,
      "step": 40250
    },
    {
      "epoch": 20.478128179043743,
      "grad_norm": 33.80299758911133,
      "learning_rate": 2.952187182095626e-05,
      "loss": 2.2019,
      "step": 40260
    },
    {
      "epoch": 20.48321464903357,
      "grad_norm": 47.18537902832031,
      "learning_rate": 2.9516785350966432e-05,
      "loss": 2.1649,
      "step": 40270
    },
    {
      "epoch": 20.488301119023397,
      "grad_norm": 30.33550453186035,
      "learning_rate": 2.9511698880976602e-05,
      "loss": 2.1705,
      "step": 40280
    },
    {
      "epoch": 20.493387589013224,
      "grad_norm": 30.5580997467041,
      "learning_rate": 2.950661241098678e-05,
      "loss": 2.1839,
      "step": 40290
    },
    {
      "epoch": 20.49847405900305,
      "grad_norm": 28.207300186157227,
      "learning_rate": 2.950152594099695e-05,
      "loss": 2.1161,
      "step": 40300
    },
    {
      "epoch": 20.503560528992878,
      "grad_norm": 30.497053146362305,
      "learning_rate": 2.9496439471007122e-05,
      "loss": 2.1767,
      "step": 40310
    },
    {
      "epoch": 20.508646998982705,
      "grad_norm": 34.38120651245117,
      "learning_rate": 2.9491353001017295e-05,
      "loss": 2.133,
      "step": 40320
    },
    {
      "epoch": 20.513733468972532,
      "grad_norm": 44.60784912109375,
      "learning_rate": 2.948626653102747e-05,
      "loss": 2.2372,
      "step": 40330
    },
    {
      "epoch": 20.51881993896236,
      "grad_norm": 25.98947525024414,
      "learning_rate": 2.9481180061037638e-05,
      "loss": 2.1648,
      "step": 40340
    },
    {
      "epoch": 20.523906408952186,
      "grad_norm": 30.464942932128906,
      "learning_rate": 2.9476093591047815e-05,
      "loss": 2.0731,
      "step": 40350
    },
    {
      "epoch": 20.528992878942013,
      "grad_norm": 31.525577545166016,
      "learning_rate": 2.9471007121057985e-05,
      "loss": 2.1592,
      "step": 40360
    },
    {
      "epoch": 20.53407934893184,
      "grad_norm": 36.54555892944336,
      "learning_rate": 2.9465920651068158e-05,
      "loss": 2.1234,
      "step": 40370
    },
    {
      "epoch": 20.539165818921667,
      "grad_norm": 29.991455078125,
      "learning_rate": 2.9460834181078335e-05,
      "loss": 2.0803,
      "step": 40380
    },
    {
      "epoch": 20.544252288911494,
      "grad_norm": 28.560094833374023,
      "learning_rate": 2.9455747711088505e-05,
      "loss": 2.1568,
      "step": 40390
    },
    {
      "epoch": 20.54933875890132,
      "grad_norm": 40.740291595458984,
      "learning_rate": 2.945066124109868e-05,
      "loss": 2.1698,
      "step": 40400
    },
    {
      "epoch": 20.554425228891148,
      "grad_norm": 34.81290054321289,
      "learning_rate": 2.944557477110885e-05,
      "loss": 2.1146,
      "step": 40410
    },
    {
      "epoch": 20.559511698880975,
      "grad_norm": 26.486886978149414,
      "learning_rate": 2.9440488301119024e-05,
      "loss": 2.1674,
      "step": 40420
    },
    {
      "epoch": 20.564598168870802,
      "grad_norm": 33.24209213256836,
      "learning_rate": 2.94354018311292e-05,
      "loss": 2.0649,
      "step": 40430
    },
    {
      "epoch": 20.56968463886063,
      "grad_norm": 34.649620056152344,
      "learning_rate": 2.943031536113937e-05,
      "loss": 2.0948,
      "step": 40440
    },
    {
      "epoch": 20.57477110885046,
      "grad_norm": 40.99386215209961,
      "learning_rate": 2.942522889114954e-05,
      "loss": 2.0743,
      "step": 40450
    },
    {
      "epoch": 20.579857578840286,
      "grad_norm": 35.33104705810547,
      "learning_rate": 2.9420142421159718e-05,
      "loss": 2.1471,
      "step": 40460
    },
    {
      "epoch": 20.584944048830113,
      "grad_norm": 37.77361297607422,
      "learning_rate": 2.941505595116989e-05,
      "loss": 2.1904,
      "step": 40470
    },
    {
      "epoch": 20.59003051881994,
      "grad_norm": 29.7571964263916,
      "learning_rate": 2.940996948118006e-05,
      "loss": 2.1698,
      "step": 40480
    },
    {
      "epoch": 20.595116988809767,
      "grad_norm": 28.611907958984375,
      "learning_rate": 2.9404883011190237e-05,
      "loss": 2.2078,
      "step": 40490
    },
    {
      "epoch": 20.600203458799594,
      "grad_norm": 34.62724685668945,
      "learning_rate": 2.9399796541200407e-05,
      "loss": 2.0546,
      "step": 40500
    },
    {
      "epoch": 20.60528992878942,
      "grad_norm": 29.87268829345703,
      "learning_rate": 2.939471007121058e-05,
      "loss": 2.0935,
      "step": 40510
    },
    {
      "epoch": 20.61037639877925,
      "grad_norm": 33.024635314941406,
      "learning_rate": 2.9389623601220757e-05,
      "loss": 2.1955,
      "step": 40520
    },
    {
      "epoch": 20.615462868769075,
      "grad_norm": 32.669288635253906,
      "learning_rate": 2.9384537131230927e-05,
      "loss": 2.1825,
      "step": 40530
    },
    {
      "epoch": 20.620549338758902,
      "grad_norm": 30.600954055786133,
      "learning_rate": 2.9379450661241097e-05,
      "loss": 2.0865,
      "step": 40540
    },
    {
      "epoch": 20.62563580874873,
      "grad_norm": 38.0496940612793,
      "learning_rate": 2.9374364191251274e-05,
      "loss": 2.1812,
      "step": 40550
    },
    {
      "epoch": 20.630722278738556,
      "grad_norm": 28.033329010009766,
      "learning_rate": 2.9369277721261447e-05,
      "loss": 2.1607,
      "step": 40560
    },
    {
      "epoch": 20.635808748728383,
      "grad_norm": 34.81268310546875,
      "learning_rate": 2.9364191251271617e-05,
      "loss": 2.184,
      "step": 40570
    },
    {
      "epoch": 20.64089521871821,
      "grad_norm": 36.127262115478516,
      "learning_rate": 2.9359104781281794e-05,
      "loss": 2.1524,
      "step": 40580
    },
    {
      "epoch": 20.645981688708037,
      "grad_norm": 29.358232498168945,
      "learning_rate": 2.9354018311291963e-05,
      "loss": 2.0726,
      "step": 40590
    },
    {
      "epoch": 20.651068158697864,
      "grad_norm": 32.285099029541016,
      "learning_rate": 2.9348931841302137e-05,
      "loss": 2.1738,
      "step": 40600
    },
    {
      "epoch": 20.65615462868769,
      "grad_norm": 36.67390441894531,
      "learning_rate": 2.9343845371312313e-05,
      "loss": 2.1324,
      "step": 40610
    },
    {
      "epoch": 20.661241098677518,
      "grad_norm": 27.57126235961914,
      "learning_rate": 2.9338758901322483e-05,
      "loss": 2.1128,
      "step": 40620
    },
    {
      "epoch": 20.666327568667345,
      "grad_norm": 37.406036376953125,
      "learning_rate": 2.9333672431332653e-05,
      "loss": 2.1084,
      "step": 40630
    },
    {
      "epoch": 20.671414038657172,
      "grad_norm": 31.501941680908203,
      "learning_rate": 2.932858596134283e-05,
      "loss": 2.0827,
      "step": 40640
    },
    {
      "epoch": 20.676500508647,
      "grad_norm": 30.292251586914062,
      "learning_rate": 2.9323499491353e-05,
      "loss": 2.194,
      "step": 40650
    },
    {
      "epoch": 20.681586978636826,
      "grad_norm": 32.9482307434082,
      "learning_rate": 2.9318413021363176e-05,
      "loss": 2.1256,
      "step": 40660
    },
    {
      "epoch": 20.686673448626653,
      "grad_norm": 28.167041778564453,
      "learning_rate": 2.931332655137335e-05,
      "loss": 2.0643,
      "step": 40670
    },
    {
      "epoch": 20.69175991861648,
      "grad_norm": 35.36332321166992,
      "learning_rate": 2.930824008138352e-05,
      "loss": 2.138,
      "step": 40680
    },
    {
      "epoch": 20.696846388606307,
      "grad_norm": 36.24738311767578,
      "learning_rate": 2.9303153611393696e-05,
      "loss": 2.0819,
      "step": 40690
    },
    {
      "epoch": 20.701932858596134,
      "grad_norm": 34.75788116455078,
      "learning_rate": 2.9298067141403866e-05,
      "loss": 2.1192,
      "step": 40700
    },
    {
      "epoch": 20.70701932858596,
      "grad_norm": 30.52783966064453,
      "learning_rate": 2.929298067141404e-05,
      "loss": 2.1608,
      "step": 40710
    },
    {
      "epoch": 20.712105798575788,
      "grad_norm": 37.904869079589844,
      "learning_rate": 2.9287894201424216e-05,
      "loss": 2.0696,
      "step": 40720
    },
    {
      "epoch": 20.717192268565615,
      "grad_norm": 29.61256980895996,
      "learning_rate": 2.9282807731434386e-05,
      "loss": 2.1118,
      "step": 40730
    },
    {
      "epoch": 20.722278738555442,
      "grad_norm": 43.4461669921875,
      "learning_rate": 2.9277721261444556e-05,
      "loss": 2.2325,
      "step": 40740
    },
    {
      "epoch": 20.72736520854527,
      "grad_norm": 32.00279235839844,
      "learning_rate": 2.9272634791454733e-05,
      "loss": 2.0693,
      "step": 40750
    },
    {
      "epoch": 20.732451678535096,
      "grad_norm": 31.51002311706543,
      "learning_rate": 2.9267548321464906e-05,
      "loss": 2.1543,
      "step": 40760
    },
    {
      "epoch": 20.737538148524923,
      "grad_norm": 23.777259826660156,
      "learning_rate": 2.9262461851475076e-05,
      "loss": 2.0105,
      "step": 40770
    },
    {
      "epoch": 20.74262461851475,
      "grad_norm": 26.797210693359375,
      "learning_rate": 2.9257375381485252e-05,
      "loss": 2.1339,
      "step": 40780
    },
    {
      "epoch": 20.747711088504577,
      "grad_norm": 28.984792709350586,
      "learning_rate": 2.9252288911495422e-05,
      "loss": 2.1539,
      "step": 40790
    },
    {
      "epoch": 20.752797558494404,
      "grad_norm": 38.21253967285156,
      "learning_rate": 2.9247202441505596e-05,
      "loss": 2.1167,
      "step": 40800
    },
    {
      "epoch": 20.75788402848423,
      "grad_norm": 44.53921890258789,
      "learning_rate": 2.9242115971515772e-05,
      "loss": 2.1849,
      "step": 40810
    },
    {
      "epoch": 20.762970498474058,
      "grad_norm": 33.198001861572266,
      "learning_rate": 2.9237029501525942e-05,
      "loss": 2.1149,
      "step": 40820
    },
    {
      "epoch": 20.768056968463885,
      "grad_norm": 41.39726257324219,
      "learning_rate": 2.9231943031536112e-05,
      "loss": 2.1717,
      "step": 40830
    },
    {
      "epoch": 20.77314343845371,
      "grad_norm": 33.54109191894531,
      "learning_rate": 2.922685656154629e-05,
      "loss": 2.107,
      "step": 40840
    },
    {
      "epoch": 20.77822990844354,
      "grad_norm": 26.56633186340332,
      "learning_rate": 2.9221770091556462e-05,
      "loss": 2.1421,
      "step": 40850
    },
    {
      "epoch": 20.783316378433366,
      "grad_norm": 29.74995231628418,
      "learning_rate": 2.9216683621566632e-05,
      "loss": 2.2109,
      "step": 40860
    },
    {
      "epoch": 20.788402848423193,
      "grad_norm": 35.69296646118164,
      "learning_rate": 2.921159715157681e-05,
      "loss": 2.1401,
      "step": 40870
    },
    {
      "epoch": 20.793489318413023,
      "grad_norm": 30.258556365966797,
      "learning_rate": 2.920651068158698e-05,
      "loss": 2.1341,
      "step": 40880
    },
    {
      "epoch": 20.79857578840285,
      "grad_norm": 31.604660034179688,
      "learning_rate": 2.9201424211597152e-05,
      "loss": 2.1109,
      "step": 40890
    },
    {
      "epoch": 20.803662258392677,
      "grad_norm": 36.682926177978516,
      "learning_rate": 2.919633774160733e-05,
      "loss": 2.1463,
      "step": 40900
    },
    {
      "epoch": 20.808748728382504,
      "grad_norm": 34.08312225341797,
      "learning_rate": 2.91912512716175e-05,
      "loss": 2.0741,
      "step": 40910
    },
    {
      "epoch": 20.81383519837233,
      "grad_norm": 30.224449157714844,
      "learning_rate": 2.9186164801627675e-05,
      "loss": 2.1633,
      "step": 40920
    },
    {
      "epoch": 20.818921668362158,
      "grad_norm": 36.41661834716797,
      "learning_rate": 2.9181078331637845e-05,
      "loss": 2.07,
      "step": 40930
    },
    {
      "epoch": 20.824008138351985,
      "grad_norm": 26.65917205810547,
      "learning_rate": 2.9175991861648018e-05,
      "loss": 2.1208,
      "step": 40940
    },
    {
      "epoch": 20.829094608341812,
      "grad_norm": 32.69105911254883,
      "learning_rate": 2.917090539165819e-05,
      "loss": 2.1377,
      "step": 40950
    },
    {
      "epoch": 20.83418107833164,
      "grad_norm": 38.02603530883789,
      "learning_rate": 2.9165818921668365e-05,
      "loss": 2.15,
      "step": 40960
    },
    {
      "epoch": 20.839267548321466,
      "grad_norm": 37.69657897949219,
      "learning_rate": 2.9160732451678535e-05,
      "loss": 2.0304,
      "step": 40970
    },
    {
      "epoch": 20.844354018311293,
      "grad_norm": 35.879634857177734,
      "learning_rate": 2.915564598168871e-05,
      "loss": 2.1689,
      "step": 40980
    },
    {
      "epoch": 20.84944048830112,
      "grad_norm": 31.703706741333008,
      "learning_rate": 2.915055951169888e-05,
      "loss": 2.1553,
      "step": 40990
    },
    {
      "epoch": 20.854526958290947,
      "grad_norm": 32.46101760864258,
      "learning_rate": 2.9145473041709054e-05,
      "loss": 2.1443,
      "step": 41000
    },
    {
      "epoch": 20.859613428280774,
      "grad_norm": 39.996124267578125,
      "learning_rate": 2.914038657171923e-05,
      "loss": 2.118,
      "step": 41010
    },
    {
      "epoch": 20.8646998982706,
      "grad_norm": 35.60411071777344,
      "learning_rate": 2.91353001017294e-05,
      "loss": 2.1959,
      "step": 41020
    },
    {
      "epoch": 20.869786368260428,
      "grad_norm": 34.24863052368164,
      "learning_rate": 2.913021363173957e-05,
      "loss": 2.1843,
      "step": 41030
    },
    {
      "epoch": 20.874872838250255,
      "grad_norm": 29.827848434448242,
      "learning_rate": 2.9125127161749748e-05,
      "loss": 2.1536,
      "step": 41040
    },
    {
      "epoch": 20.879959308240082,
      "grad_norm": 44.266963958740234,
      "learning_rate": 2.912004069175992e-05,
      "loss": 2.0793,
      "step": 41050
    },
    {
      "epoch": 20.88504577822991,
      "grad_norm": 25.12714958190918,
      "learning_rate": 2.911495422177009e-05,
      "loss": 2.0532,
      "step": 41060
    },
    {
      "epoch": 20.890132248219736,
      "grad_norm": 28.704896926879883,
      "learning_rate": 2.9109867751780267e-05,
      "loss": 2.0568,
      "step": 41070
    },
    {
      "epoch": 20.895218718209563,
      "grad_norm": 31.168903350830078,
      "learning_rate": 2.9104781281790437e-05,
      "loss": 2.1831,
      "step": 41080
    },
    {
      "epoch": 20.90030518819939,
      "grad_norm": 31.98169708251953,
      "learning_rate": 2.909969481180061e-05,
      "loss": 2.0582,
      "step": 41090
    },
    {
      "epoch": 20.905391658189217,
      "grad_norm": 38.399169921875,
      "learning_rate": 2.9094608341810787e-05,
      "loss": 2.0808,
      "step": 41100
    },
    {
      "epoch": 20.910478128179044,
      "grad_norm": 34.849605560302734,
      "learning_rate": 2.9089521871820957e-05,
      "loss": 2.0852,
      "step": 41110
    },
    {
      "epoch": 20.91556459816887,
      "grad_norm": 40.616390228271484,
      "learning_rate": 2.9084435401831127e-05,
      "loss": 2.0338,
      "step": 41120
    },
    {
      "epoch": 20.920651068158698,
      "grad_norm": 33.59844207763672,
      "learning_rate": 2.9079348931841304e-05,
      "loss": 2.1337,
      "step": 41130
    },
    {
      "epoch": 20.925737538148525,
      "grad_norm": 30.384809494018555,
      "learning_rate": 2.9074262461851477e-05,
      "loss": 2.1437,
      "step": 41140
    },
    {
      "epoch": 20.93082400813835,
      "grad_norm": 35.05390167236328,
      "learning_rate": 2.9069175991861647e-05,
      "loss": 2.1172,
      "step": 41150
    },
    {
      "epoch": 20.93591047812818,
      "grad_norm": 29.374732971191406,
      "learning_rate": 2.9064089521871824e-05,
      "loss": 2.1606,
      "step": 41160
    },
    {
      "epoch": 20.940996948118006,
      "grad_norm": 27.97056007385254,
      "learning_rate": 2.9059003051881993e-05,
      "loss": 2.1588,
      "step": 41170
    },
    {
      "epoch": 20.946083418107833,
      "grad_norm": 36.252532958984375,
      "learning_rate": 2.9053916581892167e-05,
      "loss": 2.0945,
      "step": 41180
    },
    {
      "epoch": 20.95116988809766,
      "grad_norm": 35.724056243896484,
      "learning_rate": 2.9048830111902343e-05,
      "loss": 2.1965,
      "step": 41190
    },
    {
      "epoch": 20.956256358087487,
      "grad_norm": 38.07469940185547,
      "learning_rate": 2.9043743641912513e-05,
      "loss": 2.2089,
      "step": 41200
    },
    {
      "epoch": 20.961342828077314,
      "grad_norm": 33.81527328491211,
      "learning_rate": 2.903865717192269e-05,
      "loss": 2.0771,
      "step": 41210
    },
    {
      "epoch": 20.96642929806714,
      "grad_norm": 34.03528594970703,
      "learning_rate": 2.903357070193286e-05,
      "loss": 2.1571,
      "step": 41220
    },
    {
      "epoch": 20.971515768056967,
      "grad_norm": 35.352516174316406,
      "learning_rate": 2.9028484231943033e-05,
      "loss": 2.0896,
      "step": 41230
    },
    {
      "epoch": 20.976602238046794,
      "grad_norm": 29.840938568115234,
      "learning_rate": 2.902339776195321e-05,
      "loss": 2.1715,
      "step": 41240
    },
    {
      "epoch": 20.98168870803662,
      "grad_norm": 35.765499114990234,
      "learning_rate": 2.901831129196338e-05,
      "loss": 2.1536,
      "step": 41250
    },
    {
      "epoch": 20.98677517802645,
      "grad_norm": 35.07526779174805,
      "learning_rate": 2.901322482197355e-05,
      "loss": 2.0459,
      "step": 41260
    },
    {
      "epoch": 20.991861648016275,
      "grad_norm": 27.032005310058594,
      "learning_rate": 2.9008138351983726e-05,
      "loss": 2.1685,
      "step": 41270
    },
    {
      "epoch": 20.996948118006102,
      "grad_norm": 34.58867645263672,
      "learning_rate": 2.9003051881993896e-05,
      "loss": 2.0064,
      "step": 41280
    },
    {
      "epoch": 21.0,
      "eval_loss": 4.398068904876709,
      "eval_runtime": 2.7391,
      "eval_samples_per_second": 1013.116,
      "eval_steps_per_second": 126.685,
      "step": 41286
    },
    {
      "epoch": 21.00203458799593,
      "grad_norm": 36.04710006713867,
      "learning_rate": 2.899796541200407e-05,
      "loss": 2.0446,
      "step": 41290
    },
    {
      "epoch": 21.007121057985756,
      "grad_norm": 50.249820709228516,
      "learning_rate": 2.8992878942014246e-05,
      "loss": 2.1116,
      "step": 41300
    },
    {
      "epoch": 21.012207527975583,
      "grad_norm": 29.319108963012695,
      "learning_rate": 2.8987792472024416e-05,
      "loss": 1.9794,
      "step": 41310
    },
    {
      "epoch": 21.01729399796541,
      "grad_norm": 37.78279495239258,
      "learning_rate": 2.8982706002034586e-05,
      "loss": 2.1493,
      "step": 41320
    },
    {
      "epoch": 21.022380467955237,
      "grad_norm": 29.148332595825195,
      "learning_rate": 2.8977619532044763e-05,
      "loss": 2.1002,
      "step": 41330
    },
    {
      "epoch": 21.027466937945068,
      "grad_norm": 34.22401428222656,
      "learning_rate": 2.8972533062054936e-05,
      "loss": 2.114,
      "step": 41340
    },
    {
      "epoch": 21.032553407934895,
      "grad_norm": 34.58422088623047,
      "learning_rate": 2.8967446592065106e-05,
      "loss": 2.0943,
      "step": 41350
    },
    {
      "epoch": 21.037639877924722,
      "grad_norm": 30.97365379333496,
      "learning_rate": 2.8962360122075282e-05,
      "loss": 2.1537,
      "step": 41360
    },
    {
      "epoch": 21.04272634791455,
      "grad_norm": 30.779577255249023,
      "learning_rate": 2.8957273652085452e-05,
      "loss": 2.1449,
      "step": 41370
    },
    {
      "epoch": 21.047812817904376,
      "grad_norm": 28.125648498535156,
      "learning_rate": 2.8952187182095626e-05,
      "loss": 2.096,
      "step": 41380
    },
    {
      "epoch": 21.052899287894203,
      "grad_norm": 32.58881378173828,
      "learning_rate": 2.8947100712105802e-05,
      "loss": 2.1176,
      "step": 41390
    },
    {
      "epoch": 21.05798575788403,
      "grad_norm": 30.81505012512207,
      "learning_rate": 2.8942014242115972e-05,
      "loss": 2.0534,
      "step": 41400
    },
    {
      "epoch": 21.063072227873857,
      "grad_norm": 32.53614044189453,
      "learning_rate": 2.8936927772126142e-05,
      "loss": 2.0724,
      "step": 41410
    },
    {
      "epoch": 21.068158697863684,
      "grad_norm": 36.40764236450195,
      "learning_rate": 2.893184130213632e-05,
      "loss": 2.1434,
      "step": 41420
    },
    {
      "epoch": 21.07324516785351,
      "grad_norm": 30.5307559967041,
      "learning_rate": 2.8926754832146492e-05,
      "loss": 2.0601,
      "step": 41430
    },
    {
      "epoch": 21.078331637843338,
      "grad_norm": 35.44688034057617,
      "learning_rate": 2.8921668362156662e-05,
      "loss": 2.098,
      "step": 41440
    },
    {
      "epoch": 21.083418107833165,
      "grad_norm": 38.296817779541016,
      "learning_rate": 2.891658189216684e-05,
      "loss": 2.0871,
      "step": 41450
    },
    {
      "epoch": 21.08850457782299,
      "grad_norm": 26.317758560180664,
      "learning_rate": 2.891149542217701e-05,
      "loss": 2.1424,
      "step": 41460
    },
    {
      "epoch": 21.09359104781282,
      "grad_norm": 28.752180099487305,
      "learning_rate": 2.8906408952187185e-05,
      "loss": 2.1621,
      "step": 41470
    },
    {
      "epoch": 21.098677517802646,
      "grad_norm": 36.789127349853516,
      "learning_rate": 2.890132248219736e-05,
      "loss": 2.1201,
      "step": 41480
    },
    {
      "epoch": 21.103763987792473,
      "grad_norm": 36.92852783203125,
      "learning_rate": 2.889623601220753e-05,
      "loss": 2.1358,
      "step": 41490
    },
    {
      "epoch": 21.1088504577823,
      "grad_norm": 38.49641799926758,
      "learning_rate": 2.8891149542217705e-05,
      "loss": 2.1072,
      "step": 41500
    },
    {
      "epoch": 21.113936927772126,
      "grad_norm": 29.976207733154297,
      "learning_rate": 2.8886063072227875e-05,
      "loss": 2.1018,
      "step": 41510
    },
    {
      "epoch": 21.119023397761953,
      "grad_norm": 40.9327392578125,
      "learning_rate": 2.8880976602238048e-05,
      "loss": 2.1131,
      "step": 41520
    },
    {
      "epoch": 21.12410986775178,
      "grad_norm": 28.913406372070312,
      "learning_rate": 2.8875890132248225e-05,
      "loss": 2.1396,
      "step": 41530
    },
    {
      "epoch": 21.129196337741607,
      "grad_norm": 36.50279998779297,
      "learning_rate": 2.8870803662258395e-05,
      "loss": 2.0001,
      "step": 41540
    },
    {
      "epoch": 21.134282807731434,
      "grad_norm": 41.379859924316406,
      "learning_rate": 2.8865717192268565e-05,
      "loss": 2.1706,
      "step": 41550
    },
    {
      "epoch": 21.13936927772126,
      "grad_norm": 30.449068069458008,
      "learning_rate": 2.886063072227874e-05,
      "loss": 2.1434,
      "step": 41560
    },
    {
      "epoch": 21.14445574771109,
      "grad_norm": 38.56232452392578,
      "learning_rate": 2.8855544252288915e-05,
      "loss": 2.0165,
      "step": 41570
    },
    {
      "epoch": 21.149542217700915,
      "grad_norm": 36.85388946533203,
      "learning_rate": 2.8850457782299084e-05,
      "loss": 2.0619,
      "step": 41580
    },
    {
      "epoch": 21.154628687690742,
      "grad_norm": 30.03581428527832,
      "learning_rate": 2.884537131230926e-05,
      "loss": 2.1018,
      "step": 41590
    },
    {
      "epoch": 21.15971515768057,
      "grad_norm": 29.90738868713379,
      "learning_rate": 2.884028484231943e-05,
      "loss": 2.1159,
      "step": 41600
    },
    {
      "epoch": 21.164801627670396,
      "grad_norm": 36.48137664794922,
      "learning_rate": 2.8835198372329604e-05,
      "loss": 2.1212,
      "step": 41610
    },
    {
      "epoch": 21.169888097660223,
      "grad_norm": 26.649433135986328,
      "learning_rate": 2.8830111902339778e-05,
      "loss": 2.0944,
      "step": 41620
    },
    {
      "epoch": 21.17497456765005,
      "grad_norm": 29.785667419433594,
      "learning_rate": 2.882502543234995e-05,
      "loss": 2.1705,
      "step": 41630
    },
    {
      "epoch": 21.180061037639877,
      "grad_norm": 32.39358901977539,
      "learning_rate": 2.881993896236012e-05,
      "loss": 2.1194,
      "step": 41640
    },
    {
      "epoch": 21.185147507629704,
      "grad_norm": 31.57052230834961,
      "learning_rate": 2.8814852492370297e-05,
      "loss": 2.1387,
      "step": 41650
    },
    {
      "epoch": 21.19023397761953,
      "grad_norm": 29.630128860473633,
      "learning_rate": 2.8809766022380467e-05,
      "loss": 2.0796,
      "step": 41660
    },
    {
      "epoch": 21.195320447609358,
      "grad_norm": 28.55038833618164,
      "learning_rate": 2.880467955239064e-05,
      "loss": 2.002,
      "step": 41670
    },
    {
      "epoch": 21.200406917599185,
      "grad_norm": 31.038955688476562,
      "learning_rate": 2.8799593082400817e-05,
      "loss": 2.0912,
      "step": 41680
    },
    {
      "epoch": 21.205493387589012,
      "grad_norm": 29.921337127685547,
      "learning_rate": 2.8794506612410987e-05,
      "loss": 2.1215,
      "step": 41690
    },
    {
      "epoch": 21.21057985757884,
      "grad_norm": 28.47305679321289,
      "learning_rate": 2.8789420142421157e-05,
      "loss": 2.1095,
      "step": 41700
    },
    {
      "epoch": 21.215666327568666,
      "grad_norm": 32.939109802246094,
      "learning_rate": 2.8784333672431334e-05,
      "loss": 2.0431,
      "step": 41710
    },
    {
      "epoch": 21.220752797558493,
      "grad_norm": 27.038862228393555,
      "learning_rate": 2.8779247202441507e-05,
      "loss": 2.1192,
      "step": 41720
    },
    {
      "epoch": 21.22583926754832,
      "grad_norm": 33.84616470336914,
      "learning_rate": 2.8774160732451684e-05,
      "loss": 2.065,
      "step": 41730
    },
    {
      "epoch": 21.230925737538147,
      "grad_norm": 31.333782196044922,
      "learning_rate": 2.8769074262461854e-05,
      "loss": 2.0388,
      "step": 41740
    },
    {
      "epoch": 21.236012207527974,
      "grad_norm": 37.25667190551758,
      "learning_rate": 2.8763987792472023e-05,
      "loss": 2.0639,
      "step": 41750
    },
    {
      "epoch": 21.2410986775178,
      "grad_norm": 33.516151428222656,
      "learning_rate": 2.87589013224822e-05,
      "loss": 2.1411,
      "step": 41760
    },
    {
      "epoch": 21.246185147507628,
      "grad_norm": 29.43775177001953,
      "learning_rate": 2.8753814852492373e-05,
      "loss": 2.076,
      "step": 41770
    },
    {
      "epoch": 21.25127161749746,
      "grad_norm": 31.511919021606445,
      "learning_rate": 2.8748728382502543e-05,
      "loss": 2.0842,
      "step": 41780
    },
    {
      "epoch": 21.256358087487286,
      "grad_norm": 38.882781982421875,
      "learning_rate": 2.874364191251272e-05,
      "loss": 2.1066,
      "step": 41790
    },
    {
      "epoch": 21.261444557477112,
      "grad_norm": 29.52216911315918,
      "learning_rate": 2.873855544252289e-05,
      "loss": 2.1229,
      "step": 41800
    },
    {
      "epoch": 21.26653102746694,
      "grad_norm": 28.624839782714844,
      "learning_rate": 2.8733468972533063e-05,
      "loss": 2.0458,
      "step": 41810
    },
    {
      "epoch": 21.271617497456766,
      "grad_norm": 33.8197021484375,
      "learning_rate": 2.872838250254324e-05,
      "loss": 2.0834,
      "step": 41820
    },
    {
      "epoch": 21.276703967446593,
      "grad_norm": 28.88258171081543,
      "learning_rate": 2.872329603255341e-05,
      "loss": 2.1128,
      "step": 41830
    },
    {
      "epoch": 21.28179043743642,
      "grad_norm": 32.82878875732422,
      "learning_rate": 2.871820956256358e-05,
      "loss": 2.1109,
      "step": 41840
    },
    {
      "epoch": 21.286876907426247,
      "grad_norm": 38.553245544433594,
      "learning_rate": 2.8713123092573756e-05,
      "loss": 2.1079,
      "step": 41850
    },
    {
      "epoch": 21.291963377416074,
      "grad_norm": 31.45318603515625,
      "learning_rate": 2.870803662258393e-05,
      "loss": 2.0895,
      "step": 41860
    },
    {
      "epoch": 21.2970498474059,
      "grad_norm": 34.331634521484375,
      "learning_rate": 2.87029501525941e-05,
      "loss": 2.0819,
      "step": 41870
    },
    {
      "epoch": 21.30213631739573,
      "grad_norm": 34.22773742675781,
      "learning_rate": 2.8697863682604276e-05,
      "loss": 2.0675,
      "step": 41880
    },
    {
      "epoch": 21.307222787385555,
      "grad_norm": 38.522823333740234,
      "learning_rate": 2.8692777212614446e-05,
      "loss": 2.0675,
      "step": 41890
    },
    {
      "epoch": 21.312309257375382,
      "grad_norm": 31.387598037719727,
      "learning_rate": 2.868769074262462e-05,
      "loss": 2.0772,
      "step": 41900
    },
    {
      "epoch": 21.31739572736521,
      "grad_norm": 33.99358367919922,
      "learning_rate": 2.8682604272634796e-05,
      "loss": 2.1287,
      "step": 41910
    },
    {
      "epoch": 21.322482197355036,
      "grad_norm": 39.0928840637207,
      "learning_rate": 2.8677517802644966e-05,
      "loss": 2.0973,
      "step": 41920
    },
    {
      "epoch": 21.327568667344863,
      "grad_norm": 29.072599411010742,
      "learning_rate": 2.8672431332655136e-05,
      "loss": 2.0452,
      "step": 41930
    },
    {
      "epoch": 21.33265513733469,
      "grad_norm": 30.238256454467773,
      "learning_rate": 2.8667344862665312e-05,
      "loss": 2.1096,
      "step": 41940
    },
    {
      "epoch": 21.337741607324517,
      "grad_norm": 36.05463409423828,
      "learning_rate": 2.8662258392675482e-05,
      "loss": 2.1394,
      "step": 41950
    },
    {
      "epoch": 21.342828077314344,
      "grad_norm": 41.84596252441406,
      "learning_rate": 2.8657171922685656e-05,
      "loss": 2.1544,
      "step": 41960
    },
    {
      "epoch": 21.34791454730417,
      "grad_norm": 33.86439895629883,
      "learning_rate": 2.8652085452695832e-05,
      "loss": 2.0906,
      "step": 41970
    },
    {
      "epoch": 21.353001017293998,
      "grad_norm": 34.364864349365234,
      "learning_rate": 2.8646998982706002e-05,
      "loss": 2.1037,
      "step": 41980
    },
    {
      "epoch": 21.358087487283825,
      "grad_norm": 39.50603485107422,
      "learning_rate": 2.8641912512716172e-05,
      "loss": 2.2313,
      "step": 41990
    },
    {
      "epoch": 21.363173957273652,
      "grad_norm": 39.33209228515625,
      "learning_rate": 2.863682604272635e-05,
      "loss": 2.0928,
      "step": 42000
    },
    {
      "epoch": 21.36826042726348,
      "grad_norm": 32.40229415893555,
      "learning_rate": 2.8631739572736522e-05,
      "loss": 2.0812,
      "step": 42010
    },
    {
      "epoch": 21.373346897253306,
      "grad_norm": 42.8564567565918,
      "learning_rate": 2.86266531027467e-05,
      "loss": 2.1184,
      "step": 42020
    },
    {
      "epoch": 21.378433367243133,
      "grad_norm": 27.1207332611084,
      "learning_rate": 2.862156663275687e-05,
      "loss": 1.972,
      "step": 42030
    },
    {
      "epoch": 21.38351983723296,
      "grad_norm": 34.02668380737305,
      "learning_rate": 2.861648016276704e-05,
      "loss": 2.0755,
      "step": 42040
    },
    {
      "epoch": 21.388606307222787,
      "grad_norm": 41.695762634277344,
      "learning_rate": 2.8611393692777215e-05,
      "loss": 2.1322,
      "step": 42050
    },
    {
      "epoch": 21.393692777212614,
      "grad_norm": 33.010005950927734,
      "learning_rate": 2.860630722278739e-05,
      "loss": 2.0263,
      "step": 42060
    },
    {
      "epoch": 21.39877924720244,
      "grad_norm": 29.99578285217285,
      "learning_rate": 2.860122075279756e-05,
      "loss": 2.0702,
      "step": 42070
    },
    {
      "epoch": 21.403865717192268,
      "grad_norm": 33.24672317504883,
      "learning_rate": 2.8596134282807735e-05,
      "loss": 2.1145,
      "step": 42080
    },
    {
      "epoch": 21.408952187182095,
      "grad_norm": 32.88269805908203,
      "learning_rate": 2.8591047812817905e-05,
      "loss": 2.1291,
      "step": 42090
    },
    {
      "epoch": 21.414038657171922,
      "grad_norm": 45.21542739868164,
      "learning_rate": 2.8585961342828078e-05,
      "loss": 2.0776,
      "step": 42100
    },
    {
      "epoch": 21.41912512716175,
      "grad_norm": 30.39434814453125,
      "learning_rate": 2.8580874872838255e-05,
      "loss": 2.0628,
      "step": 42110
    },
    {
      "epoch": 21.424211597151576,
      "grad_norm": 37.083900451660156,
      "learning_rate": 2.8575788402848425e-05,
      "loss": 2.0946,
      "step": 42120
    },
    {
      "epoch": 21.429298067141403,
      "grad_norm": 27.19862174987793,
      "learning_rate": 2.8570701932858595e-05,
      "loss": 2.2327,
      "step": 42130
    },
    {
      "epoch": 21.43438453713123,
      "grad_norm": 33.318603515625,
      "learning_rate": 2.856561546286877e-05,
      "loss": 2.1529,
      "step": 42140
    },
    {
      "epoch": 21.439471007121057,
      "grad_norm": 33.4036865234375,
      "learning_rate": 2.8560528992878945e-05,
      "loss": 2.0881,
      "step": 42150
    },
    {
      "epoch": 21.444557477110884,
      "grad_norm": 34.293460845947266,
      "learning_rate": 2.8555442522889114e-05,
      "loss": 2.121,
      "step": 42160
    },
    {
      "epoch": 21.44964394710071,
      "grad_norm": 37.070674896240234,
      "learning_rate": 2.855035605289929e-05,
      "loss": 2.1071,
      "step": 42170
    },
    {
      "epoch": 21.454730417090538,
      "grad_norm": 35.096405029296875,
      "learning_rate": 2.854526958290946e-05,
      "loss": 2.0903,
      "step": 42180
    },
    {
      "epoch": 21.459816887080365,
      "grad_norm": 40.81734085083008,
      "learning_rate": 2.8540183112919634e-05,
      "loss": 2.0761,
      "step": 42190
    },
    {
      "epoch": 21.46490335707019,
      "grad_norm": 31.23371124267578,
      "learning_rate": 2.853509664292981e-05,
      "loss": 2.116,
      "step": 42200
    },
    {
      "epoch": 21.46998982706002,
      "grad_norm": 34.70052719116211,
      "learning_rate": 2.853001017293998e-05,
      "loss": 2.1926,
      "step": 42210
    },
    {
      "epoch": 21.475076297049846,
      "grad_norm": 38.20057678222656,
      "learning_rate": 2.852492370295015e-05,
      "loss": 2.0384,
      "step": 42220
    },
    {
      "epoch": 21.480162767039676,
      "grad_norm": 32.28657150268555,
      "learning_rate": 2.8519837232960327e-05,
      "loss": 2.1173,
      "step": 42230
    },
    {
      "epoch": 21.485249237029503,
      "grad_norm": 31.54466438293457,
      "learning_rate": 2.85147507629705e-05,
      "loss": 2.1331,
      "step": 42240
    },
    {
      "epoch": 21.49033570701933,
      "grad_norm": 33.285213470458984,
      "learning_rate": 2.850966429298067e-05,
      "loss": 2.0078,
      "step": 42250
    },
    {
      "epoch": 21.495422177009157,
      "grad_norm": 34.849788665771484,
      "learning_rate": 2.8504577822990847e-05,
      "loss": 2.171,
      "step": 42260
    },
    {
      "epoch": 21.500508646998984,
      "grad_norm": 31.538389205932617,
      "learning_rate": 2.8499491353001017e-05,
      "loss": 2.0899,
      "step": 42270
    },
    {
      "epoch": 21.50559511698881,
      "grad_norm": 33.22402572631836,
      "learning_rate": 2.8494404883011194e-05,
      "loss": 2.0003,
      "step": 42280
    },
    {
      "epoch": 21.510681586978638,
      "grad_norm": 29.717060089111328,
      "learning_rate": 2.8489318413021364e-05,
      "loss": 1.9864,
      "step": 42290
    },
    {
      "epoch": 21.515768056968465,
      "grad_norm": 37.737850189208984,
      "learning_rate": 2.8484231943031537e-05,
      "loss": 2.1133,
      "step": 42300
    },
    {
      "epoch": 21.520854526958292,
      "grad_norm": 38.55217742919922,
      "learning_rate": 2.8479145473041714e-05,
      "loss": 2.0674,
      "step": 42310
    },
    {
      "epoch": 21.52594099694812,
      "grad_norm": 34.773826599121094,
      "learning_rate": 2.8474059003051884e-05,
      "loss": 2.0559,
      "step": 42320
    },
    {
      "epoch": 21.531027466937946,
      "grad_norm": 30.042518615722656,
      "learning_rate": 2.8468972533062054e-05,
      "loss": 2.1153,
      "step": 42330
    },
    {
      "epoch": 21.536113936927773,
      "grad_norm": 43.169578552246094,
      "learning_rate": 2.846388606307223e-05,
      "loss": 2.0809,
      "step": 42340
    },
    {
      "epoch": 21.5412004069176,
      "grad_norm": 35.14319610595703,
      "learning_rate": 2.8458799593082403e-05,
      "loss": 2.1244,
      "step": 42350
    },
    {
      "epoch": 21.546286876907427,
      "grad_norm": 41.484596252441406,
      "learning_rate": 2.8453713123092573e-05,
      "loss": 2.0915,
      "step": 42360
    },
    {
      "epoch": 21.551373346897254,
      "grad_norm": 37.3284912109375,
      "learning_rate": 2.844862665310275e-05,
      "loss": 2.0995,
      "step": 42370
    },
    {
      "epoch": 21.55645981688708,
      "grad_norm": 36.817657470703125,
      "learning_rate": 2.844354018311292e-05,
      "loss": 2.1681,
      "step": 42380
    },
    {
      "epoch": 21.561546286876908,
      "grad_norm": 34.50560760498047,
      "learning_rate": 2.8438453713123093e-05,
      "loss": 2.0978,
      "step": 42390
    },
    {
      "epoch": 21.566632756866735,
      "grad_norm": 28.72865867614746,
      "learning_rate": 2.843336724313327e-05,
      "loss": 1.9929,
      "step": 42400
    },
    {
      "epoch": 21.571719226856562,
      "grad_norm": 29.758745193481445,
      "learning_rate": 2.842828077314344e-05,
      "loss": 2.0872,
      "step": 42410
    },
    {
      "epoch": 21.57680569684639,
      "grad_norm": 34.254146575927734,
      "learning_rate": 2.842319430315361e-05,
      "loss": 2.0566,
      "step": 42420
    },
    {
      "epoch": 21.581892166836216,
      "grad_norm": 31.022764205932617,
      "learning_rate": 2.8418107833163786e-05,
      "loss": 2.0446,
      "step": 42430
    },
    {
      "epoch": 21.586978636826043,
      "grad_norm": 36.4778938293457,
      "learning_rate": 2.841302136317396e-05,
      "loss": 2.1008,
      "step": 42440
    },
    {
      "epoch": 21.59206510681587,
      "grad_norm": 26.911949157714844,
      "learning_rate": 2.840793489318413e-05,
      "loss": 2.0886,
      "step": 42450
    },
    {
      "epoch": 21.597151576805697,
      "grad_norm": 36.277259826660156,
      "learning_rate": 2.8402848423194306e-05,
      "loss": 2.0947,
      "step": 42460
    },
    {
      "epoch": 21.602238046795524,
      "grad_norm": 28.097518920898438,
      "learning_rate": 2.8397761953204476e-05,
      "loss": 2.0704,
      "step": 42470
    },
    {
      "epoch": 21.60732451678535,
      "grad_norm": 35.876441955566406,
      "learning_rate": 2.839267548321465e-05,
      "loss": 2.0556,
      "step": 42480
    },
    {
      "epoch": 21.612410986775178,
      "grad_norm": 40.459896087646484,
      "learning_rate": 2.8387589013224826e-05,
      "loss": 2.0212,
      "step": 42490
    },
    {
      "epoch": 21.617497456765005,
      "grad_norm": 30.916767120361328,
      "learning_rate": 2.8382502543234996e-05,
      "loss": 2.0261,
      "step": 42500
    },
    {
      "epoch": 21.62258392675483,
      "grad_norm": 33.59394073486328,
      "learning_rate": 2.8377416073245166e-05,
      "loss": 2.0493,
      "step": 42510
    },
    {
      "epoch": 21.62767039674466,
      "grad_norm": 38.77663040161133,
      "learning_rate": 2.8372329603255342e-05,
      "loss": 2.1581,
      "step": 42520
    },
    {
      "epoch": 21.632756866734486,
      "grad_norm": 35.362327575683594,
      "learning_rate": 2.8367243133265516e-05,
      "loss": 2.1033,
      "step": 42530
    },
    {
      "epoch": 21.637843336724313,
      "grad_norm": 29.817106246948242,
      "learning_rate": 2.8362156663275692e-05,
      "loss": 2.059,
      "step": 42540
    },
    {
      "epoch": 21.64292980671414,
      "grad_norm": 34.11811065673828,
      "learning_rate": 2.8357070193285862e-05,
      "loss": 1.9858,
      "step": 42550
    },
    {
      "epoch": 21.648016276703967,
      "grad_norm": 35.386314392089844,
      "learning_rate": 2.8351983723296032e-05,
      "loss": 2.1126,
      "step": 42560
    },
    {
      "epoch": 21.653102746693794,
      "grad_norm": 39.72725296020508,
      "learning_rate": 2.834689725330621e-05,
      "loss": 2.0963,
      "step": 42570
    },
    {
      "epoch": 21.65818921668362,
      "grad_norm": 38.74689865112305,
      "learning_rate": 2.834181078331638e-05,
      "loss": 2.0107,
      "step": 42580
    },
    {
      "epoch": 21.663275686673447,
      "grad_norm": 35.79555130004883,
      "learning_rate": 2.8336724313326552e-05,
      "loss": 2.1756,
      "step": 42590
    },
    {
      "epoch": 21.668362156663274,
      "grad_norm": 34.22414016723633,
      "learning_rate": 2.833163784333673e-05,
      "loss": 2.1041,
      "step": 42600
    },
    {
      "epoch": 21.6734486266531,
      "grad_norm": 27.274200439453125,
      "learning_rate": 2.83265513733469e-05,
      "loss": 2.0398,
      "step": 42610
    },
    {
      "epoch": 21.67853509664293,
      "grad_norm": 35.1093635559082,
      "learning_rate": 2.832146490335707e-05,
      "loss": 2.0438,
      "step": 42620
    },
    {
      "epoch": 21.683621566632755,
      "grad_norm": 30.51060676574707,
      "learning_rate": 2.8316378433367245e-05,
      "loss": 2.1995,
      "step": 42630
    },
    {
      "epoch": 21.688708036622582,
      "grad_norm": 28.49528694152832,
      "learning_rate": 2.831129196337742e-05,
      "loss": 2.0846,
      "step": 42640
    },
    {
      "epoch": 21.69379450661241,
      "grad_norm": 27.21674156188965,
      "learning_rate": 2.830620549338759e-05,
      "loss": 2.0579,
      "step": 42650
    },
    {
      "epoch": 21.698880976602236,
      "grad_norm": 31.336715698242188,
      "learning_rate": 2.8301119023397765e-05,
      "loss": 2.0468,
      "step": 42660
    },
    {
      "epoch": 21.703967446592067,
      "grad_norm": 36.24190902709961,
      "learning_rate": 2.8296032553407935e-05,
      "loss": 2.0049,
      "step": 42670
    },
    {
      "epoch": 21.709053916581894,
      "grad_norm": 35.73402786254883,
      "learning_rate": 2.8290946083418108e-05,
      "loss": 1.9982,
      "step": 42680
    },
    {
      "epoch": 21.71414038657172,
      "grad_norm": 35.474464416503906,
      "learning_rate": 2.8285859613428285e-05,
      "loss": 2.0738,
      "step": 42690
    },
    {
      "epoch": 21.719226856561548,
      "grad_norm": 40.75963592529297,
      "learning_rate": 2.8280773143438455e-05,
      "loss": 2.1166,
      "step": 42700
    },
    {
      "epoch": 21.724313326551375,
      "grad_norm": 26.610675811767578,
      "learning_rate": 2.8275686673448625e-05,
      "loss": 2.0759,
      "step": 42710
    },
    {
      "epoch": 21.729399796541202,
      "grad_norm": 32.15531921386719,
      "learning_rate": 2.82706002034588e-05,
      "loss": 2.086,
      "step": 42720
    },
    {
      "epoch": 21.73448626653103,
      "grad_norm": 33.35866165161133,
      "learning_rate": 2.8265513733468975e-05,
      "loss": 2.1275,
      "step": 42730
    },
    {
      "epoch": 21.739572736520856,
      "grad_norm": 40.77842330932617,
      "learning_rate": 2.8260427263479145e-05,
      "loss": 2.0287,
      "step": 42740
    },
    {
      "epoch": 21.744659206510683,
      "grad_norm": 35.16912078857422,
      "learning_rate": 2.825534079348932e-05,
      "loss": 2.1267,
      "step": 42750
    },
    {
      "epoch": 21.74974567650051,
      "grad_norm": 35.97316360473633,
      "learning_rate": 2.825025432349949e-05,
      "loss": 2.1054,
      "step": 42760
    },
    {
      "epoch": 21.754832146490337,
      "grad_norm": 42.55373764038086,
      "learning_rate": 2.8245167853509664e-05,
      "loss": 2.005,
      "step": 42770
    },
    {
      "epoch": 21.759918616480164,
      "grad_norm": 33.777915954589844,
      "learning_rate": 2.824008138351984e-05,
      "loss": 2.1556,
      "step": 42780
    },
    {
      "epoch": 21.76500508646999,
      "grad_norm": 28.307552337646484,
      "learning_rate": 2.823499491353001e-05,
      "loss": 2.1014,
      "step": 42790
    },
    {
      "epoch": 21.770091556459818,
      "grad_norm": 32.38087844848633,
      "learning_rate": 2.822990844354018e-05,
      "loss": 2.1237,
      "step": 42800
    },
    {
      "epoch": 21.775178026449645,
      "grad_norm": 39.907737731933594,
      "learning_rate": 2.8224821973550357e-05,
      "loss": 2.0996,
      "step": 42810
    },
    {
      "epoch": 21.78026449643947,
      "grad_norm": 31.591020584106445,
      "learning_rate": 2.821973550356053e-05,
      "loss": 2.0135,
      "step": 42820
    },
    {
      "epoch": 21.7853509664293,
      "grad_norm": 34.34022521972656,
      "learning_rate": 2.8214649033570707e-05,
      "loss": 1.9796,
      "step": 42830
    },
    {
      "epoch": 21.790437436419126,
      "grad_norm": 33.242801666259766,
      "learning_rate": 2.8209562563580877e-05,
      "loss": 2.035,
      "step": 42840
    },
    {
      "epoch": 21.795523906408953,
      "grad_norm": 41.969417572021484,
      "learning_rate": 2.8204476093591047e-05,
      "loss": 2.2066,
      "step": 42850
    },
    {
      "epoch": 21.80061037639878,
      "grad_norm": 46.065494537353516,
      "learning_rate": 2.8199389623601224e-05,
      "loss": 2.071,
      "step": 42860
    },
    {
      "epoch": 21.805696846388607,
      "grad_norm": 32.279327392578125,
      "learning_rate": 2.8194303153611397e-05,
      "loss": 2.0632,
      "step": 42870
    },
    {
      "epoch": 21.810783316378433,
      "grad_norm": 40.95052719116211,
      "learning_rate": 2.8189216683621567e-05,
      "loss": 2.1335,
      "step": 42880
    },
    {
      "epoch": 21.81586978636826,
      "grad_norm": 34.27162551879883,
      "learning_rate": 2.8184130213631744e-05,
      "loss": 2.0846,
      "step": 42890
    },
    {
      "epoch": 21.820956256358087,
      "grad_norm": 30.527786254882812,
      "learning_rate": 2.8179043743641914e-05,
      "loss": 2.0653,
      "step": 42900
    },
    {
      "epoch": 21.826042726347914,
      "grad_norm": 33.70276641845703,
      "learning_rate": 2.8173957273652084e-05,
      "loss": 2.0993,
      "step": 42910
    },
    {
      "epoch": 21.83112919633774,
      "grad_norm": 37.412906646728516,
      "learning_rate": 2.816887080366226e-05,
      "loss": 2.033,
      "step": 42920
    },
    {
      "epoch": 21.83621566632757,
      "grad_norm": 35.334747314453125,
      "learning_rate": 2.8163784333672433e-05,
      "loss": 2.083,
      "step": 42930
    },
    {
      "epoch": 21.841302136317395,
      "grad_norm": 38.13743209838867,
      "learning_rate": 2.8158697863682603e-05,
      "loss": 1.9739,
      "step": 42940
    },
    {
      "epoch": 21.846388606307222,
      "grad_norm": 35.54281234741211,
      "learning_rate": 2.815361139369278e-05,
      "loss": 2.1731,
      "step": 42950
    },
    {
      "epoch": 21.85147507629705,
      "grad_norm": 32.5272331237793,
      "learning_rate": 2.814852492370295e-05,
      "loss": 2.0922,
      "step": 42960
    },
    {
      "epoch": 21.856561546286876,
      "grad_norm": 33.299922943115234,
      "learning_rate": 2.8143438453713123e-05,
      "loss": 2.1076,
      "step": 42970
    },
    {
      "epoch": 21.861648016276703,
      "grad_norm": 28.92632484436035,
      "learning_rate": 2.81383519837233e-05,
      "loss": 2.1118,
      "step": 42980
    },
    {
      "epoch": 21.86673448626653,
      "grad_norm": 44.50676727294922,
      "learning_rate": 2.813326551373347e-05,
      "loss": 2.0205,
      "step": 42990
    },
    {
      "epoch": 21.871820956256357,
      "grad_norm": 32.85658645629883,
      "learning_rate": 2.812817904374364e-05,
      "loss": 1.9964,
      "step": 43000
    },
    {
      "epoch": 21.876907426246184,
      "grad_norm": 33.1197395324707,
      "learning_rate": 2.8123092573753816e-05,
      "loss": 2.1141,
      "step": 43010
    },
    {
      "epoch": 21.88199389623601,
      "grad_norm": 28.140012741088867,
      "learning_rate": 2.811800610376399e-05,
      "loss": 2.0542,
      "step": 43020
    },
    {
      "epoch": 21.887080366225838,
      "grad_norm": 41.28217697143555,
      "learning_rate": 2.811291963377416e-05,
      "loss": 2.0142,
      "step": 43030
    },
    {
      "epoch": 21.892166836215665,
      "grad_norm": 35.99364471435547,
      "learning_rate": 2.8107833163784336e-05,
      "loss": 2.0532,
      "step": 43040
    },
    {
      "epoch": 21.897253306205492,
      "grad_norm": 34.10272216796875,
      "learning_rate": 2.8102746693794506e-05,
      "loss": 2.0873,
      "step": 43050
    },
    {
      "epoch": 21.90233977619532,
      "grad_norm": 32.81631851196289,
      "learning_rate": 2.809766022380468e-05,
      "loss": 1.9946,
      "step": 43060
    },
    {
      "epoch": 21.907426246185146,
      "grad_norm": 33.77281188964844,
      "learning_rate": 2.8092573753814856e-05,
      "loss": 2.0853,
      "step": 43070
    },
    {
      "epoch": 21.912512716174973,
      "grad_norm": 44.07342529296875,
      "learning_rate": 2.8087487283825026e-05,
      "loss": 2.0985,
      "step": 43080
    },
    {
      "epoch": 21.9175991861648,
      "grad_norm": 35.5406608581543,
      "learning_rate": 2.8082400813835203e-05,
      "loss": 2.0736,
      "step": 43090
    },
    {
      "epoch": 21.922685656154627,
      "grad_norm": 30.61703109741211,
      "learning_rate": 2.8077314343845372e-05,
      "loss": 2.1367,
      "step": 43100
    },
    {
      "epoch": 21.927772126144454,
      "grad_norm": 31.22992515563965,
      "learning_rate": 2.8072227873855546e-05,
      "loss": 2.1054,
      "step": 43110
    },
    {
      "epoch": 21.93285859613428,
      "grad_norm": 35.04119110107422,
      "learning_rate": 2.8067141403865722e-05,
      "loss": 2.0284,
      "step": 43120
    },
    {
      "epoch": 21.93794506612411,
      "grad_norm": 29.474149703979492,
      "learning_rate": 2.8062054933875892e-05,
      "loss": 2.0743,
      "step": 43130
    },
    {
      "epoch": 21.94303153611394,
      "grad_norm": 42.54425811767578,
      "learning_rate": 2.8056968463886062e-05,
      "loss": 2.101,
      "step": 43140
    },
    {
      "epoch": 21.948118006103766,
      "grad_norm": 39.31981658935547,
      "learning_rate": 2.805188199389624e-05,
      "loss": 2.0626,
      "step": 43150
    },
    {
      "epoch": 21.953204476093592,
      "grad_norm": 41.579505920410156,
      "learning_rate": 2.8046795523906412e-05,
      "loss": 2.1366,
      "step": 43160
    },
    {
      "epoch": 21.95829094608342,
      "grad_norm": 41.83060836791992,
      "learning_rate": 2.8041709053916582e-05,
      "loss": 2.0858,
      "step": 43170
    },
    {
      "epoch": 21.963377416073246,
      "grad_norm": 30.895191192626953,
      "learning_rate": 2.803662258392676e-05,
      "loss": 2.1189,
      "step": 43180
    },
    {
      "epoch": 21.968463886063073,
      "grad_norm": 31.65761947631836,
      "learning_rate": 2.803153611393693e-05,
      "loss": 2.0846,
      "step": 43190
    },
    {
      "epoch": 21.9735503560529,
      "grad_norm": 29.07177734375,
      "learning_rate": 2.8026449643947102e-05,
      "loss": 2.1077,
      "step": 43200
    },
    {
      "epoch": 21.978636826042727,
      "grad_norm": 35.604759216308594,
      "learning_rate": 2.8021363173957275e-05,
      "loss": 2.0969,
      "step": 43210
    },
    {
      "epoch": 21.983723296032554,
      "grad_norm": 36.39888000488281,
      "learning_rate": 2.801627670396745e-05,
      "loss": 2.1086,
      "step": 43220
    },
    {
      "epoch": 21.98880976602238,
      "grad_norm": 32.55729293823242,
      "learning_rate": 2.801119023397762e-05,
      "loss": 2.0282,
      "step": 43230
    },
    {
      "epoch": 21.99389623601221,
      "grad_norm": 29.058317184448242,
      "learning_rate": 2.8006103763987795e-05,
      "loss": 2.1594,
      "step": 43240
    },
    {
      "epoch": 21.998982706002035,
      "grad_norm": 27.87195587158203,
      "learning_rate": 2.8001017293997965e-05,
      "loss": 2.1313,
      "step": 43250
    },
    {
      "epoch": 22.0,
      "eval_loss": 4.435009002685547,
      "eval_runtime": 2.7413,
      "eval_samples_per_second": 1012.282,
      "eval_steps_per_second": 126.581,
      "step": 43252
    },
    {
      "epoch": 22.004069175991862,
      "grad_norm": 33.53292465209961,
      "learning_rate": 2.7995930824008138e-05,
      "loss": 1.9557,
      "step": 43260
    },
    {
      "epoch": 22.00915564598169,
      "grad_norm": 34.72980880737305,
      "learning_rate": 2.7990844354018315e-05,
      "loss": 2.0789,
      "step": 43270
    },
    {
      "epoch": 22.014242115971516,
      "grad_norm": 48.21207046508789,
      "learning_rate": 2.7985757884028485e-05,
      "loss": 2.0418,
      "step": 43280
    },
    {
      "epoch": 22.019328585961343,
      "grad_norm": 33.58070373535156,
      "learning_rate": 2.7980671414038655e-05,
      "loss": 2.0466,
      "step": 43290
    },
    {
      "epoch": 22.02441505595117,
      "grad_norm": 37.7282600402832,
      "learning_rate": 2.797558494404883e-05,
      "loss": 1.9772,
      "step": 43300
    },
    {
      "epoch": 22.029501525940997,
      "grad_norm": 31.25157928466797,
      "learning_rate": 2.7970498474059005e-05,
      "loss": 2.1403,
      "step": 43310
    },
    {
      "epoch": 22.034587995930824,
      "grad_norm": 33.495849609375,
      "learning_rate": 2.7965412004069175e-05,
      "loss": 2.0462,
      "step": 43320
    },
    {
      "epoch": 22.03967446592065,
      "grad_norm": 41.14691925048828,
      "learning_rate": 2.796032553407935e-05,
      "loss": 2.1183,
      "step": 43330
    },
    {
      "epoch": 22.044760935910478,
      "grad_norm": 33.482398986816406,
      "learning_rate": 2.795523906408952e-05,
      "loss": 2.0952,
      "step": 43340
    },
    {
      "epoch": 22.049847405900305,
      "grad_norm": 33.4313850402832,
      "learning_rate": 2.7950152594099698e-05,
      "loss": 2.0601,
      "step": 43350
    },
    {
      "epoch": 22.054933875890132,
      "grad_norm": 42.60820007324219,
      "learning_rate": 2.794506612410987e-05,
      "loss": 2.0695,
      "step": 43360
    },
    {
      "epoch": 22.06002034587996,
      "grad_norm": 40.37761688232422,
      "learning_rate": 2.793997965412004e-05,
      "loss": 2.0304,
      "step": 43370
    },
    {
      "epoch": 22.065106815869786,
      "grad_norm": 33.00302505493164,
      "learning_rate": 2.7934893184130218e-05,
      "loss": 2.0277,
      "step": 43380
    },
    {
      "epoch": 22.070193285859613,
      "grad_norm": 37.90838623046875,
      "learning_rate": 2.7929806714140387e-05,
      "loss": 1.985,
      "step": 43390
    },
    {
      "epoch": 22.07527975584944,
      "grad_norm": 35.962791442871094,
      "learning_rate": 2.792472024415056e-05,
      "loss": 2.0794,
      "step": 43400
    },
    {
      "epoch": 22.080366225839267,
      "grad_norm": 27.99739646911621,
      "learning_rate": 2.7919633774160737e-05,
      "loss": 2.1015,
      "step": 43410
    },
    {
      "epoch": 22.085452695829094,
      "grad_norm": 34.25790786743164,
      "learning_rate": 2.7914547304170907e-05,
      "loss": 2.0184,
      "step": 43420
    },
    {
      "epoch": 22.09053916581892,
      "grad_norm": 37.10643005371094,
      "learning_rate": 2.7909460834181077e-05,
      "loss": 2.0293,
      "step": 43430
    },
    {
      "epoch": 22.095625635808748,
      "grad_norm": 34.14979934692383,
      "learning_rate": 2.7904374364191254e-05,
      "loss": 2.0047,
      "step": 43440
    },
    {
      "epoch": 22.100712105798575,
      "grad_norm": 38.63032913208008,
      "learning_rate": 2.7899287894201427e-05,
      "loss": 2.0543,
      "step": 43450
    },
    {
      "epoch": 22.105798575788402,
      "grad_norm": 32.9798698425293,
      "learning_rate": 2.7894201424211597e-05,
      "loss": 1.9812,
      "step": 43460
    },
    {
      "epoch": 22.11088504577823,
      "grad_norm": 38.338645935058594,
      "learning_rate": 2.7889114954221774e-05,
      "loss": 1.9768,
      "step": 43470
    },
    {
      "epoch": 22.115971515768056,
      "grad_norm": 28.675113677978516,
      "learning_rate": 2.7884028484231944e-05,
      "loss": 2.1065,
      "step": 43480
    },
    {
      "epoch": 22.121057985757883,
      "grad_norm": 33.76621627807617,
      "learning_rate": 2.7878942014242117e-05,
      "loss": 2.1118,
      "step": 43490
    },
    {
      "epoch": 22.12614445574771,
      "grad_norm": 32.92446517944336,
      "learning_rate": 2.7873855544252294e-05,
      "loss": 2.0759,
      "step": 43500
    },
    {
      "epoch": 22.131230925737537,
      "grad_norm": 36.77465057373047,
      "learning_rate": 2.7868769074262463e-05,
      "loss": 2.0819,
      "step": 43510
    },
    {
      "epoch": 22.136317395727364,
      "grad_norm": 33.53413391113281,
      "learning_rate": 2.7863682604272633e-05,
      "loss": 2.0706,
      "step": 43520
    },
    {
      "epoch": 22.14140386571719,
      "grad_norm": 27.05391502380371,
      "learning_rate": 2.785859613428281e-05,
      "loss": 2.0292,
      "step": 43530
    },
    {
      "epoch": 22.146490335707018,
      "grad_norm": 39.777183532714844,
      "learning_rate": 2.785350966429298e-05,
      "loss": 2.0286,
      "step": 43540
    },
    {
      "epoch": 22.151576805696845,
      "grad_norm": 38.437068939208984,
      "learning_rate": 2.7848423194303153e-05,
      "loss": 2.0072,
      "step": 43550
    },
    {
      "epoch": 22.15666327568667,
      "grad_norm": 45.801883697509766,
      "learning_rate": 2.784333672431333e-05,
      "loss": 2.1231,
      "step": 43560
    },
    {
      "epoch": 22.161749745676502,
      "grad_norm": 29.524696350097656,
      "learning_rate": 2.78382502543235e-05,
      "loss": 2.006,
      "step": 43570
    },
    {
      "epoch": 22.16683621566633,
      "grad_norm": 30.101551055908203,
      "learning_rate": 2.783316378433367e-05,
      "loss": 2.0668,
      "step": 43580
    },
    {
      "epoch": 22.171922685656156,
      "grad_norm": 33.985450744628906,
      "learning_rate": 2.7828077314343846e-05,
      "loss": 2.0787,
      "step": 43590
    },
    {
      "epoch": 22.177009155645983,
      "grad_norm": 36.93098449707031,
      "learning_rate": 2.782299084435402e-05,
      "loss": 1.987,
      "step": 43600
    },
    {
      "epoch": 22.18209562563581,
      "grad_norm": 32.19481658935547,
      "learning_rate": 2.781790437436419e-05,
      "loss": 2.0031,
      "step": 43610
    },
    {
      "epoch": 22.187182095625637,
      "grad_norm": 36.242008209228516,
      "learning_rate": 2.7812817904374366e-05,
      "loss": 1.9271,
      "step": 43620
    },
    {
      "epoch": 22.192268565615464,
      "grad_norm": 27.9896183013916,
      "learning_rate": 2.7807731434384536e-05,
      "loss": 1.9995,
      "step": 43630
    },
    {
      "epoch": 22.19735503560529,
      "grad_norm": 32.138328552246094,
      "learning_rate": 2.7802644964394713e-05,
      "loss": 2.0227,
      "step": 43640
    },
    {
      "epoch": 22.202441505595118,
      "grad_norm": 30.810802459716797,
      "learning_rate": 2.7797558494404886e-05,
      "loss": 1.9216,
      "step": 43650
    },
    {
      "epoch": 22.207527975584945,
      "grad_norm": 34.69437026977539,
      "learning_rate": 2.7792472024415056e-05,
      "loss": 2.1118,
      "step": 43660
    },
    {
      "epoch": 22.212614445574772,
      "grad_norm": 35.326942443847656,
      "learning_rate": 2.7787385554425233e-05,
      "loss": 1.9911,
      "step": 43670
    },
    {
      "epoch": 22.2177009155646,
      "grad_norm": 38.3615837097168,
      "learning_rate": 2.7782299084435402e-05,
      "loss": 1.9844,
      "step": 43680
    },
    {
      "epoch": 22.222787385554426,
      "grad_norm": 39.53590393066406,
      "learning_rate": 2.7777212614445576e-05,
      "loss": 1.9984,
      "step": 43690
    },
    {
      "epoch": 22.227873855544253,
      "grad_norm": 36.45431900024414,
      "learning_rate": 2.7772126144455752e-05,
      "loss": 2.1343,
      "step": 43700
    },
    {
      "epoch": 22.23296032553408,
      "grad_norm": 44.33462142944336,
      "learning_rate": 2.7767039674465922e-05,
      "loss": 2.0032,
      "step": 43710
    },
    {
      "epoch": 22.238046795523907,
      "grad_norm": 40.959896087646484,
      "learning_rate": 2.7761953204476092e-05,
      "loss": 2.1562,
      "step": 43720
    },
    {
      "epoch": 22.243133265513734,
      "grad_norm": 30.403295516967773,
      "learning_rate": 2.775686673448627e-05,
      "loss": 2.0999,
      "step": 43730
    },
    {
      "epoch": 22.24821973550356,
      "grad_norm": 31.503843307495117,
      "learning_rate": 2.7751780264496442e-05,
      "loss": 2.1081,
      "step": 43740
    },
    {
      "epoch": 22.253306205493388,
      "grad_norm": 34.00200271606445,
      "learning_rate": 2.7746693794506612e-05,
      "loss": 1.9434,
      "step": 43750
    },
    {
      "epoch": 22.258392675483215,
      "grad_norm": 35.779666900634766,
      "learning_rate": 2.774160732451679e-05,
      "loss": 2.0188,
      "step": 43760
    },
    {
      "epoch": 22.263479145473042,
      "grad_norm": 26.52848243713379,
      "learning_rate": 2.773652085452696e-05,
      "loss": 1.9633,
      "step": 43770
    },
    {
      "epoch": 22.26856561546287,
      "grad_norm": 29.73011016845703,
      "learning_rate": 2.7731434384537132e-05,
      "loss": 1.9657,
      "step": 43780
    },
    {
      "epoch": 22.273652085452696,
      "grad_norm": 29.47369956970215,
      "learning_rate": 2.772634791454731e-05,
      "loss": 2.1356,
      "step": 43790
    },
    {
      "epoch": 22.278738555442523,
      "grad_norm": 33.49742889404297,
      "learning_rate": 2.772126144455748e-05,
      "loss": 2.0593,
      "step": 43800
    },
    {
      "epoch": 22.28382502543235,
      "grad_norm": 35.058448791503906,
      "learning_rate": 2.771617497456765e-05,
      "loss": 2.0667,
      "step": 43810
    },
    {
      "epoch": 22.288911495422177,
      "grad_norm": 48.575984954833984,
      "learning_rate": 2.7711088504577825e-05,
      "loss": 1.9866,
      "step": 43820
    },
    {
      "epoch": 22.293997965412004,
      "grad_norm": 33.23904800415039,
      "learning_rate": 2.7706002034588e-05,
      "loss": 2.1249,
      "step": 43830
    },
    {
      "epoch": 22.29908443540183,
      "grad_norm": 43.876731872558594,
      "learning_rate": 2.7700915564598168e-05,
      "loss": 2.0903,
      "step": 43840
    },
    {
      "epoch": 22.304170905391658,
      "grad_norm": 44.9362678527832,
      "learning_rate": 2.7695829094608345e-05,
      "loss": 2.0017,
      "step": 43850
    },
    {
      "epoch": 22.309257375381485,
      "grad_norm": 40.12593078613281,
      "learning_rate": 2.7690742624618515e-05,
      "loss": 1.972,
      "step": 43860
    },
    {
      "epoch": 22.31434384537131,
      "grad_norm": 32.268310546875,
      "learning_rate": 2.7685656154628685e-05,
      "loss": 1.9608,
      "step": 43870
    },
    {
      "epoch": 22.31943031536114,
      "grad_norm": 44.15818405151367,
      "learning_rate": 2.768056968463886e-05,
      "loss": 2.0827,
      "step": 43880
    },
    {
      "epoch": 22.324516785350966,
      "grad_norm": 31.31615447998047,
      "learning_rate": 2.7675483214649035e-05,
      "loss": 2.0523,
      "step": 43890
    },
    {
      "epoch": 22.329603255340793,
      "grad_norm": 35.09787368774414,
      "learning_rate": 2.767039674465921e-05,
      "loss": 2.0265,
      "step": 43900
    },
    {
      "epoch": 22.33468972533062,
      "grad_norm": 34.941322326660156,
      "learning_rate": 2.766531027466938e-05,
      "loss": 1.9883,
      "step": 43910
    },
    {
      "epoch": 22.339776195320447,
      "grad_norm": 31.945289611816406,
      "learning_rate": 2.766022380467955e-05,
      "loss": 2.0337,
      "step": 43920
    },
    {
      "epoch": 22.344862665310274,
      "grad_norm": 32.030521392822266,
      "learning_rate": 2.7655137334689728e-05,
      "loss": 2.083,
      "step": 43930
    },
    {
      "epoch": 22.3499491353001,
      "grad_norm": 36.81962203979492,
      "learning_rate": 2.76500508646999e-05,
      "loss": 2.0158,
      "step": 43940
    },
    {
      "epoch": 22.355035605289928,
      "grad_norm": 27.627962112426758,
      "learning_rate": 2.764496439471007e-05,
      "loss": 2.0215,
      "step": 43950
    },
    {
      "epoch": 22.360122075279754,
      "grad_norm": 31.169147491455078,
      "learning_rate": 2.7639877924720248e-05,
      "loss": 2.1091,
      "step": 43960
    },
    {
      "epoch": 22.36520854526958,
      "grad_norm": 29.621129989624023,
      "learning_rate": 2.7634791454730417e-05,
      "loss": 2.0845,
      "step": 43970
    },
    {
      "epoch": 22.37029501525941,
      "grad_norm": 32.16844940185547,
      "learning_rate": 2.762970498474059e-05,
      "loss": 2.0282,
      "step": 43980
    },
    {
      "epoch": 22.375381485249235,
      "grad_norm": 40.08930969238281,
      "learning_rate": 2.7624618514750767e-05,
      "loss": 2.0059,
      "step": 43990
    },
    {
      "epoch": 22.380467955239062,
      "grad_norm": 40.29178237915039,
      "learning_rate": 2.7619532044760937e-05,
      "loss": 2.0487,
      "step": 44000
    },
    {
      "epoch": 22.38555442522889,
      "grad_norm": 34.574127197265625,
      "learning_rate": 2.7614445574771107e-05,
      "loss": 2.0216,
      "step": 44010
    },
    {
      "epoch": 22.39064089521872,
      "grad_norm": 38.775875091552734,
      "learning_rate": 2.7609359104781284e-05,
      "loss": 2.0993,
      "step": 44020
    },
    {
      "epoch": 22.395727365208547,
      "grad_norm": 33.83543014526367,
      "learning_rate": 2.7604272634791457e-05,
      "loss": 1.9304,
      "step": 44030
    },
    {
      "epoch": 22.400813835198374,
      "grad_norm": 36.82752227783203,
      "learning_rate": 2.7599186164801627e-05,
      "loss": 2.0876,
      "step": 44040
    },
    {
      "epoch": 22.4059003051882,
      "grad_norm": 30.773977279663086,
      "learning_rate": 2.7594099694811804e-05,
      "loss": 2.027,
      "step": 44050
    },
    {
      "epoch": 22.410986775178028,
      "grad_norm": 25.1766414642334,
      "learning_rate": 2.7589013224821974e-05,
      "loss": 2.1455,
      "step": 44060
    },
    {
      "epoch": 22.416073245167855,
      "grad_norm": 32.15791320800781,
      "learning_rate": 2.7583926754832147e-05,
      "loss": 2.1226,
      "step": 44070
    },
    {
      "epoch": 22.421159715157682,
      "grad_norm": 32.69231414794922,
      "learning_rate": 2.7578840284842324e-05,
      "loss": 2.0835,
      "step": 44080
    },
    {
      "epoch": 22.42624618514751,
      "grad_norm": 33.30265426635742,
      "learning_rate": 2.7573753814852493e-05,
      "loss": 2.0938,
      "step": 44090
    },
    {
      "epoch": 22.431332655137336,
      "grad_norm": 43.52494812011719,
      "learning_rate": 2.7568667344862663e-05,
      "loss": 2.0121,
      "step": 44100
    },
    {
      "epoch": 22.436419125127163,
      "grad_norm": 35.79439163208008,
      "learning_rate": 2.756358087487284e-05,
      "loss": 2.0398,
      "step": 44110
    },
    {
      "epoch": 22.44150559511699,
      "grad_norm": 30.547382354736328,
      "learning_rate": 2.7558494404883013e-05,
      "loss": 1.9872,
      "step": 44120
    },
    {
      "epoch": 22.446592065106817,
      "grad_norm": 32.51185989379883,
      "learning_rate": 2.7553407934893183e-05,
      "loss": 2.0762,
      "step": 44130
    },
    {
      "epoch": 22.451678535096644,
      "grad_norm": 32.89393997192383,
      "learning_rate": 2.754832146490336e-05,
      "loss": 2.0216,
      "step": 44140
    },
    {
      "epoch": 22.45676500508647,
      "grad_norm": 31.90268325805664,
      "learning_rate": 2.754323499491353e-05,
      "loss": 2.0555,
      "step": 44150
    },
    {
      "epoch": 22.461851475076298,
      "grad_norm": 28.947660446166992,
      "learning_rate": 2.7538148524923706e-05,
      "loss": 2.1227,
      "step": 44160
    },
    {
      "epoch": 22.466937945066125,
      "grad_norm": 34.58647918701172,
      "learning_rate": 2.7533062054933876e-05,
      "loss": 2.0645,
      "step": 44170
    },
    {
      "epoch": 22.47202441505595,
      "grad_norm": 30.519071578979492,
      "learning_rate": 2.752797558494405e-05,
      "loss": 2.0655,
      "step": 44180
    },
    {
      "epoch": 22.47711088504578,
      "grad_norm": 40.55244445800781,
      "learning_rate": 2.7522889114954226e-05,
      "loss": 1.9852,
      "step": 44190
    },
    {
      "epoch": 22.482197355035606,
      "grad_norm": 30.605485916137695,
      "learning_rate": 2.7517802644964396e-05,
      "loss": 2.0721,
      "step": 44200
    },
    {
      "epoch": 22.487283825025433,
      "grad_norm": 42.9391975402832,
      "learning_rate": 2.7512716174974566e-05,
      "loss": 2.068,
      "step": 44210
    },
    {
      "epoch": 22.49237029501526,
      "grad_norm": 32.27115249633789,
      "learning_rate": 2.7507629704984743e-05,
      "loss": 2.034,
      "step": 44220
    },
    {
      "epoch": 22.497456765005087,
      "grad_norm": 35.723941802978516,
      "learning_rate": 2.7502543234994916e-05,
      "loss": 2.0269,
      "step": 44230
    },
    {
      "epoch": 22.502543234994913,
      "grad_norm": 35.002235412597656,
      "learning_rate": 2.7497456765005086e-05,
      "loss": 2.0536,
      "step": 44240
    },
    {
      "epoch": 22.50762970498474,
      "grad_norm": 34.85966110229492,
      "learning_rate": 2.7492370295015263e-05,
      "loss": 1.9969,
      "step": 44250
    },
    {
      "epoch": 22.512716174974567,
      "grad_norm": 36.94814682006836,
      "learning_rate": 2.7487283825025432e-05,
      "loss": 2.0161,
      "step": 44260
    },
    {
      "epoch": 22.517802644964394,
      "grad_norm": 34.232425689697266,
      "learning_rate": 2.7482197355035606e-05,
      "loss": 2.0844,
      "step": 44270
    },
    {
      "epoch": 22.52288911495422,
      "grad_norm": 26.754619598388672,
      "learning_rate": 2.7477110885045782e-05,
      "loss": 2.0241,
      "step": 44280
    },
    {
      "epoch": 22.52797558494405,
      "grad_norm": 31.846471786499023,
      "learning_rate": 2.7472024415055952e-05,
      "loss": 1.9378,
      "step": 44290
    },
    {
      "epoch": 22.533062054933875,
      "grad_norm": 30.868289947509766,
      "learning_rate": 2.7466937945066122e-05,
      "loss": 2.0877,
      "step": 44300
    },
    {
      "epoch": 22.538148524923702,
      "grad_norm": 33.30907440185547,
      "learning_rate": 2.74618514750763e-05,
      "loss": 2.0354,
      "step": 44310
    },
    {
      "epoch": 22.54323499491353,
      "grad_norm": 40.194942474365234,
      "learning_rate": 2.7456765005086472e-05,
      "loss": 1.9737,
      "step": 44320
    },
    {
      "epoch": 22.548321464903356,
      "grad_norm": 34.27427673339844,
      "learning_rate": 2.7451678535096642e-05,
      "loss": 2.027,
      "step": 44330
    },
    {
      "epoch": 22.553407934893183,
      "grad_norm": 36.97737503051758,
      "learning_rate": 2.744659206510682e-05,
      "loss": 2.0096,
      "step": 44340
    },
    {
      "epoch": 22.55849440488301,
      "grad_norm": 33.81875228881836,
      "learning_rate": 2.744150559511699e-05,
      "loss": 1.9971,
      "step": 44350
    },
    {
      "epoch": 22.563580874872837,
      "grad_norm": 37.14067077636719,
      "learning_rate": 2.7436419125127162e-05,
      "loss": 1.9364,
      "step": 44360
    },
    {
      "epoch": 22.568667344862664,
      "grad_norm": 39.88111877441406,
      "learning_rate": 2.743133265513734e-05,
      "loss": 2.0334,
      "step": 44370
    },
    {
      "epoch": 22.57375381485249,
      "grad_norm": 40.614349365234375,
      "learning_rate": 2.742624618514751e-05,
      "loss": 2.0613,
      "step": 44380
    },
    {
      "epoch": 22.578840284842318,
      "grad_norm": 43.5611686706543,
      "learning_rate": 2.742115971515768e-05,
      "loss": 1.9952,
      "step": 44390
    },
    {
      "epoch": 22.583926754832145,
      "grad_norm": 42.449363708496094,
      "learning_rate": 2.7416073245167855e-05,
      "loss": 2.0618,
      "step": 44400
    },
    {
      "epoch": 22.589013224821972,
      "grad_norm": 37.83925247192383,
      "learning_rate": 2.741098677517803e-05,
      "loss": 2.0805,
      "step": 44410
    },
    {
      "epoch": 22.5940996948118,
      "grad_norm": 34.25170135498047,
      "learning_rate": 2.7405900305188198e-05,
      "loss": 2.0408,
      "step": 44420
    },
    {
      "epoch": 22.599186164801626,
      "grad_norm": 34.534584045410156,
      "learning_rate": 2.7400813835198375e-05,
      "loss": 2.0746,
      "step": 44430
    },
    {
      "epoch": 22.604272634791453,
      "grad_norm": 29.09772300720215,
      "learning_rate": 2.7395727365208545e-05,
      "loss": 2.0645,
      "step": 44440
    },
    {
      "epoch": 22.60935910478128,
      "grad_norm": 31.64373207092285,
      "learning_rate": 2.739064089521872e-05,
      "loss": 2.1027,
      "step": 44450
    },
    {
      "epoch": 22.61444557477111,
      "grad_norm": 39.14202880859375,
      "learning_rate": 2.7385554425228895e-05,
      "loss": 2.0345,
      "step": 44460
    },
    {
      "epoch": 22.619532044760938,
      "grad_norm": 37.98154830932617,
      "learning_rate": 2.7380467955239065e-05,
      "loss": 2.0087,
      "step": 44470
    },
    {
      "epoch": 22.624618514750765,
      "grad_norm": 38.88151931762695,
      "learning_rate": 2.737538148524924e-05,
      "loss": 1.9246,
      "step": 44480
    },
    {
      "epoch": 22.62970498474059,
      "grad_norm": 33.14446258544922,
      "learning_rate": 2.737029501525941e-05,
      "loss": 2.1552,
      "step": 44490
    },
    {
      "epoch": 22.63479145473042,
      "grad_norm": 31.513465881347656,
      "learning_rate": 2.736520854526958e-05,
      "loss": 2.0362,
      "step": 44500
    },
    {
      "epoch": 22.639877924720246,
      "grad_norm": 47.40877914428711,
      "learning_rate": 2.7360122075279758e-05,
      "loss": 2.1026,
      "step": 44510
    },
    {
      "epoch": 22.644964394710072,
      "grad_norm": 36.616180419921875,
      "learning_rate": 2.735503560528993e-05,
      "loss": 2.0169,
      "step": 44520
    },
    {
      "epoch": 22.6500508646999,
      "grad_norm": 33.08859634399414,
      "learning_rate": 2.73499491353001e-05,
      "loss": 2.0526,
      "step": 44530
    },
    {
      "epoch": 22.655137334689726,
      "grad_norm": 40.879817962646484,
      "learning_rate": 2.7344862665310278e-05,
      "loss": 2.0195,
      "step": 44540
    },
    {
      "epoch": 22.660223804679553,
      "grad_norm": 32.622886657714844,
      "learning_rate": 2.7339776195320447e-05,
      "loss": 1.9871,
      "step": 44550
    },
    {
      "epoch": 22.66531027466938,
      "grad_norm": 33.19696044921875,
      "learning_rate": 2.733468972533062e-05,
      "loss": 2.0322,
      "step": 44560
    },
    {
      "epoch": 22.670396744659207,
      "grad_norm": 34.67409896850586,
      "learning_rate": 2.7329603255340797e-05,
      "loss": 2.1103,
      "step": 44570
    },
    {
      "epoch": 22.675483214649034,
      "grad_norm": 40.65532684326172,
      "learning_rate": 2.7324516785350967e-05,
      "loss": 2.076,
      "step": 44580
    },
    {
      "epoch": 22.68056968463886,
      "grad_norm": 43.038238525390625,
      "learning_rate": 2.7319430315361137e-05,
      "loss": 2.0319,
      "step": 44590
    },
    {
      "epoch": 22.68565615462869,
      "grad_norm": 35.371131896972656,
      "learning_rate": 2.7314343845371314e-05,
      "loss": 2.0342,
      "step": 44600
    },
    {
      "epoch": 22.690742624618515,
      "grad_norm": 35.9970703125,
      "learning_rate": 2.7309257375381487e-05,
      "loss": 1.9518,
      "step": 44610
    },
    {
      "epoch": 22.695829094608342,
      "grad_norm": 34.49226760864258,
      "learning_rate": 2.7304170905391657e-05,
      "loss": 2.0375,
      "step": 44620
    },
    {
      "epoch": 22.70091556459817,
      "grad_norm": 32.413597106933594,
      "learning_rate": 2.7299084435401834e-05,
      "loss": 1.9988,
      "step": 44630
    },
    {
      "epoch": 22.706002034587996,
      "grad_norm": 43.16256332397461,
      "learning_rate": 2.7293997965412004e-05,
      "loss": 2.0034,
      "step": 44640
    },
    {
      "epoch": 22.711088504577823,
      "grad_norm": 39.390892028808594,
      "learning_rate": 2.7288911495422177e-05,
      "loss": 2.0993,
      "step": 44650
    },
    {
      "epoch": 22.71617497456765,
      "grad_norm": 36.625152587890625,
      "learning_rate": 2.7283825025432354e-05,
      "loss": 1.9801,
      "step": 44660
    },
    {
      "epoch": 22.721261444557477,
      "grad_norm": 31.74110221862793,
      "learning_rate": 2.7278738555442523e-05,
      "loss": 2.1003,
      "step": 44670
    },
    {
      "epoch": 22.726347914547304,
      "grad_norm": 39.800289154052734,
      "learning_rate": 2.7273652085452693e-05,
      "loss": 2.0275,
      "step": 44680
    },
    {
      "epoch": 22.73143438453713,
      "grad_norm": 38.978633880615234,
      "learning_rate": 2.726856561546287e-05,
      "loss": 2.0702,
      "step": 44690
    },
    {
      "epoch": 22.736520854526958,
      "grad_norm": 38.681705474853516,
      "learning_rate": 2.7263479145473043e-05,
      "loss": 2.0294,
      "step": 44700
    },
    {
      "epoch": 22.741607324516785,
      "grad_norm": 36.24361801147461,
      "learning_rate": 2.725839267548322e-05,
      "loss": 2.0586,
      "step": 44710
    },
    {
      "epoch": 22.746693794506612,
      "grad_norm": 41.64812469482422,
      "learning_rate": 2.725330620549339e-05,
      "loss": 2.0232,
      "step": 44720
    },
    {
      "epoch": 22.75178026449644,
      "grad_norm": 39.608489990234375,
      "learning_rate": 2.724821973550356e-05,
      "loss": 2.0196,
      "step": 44730
    },
    {
      "epoch": 22.756866734486266,
      "grad_norm": 43.796321868896484,
      "learning_rate": 2.7243133265513736e-05,
      "loss": 2.1247,
      "step": 44740
    },
    {
      "epoch": 22.761953204476093,
      "grad_norm": 35.48135757446289,
      "learning_rate": 2.723804679552391e-05,
      "loss": 2.0185,
      "step": 44750
    },
    {
      "epoch": 22.76703967446592,
      "grad_norm": 44.456600189208984,
      "learning_rate": 2.723296032553408e-05,
      "loss": 2.0749,
      "step": 44760
    },
    {
      "epoch": 22.772126144455747,
      "grad_norm": 32.09415817260742,
      "learning_rate": 2.7227873855544256e-05,
      "loss": 1.8989,
      "step": 44770
    },
    {
      "epoch": 22.777212614445574,
      "grad_norm": 42.7186279296875,
      "learning_rate": 2.7222787385554426e-05,
      "loss": 1.9568,
      "step": 44780
    },
    {
      "epoch": 22.7822990844354,
      "grad_norm": 34.26142501831055,
      "learning_rate": 2.72177009155646e-05,
      "loss": 2.0232,
      "step": 44790
    },
    {
      "epoch": 22.787385554425228,
      "grad_norm": 42.72929000854492,
      "learning_rate": 2.7212614445574776e-05,
      "loss": 2.1108,
      "step": 44800
    },
    {
      "epoch": 22.792472024415055,
      "grad_norm": 34.62742233276367,
      "learning_rate": 2.7207527975584946e-05,
      "loss": 2.0208,
      "step": 44810
    },
    {
      "epoch": 22.797558494404882,
      "grad_norm": 37.68087387084961,
      "learning_rate": 2.7202441505595116e-05,
      "loss": 1.9847,
      "step": 44820
    },
    {
      "epoch": 22.80264496439471,
      "grad_norm": 35.979576110839844,
      "learning_rate": 2.7197355035605293e-05,
      "loss": 1.9543,
      "step": 44830
    },
    {
      "epoch": 22.807731434384536,
      "grad_norm": 41.944007873535156,
      "learning_rate": 2.7192268565615462e-05,
      "loss": 2.0105,
      "step": 44840
    },
    {
      "epoch": 22.812817904374363,
      "grad_norm": 39.09984588623047,
      "learning_rate": 2.7187182095625636e-05,
      "loss": 2.0824,
      "step": 44850
    },
    {
      "epoch": 22.81790437436419,
      "grad_norm": 32.3842887878418,
      "learning_rate": 2.7182095625635812e-05,
      "loss": 2.0792,
      "step": 44860
    },
    {
      "epoch": 22.822990844354017,
      "grad_norm": 37.88364791870117,
      "learning_rate": 2.7177009155645982e-05,
      "loss": 2.1203,
      "step": 44870
    },
    {
      "epoch": 22.828077314343844,
      "grad_norm": 31.29741859436035,
      "learning_rate": 2.7171922685656152e-05,
      "loss": 2.058,
      "step": 44880
    },
    {
      "epoch": 22.83316378433367,
      "grad_norm": 30.520750045776367,
      "learning_rate": 2.716683621566633e-05,
      "loss": 2.0027,
      "step": 44890
    },
    {
      "epoch": 22.838250254323498,
      "grad_norm": 31.28221321105957,
      "learning_rate": 2.7161749745676502e-05,
      "loss": 2.0379,
      "step": 44900
    },
    {
      "epoch": 22.843336724313325,
      "grad_norm": 35.38347625732422,
      "learning_rate": 2.7156663275686672e-05,
      "loss": 1.9945,
      "step": 44910
    },
    {
      "epoch": 22.848423194303155,
      "grad_norm": 31.18214225769043,
      "learning_rate": 2.715157680569685e-05,
      "loss": 1.963,
      "step": 44920
    },
    {
      "epoch": 22.853509664292982,
      "grad_norm": 37.33646774291992,
      "learning_rate": 2.714649033570702e-05,
      "loss": 1.935,
      "step": 44930
    },
    {
      "epoch": 22.85859613428281,
      "grad_norm": 37.36764907836914,
      "learning_rate": 2.7141403865717192e-05,
      "loss": 2.081,
      "step": 44940
    },
    {
      "epoch": 22.863682604272636,
      "grad_norm": 38.151309967041016,
      "learning_rate": 2.713631739572737e-05,
      "loss": 1.9643,
      "step": 44950
    },
    {
      "epoch": 22.868769074262463,
      "grad_norm": 39.52472686767578,
      "learning_rate": 2.713123092573754e-05,
      "loss": 1.9449,
      "step": 44960
    },
    {
      "epoch": 22.87385554425229,
      "grad_norm": 31.80577850341797,
      "learning_rate": 2.7126144455747715e-05,
      "loss": 2.0083,
      "step": 44970
    },
    {
      "epoch": 22.878942014242117,
      "grad_norm": 33.57508850097656,
      "learning_rate": 2.7121057985757885e-05,
      "loss": 2.0619,
      "step": 44980
    },
    {
      "epoch": 22.884028484231944,
      "grad_norm": 34.66614532470703,
      "learning_rate": 2.711597151576806e-05,
      "loss": 2.0441,
      "step": 44990
    },
    {
      "epoch": 22.88911495422177,
      "grad_norm": 36.21914291381836,
      "learning_rate": 2.7110885045778235e-05,
      "loss": 1.9935,
      "step": 45000
    },
    {
      "epoch": 22.894201424211598,
      "grad_norm": 32.947208404541016,
      "learning_rate": 2.7105798575788405e-05,
      "loss": 2.0992,
      "step": 45010
    },
    {
      "epoch": 22.899287894201425,
      "grad_norm": 35.05203628540039,
      "learning_rate": 2.7100712105798575e-05,
      "loss": 2.0452,
      "step": 45020
    },
    {
      "epoch": 22.904374364191252,
      "grad_norm": 48.28404998779297,
      "learning_rate": 2.709562563580875e-05,
      "loss": 2.0013,
      "step": 45030
    },
    {
      "epoch": 22.90946083418108,
      "grad_norm": 33.651763916015625,
      "learning_rate": 2.7090539165818925e-05,
      "loss": 2.061,
      "step": 45040
    },
    {
      "epoch": 22.914547304170906,
      "grad_norm": 32.358699798583984,
      "learning_rate": 2.7085452695829095e-05,
      "loss": 1.9359,
      "step": 45050
    },
    {
      "epoch": 22.919633774160733,
      "grad_norm": 40.811912536621094,
      "learning_rate": 2.708036622583927e-05,
      "loss": 2.0875,
      "step": 45060
    },
    {
      "epoch": 22.92472024415056,
      "grad_norm": 26.972795486450195,
      "learning_rate": 2.707527975584944e-05,
      "loss": 2.112,
      "step": 45070
    },
    {
      "epoch": 22.929806714140387,
      "grad_norm": 40.58733367919922,
      "learning_rate": 2.7070193285859614e-05,
      "loss": 2.0595,
      "step": 45080
    },
    {
      "epoch": 22.934893184130214,
      "grad_norm": 29.953649520874023,
      "learning_rate": 2.706510681586979e-05,
      "loss": 2.0368,
      "step": 45090
    },
    {
      "epoch": 22.93997965412004,
      "grad_norm": 35.728328704833984,
      "learning_rate": 2.706002034587996e-05,
      "loss": 2.1066,
      "step": 45100
    },
    {
      "epoch": 22.945066124109868,
      "grad_norm": 31.715572357177734,
      "learning_rate": 2.705493387589013e-05,
      "loss": 2.0296,
      "step": 45110
    },
    {
      "epoch": 22.950152594099695,
      "grad_norm": 32.69247055053711,
      "learning_rate": 2.7049847405900308e-05,
      "loss": 1.9611,
      "step": 45120
    },
    {
      "epoch": 22.955239064089522,
      "grad_norm": 36.75357437133789,
      "learning_rate": 2.704476093591048e-05,
      "loss": 1.9767,
      "step": 45130
    },
    {
      "epoch": 22.96032553407935,
      "grad_norm": 33.04874801635742,
      "learning_rate": 2.703967446592065e-05,
      "loss": 2.0431,
      "step": 45140
    },
    {
      "epoch": 22.965412004069176,
      "grad_norm": 31.948747634887695,
      "learning_rate": 2.7034587995930827e-05,
      "loss": 2.0392,
      "step": 45150
    },
    {
      "epoch": 22.970498474059003,
      "grad_norm": 35.56926727294922,
      "learning_rate": 2.7029501525940997e-05,
      "loss": 2.0403,
      "step": 45160
    },
    {
      "epoch": 22.97558494404883,
      "grad_norm": 41.29051208496094,
      "learning_rate": 2.7024415055951167e-05,
      "loss": 2.023,
      "step": 45170
    },
    {
      "epoch": 22.980671414038657,
      "grad_norm": 39.13153076171875,
      "learning_rate": 2.7019328585961344e-05,
      "loss": 1.9615,
      "step": 45180
    },
    {
      "epoch": 22.985757884028484,
      "grad_norm": 30.563047409057617,
      "learning_rate": 2.7014242115971517e-05,
      "loss": 2.0017,
      "step": 45190
    },
    {
      "epoch": 22.99084435401831,
      "grad_norm": 34.32279968261719,
      "learning_rate": 2.7009155645981687e-05,
      "loss": 2.0441,
      "step": 45200
    },
    {
      "epoch": 22.995930824008138,
      "grad_norm": 30.6851749420166,
      "learning_rate": 2.7004069175991864e-05,
      "loss": 2.0528,
      "step": 45210
    },
    {
      "epoch": 23.0,
      "eval_loss": 4.471407890319824,
      "eval_runtime": 2.7279,
      "eval_samples_per_second": 1017.284,
      "eval_steps_per_second": 127.206,
      "step": 45218
    },
    {
      "epoch": 23.001017293997965,
      "grad_norm": 33.229164123535156,
      "learning_rate": 2.6998982706002034e-05,
      "loss": 1.9733,
      "step": 45220
    },
    {
      "epoch": 23.00610376398779,
      "grad_norm": 28.427682876586914,
      "learning_rate": 2.6993896236012207e-05,
      "loss": 2.0274,
      "step": 45230
    },
    {
      "epoch": 23.01119023397762,
      "grad_norm": 34.877044677734375,
      "learning_rate": 2.6988809766022384e-05,
      "loss": 2.0633,
      "step": 45240
    },
    {
      "epoch": 23.016276703967446,
      "grad_norm": 33.99641036987305,
      "learning_rate": 2.6983723296032553e-05,
      "loss": 1.9956,
      "step": 45250
    },
    {
      "epoch": 23.021363173957273,
      "grad_norm": 40.734947204589844,
      "learning_rate": 2.697863682604273e-05,
      "loss": 2.0593,
      "step": 45260
    },
    {
      "epoch": 23.0264496439471,
      "grad_norm": 28.08223533630371,
      "learning_rate": 2.69735503560529e-05,
      "loss": 2.0125,
      "step": 45270
    },
    {
      "epoch": 23.031536113936927,
      "grad_norm": 34.70433044433594,
      "learning_rate": 2.6968463886063073e-05,
      "loss": 1.9317,
      "step": 45280
    },
    {
      "epoch": 23.036622583926754,
      "grad_norm": 34.79243850708008,
      "learning_rate": 2.696337741607325e-05,
      "loss": 1.9632,
      "step": 45290
    },
    {
      "epoch": 23.04170905391658,
      "grad_norm": 37.86808776855469,
      "learning_rate": 2.695829094608342e-05,
      "loss": 2.0235,
      "step": 45300
    },
    {
      "epoch": 23.046795523906408,
      "grad_norm": 33.084716796875,
      "learning_rate": 2.695320447609359e-05,
      "loss": 1.9594,
      "step": 45310
    },
    {
      "epoch": 23.051881993896234,
      "grad_norm": 34.50536346435547,
      "learning_rate": 2.6948118006103766e-05,
      "loss": 1.9942,
      "step": 45320
    },
    {
      "epoch": 23.05696846388606,
      "grad_norm": 52.524314880371094,
      "learning_rate": 2.694303153611394e-05,
      "loss": 1.9996,
      "step": 45330
    },
    {
      "epoch": 23.06205493387589,
      "grad_norm": 30.961219787597656,
      "learning_rate": 2.693794506612411e-05,
      "loss": 2.0707,
      "step": 45340
    },
    {
      "epoch": 23.067141403865715,
      "grad_norm": 36.70902633666992,
      "learning_rate": 2.6932858596134286e-05,
      "loss": 2.0993,
      "step": 45350
    },
    {
      "epoch": 23.072227873855546,
      "grad_norm": 37.13393783569336,
      "learning_rate": 2.6927772126144456e-05,
      "loss": 2.0266,
      "step": 45360
    },
    {
      "epoch": 23.077314343845373,
      "grad_norm": 41.368385314941406,
      "learning_rate": 2.692268565615463e-05,
      "loss": 1.9934,
      "step": 45370
    },
    {
      "epoch": 23.0824008138352,
      "grad_norm": 41.94026184082031,
      "learning_rate": 2.6917599186164806e-05,
      "loss": 1.9527,
      "step": 45380
    },
    {
      "epoch": 23.087487283825027,
      "grad_norm": 30.633460998535156,
      "learning_rate": 2.6912512716174976e-05,
      "loss": 2.0264,
      "step": 45390
    },
    {
      "epoch": 23.092573753814854,
      "grad_norm": 37.11309051513672,
      "learning_rate": 2.6907426246185146e-05,
      "loss": 1.9799,
      "step": 45400
    },
    {
      "epoch": 23.09766022380468,
      "grad_norm": 37.68522262573242,
      "learning_rate": 2.6902339776195323e-05,
      "loss": 1.9628,
      "step": 45410
    },
    {
      "epoch": 23.102746693794508,
      "grad_norm": 42.33916473388672,
      "learning_rate": 2.6897253306205496e-05,
      "loss": 1.9787,
      "step": 45420
    },
    {
      "epoch": 23.107833163784335,
      "grad_norm": 33.82182312011719,
      "learning_rate": 2.6892166836215666e-05,
      "loss": 1.8969,
      "step": 45430
    },
    {
      "epoch": 23.112919633774162,
      "grad_norm": 41.90625762939453,
      "learning_rate": 2.6887080366225842e-05,
      "loss": 2.0018,
      "step": 45440
    },
    {
      "epoch": 23.11800610376399,
      "grad_norm": 32.563926696777344,
      "learning_rate": 2.6881993896236012e-05,
      "loss": 1.9834,
      "step": 45450
    },
    {
      "epoch": 23.123092573753816,
      "grad_norm": 39.020790100097656,
      "learning_rate": 2.6876907426246186e-05,
      "loss": 2.0273,
      "step": 45460
    },
    {
      "epoch": 23.128179043743643,
      "grad_norm": 34.508514404296875,
      "learning_rate": 2.687182095625636e-05,
      "loss": 2.0375,
      "step": 45470
    },
    {
      "epoch": 23.13326551373347,
      "grad_norm": 33.14737319946289,
      "learning_rate": 2.6866734486266532e-05,
      "loss": 2.0476,
      "step": 45480
    },
    {
      "epoch": 23.138351983723297,
      "grad_norm": 38.56998062133789,
      "learning_rate": 2.6861648016276702e-05,
      "loss": 2.0568,
      "step": 45490
    },
    {
      "epoch": 23.143438453713124,
      "grad_norm": 39.28364944458008,
      "learning_rate": 2.685656154628688e-05,
      "loss": 2.0219,
      "step": 45500
    },
    {
      "epoch": 23.14852492370295,
      "grad_norm": 43.004295349121094,
      "learning_rate": 2.685147507629705e-05,
      "loss": 1.9763,
      "step": 45510
    },
    {
      "epoch": 23.153611393692778,
      "grad_norm": 33.5175666809082,
      "learning_rate": 2.6846388606307225e-05,
      "loss": 2.0247,
      "step": 45520
    },
    {
      "epoch": 23.158697863682605,
      "grad_norm": 36.323631286621094,
      "learning_rate": 2.68413021363174e-05,
      "loss": 2.0166,
      "step": 45530
    },
    {
      "epoch": 23.16378433367243,
      "grad_norm": 32.29093551635742,
      "learning_rate": 2.683621566632757e-05,
      "loss": 2.0253,
      "step": 45540
    },
    {
      "epoch": 23.16887080366226,
      "grad_norm": 43.359134674072266,
      "learning_rate": 2.6831129196337745e-05,
      "loss": 2.044,
      "step": 45550
    },
    {
      "epoch": 23.173957273652086,
      "grad_norm": 34.5047607421875,
      "learning_rate": 2.6826042726347915e-05,
      "loss": 2.0917,
      "step": 45560
    },
    {
      "epoch": 23.179043743641913,
      "grad_norm": 39.428627014160156,
      "learning_rate": 2.682095625635809e-05,
      "loss": 2.0306,
      "step": 45570
    },
    {
      "epoch": 23.18413021363174,
      "grad_norm": 47.28644561767578,
      "learning_rate": 2.6815869786368265e-05,
      "loss": 2.1084,
      "step": 45580
    },
    {
      "epoch": 23.189216683621567,
      "grad_norm": 31.46476173400879,
      "learning_rate": 2.6810783316378435e-05,
      "loss": 1.961,
      "step": 45590
    },
    {
      "epoch": 23.194303153611393,
      "grad_norm": 35.99799346923828,
      "learning_rate": 2.6805696846388605e-05,
      "loss": 1.9684,
      "step": 45600
    },
    {
      "epoch": 23.19938962360122,
      "grad_norm": 40.950279235839844,
      "learning_rate": 2.680061037639878e-05,
      "loss": 1.9789,
      "step": 45610
    },
    {
      "epoch": 23.204476093591047,
      "grad_norm": 36.715232849121094,
      "learning_rate": 2.6795523906408955e-05,
      "loss": 2.0546,
      "step": 45620
    },
    {
      "epoch": 23.209562563580874,
      "grad_norm": 31.791458129882812,
      "learning_rate": 2.6790437436419125e-05,
      "loss": 1.9961,
      "step": 45630
    },
    {
      "epoch": 23.2146490335707,
      "grad_norm": 30.466333389282227,
      "learning_rate": 2.67853509664293e-05,
      "loss": 1.9567,
      "step": 45640
    },
    {
      "epoch": 23.21973550356053,
      "grad_norm": 26.38557243347168,
      "learning_rate": 2.678026449643947e-05,
      "loss": 2.0678,
      "step": 45650
    },
    {
      "epoch": 23.224821973550355,
      "grad_norm": 33.57728958129883,
      "learning_rate": 2.6775178026449644e-05,
      "loss": 1.9645,
      "step": 45660
    },
    {
      "epoch": 23.229908443540182,
      "grad_norm": 28.325790405273438,
      "learning_rate": 2.677009155645982e-05,
      "loss": 2.0062,
      "step": 45670
    },
    {
      "epoch": 23.23499491353001,
      "grad_norm": 33.4265251159668,
      "learning_rate": 2.676500508646999e-05,
      "loss": 2.01,
      "step": 45680
    },
    {
      "epoch": 23.240081383519836,
      "grad_norm": 42.83269119262695,
      "learning_rate": 2.675991861648016e-05,
      "loss": 2.0061,
      "step": 45690
    },
    {
      "epoch": 23.245167853509663,
      "grad_norm": 29.065805435180664,
      "learning_rate": 2.6754832146490338e-05,
      "loss": 2.0031,
      "step": 45700
    },
    {
      "epoch": 23.25025432349949,
      "grad_norm": 30.081443786621094,
      "learning_rate": 2.674974567650051e-05,
      "loss": 1.9732,
      "step": 45710
    },
    {
      "epoch": 23.255340793489317,
      "grad_norm": 36.76676559448242,
      "learning_rate": 2.674465920651068e-05,
      "loss": 2.034,
      "step": 45720
    },
    {
      "epoch": 23.260427263479144,
      "grad_norm": 32.74334716796875,
      "learning_rate": 2.6739572736520857e-05,
      "loss": 2.0022,
      "step": 45730
    },
    {
      "epoch": 23.26551373346897,
      "grad_norm": 40.22612762451172,
      "learning_rate": 2.6734486266531027e-05,
      "loss": 2.0292,
      "step": 45740
    },
    {
      "epoch": 23.270600203458798,
      "grad_norm": 34.1093635559082,
      "learning_rate": 2.67293997965412e-05,
      "loss": 2.0295,
      "step": 45750
    },
    {
      "epoch": 23.275686673448625,
      "grad_norm": 48.27487564086914,
      "learning_rate": 2.6724313326551377e-05,
      "loss": 1.9642,
      "step": 45760
    },
    {
      "epoch": 23.280773143438452,
      "grad_norm": 33.631591796875,
      "learning_rate": 2.6719226856561547e-05,
      "loss": 2.0442,
      "step": 45770
    },
    {
      "epoch": 23.28585961342828,
      "grad_norm": 32.54728698730469,
      "learning_rate": 2.6714140386571724e-05,
      "loss": 1.9721,
      "step": 45780
    },
    {
      "epoch": 23.290946083418106,
      "grad_norm": 42.042030334472656,
      "learning_rate": 2.6709053916581894e-05,
      "loss": 1.9572,
      "step": 45790
    },
    {
      "epoch": 23.296032553407933,
      "grad_norm": 34.71902847290039,
      "learning_rate": 2.6703967446592064e-05,
      "loss": 2.0371,
      "step": 45800
    },
    {
      "epoch": 23.301119023397764,
      "grad_norm": 32.55048370361328,
      "learning_rate": 2.669888097660224e-05,
      "loss": 2.0275,
      "step": 45810
    },
    {
      "epoch": 23.30620549338759,
      "grad_norm": 28.110166549682617,
      "learning_rate": 2.6693794506612414e-05,
      "loss": 2.0219,
      "step": 45820
    },
    {
      "epoch": 23.311291963377418,
      "grad_norm": 29.8768253326416,
      "learning_rate": 2.6688708036622583e-05,
      "loss": 2.0662,
      "step": 45830
    },
    {
      "epoch": 23.316378433367245,
      "grad_norm": 34.48395919799805,
      "learning_rate": 2.668362156663276e-05,
      "loss": 2.1557,
      "step": 45840
    },
    {
      "epoch": 23.32146490335707,
      "grad_norm": 37.97478103637695,
      "learning_rate": 2.667853509664293e-05,
      "loss": 2.1215,
      "step": 45850
    },
    {
      "epoch": 23.3265513733469,
      "grad_norm": 37.141544342041016,
      "learning_rate": 2.6673448626653103e-05,
      "loss": 1.9969,
      "step": 45860
    },
    {
      "epoch": 23.331637843336726,
      "grad_norm": 34.73992919921875,
      "learning_rate": 2.666836215666328e-05,
      "loss": 2.0431,
      "step": 45870
    },
    {
      "epoch": 23.336724313326553,
      "grad_norm": 31.546995162963867,
      "learning_rate": 2.666327568667345e-05,
      "loss": 1.9813,
      "step": 45880
    },
    {
      "epoch": 23.34181078331638,
      "grad_norm": 37.939083099365234,
      "learning_rate": 2.665818921668362e-05,
      "loss": 1.9573,
      "step": 45890
    },
    {
      "epoch": 23.346897253306206,
      "grad_norm": 34.13216018676758,
      "learning_rate": 2.6653102746693796e-05,
      "loss": 2.0481,
      "step": 45900
    },
    {
      "epoch": 23.351983723296033,
      "grad_norm": 31.655620574951172,
      "learning_rate": 2.664801627670397e-05,
      "loss": 2.0463,
      "step": 45910
    },
    {
      "epoch": 23.35707019328586,
      "grad_norm": 42.64329147338867,
      "learning_rate": 2.664292980671414e-05,
      "loss": 2.0174,
      "step": 45920
    },
    {
      "epoch": 23.362156663275687,
      "grad_norm": 30.133211135864258,
      "learning_rate": 2.6637843336724316e-05,
      "loss": 2.0171,
      "step": 45930
    },
    {
      "epoch": 23.367243133265514,
      "grad_norm": 35.92192077636719,
      "learning_rate": 2.6632756866734486e-05,
      "loss": 2.0674,
      "step": 45940
    },
    {
      "epoch": 23.37232960325534,
      "grad_norm": 33.442955017089844,
      "learning_rate": 2.662767039674466e-05,
      "loss": 1.9864,
      "step": 45950
    },
    {
      "epoch": 23.37741607324517,
      "grad_norm": 30.770261764526367,
      "learning_rate": 2.6622583926754836e-05,
      "loss": 2.0183,
      "step": 45960
    },
    {
      "epoch": 23.382502543234995,
      "grad_norm": 35.17481994628906,
      "learning_rate": 2.6617497456765006e-05,
      "loss": 1.9996,
      "step": 45970
    },
    {
      "epoch": 23.387589013224822,
      "grad_norm": 31.569538116455078,
      "learning_rate": 2.6612410986775176e-05,
      "loss": 1.942,
      "step": 45980
    },
    {
      "epoch": 23.39267548321465,
      "grad_norm": 36.50912857055664,
      "learning_rate": 2.6607324516785353e-05,
      "loss": 2.0506,
      "step": 45990
    },
    {
      "epoch": 23.397761953204476,
      "grad_norm": 39.66355514526367,
      "learning_rate": 2.6602238046795526e-05,
      "loss": 2.0172,
      "step": 46000
    },
    {
      "epoch": 23.402848423194303,
      "grad_norm": 32.26509094238281,
      "learning_rate": 2.6597151576805696e-05,
      "loss": 1.9898,
      "step": 46010
    },
    {
      "epoch": 23.40793489318413,
      "grad_norm": 36.490360260009766,
      "learning_rate": 2.6592065106815872e-05,
      "loss": 2.0186,
      "step": 46020
    },
    {
      "epoch": 23.413021363173957,
      "grad_norm": 29.72493553161621,
      "learning_rate": 2.6586978636826042e-05,
      "loss": 1.9805,
      "step": 46030
    },
    {
      "epoch": 23.418107833163784,
      "grad_norm": 31.439979553222656,
      "learning_rate": 2.6581892166836216e-05,
      "loss": 1.9729,
      "step": 46040
    },
    {
      "epoch": 23.42319430315361,
      "grad_norm": 45.85848617553711,
      "learning_rate": 2.6576805696846392e-05,
      "loss": 1.9671,
      "step": 46050
    },
    {
      "epoch": 23.428280773143438,
      "grad_norm": 36.445499420166016,
      "learning_rate": 2.6571719226856562e-05,
      "loss": 2.038,
      "step": 46060
    },
    {
      "epoch": 23.433367243133265,
      "grad_norm": 39.40951919555664,
      "learning_rate": 2.656663275686674e-05,
      "loss": 1.9693,
      "step": 46070
    },
    {
      "epoch": 23.438453713123092,
      "grad_norm": 35.518211364746094,
      "learning_rate": 2.656154628687691e-05,
      "loss": 1.8286,
      "step": 46080
    },
    {
      "epoch": 23.44354018311292,
      "grad_norm": 26.265117645263672,
      "learning_rate": 2.6556459816887082e-05,
      "loss": 1.8825,
      "step": 46090
    },
    {
      "epoch": 23.448626653102746,
      "grad_norm": 42.20403289794922,
      "learning_rate": 2.6551373346897255e-05,
      "loss": 2.0275,
      "step": 46100
    },
    {
      "epoch": 23.453713123092573,
      "grad_norm": 29.2174129486084,
      "learning_rate": 2.654628687690743e-05,
      "loss": 1.9597,
      "step": 46110
    },
    {
      "epoch": 23.4587995930824,
      "grad_norm": 35.80753707885742,
      "learning_rate": 2.65412004069176e-05,
      "loss": 1.9614,
      "step": 46120
    },
    {
      "epoch": 23.463886063072227,
      "grad_norm": 31.84465789794922,
      "learning_rate": 2.6536113936927775e-05,
      "loss": 1.9724,
      "step": 46130
    },
    {
      "epoch": 23.468972533062054,
      "grad_norm": 39.81012725830078,
      "learning_rate": 2.6531027466937945e-05,
      "loss": 1.9688,
      "step": 46140
    },
    {
      "epoch": 23.47405900305188,
      "grad_norm": 33.70734405517578,
      "learning_rate": 2.652594099694812e-05,
      "loss": 1.9727,
      "step": 46150
    },
    {
      "epoch": 23.479145473041708,
      "grad_norm": 32.51864242553711,
      "learning_rate": 2.6520854526958295e-05,
      "loss": 2.0573,
      "step": 46160
    },
    {
      "epoch": 23.484231943031535,
      "grad_norm": 41.39777374267578,
      "learning_rate": 2.6515768056968465e-05,
      "loss": 2.0597,
      "step": 46170
    },
    {
      "epoch": 23.489318413021362,
      "grad_norm": 45.84151840209961,
      "learning_rate": 2.6510681586978635e-05,
      "loss": 1.9532,
      "step": 46180
    },
    {
      "epoch": 23.49440488301119,
      "grad_norm": 32.46929168701172,
      "learning_rate": 2.650559511698881e-05,
      "loss": 2.046,
      "step": 46190
    },
    {
      "epoch": 23.499491353001016,
      "grad_norm": 36.53240966796875,
      "learning_rate": 2.6500508646998985e-05,
      "loss": 1.9747,
      "step": 46200
    },
    {
      "epoch": 23.504577822990843,
      "grad_norm": 34.39446258544922,
      "learning_rate": 2.6495422177009155e-05,
      "loss": 1.9485,
      "step": 46210
    },
    {
      "epoch": 23.50966429298067,
      "grad_norm": 35.29508590698242,
      "learning_rate": 2.649033570701933e-05,
      "loss": 1.932,
      "step": 46220
    },
    {
      "epoch": 23.514750762970497,
      "grad_norm": 41.179283142089844,
      "learning_rate": 2.64852492370295e-05,
      "loss": 2.0694,
      "step": 46230
    },
    {
      "epoch": 23.519837232960327,
      "grad_norm": 39.210205078125,
      "learning_rate": 2.6480162767039674e-05,
      "loss": 1.9311,
      "step": 46240
    },
    {
      "epoch": 23.524923702950154,
      "grad_norm": 31.32854461669922,
      "learning_rate": 2.647507629704985e-05,
      "loss": 2.0069,
      "step": 46250
    },
    {
      "epoch": 23.53001017293998,
      "grad_norm": 31.64346694946289,
      "learning_rate": 2.646998982706002e-05,
      "loss": 1.9612,
      "step": 46260
    },
    {
      "epoch": 23.53509664292981,
      "grad_norm": 31.366954803466797,
      "learning_rate": 2.646490335707019e-05,
      "loss": 1.9725,
      "step": 46270
    },
    {
      "epoch": 23.540183112919635,
      "grad_norm": 39.73457336425781,
      "learning_rate": 2.6459816887080368e-05,
      "loss": 1.9925,
      "step": 46280
    },
    {
      "epoch": 23.545269582909462,
      "grad_norm": 44.562137603759766,
      "learning_rate": 2.645473041709054e-05,
      "loss": 1.9605,
      "step": 46290
    },
    {
      "epoch": 23.55035605289929,
      "grad_norm": 38.73934555053711,
      "learning_rate": 2.644964394710071e-05,
      "loss": 2.0354,
      "step": 46300
    },
    {
      "epoch": 23.555442522889116,
      "grad_norm": 31.4317626953125,
      "learning_rate": 2.6444557477110887e-05,
      "loss": 1.9411,
      "step": 46310
    },
    {
      "epoch": 23.560528992878943,
      "grad_norm": 30.68964385986328,
      "learning_rate": 2.6439471007121057e-05,
      "loss": 1.9592,
      "step": 46320
    },
    {
      "epoch": 23.56561546286877,
      "grad_norm": 42.87151336669922,
      "learning_rate": 2.6434384537131234e-05,
      "loss": 2.0756,
      "step": 46330
    },
    {
      "epoch": 23.570701932858597,
      "grad_norm": 44.21104431152344,
      "learning_rate": 2.6429298067141407e-05,
      "loss": 1.9223,
      "step": 46340
    },
    {
      "epoch": 23.575788402848424,
      "grad_norm": 36.16923904418945,
      "learning_rate": 2.6424211597151577e-05,
      "loss": 1.9532,
      "step": 46350
    },
    {
      "epoch": 23.58087487283825,
      "grad_norm": 36.34502029418945,
      "learning_rate": 2.6419125127161754e-05,
      "loss": 1.9954,
      "step": 46360
    },
    {
      "epoch": 23.585961342828078,
      "grad_norm": 32.47675704956055,
      "learning_rate": 2.6414038657171924e-05,
      "loss": 2.0622,
      "step": 46370
    },
    {
      "epoch": 23.591047812817905,
      "grad_norm": 32.2384147644043,
      "learning_rate": 2.6408952187182097e-05,
      "loss": 2.1134,
      "step": 46380
    },
    {
      "epoch": 23.596134282807732,
      "grad_norm": 37.47058868408203,
      "learning_rate": 2.6403865717192274e-05,
      "loss": 1.899,
      "step": 46390
    },
    {
      "epoch": 23.60122075279756,
      "grad_norm": 39.52390670776367,
      "learning_rate": 2.6398779247202444e-05,
      "loss": 1.855,
      "step": 46400
    },
    {
      "epoch": 23.606307222787386,
      "grad_norm": 35.00797653198242,
      "learning_rate": 2.6393692777212614e-05,
      "loss": 1.9906,
      "step": 46410
    },
    {
      "epoch": 23.611393692777213,
      "grad_norm": 50.532413482666016,
      "learning_rate": 2.638860630722279e-05,
      "loss": 2.0492,
      "step": 46420
    },
    {
      "epoch": 23.61648016276704,
      "grad_norm": 40.94015884399414,
      "learning_rate": 2.638351983723296e-05,
      "loss": 1.9209,
      "step": 46430
    },
    {
      "epoch": 23.621566632756867,
      "grad_norm": 36.04197692871094,
      "learning_rate": 2.6378433367243133e-05,
      "loss": 1.9953,
      "step": 46440
    },
    {
      "epoch": 23.626653102746694,
      "grad_norm": 34.875797271728516,
      "learning_rate": 2.637334689725331e-05,
      "loss": 2.0271,
      "step": 46450
    },
    {
      "epoch": 23.63173957273652,
      "grad_norm": 35.18647384643555,
      "learning_rate": 2.636826042726348e-05,
      "loss": 2.0247,
      "step": 46460
    },
    {
      "epoch": 23.636826042726348,
      "grad_norm": 33.03071212768555,
      "learning_rate": 2.636317395727365e-05,
      "loss": 2.0652,
      "step": 46470
    },
    {
      "epoch": 23.641912512716175,
      "grad_norm": 46.887840270996094,
      "learning_rate": 2.6358087487283826e-05,
      "loss": 2.0129,
      "step": 46480
    },
    {
      "epoch": 23.646998982706002,
      "grad_norm": 35.21656799316406,
      "learning_rate": 2.6353001017294e-05,
      "loss": 2.0496,
      "step": 46490
    },
    {
      "epoch": 23.65208545269583,
      "grad_norm": 40.648681640625,
      "learning_rate": 2.634791454730417e-05,
      "loss": 2.0272,
      "step": 46500
    },
    {
      "epoch": 23.657171922685656,
      "grad_norm": 30.330608367919922,
      "learning_rate": 2.6342828077314346e-05,
      "loss": 1.9759,
      "step": 46510
    },
    {
      "epoch": 23.662258392675483,
      "grad_norm": 37.87132263183594,
      "learning_rate": 2.6337741607324516e-05,
      "loss": 2.0194,
      "step": 46520
    },
    {
      "epoch": 23.66734486266531,
      "grad_norm": 35.491085052490234,
      "learning_rate": 2.633265513733469e-05,
      "loss": 1.9559,
      "step": 46530
    },
    {
      "epoch": 23.672431332655137,
      "grad_norm": 40.21227264404297,
      "learning_rate": 2.6327568667344866e-05,
      "loss": 1.9527,
      "step": 46540
    },
    {
      "epoch": 23.677517802644964,
      "grad_norm": 37.57525634765625,
      "learning_rate": 2.6322482197355036e-05,
      "loss": 1.9776,
      "step": 46550
    },
    {
      "epoch": 23.68260427263479,
      "grad_norm": 40.31208419799805,
      "learning_rate": 2.6317395727365206e-05,
      "loss": 1.9187,
      "step": 46560
    },
    {
      "epoch": 23.687690742624618,
      "grad_norm": 33.56293487548828,
      "learning_rate": 2.6312309257375383e-05,
      "loss": 2.0801,
      "step": 46570
    },
    {
      "epoch": 23.692777212614445,
      "grad_norm": 40.15161895751953,
      "learning_rate": 2.6307222787385556e-05,
      "loss": 1.9711,
      "step": 46580
    },
    {
      "epoch": 23.69786368260427,
      "grad_norm": 42.569610595703125,
      "learning_rate": 2.6302136317395733e-05,
      "loss": 2.0858,
      "step": 46590
    },
    {
      "epoch": 23.7029501525941,
      "grad_norm": 35.585792541503906,
      "learning_rate": 2.6297049847405902e-05,
      "loss": 1.9631,
      "step": 46600
    },
    {
      "epoch": 23.708036622583926,
      "grad_norm": 30.1980037689209,
      "learning_rate": 2.6291963377416072e-05,
      "loss": 1.9935,
      "step": 46610
    },
    {
      "epoch": 23.713123092573753,
      "grad_norm": 36.25276565551758,
      "learning_rate": 2.628687690742625e-05,
      "loss": 1.991,
      "step": 46620
    },
    {
      "epoch": 23.71820956256358,
      "grad_norm": 30.57817268371582,
      "learning_rate": 2.6281790437436422e-05,
      "loss": 2.0026,
      "step": 46630
    },
    {
      "epoch": 23.723296032553407,
      "grad_norm": 34.96873474121094,
      "learning_rate": 2.6276703967446592e-05,
      "loss": 1.9144,
      "step": 46640
    },
    {
      "epoch": 23.728382502543234,
      "grad_norm": 39.224853515625,
      "learning_rate": 2.627161749745677e-05,
      "loss": 1.8817,
      "step": 46650
    },
    {
      "epoch": 23.73346897253306,
      "grad_norm": 33.63825225830078,
      "learning_rate": 2.626653102746694e-05,
      "loss": 2.0154,
      "step": 46660
    },
    {
      "epoch": 23.738555442522888,
      "grad_norm": 26.804725646972656,
      "learning_rate": 2.6261444557477112e-05,
      "loss": 2.0323,
      "step": 46670
    },
    {
      "epoch": 23.743641912512714,
      "grad_norm": 33.71437072753906,
      "learning_rate": 2.625635808748729e-05,
      "loss": 2.0629,
      "step": 46680
    },
    {
      "epoch": 23.74872838250254,
      "grad_norm": 39.03465270996094,
      "learning_rate": 2.625127161749746e-05,
      "loss": 1.8844,
      "step": 46690
    },
    {
      "epoch": 23.753814852492372,
      "grad_norm": 35.27695083618164,
      "learning_rate": 2.624618514750763e-05,
      "loss": 2.0,
      "step": 46700
    },
    {
      "epoch": 23.7589013224822,
      "grad_norm": 31.031469345092773,
      "learning_rate": 2.6241098677517805e-05,
      "loss": 1.9637,
      "step": 46710
    },
    {
      "epoch": 23.763987792472026,
      "grad_norm": 34.95408248901367,
      "learning_rate": 2.623601220752798e-05,
      "loss": 2.0156,
      "step": 46720
    },
    {
      "epoch": 23.769074262461853,
      "grad_norm": 37.707130432128906,
      "learning_rate": 2.623092573753815e-05,
      "loss": 1.9428,
      "step": 46730
    },
    {
      "epoch": 23.77416073245168,
      "grad_norm": 35.099639892578125,
      "learning_rate": 2.6225839267548325e-05,
      "loss": 2.0308,
      "step": 46740
    },
    {
      "epoch": 23.779247202441507,
      "grad_norm": 33.92451858520508,
      "learning_rate": 2.6220752797558495e-05,
      "loss": 2.0258,
      "step": 46750
    },
    {
      "epoch": 23.784333672431334,
      "grad_norm": 53.51905059814453,
      "learning_rate": 2.6215666327568665e-05,
      "loss": 2.0318,
      "step": 46760
    },
    {
      "epoch": 23.78942014242116,
      "grad_norm": 40.48414993286133,
      "learning_rate": 2.621057985757884e-05,
      "loss": 2.0346,
      "step": 46770
    },
    {
      "epoch": 23.794506612410988,
      "grad_norm": 35.561729431152344,
      "learning_rate": 2.6205493387589015e-05,
      "loss": 1.9667,
      "step": 46780
    },
    {
      "epoch": 23.799593082400815,
      "grad_norm": 36.27803421020508,
      "learning_rate": 2.6200406917599185e-05,
      "loss": 2.0132,
      "step": 46790
    },
    {
      "epoch": 23.804679552390642,
      "grad_norm": 34.55051803588867,
      "learning_rate": 2.619532044760936e-05,
      "loss": 1.9953,
      "step": 46800
    },
    {
      "epoch": 23.80976602238047,
      "grad_norm": 38.279720306396484,
      "learning_rate": 2.619023397761953e-05,
      "loss": 1.9799,
      "step": 46810
    },
    {
      "epoch": 23.814852492370296,
      "grad_norm": 37.21195602416992,
      "learning_rate": 2.6185147507629705e-05,
      "loss": 2.0253,
      "step": 46820
    },
    {
      "epoch": 23.819938962360123,
      "grad_norm": 33.72636032104492,
      "learning_rate": 2.618006103763988e-05,
      "loss": 1.986,
      "step": 46830
    },
    {
      "epoch": 23.82502543234995,
      "grad_norm": 38.805641174316406,
      "learning_rate": 2.617497456765005e-05,
      "loss": 1.9065,
      "step": 46840
    },
    {
      "epoch": 23.830111902339777,
      "grad_norm": 32.57310104370117,
      "learning_rate": 2.616988809766022e-05,
      "loss": 2.0206,
      "step": 46850
    },
    {
      "epoch": 23.835198372329604,
      "grad_norm": 28.87939453125,
      "learning_rate": 2.6164801627670398e-05,
      "loss": 1.9866,
      "step": 46860
    },
    {
      "epoch": 23.84028484231943,
      "grad_norm": 33.00425338745117,
      "learning_rate": 2.615971515768057e-05,
      "loss": 1.9637,
      "step": 46870
    },
    {
      "epoch": 23.845371312309258,
      "grad_norm": 36.50287628173828,
      "learning_rate": 2.6154628687690748e-05,
      "loss": 1.926,
      "step": 46880
    },
    {
      "epoch": 23.850457782299085,
      "grad_norm": 31.403644561767578,
      "learning_rate": 2.6149542217700917e-05,
      "loss": 2.033,
      "step": 46890
    },
    {
      "epoch": 23.85554425228891,
      "grad_norm": 35.70335006713867,
      "learning_rate": 2.6144455747711087e-05,
      "loss": 2.057,
      "step": 46900
    },
    {
      "epoch": 23.86063072227874,
      "grad_norm": 40.1674690246582,
      "learning_rate": 2.6139369277721264e-05,
      "loss": 1.9847,
      "step": 46910
    },
    {
      "epoch": 23.865717192268566,
      "grad_norm": 40.51749038696289,
      "learning_rate": 2.6134282807731437e-05,
      "loss": 1.9851,
      "step": 46920
    },
    {
      "epoch": 23.870803662258393,
      "grad_norm": 36.454830169677734,
      "learning_rate": 2.6129196337741607e-05,
      "loss": 2.0301,
      "step": 46930
    },
    {
      "epoch": 23.87589013224822,
      "grad_norm": 38.52028274536133,
      "learning_rate": 2.6124109867751784e-05,
      "loss": 1.9043,
      "step": 46940
    },
    {
      "epoch": 23.880976602238047,
      "grad_norm": 38.59312438964844,
      "learning_rate": 2.6119023397761954e-05,
      "loss": 1.9985,
      "step": 46950
    },
    {
      "epoch": 23.886063072227874,
      "grad_norm": 40.2048454284668,
      "learning_rate": 2.6113936927772127e-05,
      "loss": 1.8917,
      "step": 46960
    },
    {
      "epoch": 23.8911495422177,
      "grad_norm": 46.693511962890625,
      "learning_rate": 2.6108850457782304e-05,
      "loss": 1.9,
      "step": 46970
    },
    {
      "epoch": 23.896236012207527,
      "grad_norm": 31.23906898498535,
      "learning_rate": 2.6103763987792474e-05,
      "loss": 1.9349,
      "step": 46980
    },
    {
      "epoch": 23.901322482197354,
      "grad_norm": 31.407297134399414,
      "learning_rate": 2.6098677517802644e-05,
      "loss": 2.0562,
      "step": 46990
    },
    {
      "epoch": 23.90640895218718,
      "grad_norm": 42.34575271606445,
      "learning_rate": 2.609359104781282e-05,
      "loss": 2.0004,
      "step": 47000
    },
    {
      "epoch": 23.91149542217701,
      "grad_norm": 34.220008850097656,
      "learning_rate": 2.6088504577822993e-05,
      "loss": 2.03,
      "step": 47010
    },
    {
      "epoch": 23.916581892166835,
      "grad_norm": 35.420169830322266,
      "learning_rate": 2.6083418107833163e-05,
      "loss": 1.9824,
      "step": 47020
    },
    {
      "epoch": 23.921668362156662,
      "grad_norm": 37.02443313598633,
      "learning_rate": 2.607833163784334e-05,
      "loss": 1.9771,
      "step": 47030
    },
    {
      "epoch": 23.92675483214649,
      "grad_norm": 34.197940826416016,
      "learning_rate": 2.607324516785351e-05,
      "loss": 1.9876,
      "step": 47040
    },
    {
      "epoch": 23.931841302136316,
      "grad_norm": 28.757381439208984,
      "learning_rate": 2.6068158697863683e-05,
      "loss": 2.0403,
      "step": 47050
    },
    {
      "epoch": 23.936927772126143,
      "grad_norm": 30.775270462036133,
      "learning_rate": 2.6063072227873856e-05,
      "loss": 1.927,
      "step": 47060
    },
    {
      "epoch": 23.94201424211597,
      "grad_norm": 32.65831756591797,
      "learning_rate": 2.605798575788403e-05,
      "loss": 1.9508,
      "step": 47070
    },
    {
      "epoch": 23.947100712105797,
      "grad_norm": 32.09539031982422,
      "learning_rate": 2.60528992878942e-05,
      "loss": 1.9489,
      "step": 47080
    },
    {
      "epoch": 23.952187182095624,
      "grad_norm": 32.70964813232422,
      "learning_rate": 2.6047812817904376e-05,
      "loss": 2.0313,
      "step": 47090
    },
    {
      "epoch": 23.95727365208545,
      "grad_norm": 37.33466339111328,
      "learning_rate": 2.6042726347914546e-05,
      "loss": 1.9718,
      "step": 47100
    },
    {
      "epoch": 23.962360122075278,
      "grad_norm": 40.53776168823242,
      "learning_rate": 2.603763987792472e-05,
      "loss": 1.9843,
      "step": 47110
    },
    {
      "epoch": 23.967446592065105,
      "grad_norm": 32.938697814941406,
      "learning_rate": 2.6032553407934896e-05,
      "loss": 1.9498,
      "step": 47120
    },
    {
      "epoch": 23.972533062054932,
      "grad_norm": 32.42924880981445,
      "learning_rate": 2.6027466937945066e-05,
      "loss": 1.9654,
      "step": 47130
    },
    {
      "epoch": 23.977619532044763,
      "grad_norm": 27.030162811279297,
      "learning_rate": 2.6022380467955243e-05,
      "loss": 1.9218,
      "step": 47140
    },
    {
      "epoch": 23.98270600203459,
      "grad_norm": 38.75148010253906,
      "learning_rate": 2.6017293997965413e-05,
      "loss": 1.9508,
      "step": 47150
    },
    {
      "epoch": 23.987792472024417,
      "grad_norm": 45.47386932373047,
      "learning_rate": 2.6012207527975586e-05,
      "loss": 1.9797,
      "step": 47160
    },
    {
      "epoch": 23.992878942014244,
      "grad_norm": 31.052276611328125,
      "learning_rate": 2.6007121057985763e-05,
      "loss": 1.9706,
      "step": 47170
    },
    {
      "epoch": 23.99796541200407,
      "grad_norm": 37.690486907958984,
      "learning_rate": 2.6002034587995932e-05,
      "loss": 1.9985,
      "step": 47180
    },
    {
      "epoch": 24.0,
      "eval_loss": 4.520573616027832,
      "eval_runtime": 2.7321,
      "eval_samples_per_second": 1015.699,
      "eval_steps_per_second": 127.008,
      "step": 47184
    },
    {
      "epoch": 24.003051881993898,
      "grad_norm": 35.98485565185547,
      "learning_rate": 2.5996948118006102e-05,
      "loss": 1.9182,
      "step": 47190
    },
    {
      "epoch": 24.008138351983725,
      "grad_norm": 44.02801513671875,
      "learning_rate": 2.599186164801628e-05,
      "loss": 1.9122,
      "step": 47200
    },
    {
      "epoch": 24.01322482197355,
      "grad_norm": 38.112831115722656,
      "learning_rate": 2.5986775178026452e-05,
      "loss": 2.0431,
      "step": 47210
    },
    {
      "epoch": 24.01831129196338,
      "grad_norm": 36.071048736572266,
      "learning_rate": 2.5981688708036622e-05,
      "loss": 1.9411,
      "step": 47220
    },
    {
      "epoch": 24.023397761953206,
      "grad_norm": 30.398303985595703,
      "learning_rate": 2.59766022380468e-05,
      "loss": 1.9508,
      "step": 47230
    },
    {
      "epoch": 24.028484231943033,
      "grad_norm": 35.211334228515625,
      "learning_rate": 2.597151576805697e-05,
      "loss": 1.9324,
      "step": 47240
    },
    {
      "epoch": 24.03357070193286,
      "grad_norm": 38.39194107055664,
      "learning_rate": 2.5966429298067142e-05,
      "loss": 1.9981,
      "step": 47250
    },
    {
      "epoch": 24.038657171922686,
      "grad_norm": 38.62260055541992,
      "learning_rate": 2.596134282807732e-05,
      "loss": 1.9936,
      "step": 47260
    },
    {
      "epoch": 24.043743641912513,
      "grad_norm": 30.542688369750977,
      "learning_rate": 2.595625635808749e-05,
      "loss": 1.9895,
      "step": 47270
    },
    {
      "epoch": 24.04883011190234,
      "grad_norm": 37.40625762939453,
      "learning_rate": 2.595116988809766e-05,
      "loss": 1.9076,
      "step": 47280
    },
    {
      "epoch": 24.053916581892167,
      "grad_norm": 36.73552703857422,
      "learning_rate": 2.5946083418107835e-05,
      "loss": 2.0126,
      "step": 47290
    },
    {
      "epoch": 24.059003051881994,
      "grad_norm": 38.57413864135742,
      "learning_rate": 2.594099694811801e-05,
      "loss": 1.8099,
      "step": 47300
    },
    {
      "epoch": 24.06408952187182,
      "grad_norm": 36.61906814575195,
      "learning_rate": 2.593591047812818e-05,
      "loss": 1.9242,
      "step": 47310
    },
    {
      "epoch": 24.06917599186165,
      "grad_norm": 38.24815368652344,
      "learning_rate": 2.5930824008138355e-05,
      "loss": 1.9644,
      "step": 47320
    },
    {
      "epoch": 24.074262461851475,
      "grad_norm": 39.947811126708984,
      "learning_rate": 2.5925737538148525e-05,
      "loss": 1.9082,
      "step": 47330
    },
    {
      "epoch": 24.079348931841302,
      "grad_norm": 34.670509338378906,
      "learning_rate": 2.5920651068158698e-05,
      "loss": 2.0002,
      "step": 47340
    },
    {
      "epoch": 24.08443540183113,
      "grad_norm": 34.192291259765625,
      "learning_rate": 2.5915564598168875e-05,
      "loss": 1.9106,
      "step": 47350
    },
    {
      "epoch": 24.089521871820956,
      "grad_norm": 43.15157699584961,
      "learning_rate": 2.5910478128179045e-05,
      "loss": 1.9928,
      "step": 47360
    },
    {
      "epoch": 24.094608341810783,
      "grad_norm": 38.97237014770508,
      "learning_rate": 2.5905391658189215e-05,
      "loss": 1.9359,
      "step": 47370
    },
    {
      "epoch": 24.09969481180061,
      "grad_norm": 39.617340087890625,
      "learning_rate": 2.590030518819939e-05,
      "loss": 1.9073,
      "step": 47380
    },
    {
      "epoch": 24.104781281790437,
      "grad_norm": 32.928890228271484,
      "learning_rate": 2.589521871820956e-05,
      "loss": 1.9705,
      "step": 47390
    },
    {
      "epoch": 24.109867751780264,
      "grad_norm": 35.554691314697266,
      "learning_rate": 2.5890132248219738e-05,
      "loss": 1.9271,
      "step": 47400
    },
    {
      "epoch": 24.11495422177009,
      "grad_norm": 37.55084991455078,
      "learning_rate": 2.588504577822991e-05,
      "loss": 2.0051,
      "step": 47410
    },
    {
      "epoch": 24.120040691759918,
      "grad_norm": 36.48657989501953,
      "learning_rate": 2.587995930824008e-05,
      "loss": 1.9558,
      "step": 47420
    },
    {
      "epoch": 24.125127161749745,
      "grad_norm": 50.37348937988281,
      "learning_rate": 2.5874872838250258e-05,
      "loss": 2.0023,
      "step": 47430
    },
    {
      "epoch": 24.130213631739572,
      "grad_norm": 32.701473236083984,
      "learning_rate": 2.5869786368260428e-05,
      "loss": 2.0163,
      "step": 47440
    },
    {
      "epoch": 24.1353001017294,
      "grad_norm": 34.430625915527344,
      "learning_rate": 2.58646998982706e-05,
      "loss": 1.8992,
      "step": 47450
    },
    {
      "epoch": 24.140386571719226,
      "grad_norm": 31.328166961669922,
      "learning_rate": 2.5859613428280778e-05,
      "loss": 1.9556,
      "step": 47460
    },
    {
      "epoch": 24.145473041709053,
      "grad_norm": 38.3323974609375,
      "learning_rate": 2.5854526958290947e-05,
      "loss": 1.9708,
      "step": 47470
    },
    {
      "epoch": 24.15055951169888,
      "grad_norm": 34.89426803588867,
      "learning_rate": 2.5849440488301117e-05,
      "loss": 1.9975,
      "step": 47480
    },
    {
      "epoch": 24.155645981688707,
      "grad_norm": 33.41873550415039,
      "learning_rate": 2.5844354018311294e-05,
      "loss": 1.9874,
      "step": 47490
    },
    {
      "epoch": 24.160732451678534,
      "grad_norm": 42.64263916015625,
      "learning_rate": 2.5839267548321467e-05,
      "loss": 2.0092,
      "step": 47500
    },
    {
      "epoch": 24.16581892166836,
      "grad_norm": 38.986270904541016,
      "learning_rate": 2.5834181078331637e-05,
      "loss": 1.9461,
      "step": 47510
    },
    {
      "epoch": 24.170905391658188,
      "grad_norm": 42.97722244262695,
      "learning_rate": 2.5829094608341814e-05,
      "loss": 2.0283,
      "step": 47520
    },
    {
      "epoch": 24.175991861648015,
      "grad_norm": 36.96344757080078,
      "learning_rate": 2.5824008138351984e-05,
      "loss": 2.076,
      "step": 47530
    },
    {
      "epoch": 24.181078331637842,
      "grad_norm": 31.33226203918457,
      "learning_rate": 2.5818921668362157e-05,
      "loss": 1.9938,
      "step": 47540
    },
    {
      "epoch": 24.18616480162767,
      "grad_norm": 44.3767204284668,
      "learning_rate": 2.5813835198372334e-05,
      "loss": 2.0009,
      "step": 47550
    },
    {
      "epoch": 24.191251271617496,
      "grad_norm": 33.26982116699219,
      "learning_rate": 2.5808748728382504e-05,
      "loss": 2.0022,
      "step": 47560
    },
    {
      "epoch": 24.196337741607323,
      "grad_norm": 30.279001235961914,
      "learning_rate": 2.5803662258392674e-05,
      "loss": 1.9973,
      "step": 47570
    },
    {
      "epoch": 24.20142421159715,
      "grad_norm": 28.526477813720703,
      "learning_rate": 2.579857578840285e-05,
      "loss": 1.9743,
      "step": 47580
    },
    {
      "epoch": 24.20651068158698,
      "grad_norm": 48.45512008666992,
      "learning_rate": 2.5793489318413023e-05,
      "loss": 1.9499,
      "step": 47590
    },
    {
      "epoch": 24.211597151576807,
      "grad_norm": 31.842561721801758,
      "learning_rate": 2.5788402848423193e-05,
      "loss": 1.8433,
      "step": 47600
    },
    {
      "epoch": 24.216683621566634,
      "grad_norm": 46.3380241394043,
      "learning_rate": 2.578331637843337e-05,
      "loss": 1.9981,
      "step": 47610
    },
    {
      "epoch": 24.22177009155646,
      "grad_norm": 25.312782287597656,
      "learning_rate": 2.577822990844354e-05,
      "loss": 1.986,
      "step": 47620
    },
    {
      "epoch": 24.22685656154629,
      "grad_norm": 31.295236587524414,
      "learning_rate": 2.5773143438453713e-05,
      "loss": 2.0201,
      "step": 47630
    },
    {
      "epoch": 24.231943031536115,
      "grad_norm": 38.935523986816406,
      "learning_rate": 2.576805696846389e-05,
      "loss": 1.9794,
      "step": 47640
    },
    {
      "epoch": 24.237029501525942,
      "grad_norm": 34.59131622314453,
      "learning_rate": 2.576297049847406e-05,
      "loss": 1.8159,
      "step": 47650
    },
    {
      "epoch": 24.24211597151577,
      "grad_norm": 29.942663192749023,
      "learning_rate": 2.575788402848423e-05,
      "loss": 1.9891,
      "step": 47660
    },
    {
      "epoch": 24.247202441505596,
      "grad_norm": 33.13824462890625,
      "learning_rate": 2.5752797558494406e-05,
      "loss": 1.8834,
      "step": 47670
    },
    {
      "epoch": 24.252288911495423,
      "grad_norm": 35.910438537597656,
      "learning_rate": 2.574771108850458e-05,
      "loss": 1.9421,
      "step": 47680
    },
    {
      "epoch": 24.25737538148525,
      "grad_norm": 35.76480484008789,
      "learning_rate": 2.5742624618514756e-05,
      "loss": 1.9129,
      "step": 47690
    },
    {
      "epoch": 24.262461851475077,
      "grad_norm": 50.42509460449219,
      "learning_rate": 2.5737538148524926e-05,
      "loss": 1.9252,
      "step": 47700
    },
    {
      "epoch": 24.267548321464904,
      "grad_norm": 39.51335525512695,
      "learning_rate": 2.5732451678535096e-05,
      "loss": 2.007,
      "step": 47710
    },
    {
      "epoch": 24.27263479145473,
      "grad_norm": 41.33040237426758,
      "learning_rate": 2.5727365208545273e-05,
      "loss": 1.9686,
      "step": 47720
    },
    {
      "epoch": 24.277721261444558,
      "grad_norm": 32.8103141784668,
      "learning_rate": 2.5722278738555443e-05,
      "loss": 2.0416,
      "step": 47730
    },
    {
      "epoch": 24.282807731434385,
      "grad_norm": 35.0989875793457,
      "learning_rate": 2.5717192268565616e-05,
      "loss": 1.9945,
      "step": 47740
    },
    {
      "epoch": 24.287894201424212,
      "grad_norm": 31.879056930541992,
      "learning_rate": 2.5712105798575793e-05,
      "loss": 2.027,
      "step": 47750
    },
    {
      "epoch": 24.29298067141404,
      "grad_norm": 39.489601135253906,
      "learning_rate": 2.5707019328585962e-05,
      "loss": 1.9654,
      "step": 47760
    },
    {
      "epoch": 24.298067141403866,
      "grad_norm": 40.3422966003418,
      "learning_rate": 2.5701932858596132e-05,
      "loss": 2.035,
      "step": 47770
    },
    {
      "epoch": 24.303153611393693,
      "grad_norm": 41.4653434753418,
      "learning_rate": 2.569684638860631e-05,
      "loss": 1.9302,
      "step": 47780
    },
    {
      "epoch": 24.30824008138352,
      "grad_norm": 42.98496627807617,
      "learning_rate": 2.5691759918616482e-05,
      "loss": 1.987,
      "step": 47790
    },
    {
      "epoch": 24.313326551373347,
      "grad_norm": 33.47582244873047,
      "learning_rate": 2.5686673448626652e-05,
      "loss": 1.9766,
      "step": 47800
    },
    {
      "epoch": 24.318413021363174,
      "grad_norm": 32.33175277709961,
      "learning_rate": 2.568158697863683e-05,
      "loss": 1.9512,
      "step": 47810
    },
    {
      "epoch": 24.323499491353,
      "grad_norm": 39.37046432495117,
      "learning_rate": 2.5676500508647e-05,
      "loss": 2.0182,
      "step": 47820
    },
    {
      "epoch": 24.328585961342828,
      "grad_norm": 42.4243049621582,
      "learning_rate": 2.5671414038657172e-05,
      "loss": 1.9875,
      "step": 47830
    },
    {
      "epoch": 24.333672431332655,
      "grad_norm": 45.836647033691406,
      "learning_rate": 2.566632756866735e-05,
      "loss": 1.9784,
      "step": 47840
    },
    {
      "epoch": 24.338758901322482,
      "grad_norm": 49.03662109375,
      "learning_rate": 2.566124109867752e-05,
      "loss": 1.9549,
      "step": 47850
    },
    {
      "epoch": 24.34384537131231,
      "grad_norm": 36.93636703491211,
      "learning_rate": 2.565615462868769e-05,
      "loss": 1.9435,
      "step": 47860
    },
    {
      "epoch": 24.348931841302136,
      "grad_norm": 42.307342529296875,
      "learning_rate": 2.5651068158697865e-05,
      "loss": 1.9383,
      "step": 47870
    },
    {
      "epoch": 24.354018311291963,
      "grad_norm": 32.332672119140625,
      "learning_rate": 2.564598168870804e-05,
      "loss": 1.9806,
      "step": 47880
    },
    {
      "epoch": 24.35910478128179,
      "grad_norm": 40.706119537353516,
      "learning_rate": 2.564089521871821e-05,
      "loss": 1.9647,
      "step": 47890
    },
    {
      "epoch": 24.364191251271617,
      "grad_norm": 32.98504638671875,
      "learning_rate": 2.5635808748728385e-05,
      "loss": 1.9354,
      "step": 47900
    },
    {
      "epoch": 24.369277721261444,
      "grad_norm": 29.609275817871094,
      "learning_rate": 2.5630722278738555e-05,
      "loss": 1.8817,
      "step": 47910
    },
    {
      "epoch": 24.37436419125127,
      "grad_norm": 38.622474670410156,
      "learning_rate": 2.5625635808748728e-05,
      "loss": 1.9196,
      "step": 47920
    },
    {
      "epoch": 24.379450661241098,
      "grad_norm": 41.44679641723633,
      "learning_rate": 2.5620549338758905e-05,
      "loss": 2.0037,
      "step": 47930
    },
    {
      "epoch": 24.384537131230925,
      "grad_norm": 29.924591064453125,
      "learning_rate": 2.5615462868769075e-05,
      "loss": 1.9554,
      "step": 47940
    },
    {
      "epoch": 24.38962360122075,
      "grad_norm": 32.947792053222656,
      "learning_rate": 2.561037639877925e-05,
      "loss": 1.9483,
      "step": 47950
    },
    {
      "epoch": 24.39471007121058,
      "grad_norm": 43.42851257324219,
      "learning_rate": 2.560528992878942e-05,
      "loss": 1.8712,
      "step": 47960
    },
    {
      "epoch": 24.399796541200406,
      "grad_norm": 37.674861907958984,
      "learning_rate": 2.5600203458799595e-05,
      "loss": 2.0131,
      "step": 47970
    },
    {
      "epoch": 24.404883011190233,
      "grad_norm": 43.53973388671875,
      "learning_rate": 2.559511698880977e-05,
      "loss": 2.0046,
      "step": 47980
    },
    {
      "epoch": 24.40996948118006,
      "grad_norm": 35.84499740600586,
      "learning_rate": 2.559003051881994e-05,
      "loss": 1.8343,
      "step": 47990
    },
    {
      "epoch": 24.415055951169887,
      "grad_norm": 40.144561767578125,
      "learning_rate": 2.558494404883011e-05,
      "loss": 1.9631,
      "step": 48000
    },
    {
      "epoch": 24.420142421159714,
      "grad_norm": 36.53496551513672,
      "learning_rate": 2.5579857578840288e-05,
      "loss": 1.9981,
      "step": 48010
    },
    {
      "epoch": 24.42522889114954,
      "grad_norm": 34.87754821777344,
      "learning_rate": 2.557477110885046e-05,
      "loss": 1.9907,
      "step": 48020
    },
    {
      "epoch": 24.43031536113937,
      "grad_norm": 34.46193313598633,
      "learning_rate": 2.556968463886063e-05,
      "loss": 1.8631,
      "step": 48030
    },
    {
      "epoch": 24.435401831129198,
      "grad_norm": 36.758914947509766,
      "learning_rate": 2.5564598168870808e-05,
      "loss": 1.9838,
      "step": 48040
    },
    {
      "epoch": 24.440488301119025,
      "grad_norm": 33.44013595581055,
      "learning_rate": 2.5559511698880977e-05,
      "loss": 2.0206,
      "step": 48050
    },
    {
      "epoch": 24.445574771108852,
      "grad_norm": 36.72606658935547,
      "learning_rate": 2.5554425228891147e-05,
      "loss": 2.0401,
      "step": 48060
    },
    {
      "epoch": 24.45066124109868,
      "grad_norm": 28.115646362304688,
      "learning_rate": 2.5549338758901324e-05,
      "loss": 2.0341,
      "step": 48070
    },
    {
      "epoch": 24.455747711088506,
      "grad_norm": 32.13698959350586,
      "learning_rate": 2.5544252288911497e-05,
      "loss": 1.9273,
      "step": 48080
    },
    {
      "epoch": 24.460834181078333,
      "grad_norm": 38.0784797668457,
      "learning_rate": 2.5539165818921667e-05,
      "loss": 1.9745,
      "step": 48090
    },
    {
      "epoch": 24.46592065106816,
      "grad_norm": 31.226558685302734,
      "learning_rate": 2.5534079348931844e-05,
      "loss": 1.9384,
      "step": 48100
    },
    {
      "epoch": 24.471007121057987,
      "grad_norm": 28.530927658081055,
      "learning_rate": 2.5528992878942014e-05,
      "loss": 1.9221,
      "step": 48110
    },
    {
      "epoch": 24.476093591047814,
      "grad_norm": 32.30976486206055,
      "learning_rate": 2.5523906408952187e-05,
      "loss": 1.9962,
      "step": 48120
    },
    {
      "epoch": 24.48118006103764,
      "grad_norm": 38.99931716918945,
      "learning_rate": 2.5518819938962364e-05,
      "loss": 1.9092,
      "step": 48130
    },
    {
      "epoch": 24.486266531027468,
      "grad_norm": 43.75464630126953,
      "learning_rate": 2.5513733468972534e-05,
      "loss": 1.8765,
      "step": 48140
    },
    {
      "epoch": 24.491353001017295,
      "grad_norm": 55.16194534301758,
      "learning_rate": 2.5508646998982704e-05,
      "loss": 1.8731,
      "step": 48150
    },
    {
      "epoch": 24.496439471007122,
      "grad_norm": 43.401363372802734,
      "learning_rate": 2.550356052899288e-05,
      "loss": 1.9315,
      "step": 48160
    },
    {
      "epoch": 24.50152594099695,
      "grad_norm": 36.48079299926758,
      "learning_rate": 2.5498474059003053e-05,
      "loss": 1.9238,
      "step": 48170
    },
    {
      "epoch": 24.506612410986776,
      "grad_norm": 35.07143783569336,
      "learning_rate": 2.5493387589013223e-05,
      "loss": 1.9882,
      "step": 48180
    },
    {
      "epoch": 24.511698880976603,
      "grad_norm": 28.905799865722656,
      "learning_rate": 2.54883011190234e-05,
      "loss": 1.9596,
      "step": 48190
    },
    {
      "epoch": 24.51678535096643,
      "grad_norm": 41.702415466308594,
      "learning_rate": 2.548321464903357e-05,
      "loss": 1.9177,
      "step": 48200
    },
    {
      "epoch": 24.521871820956257,
      "grad_norm": 35.77085494995117,
      "learning_rate": 2.5478128179043747e-05,
      "loss": 1.9299,
      "step": 48210
    },
    {
      "epoch": 24.526958290946084,
      "grad_norm": 33.7027587890625,
      "learning_rate": 2.547304170905392e-05,
      "loss": 1.9102,
      "step": 48220
    },
    {
      "epoch": 24.53204476093591,
      "grad_norm": 35.076324462890625,
      "learning_rate": 2.546795523906409e-05,
      "loss": 1.9703,
      "step": 48230
    },
    {
      "epoch": 24.537131230925738,
      "grad_norm": 33.713890075683594,
      "learning_rate": 2.5462868769074266e-05,
      "loss": 2.0061,
      "step": 48240
    },
    {
      "epoch": 24.542217700915565,
      "grad_norm": 31.752750396728516,
      "learning_rate": 2.5457782299084436e-05,
      "loss": 1.941,
      "step": 48250
    },
    {
      "epoch": 24.54730417090539,
      "grad_norm": 38.75092315673828,
      "learning_rate": 2.545269582909461e-05,
      "loss": 1.9813,
      "step": 48260
    },
    {
      "epoch": 24.55239064089522,
      "grad_norm": 41.45889663696289,
      "learning_rate": 2.5447609359104786e-05,
      "loss": 1.985,
      "step": 48270
    },
    {
      "epoch": 24.557477110885046,
      "grad_norm": 31.138675689697266,
      "learning_rate": 2.5442522889114956e-05,
      "loss": 1.9009,
      "step": 48280
    },
    {
      "epoch": 24.562563580874873,
      "grad_norm": 30.504671096801758,
      "learning_rate": 2.5437436419125126e-05,
      "loss": 1.9819,
      "step": 48290
    },
    {
      "epoch": 24.5676500508647,
      "grad_norm": 38.25849914550781,
      "learning_rate": 2.5432349949135303e-05,
      "loss": 1.9925,
      "step": 48300
    },
    {
      "epoch": 24.572736520854527,
      "grad_norm": 40.00360107421875,
      "learning_rate": 2.5427263479145476e-05,
      "loss": 1.9968,
      "step": 48310
    },
    {
      "epoch": 24.577822990844354,
      "grad_norm": 38.447021484375,
      "learning_rate": 2.5422177009155646e-05,
      "loss": 1.9653,
      "step": 48320
    },
    {
      "epoch": 24.58290946083418,
      "grad_norm": 39.3142204284668,
      "learning_rate": 2.5417090539165823e-05,
      "loss": 1.9272,
      "step": 48330
    },
    {
      "epoch": 24.587995930824007,
      "grad_norm": 50.069095611572266,
      "learning_rate": 2.5412004069175992e-05,
      "loss": 1.9395,
      "step": 48340
    },
    {
      "epoch": 24.593082400813834,
      "grad_norm": 33.26361083984375,
      "learning_rate": 2.5406917599186166e-05,
      "loss": 1.9375,
      "step": 48350
    },
    {
      "epoch": 24.59816887080366,
      "grad_norm": 34.46916961669922,
      "learning_rate": 2.540183112919634e-05,
      "loss": 1.9508,
      "step": 48360
    },
    {
      "epoch": 24.60325534079349,
      "grad_norm": 35.23271942138672,
      "learning_rate": 2.5396744659206512e-05,
      "loss": 1.9341,
      "step": 48370
    },
    {
      "epoch": 24.608341810783315,
      "grad_norm": 46.29035186767578,
      "learning_rate": 2.5391658189216682e-05,
      "loss": 1.9493,
      "step": 48380
    },
    {
      "epoch": 24.613428280773142,
      "grad_norm": 37.4078254699707,
      "learning_rate": 2.538657171922686e-05,
      "loss": 1.9599,
      "step": 48390
    },
    {
      "epoch": 24.61851475076297,
      "grad_norm": 29.42044448852539,
      "learning_rate": 2.538148524923703e-05,
      "loss": 2.0047,
      "step": 48400
    },
    {
      "epoch": 24.623601220752796,
      "grad_norm": 31.058897018432617,
      "learning_rate": 2.5376398779247202e-05,
      "loss": 1.979,
      "step": 48410
    },
    {
      "epoch": 24.628687690742623,
      "grad_norm": 37.057621002197266,
      "learning_rate": 2.537131230925738e-05,
      "loss": 1.9601,
      "step": 48420
    },
    {
      "epoch": 24.63377416073245,
      "grad_norm": 37.40132522583008,
      "learning_rate": 2.536622583926755e-05,
      "loss": 2.036,
      "step": 48430
    },
    {
      "epoch": 24.638860630722277,
      "grad_norm": 38.7337532043457,
      "learning_rate": 2.536113936927772e-05,
      "loss": 1.9605,
      "step": 48440
    },
    {
      "epoch": 24.643947100712104,
      "grad_norm": 34.7530517578125,
      "learning_rate": 2.5356052899287895e-05,
      "loss": 1.9164,
      "step": 48450
    },
    {
      "epoch": 24.64903357070193,
      "grad_norm": 31.386369705200195,
      "learning_rate": 2.535096642929807e-05,
      "loss": 1.9486,
      "step": 48460
    },
    {
      "epoch": 24.654120040691758,
      "grad_norm": 33.56148910522461,
      "learning_rate": 2.534587995930824e-05,
      "loss": 1.962,
      "step": 48470
    },
    {
      "epoch": 24.659206510681585,
      "grad_norm": 39.84840774536133,
      "learning_rate": 2.5340793489318415e-05,
      "loss": 2.0015,
      "step": 48480
    },
    {
      "epoch": 24.664292980671416,
      "grad_norm": 34.15365219116211,
      "learning_rate": 2.5335707019328585e-05,
      "loss": 1.82,
      "step": 48490
    },
    {
      "epoch": 24.669379450661243,
      "grad_norm": 29.648855209350586,
      "learning_rate": 2.533062054933876e-05,
      "loss": 1.9893,
      "step": 48500
    },
    {
      "epoch": 24.67446592065107,
      "grad_norm": 26.48423194885254,
      "learning_rate": 2.5325534079348935e-05,
      "loss": 1.9504,
      "step": 48510
    },
    {
      "epoch": 24.679552390640897,
      "grad_norm": 39.26964569091797,
      "learning_rate": 2.5320447609359105e-05,
      "loss": 1.9715,
      "step": 48520
    },
    {
      "epoch": 24.684638860630724,
      "grad_norm": 31.19871711730957,
      "learning_rate": 2.531536113936928e-05,
      "loss": 1.8558,
      "step": 48530
    },
    {
      "epoch": 24.68972533062055,
      "grad_norm": 37.271244049072266,
      "learning_rate": 2.531027466937945e-05,
      "loss": 1.9502,
      "step": 48540
    },
    {
      "epoch": 24.694811800610378,
      "grad_norm": 27.425884246826172,
      "learning_rate": 2.5305188199389625e-05,
      "loss": 1.9503,
      "step": 48550
    },
    {
      "epoch": 24.699898270600205,
      "grad_norm": 38.17222213745117,
      "learning_rate": 2.53001017293998e-05,
      "loss": 2.0305,
      "step": 48560
    },
    {
      "epoch": 24.70498474059003,
      "grad_norm": 33.347537994384766,
      "learning_rate": 2.529501525940997e-05,
      "loss": 1.9179,
      "step": 48570
    },
    {
      "epoch": 24.71007121057986,
      "grad_norm": 34.56190872192383,
      "learning_rate": 2.528992878942014e-05,
      "loss": 2.1154,
      "step": 48580
    },
    {
      "epoch": 24.715157680569686,
      "grad_norm": 30.386648178100586,
      "learning_rate": 2.5284842319430318e-05,
      "loss": 2.0093,
      "step": 48590
    },
    {
      "epoch": 24.720244150559513,
      "grad_norm": 37.18312454223633,
      "learning_rate": 2.527975584944049e-05,
      "loss": 1.9266,
      "step": 48600
    },
    {
      "epoch": 24.72533062054934,
      "grad_norm": 36.62267303466797,
      "learning_rate": 2.527466937945066e-05,
      "loss": 1.9594,
      "step": 48610
    },
    {
      "epoch": 24.730417090539166,
      "grad_norm": 38.50395965576172,
      "learning_rate": 2.5269582909460838e-05,
      "loss": 1.9793,
      "step": 48620
    },
    {
      "epoch": 24.735503560528993,
      "grad_norm": 40.3451042175293,
      "learning_rate": 2.5264496439471007e-05,
      "loss": 2.0794,
      "step": 48630
    },
    {
      "epoch": 24.74059003051882,
      "grad_norm": 40.62002944946289,
      "learning_rate": 2.525940996948118e-05,
      "loss": 2.0145,
      "step": 48640
    },
    {
      "epoch": 24.745676500508647,
      "grad_norm": 29.441085815429688,
      "learning_rate": 2.5254323499491357e-05,
      "loss": 1.8834,
      "step": 48650
    },
    {
      "epoch": 24.750762970498474,
      "grad_norm": 32.49253463745117,
      "learning_rate": 2.5249237029501527e-05,
      "loss": 1.9976,
      "step": 48660
    },
    {
      "epoch": 24.7558494404883,
      "grad_norm": 38.793907165527344,
      "learning_rate": 2.5244150559511697e-05,
      "loss": 1.9571,
      "step": 48670
    },
    {
      "epoch": 24.76093591047813,
      "grad_norm": 43.879295349121094,
      "learning_rate": 2.5239064089521874e-05,
      "loss": 1.9368,
      "step": 48680
    },
    {
      "epoch": 24.766022380467955,
      "grad_norm": 35.463951110839844,
      "learning_rate": 2.5233977619532044e-05,
      "loss": 2.0044,
      "step": 48690
    },
    {
      "epoch": 24.771108850457782,
      "grad_norm": 40.65935134887695,
      "learning_rate": 2.5228891149542217e-05,
      "loss": 1.9008,
      "step": 48700
    },
    {
      "epoch": 24.77619532044761,
      "grad_norm": 33.11025619506836,
      "learning_rate": 2.5223804679552394e-05,
      "loss": 2.0024,
      "step": 48710
    },
    {
      "epoch": 24.781281790437436,
      "grad_norm": 39.9984016418457,
      "learning_rate": 2.5218718209562564e-05,
      "loss": 1.929,
      "step": 48720
    },
    {
      "epoch": 24.786368260427263,
      "grad_norm": 35.310951232910156,
      "learning_rate": 2.5213631739572734e-05,
      "loss": 1.886,
      "step": 48730
    },
    {
      "epoch": 24.79145473041709,
      "grad_norm": 38.07381820678711,
      "learning_rate": 2.520854526958291e-05,
      "loss": 1.935,
      "step": 48740
    },
    {
      "epoch": 24.796541200406917,
      "grad_norm": 38.80889892578125,
      "learning_rate": 2.5203458799593083e-05,
      "loss": 2.0104,
      "step": 48750
    },
    {
      "epoch": 24.801627670396744,
      "grad_norm": 29.119251251220703,
      "learning_rate": 2.519837232960326e-05,
      "loss": 1.9525,
      "step": 48760
    },
    {
      "epoch": 24.80671414038657,
      "grad_norm": 39.83728790283203,
      "learning_rate": 2.519328585961343e-05,
      "loss": 1.9141,
      "step": 48770
    },
    {
      "epoch": 24.811800610376398,
      "grad_norm": 37.39599609375,
      "learning_rate": 2.51881993896236e-05,
      "loss": 1.939,
      "step": 48780
    },
    {
      "epoch": 24.816887080366225,
      "grad_norm": 40.654754638671875,
      "learning_rate": 2.5183112919633777e-05,
      "loss": 1.9431,
      "step": 48790
    },
    {
      "epoch": 24.821973550356052,
      "grad_norm": 31.486623764038086,
      "learning_rate": 2.517802644964395e-05,
      "loss": 1.9102,
      "step": 48800
    },
    {
      "epoch": 24.82706002034588,
      "grad_norm": 36.620033264160156,
      "learning_rate": 2.517293997965412e-05,
      "loss": 2.0231,
      "step": 48810
    },
    {
      "epoch": 24.832146490335706,
      "grad_norm": 28.39190101623535,
      "learning_rate": 2.5167853509664296e-05,
      "loss": 1.9078,
      "step": 48820
    },
    {
      "epoch": 24.837232960325533,
      "grad_norm": 34.512306213378906,
      "learning_rate": 2.5162767039674466e-05,
      "loss": 1.9206,
      "step": 48830
    },
    {
      "epoch": 24.84231943031536,
      "grad_norm": 31.7357177734375,
      "learning_rate": 2.515768056968464e-05,
      "loss": 1.8348,
      "step": 48840
    },
    {
      "epoch": 24.847405900305187,
      "grad_norm": 37.76456832885742,
      "learning_rate": 2.5152594099694816e-05,
      "loss": 1.8288,
      "step": 48850
    },
    {
      "epoch": 24.852492370295014,
      "grad_norm": 33.130733489990234,
      "learning_rate": 2.5147507629704986e-05,
      "loss": 1.9556,
      "step": 48860
    },
    {
      "epoch": 24.85757884028484,
      "grad_norm": 35.4602165222168,
      "learning_rate": 2.5142421159715156e-05,
      "loss": 1.95,
      "step": 48870
    },
    {
      "epoch": 24.862665310274668,
      "grad_norm": 41.17063903808594,
      "learning_rate": 2.5137334689725333e-05,
      "loss": 1.9211,
      "step": 48880
    },
    {
      "epoch": 24.867751780264495,
      "grad_norm": 39.22504425048828,
      "learning_rate": 2.5132248219735506e-05,
      "loss": 2.008,
      "step": 48890
    },
    {
      "epoch": 24.872838250254322,
      "grad_norm": 27.882204055786133,
      "learning_rate": 2.5127161749745676e-05,
      "loss": 1.9958,
      "step": 48900
    },
    {
      "epoch": 24.87792472024415,
      "grad_norm": 39.13938903808594,
      "learning_rate": 2.5122075279755853e-05,
      "loss": 1.9298,
      "step": 48910
    },
    {
      "epoch": 24.88301119023398,
      "grad_norm": 31.639301300048828,
      "learning_rate": 2.5116988809766022e-05,
      "loss": 1.9104,
      "step": 48920
    },
    {
      "epoch": 24.888097660223806,
      "grad_norm": 36.03168487548828,
      "learning_rate": 2.5111902339776196e-05,
      "loss": 1.8989,
      "step": 48930
    },
    {
      "epoch": 24.893184130213633,
      "grad_norm": 32.753047943115234,
      "learning_rate": 2.5106815869786372e-05,
      "loss": 2.0439,
      "step": 48940
    },
    {
      "epoch": 24.89827060020346,
      "grad_norm": 36.378570556640625,
      "learning_rate": 2.5101729399796542e-05,
      "loss": 1.9601,
      "step": 48950
    },
    {
      "epoch": 24.903357070193287,
      "grad_norm": 37.180992126464844,
      "learning_rate": 2.5096642929806712e-05,
      "loss": 1.98,
      "step": 48960
    },
    {
      "epoch": 24.908443540183114,
      "grad_norm": 35.598350524902344,
      "learning_rate": 2.509155645981689e-05,
      "loss": 1.9254,
      "step": 48970
    },
    {
      "epoch": 24.91353001017294,
      "grad_norm": 32.60762023925781,
      "learning_rate": 2.5086469989827062e-05,
      "loss": 1.8618,
      "step": 48980
    },
    {
      "epoch": 24.91861648016277,
      "grad_norm": 41.608455657958984,
      "learning_rate": 2.5081383519837232e-05,
      "loss": 1.9912,
      "step": 48990
    },
    {
      "epoch": 24.923702950152595,
      "grad_norm": 37.354835510253906,
      "learning_rate": 2.507629704984741e-05,
      "loss": 1.9328,
      "step": 49000
    },
    {
      "epoch": 24.928789420142422,
      "grad_norm": 30.495441436767578,
      "learning_rate": 2.507121057985758e-05,
      "loss": 1.9125,
      "step": 49010
    },
    {
      "epoch": 24.93387589013225,
      "grad_norm": 34.079627990722656,
      "learning_rate": 2.5066124109867755e-05,
      "loss": 1.9623,
      "step": 49020
    },
    {
      "epoch": 24.938962360122076,
      "grad_norm": 37.83851623535156,
      "learning_rate": 2.5061037639877925e-05,
      "loss": 1.8922,
      "step": 49030
    },
    {
      "epoch": 24.944048830111903,
      "grad_norm": 40.979923248291016,
      "learning_rate": 2.50559511698881e-05,
      "loss": 1.9747,
      "step": 49040
    },
    {
      "epoch": 24.94913530010173,
      "grad_norm": 28.411325454711914,
      "learning_rate": 2.5050864699898275e-05,
      "loss": 1.8854,
      "step": 49050
    },
    {
      "epoch": 24.954221770091557,
      "grad_norm": 33.770477294921875,
      "learning_rate": 2.5045778229908445e-05,
      "loss": 1.9239,
      "step": 49060
    },
    {
      "epoch": 24.959308240081384,
      "grad_norm": 32.19306182861328,
      "learning_rate": 2.5040691759918615e-05,
      "loss": 1.9583,
      "step": 49070
    },
    {
      "epoch": 24.96439471007121,
      "grad_norm": 33.875701904296875,
      "learning_rate": 2.503560528992879e-05,
      "loss": 1.8894,
      "step": 49080
    },
    {
      "epoch": 24.969481180061038,
      "grad_norm": 32.936702728271484,
      "learning_rate": 2.5030518819938965e-05,
      "loss": 1.9675,
      "step": 49090
    },
    {
      "epoch": 24.974567650050865,
      "grad_norm": 45.92161560058594,
      "learning_rate": 2.5025432349949135e-05,
      "loss": 1.9998,
      "step": 49100
    },
    {
      "epoch": 24.979654120040692,
      "grad_norm": 34.6057243347168,
      "learning_rate": 2.502034587995931e-05,
      "loss": 1.8991,
      "step": 49110
    },
    {
      "epoch": 24.98474059003052,
      "grad_norm": 36.434906005859375,
      "learning_rate": 2.501525940996948e-05,
      "loss": 2.0031,
      "step": 49120
    },
    {
      "epoch": 24.989827060020346,
      "grad_norm": 41.171268463134766,
      "learning_rate": 2.5010172939979655e-05,
      "loss": 1.948,
      "step": 49130
    },
    {
      "epoch": 24.994913530010173,
      "grad_norm": 34.503360748291016,
      "learning_rate": 2.500508646998983e-05,
      "loss": 2.0172,
      "step": 49140
    },
    {
      "epoch": 25.0,
      "grad_norm": 36.471561431884766,
      "learning_rate": 2.5e-05,
      "loss": 1.9119,
      "step": 49150
    },
    {
      "epoch": 25.0,
      "eval_loss": 4.560948848724365,
      "eval_runtime": 2.7338,
      "eval_samples_per_second": 1015.068,
      "eval_steps_per_second": 126.929,
      "step": 49150
    },
    {
      "epoch": 25.005086469989827,
      "grad_norm": 31.39484405517578,
      "learning_rate": 2.4994913530010174e-05,
      "loss": 1.8789,
      "step": 49160
    },
    {
      "epoch": 25.010172939979654,
      "grad_norm": 40.92361068725586,
      "learning_rate": 2.4989827060020344e-05,
      "loss": 1.8482,
      "step": 49170
    },
    {
      "epoch": 25.01525940996948,
      "grad_norm": 38.36813735961914,
      "learning_rate": 2.498474059003052e-05,
      "loss": 1.9031,
      "step": 49180
    },
    {
      "epoch": 25.020345879959308,
      "grad_norm": 32.860374450683594,
      "learning_rate": 2.4979654120040694e-05,
      "loss": 1.9485,
      "step": 49190
    },
    {
      "epoch": 25.025432349949135,
      "grad_norm": 40.48760986328125,
      "learning_rate": 2.4974567650050864e-05,
      "loss": 1.842,
      "step": 49200
    },
    {
      "epoch": 25.030518819938962,
      "grad_norm": 33.77052688598633,
      "learning_rate": 2.4969481180061037e-05,
      "loss": 1.93,
      "step": 49210
    },
    {
      "epoch": 25.03560528992879,
      "grad_norm": 27.096845626831055,
      "learning_rate": 2.496439471007121e-05,
      "loss": 1.969,
      "step": 49220
    },
    {
      "epoch": 25.040691759918616,
      "grad_norm": 44.91704177856445,
      "learning_rate": 2.4959308240081387e-05,
      "loss": 1.8994,
      "step": 49230
    },
    {
      "epoch": 25.045778229908443,
      "grad_norm": 33.97187805175781,
      "learning_rate": 2.4954221770091557e-05,
      "loss": 1.9422,
      "step": 49240
    },
    {
      "epoch": 25.05086469989827,
      "grad_norm": 39.0309944152832,
      "learning_rate": 2.494913530010173e-05,
      "loss": 1.8717,
      "step": 49250
    },
    {
      "epoch": 25.055951169888097,
      "grad_norm": 43.150489807128906,
      "learning_rate": 2.4944048830111904e-05,
      "loss": 1.9259,
      "step": 49260
    },
    {
      "epoch": 25.061037639877924,
      "grad_norm": 34.888587951660156,
      "learning_rate": 2.4938962360122077e-05,
      "loss": 1.933,
      "step": 49270
    },
    {
      "epoch": 25.06612410986775,
      "grad_norm": 35.088340759277344,
      "learning_rate": 2.493387589013225e-05,
      "loss": 1.8756,
      "step": 49280
    },
    {
      "epoch": 25.071210579857578,
      "grad_norm": 32.375404357910156,
      "learning_rate": 2.4928789420142424e-05,
      "loss": 1.9393,
      "step": 49290
    },
    {
      "epoch": 25.076297049847405,
      "grad_norm": 37.1890754699707,
      "learning_rate": 2.4923702950152594e-05,
      "loss": 1.9158,
      "step": 49300
    },
    {
      "epoch": 25.08138351983723,
      "grad_norm": 35.44257354736328,
      "learning_rate": 2.4918616480162767e-05,
      "loss": 2.0251,
      "step": 49310
    },
    {
      "epoch": 25.08646998982706,
      "grad_norm": 36.2808952331543,
      "learning_rate": 2.491353001017294e-05,
      "loss": 1.9971,
      "step": 49320
    },
    {
      "epoch": 25.091556459816886,
      "grad_norm": 34.8821907043457,
      "learning_rate": 2.4908443540183113e-05,
      "loss": 1.9212,
      "step": 49330
    },
    {
      "epoch": 25.096642929806713,
      "grad_norm": 33.81391143798828,
      "learning_rate": 2.4903357070193287e-05,
      "loss": 1.9294,
      "step": 49340
    },
    {
      "epoch": 25.10172939979654,
      "grad_norm": 32.070281982421875,
      "learning_rate": 2.489827060020346e-05,
      "loss": 1.9589,
      "step": 49350
    },
    {
      "epoch": 25.106815869786367,
      "grad_norm": 41.805973052978516,
      "learning_rate": 2.4893184130213633e-05,
      "loss": 1.8263,
      "step": 49360
    },
    {
      "epoch": 25.111902339776194,
      "grad_norm": 34.68045425415039,
      "learning_rate": 2.4888097660223807e-05,
      "loss": 1.9074,
      "step": 49370
    },
    {
      "epoch": 25.116988809766024,
      "grad_norm": 46.98849868774414,
      "learning_rate": 2.488301119023398e-05,
      "loss": 1.9884,
      "step": 49380
    },
    {
      "epoch": 25.12207527975585,
      "grad_norm": 32.44158935546875,
      "learning_rate": 2.4877924720244153e-05,
      "loss": 1.9242,
      "step": 49390
    },
    {
      "epoch": 25.127161749745678,
      "grad_norm": 34.18362045288086,
      "learning_rate": 2.4872838250254323e-05,
      "loss": 1.9697,
      "step": 49400
    },
    {
      "epoch": 25.132248219735505,
      "grad_norm": 39.945438385009766,
      "learning_rate": 2.4867751780264496e-05,
      "loss": 1.8806,
      "step": 49410
    },
    {
      "epoch": 25.137334689725332,
      "grad_norm": 47.98004150390625,
      "learning_rate": 2.4862665310274673e-05,
      "loss": 1.9391,
      "step": 49420
    },
    {
      "epoch": 25.14242115971516,
      "grad_norm": 35.094703674316406,
      "learning_rate": 2.4857578840284843e-05,
      "loss": 1.9527,
      "step": 49430
    },
    {
      "epoch": 25.147507629704986,
      "grad_norm": 31.2946834564209,
      "learning_rate": 2.4852492370295016e-05,
      "loss": 1.8988,
      "step": 49440
    },
    {
      "epoch": 25.152594099694813,
      "grad_norm": 39.97712326049805,
      "learning_rate": 2.484740590030519e-05,
      "loss": 1.9563,
      "step": 49450
    },
    {
      "epoch": 25.15768056968464,
      "grad_norm": 38.90068817138672,
      "learning_rate": 2.4842319430315363e-05,
      "loss": 1.8753,
      "step": 49460
    },
    {
      "epoch": 25.162767039674467,
      "grad_norm": 35.681671142578125,
      "learning_rate": 2.4837232960325536e-05,
      "loss": 1.8881,
      "step": 49470
    },
    {
      "epoch": 25.167853509664294,
      "grad_norm": 37.39442443847656,
      "learning_rate": 2.483214649033571e-05,
      "loss": 1.9693,
      "step": 49480
    },
    {
      "epoch": 25.17293997965412,
      "grad_norm": 32.67555618286133,
      "learning_rate": 2.4827060020345883e-05,
      "loss": 1.9357,
      "step": 49490
    },
    {
      "epoch": 25.178026449643948,
      "grad_norm": 36.23991012573242,
      "learning_rate": 2.4821973550356053e-05,
      "loss": 1.8967,
      "step": 49500
    },
    {
      "epoch": 25.183112919633775,
      "grad_norm": 33.5008430480957,
      "learning_rate": 2.4816887080366226e-05,
      "loss": 1.8401,
      "step": 49510
    },
    {
      "epoch": 25.188199389623602,
      "grad_norm": 35.912105560302734,
      "learning_rate": 2.4811800610376402e-05,
      "loss": 1.9653,
      "step": 49520
    },
    {
      "epoch": 25.19328585961343,
      "grad_norm": 45.61634063720703,
      "learning_rate": 2.4806714140386572e-05,
      "loss": 1.9434,
      "step": 49530
    },
    {
      "epoch": 25.198372329603256,
      "grad_norm": 28.623682022094727,
      "learning_rate": 2.4801627670396746e-05,
      "loss": 1.894,
      "step": 49540
    },
    {
      "epoch": 25.203458799593083,
      "grad_norm": 39.18720245361328,
      "learning_rate": 2.479654120040692e-05,
      "loss": 1.9053,
      "step": 49550
    },
    {
      "epoch": 25.20854526958291,
      "grad_norm": 39.82411575317383,
      "learning_rate": 2.4791454730417092e-05,
      "loss": 1.9103,
      "step": 49560
    },
    {
      "epoch": 25.213631739572737,
      "grad_norm": 33.940521240234375,
      "learning_rate": 2.4786368260427265e-05,
      "loss": 1.9406,
      "step": 49570
    },
    {
      "epoch": 25.218718209562564,
      "grad_norm": 32.31858825683594,
      "learning_rate": 2.478128179043744e-05,
      "loss": 1.9888,
      "step": 49580
    },
    {
      "epoch": 25.22380467955239,
      "grad_norm": 29.655790328979492,
      "learning_rate": 2.477619532044761e-05,
      "loss": 1.9235,
      "step": 49590
    },
    {
      "epoch": 25.228891149542218,
      "grad_norm": 36.94559097290039,
      "learning_rate": 2.4771108850457782e-05,
      "loss": 1.831,
      "step": 49600
    },
    {
      "epoch": 25.233977619532045,
      "grad_norm": 37.7459831237793,
      "learning_rate": 2.476602238046796e-05,
      "loss": 1.9086,
      "step": 49610
    },
    {
      "epoch": 25.23906408952187,
      "grad_norm": 32.869102478027344,
      "learning_rate": 2.476093591047813e-05,
      "loss": 1.9556,
      "step": 49620
    },
    {
      "epoch": 25.2441505595117,
      "grad_norm": 33.611778259277344,
      "learning_rate": 2.4755849440488302e-05,
      "loss": 1.9749,
      "step": 49630
    },
    {
      "epoch": 25.249237029501526,
      "grad_norm": 36.8590202331543,
      "learning_rate": 2.4750762970498475e-05,
      "loss": 1.9366,
      "step": 49640
    },
    {
      "epoch": 25.254323499491353,
      "grad_norm": 34.60536575317383,
      "learning_rate": 2.474567650050865e-05,
      "loss": 2.056,
      "step": 49650
    },
    {
      "epoch": 25.25940996948118,
      "grad_norm": 45.479774475097656,
      "learning_rate": 2.474059003051882e-05,
      "loss": 1.907,
      "step": 49660
    },
    {
      "epoch": 25.264496439471007,
      "grad_norm": 34.89570999145508,
      "learning_rate": 2.4735503560528995e-05,
      "loss": 1.9284,
      "step": 49670
    },
    {
      "epoch": 25.269582909460834,
      "grad_norm": 29.616758346557617,
      "learning_rate": 2.4730417090539168e-05,
      "loss": 1.8334,
      "step": 49680
    },
    {
      "epoch": 25.27466937945066,
      "grad_norm": 40.05548858642578,
      "learning_rate": 2.4725330620549338e-05,
      "loss": 1.8599,
      "step": 49690
    },
    {
      "epoch": 25.279755849440487,
      "grad_norm": 36.696258544921875,
      "learning_rate": 2.472024415055951e-05,
      "loss": 1.9659,
      "step": 49700
    },
    {
      "epoch": 25.284842319430314,
      "grad_norm": 36.604736328125,
      "learning_rate": 2.4715157680569688e-05,
      "loss": 1.9384,
      "step": 49710
    },
    {
      "epoch": 25.28992878942014,
      "grad_norm": 35.14529037475586,
      "learning_rate": 2.4710071210579858e-05,
      "loss": 1.9152,
      "step": 49720
    },
    {
      "epoch": 25.29501525940997,
      "grad_norm": 40.2806510925293,
      "learning_rate": 2.470498474059003e-05,
      "loss": 1.8927,
      "step": 49730
    },
    {
      "epoch": 25.300101729399795,
      "grad_norm": 32.412933349609375,
      "learning_rate": 2.4699898270600204e-05,
      "loss": 1.9283,
      "step": 49740
    },
    {
      "epoch": 25.305188199389622,
      "grad_norm": 34.22091293334961,
      "learning_rate": 2.4694811800610378e-05,
      "loss": 1.8894,
      "step": 49750
    },
    {
      "epoch": 25.31027466937945,
      "grad_norm": 41.40024185180664,
      "learning_rate": 2.468972533062055e-05,
      "loss": 1.9329,
      "step": 49760
    },
    {
      "epoch": 25.315361139369276,
      "grad_norm": 36.28358459472656,
      "learning_rate": 2.4684638860630724e-05,
      "loss": 1.8616,
      "step": 49770
    },
    {
      "epoch": 25.320447609359103,
      "grad_norm": 35.166786193847656,
      "learning_rate": 2.4679552390640898e-05,
      "loss": 1.9119,
      "step": 49780
    },
    {
      "epoch": 25.32553407934893,
      "grad_norm": 34.54508972167969,
      "learning_rate": 2.4674465920651068e-05,
      "loss": 1.8549,
      "step": 49790
    },
    {
      "epoch": 25.330620549338757,
      "grad_norm": 34.546993255615234,
      "learning_rate": 2.466937945066124e-05,
      "loss": 1.8234,
      "step": 49800
    },
    {
      "epoch": 25.335707019328584,
      "grad_norm": 37.352535247802734,
      "learning_rate": 2.4664292980671417e-05,
      "loss": 1.8691,
      "step": 49810
    },
    {
      "epoch": 25.340793489318415,
      "grad_norm": 42.719871520996094,
      "learning_rate": 2.4659206510681587e-05,
      "loss": 1.917,
      "step": 49820
    },
    {
      "epoch": 25.345879959308242,
      "grad_norm": 31.065805435180664,
      "learning_rate": 2.465412004069176e-05,
      "loss": 1.8947,
      "step": 49830
    },
    {
      "epoch": 25.35096642929807,
      "grad_norm": 36.83116149902344,
      "learning_rate": 2.4649033570701934e-05,
      "loss": 1.9497,
      "step": 49840
    },
    {
      "epoch": 25.356052899287896,
      "grad_norm": 33.32059860229492,
      "learning_rate": 2.4643947100712107e-05,
      "loss": 1.941,
      "step": 49850
    },
    {
      "epoch": 25.361139369277723,
      "grad_norm": 31.90084457397461,
      "learning_rate": 2.463886063072228e-05,
      "loss": 1.984,
      "step": 49860
    },
    {
      "epoch": 25.36622583926755,
      "grad_norm": 36.426597595214844,
      "learning_rate": 2.4633774160732454e-05,
      "loss": 2.0107,
      "step": 49870
    },
    {
      "epoch": 25.371312309257377,
      "grad_norm": 38.28730010986328,
      "learning_rate": 2.4628687690742624e-05,
      "loss": 1.9282,
      "step": 49880
    },
    {
      "epoch": 25.376398779247204,
      "grad_norm": 36.813777923583984,
      "learning_rate": 2.4623601220752797e-05,
      "loss": 1.9244,
      "step": 49890
    },
    {
      "epoch": 25.38148524923703,
      "grad_norm": 31.789531707763672,
      "learning_rate": 2.4618514750762974e-05,
      "loss": 1.9602,
      "step": 49900
    },
    {
      "epoch": 25.386571719226858,
      "grad_norm": 37.51559066772461,
      "learning_rate": 2.4613428280773147e-05,
      "loss": 1.961,
      "step": 49910
    },
    {
      "epoch": 25.391658189216685,
      "grad_norm": 31.23957061767578,
      "learning_rate": 2.4608341810783317e-05,
      "loss": 1.9584,
      "step": 49920
    },
    {
      "epoch": 25.39674465920651,
      "grad_norm": 40.090309143066406,
      "learning_rate": 2.460325534079349e-05,
      "loss": 1.8463,
      "step": 49930
    },
    {
      "epoch": 25.40183112919634,
      "grad_norm": 45.22180938720703,
      "learning_rate": 2.4598168870803663e-05,
      "loss": 1.9712,
      "step": 49940
    },
    {
      "epoch": 25.406917599186166,
      "grad_norm": 47.04026412963867,
      "learning_rate": 2.4593082400813837e-05,
      "loss": 1.9605,
      "step": 49950
    },
    {
      "epoch": 25.412004069175993,
      "grad_norm": 37.604244232177734,
      "learning_rate": 2.458799593082401e-05,
      "loss": 1.9144,
      "step": 49960
    },
    {
      "epoch": 25.41709053916582,
      "grad_norm": 35.502113342285156,
      "learning_rate": 2.4582909460834183e-05,
      "loss": 1.9764,
      "step": 49970
    },
    {
      "epoch": 25.422177009155646,
      "grad_norm": 51.60757827758789,
      "learning_rate": 2.4577822990844353e-05,
      "loss": 1.911,
      "step": 49980
    },
    {
      "epoch": 25.427263479145473,
      "grad_norm": 33.775115966796875,
      "learning_rate": 2.4572736520854526e-05,
      "loss": 1.9791,
      "step": 49990
    },
    {
      "epoch": 25.4323499491353,
      "grad_norm": 43.27019119262695,
      "learning_rate": 2.4567650050864703e-05,
      "loss": 1.9226,
      "step": 50000
    },
    {
      "epoch": 25.437436419125127,
      "grad_norm": 41.20526885986328,
      "learning_rate": 2.4562563580874873e-05,
      "loss": 1.8779,
      "step": 50010
    },
    {
      "epoch": 25.442522889114954,
      "grad_norm": 32.01722717285156,
      "learning_rate": 2.4557477110885046e-05,
      "loss": 2.0181,
      "step": 50020
    },
    {
      "epoch": 25.44760935910478,
      "grad_norm": 36.38176727294922,
      "learning_rate": 2.455239064089522e-05,
      "loss": 1.9862,
      "step": 50030
    },
    {
      "epoch": 25.45269582909461,
      "grad_norm": 34.65071487426758,
      "learning_rate": 2.4547304170905393e-05,
      "loss": 1.9575,
      "step": 50040
    },
    {
      "epoch": 25.457782299084435,
      "grad_norm": 40.06050109863281,
      "learning_rate": 2.4542217700915566e-05,
      "loss": 1.9358,
      "step": 50050
    },
    {
      "epoch": 25.462868769074262,
      "grad_norm": 35.686485290527344,
      "learning_rate": 2.453713123092574e-05,
      "loss": 1.9029,
      "step": 50060
    },
    {
      "epoch": 25.46795523906409,
      "grad_norm": 38.508262634277344,
      "learning_rate": 2.4532044760935913e-05,
      "loss": 1.8939,
      "step": 50070
    },
    {
      "epoch": 25.473041709053916,
      "grad_norm": 34.30076217651367,
      "learning_rate": 2.4526958290946083e-05,
      "loss": 1.8602,
      "step": 50080
    },
    {
      "epoch": 25.478128179043743,
      "grad_norm": 27.464706420898438,
      "learning_rate": 2.452187182095626e-05,
      "loss": 1.9523,
      "step": 50090
    },
    {
      "epoch": 25.48321464903357,
      "grad_norm": 38.28023910522461,
      "learning_rate": 2.4516785350966432e-05,
      "loss": 1.9129,
      "step": 50100
    },
    {
      "epoch": 25.488301119023397,
      "grad_norm": 41.683998107910156,
      "learning_rate": 2.4511698880976602e-05,
      "loss": 1.9248,
      "step": 50110
    },
    {
      "epoch": 25.493387589013224,
      "grad_norm": 37.37273025512695,
      "learning_rate": 2.4506612410986776e-05,
      "loss": 1.8697,
      "step": 50120
    },
    {
      "epoch": 25.49847405900305,
      "grad_norm": 34.60917663574219,
      "learning_rate": 2.450152594099695e-05,
      "loss": 1.8802,
      "step": 50130
    },
    {
      "epoch": 25.503560528992878,
      "grad_norm": 39.474517822265625,
      "learning_rate": 2.4496439471007122e-05,
      "loss": 1.9181,
      "step": 50140
    },
    {
      "epoch": 25.508646998982705,
      "grad_norm": 52.34354782104492,
      "learning_rate": 2.4491353001017295e-05,
      "loss": 1.8955,
      "step": 50150
    },
    {
      "epoch": 25.513733468972532,
      "grad_norm": 43.77267074584961,
      "learning_rate": 2.448626653102747e-05,
      "loss": 1.9843,
      "step": 50160
    },
    {
      "epoch": 25.51881993896236,
      "grad_norm": 35.12635803222656,
      "learning_rate": 2.4481180061037642e-05,
      "loss": 1.8979,
      "step": 50170
    },
    {
      "epoch": 25.523906408952186,
      "grad_norm": 32.94468688964844,
      "learning_rate": 2.4476093591047812e-05,
      "loss": 1.8517,
      "step": 50180
    },
    {
      "epoch": 25.528992878942013,
      "grad_norm": 34.2470817565918,
      "learning_rate": 2.447100712105799e-05,
      "loss": 1.9937,
      "step": 50190
    },
    {
      "epoch": 25.53407934893184,
      "grad_norm": 38.64905548095703,
      "learning_rate": 2.4465920651068162e-05,
      "loss": 1.9348,
      "step": 50200
    },
    {
      "epoch": 25.539165818921667,
      "grad_norm": 31.940715789794922,
      "learning_rate": 2.4460834181078332e-05,
      "loss": 1.9722,
      "step": 50210
    },
    {
      "epoch": 25.544252288911494,
      "grad_norm": 25.359539031982422,
      "learning_rate": 2.4455747711088505e-05,
      "loss": 1.857,
      "step": 50220
    },
    {
      "epoch": 25.54933875890132,
      "grad_norm": 30.247140884399414,
      "learning_rate": 2.445066124109868e-05,
      "loss": 1.9364,
      "step": 50230
    },
    {
      "epoch": 25.554425228891148,
      "grad_norm": 33.63775634765625,
      "learning_rate": 2.444557477110885e-05,
      "loss": 1.8985,
      "step": 50240
    },
    {
      "epoch": 25.559511698880975,
      "grad_norm": 44.91663360595703,
      "learning_rate": 2.4440488301119025e-05,
      "loss": 1.905,
      "step": 50250
    },
    {
      "epoch": 25.564598168870802,
      "grad_norm": 35.79835891723633,
      "learning_rate": 2.4435401831129198e-05,
      "loss": 1.8296,
      "step": 50260
    },
    {
      "epoch": 25.56968463886063,
      "grad_norm": 30.006256103515625,
      "learning_rate": 2.4430315361139368e-05,
      "loss": 1.9111,
      "step": 50270
    },
    {
      "epoch": 25.57477110885046,
      "grad_norm": 30.41160774230957,
      "learning_rate": 2.442522889114954e-05,
      "loss": 1.864,
      "step": 50280
    },
    {
      "epoch": 25.579857578840286,
      "grad_norm": 42.952850341796875,
      "learning_rate": 2.4420142421159718e-05,
      "loss": 1.9521,
      "step": 50290
    },
    {
      "epoch": 25.584944048830113,
      "grad_norm": 34.853248596191406,
      "learning_rate": 2.441505595116989e-05,
      "loss": 1.861,
      "step": 50300
    },
    {
      "epoch": 25.59003051881994,
      "grad_norm": 42.03774642944336,
      "learning_rate": 2.440996948118006e-05,
      "loss": 1.8561,
      "step": 50310
    },
    {
      "epoch": 25.595116988809767,
      "grad_norm": 37.87155532836914,
      "learning_rate": 2.4404883011190234e-05,
      "loss": 1.9487,
      "step": 50320
    },
    {
      "epoch": 25.600203458799594,
      "grad_norm": 41.306819915771484,
      "learning_rate": 2.4399796541200408e-05,
      "loss": 1.8454,
      "step": 50330
    },
    {
      "epoch": 25.60528992878942,
      "grad_norm": 33.160152435302734,
      "learning_rate": 2.439471007121058e-05,
      "loss": 1.8666,
      "step": 50340
    },
    {
      "epoch": 25.61037639877925,
      "grad_norm": 34.40985107421875,
      "learning_rate": 2.4389623601220754e-05,
      "loss": 1.7868,
      "step": 50350
    },
    {
      "epoch": 25.615462868769075,
      "grad_norm": 33.54729080200195,
      "learning_rate": 2.4384537131230928e-05,
      "loss": 1.8459,
      "step": 50360
    },
    {
      "epoch": 25.620549338758902,
      "grad_norm": 40.50332260131836,
      "learning_rate": 2.4379450661241098e-05,
      "loss": 1.9491,
      "step": 50370
    },
    {
      "epoch": 25.62563580874873,
      "grad_norm": 39.15140914916992,
      "learning_rate": 2.4374364191251274e-05,
      "loss": 1.9371,
      "step": 50380
    },
    {
      "epoch": 25.630722278738556,
      "grad_norm": 34.30918884277344,
      "learning_rate": 2.4369277721261447e-05,
      "loss": 1.9955,
      "step": 50390
    },
    {
      "epoch": 25.635808748728383,
      "grad_norm": 33.34307098388672,
      "learning_rate": 2.4364191251271617e-05,
      "loss": 1.9211,
      "step": 50400
    },
    {
      "epoch": 25.64089521871821,
      "grad_norm": 32.876502990722656,
      "learning_rate": 2.435910478128179e-05,
      "loss": 1.8696,
      "step": 50410
    },
    {
      "epoch": 25.645981688708037,
      "grad_norm": 32.88128662109375,
      "learning_rate": 2.4354018311291964e-05,
      "loss": 1.8876,
      "step": 50420
    },
    {
      "epoch": 25.651068158697864,
      "grad_norm": 44.77682876586914,
      "learning_rate": 2.4348931841302137e-05,
      "loss": 1.9256,
      "step": 50430
    },
    {
      "epoch": 25.65615462868769,
      "grad_norm": 38.782012939453125,
      "learning_rate": 2.434384537131231e-05,
      "loss": 1.9177,
      "step": 50440
    },
    {
      "epoch": 25.661241098677518,
      "grad_norm": 26.5346736907959,
      "learning_rate": 2.4338758901322484e-05,
      "loss": 1.9459,
      "step": 50450
    },
    {
      "epoch": 25.666327568667345,
      "grad_norm": 31.351476669311523,
      "learning_rate": 2.4333672431332657e-05,
      "loss": 1.8772,
      "step": 50460
    },
    {
      "epoch": 25.671414038657172,
      "grad_norm": 40.082679748535156,
      "learning_rate": 2.4328585961342827e-05,
      "loss": 1.9732,
      "step": 50470
    },
    {
      "epoch": 25.676500508647,
      "grad_norm": 35.248023986816406,
      "learning_rate": 2.4323499491353004e-05,
      "loss": 1.8568,
      "step": 50480
    },
    {
      "epoch": 25.681586978636826,
      "grad_norm": 31.668195724487305,
      "learning_rate": 2.4318413021363177e-05,
      "loss": 1.9278,
      "step": 50490
    },
    {
      "epoch": 25.686673448626653,
      "grad_norm": 34.291622161865234,
      "learning_rate": 2.4313326551373347e-05,
      "loss": 1.9561,
      "step": 50500
    },
    {
      "epoch": 25.69175991861648,
      "grad_norm": 36.12406921386719,
      "learning_rate": 2.430824008138352e-05,
      "loss": 1.8654,
      "step": 50510
    },
    {
      "epoch": 25.696846388606307,
      "grad_norm": 41.83882141113281,
      "learning_rate": 2.4303153611393693e-05,
      "loss": 1.8342,
      "step": 50520
    },
    {
      "epoch": 25.701932858596134,
      "grad_norm": 31.891559600830078,
      "learning_rate": 2.4298067141403867e-05,
      "loss": 1.9528,
      "step": 50530
    },
    {
      "epoch": 25.70701932858596,
      "grad_norm": 30.14550018310547,
      "learning_rate": 2.429298067141404e-05,
      "loss": 1.9857,
      "step": 50540
    },
    {
      "epoch": 25.712105798575788,
      "grad_norm": 39.4228515625,
      "learning_rate": 2.4287894201424213e-05,
      "loss": 1.9152,
      "step": 50550
    },
    {
      "epoch": 25.717192268565615,
      "grad_norm": 36.175811767578125,
      "learning_rate": 2.4282807731434383e-05,
      "loss": 1.8998,
      "step": 50560
    },
    {
      "epoch": 25.722278738555442,
      "grad_norm": 34.65328598022461,
      "learning_rate": 2.427772126144456e-05,
      "loss": 1.8999,
      "step": 50570
    },
    {
      "epoch": 25.72736520854527,
      "grad_norm": 38.149227142333984,
      "learning_rate": 2.4272634791454733e-05,
      "loss": 1.9233,
      "step": 50580
    },
    {
      "epoch": 25.732451678535096,
      "grad_norm": 36.210025787353516,
      "learning_rate": 2.4267548321464906e-05,
      "loss": 1.982,
      "step": 50590
    },
    {
      "epoch": 25.737538148524923,
      "grad_norm": 33.52908706665039,
      "learning_rate": 2.4262461851475076e-05,
      "loss": 1.869,
      "step": 50600
    },
    {
      "epoch": 25.74262461851475,
      "grad_norm": 34.7794189453125,
      "learning_rate": 2.425737538148525e-05,
      "loss": 1.8847,
      "step": 50610
    },
    {
      "epoch": 25.747711088504577,
      "grad_norm": 32.67730712890625,
      "learning_rate": 2.4252288911495423e-05,
      "loss": 1.8922,
      "step": 50620
    },
    {
      "epoch": 25.752797558494404,
      "grad_norm": 34.210899353027344,
      "learning_rate": 2.4247202441505596e-05,
      "loss": 1.9409,
      "step": 50630
    },
    {
      "epoch": 25.75788402848423,
      "grad_norm": 33.76942443847656,
      "learning_rate": 2.424211597151577e-05,
      "loss": 1.9371,
      "step": 50640
    },
    {
      "epoch": 25.762970498474058,
      "grad_norm": 39.757537841796875,
      "learning_rate": 2.4237029501525943e-05,
      "loss": 1.9353,
      "step": 50650
    },
    {
      "epoch": 25.768056968463885,
      "grad_norm": 38.095096588134766,
      "learning_rate": 2.4231943031536113e-05,
      "loss": 1.885,
      "step": 50660
    },
    {
      "epoch": 25.77314343845371,
      "grad_norm": 37.981468200683594,
      "learning_rate": 2.422685656154629e-05,
      "loss": 1.7581,
      "step": 50670
    },
    {
      "epoch": 25.77822990844354,
      "grad_norm": 30.552501678466797,
      "learning_rate": 2.4221770091556462e-05,
      "loss": 1.9114,
      "step": 50680
    },
    {
      "epoch": 25.783316378433366,
      "grad_norm": 39.86091995239258,
      "learning_rate": 2.4216683621566632e-05,
      "loss": 1.9088,
      "step": 50690
    },
    {
      "epoch": 25.788402848423193,
      "grad_norm": 37.32025909423828,
      "learning_rate": 2.4211597151576806e-05,
      "loss": 1.996,
      "step": 50700
    },
    {
      "epoch": 25.793489318413023,
      "grad_norm": 42.788848876953125,
      "learning_rate": 2.420651068158698e-05,
      "loss": 1.9001,
      "step": 50710
    },
    {
      "epoch": 25.79857578840285,
      "grad_norm": 33.66411590576172,
      "learning_rate": 2.4201424211597156e-05,
      "loss": 1.813,
      "step": 50720
    },
    {
      "epoch": 25.803662258392677,
      "grad_norm": 39.188289642333984,
      "learning_rate": 2.4196337741607325e-05,
      "loss": 1.9103,
      "step": 50730
    },
    {
      "epoch": 25.808748728382504,
      "grad_norm": 35.32512664794922,
      "learning_rate": 2.41912512716175e-05,
      "loss": 1.9919,
      "step": 50740
    },
    {
      "epoch": 25.81383519837233,
      "grad_norm": 36.3408088684082,
      "learning_rate": 2.4186164801627672e-05,
      "loss": 1.9497,
      "step": 50750
    },
    {
      "epoch": 25.818921668362158,
      "grad_norm": 41.85051727294922,
      "learning_rate": 2.4181078331637845e-05,
      "loss": 1.8935,
      "step": 50760
    },
    {
      "epoch": 25.824008138351985,
      "grad_norm": 37.560001373291016,
      "learning_rate": 2.417599186164802e-05,
      "loss": 1.9322,
      "step": 50770
    },
    {
      "epoch": 25.829094608341812,
      "grad_norm": 35.28505325317383,
      "learning_rate": 2.4170905391658192e-05,
      "loss": 1.884,
      "step": 50780
    },
    {
      "epoch": 25.83418107833164,
      "grad_norm": 30.790699005126953,
      "learning_rate": 2.4165818921668362e-05,
      "loss": 1.8568,
      "step": 50790
    },
    {
      "epoch": 25.839267548321466,
      "grad_norm": 32.62311553955078,
      "learning_rate": 2.4160732451678535e-05,
      "loss": 1.8853,
      "step": 50800
    },
    {
      "epoch": 25.844354018311293,
      "grad_norm": 35.04296112060547,
      "learning_rate": 2.415564598168871e-05,
      "loss": 1.8304,
      "step": 50810
    },
    {
      "epoch": 25.84944048830112,
      "grad_norm": 51.653324127197266,
      "learning_rate": 2.415055951169888e-05,
      "loss": 1.9535,
      "step": 50820
    },
    {
      "epoch": 25.854526958290947,
      "grad_norm": 33.31044387817383,
      "learning_rate": 2.4145473041709055e-05,
      "loss": 1.919,
      "step": 50830
    },
    {
      "epoch": 25.859613428280774,
      "grad_norm": 35.86228942871094,
      "learning_rate": 2.4140386571719228e-05,
      "loss": 1.8985,
      "step": 50840
    },
    {
      "epoch": 25.8646998982706,
      "grad_norm": 34.69445037841797,
      "learning_rate": 2.41353001017294e-05,
      "loss": 1.8241,
      "step": 50850
    },
    {
      "epoch": 25.869786368260428,
      "grad_norm": 33.46048355102539,
      "learning_rate": 2.4130213631739575e-05,
      "loss": 1.9795,
      "step": 50860
    },
    {
      "epoch": 25.874872838250255,
      "grad_norm": 43.90825271606445,
      "learning_rate": 2.4125127161749748e-05,
      "loss": 1.8918,
      "step": 50870
    },
    {
      "epoch": 25.879959308240082,
      "grad_norm": 38.51898193359375,
      "learning_rate": 2.412004069175992e-05,
      "loss": 1.9657,
      "step": 50880
    },
    {
      "epoch": 25.88504577822991,
      "grad_norm": 41.20375442504883,
      "learning_rate": 2.411495422177009e-05,
      "loss": 1.9171,
      "step": 50890
    },
    {
      "epoch": 25.890132248219736,
      "grad_norm": 31.464841842651367,
      "learning_rate": 2.4109867751780265e-05,
      "loss": 2.0007,
      "step": 50900
    },
    {
      "epoch": 25.895218718209563,
      "grad_norm": 29.18320655822754,
      "learning_rate": 2.410478128179044e-05,
      "loss": 2.0349,
      "step": 50910
    },
    {
      "epoch": 25.90030518819939,
      "grad_norm": 33.149009704589844,
      "learning_rate": 2.409969481180061e-05,
      "loss": 1.9603,
      "step": 50920
    },
    {
      "epoch": 25.905391658189217,
      "grad_norm": 38.20452117919922,
      "learning_rate": 2.4094608341810784e-05,
      "loss": 1.8962,
      "step": 50930
    },
    {
      "epoch": 25.910478128179044,
      "grad_norm": 48.278404235839844,
      "learning_rate": 2.4089521871820958e-05,
      "loss": 1.9256,
      "step": 50940
    },
    {
      "epoch": 25.91556459816887,
      "grad_norm": 32.27830505371094,
      "learning_rate": 2.4084435401831128e-05,
      "loss": 1.921,
      "step": 50950
    },
    {
      "epoch": 25.920651068158698,
      "grad_norm": 41.46000289916992,
      "learning_rate": 2.4079348931841304e-05,
      "loss": 1.9266,
      "step": 50960
    },
    {
      "epoch": 25.925737538148525,
      "grad_norm": 44.01825714111328,
      "learning_rate": 2.4074262461851477e-05,
      "loss": 1.9152,
      "step": 50970
    },
    {
      "epoch": 25.93082400813835,
      "grad_norm": 44.02970886230469,
      "learning_rate": 2.406917599186165e-05,
      "loss": 1.8804,
      "step": 50980
    },
    {
      "epoch": 25.93591047812818,
      "grad_norm": 37.16411209106445,
      "learning_rate": 2.406408952187182e-05,
      "loss": 1.8506,
      "step": 50990
    },
    {
      "epoch": 25.940996948118006,
      "grad_norm": 32.000911712646484,
      "learning_rate": 2.4059003051881994e-05,
      "loss": 1.9401,
      "step": 51000
    },
    {
      "epoch": 25.946083418107833,
      "grad_norm": 38.49298095703125,
      "learning_rate": 2.405391658189217e-05,
      "loss": 2.038,
      "step": 51010
    },
    {
      "epoch": 25.95116988809766,
      "grad_norm": 29.459758758544922,
      "learning_rate": 2.404883011190234e-05,
      "loss": 1.9271,
      "step": 51020
    },
    {
      "epoch": 25.956256358087487,
      "grad_norm": 41.16456604003906,
      "learning_rate": 2.4043743641912514e-05,
      "loss": 1.946,
      "step": 51030
    },
    {
      "epoch": 25.961342828077314,
      "grad_norm": 57.04960632324219,
      "learning_rate": 2.4038657171922687e-05,
      "loss": 1.8621,
      "step": 51040
    },
    {
      "epoch": 25.96642929806714,
      "grad_norm": 37.69633483886719,
      "learning_rate": 2.403357070193286e-05,
      "loss": 1.8982,
      "step": 51050
    },
    {
      "epoch": 25.971515768056967,
      "grad_norm": 33.19734573364258,
      "learning_rate": 2.4028484231943034e-05,
      "loss": 1.9166,
      "step": 51060
    },
    {
      "epoch": 25.976602238046794,
      "grad_norm": 33.820316314697266,
      "learning_rate": 2.4023397761953207e-05,
      "loss": 1.9376,
      "step": 51070
    },
    {
      "epoch": 25.98168870803662,
      "grad_norm": 44.51713180541992,
      "learning_rate": 2.4018311291963377e-05,
      "loss": 1.8613,
      "step": 51080
    },
    {
      "epoch": 25.98677517802645,
      "grad_norm": 40.250858306884766,
      "learning_rate": 2.401322482197355e-05,
      "loss": 1.9301,
      "step": 51090
    },
    {
      "epoch": 25.991861648016275,
      "grad_norm": 37.026153564453125,
      "learning_rate": 2.4008138351983723e-05,
      "loss": 1.8798,
      "step": 51100
    },
    {
      "epoch": 25.996948118006102,
      "grad_norm": 32.846954345703125,
      "learning_rate": 2.40030518819939e-05,
      "loss": 1.8843,
      "step": 51110
    },
    {
      "epoch": 26.0,
      "eval_loss": 4.609475135803223,
      "eval_runtime": 2.617,
      "eval_samples_per_second": 1060.39,
      "eval_steps_per_second": 132.596,
      "step": 51116
    },
    {
      "epoch": 26.00203458799593,
      "grad_norm": 45.27024841308594,
      "learning_rate": 2.399796541200407e-05,
      "loss": 1.8564,
      "step": 51120
    },
    {
      "epoch": 26.007121057985756,
      "grad_norm": 34.53850555419922,
      "learning_rate": 2.3992878942014243e-05,
      "loss": 1.8466,
      "step": 51130
    },
    {
      "epoch": 26.012207527975583,
      "grad_norm": 35.44054412841797,
      "learning_rate": 2.3987792472024416e-05,
      "loss": 1.9582,
      "step": 51140
    },
    {
      "epoch": 26.01729399796541,
      "grad_norm": 34.4354133605957,
      "learning_rate": 2.398270600203459e-05,
      "loss": 1.9358,
      "step": 51150
    },
    {
      "epoch": 26.022380467955237,
      "grad_norm": 35.79426956176758,
      "learning_rate": 2.3977619532044763e-05,
      "loss": 1.8802,
      "step": 51160
    },
    {
      "epoch": 26.027466937945068,
      "grad_norm": 33.45478439331055,
      "learning_rate": 2.3972533062054936e-05,
      "loss": 1.8807,
      "step": 51170
    },
    {
      "epoch": 26.032553407934895,
      "grad_norm": 34.310733795166016,
      "learning_rate": 2.3967446592065106e-05,
      "loss": 1.845,
      "step": 51180
    },
    {
      "epoch": 26.037639877924722,
      "grad_norm": 35.429161071777344,
      "learning_rate": 2.396236012207528e-05,
      "loss": 1.9622,
      "step": 51190
    },
    {
      "epoch": 26.04272634791455,
      "grad_norm": 36.57674026489258,
      "learning_rate": 2.3957273652085456e-05,
      "loss": 1.9471,
      "step": 51200
    },
    {
      "epoch": 26.047812817904376,
      "grad_norm": 30.39580535888672,
      "learning_rate": 2.3952187182095626e-05,
      "loss": 1.8928,
      "step": 51210
    },
    {
      "epoch": 26.052899287894203,
      "grad_norm": 41.64149475097656,
      "learning_rate": 2.39471007121058e-05,
      "loss": 1.8975,
      "step": 51220
    },
    {
      "epoch": 26.05798575788403,
      "grad_norm": 42.50218963623047,
      "learning_rate": 2.3942014242115973e-05,
      "loss": 1.8648,
      "step": 51230
    },
    {
      "epoch": 26.063072227873857,
      "grad_norm": 26.54448890686035,
      "learning_rate": 2.3936927772126146e-05,
      "loss": 1.8653,
      "step": 51240
    },
    {
      "epoch": 26.068158697863684,
      "grad_norm": 41.34566116333008,
      "learning_rate": 2.393184130213632e-05,
      "loss": 1.908,
      "step": 51250
    },
    {
      "epoch": 26.07324516785351,
      "grad_norm": 40.161224365234375,
      "learning_rate": 2.3926754832146492e-05,
      "loss": 1.9325,
      "step": 51260
    },
    {
      "epoch": 26.078331637843338,
      "grad_norm": 44.21552658081055,
      "learning_rate": 2.3921668362156666e-05,
      "loss": 1.9591,
      "step": 51270
    },
    {
      "epoch": 26.083418107833165,
      "grad_norm": 37.37748718261719,
      "learning_rate": 2.3916581892166836e-05,
      "loss": 1.9636,
      "step": 51280
    },
    {
      "epoch": 26.08850457782299,
      "grad_norm": 34.761905670166016,
      "learning_rate": 2.391149542217701e-05,
      "loss": 1.9272,
      "step": 51290
    },
    {
      "epoch": 26.09359104781282,
      "grad_norm": 37.82987976074219,
      "learning_rate": 2.3906408952187186e-05,
      "loss": 1.925,
      "step": 51300
    },
    {
      "epoch": 26.098677517802646,
      "grad_norm": 36.37874984741211,
      "learning_rate": 2.3901322482197355e-05,
      "loss": 1.9204,
      "step": 51310
    },
    {
      "epoch": 26.103763987792473,
      "grad_norm": 44.213504791259766,
      "learning_rate": 2.389623601220753e-05,
      "loss": 1.8666,
      "step": 51320
    },
    {
      "epoch": 26.1088504577823,
      "grad_norm": 35.760780334472656,
      "learning_rate": 2.3891149542217702e-05,
      "loss": 1.8677,
      "step": 51330
    },
    {
      "epoch": 26.113936927772126,
      "grad_norm": 28.340856552124023,
      "learning_rate": 2.3886063072227875e-05,
      "loss": 2.0065,
      "step": 51340
    },
    {
      "epoch": 26.119023397761953,
      "grad_norm": 29.61988067626953,
      "learning_rate": 2.388097660223805e-05,
      "loss": 1.9243,
      "step": 51350
    },
    {
      "epoch": 26.12410986775178,
      "grad_norm": 41.61155319213867,
      "learning_rate": 2.3875890132248222e-05,
      "loss": 1.8539,
      "step": 51360
    },
    {
      "epoch": 26.129196337741607,
      "grad_norm": 31.250768661499023,
      "learning_rate": 2.3870803662258392e-05,
      "loss": 1.9102,
      "step": 51370
    },
    {
      "epoch": 26.134282807731434,
      "grad_norm": 35.966487884521484,
      "learning_rate": 2.3865717192268565e-05,
      "loss": 1.9404,
      "step": 51380
    },
    {
      "epoch": 26.13936927772126,
      "grad_norm": 38.06824493408203,
      "learning_rate": 2.3860630722278742e-05,
      "loss": 1.8103,
      "step": 51390
    },
    {
      "epoch": 26.14445574771109,
      "grad_norm": 36.44779968261719,
      "learning_rate": 2.3855544252288915e-05,
      "loss": 1.8916,
      "step": 51400
    },
    {
      "epoch": 26.149542217700915,
      "grad_norm": 29.327796936035156,
      "learning_rate": 2.3850457782299085e-05,
      "loss": 1.8864,
      "step": 51410
    },
    {
      "epoch": 26.154628687690742,
      "grad_norm": 41.05739974975586,
      "learning_rate": 2.3845371312309258e-05,
      "loss": 1.8658,
      "step": 51420
    },
    {
      "epoch": 26.15971515768057,
      "grad_norm": 38.057674407958984,
      "learning_rate": 2.384028484231943e-05,
      "loss": 1.8527,
      "step": 51430
    },
    {
      "epoch": 26.164801627670396,
      "grad_norm": 36.20002746582031,
      "learning_rate": 2.3835198372329605e-05,
      "loss": 1.8323,
      "step": 51440
    },
    {
      "epoch": 26.169888097660223,
      "grad_norm": 32.417972564697266,
      "learning_rate": 2.3830111902339778e-05,
      "loss": 1.8754,
      "step": 51450
    },
    {
      "epoch": 26.17497456765005,
      "grad_norm": 39.92974853515625,
      "learning_rate": 2.382502543234995e-05,
      "loss": 1.8181,
      "step": 51460
    },
    {
      "epoch": 26.180061037639877,
      "grad_norm": 45.03392028808594,
      "learning_rate": 2.381993896236012e-05,
      "loss": 1.9351,
      "step": 51470
    },
    {
      "epoch": 26.185147507629704,
      "grad_norm": 41.8353271484375,
      "learning_rate": 2.3814852492370295e-05,
      "loss": 1.7951,
      "step": 51480
    },
    {
      "epoch": 26.19023397761953,
      "grad_norm": 31.671463012695312,
      "learning_rate": 2.380976602238047e-05,
      "loss": 1.8567,
      "step": 51490
    },
    {
      "epoch": 26.195320447609358,
      "grad_norm": 30.048280715942383,
      "learning_rate": 2.380467955239064e-05,
      "loss": 1.8852,
      "step": 51500
    },
    {
      "epoch": 26.200406917599185,
      "grad_norm": 32.998207092285156,
      "learning_rate": 2.3799593082400814e-05,
      "loss": 1.8037,
      "step": 51510
    },
    {
      "epoch": 26.205493387589012,
      "grad_norm": 37.01240539550781,
      "learning_rate": 2.3794506612410988e-05,
      "loss": 1.8304,
      "step": 51520
    },
    {
      "epoch": 26.21057985757884,
      "grad_norm": 33.4874382019043,
      "learning_rate": 2.378942014242116e-05,
      "loss": 1.9942,
      "step": 51530
    },
    {
      "epoch": 26.215666327568666,
      "grad_norm": 51.96592330932617,
      "learning_rate": 2.3784333672431334e-05,
      "loss": 1.9332,
      "step": 51540
    },
    {
      "epoch": 26.220752797558493,
      "grad_norm": 42.715328216552734,
      "learning_rate": 2.3779247202441507e-05,
      "loss": 1.814,
      "step": 51550
    },
    {
      "epoch": 26.22583926754832,
      "grad_norm": 34.233699798583984,
      "learning_rate": 2.377416073245168e-05,
      "loss": 1.9216,
      "step": 51560
    },
    {
      "epoch": 26.230925737538147,
      "grad_norm": 32.08708953857422,
      "learning_rate": 2.376907426246185e-05,
      "loss": 1.8096,
      "step": 51570
    },
    {
      "epoch": 26.236012207527974,
      "grad_norm": 37.00834274291992,
      "learning_rate": 2.3763987792472024e-05,
      "loss": 1.8453,
      "step": 51580
    },
    {
      "epoch": 26.2410986775178,
      "grad_norm": 47.21843338012695,
      "learning_rate": 2.37589013224822e-05,
      "loss": 1.9482,
      "step": 51590
    },
    {
      "epoch": 26.246185147507628,
      "grad_norm": 36.75825500488281,
      "learning_rate": 2.375381485249237e-05,
      "loss": 1.8139,
      "step": 51600
    },
    {
      "epoch": 26.25127161749746,
      "grad_norm": 37.190162658691406,
      "learning_rate": 2.3748728382502544e-05,
      "loss": 1.8272,
      "step": 51610
    },
    {
      "epoch": 26.256358087487286,
      "grad_norm": 40.54446029663086,
      "learning_rate": 2.3743641912512717e-05,
      "loss": 1.9203,
      "step": 51620
    },
    {
      "epoch": 26.261444557477112,
      "grad_norm": 40.659332275390625,
      "learning_rate": 2.373855544252289e-05,
      "loss": 1.8963,
      "step": 51630
    },
    {
      "epoch": 26.26653102746694,
      "grad_norm": 34.14765548706055,
      "learning_rate": 2.3733468972533064e-05,
      "loss": 1.9535,
      "step": 51640
    },
    {
      "epoch": 26.271617497456766,
      "grad_norm": 36.00255584716797,
      "learning_rate": 2.3728382502543237e-05,
      "loss": 1.9277,
      "step": 51650
    },
    {
      "epoch": 26.276703967446593,
      "grad_norm": 37.35713577270508,
      "learning_rate": 2.372329603255341e-05,
      "loss": 1.8347,
      "step": 51660
    },
    {
      "epoch": 26.28179043743642,
      "grad_norm": 38.14093780517578,
      "learning_rate": 2.371820956256358e-05,
      "loss": 1.8243,
      "step": 51670
    },
    {
      "epoch": 26.286876907426247,
      "grad_norm": 37.37928771972656,
      "learning_rate": 2.3713123092573757e-05,
      "loss": 1.754,
      "step": 51680
    },
    {
      "epoch": 26.291963377416074,
      "grad_norm": 33.740478515625,
      "learning_rate": 2.370803662258393e-05,
      "loss": 1.8746,
      "step": 51690
    },
    {
      "epoch": 26.2970498474059,
      "grad_norm": 37.8765983581543,
      "learning_rate": 2.37029501525941e-05,
      "loss": 1.8949,
      "step": 51700
    },
    {
      "epoch": 26.30213631739573,
      "grad_norm": 36.405616760253906,
      "learning_rate": 2.3697863682604273e-05,
      "loss": 1.8352,
      "step": 51710
    },
    {
      "epoch": 26.307222787385555,
      "grad_norm": 29.997283935546875,
      "learning_rate": 2.3692777212614446e-05,
      "loss": 1.8385,
      "step": 51720
    },
    {
      "epoch": 26.312309257375382,
      "grad_norm": 39.419273376464844,
      "learning_rate": 2.368769074262462e-05,
      "loss": 1.9389,
      "step": 51730
    },
    {
      "epoch": 26.31739572736521,
      "grad_norm": 35.09614181518555,
      "learning_rate": 2.3682604272634793e-05,
      "loss": 1.9115,
      "step": 51740
    },
    {
      "epoch": 26.322482197355036,
      "grad_norm": 34.10386276245117,
      "learning_rate": 2.3677517802644966e-05,
      "loss": 1.8448,
      "step": 51750
    },
    {
      "epoch": 26.327568667344863,
      "grad_norm": 39.33746337890625,
      "learning_rate": 2.3672431332655136e-05,
      "loss": 1.8878,
      "step": 51760
    },
    {
      "epoch": 26.33265513733469,
      "grad_norm": 40.84610366821289,
      "learning_rate": 2.366734486266531e-05,
      "loss": 1.9036,
      "step": 51770
    },
    {
      "epoch": 26.337741607324517,
      "grad_norm": 44.03322982788086,
      "learning_rate": 2.3662258392675486e-05,
      "loss": 1.911,
      "step": 51780
    },
    {
      "epoch": 26.342828077314344,
      "grad_norm": 33.1276741027832,
      "learning_rate": 2.365717192268566e-05,
      "loss": 1.8757,
      "step": 51790
    },
    {
      "epoch": 26.34791454730417,
      "grad_norm": 59.06878662109375,
      "learning_rate": 2.365208545269583e-05,
      "loss": 1.812,
      "step": 51800
    },
    {
      "epoch": 26.353001017293998,
      "grad_norm": 34.37960433959961,
      "learning_rate": 2.3646998982706003e-05,
      "loss": 1.9262,
      "step": 51810
    },
    {
      "epoch": 26.358087487283825,
      "grad_norm": 41.274051666259766,
      "learning_rate": 2.3641912512716176e-05,
      "loss": 1.8752,
      "step": 51820
    },
    {
      "epoch": 26.363173957273652,
      "grad_norm": 36.30455780029297,
      "learning_rate": 2.363682604272635e-05,
      "loss": 1.8138,
      "step": 51830
    },
    {
      "epoch": 26.36826042726348,
      "grad_norm": 40.017311096191406,
      "learning_rate": 2.3631739572736522e-05,
      "loss": 1.8025,
      "step": 51840
    },
    {
      "epoch": 26.373346897253306,
      "grad_norm": 38.721954345703125,
      "learning_rate": 2.3626653102746696e-05,
      "loss": 1.9225,
      "step": 51850
    },
    {
      "epoch": 26.378433367243133,
      "grad_norm": 38.269527435302734,
      "learning_rate": 2.3621566632756866e-05,
      "loss": 1.8981,
      "step": 51860
    },
    {
      "epoch": 26.38351983723296,
      "grad_norm": 36.9765739440918,
      "learning_rate": 2.3616480162767042e-05,
      "loss": 1.9392,
      "step": 51870
    },
    {
      "epoch": 26.388606307222787,
      "grad_norm": 35.53730010986328,
      "learning_rate": 2.3611393692777216e-05,
      "loss": 1.907,
      "step": 51880
    },
    {
      "epoch": 26.393692777212614,
      "grad_norm": 36.25215530395508,
      "learning_rate": 2.3606307222787386e-05,
      "loss": 1.8859,
      "step": 51890
    },
    {
      "epoch": 26.39877924720244,
      "grad_norm": 38.351558685302734,
      "learning_rate": 2.360122075279756e-05,
      "loss": 1.8329,
      "step": 51900
    },
    {
      "epoch": 26.403865717192268,
      "grad_norm": 44.845176696777344,
      "learning_rate": 2.3596134282807732e-05,
      "loss": 1.8077,
      "step": 51910
    },
    {
      "epoch": 26.408952187182095,
      "grad_norm": 36.83845138549805,
      "learning_rate": 2.3591047812817905e-05,
      "loss": 1.9074,
      "step": 51920
    },
    {
      "epoch": 26.414038657171922,
      "grad_norm": 38.61442565917969,
      "learning_rate": 2.358596134282808e-05,
      "loss": 1.8203,
      "step": 51930
    },
    {
      "epoch": 26.41912512716175,
      "grad_norm": 33.82979202270508,
      "learning_rate": 2.3580874872838252e-05,
      "loss": 1.8569,
      "step": 51940
    },
    {
      "epoch": 26.424211597151576,
      "grad_norm": 44.47465515136719,
      "learning_rate": 2.3575788402848425e-05,
      "loss": 1.9103,
      "step": 51950
    },
    {
      "epoch": 26.429298067141403,
      "grad_norm": 35.41310501098633,
      "learning_rate": 2.3570701932858595e-05,
      "loss": 1.8852,
      "step": 51960
    },
    {
      "epoch": 26.43438453713123,
      "grad_norm": 34.1987190246582,
      "learning_rate": 2.3565615462868772e-05,
      "loss": 1.8826,
      "step": 51970
    },
    {
      "epoch": 26.439471007121057,
      "grad_norm": 37.20155334472656,
      "learning_rate": 2.3560528992878945e-05,
      "loss": 1.9157,
      "step": 51980
    },
    {
      "epoch": 26.444557477110884,
      "grad_norm": 36.820762634277344,
      "learning_rate": 2.3555442522889115e-05,
      "loss": 1.8651,
      "step": 51990
    },
    {
      "epoch": 26.44964394710071,
      "grad_norm": 38.734474182128906,
      "learning_rate": 2.3550356052899288e-05,
      "loss": 1.9003,
      "step": 52000
    },
    {
      "epoch": 26.454730417090538,
      "grad_norm": 40.11432647705078,
      "learning_rate": 2.354526958290946e-05,
      "loss": 1.8758,
      "step": 52010
    },
    {
      "epoch": 26.459816887080365,
      "grad_norm": 35.69856262207031,
      "learning_rate": 2.3540183112919635e-05,
      "loss": 1.8643,
      "step": 52020
    },
    {
      "epoch": 26.46490335707019,
      "grad_norm": 34.57836151123047,
      "learning_rate": 2.3535096642929808e-05,
      "loss": 1.8757,
      "step": 52030
    },
    {
      "epoch": 26.46998982706002,
      "grad_norm": 39.61623001098633,
      "learning_rate": 2.353001017293998e-05,
      "loss": 2.0139,
      "step": 52040
    },
    {
      "epoch": 26.475076297049846,
      "grad_norm": 44.020965576171875,
      "learning_rate": 2.352492370295015e-05,
      "loss": 1.9602,
      "step": 52050
    },
    {
      "epoch": 26.480162767039676,
      "grad_norm": 32.51112365722656,
      "learning_rate": 2.3519837232960325e-05,
      "loss": 1.8336,
      "step": 52060
    },
    {
      "epoch": 26.485249237029503,
      "grad_norm": 36.226524353027344,
      "learning_rate": 2.35147507629705e-05,
      "loss": 1.8745,
      "step": 52070
    },
    {
      "epoch": 26.49033570701933,
      "grad_norm": 37.29295349121094,
      "learning_rate": 2.3509664292980674e-05,
      "loss": 1.9061,
      "step": 52080
    },
    {
      "epoch": 26.495422177009157,
      "grad_norm": 34.02679443359375,
      "learning_rate": 2.3504577822990844e-05,
      "loss": 1.8616,
      "step": 52090
    },
    {
      "epoch": 26.500508646998984,
      "grad_norm": 34.910987854003906,
      "learning_rate": 2.3499491353001018e-05,
      "loss": 1.8128,
      "step": 52100
    },
    {
      "epoch": 26.50559511698881,
      "grad_norm": 38.171810150146484,
      "learning_rate": 2.349440488301119e-05,
      "loss": 1.8645,
      "step": 52110
    },
    {
      "epoch": 26.510681586978638,
      "grad_norm": 37.70338821411133,
      "learning_rate": 2.3489318413021364e-05,
      "loss": 1.8309,
      "step": 52120
    },
    {
      "epoch": 26.515768056968465,
      "grad_norm": 36.99320983886719,
      "learning_rate": 2.3484231943031537e-05,
      "loss": 1.8928,
      "step": 52130
    },
    {
      "epoch": 26.520854526958292,
      "grad_norm": 39.446903228759766,
      "learning_rate": 2.347914547304171e-05,
      "loss": 1.9098,
      "step": 52140
    },
    {
      "epoch": 26.52594099694812,
      "grad_norm": 29.939037322998047,
      "learning_rate": 2.347405900305188e-05,
      "loss": 1.8683,
      "step": 52150
    },
    {
      "epoch": 26.531027466937946,
      "grad_norm": 36.5294189453125,
      "learning_rate": 2.3468972533062057e-05,
      "loss": 1.8391,
      "step": 52160
    },
    {
      "epoch": 26.536113936927773,
      "grad_norm": 40.50370788574219,
      "learning_rate": 2.346388606307223e-05,
      "loss": 1.9235,
      "step": 52170
    },
    {
      "epoch": 26.5412004069176,
      "grad_norm": 33.92979431152344,
      "learning_rate": 2.34587995930824e-05,
      "loss": 1.8954,
      "step": 52180
    },
    {
      "epoch": 26.546286876907427,
      "grad_norm": 28.899192810058594,
      "learning_rate": 2.3453713123092574e-05,
      "loss": 1.9806,
      "step": 52190
    },
    {
      "epoch": 26.551373346897254,
      "grad_norm": 42.553401947021484,
      "learning_rate": 2.3448626653102747e-05,
      "loss": 1.8243,
      "step": 52200
    },
    {
      "epoch": 26.55645981688708,
      "grad_norm": 42.739288330078125,
      "learning_rate": 2.344354018311292e-05,
      "loss": 1.971,
      "step": 52210
    },
    {
      "epoch": 26.561546286876908,
      "grad_norm": 36.1782341003418,
      "learning_rate": 2.3438453713123094e-05,
      "loss": 1.9347,
      "step": 52220
    },
    {
      "epoch": 26.566632756866735,
      "grad_norm": 37.651546478271484,
      "learning_rate": 2.3433367243133267e-05,
      "loss": 1.8842,
      "step": 52230
    },
    {
      "epoch": 26.571719226856562,
      "grad_norm": 35.3164176940918,
      "learning_rate": 2.342828077314344e-05,
      "loss": 1.9255,
      "step": 52240
    },
    {
      "epoch": 26.57680569684639,
      "grad_norm": 46.089622497558594,
      "learning_rate": 2.342319430315361e-05,
      "loss": 1.9316,
      "step": 52250
    },
    {
      "epoch": 26.581892166836216,
      "grad_norm": 41.223575592041016,
      "learning_rate": 2.3418107833163787e-05,
      "loss": 1.9484,
      "step": 52260
    },
    {
      "epoch": 26.586978636826043,
      "grad_norm": 33.186805725097656,
      "learning_rate": 2.341302136317396e-05,
      "loss": 1.8809,
      "step": 52270
    },
    {
      "epoch": 26.59206510681587,
      "grad_norm": 33.497406005859375,
      "learning_rate": 2.340793489318413e-05,
      "loss": 1.8969,
      "step": 52280
    },
    {
      "epoch": 26.597151576805697,
      "grad_norm": 36.45295715332031,
      "learning_rate": 2.3402848423194303e-05,
      "loss": 1.8817,
      "step": 52290
    },
    {
      "epoch": 26.602238046795524,
      "grad_norm": 44.10691452026367,
      "learning_rate": 2.3397761953204476e-05,
      "loss": 1.8694,
      "step": 52300
    },
    {
      "epoch": 26.60732451678535,
      "grad_norm": 33.86290740966797,
      "learning_rate": 2.339267548321465e-05,
      "loss": 1.8313,
      "step": 52310
    },
    {
      "epoch": 26.612410986775178,
      "grad_norm": 43.19839859008789,
      "learning_rate": 2.3387589013224823e-05,
      "loss": 1.9649,
      "step": 52320
    },
    {
      "epoch": 26.617497456765005,
      "grad_norm": 37.133750915527344,
      "learning_rate": 2.3382502543234996e-05,
      "loss": 1.8262,
      "step": 52330
    },
    {
      "epoch": 26.62258392675483,
      "grad_norm": 54.80152130126953,
      "learning_rate": 2.337741607324517e-05,
      "loss": 1.9955,
      "step": 52340
    },
    {
      "epoch": 26.62767039674466,
      "grad_norm": 34.72041320800781,
      "learning_rate": 2.3372329603255343e-05,
      "loss": 1.8714,
      "step": 52350
    },
    {
      "epoch": 26.632756866734486,
      "grad_norm": 38.08281326293945,
      "learning_rate": 2.3367243133265516e-05,
      "loss": 1.9155,
      "step": 52360
    },
    {
      "epoch": 26.637843336724313,
      "grad_norm": 40.50242233276367,
      "learning_rate": 2.336215666327569e-05,
      "loss": 1.8581,
      "step": 52370
    },
    {
      "epoch": 26.64292980671414,
      "grad_norm": 37.85396194458008,
      "learning_rate": 2.335707019328586e-05,
      "loss": 1.8276,
      "step": 52380
    },
    {
      "epoch": 26.648016276703967,
      "grad_norm": 31.770936965942383,
      "learning_rate": 2.3351983723296033e-05,
      "loss": 1.877,
      "step": 52390
    },
    {
      "epoch": 26.653102746693794,
      "grad_norm": 33.542964935302734,
      "learning_rate": 2.3346897253306206e-05,
      "loss": 1.8539,
      "step": 52400
    },
    {
      "epoch": 26.65818921668362,
      "grad_norm": 34.546142578125,
      "learning_rate": 2.334181078331638e-05,
      "loss": 1.8437,
      "step": 52410
    },
    {
      "epoch": 26.663275686673447,
      "grad_norm": 43.94187545776367,
      "learning_rate": 2.3336724313326552e-05,
      "loss": 1.942,
      "step": 52420
    },
    {
      "epoch": 26.668362156663274,
      "grad_norm": 32.49008560180664,
      "learning_rate": 2.3331637843336726e-05,
      "loss": 1.8917,
      "step": 52430
    },
    {
      "epoch": 26.6734486266531,
      "grad_norm": 37.46620559692383,
      "learning_rate": 2.3326551373346896e-05,
      "loss": 1.8412,
      "step": 52440
    },
    {
      "epoch": 26.67853509664293,
      "grad_norm": 35.48727798461914,
      "learning_rate": 2.3321464903357072e-05,
      "loss": 1.8667,
      "step": 52450
    },
    {
      "epoch": 26.683621566632755,
      "grad_norm": 41.14035415649414,
      "learning_rate": 2.3316378433367246e-05,
      "loss": 1.9402,
      "step": 52460
    },
    {
      "epoch": 26.688708036622582,
      "grad_norm": 42.170169830322266,
      "learning_rate": 2.331129196337742e-05,
      "loss": 1.8339,
      "step": 52470
    },
    {
      "epoch": 26.69379450661241,
      "grad_norm": 32.1453971862793,
      "learning_rate": 2.330620549338759e-05,
      "loss": 1.8704,
      "step": 52480
    },
    {
      "epoch": 26.698880976602236,
      "grad_norm": 43.653228759765625,
      "learning_rate": 2.3301119023397762e-05,
      "loss": 1.8473,
      "step": 52490
    },
    {
      "epoch": 26.703967446592067,
      "grad_norm": 44.17331314086914,
      "learning_rate": 2.329603255340794e-05,
      "loss": 1.8622,
      "step": 52500
    },
    {
      "epoch": 26.709053916581894,
      "grad_norm": 34.71834182739258,
      "learning_rate": 2.329094608341811e-05,
      "loss": 1.8823,
      "step": 52510
    },
    {
      "epoch": 26.71414038657172,
      "grad_norm": 33.2998161315918,
      "learning_rate": 2.3285859613428282e-05,
      "loss": 1.8873,
      "step": 52520
    },
    {
      "epoch": 26.719226856561548,
      "grad_norm": 30.666242599487305,
      "learning_rate": 2.3280773143438455e-05,
      "loss": 1.9632,
      "step": 52530
    },
    {
      "epoch": 26.724313326551375,
      "grad_norm": 44.435462951660156,
      "learning_rate": 2.3275686673448625e-05,
      "loss": 1.8546,
      "step": 52540
    },
    {
      "epoch": 26.729399796541202,
      "grad_norm": 41.71316909790039,
      "learning_rate": 2.3270600203458802e-05,
      "loss": 1.921,
      "step": 52550
    },
    {
      "epoch": 26.73448626653103,
      "grad_norm": 42.78788757324219,
      "learning_rate": 2.3265513733468975e-05,
      "loss": 1.8504,
      "step": 52560
    },
    {
      "epoch": 26.739572736520856,
      "grad_norm": 34.78504180908203,
      "learning_rate": 2.3260427263479145e-05,
      "loss": 1.8457,
      "step": 52570
    },
    {
      "epoch": 26.744659206510683,
      "grad_norm": 41.116939544677734,
      "learning_rate": 2.3255340793489318e-05,
      "loss": 1.9243,
      "step": 52580
    },
    {
      "epoch": 26.74974567650051,
      "grad_norm": 40.61810302734375,
      "learning_rate": 2.325025432349949e-05,
      "loss": 1.7592,
      "step": 52590
    },
    {
      "epoch": 26.754832146490337,
      "grad_norm": 43.32035446166992,
      "learning_rate": 2.3245167853509668e-05,
      "loss": 1.9517,
      "step": 52600
    },
    {
      "epoch": 26.759918616480164,
      "grad_norm": 44.48522186279297,
      "learning_rate": 2.3240081383519838e-05,
      "loss": 1.8527,
      "step": 52610
    },
    {
      "epoch": 26.76500508646999,
      "grad_norm": 40.91933822631836,
      "learning_rate": 2.323499491353001e-05,
      "loss": 1.7911,
      "step": 52620
    },
    {
      "epoch": 26.770091556459818,
      "grad_norm": 38.127525329589844,
      "learning_rate": 2.3229908443540185e-05,
      "loss": 1.8446,
      "step": 52630
    },
    {
      "epoch": 26.775178026449645,
      "grad_norm": 31.79191780090332,
      "learning_rate": 2.3224821973550358e-05,
      "loss": 1.9679,
      "step": 52640
    },
    {
      "epoch": 26.78026449643947,
      "grad_norm": 33.93827819824219,
      "learning_rate": 2.321973550356053e-05,
      "loss": 1.7978,
      "step": 52650
    },
    {
      "epoch": 26.7853509664293,
      "grad_norm": 37.78143310546875,
      "learning_rate": 2.3214649033570704e-05,
      "loss": 1.9396,
      "step": 52660
    },
    {
      "epoch": 26.790437436419126,
      "grad_norm": 40.82326126098633,
      "learning_rate": 2.3209562563580874e-05,
      "loss": 1.9678,
      "step": 52670
    },
    {
      "epoch": 26.795523906408953,
      "grad_norm": 46.29830551147461,
      "learning_rate": 2.3204476093591048e-05,
      "loss": 1.8523,
      "step": 52680
    },
    {
      "epoch": 26.80061037639878,
      "grad_norm": 41.868080139160156,
      "learning_rate": 2.319938962360122e-05,
      "loss": 1.9099,
      "step": 52690
    },
    {
      "epoch": 26.805696846388607,
      "grad_norm": 31.5700740814209,
      "learning_rate": 2.3194303153611394e-05,
      "loss": 1.8421,
      "step": 52700
    },
    {
      "epoch": 26.810783316378433,
      "grad_norm": 34.16740417480469,
      "learning_rate": 2.3189216683621567e-05,
      "loss": 1.9066,
      "step": 52710
    },
    {
      "epoch": 26.81586978636826,
      "grad_norm": 26.19293975830078,
      "learning_rate": 2.318413021363174e-05,
      "loss": 1.8868,
      "step": 52720
    },
    {
      "epoch": 26.820956256358087,
      "grad_norm": 43.195228576660156,
      "learning_rate": 2.3179043743641914e-05,
      "loss": 1.9662,
      "step": 52730
    },
    {
      "epoch": 26.826042726347914,
      "grad_norm": 34.84523010253906,
      "learning_rate": 2.3173957273652087e-05,
      "loss": 1.8651,
      "step": 52740
    },
    {
      "epoch": 26.83112919633774,
      "grad_norm": 39.941959381103516,
      "learning_rate": 2.316887080366226e-05,
      "loss": 1.8411,
      "step": 52750
    },
    {
      "epoch": 26.83621566632757,
      "grad_norm": 45.30937957763672,
      "learning_rate": 2.3163784333672434e-05,
      "loss": 1.9334,
      "step": 52760
    },
    {
      "epoch": 26.841302136317395,
      "grad_norm": 32.388397216796875,
      "learning_rate": 2.3158697863682604e-05,
      "loss": 1.8568,
      "step": 52770
    },
    {
      "epoch": 26.846388606307222,
      "grad_norm": 39.853858947753906,
      "learning_rate": 2.3153611393692777e-05,
      "loss": 1.9612,
      "step": 52780
    },
    {
      "epoch": 26.85147507629705,
      "grad_norm": 35.00591278076172,
      "learning_rate": 2.3148524923702954e-05,
      "loss": 1.9391,
      "step": 52790
    },
    {
      "epoch": 26.856561546286876,
      "grad_norm": 40.113861083984375,
      "learning_rate": 2.3143438453713124e-05,
      "loss": 1.875,
      "step": 52800
    },
    {
      "epoch": 26.861648016276703,
      "grad_norm": 29.55021858215332,
      "learning_rate": 2.3138351983723297e-05,
      "loss": 1.8335,
      "step": 52810
    },
    {
      "epoch": 26.86673448626653,
      "grad_norm": 46.6290168762207,
      "learning_rate": 2.313326551373347e-05,
      "loss": 1.8445,
      "step": 52820
    },
    {
      "epoch": 26.871820956256357,
      "grad_norm": 41.5390739440918,
      "learning_rate": 2.3128179043743643e-05,
      "loss": 1.7776,
      "step": 52830
    },
    {
      "epoch": 26.876907426246184,
      "grad_norm": 34.00804901123047,
      "learning_rate": 2.3123092573753817e-05,
      "loss": 1.9313,
      "step": 52840
    },
    {
      "epoch": 26.88199389623601,
      "grad_norm": 37.05937576293945,
      "learning_rate": 2.311800610376399e-05,
      "loss": 1.8863,
      "step": 52850
    },
    {
      "epoch": 26.887080366225838,
      "grad_norm": 35.964332580566406,
      "learning_rate": 2.311291963377416e-05,
      "loss": 1.8636,
      "step": 52860
    },
    {
      "epoch": 26.892166836215665,
      "grad_norm": 34.846073150634766,
      "learning_rate": 2.3107833163784333e-05,
      "loss": 1.8411,
      "step": 52870
    },
    {
      "epoch": 26.897253306205492,
      "grad_norm": 45.159061431884766,
      "learning_rate": 2.3102746693794507e-05,
      "loss": 1.8807,
      "step": 52880
    },
    {
      "epoch": 26.90233977619532,
      "grad_norm": 40.7502326965332,
      "learning_rate": 2.3097660223804683e-05,
      "loss": 1.8092,
      "step": 52890
    },
    {
      "epoch": 26.907426246185146,
      "grad_norm": 40.41880798339844,
      "learning_rate": 2.3092573753814853e-05,
      "loss": 1.847,
      "step": 52900
    },
    {
      "epoch": 26.912512716174973,
      "grad_norm": 32.59543228149414,
      "learning_rate": 2.3087487283825026e-05,
      "loss": 1.8234,
      "step": 52910
    },
    {
      "epoch": 26.9175991861648,
      "grad_norm": 35.6192626953125,
      "learning_rate": 2.30824008138352e-05,
      "loss": 1.9337,
      "step": 52920
    },
    {
      "epoch": 26.922685656154627,
      "grad_norm": 37.75228500366211,
      "learning_rate": 2.3077314343845373e-05,
      "loss": 1.8416,
      "step": 52930
    },
    {
      "epoch": 26.927772126144454,
      "grad_norm": 38.413944244384766,
      "learning_rate": 2.3072227873855546e-05,
      "loss": 1.8827,
      "step": 52940
    },
    {
      "epoch": 26.93285859613428,
      "grad_norm": 50.511627197265625,
      "learning_rate": 2.306714140386572e-05,
      "loss": 1.8655,
      "step": 52950
    },
    {
      "epoch": 26.93794506612411,
      "grad_norm": 36.370399475097656,
      "learning_rate": 2.306205493387589e-05,
      "loss": 1.7927,
      "step": 52960
    },
    {
      "epoch": 26.94303153611394,
      "grad_norm": 36.97682571411133,
      "learning_rate": 2.3056968463886063e-05,
      "loss": 1.9139,
      "step": 52970
    },
    {
      "epoch": 26.948118006103766,
      "grad_norm": 34.731658935546875,
      "learning_rate": 2.305188199389624e-05,
      "loss": 1.7656,
      "step": 52980
    },
    {
      "epoch": 26.953204476093592,
      "grad_norm": 31.876127243041992,
      "learning_rate": 2.304679552390641e-05,
      "loss": 1.8442,
      "step": 52990
    },
    {
      "epoch": 26.95829094608342,
      "grad_norm": 36.58913040161133,
      "learning_rate": 2.3041709053916582e-05,
      "loss": 1.93,
      "step": 53000
    },
    {
      "epoch": 26.963377416073246,
      "grad_norm": 38.7761344909668,
      "learning_rate": 2.3036622583926756e-05,
      "loss": 1.8545,
      "step": 53010
    },
    {
      "epoch": 26.968463886063073,
      "grad_norm": 49.4498176574707,
      "learning_rate": 2.303153611393693e-05,
      "loss": 1.7797,
      "step": 53020
    },
    {
      "epoch": 26.9735503560529,
      "grad_norm": 29.182994842529297,
      "learning_rate": 2.3026449643947102e-05,
      "loss": 1.864,
      "step": 53030
    },
    {
      "epoch": 26.978636826042727,
      "grad_norm": 27.274688720703125,
      "learning_rate": 2.3021363173957276e-05,
      "loss": 1.8685,
      "step": 53040
    },
    {
      "epoch": 26.983723296032554,
      "grad_norm": 32.91615295410156,
      "learning_rate": 2.301627670396745e-05,
      "loss": 1.8287,
      "step": 53050
    },
    {
      "epoch": 26.98880976602238,
      "grad_norm": 44.225093841552734,
      "learning_rate": 2.301119023397762e-05,
      "loss": 1.9022,
      "step": 53060
    },
    {
      "epoch": 26.99389623601221,
      "grad_norm": 37.01106643676758,
      "learning_rate": 2.3006103763987792e-05,
      "loss": 1.9567,
      "step": 53070
    },
    {
      "epoch": 26.998982706002035,
      "grad_norm": 41.246055603027344,
      "learning_rate": 2.300101729399797e-05,
      "loss": 1.9701,
      "step": 53080
    },
    {
      "epoch": 27.0,
      "eval_loss": 4.660092353820801,
      "eval_runtime": 2.6544,
      "eval_samples_per_second": 1045.438,
      "eval_steps_per_second": 130.727,
      "step": 53082
    },
    {
      "epoch": 27.004069175991862,
      "grad_norm": 39.9490966796875,
      "learning_rate": 2.299593082400814e-05,
      "loss": 1.8245,
      "step": 53090
    },
    {
      "epoch": 27.00915564598169,
      "grad_norm": 42.09188461303711,
      "learning_rate": 2.2990844354018312e-05,
      "loss": 1.7801,
      "step": 53100
    },
    {
      "epoch": 27.014242115971516,
      "grad_norm": 39.06148147583008,
      "learning_rate": 2.2985757884028485e-05,
      "loss": 1.8343,
      "step": 53110
    },
    {
      "epoch": 27.019328585961343,
      "grad_norm": 52.955055236816406,
      "learning_rate": 2.298067141403866e-05,
      "loss": 1.7983,
      "step": 53120
    },
    {
      "epoch": 27.02441505595117,
      "grad_norm": 32.225303649902344,
      "learning_rate": 2.2975584944048832e-05,
      "loss": 1.8589,
      "step": 53130
    },
    {
      "epoch": 27.029501525940997,
      "grad_norm": 42.5329704284668,
      "learning_rate": 2.2970498474059005e-05,
      "loss": 1.8567,
      "step": 53140
    },
    {
      "epoch": 27.034587995930824,
      "grad_norm": 31.814315795898438,
      "learning_rate": 2.296541200406918e-05,
      "loss": 1.8636,
      "step": 53150
    },
    {
      "epoch": 27.03967446592065,
      "grad_norm": 34.27843475341797,
      "learning_rate": 2.2960325534079348e-05,
      "loss": 1.8091,
      "step": 53160
    },
    {
      "epoch": 27.044760935910478,
      "grad_norm": 35.86032485961914,
      "learning_rate": 2.295523906408952e-05,
      "loss": 1.9387,
      "step": 53170
    },
    {
      "epoch": 27.049847405900305,
      "grad_norm": 36.89921569824219,
      "learning_rate": 2.2950152594099698e-05,
      "loss": 1.7403,
      "step": 53180
    },
    {
      "epoch": 27.054933875890132,
      "grad_norm": 34.41679000854492,
      "learning_rate": 2.2945066124109868e-05,
      "loss": 1.8045,
      "step": 53190
    },
    {
      "epoch": 27.06002034587996,
      "grad_norm": 33.61936569213867,
      "learning_rate": 2.293997965412004e-05,
      "loss": 1.876,
      "step": 53200
    },
    {
      "epoch": 27.065106815869786,
      "grad_norm": 40.167457580566406,
      "learning_rate": 2.2934893184130215e-05,
      "loss": 1.7999,
      "step": 53210
    },
    {
      "epoch": 27.070193285859613,
      "grad_norm": 31.27675437927246,
      "learning_rate": 2.2929806714140388e-05,
      "loss": 1.8644,
      "step": 53220
    },
    {
      "epoch": 27.07527975584944,
      "grad_norm": 29.710969924926758,
      "learning_rate": 2.292472024415056e-05,
      "loss": 1.8665,
      "step": 53230
    },
    {
      "epoch": 27.080366225839267,
      "grad_norm": 35.85112380981445,
      "learning_rate": 2.2919633774160734e-05,
      "loss": 1.8697,
      "step": 53240
    },
    {
      "epoch": 27.085452695829094,
      "grad_norm": 36.583152770996094,
      "learning_rate": 2.2914547304170904e-05,
      "loss": 1.8231,
      "step": 53250
    },
    {
      "epoch": 27.09053916581892,
      "grad_norm": 39.43360900878906,
      "learning_rate": 2.2909460834181078e-05,
      "loss": 1.7576,
      "step": 53260
    },
    {
      "epoch": 27.095625635808748,
      "grad_norm": 35.60535430908203,
      "learning_rate": 2.2904374364191254e-05,
      "loss": 1.7994,
      "step": 53270
    },
    {
      "epoch": 27.100712105798575,
      "grad_norm": 35.473819732666016,
      "learning_rate": 2.2899287894201428e-05,
      "loss": 1.8532,
      "step": 53280
    },
    {
      "epoch": 27.105798575788402,
      "grad_norm": 39.25538635253906,
      "learning_rate": 2.2894201424211598e-05,
      "loss": 1.8886,
      "step": 53290
    },
    {
      "epoch": 27.11088504577823,
      "grad_norm": 31.73212242126465,
      "learning_rate": 2.288911495422177e-05,
      "loss": 1.8873,
      "step": 53300
    },
    {
      "epoch": 27.115971515768056,
      "grad_norm": 36.6458740234375,
      "learning_rate": 2.2884028484231944e-05,
      "loss": 1.92,
      "step": 53310
    },
    {
      "epoch": 27.121057985757883,
      "grad_norm": 33.26617431640625,
      "learning_rate": 2.2878942014242117e-05,
      "loss": 1.8488,
      "step": 53320
    },
    {
      "epoch": 27.12614445574771,
      "grad_norm": 44.33732604980469,
      "learning_rate": 2.287385554425229e-05,
      "loss": 1.8004,
      "step": 53330
    },
    {
      "epoch": 27.131230925737537,
      "grad_norm": 41.95560073852539,
      "learning_rate": 2.2868769074262464e-05,
      "loss": 1.8155,
      "step": 53340
    },
    {
      "epoch": 27.136317395727364,
      "grad_norm": 37.004478454589844,
      "learning_rate": 2.2863682604272634e-05,
      "loss": 1.8453,
      "step": 53350
    },
    {
      "epoch": 27.14140386571719,
      "grad_norm": 49.398521423339844,
      "learning_rate": 2.2858596134282807e-05,
      "loss": 1.8515,
      "step": 53360
    },
    {
      "epoch": 27.146490335707018,
      "grad_norm": 34.69414138793945,
      "learning_rate": 2.2853509664292984e-05,
      "loss": 1.9107,
      "step": 53370
    },
    {
      "epoch": 27.151576805696845,
      "grad_norm": 35.081424713134766,
      "learning_rate": 2.2848423194303154e-05,
      "loss": 1.8626,
      "step": 53380
    },
    {
      "epoch": 27.15666327568667,
      "grad_norm": 36.56060028076172,
      "learning_rate": 2.2843336724313327e-05,
      "loss": 1.8417,
      "step": 53390
    },
    {
      "epoch": 27.161749745676502,
      "grad_norm": 34.35264205932617,
      "learning_rate": 2.28382502543235e-05,
      "loss": 1.8257,
      "step": 53400
    },
    {
      "epoch": 27.16683621566633,
      "grad_norm": 38.558109283447266,
      "learning_rate": 2.2833163784333673e-05,
      "loss": 1.8514,
      "step": 53410
    },
    {
      "epoch": 27.171922685656156,
      "grad_norm": 36.34027099609375,
      "learning_rate": 2.2828077314343847e-05,
      "loss": 1.8549,
      "step": 53420
    },
    {
      "epoch": 27.177009155645983,
      "grad_norm": 39.15583038330078,
      "learning_rate": 2.282299084435402e-05,
      "loss": 1.8199,
      "step": 53430
    },
    {
      "epoch": 27.18209562563581,
      "grad_norm": 35.63859939575195,
      "learning_rate": 2.2817904374364193e-05,
      "loss": 1.8496,
      "step": 53440
    },
    {
      "epoch": 27.187182095625637,
      "grad_norm": 39.02492141723633,
      "learning_rate": 2.2812817904374363e-05,
      "loss": 1.8098,
      "step": 53450
    },
    {
      "epoch": 27.192268565615464,
      "grad_norm": 34.86107635498047,
      "learning_rate": 2.280773143438454e-05,
      "loss": 1.9052,
      "step": 53460
    },
    {
      "epoch": 27.19735503560529,
      "grad_norm": 44.37839126586914,
      "learning_rate": 2.2802644964394713e-05,
      "loss": 1.826,
      "step": 53470
    },
    {
      "epoch": 27.202441505595118,
      "grad_norm": 37.68873977661133,
      "learning_rate": 2.2797558494404883e-05,
      "loss": 1.8282,
      "step": 53480
    },
    {
      "epoch": 27.207527975584945,
      "grad_norm": 41.945960998535156,
      "learning_rate": 2.2792472024415056e-05,
      "loss": 1.9179,
      "step": 53490
    },
    {
      "epoch": 27.212614445574772,
      "grad_norm": 36.56332778930664,
      "learning_rate": 2.278738555442523e-05,
      "loss": 1.9363,
      "step": 53500
    },
    {
      "epoch": 27.2177009155646,
      "grad_norm": 35.80892562866211,
      "learning_rate": 2.2782299084435403e-05,
      "loss": 1.8924,
      "step": 53510
    },
    {
      "epoch": 27.222787385554426,
      "grad_norm": 45.06327438354492,
      "learning_rate": 2.2777212614445576e-05,
      "loss": 1.8864,
      "step": 53520
    },
    {
      "epoch": 27.227873855544253,
      "grad_norm": 41.83877944946289,
      "learning_rate": 2.277212614445575e-05,
      "loss": 1.758,
      "step": 53530
    },
    {
      "epoch": 27.23296032553408,
      "grad_norm": 36.04954528808594,
      "learning_rate": 2.2767039674465923e-05,
      "loss": 1.9267,
      "step": 53540
    },
    {
      "epoch": 27.238046795523907,
      "grad_norm": 42.300201416015625,
      "learning_rate": 2.2761953204476093e-05,
      "loss": 1.9653,
      "step": 53550
    },
    {
      "epoch": 27.243133265513734,
      "grad_norm": 37.81809616088867,
      "learning_rate": 2.275686673448627e-05,
      "loss": 1.8677,
      "step": 53560
    },
    {
      "epoch": 27.24821973550356,
      "grad_norm": 41.94874954223633,
      "learning_rate": 2.2751780264496443e-05,
      "loss": 1.8067,
      "step": 53570
    },
    {
      "epoch": 27.253306205493388,
      "grad_norm": 34.97592544555664,
      "learning_rate": 2.2746693794506613e-05,
      "loss": 1.857,
      "step": 53580
    },
    {
      "epoch": 27.258392675483215,
      "grad_norm": 39.77320098876953,
      "learning_rate": 2.2741607324516786e-05,
      "loss": 1.907,
      "step": 53590
    },
    {
      "epoch": 27.263479145473042,
      "grad_norm": 34.52304458618164,
      "learning_rate": 2.273652085452696e-05,
      "loss": 1.8592,
      "step": 53600
    },
    {
      "epoch": 27.26856561546287,
      "grad_norm": 46.084320068359375,
      "learning_rate": 2.2731434384537132e-05,
      "loss": 1.782,
      "step": 53610
    },
    {
      "epoch": 27.273652085452696,
      "grad_norm": 43.104339599609375,
      "learning_rate": 2.2726347914547306e-05,
      "loss": 1.8708,
      "step": 53620
    },
    {
      "epoch": 27.278738555442523,
      "grad_norm": 36.79987335205078,
      "learning_rate": 2.272126144455748e-05,
      "loss": 1.7774,
      "step": 53630
    },
    {
      "epoch": 27.28382502543235,
      "grad_norm": 38.03303909301758,
      "learning_rate": 2.271617497456765e-05,
      "loss": 1.8984,
      "step": 53640
    },
    {
      "epoch": 27.288911495422177,
      "grad_norm": 36.75897216796875,
      "learning_rate": 2.2711088504577822e-05,
      "loss": 1.858,
      "step": 53650
    },
    {
      "epoch": 27.293997965412004,
      "grad_norm": 36.988895416259766,
      "learning_rate": 2.2706002034588e-05,
      "loss": 1.8869,
      "step": 53660
    },
    {
      "epoch": 27.29908443540183,
      "grad_norm": 48.991233825683594,
      "learning_rate": 2.270091556459817e-05,
      "loss": 1.8228,
      "step": 53670
    },
    {
      "epoch": 27.304170905391658,
      "grad_norm": 38.044464111328125,
      "learning_rate": 2.2695829094608342e-05,
      "loss": 1.8998,
      "step": 53680
    },
    {
      "epoch": 27.309257375381485,
      "grad_norm": 34.68824768066406,
      "learning_rate": 2.2690742624618515e-05,
      "loss": 1.7929,
      "step": 53690
    },
    {
      "epoch": 27.31434384537131,
      "grad_norm": 39.15595626831055,
      "learning_rate": 2.268565615462869e-05,
      "loss": 1.7299,
      "step": 53700
    },
    {
      "epoch": 27.31943031536114,
      "grad_norm": 43.820621490478516,
      "learning_rate": 2.2680569684638862e-05,
      "loss": 1.871,
      "step": 53710
    },
    {
      "epoch": 27.324516785350966,
      "grad_norm": 41.76144790649414,
      "learning_rate": 2.2675483214649035e-05,
      "loss": 1.8834,
      "step": 53720
    },
    {
      "epoch": 27.329603255340793,
      "grad_norm": 44.213932037353516,
      "learning_rate": 2.267039674465921e-05,
      "loss": 1.9435,
      "step": 53730
    },
    {
      "epoch": 27.33468972533062,
      "grad_norm": 31.610279083251953,
      "learning_rate": 2.2665310274669378e-05,
      "loss": 1.7851,
      "step": 53740
    },
    {
      "epoch": 27.339776195320447,
      "grad_norm": 39.53329849243164,
      "learning_rate": 2.2660223804679555e-05,
      "loss": 1.8685,
      "step": 53750
    },
    {
      "epoch": 27.344862665310274,
      "grad_norm": 35.94776153564453,
      "learning_rate": 2.2655137334689728e-05,
      "loss": 1.7955,
      "step": 53760
    },
    {
      "epoch": 27.3499491353001,
      "grad_norm": 36.84391403198242,
      "learning_rate": 2.2650050864699898e-05,
      "loss": 1.858,
      "step": 53770
    },
    {
      "epoch": 27.355035605289928,
      "grad_norm": 35.891944885253906,
      "learning_rate": 2.264496439471007e-05,
      "loss": 1.8797,
      "step": 53780
    },
    {
      "epoch": 27.360122075279754,
      "grad_norm": 37.35226058959961,
      "learning_rate": 2.2639877924720245e-05,
      "loss": 1.9789,
      "step": 53790
    },
    {
      "epoch": 27.36520854526958,
      "grad_norm": 38.135833740234375,
      "learning_rate": 2.2634791454730418e-05,
      "loss": 1.9248,
      "step": 53800
    },
    {
      "epoch": 27.37029501525941,
      "grad_norm": 41.14030838012695,
      "learning_rate": 2.262970498474059e-05,
      "loss": 1.894,
      "step": 53810
    },
    {
      "epoch": 27.375381485249235,
      "grad_norm": 39.1795539855957,
      "learning_rate": 2.2624618514750764e-05,
      "loss": 1.8532,
      "step": 53820
    },
    {
      "epoch": 27.380467955239062,
      "grad_norm": 33.73796081542969,
      "learning_rate": 2.2619532044760938e-05,
      "loss": 1.8685,
      "step": 53830
    },
    {
      "epoch": 27.38555442522889,
      "grad_norm": 33.28217697143555,
      "learning_rate": 2.2614445574771108e-05,
      "loss": 1.8182,
      "step": 53840
    },
    {
      "epoch": 27.39064089521872,
      "grad_norm": 32.576107025146484,
      "learning_rate": 2.2609359104781284e-05,
      "loss": 1.9019,
      "step": 53850
    },
    {
      "epoch": 27.395727365208547,
      "grad_norm": 35.539878845214844,
      "learning_rate": 2.2604272634791458e-05,
      "loss": 1.8342,
      "step": 53860
    },
    {
      "epoch": 27.400813835198374,
      "grad_norm": 36.89085388183594,
      "learning_rate": 2.2599186164801628e-05,
      "loss": 1.8082,
      "step": 53870
    },
    {
      "epoch": 27.4059003051882,
      "grad_norm": 39.18721008300781,
      "learning_rate": 2.25940996948118e-05,
      "loss": 1.8542,
      "step": 53880
    },
    {
      "epoch": 27.410986775178028,
      "grad_norm": 35.211334228515625,
      "learning_rate": 2.2589013224821974e-05,
      "loss": 1.7881,
      "step": 53890
    },
    {
      "epoch": 27.416073245167855,
      "grad_norm": 34.78392791748047,
      "learning_rate": 2.2583926754832147e-05,
      "loss": 1.863,
      "step": 53900
    },
    {
      "epoch": 27.421159715157682,
      "grad_norm": 27.538288116455078,
      "learning_rate": 2.257884028484232e-05,
      "loss": 1.7756,
      "step": 53910
    },
    {
      "epoch": 27.42624618514751,
      "grad_norm": 29.778562545776367,
      "learning_rate": 2.2573753814852494e-05,
      "loss": 1.8174,
      "step": 53920
    },
    {
      "epoch": 27.431332655137336,
      "grad_norm": 46.85552215576172,
      "learning_rate": 2.2568667344862664e-05,
      "loss": 1.8944,
      "step": 53930
    },
    {
      "epoch": 27.436419125127163,
      "grad_norm": 40.2160530090332,
      "learning_rate": 2.256358087487284e-05,
      "loss": 1.8492,
      "step": 53940
    },
    {
      "epoch": 27.44150559511699,
      "grad_norm": 30.694276809692383,
      "learning_rate": 2.2558494404883014e-05,
      "loss": 1.8933,
      "step": 53950
    },
    {
      "epoch": 27.446592065106817,
      "grad_norm": 31.36161994934082,
      "learning_rate": 2.2553407934893187e-05,
      "loss": 1.9737,
      "step": 53960
    },
    {
      "epoch": 27.451678535096644,
      "grad_norm": 36.20463180541992,
      "learning_rate": 2.2548321464903357e-05,
      "loss": 1.8297,
      "step": 53970
    },
    {
      "epoch": 27.45676500508647,
      "grad_norm": 38.58957290649414,
      "learning_rate": 2.254323499491353e-05,
      "loss": 1.8013,
      "step": 53980
    },
    {
      "epoch": 27.461851475076298,
      "grad_norm": 35.60566329956055,
      "learning_rate": 2.2538148524923703e-05,
      "loss": 1.8999,
      "step": 53990
    },
    {
      "epoch": 27.466937945066125,
      "grad_norm": 38.55915069580078,
      "learning_rate": 2.2533062054933877e-05,
      "loss": 1.8476,
      "step": 54000
    },
    {
      "epoch": 27.47202441505595,
      "grad_norm": 45.11330795288086,
      "learning_rate": 2.252797558494405e-05,
      "loss": 1.8144,
      "step": 54010
    },
    {
      "epoch": 27.47711088504578,
      "grad_norm": 33.69163131713867,
      "learning_rate": 2.2522889114954223e-05,
      "loss": 1.7847,
      "step": 54020
    },
    {
      "epoch": 27.482197355035606,
      "grad_norm": 40.524784088134766,
      "learning_rate": 2.2517802644964393e-05,
      "loss": 1.8954,
      "step": 54030
    },
    {
      "epoch": 27.487283825025433,
      "grad_norm": 29.928768157958984,
      "learning_rate": 2.251271617497457e-05,
      "loss": 1.8578,
      "step": 54040
    },
    {
      "epoch": 27.49237029501526,
      "grad_norm": 31.84847640991211,
      "learning_rate": 2.2507629704984743e-05,
      "loss": 1.8351,
      "step": 54050
    },
    {
      "epoch": 27.497456765005087,
      "grad_norm": 41.14598083496094,
      "learning_rate": 2.2502543234994913e-05,
      "loss": 1.851,
      "step": 54060
    },
    {
      "epoch": 27.502543234994913,
      "grad_norm": 30.249649047851562,
      "learning_rate": 2.2497456765005086e-05,
      "loss": 1.7754,
      "step": 54070
    },
    {
      "epoch": 27.50762970498474,
      "grad_norm": 41.46213912963867,
      "learning_rate": 2.249237029501526e-05,
      "loss": 1.8778,
      "step": 54080
    },
    {
      "epoch": 27.512716174974567,
      "grad_norm": 34.98521423339844,
      "learning_rate": 2.2487283825025436e-05,
      "loss": 1.7796,
      "step": 54090
    },
    {
      "epoch": 27.517802644964394,
      "grad_norm": 39.97200393676758,
      "learning_rate": 2.2482197355035606e-05,
      "loss": 1.8862,
      "step": 54100
    },
    {
      "epoch": 27.52288911495422,
      "grad_norm": 36.810970306396484,
      "learning_rate": 2.247711088504578e-05,
      "loss": 1.9127,
      "step": 54110
    },
    {
      "epoch": 27.52797558494405,
      "grad_norm": 37.04616928100586,
      "learning_rate": 2.2472024415055953e-05,
      "loss": 1.844,
      "step": 54120
    },
    {
      "epoch": 27.533062054933875,
      "grad_norm": 43.515506744384766,
      "learning_rate": 2.2466937945066126e-05,
      "loss": 1.793,
      "step": 54130
    },
    {
      "epoch": 27.538148524923702,
      "grad_norm": 31.490509033203125,
      "learning_rate": 2.24618514750763e-05,
      "loss": 1.7286,
      "step": 54140
    },
    {
      "epoch": 27.54323499491353,
      "grad_norm": 34.32490158081055,
      "learning_rate": 2.2456765005086473e-05,
      "loss": 1.8312,
      "step": 54150
    },
    {
      "epoch": 27.548321464903356,
      "grad_norm": 43.70221710205078,
      "learning_rate": 2.2451678535096643e-05,
      "loss": 1.8232,
      "step": 54160
    },
    {
      "epoch": 27.553407934893183,
      "grad_norm": 35.12739562988281,
      "learning_rate": 2.2446592065106816e-05,
      "loss": 1.8519,
      "step": 54170
    },
    {
      "epoch": 27.55849440488301,
      "grad_norm": 32.43619918823242,
      "learning_rate": 2.244150559511699e-05,
      "loss": 1.8091,
      "step": 54180
    },
    {
      "epoch": 27.563580874872837,
      "grad_norm": 40.01922607421875,
      "learning_rate": 2.2436419125127162e-05,
      "loss": 1.9157,
      "step": 54190
    },
    {
      "epoch": 27.568667344862664,
      "grad_norm": 44.71791076660156,
      "learning_rate": 2.2431332655137336e-05,
      "loss": 1.9406,
      "step": 54200
    },
    {
      "epoch": 27.57375381485249,
      "grad_norm": 45.90235137939453,
      "learning_rate": 2.242624618514751e-05,
      "loss": 1.9009,
      "step": 54210
    },
    {
      "epoch": 27.578840284842318,
      "grad_norm": 47.01628112792969,
      "learning_rate": 2.2421159715157682e-05,
      "loss": 1.8335,
      "step": 54220
    },
    {
      "epoch": 27.583926754832145,
      "grad_norm": 40.9119873046875,
      "learning_rate": 2.2416073245167855e-05,
      "loss": 1.865,
      "step": 54230
    },
    {
      "epoch": 27.589013224821972,
      "grad_norm": 37.60660171508789,
      "learning_rate": 2.241098677517803e-05,
      "loss": 1.8469,
      "step": 54240
    },
    {
      "epoch": 27.5940996948118,
      "grad_norm": 32.24099349975586,
      "learning_rate": 2.2405900305188202e-05,
      "loss": 1.8777,
      "step": 54250
    },
    {
      "epoch": 27.599186164801626,
      "grad_norm": 39.66086959838867,
      "learning_rate": 2.2400813835198372e-05,
      "loss": 1.859,
      "step": 54260
    },
    {
      "epoch": 27.604272634791453,
      "grad_norm": 48.86186981201172,
      "learning_rate": 2.2395727365208545e-05,
      "loss": 1.823,
      "step": 54270
    },
    {
      "epoch": 27.60935910478128,
      "grad_norm": 38.235755920410156,
      "learning_rate": 2.2390640895218722e-05,
      "loss": 1.8767,
      "step": 54280
    },
    {
      "epoch": 27.61444557477111,
      "grad_norm": 39.45427322387695,
      "learning_rate": 2.2385554425228892e-05,
      "loss": 1.9121,
      "step": 54290
    },
    {
      "epoch": 27.619532044760938,
      "grad_norm": 35.610328674316406,
      "learning_rate": 2.2380467955239065e-05,
      "loss": 1.804,
      "step": 54300
    },
    {
      "epoch": 27.624618514750765,
      "grad_norm": 39.78864288330078,
      "learning_rate": 2.237538148524924e-05,
      "loss": 1.8971,
      "step": 54310
    },
    {
      "epoch": 27.62970498474059,
      "grad_norm": 37.197200775146484,
      "learning_rate": 2.2370295015259408e-05,
      "loss": 1.9194,
      "step": 54320
    },
    {
      "epoch": 27.63479145473042,
      "grad_norm": 32.8437614440918,
      "learning_rate": 2.2365208545269585e-05,
      "loss": 1.8134,
      "step": 54330
    },
    {
      "epoch": 27.639877924720246,
      "grad_norm": 41.18789291381836,
      "learning_rate": 2.2360122075279758e-05,
      "loss": 1.8538,
      "step": 54340
    },
    {
      "epoch": 27.644964394710072,
      "grad_norm": 43.802032470703125,
      "learning_rate": 2.235503560528993e-05,
      "loss": 1.7882,
      "step": 54350
    },
    {
      "epoch": 27.6500508646999,
      "grad_norm": 38.053829193115234,
      "learning_rate": 2.23499491353001e-05,
      "loss": 1.8093,
      "step": 54360
    },
    {
      "epoch": 27.655137334689726,
      "grad_norm": 34.3441162109375,
      "learning_rate": 2.2344862665310275e-05,
      "loss": 1.9092,
      "step": 54370
    },
    {
      "epoch": 27.660223804679553,
      "grad_norm": 34.79649353027344,
      "learning_rate": 2.233977619532045e-05,
      "loss": 1.8382,
      "step": 54380
    },
    {
      "epoch": 27.66531027466938,
      "grad_norm": 42.78205490112305,
      "learning_rate": 2.233468972533062e-05,
      "loss": 1.8802,
      "step": 54390
    },
    {
      "epoch": 27.670396744659207,
      "grad_norm": 33.513282775878906,
      "learning_rate": 2.2329603255340794e-05,
      "loss": 1.867,
      "step": 54400
    },
    {
      "epoch": 27.675483214649034,
      "grad_norm": 39.224910736083984,
      "learning_rate": 2.2324516785350968e-05,
      "loss": 1.9191,
      "step": 54410
    },
    {
      "epoch": 27.68056968463886,
      "grad_norm": 32.3952751159668,
      "learning_rate": 2.231943031536114e-05,
      "loss": 1.9188,
      "step": 54420
    },
    {
      "epoch": 27.68565615462869,
      "grad_norm": 40.67999267578125,
      "learning_rate": 2.2314343845371314e-05,
      "loss": 1.8564,
      "step": 54430
    },
    {
      "epoch": 27.690742624618515,
      "grad_norm": 36.066322326660156,
      "learning_rate": 2.2309257375381488e-05,
      "loss": 1.918,
      "step": 54440
    },
    {
      "epoch": 27.695829094608342,
      "grad_norm": 45.12908935546875,
      "learning_rate": 2.2304170905391658e-05,
      "loss": 1.8751,
      "step": 54450
    },
    {
      "epoch": 27.70091556459817,
      "grad_norm": 38.140159606933594,
      "learning_rate": 2.229908443540183e-05,
      "loss": 1.8714,
      "step": 54460
    },
    {
      "epoch": 27.706002034587996,
      "grad_norm": 41.59008026123047,
      "learning_rate": 2.2293997965412004e-05,
      "loss": 1.8506,
      "step": 54470
    },
    {
      "epoch": 27.711088504577823,
      "grad_norm": 31.496288299560547,
      "learning_rate": 2.2288911495422177e-05,
      "loss": 1.8882,
      "step": 54480
    },
    {
      "epoch": 27.71617497456765,
      "grad_norm": 35.27456283569336,
      "learning_rate": 2.228382502543235e-05,
      "loss": 1.8997,
      "step": 54490
    },
    {
      "epoch": 27.721261444557477,
      "grad_norm": 38.27814865112305,
      "learning_rate": 2.2278738555442524e-05,
      "loss": 1.8392,
      "step": 54500
    },
    {
      "epoch": 27.726347914547304,
      "grad_norm": 43.72593688964844,
      "learning_rate": 2.2273652085452697e-05,
      "loss": 1.7824,
      "step": 54510
    },
    {
      "epoch": 27.73143438453713,
      "grad_norm": 34.96174240112305,
      "learning_rate": 2.226856561546287e-05,
      "loss": 1.7974,
      "step": 54520
    },
    {
      "epoch": 27.736520854526958,
      "grad_norm": 35.64036178588867,
      "learning_rate": 2.2263479145473044e-05,
      "loss": 1.7842,
      "step": 54530
    },
    {
      "epoch": 27.741607324516785,
      "grad_norm": 30.448286056518555,
      "learning_rate": 2.2258392675483217e-05,
      "loss": 1.7674,
      "step": 54540
    },
    {
      "epoch": 27.746693794506612,
      "grad_norm": 37.57359313964844,
      "learning_rate": 2.2253306205493387e-05,
      "loss": 1.899,
      "step": 54550
    },
    {
      "epoch": 27.75178026449644,
      "grad_norm": 31.174137115478516,
      "learning_rate": 2.224821973550356e-05,
      "loss": 1.8509,
      "step": 54560
    },
    {
      "epoch": 27.756866734486266,
      "grad_norm": 34.28485870361328,
      "learning_rate": 2.2243133265513737e-05,
      "loss": 1.7396,
      "step": 54570
    },
    {
      "epoch": 27.761953204476093,
      "grad_norm": 39.38080596923828,
      "learning_rate": 2.2238046795523907e-05,
      "loss": 1.8541,
      "step": 54580
    },
    {
      "epoch": 27.76703967446592,
      "grad_norm": 37.50821304321289,
      "learning_rate": 2.223296032553408e-05,
      "loss": 1.8229,
      "step": 54590
    },
    {
      "epoch": 27.772126144455747,
      "grad_norm": 32.28335189819336,
      "learning_rate": 2.2227873855544253e-05,
      "loss": 1.9261,
      "step": 54600
    },
    {
      "epoch": 27.777212614445574,
      "grad_norm": 34.72346496582031,
      "learning_rate": 2.2222787385554427e-05,
      "loss": 1.8236,
      "step": 54610
    },
    {
      "epoch": 27.7822990844354,
      "grad_norm": 32.14606475830078,
      "learning_rate": 2.22177009155646e-05,
      "loss": 1.8661,
      "step": 54620
    },
    {
      "epoch": 27.787385554425228,
      "grad_norm": 34.45896911621094,
      "learning_rate": 2.2212614445574773e-05,
      "loss": 1.8307,
      "step": 54630
    },
    {
      "epoch": 27.792472024415055,
      "grad_norm": 38.463558197021484,
      "learning_rate": 2.2207527975584946e-05,
      "loss": 1.7727,
      "step": 54640
    },
    {
      "epoch": 27.797558494404882,
      "grad_norm": 31.867618560791016,
      "learning_rate": 2.2202441505595116e-05,
      "loss": 1.8735,
      "step": 54650
    },
    {
      "epoch": 27.80264496439471,
      "grad_norm": 38.74317932128906,
      "learning_rate": 2.219735503560529e-05,
      "loss": 1.7861,
      "step": 54660
    },
    {
      "epoch": 27.807731434384536,
      "grad_norm": 40.10606384277344,
      "learning_rate": 2.2192268565615466e-05,
      "loss": 1.8827,
      "step": 54670
    },
    {
      "epoch": 27.812817904374363,
      "grad_norm": 37.098716735839844,
      "learning_rate": 2.2187182095625636e-05,
      "loss": 1.7578,
      "step": 54680
    },
    {
      "epoch": 27.81790437436419,
      "grad_norm": 37.129940032958984,
      "learning_rate": 2.218209562563581e-05,
      "loss": 1.7798,
      "step": 54690
    },
    {
      "epoch": 27.822990844354017,
      "grad_norm": 34.14958190917969,
      "learning_rate": 2.2177009155645983e-05,
      "loss": 1.8236,
      "step": 54700
    },
    {
      "epoch": 27.828077314343844,
      "grad_norm": 39.29309844970703,
      "learning_rate": 2.2171922685656156e-05,
      "loss": 1.8856,
      "step": 54710
    },
    {
      "epoch": 27.83316378433367,
      "grad_norm": 35.0221061706543,
      "learning_rate": 2.216683621566633e-05,
      "loss": 1.846,
      "step": 54720
    },
    {
      "epoch": 27.838250254323498,
      "grad_norm": 38.695533752441406,
      "learning_rate": 2.2161749745676503e-05,
      "loss": 1.8247,
      "step": 54730
    },
    {
      "epoch": 27.843336724313325,
      "grad_norm": 42.54990005493164,
      "learning_rate": 2.2156663275686673e-05,
      "loss": 1.8865,
      "step": 54740
    },
    {
      "epoch": 27.848423194303155,
      "grad_norm": 41.418357849121094,
      "learning_rate": 2.2151576805696846e-05,
      "loss": 1.8596,
      "step": 54750
    },
    {
      "epoch": 27.853509664292982,
      "grad_norm": 34.46931838989258,
      "learning_rate": 2.2146490335707022e-05,
      "loss": 1.8724,
      "step": 54760
    },
    {
      "epoch": 27.85859613428281,
      "grad_norm": 37.83645248413086,
      "learning_rate": 2.2141403865717196e-05,
      "loss": 1.8479,
      "step": 54770
    },
    {
      "epoch": 27.863682604272636,
      "grad_norm": 41.492950439453125,
      "learning_rate": 2.2136317395727366e-05,
      "loss": 1.8954,
      "step": 54780
    },
    {
      "epoch": 27.868769074262463,
      "grad_norm": 38.16449737548828,
      "learning_rate": 2.213123092573754e-05,
      "loss": 1.8988,
      "step": 54790
    },
    {
      "epoch": 27.87385554425229,
      "grad_norm": 45.54218292236328,
      "learning_rate": 2.2126144455747712e-05,
      "loss": 1.9547,
      "step": 54800
    },
    {
      "epoch": 27.878942014242117,
      "grad_norm": 34.34137725830078,
      "learning_rate": 2.2121057985757885e-05,
      "loss": 1.7981,
      "step": 54810
    },
    {
      "epoch": 27.884028484231944,
      "grad_norm": 36.01227569580078,
      "learning_rate": 2.211597151576806e-05,
      "loss": 1.7847,
      "step": 54820
    },
    {
      "epoch": 27.88911495422177,
      "grad_norm": 35.01921844482422,
      "learning_rate": 2.2110885045778232e-05,
      "loss": 1.9807,
      "step": 54830
    },
    {
      "epoch": 27.894201424211598,
      "grad_norm": 38.4608154296875,
      "learning_rate": 2.2105798575788402e-05,
      "loss": 1.7927,
      "step": 54840
    },
    {
      "epoch": 27.899287894201425,
      "grad_norm": 49.68396759033203,
      "learning_rate": 2.2100712105798575e-05,
      "loss": 1.8879,
      "step": 54850
    },
    {
      "epoch": 27.904374364191252,
      "grad_norm": 37.55970764160156,
      "learning_rate": 2.2095625635808752e-05,
      "loss": 1.7362,
      "step": 54860
    },
    {
      "epoch": 27.90946083418108,
      "grad_norm": 42.921443939208984,
      "learning_rate": 2.2090539165818922e-05,
      "loss": 1.8581,
      "step": 54870
    },
    {
      "epoch": 27.914547304170906,
      "grad_norm": 35.018733978271484,
      "learning_rate": 2.2085452695829095e-05,
      "loss": 1.7671,
      "step": 54880
    },
    {
      "epoch": 27.919633774160733,
      "grad_norm": 34.851890563964844,
      "learning_rate": 2.208036622583927e-05,
      "loss": 1.8716,
      "step": 54890
    },
    {
      "epoch": 27.92472024415056,
      "grad_norm": 30.508201599121094,
      "learning_rate": 2.207527975584944e-05,
      "loss": 1.8729,
      "step": 54900
    },
    {
      "epoch": 27.929806714140387,
      "grad_norm": 42.28940200805664,
      "learning_rate": 2.2070193285859615e-05,
      "loss": 1.8977,
      "step": 54910
    },
    {
      "epoch": 27.934893184130214,
      "grad_norm": 33.39496994018555,
      "learning_rate": 2.2065106815869788e-05,
      "loss": 1.7783,
      "step": 54920
    },
    {
      "epoch": 27.93997965412004,
      "grad_norm": 41.40541458129883,
      "learning_rate": 2.206002034587996e-05,
      "loss": 1.8289,
      "step": 54930
    },
    {
      "epoch": 27.945066124109868,
      "grad_norm": 32.551658630371094,
      "learning_rate": 2.205493387589013e-05,
      "loss": 1.9286,
      "step": 54940
    },
    {
      "epoch": 27.950152594099695,
      "grad_norm": 44.736331939697266,
      "learning_rate": 2.2049847405900305e-05,
      "loss": 1.8008,
      "step": 54950
    },
    {
      "epoch": 27.955239064089522,
      "grad_norm": 33.96603012084961,
      "learning_rate": 2.204476093591048e-05,
      "loss": 1.7808,
      "step": 54960
    },
    {
      "epoch": 27.96032553407935,
      "grad_norm": 37.20417404174805,
      "learning_rate": 2.203967446592065e-05,
      "loss": 1.817,
      "step": 54970
    },
    {
      "epoch": 27.965412004069176,
      "grad_norm": 38.26780700683594,
      "learning_rate": 2.2034587995930825e-05,
      "loss": 1.8611,
      "step": 54980
    },
    {
      "epoch": 27.970498474059003,
      "grad_norm": 41.3734016418457,
      "learning_rate": 2.2029501525940998e-05,
      "loss": 1.8241,
      "step": 54990
    },
    {
      "epoch": 27.97558494404883,
      "grad_norm": 42.118980407714844,
      "learning_rate": 2.202441505595117e-05,
      "loss": 1.8085,
      "step": 55000
    },
    {
      "epoch": 27.980671414038657,
      "grad_norm": 31.571075439453125,
      "learning_rate": 2.2019328585961344e-05,
      "loss": 1.8646,
      "step": 55010
    },
    {
      "epoch": 27.985757884028484,
      "grad_norm": 46.8089485168457,
      "learning_rate": 2.2014242115971518e-05,
      "loss": 1.8997,
      "step": 55020
    },
    {
      "epoch": 27.99084435401831,
      "grad_norm": 41.931278228759766,
      "learning_rate": 2.200915564598169e-05,
      "loss": 1.9155,
      "step": 55030
    },
    {
      "epoch": 27.995930824008138,
      "grad_norm": 40.918601989746094,
      "learning_rate": 2.200406917599186e-05,
      "loss": 1.8606,
      "step": 55040
    },
    {
      "epoch": 28.0,
      "eval_loss": 4.701181888580322,
      "eval_runtime": 2.6179,
      "eval_samples_per_second": 1060.03,
      "eval_steps_per_second": 132.551,
      "step": 55048
    },
    {
      "epoch": 28.001017293997965,
      "grad_norm": 42.14381408691406,
      "learning_rate": 2.1998982706002037e-05,
      "loss": 1.7812,
      "step": 55050
    },
    {
      "epoch": 28.00610376398779,
      "grad_norm": 45.34716796875,
      "learning_rate": 2.199389623601221e-05,
      "loss": 1.8302,
      "step": 55060
    },
    {
      "epoch": 28.01119023397762,
      "grad_norm": 28.656644821166992,
      "learning_rate": 2.198880976602238e-05,
      "loss": 1.8318,
      "step": 55070
    },
    {
      "epoch": 28.016276703967446,
      "grad_norm": 37.687744140625,
      "learning_rate": 2.1983723296032554e-05,
      "loss": 1.7341,
      "step": 55080
    },
    {
      "epoch": 28.021363173957273,
      "grad_norm": 49.158180236816406,
      "learning_rate": 2.1978636826042727e-05,
      "loss": 1.9267,
      "step": 55090
    },
    {
      "epoch": 28.0264496439471,
      "grad_norm": 41.84709167480469,
      "learning_rate": 2.19735503560529e-05,
      "loss": 1.8773,
      "step": 55100
    },
    {
      "epoch": 28.031536113936927,
      "grad_norm": 38.11353302001953,
      "learning_rate": 2.1968463886063074e-05,
      "loss": 1.8043,
      "step": 55110
    },
    {
      "epoch": 28.036622583926754,
      "grad_norm": 41.30530548095703,
      "learning_rate": 2.1963377416073247e-05,
      "loss": 1.7907,
      "step": 55120
    },
    {
      "epoch": 28.04170905391658,
      "grad_norm": 39.20310974121094,
      "learning_rate": 2.1958290946083417e-05,
      "loss": 1.8144,
      "step": 55130
    },
    {
      "epoch": 28.046795523906408,
      "grad_norm": 33.17015075683594,
      "learning_rate": 2.195320447609359e-05,
      "loss": 1.7645,
      "step": 55140
    },
    {
      "epoch": 28.051881993896234,
      "grad_norm": 31.98599624633789,
      "learning_rate": 2.1948118006103767e-05,
      "loss": 1.8719,
      "step": 55150
    },
    {
      "epoch": 28.05696846388606,
      "grad_norm": 40.42881393432617,
      "learning_rate": 2.194303153611394e-05,
      "loss": 1.9201,
      "step": 55160
    },
    {
      "epoch": 28.06205493387589,
      "grad_norm": 32.649105072021484,
      "learning_rate": 2.193794506612411e-05,
      "loss": 1.819,
      "step": 55170
    },
    {
      "epoch": 28.067141403865715,
      "grad_norm": 32.83311462402344,
      "learning_rate": 2.1932858596134283e-05,
      "loss": 1.8087,
      "step": 55180
    },
    {
      "epoch": 28.072227873855546,
      "grad_norm": 35.541343688964844,
      "learning_rate": 2.1927772126144457e-05,
      "loss": 1.8332,
      "step": 55190
    },
    {
      "epoch": 28.077314343845373,
      "grad_norm": 41.22298049926758,
      "learning_rate": 2.192268565615463e-05,
      "loss": 1.7731,
      "step": 55200
    },
    {
      "epoch": 28.0824008138352,
      "grad_norm": 34.17264175415039,
      "learning_rate": 2.1917599186164803e-05,
      "loss": 1.8136,
      "step": 55210
    },
    {
      "epoch": 28.087487283825027,
      "grad_norm": 40.41331100463867,
      "learning_rate": 2.1912512716174976e-05,
      "loss": 1.879,
      "step": 55220
    },
    {
      "epoch": 28.092573753814854,
      "grad_norm": 36.62367630004883,
      "learning_rate": 2.1907426246185146e-05,
      "loss": 1.7547,
      "step": 55230
    },
    {
      "epoch": 28.09766022380468,
      "grad_norm": 42.238128662109375,
      "learning_rate": 2.1902339776195323e-05,
      "loss": 1.7832,
      "step": 55240
    },
    {
      "epoch": 28.102746693794508,
      "grad_norm": 35.78323745727539,
      "learning_rate": 2.1897253306205496e-05,
      "loss": 1.8089,
      "step": 55250
    },
    {
      "epoch": 28.107833163784335,
      "grad_norm": 32.47917938232422,
      "learning_rate": 2.1892166836215666e-05,
      "loss": 1.7832,
      "step": 55260
    },
    {
      "epoch": 28.112919633774162,
      "grad_norm": 37.873626708984375,
      "learning_rate": 2.188708036622584e-05,
      "loss": 1.8964,
      "step": 55270
    },
    {
      "epoch": 28.11800610376399,
      "grad_norm": 44.708003997802734,
      "learning_rate": 2.1881993896236013e-05,
      "loss": 1.8353,
      "step": 55280
    },
    {
      "epoch": 28.123092573753816,
      "grad_norm": 45.93353271484375,
      "learning_rate": 2.1876907426246186e-05,
      "loss": 1.8014,
      "step": 55290
    },
    {
      "epoch": 28.128179043743643,
      "grad_norm": 42.45811462402344,
      "learning_rate": 2.187182095625636e-05,
      "loss": 1.7943,
      "step": 55300
    },
    {
      "epoch": 28.13326551373347,
      "grad_norm": 39.6326904296875,
      "learning_rate": 2.1866734486266533e-05,
      "loss": 1.7753,
      "step": 55310
    },
    {
      "epoch": 28.138351983723297,
      "grad_norm": 36.64140701293945,
      "learning_rate": 2.1861648016276706e-05,
      "loss": 1.7827,
      "step": 55320
    },
    {
      "epoch": 28.143438453713124,
      "grad_norm": 37.34019088745117,
      "learning_rate": 2.1856561546286876e-05,
      "loss": 1.9307,
      "step": 55330
    },
    {
      "epoch": 28.14852492370295,
      "grad_norm": 40.98779296875,
      "learning_rate": 2.1851475076297052e-05,
      "loss": 1.8285,
      "step": 55340
    },
    {
      "epoch": 28.153611393692778,
      "grad_norm": 35.72509765625,
      "learning_rate": 2.1846388606307226e-05,
      "loss": 1.8736,
      "step": 55350
    },
    {
      "epoch": 28.158697863682605,
      "grad_norm": 33.97129440307617,
      "learning_rate": 2.1841302136317396e-05,
      "loss": 1.8922,
      "step": 55360
    },
    {
      "epoch": 28.16378433367243,
      "grad_norm": 34.557762145996094,
      "learning_rate": 2.183621566632757e-05,
      "loss": 1.8011,
      "step": 55370
    },
    {
      "epoch": 28.16887080366226,
      "grad_norm": 30.97678565979004,
      "learning_rate": 2.1831129196337742e-05,
      "loss": 1.8441,
      "step": 55380
    },
    {
      "epoch": 28.173957273652086,
      "grad_norm": 37.064430236816406,
      "learning_rate": 2.1826042726347915e-05,
      "loss": 1.897,
      "step": 55390
    },
    {
      "epoch": 28.179043743641913,
      "grad_norm": 40.57789611816406,
      "learning_rate": 2.182095625635809e-05,
      "loss": 1.8958,
      "step": 55400
    },
    {
      "epoch": 28.18413021363174,
      "grad_norm": 32.1529426574707,
      "learning_rate": 2.1815869786368262e-05,
      "loss": 1.8038,
      "step": 55410
    },
    {
      "epoch": 28.189216683621567,
      "grad_norm": 31.526029586791992,
      "learning_rate": 2.1810783316378432e-05,
      "loss": 1.8051,
      "step": 55420
    },
    {
      "epoch": 28.194303153611393,
      "grad_norm": 33.21157455444336,
      "learning_rate": 2.1805696846388605e-05,
      "loss": 1.7823,
      "step": 55430
    },
    {
      "epoch": 28.19938962360122,
      "grad_norm": 35.120582580566406,
      "learning_rate": 2.1800610376398782e-05,
      "loss": 1.8836,
      "step": 55440
    },
    {
      "epoch": 28.204476093591047,
      "grad_norm": 40.17757034301758,
      "learning_rate": 2.1795523906408955e-05,
      "loss": 1.8109,
      "step": 55450
    },
    {
      "epoch": 28.209562563580874,
      "grad_norm": 34.42441940307617,
      "learning_rate": 2.1790437436419125e-05,
      "loss": 1.7628,
      "step": 55460
    },
    {
      "epoch": 28.2146490335707,
      "grad_norm": 45.70390319824219,
      "learning_rate": 2.17853509664293e-05,
      "loss": 1.799,
      "step": 55470
    },
    {
      "epoch": 28.21973550356053,
      "grad_norm": 45.36349868774414,
      "learning_rate": 2.178026449643947e-05,
      "loss": 1.8072,
      "step": 55480
    },
    {
      "epoch": 28.224821973550355,
      "grad_norm": 36.073707580566406,
      "learning_rate": 2.1775178026449645e-05,
      "loss": 1.8377,
      "step": 55490
    },
    {
      "epoch": 28.229908443540182,
      "grad_norm": 43.973819732666016,
      "learning_rate": 2.1770091556459818e-05,
      "loss": 1.7947,
      "step": 55500
    },
    {
      "epoch": 28.23499491353001,
      "grad_norm": 37.79264450073242,
      "learning_rate": 2.176500508646999e-05,
      "loss": 1.8697,
      "step": 55510
    },
    {
      "epoch": 28.240081383519836,
      "grad_norm": 38.689537048339844,
      "learning_rate": 2.175991861648016e-05,
      "loss": 1.8372,
      "step": 55520
    },
    {
      "epoch": 28.245167853509663,
      "grad_norm": 43.92523193359375,
      "learning_rate": 2.1754832146490338e-05,
      "loss": 1.902,
      "step": 55530
    },
    {
      "epoch": 28.25025432349949,
      "grad_norm": 33.94388198852539,
      "learning_rate": 2.174974567650051e-05,
      "loss": 1.8214,
      "step": 55540
    },
    {
      "epoch": 28.255340793489317,
      "grad_norm": 40.76364517211914,
      "learning_rate": 2.174465920651068e-05,
      "loss": 1.8461,
      "step": 55550
    },
    {
      "epoch": 28.260427263479144,
      "grad_norm": 34.6638298034668,
      "learning_rate": 2.1739572736520855e-05,
      "loss": 1.7926,
      "step": 55560
    },
    {
      "epoch": 28.26551373346897,
      "grad_norm": 34.30099105834961,
      "learning_rate": 2.1734486266531028e-05,
      "loss": 1.8555,
      "step": 55570
    },
    {
      "epoch": 28.270600203458798,
      "grad_norm": 34.10649490356445,
      "learning_rate": 2.17293997965412e-05,
      "loss": 1.7929,
      "step": 55580
    },
    {
      "epoch": 28.275686673448625,
      "grad_norm": 33.14213943481445,
      "learning_rate": 2.1724313326551374e-05,
      "loss": 1.8631,
      "step": 55590
    },
    {
      "epoch": 28.280773143438452,
      "grad_norm": 32.4892692565918,
      "learning_rate": 2.1719226856561548e-05,
      "loss": 1.8993,
      "step": 55600
    },
    {
      "epoch": 28.28585961342828,
      "grad_norm": 49.96656036376953,
      "learning_rate": 2.171414038657172e-05,
      "loss": 1.7173,
      "step": 55610
    },
    {
      "epoch": 28.290946083418106,
      "grad_norm": 39.563819885253906,
      "learning_rate": 2.170905391658189e-05,
      "loss": 1.8143,
      "step": 55620
    },
    {
      "epoch": 28.296032553407933,
      "grad_norm": 39.22744369506836,
      "learning_rate": 2.1703967446592067e-05,
      "loss": 1.8694,
      "step": 55630
    },
    {
      "epoch": 28.301119023397764,
      "grad_norm": 37.583709716796875,
      "learning_rate": 2.169888097660224e-05,
      "loss": 1.7877,
      "step": 55640
    },
    {
      "epoch": 28.30620549338759,
      "grad_norm": 41.778770446777344,
      "learning_rate": 2.169379450661241e-05,
      "loss": 1.8308,
      "step": 55650
    },
    {
      "epoch": 28.311291963377418,
      "grad_norm": 36.40782165527344,
      "learning_rate": 2.1688708036622584e-05,
      "loss": 1.776,
      "step": 55660
    },
    {
      "epoch": 28.316378433367245,
      "grad_norm": 30.746538162231445,
      "learning_rate": 2.1683621566632757e-05,
      "loss": 1.8444,
      "step": 55670
    },
    {
      "epoch": 28.32146490335707,
      "grad_norm": 31.895877838134766,
      "learning_rate": 2.167853509664293e-05,
      "loss": 1.7875,
      "step": 55680
    },
    {
      "epoch": 28.3265513733469,
      "grad_norm": 40.336997985839844,
      "learning_rate": 2.1673448626653104e-05,
      "loss": 1.8547,
      "step": 55690
    },
    {
      "epoch": 28.331637843336726,
      "grad_norm": 37.92008972167969,
      "learning_rate": 2.1668362156663277e-05,
      "loss": 1.9035,
      "step": 55700
    },
    {
      "epoch": 28.336724313326553,
      "grad_norm": 36.7274055480957,
      "learning_rate": 2.166327568667345e-05,
      "loss": 1.7883,
      "step": 55710
    },
    {
      "epoch": 28.34181078331638,
      "grad_norm": 30.615982055664062,
      "learning_rate": 2.1658189216683624e-05,
      "loss": 1.8577,
      "step": 55720
    },
    {
      "epoch": 28.346897253306206,
      "grad_norm": 43.8974494934082,
      "learning_rate": 2.1653102746693797e-05,
      "loss": 1.784,
      "step": 55730
    },
    {
      "epoch": 28.351983723296033,
      "grad_norm": 35.898799896240234,
      "learning_rate": 2.164801627670397e-05,
      "loss": 1.8497,
      "step": 55740
    },
    {
      "epoch": 28.35707019328586,
      "grad_norm": 36.27899169921875,
      "learning_rate": 2.164292980671414e-05,
      "loss": 1.8373,
      "step": 55750
    },
    {
      "epoch": 28.362156663275687,
      "grad_norm": 36.511817932128906,
      "learning_rate": 2.1637843336724313e-05,
      "loss": 1.8312,
      "step": 55760
    },
    {
      "epoch": 28.367243133265514,
      "grad_norm": 42.139339447021484,
      "learning_rate": 2.1632756866734487e-05,
      "loss": 1.7515,
      "step": 55770
    },
    {
      "epoch": 28.37232960325534,
      "grad_norm": 35.12419509887695,
      "learning_rate": 2.162767039674466e-05,
      "loss": 1.7983,
      "step": 55780
    },
    {
      "epoch": 28.37741607324517,
      "grad_norm": 36.2220344543457,
      "learning_rate": 2.1622583926754833e-05,
      "loss": 1.7828,
      "step": 55790
    },
    {
      "epoch": 28.382502543234995,
      "grad_norm": 31.488140106201172,
      "learning_rate": 2.1617497456765006e-05,
      "loss": 1.7712,
      "step": 55800
    },
    {
      "epoch": 28.387589013224822,
      "grad_norm": 39.724708557128906,
      "learning_rate": 2.1612410986775176e-05,
      "loss": 1.9102,
      "step": 55810
    },
    {
      "epoch": 28.39267548321465,
      "grad_norm": 33.02339553833008,
      "learning_rate": 2.1607324516785353e-05,
      "loss": 1.7742,
      "step": 55820
    },
    {
      "epoch": 28.397761953204476,
      "grad_norm": 42.39246368408203,
      "learning_rate": 2.1602238046795526e-05,
      "loss": 1.8536,
      "step": 55830
    },
    {
      "epoch": 28.402848423194303,
      "grad_norm": 36.49177932739258,
      "learning_rate": 2.15971515768057e-05,
      "loss": 1.8133,
      "step": 55840
    },
    {
      "epoch": 28.40793489318413,
      "grad_norm": 32.41950225830078,
      "learning_rate": 2.159206510681587e-05,
      "loss": 1.8032,
      "step": 55850
    },
    {
      "epoch": 28.413021363173957,
      "grad_norm": 42.479881286621094,
      "learning_rate": 2.1586978636826043e-05,
      "loss": 1.9608,
      "step": 55860
    },
    {
      "epoch": 28.418107833163784,
      "grad_norm": 32.27059555053711,
      "learning_rate": 2.158189216683622e-05,
      "loss": 1.8398,
      "step": 55870
    },
    {
      "epoch": 28.42319430315361,
      "grad_norm": 38.14390563964844,
      "learning_rate": 2.157680569684639e-05,
      "loss": 1.77,
      "step": 55880
    },
    {
      "epoch": 28.428280773143438,
      "grad_norm": 31.98154640197754,
      "learning_rate": 2.1571719226856563e-05,
      "loss": 1.8378,
      "step": 55890
    },
    {
      "epoch": 28.433367243133265,
      "grad_norm": 35.453094482421875,
      "learning_rate": 2.1566632756866736e-05,
      "loss": 1.7467,
      "step": 55900
    },
    {
      "epoch": 28.438453713123092,
      "grad_norm": 43.474178314208984,
      "learning_rate": 2.1561546286876906e-05,
      "loss": 1.7539,
      "step": 55910
    },
    {
      "epoch": 28.44354018311292,
      "grad_norm": 31.084293365478516,
      "learning_rate": 2.1556459816887082e-05,
      "loss": 1.9397,
      "step": 55920
    },
    {
      "epoch": 28.448626653102746,
      "grad_norm": 32.29756164550781,
      "learning_rate": 2.1551373346897256e-05,
      "loss": 1.792,
      "step": 55930
    },
    {
      "epoch": 28.453713123092573,
      "grad_norm": 36.45275115966797,
      "learning_rate": 2.1546286876907426e-05,
      "loss": 1.8223,
      "step": 55940
    },
    {
      "epoch": 28.4587995930824,
      "grad_norm": 45.81476593017578,
      "learning_rate": 2.15412004069176e-05,
      "loss": 1.7255,
      "step": 55950
    },
    {
      "epoch": 28.463886063072227,
      "grad_norm": 37.2373161315918,
      "learning_rate": 2.1536113936927772e-05,
      "loss": 1.8632,
      "step": 55960
    },
    {
      "epoch": 28.468972533062054,
      "grad_norm": 39.42781066894531,
      "learning_rate": 2.153102746693795e-05,
      "loss": 1.7578,
      "step": 55970
    },
    {
      "epoch": 28.47405900305188,
      "grad_norm": 38.288673400878906,
      "learning_rate": 2.152594099694812e-05,
      "loss": 1.7452,
      "step": 55980
    },
    {
      "epoch": 28.479145473041708,
      "grad_norm": 40.759185791015625,
      "learning_rate": 2.1520854526958292e-05,
      "loss": 1.7952,
      "step": 55990
    },
    {
      "epoch": 28.484231943031535,
      "grad_norm": 36.0460205078125,
      "learning_rate": 2.1515768056968465e-05,
      "loss": 1.8152,
      "step": 56000
    },
    {
      "epoch": 28.489318413021362,
      "grad_norm": 34.37900161743164,
      "learning_rate": 2.151068158697864e-05,
      "loss": 1.8325,
      "step": 56010
    },
    {
      "epoch": 28.49440488301119,
      "grad_norm": 33.34136962890625,
      "learning_rate": 2.1505595116988812e-05,
      "loss": 1.776,
      "step": 56020
    },
    {
      "epoch": 28.499491353001016,
      "grad_norm": 52.69012451171875,
      "learning_rate": 2.1500508646998985e-05,
      "loss": 1.827,
      "step": 56030
    },
    {
      "epoch": 28.504577822990843,
      "grad_norm": 46.600093841552734,
      "learning_rate": 2.1495422177009155e-05,
      "loss": 1.8288,
      "step": 56040
    },
    {
      "epoch": 28.50966429298067,
      "grad_norm": 47.64593505859375,
      "learning_rate": 2.149033570701933e-05,
      "loss": 1.8307,
      "step": 56050
    },
    {
      "epoch": 28.514750762970497,
      "grad_norm": 37.669071197509766,
      "learning_rate": 2.14852492370295e-05,
      "loss": 1.8428,
      "step": 56060
    },
    {
      "epoch": 28.519837232960327,
      "grad_norm": 35.17718505859375,
      "learning_rate": 2.1480162767039675e-05,
      "loss": 1.9295,
      "step": 56070
    },
    {
      "epoch": 28.524923702950154,
      "grad_norm": 43.578941345214844,
      "learning_rate": 2.1475076297049848e-05,
      "loss": 1.7773,
      "step": 56080
    },
    {
      "epoch": 28.53001017293998,
      "grad_norm": 33.73526382446289,
      "learning_rate": 2.146998982706002e-05,
      "loss": 1.715,
      "step": 56090
    },
    {
      "epoch": 28.53509664292981,
      "grad_norm": 35.889163970947266,
      "learning_rate": 2.146490335707019e-05,
      "loss": 1.785,
      "step": 56100
    },
    {
      "epoch": 28.540183112919635,
      "grad_norm": 46.80464172363281,
      "learning_rate": 2.1459816887080368e-05,
      "loss": 1.7938,
      "step": 56110
    },
    {
      "epoch": 28.545269582909462,
      "grad_norm": 34.48244857788086,
      "learning_rate": 2.145473041709054e-05,
      "loss": 1.8489,
      "step": 56120
    },
    {
      "epoch": 28.55035605289929,
      "grad_norm": 30.215347290039062,
      "learning_rate": 2.1449643947100715e-05,
      "loss": 1.7778,
      "step": 56130
    },
    {
      "epoch": 28.555442522889116,
      "grad_norm": 31.128768920898438,
      "learning_rate": 2.1444557477110885e-05,
      "loss": 1.7986,
      "step": 56140
    },
    {
      "epoch": 28.560528992878943,
      "grad_norm": 31.88654899597168,
      "learning_rate": 2.1439471007121058e-05,
      "loss": 1.8377,
      "step": 56150
    },
    {
      "epoch": 28.56561546286877,
      "grad_norm": 33.94419860839844,
      "learning_rate": 2.1434384537131234e-05,
      "loss": 1.7507,
      "step": 56160
    },
    {
      "epoch": 28.570701932858597,
      "grad_norm": 34.44230651855469,
      "learning_rate": 2.1429298067141404e-05,
      "loss": 1.8346,
      "step": 56170
    },
    {
      "epoch": 28.575788402848424,
      "grad_norm": 32.343868255615234,
      "learning_rate": 2.1424211597151578e-05,
      "loss": 1.8281,
      "step": 56180
    },
    {
      "epoch": 28.58087487283825,
      "grad_norm": 40.26865005493164,
      "learning_rate": 2.141912512716175e-05,
      "loss": 1.8159,
      "step": 56190
    },
    {
      "epoch": 28.585961342828078,
      "grad_norm": 45.0289192199707,
      "learning_rate": 2.1414038657171924e-05,
      "loss": 1.7576,
      "step": 56200
    },
    {
      "epoch": 28.591047812817905,
      "grad_norm": 38.8563232421875,
      "learning_rate": 2.1408952187182097e-05,
      "loss": 1.8749,
      "step": 56210
    },
    {
      "epoch": 28.596134282807732,
      "grad_norm": 37.383174896240234,
      "learning_rate": 2.140386571719227e-05,
      "loss": 1.8024,
      "step": 56220
    },
    {
      "epoch": 28.60122075279756,
      "grad_norm": 43.098960876464844,
      "learning_rate": 2.139877924720244e-05,
      "loss": 1.7544,
      "step": 56230
    },
    {
      "epoch": 28.606307222787386,
      "grad_norm": 35.36683654785156,
      "learning_rate": 2.1393692777212614e-05,
      "loss": 1.731,
      "step": 56240
    },
    {
      "epoch": 28.611393692777213,
      "grad_norm": 36.341434478759766,
      "learning_rate": 2.1388606307222787e-05,
      "loss": 1.8018,
      "step": 56250
    },
    {
      "epoch": 28.61648016276704,
      "grad_norm": 33.71934509277344,
      "learning_rate": 2.1383519837232964e-05,
      "loss": 1.7728,
      "step": 56260
    },
    {
      "epoch": 28.621566632756867,
      "grad_norm": 42.258689880371094,
      "learning_rate": 2.1378433367243134e-05,
      "loss": 1.8753,
      "step": 56270
    },
    {
      "epoch": 28.626653102746694,
      "grad_norm": 35.35713195800781,
      "learning_rate": 2.1373346897253307e-05,
      "loss": 1.7564,
      "step": 56280
    },
    {
      "epoch": 28.63173957273652,
      "grad_norm": 52.69762420654297,
      "learning_rate": 2.136826042726348e-05,
      "loss": 1.8404,
      "step": 56290
    },
    {
      "epoch": 28.636826042726348,
      "grad_norm": 48.68538284301758,
      "learning_rate": 2.1363173957273654e-05,
      "loss": 1.846,
      "step": 56300
    },
    {
      "epoch": 28.641912512716175,
      "grad_norm": 39.85748291015625,
      "learning_rate": 2.1358087487283827e-05,
      "loss": 1.8748,
      "step": 56310
    },
    {
      "epoch": 28.646998982706002,
      "grad_norm": 30.356008529663086,
      "learning_rate": 2.1353001017294e-05,
      "loss": 1.9013,
      "step": 56320
    },
    {
      "epoch": 28.65208545269583,
      "grad_norm": 38.726314544677734,
      "learning_rate": 2.134791454730417e-05,
      "loss": 1.6896,
      "step": 56330
    },
    {
      "epoch": 28.657171922685656,
      "grad_norm": 28.753145217895508,
      "learning_rate": 2.1342828077314343e-05,
      "loss": 1.8648,
      "step": 56340
    },
    {
      "epoch": 28.662258392675483,
      "grad_norm": 40.793304443359375,
      "learning_rate": 2.133774160732452e-05,
      "loss": 1.7605,
      "step": 56350
    },
    {
      "epoch": 28.66734486266531,
      "grad_norm": 40.60822677612305,
      "learning_rate": 2.133265513733469e-05,
      "loss": 1.8402,
      "step": 56360
    },
    {
      "epoch": 28.672431332655137,
      "grad_norm": 40.91518020629883,
      "learning_rate": 2.1327568667344863e-05,
      "loss": 1.8303,
      "step": 56370
    },
    {
      "epoch": 28.677517802644964,
      "grad_norm": 38.50875473022461,
      "learning_rate": 2.1322482197355036e-05,
      "loss": 1.798,
      "step": 56380
    },
    {
      "epoch": 28.68260427263479,
      "grad_norm": 48.224308013916016,
      "learning_rate": 2.131739572736521e-05,
      "loss": 1.789,
      "step": 56390
    },
    {
      "epoch": 28.687690742624618,
      "grad_norm": 32.751243591308594,
      "learning_rate": 2.1312309257375383e-05,
      "loss": 1.7763,
      "step": 56400
    },
    {
      "epoch": 28.692777212614445,
      "grad_norm": 37.04119873046875,
      "learning_rate": 2.1307222787385556e-05,
      "loss": 1.8775,
      "step": 56410
    },
    {
      "epoch": 28.69786368260427,
      "grad_norm": 40.94225311279297,
      "learning_rate": 2.130213631739573e-05,
      "loss": 1.7673,
      "step": 56420
    },
    {
      "epoch": 28.7029501525941,
      "grad_norm": 33.8138542175293,
      "learning_rate": 2.12970498474059e-05,
      "loss": 1.7918,
      "step": 56430
    },
    {
      "epoch": 28.708036622583926,
      "grad_norm": 33.73882293701172,
      "learning_rate": 2.1291963377416073e-05,
      "loss": 1.8534,
      "step": 56440
    },
    {
      "epoch": 28.713123092573753,
      "grad_norm": 38.21542739868164,
      "learning_rate": 2.128687690742625e-05,
      "loss": 1.8576,
      "step": 56450
    },
    {
      "epoch": 28.71820956256358,
      "grad_norm": 34.78572463989258,
      "learning_rate": 2.128179043743642e-05,
      "loss": 1.7158,
      "step": 56460
    },
    {
      "epoch": 28.723296032553407,
      "grad_norm": 36.88906478881836,
      "learning_rate": 2.1276703967446593e-05,
      "loss": 1.7379,
      "step": 56470
    },
    {
      "epoch": 28.728382502543234,
      "grad_norm": 35.79309844970703,
      "learning_rate": 2.1271617497456766e-05,
      "loss": 1.9057,
      "step": 56480
    },
    {
      "epoch": 28.73346897253306,
      "grad_norm": 38.150169372558594,
      "learning_rate": 2.126653102746694e-05,
      "loss": 1.8472,
      "step": 56490
    },
    {
      "epoch": 28.738555442522888,
      "grad_norm": 42.57701110839844,
      "learning_rate": 2.1261444557477112e-05,
      "loss": 1.803,
      "step": 56500
    },
    {
      "epoch": 28.743641912512714,
      "grad_norm": 40.843204498291016,
      "learning_rate": 2.1256358087487286e-05,
      "loss": 1.7971,
      "step": 56510
    },
    {
      "epoch": 28.74872838250254,
      "grad_norm": 33.807167053222656,
      "learning_rate": 2.125127161749746e-05,
      "loss": 1.885,
      "step": 56520
    },
    {
      "epoch": 28.753814852492372,
      "grad_norm": 32.012237548828125,
      "learning_rate": 2.124618514750763e-05,
      "loss": 1.8173,
      "step": 56530
    },
    {
      "epoch": 28.7589013224822,
      "grad_norm": 38.222904205322266,
      "learning_rate": 2.1241098677517802e-05,
      "loss": 1.7807,
      "step": 56540
    },
    {
      "epoch": 28.763987792472026,
      "grad_norm": 39.613792419433594,
      "learning_rate": 2.123601220752798e-05,
      "loss": 1.8593,
      "step": 56550
    },
    {
      "epoch": 28.769074262461853,
      "grad_norm": 33.68795394897461,
      "learning_rate": 2.123092573753815e-05,
      "loss": 1.8316,
      "step": 56560
    },
    {
      "epoch": 28.77416073245168,
      "grad_norm": 30.762365341186523,
      "learning_rate": 2.1225839267548322e-05,
      "loss": 1.8138,
      "step": 56570
    },
    {
      "epoch": 28.779247202441507,
      "grad_norm": 39.453582763671875,
      "learning_rate": 2.1220752797558495e-05,
      "loss": 1.8007,
      "step": 56580
    },
    {
      "epoch": 28.784333672431334,
      "grad_norm": 31.63071060180664,
      "learning_rate": 2.121566632756867e-05,
      "loss": 1.7733,
      "step": 56590
    },
    {
      "epoch": 28.78942014242116,
      "grad_norm": 33.77704620361328,
      "learning_rate": 2.1210579857578842e-05,
      "loss": 1.8466,
      "step": 56600
    },
    {
      "epoch": 28.794506612410988,
      "grad_norm": 41.042625427246094,
      "learning_rate": 2.1205493387589015e-05,
      "loss": 1.7908,
      "step": 56610
    },
    {
      "epoch": 28.799593082400815,
      "grad_norm": 42.06964111328125,
      "learning_rate": 2.1200406917599185e-05,
      "loss": 1.8484,
      "step": 56620
    },
    {
      "epoch": 28.804679552390642,
      "grad_norm": 29.387365341186523,
      "learning_rate": 2.119532044760936e-05,
      "loss": 1.8703,
      "step": 56630
    },
    {
      "epoch": 28.80976602238047,
      "grad_norm": 35.38336944580078,
      "learning_rate": 2.1190233977619535e-05,
      "loss": 1.7584,
      "step": 56640
    },
    {
      "epoch": 28.814852492370296,
      "grad_norm": 31.3781681060791,
      "learning_rate": 2.118514750762971e-05,
      "loss": 1.7682,
      "step": 56650
    },
    {
      "epoch": 28.819938962360123,
      "grad_norm": 37.34095764160156,
      "learning_rate": 2.1180061037639878e-05,
      "loss": 1.8892,
      "step": 56660
    },
    {
      "epoch": 28.82502543234995,
      "grad_norm": 41.859588623046875,
      "learning_rate": 2.117497456765005e-05,
      "loss": 1.7775,
      "step": 56670
    },
    {
      "epoch": 28.830111902339777,
      "grad_norm": 35.70215606689453,
      "learning_rate": 2.1169888097660225e-05,
      "loss": 1.7443,
      "step": 56680
    },
    {
      "epoch": 28.835198372329604,
      "grad_norm": 38.39647674560547,
      "learning_rate": 2.1164801627670398e-05,
      "loss": 1.8904,
      "step": 56690
    },
    {
      "epoch": 28.84028484231943,
      "grad_norm": 30.127403259277344,
      "learning_rate": 2.115971515768057e-05,
      "loss": 1.7975,
      "step": 56700
    },
    {
      "epoch": 28.845371312309258,
      "grad_norm": 30.884050369262695,
      "learning_rate": 2.1154628687690745e-05,
      "loss": 1.827,
      "step": 56710
    },
    {
      "epoch": 28.850457782299085,
      "grad_norm": 41.448665618896484,
      "learning_rate": 2.1149542217700915e-05,
      "loss": 1.8463,
      "step": 56720
    },
    {
      "epoch": 28.85554425228891,
      "grad_norm": 32.7889404296875,
      "learning_rate": 2.1144455747711088e-05,
      "loss": 1.8141,
      "step": 56730
    },
    {
      "epoch": 28.86063072227874,
      "grad_norm": 31.253822326660156,
      "learning_rate": 2.1139369277721264e-05,
      "loss": 1.8096,
      "step": 56740
    },
    {
      "epoch": 28.865717192268566,
      "grad_norm": 41.14095687866211,
      "learning_rate": 2.1134282807731434e-05,
      "loss": 1.8522,
      "step": 56750
    },
    {
      "epoch": 28.870803662258393,
      "grad_norm": 46.2357177734375,
      "learning_rate": 2.1129196337741608e-05,
      "loss": 1.729,
      "step": 56760
    },
    {
      "epoch": 28.87589013224822,
      "grad_norm": 33.74618148803711,
      "learning_rate": 2.112410986775178e-05,
      "loss": 1.7991,
      "step": 56770
    },
    {
      "epoch": 28.880976602238047,
      "grad_norm": 43.551753997802734,
      "learning_rate": 2.1119023397761954e-05,
      "loss": 1.7325,
      "step": 56780
    },
    {
      "epoch": 28.886063072227874,
      "grad_norm": 35.92081832885742,
      "learning_rate": 2.1113936927772127e-05,
      "loss": 1.8072,
      "step": 56790
    },
    {
      "epoch": 28.8911495422177,
      "grad_norm": 35.504886627197266,
      "learning_rate": 2.11088504577823e-05,
      "loss": 1.822,
      "step": 56800
    },
    {
      "epoch": 28.896236012207527,
      "grad_norm": 39.945945739746094,
      "learning_rate": 2.1103763987792474e-05,
      "loss": 1.8022,
      "step": 56810
    },
    {
      "epoch": 28.901322482197354,
      "grad_norm": 43.220359802246094,
      "learning_rate": 2.1098677517802644e-05,
      "loss": 1.7098,
      "step": 56820
    },
    {
      "epoch": 28.90640895218718,
      "grad_norm": 38.88615798950195,
      "learning_rate": 2.109359104781282e-05,
      "loss": 1.8802,
      "step": 56830
    },
    {
      "epoch": 28.91149542217701,
      "grad_norm": 35.86853790283203,
      "learning_rate": 2.1088504577822994e-05,
      "loss": 1.7651,
      "step": 56840
    },
    {
      "epoch": 28.916581892166835,
      "grad_norm": 36.93556213378906,
      "learning_rate": 2.1083418107833164e-05,
      "loss": 1.8258,
      "step": 56850
    },
    {
      "epoch": 28.921668362156662,
      "grad_norm": 38.65652847290039,
      "learning_rate": 2.1078331637843337e-05,
      "loss": 1.8614,
      "step": 56860
    },
    {
      "epoch": 28.92675483214649,
      "grad_norm": 38.808143615722656,
      "learning_rate": 2.107324516785351e-05,
      "loss": 1.8904,
      "step": 56870
    },
    {
      "epoch": 28.931841302136316,
      "grad_norm": 34.44149398803711,
      "learning_rate": 2.1068158697863684e-05,
      "loss": 1.753,
      "step": 56880
    },
    {
      "epoch": 28.936927772126143,
      "grad_norm": 35.848209381103516,
      "learning_rate": 2.1063072227873857e-05,
      "loss": 1.7686,
      "step": 56890
    },
    {
      "epoch": 28.94201424211597,
      "grad_norm": 34.98648452758789,
      "learning_rate": 2.105798575788403e-05,
      "loss": 1.826,
      "step": 56900
    },
    {
      "epoch": 28.947100712105797,
      "grad_norm": 59.04232406616211,
      "learning_rate": 2.10528992878942e-05,
      "loss": 1.823,
      "step": 56910
    },
    {
      "epoch": 28.952187182095624,
      "grad_norm": 33.25230407714844,
      "learning_rate": 2.1047812817904373e-05,
      "loss": 1.6866,
      "step": 56920
    },
    {
      "epoch": 28.95727365208545,
      "grad_norm": 36.27031707763672,
      "learning_rate": 2.104272634791455e-05,
      "loss": 1.9404,
      "step": 56930
    },
    {
      "epoch": 28.962360122075278,
      "grad_norm": 29.622907638549805,
      "learning_rate": 2.1037639877924723e-05,
      "loss": 1.8975,
      "step": 56940
    },
    {
      "epoch": 28.967446592065105,
      "grad_norm": 32.429752349853516,
      "learning_rate": 2.1032553407934893e-05,
      "loss": 1.8343,
      "step": 56950
    },
    {
      "epoch": 28.972533062054932,
      "grad_norm": 31.9429988861084,
      "learning_rate": 2.1027466937945067e-05,
      "loss": 1.783,
      "step": 56960
    },
    {
      "epoch": 28.977619532044763,
      "grad_norm": 42.8699951171875,
      "learning_rate": 2.102238046795524e-05,
      "loss": 1.7441,
      "step": 56970
    },
    {
      "epoch": 28.98270600203459,
      "grad_norm": 32.852081298828125,
      "learning_rate": 2.1017293997965413e-05,
      "loss": 1.8722,
      "step": 56980
    },
    {
      "epoch": 28.987792472024417,
      "grad_norm": 34.96666717529297,
      "learning_rate": 2.1012207527975586e-05,
      "loss": 1.8288,
      "step": 56990
    },
    {
      "epoch": 28.992878942014244,
      "grad_norm": 33.45086669921875,
      "learning_rate": 2.100712105798576e-05,
      "loss": 1.8475,
      "step": 57000
    },
    {
      "epoch": 28.99796541200407,
      "grad_norm": 40.14979553222656,
      "learning_rate": 2.100203458799593e-05,
      "loss": 1.756,
      "step": 57010
    },
    {
      "epoch": 29.0,
      "eval_loss": 4.740759372711182,
      "eval_runtime": 2.7193,
      "eval_samples_per_second": 1020.479,
      "eval_steps_per_second": 127.606,
      "step": 57014
    },
    {
      "epoch": 29.003051881993898,
      "grad_norm": 34.703433990478516,
      "learning_rate": 2.0996948118006106e-05,
      "loss": 1.8082,
      "step": 57020
    },
    {
      "epoch": 29.008138351983725,
      "grad_norm": 46.987735748291016,
      "learning_rate": 2.099186164801628e-05,
      "loss": 1.777,
      "step": 57030
    },
    {
      "epoch": 29.01322482197355,
      "grad_norm": 33.8441162109375,
      "learning_rate": 2.098677517802645e-05,
      "loss": 1.8778,
      "step": 57040
    },
    {
      "epoch": 29.01831129196338,
      "grad_norm": 39.985591888427734,
      "learning_rate": 2.0981688708036623e-05,
      "loss": 1.7828,
      "step": 57050
    },
    {
      "epoch": 29.023397761953206,
      "grad_norm": 37.28425979614258,
      "learning_rate": 2.0976602238046796e-05,
      "loss": 1.7053,
      "step": 57060
    },
    {
      "epoch": 29.028484231943033,
      "grad_norm": 39.630001068115234,
      "learning_rate": 2.097151576805697e-05,
      "loss": 1.8356,
      "step": 57070
    },
    {
      "epoch": 29.03357070193286,
      "grad_norm": 33.832881927490234,
      "learning_rate": 2.0966429298067142e-05,
      "loss": 1.6583,
      "step": 57080
    },
    {
      "epoch": 29.038657171922686,
      "grad_norm": 43.115962982177734,
      "learning_rate": 2.0961342828077316e-05,
      "loss": 1.8498,
      "step": 57090
    },
    {
      "epoch": 29.043743641912513,
      "grad_norm": 36.627845764160156,
      "learning_rate": 2.095625635808749e-05,
      "loss": 1.8039,
      "step": 57100
    },
    {
      "epoch": 29.04883011190234,
      "grad_norm": 33.76409149169922,
      "learning_rate": 2.095116988809766e-05,
      "loss": 1.7112,
      "step": 57110
    },
    {
      "epoch": 29.053916581892167,
      "grad_norm": 37.626914978027344,
      "learning_rate": 2.0946083418107836e-05,
      "loss": 1.8371,
      "step": 57120
    },
    {
      "epoch": 29.059003051881994,
      "grad_norm": 43.18842315673828,
      "learning_rate": 2.094099694811801e-05,
      "loss": 1.759,
      "step": 57130
    },
    {
      "epoch": 29.06408952187182,
      "grad_norm": 43.03972244262695,
      "learning_rate": 2.093591047812818e-05,
      "loss": 1.8496,
      "step": 57140
    },
    {
      "epoch": 29.06917599186165,
      "grad_norm": 43.180824279785156,
      "learning_rate": 2.0930824008138352e-05,
      "loss": 1.8347,
      "step": 57150
    },
    {
      "epoch": 29.074262461851475,
      "grad_norm": 47.37449645996094,
      "learning_rate": 2.0925737538148525e-05,
      "loss": 1.8312,
      "step": 57160
    },
    {
      "epoch": 29.079348931841302,
      "grad_norm": 40.75994873046875,
      "learning_rate": 2.09206510681587e-05,
      "loss": 1.7931,
      "step": 57170
    },
    {
      "epoch": 29.08443540183113,
      "grad_norm": 40.38997268676758,
      "learning_rate": 2.0915564598168872e-05,
      "loss": 1.7537,
      "step": 57180
    },
    {
      "epoch": 29.089521871820956,
      "grad_norm": 34.66210174560547,
      "learning_rate": 2.0910478128179045e-05,
      "loss": 1.8041,
      "step": 57190
    },
    {
      "epoch": 29.094608341810783,
      "grad_norm": 34.23297119140625,
      "learning_rate": 2.090539165818922e-05,
      "loss": 1.7707,
      "step": 57200
    },
    {
      "epoch": 29.09969481180061,
      "grad_norm": 36.0770149230957,
      "learning_rate": 2.090030518819939e-05,
      "loss": 1.7521,
      "step": 57210
    },
    {
      "epoch": 29.104781281790437,
      "grad_norm": 36.120845794677734,
      "learning_rate": 2.0895218718209565e-05,
      "loss": 1.8132,
      "step": 57220
    },
    {
      "epoch": 29.109867751780264,
      "grad_norm": 34.539039611816406,
      "learning_rate": 2.089013224821974e-05,
      "loss": 1.782,
      "step": 57230
    },
    {
      "epoch": 29.11495422177009,
      "grad_norm": 34.3187370300293,
      "learning_rate": 2.0885045778229908e-05,
      "loss": 1.7783,
      "step": 57240
    },
    {
      "epoch": 29.120040691759918,
      "grad_norm": 40.61684799194336,
      "learning_rate": 2.087995930824008e-05,
      "loss": 1.7814,
      "step": 57250
    },
    {
      "epoch": 29.125127161749745,
      "grad_norm": 52.74679183959961,
      "learning_rate": 2.0874872838250255e-05,
      "loss": 1.8076,
      "step": 57260
    },
    {
      "epoch": 29.130213631739572,
      "grad_norm": 43.15544509887695,
      "learning_rate": 2.0869786368260428e-05,
      "loss": 1.7928,
      "step": 57270
    },
    {
      "epoch": 29.1353001017294,
      "grad_norm": 33.924537658691406,
      "learning_rate": 2.08646998982706e-05,
      "loss": 1.7716,
      "step": 57280
    },
    {
      "epoch": 29.140386571719226,
      "grad_norm": 46.59196472167969,
      "learning_rate": 2.0859613428280775e-05,
      "loss": 1.7941,
      "step": 57290
    },
    {
      "epoch": 29.145473041709053,
      "grad_norm": 41.929866790771484,
      "learning_rate": 2.0854526958290945e-05,
      "loss": 1.7533,
      "step": 57300
    },
    {
      "epoch": 29.15055951169888,
      "grad_norm": 31.404573440551758,
      "learning_rate": 2.084944048830112e-05,
      "loss": 1.6914,
      "step": 57310
    },
    {
      "epoch": 29.155645981688707,
      "grad_norm": 40.44366455078125,
      "learning_rate": 2.0844354018311294e-05,
      "loss": 1.7668,
      "step": 57320
    },
    {
      "epoch": 29.160732451678534,
      "grad_norm": 32.5046272277832,
      "learning_rate": 2.0839267548321468e-05,
      "loss": 1.7876,
      "step": 57330
    },
    {
      "epoch": 29.16581892166836,
      "grad_norm": 53.81498718261719,
      "learning_rate": 2.0834181078331638e-05,
      "loss": 1.7777,
      "step": 57340
    },
    {
      "epoch": 29.170905391658188,
      "grad_norm": 51.932456970214844,
      "learning_rate": 2.082909460834181e-05,
      "loss": 1.9016,
      "step": 57350
    },
    {
      "epoch": 29.175991861648015,
      "grad_norm": 41.13242721557617,
      "learning_rate": 2.0824008138351984e-05,
      "loss": 1.8332,
      "step": 57360
    },
    {
      "epoch": 29.181078331637842,
      "grad_norm": 44.73500442504883,
      "learning_rate": 2.0818921668362158e-05,
      "loss": 1.7325,
      "step": 57370
    },
    {
      "epoch": 29.18616480162767,
      "grad_norm": 40.10648727416992,
      "learning_rate": 2.081383519837233e-05,
      "loss": 1.7649,
      "step": 57380
    },
    {
      "epoch": 29.191251271617496,
      "grad_norm": 34.75294494628906,
      "learning_rate": 2.0808748728382504e-05,
      "loss": 1.7845,
      "step": 57390
    },
    {
      "epoch": 29.196337741607323,
      "grad_norm": 35.00023651123047,
      "learning_rate": 2.0803662258392674e-05,
      "loss": 1.8457,
      "step": 57400
    },
    {
      "epoch": 29.20142421159715,
      "grad_norm": 35.547298431396484,
      "learning_rate": 2.079857578840285e-05,
      "loss": 1.7707,
      "step": 57410
    },
    {
      "epoch": 29.20651068158698,
      "grad_norm": 38.4365348815918,
      "learning_rate": 2.0793489318413024e-05,
      "loss": 1.7555,
      "step": 57420
    },
    {
      "epoch": 29.211597151576807,
      "grad_norm": 42.91907501220703,
      "learning_rate": 2.0788402848423194e-05,
      "loss": 1.7482,
      "step": 57430
    },
    {
      "epoch": 29.216683621566634,
      "grad_norm": 37.857269287109375,
      "learning_rate": 2.0783316378433367e-05,
      "loss": 1.704,
      "step": 57440
    },
    {
      "epoch": 29.22177009155646,
      "grad_norm": 46.42882537841797,
      "learning_rate": 2.077822990844354e-05,
      "loss": 1.7472,
      "step": 57450
    },
    {
      "epoch": 29.22685656154629,
      "grad_norm": 44.216575622558594,
      "learning_rate": 2.0773143438453717e-05,
      "loss": 1.7837,
      "step": 57460
    },
    {
      "epoch": 29.231943031536115,
      "grad_norm": 36.86673355102539,
      "learning_rate": 2.0768056968463887e-05,
      "loss": 1.7985,
      "step": 57470
    },
    {
      "epoch": 29.237029501525942,
      "grad_norm": 49.10420608520508,
      "learning_rate": 2.076297049847406e-05,
      "loss": 1.8,
      "step": 57480
    },
    {
      "epoch": 29.24211597151577,
      "grad_norm": 47.0205192565918,
      "learning_rate": 2.0757884028484233e-05,
      "loss": 1.7173,
      "step": 57490
    },
    {
      "epoch": 29.247202441505596,
      "grad_norm": 35.607723236083984,
      "learning_rate": 2.0752797558494407e-05,
      "loss": 1.7866,
      "step": 57500
    },
    {
      "epoch": 29.252288911495423,
      "grad_norm": 36.8076057434082,
      "learning_rate": 2.074771108850458e-05,
      "loss": 1.7399,
      "step": 57510
    },
    {
      "epoch": 29.25737538148525,
      "grad_norm": 32.9122314453125,
      "learning_rate": 2.0742624618514753e-05,
      "loss": 1.819,
      "step": 57520
    },
    {
      "epoch": 29.262461851475077,
      "grad_norm": 37.2904052734375,
      "learning_rate": 2.0737538148524923e-05,
      "loss": 1.7949,
      "step": 57530
    },
    {
      "epoch": 29.267548321464904,
      "grad_norm": 31.427474975585938,
      "learning_rate": 2.0732451678535097e-05,
      "loss": 1.7713,
      "step": 57540
    },
    {
      "epoch": 29.27263479145473,
      "grad_norm": 34.991031646728516,
      "learning_rate": 2.072736520854527e-05,
      "loss": 1.8032,
      "step": 57550
    },
    {
      "epoch": 29.277721261444558,
      "grad_norm": 37.99468994140625,
      "learning_rate": 2.0722278738555443e-05,
      "loss": 1.7902,
      "step": 57560
    },
    {
      "epoch": 29.282807731434385,
      "grad_norm": 41.202613830566406,
      "learning_rate": 2.0717192268565616e-05,
      "loss": 1.6917,
      "step": 57570
    },
    {
      "epoch": 29.287894201424212,
      "grad_norm": 31.336849212646484,
      "learning_rate": 2.071210579857579e-05,
      "loss": 1.7852,
      "step": 57580
    },
    {
      "epoch": 29.29298067141404,
      "grad_norm": 36.74501037597656,
      "learning_rate": 2.0707019328585963e-05,
      "loss": 1.7502,
      "step": 57590
    },
    {
      "epoch": 29.298067141403866,
      "grad_norm": 34.62382507324219,
      "learning_rate": 2.0701932858596136e-05,
      "loss": 1.8571,
      "step": 57600
    },
    {
      "epoch": 29.303153611393693,
      "grad_norm": 36.631011962890625,
      "learning_rate": 2.069684638860631e-05,
      "loss": 1.8315,
      "step": 57610
    },
    {
      "epoch": 29.30824008138352,
      "grad_norm": 32.696937561035156,
      "learning_rate": 2.0691759918616483e-05,
      "loss": 1.876,
      "step": 57620
    },
    {
      "epoch": 29.313326551373347,
      "grad_norm": 36.8185920715332,
      "learning_rate": 2.0686673448626653e-05,
      "loss": 1.7445,
      "step": 57630
    },
    {
      "epoch": 29.318413021363174,
      "grad_norm": 45.72298049926758,
      "learning_rate": 2.0681586978636826e-05,
      "loss": 1.8212,
      "step": 57640
    },
    {
      "epoch": 29.323499491353,
      "grad_norm": 31.551103591918945,
      "learning_rate": 2.0676500508647003e-05,
      "loss": 1.7335,
      "step": 57650
    },
    {
      "epoch": 29.328585961342828,
      "grad_norm": 39.210296630859375,
      "learning_rate": 2.0671414038657173e-05,
      "loss": 1.8125,
      "step": 57660
    },
    {
      "epoch": 29.333672431332655,
      "grad_norm": 33.81834030151367,
      "learning_rate": 2.0666327568667346e-05,
      "loss": 1.7291,
      "step": 57670
    },
    {
      "epoch": 29.338758901322482,
      "grad_norm": 42.50006866455078,
      "learning_rate": 2.066124109867752e-05,
      "loss": 1.7918,
      "step": 57680
    },
    {
      "epoch": 29.34384537131231,
      "grad_norm": 47.69288635253906,
      "learning_rate": 2.065615462868769e-05,
      "loss": 1.7825,
      "step": 57690
    },
    {
      "epoch": 29.348931841302136,
      "grad_norm": 32.19041061401367,
      "learning_rate": 2.0651068158697866e-05,
      "loss": 1.6973,
      "step": 57700
    },
    {
      "epoch": 29.354018311291963,
      "grad_norm": 35.91337966918945,
      "learning_rate": 2.064598168870804e-05,
      "loss": 1.8405,
      "step": 57710
    },
    {
      "epoch": 29.35910478128179,
      "grad_norm": 32.84614944458008,
      "learning_rate": 2.064089521871821e-05,
      "loss": 1.8257,
      "step": 57720
    },
    {
      "epoch": 29.364191251271617,
      "grad_norm": 42.12654113769531,
      "learning_rate": 2.0635808748728382e-05,
      "loss": 1.8093,
      "step": 57730
    },
    {
      "epoch": 29.369277721261444,
      "grad_norm": 42.53580093383789,
      "learning_rate": 2.0630722278738555e-05,
      "loss": 1.8702,
      "step": 57740
    },
    {
      "epoch": 29.37436419125127,
      "grad_norm": 47.25734329223633,
      "learning_rate": 2.0625635808748732e-05,
      "loss": 1.8383,
      "step": 57750
    },
    {
      "epoch": 29.379450661241098,
      "grad_norm": 34.20423889160156,
      "learning_rate": 2.0620549338758902e-05,
      "loss": 1.8108,
      "step": 57760
    },
    {
      "epoch": 29.384537131230925,
      "grad_norm": 46.610347747802734,
      "learning_rate": 2.0615462868769075e-05,
      "loss": 1.7975,
      "step": 57770
    },
    {
      "epoch": 29.38962360122075,
      "grad_norm": 35.10329055786133,
      "learning_rate": 2.061037639877925e-05,
      "loss": 1.8252,
      "step": 57780
    },
    {
      "epoch": 29.39471007121058,
      "grad_norm": 38.67566680908203,
      "learning_rate": 2.0605289928789422e-05,
      "loss": 1.8034,
      "step": 57790
    },
    {
      "epoch": 29.399796541200406,
      "grad_norm": 36.168582916259766,
      "learning_rate": 2.0600203458799595e-05,
      "loss": 1.798,
      "step": 57800
    },
    {
      "epoch": 29.404883011190233,
      "grad_norm": 40.92615509033203,
      "learning_rate": 2.059511698880977e-05,
      "loss": 1.8114,
      "step": 57810
    },
    {
      "epoch": 29.40996948118006,
      "grad_norm": 37.614742279052734,
      "learning_rate": 2.0590030518819938e-05,
      "loss": 1.8977,
      "step": 57820
    },
    {
      "epoch": 29.415055951169887,
      "grad_norm": 36.20222854614258,
      "learning_rate": 2.058494404883011e-05,
      "loss": 1.7206,
      "step": 57830
    },
    {
      "epoch": 29.420142421159714,
      "grad_norm": 30.592649459838867,
      "learning_rate": 2.0579857578840285e-05,
      "loss": 1.73,
      "step": 57840
    },
    {
      "epoch": 29.42522889114954,
      "grad_norm": 45.140567779541016,
      "learning_rate": 2.0574771108850458e-05,
      "loss": 1.8186,
      "step": 57850
    },
    {
      "epoch": 29.43031536113937,
      "grad_norm": 33.564430236816406,
      "learning_rate": 2.056968463886063e-05,
      "loss": 1.8591,
      "step": 57860
    },
    {
      "epoch": 29.435401831129198,
      "grad_norm": 33.5476188659668,
      "learning_rate": 2.0564598168870805e-05,
      "loss": 1.7337,
      "step": 57870
    },
    {
      "epoch": 29.440488301119025,
      "grad_norm": 46.734745025634766,
      "learning_rate": 2.0559511698880978e-05,
      "loss": 1.8291,
      "step": 57880
    },
    {
      "epoch": 29.445574771108852,
      "grad_norm": 34.32332992553711,
      "learning_rate": 2.055442522889115e-05,
      "loss": 1.7605,
      "step": 57890
    },
    {
      "epoch": 29.45066124109868,
      "grad_norm": 41.1474609375,
      "learning_rate": 2.0549338758901324e-05,
      "loss": 1.749,
      "step": 57900
    },
    {
      "epoch": 29.455747711088506,
      "grad_norm": 45.93611145019531,
      "learning_rate": 2.0544252288911498e-05,
      "loss": 1.7558,
      "step": 57910
    },
    {
      "epoch": 29.460834181078333,
      "grad_norm": 36.052425384521484,
      "learning_rate": 2.0539165818921668e-05,
      "loss": 1.7507,
      "step": 57920
    },
    {
      "epoch": 29.46592065106816,
      "grad_norm": 50.83965301513672,
      "learning_rate": 2.053407934893184e-05,
      "loss": 1.7476,
      "step": 57930
    },
    {
      "epoch": 29.471007121057987,
      "grad_norm": 48.419864654541016,
      "learning_rate": 2.0528992878942018e-05,
      "loss": 1.802,
      "step": 57940
    },
    {
      "epoch": 29.476093591047814,
      "grad_norm": 34.75941848754883,
      "learning_rate": 2.0523906408952188e-05,
      "loss": 1.8511,
      "step": 57950
    },
    {
      "epoch": 29.48118006103764,
      "grad_norm": 30.811635971069336,
      "learning_rate": 2.051881993896236e-05,
      "loss": 1.907,
      "step": 57960
    },
    {
      "epoch": 29.486266531027468,
      "grad_norm": 36.4294319152832,
      "learning_rate": 2.0513733468972534e-05,
      "loss": 1.7129,
      "step": 57970
    },
    {
      "epoch": 29.491353001017295,
      "grad_norm": 45.76607894897461,
      "learning_rate": 2.0508646998982707e-05,
      "loss": 1.7593,
      "step": 57980
    },
    {
      "epoch": 29.496439471007122,
      "grad_norm": 42.94731140136719,
      "learning_rate": 2.050356052899288e-05,
      "loss": 1.8388,
      "step": 57990
    },
    {
      "epoch": 29.50152594099695,
      "grad_norm": 35.200836181640625,
      "learning_rate": 2.0498474059003054e-05,
      "loss": 1.798,
      "step": 58000
    },
    {
      "epoch": 29.506612410986776,
      "grad_norm": 35.832698822021484,
      "learning_rate": 2.0493387589013227e-05,
      "loss": 1.6998,
      "step": 58010
    },
    {
      "epoch": 29.511698880976603,
      "grad_norm": 40.48236083984375,
      "learning_rate": 2.0488301119023397e-05,
      "loss": 1.7378,
      "step": 58020
    },
    {
      "epoch": 29.51678535096643,
      "grad_norm": 40.799957275390625,
      "learning_rate": 2.048321464903357e-05,
      "loss": 1.8215,
      "step": 58030
    },
    {
      "epoch": 29.521871820956257,
      "grad_norm": 35.05455780029297,
      "learning_rate": 2.0478128179043747e-05,
      "loss": 1.7207,
      "step": 58040
    },
    {
      "epoch": 29.526958290946084,
      "grad_norm": 40.48691940307617,
      "learning_rate": 2.0473041709053917e-05,
      "loss": 1.8053,
      "step": 58050
    },
    {
      "epoch": 29.53204476093591,
      "grad_norm": 42.17864227294922,
      "learning_rate": 2.046795523906409e-05,
      "loss": 1.8176,
      "step": 58060
    },
    {
      "epoch": 29.537131230925738,
      "grad_norm": 46.39002990722656,
      "learning_rate": 2.0462868769074263e-05,
      "loss": 1.8814,
      "step": 58070
    },
    {
      "epoch": 29.542217700915565,
      "grad_norm": 43.66688919067383,
      "learning_rate": 2.0457782299084437e-05,
      "loss": 1.8005,
      "step": 58080
    },
    {
      "epoch": 29.54730417090539,
      "grad_norm": 41.47979736328125,
      "learning_rate": 2.045269582909461e-05,
      "loss": 1.7233,
      "step": 58090
    },
    {
      "epoch": 29.55239064089522,
      "grad_norm": 35.556617736816406,
      "learning_rate": 2.0447609359104783e-05,
      "loss": 1.7484,
      "step": 58100
    },
    {
      "epoch": 29.557477110885046,
      "grad_norm": 33.17704772949219,
      "learning_rate": 2.0442522889114953e-05,
      "loss": 1.8681,
      "step": 58110
    },
    {
      "epoch": 29.562563580874873,
      "grad_norm": 34.928192138671875,
      "learning_rate": 2.0437436419125127e-05,
      "loss": 1.8566,
      "step": 58120
    },
    {
      "epoch": 29.5676500508647,
      "grad_norm": 40.897972106933594,
      "learning_rate": 2.0432349949135303e-05,
      "loss": 1.7891,
      "step": 58130
    },
    {
      "epoch": 29.572736520854527,
      "grad_norm": 37.30601501464844,
      "learning_rate": 2.0427263479145476e-05,
      "loss": 1.8553,
      "step": 58140
    },
    {
      "epoch": 29.577822990844354,
      "grad_norm": 32.40644454956055,
      "learning_rate": 2.0422177009155646e-05,
      "loss": 1.6836,
      "step": 58150
    },
    {
      "epoch": 29.58290946083418,
      "grad_norm": 36.47860336303711,
      "learning_rate": 2.041709053916582e-05,
      "loss": 1.8452,
      "step": 58160
    },
    {
      "epoch": 29.587995930824007,
      "grad_norm": 45.26914596557617,
      "learning_rate": 2.0412004069175993e-05,
      "loss": 1.8781,
      "step": 58170
    },
    {
      "epoch": 29.593082400813834,
      "grad_norm": 38.4191780090332,
      "learning_rate": 2.0406917599186166e-05,
      "loss": 1.8196,
      "step": 58180
    },
    {
      "epoch": 29.59816887080366,
      "grad_norm": 39.349300384521484,
      "learning_rate": 2.040183112919634e-05,
      "loss": 1.8611,
      "step": 58190
    },
    {
      "epoch": 29.60325534079349,
      "grad_norm": 35.3543815612793,
      "learning_rate": 2.0396744659206513e-05,
      "loss": 1.7526,
      "step": 58200
    },
    {
      "epoch": 29.608341810783315,
      "grad_norm": 50.239471435546875,
      "learning_rate": 2.0391658189216683e-05,
      "loss": 1.8254,
      "step": 58210
    },
    {
      "epoch": 29.613428280773142,
      "grad_norm": 37.58160400390625,
      "learning_rate": 2.0386571719226856e-05,
      "loss": 1.7592,
      "step": 58220
    },
    {
      "epoch": 29.61851475076297,
      "grad_norm": 34.95154571533203,
      "learning_rate": 2.0381485249237033e-05,
      "loss": 1.8509,
      "step": 58230
    },
    {
      "epoch": 29.623601220752796,
      "grad_norm": 36.907535552978516,
      "learning_rate": 2.0376398779247203e-05,
      "loss": 1.7563,
      "step": 58240
    },
    {
      "epoch": 29.628687690742623,
      "grad_norm": 42.401763916015625,
      "learning_rate": 2.0371312309257376e-05,
      "loss": 1.7768,
      "step": 58250
    },
    {
      "epoch": 29.63377416073245,
      "grad_norm": 37.44890213012695,
      "learning_rate": 2.036622583926755e-05,
      "loss": 1.7865,
      "step": 58260
    },
    {
      "epoch": 29.638860630722277,
      "grad_norm": 41.66810989379883,
      "learning_rate": 2.0361139369277722e-05,
      "loss": 1.8537,
      "step": 58270
    },
    {
      "epoch": 29.643947100712104,
      "grad_norm": 49.75593566894531,
      "learning_rate": 2.0356052899287896e-05,
      "loss": 1.7529,
      "step": 58280
    },
    {
      "epoch": 29.64903357070193,
      "grad_norm": 37.39274597167969,
      "learning_rate": 2.035096642929807e-05,
      "loss": 1.8309,
      "step": 58290
    },
    {
      "epoch": 29.654120040691758,
      "grad_norm": 37.90442657470703,
      "learning_rate": 2.0345879959308242e-05,
      "loss": 1.8405,
      "step": 58300
    },
    {
      "epoch": 29.659206510681585,
      "grad_norm": 35.14435958862305,
      "learning_rate": 2.0340793489318412e-05,
      "loss": 1.7236,
      "step": 58310
    },
    {
      "epoch": 29.664292980671416,
      "grad_norm": 34.27296447753906,
      "learning_rate": 2.0335707019328585e-05,
      "loss": 1.67,
      "step": 58320
    },
    {
      "epoch": 29.669379450661243,
      "grad_norm": 45.117706298828125,
      "learning_rate": 2.0330620549338762e-05,
      "loss": 1.836,
      "step": 58330
    },
    {
      "epoch": 29.67446592065107,
      "grad_norm": 36.753143310546875,
      "learning_rate": 2.0325534079348932e-05,
      "loss": 1.6978,
      "step": 58340
    },
    {
      "epoch": 29.679552390640897,
      "grad_norm": 31.680856704711914,
      "learning_rate": 2.0320447609359105e-05,
      "loss": 1.8603,
      "step": 58350
    },
    {
      "epoch": 29.684638860630724,
      "grad_norm": 38.490379333496094,
      "learning_rate": 2.031536113936928e-05,
      "loss": 1.763,
      "step": 58360
    },
    {
      "epoch": 29.68972533062055,
      "grad_norm": 37.275577545166016,
      "learning_rate": 2.0310274669379452e-05,
      "loss": 1.8255,
      "step": 58370
    },
    {
      "epoch": 29.694811800610378,
      "grad_norm": 38.0830192565918,
      "learning_rate": 2.0305188199389625e-05,
      "loss": 1.7633,
      "step": 58380
    },
    {
      "epoch": 29.699898270600205,
      "grad_norm": 43.63587188720703,
      "learning_rate": 2.03001017293998e-05,
      "loss": 1.7292,
      "step": 58390
    },
    {
      "epoch": 29.70498474059003,
      "grad_norm": 44.60235595703125,
      "learning_rate": 2.029501525940997e-05,
      "loss": 1.8178,
      "step": 58400
    },
    {
      "epoch": 29.71007121057986,
      "grad_norm": 34.21233367919922,
      "learning_rate": 2.028992878942014e-05,
      "loss": 1.865,
      "step": 58410
    },
    {
      "epoch": 29.715157680569686,
      "grad_norm": 40.47705841064453,
      "learning_rate": 2.0284842319430318e-05,
      "loss": 1.7577,
      "step": 58420
    },
    {
      "epoch": 29.720244150559513,
      "grad_norm": 36.61079406738281,
      "learning_rate": 2.027975584944049e-05,
      "loss": 1.916,
      "step": 58430
    },
    {
      "epoch": 29.72533062054934,
      "grad_norm": 42.084129333496094,
      "learning_rate": 2.027466937945066e-05,
      "loss": 1.8348,
      "step": 58440
    },
    {
      "epoch": 29.730417090539166,
      "grad_norm": 38.87647247314453,
      "learning_rate": 2.0269582909460835e-05,
      "loss": 1.7664,
      "step": 58450
    },
    {
      "epoch": 29.735503560528993,
      "grad_norm": 42.80237579345703,
      "learning_rate": 2.0264496439471008e-05,
      "loss": 1.8051,
      "step": 58460
    },
    {
      "epoch": 29.74059003051882,
      "grad_norm": 45.574676513671875,
      "learning_rate": 2.025940996948118e-05,
      "loss": 1.7378,
      "step": 58470
    },
    {
      "epoch": 29.745676500508647,
      "grad_norm": 35.0992317199707,
      "learning_rate": 2.0254323499491354e-05,
      "loss": 1.822,
      "step": 58480
    },
    {
      "epoch": 29.750762970498474,
      "grad_norm": 32.54720687866211,
      "learning_rate": 2.0249237029501528e-05,
      "loss": 1.7525,
      "step": 58490
    },
    {
      "epoch": 29.7558494404883,
      "grad_norm": 47.431400299072266,
      "learning_rate": 2.0244150559511698e-05,
      "loss": 1.7816,
      "step": 58500
    },
    {
      "epoch": 29.76093591047813,
      "grad_norm": 38.776371002197266,
      "learning_rate": 2.023906408952187e-05,
      "loss": 1.8062,
      "step": 58510
    },
    {
      "epoch": 29.766022380467955,
      "grad_norm": 39.96772384643555,
      "learning_rate": 2.0233977619532048e-05,
      "loss": 1.7538,
      "step": 58520
    },
    {
      "epoch": 29.771108850457782,
      "grad_norm": 36.317047119140625,
      "learning_rate": 2.0228891149542218e-05,
      "loss": 1.7706,
      "step": 58530
    },
    {
      "epoch": 29.77619532044761,
      "grad_norm": 39.662715911865234,
      "learning_rate": 2.022380467955239e-05,
      "loss": 1.7634,
      "step": 58540
    },
    {
      "epoch": 29.781281790437436,
      "grad_norm": 41.160888671875,
      "learning_rate": 2.0218718209562564e-05,
      "loss": 1.8006,
      "step": 58550
    },
    {
      "epoch": 29.786368260427263,
      "grad_norm": 32.86709213256836,
      "learning_rate": 2.0213631739572737e-05,
      "loss": 1.7691,
      "step": 58560
    },
    {
      "epoch": 29.79145473041709,
      "grad_norm": 43.05264663696289,
      "learning_rate": 2.020854526958291e-05,
      "loss": 1.7348,
      "step": 58570
    },
    {
      "epoch": 29.796541200406917,
      "grad_norm": 42.90070343017578,
      "learning_rate": 2.0203458799593084e-05,
      "loss": 1.7743,
      "step": 58580
    },
    {
      "epoch": 29.801627670396744,
      "grad_norm": 40.212890625,
      "learning_rate": 2.0198372329603257e-05,
      "loss": 1.7931,
      "step": 58590
    },
    {
      "epoch": 29.80671414038657,
      "grad_norm": 31.812145233154297,
      "learning_rate": 2.0193285859613427e-05,
      "loss": 1.7874,
      "step": 58600
    },
    {
      "epoch": 29.811800610376398,
      "grad_norm": 38.52268981933594,
      "learning_rate": 2.0188199389623604e-05,
      "loss": 1.7986,
      "step": 58610
    },
    {
      "epoch": 29.816887080366225,
      "grad_norm": 36.42605209350586,
      "learning_rate": 2.0183112919633777e-05,
      "loss": 1.8879,
      "step": 58620
    },
    {
      "epoch": 29.821973550356052,
      "grad_norm": 50.42636489868164,
      "learning_rate": 2.0178026449643947e-05,
      "loss": 1.9319,
      "step": 58630
    },
    {
      "epoch": 29.82706002034588,
      "grad_norm": 35.86513137817383,
      "learning_rate": 2.017293997965412e-05,
      "loss": 1.7861,
      "step": 58640
    },
    {
      "epoch": 29.832146490335706,
      "grad_norm": 37.683589935302734,
      "learning_rate": 2.0167853509664294e-05,
      "loss": 1.7783,
      "step": 58650
    },
    {
      "epoch": 29.837232960325533,
      "grad_norm": 38.46695327758789,
      "learning_rate": 2.0162767039674467e-05,
      "loss": 1.7791,
      "step": 58660
    },
    {
      "epoch": 29.84231943031536,
      "grad_norm": 50.48146057128906,
      "learning_rate": 2.015768056968464e-05,
      "loss": 1.761,
      "step": 58670
    },
    {
      "epoch": 29.847405900305187,
      "grad_norm": 31.178821563720703,
      "learning_rate": 2.0152594099694813e-05,
      "loss": 1.7812,
      "step": 58680
    },
    {
      "epoch": 29.852492370295014,
      "grad_norm": 51.332820892333984,
      "learning_rate": 2.0147507629704987e-05,
      "loss": 1.861,
      "step": 58690
    },
    {
      "epoch": 29.85757884028484,
      "grad_norm": 35.8253288269043,
      "learning_rate": 2.0142421159715157e-05,
      "loss": 1.6843,
      "step": 58700
    },
    {
      "epoch": 29.862665310274668,
      "grad_norm": 39.39692687988281,
      "learning_rate": 2.0137334689725333e-05,
      "loss": 1.767,
      "step": 58710
    },
    {
      "epoch": 29.867751780264495,
      "grad_norm": 49.935359954833984,
      "learning_rate": 2.0132248219735506e-05,
      "loss": 1.7457,
      "step": 58720
    },
    {
      "epoch": 29.872838250254322,
      "grad_norm": 34.5847282409668,
      "learning_rate": 2.0127161749745676e-05,
      "loss": 1.8023,
      "step": 58730
    },
    {
      "epoch": 29.87792472024415,
      "grad_norm": 40.61655044555664,
      "learning_rate": 2.012207527975585e-05,
      "loss": 1.7936,
      "step": 58740
    },
    {
      "epoch": 29.88301119023398,
      "grad_norm": 48.913272857666016,
      "learning_rate": 2.0116988809766023e-05,
      "loss": 1.8369,
      "step": 58750
    },
    {
      "epoch": 29.888097660223806,
      "grad_norm": 32.45222854614258,
      "learning_rate": 2.0111902339776196e-05,
      "loss": 1.7909,
      "step": 58760
    },
    {
      "epoch": 29.893184130213633,
      "grad_norm": 51.78002166748047,
      "learning_rate": 2.010681586978637e-05,
      "loss": 1.834,
      "step": 58770
    },
    {
      "epoch": 29.89827060020346,
      "grad_norm": 42.032936096191406,
      "learning_rate": 2.0101729399796543e-05,
      "loss": 1.7712,
      "step": 58780
    },
    {
      "epoch": 29.903357070193287,
      "grad_norm": 32.60554122924805,
      "learning_rate": 2.0096642929806713e-05,
      "loss": 1.8385,
      "step": 58790
    },
    {
      "epoch": 29.908443540183114,
      "grad_norm": 38.47895431518555,
      "learning_rate": 2.0091556459816886e-05,
      "loss": 1.8262,
      "step": 58800
    },
    {
      "epoch": 29.91353001017294,
      "grad_norm": 40.35673141479492,
      "learning_rate": 2.0086469989827063e-05,
      "loss": 1.8124,
      "step": 58810
    },
    {
      "epoch": 29.91861648016277,
      "grad_norm": 43.43084716796875,
      "learning_rate": 2.0081383519837236e-05,
      "loss": 1.7205,
      "step": 58820
    },
    {
      "epoch": 29.923702950152595,
      "grad_norm": 33.6540641784668,
      "learning_rate": 2.0076297049847406e-05,
      "loss": 1.8386,
      "step": 58830
    },
    {
      "epoch": 29.928789420142422,
      "grad_norm": 54.39407730102539,
      "learning_rate": 2.007121057985758e-05,
      "loss": 1.752,
      "step": 58840
    },
    {
      "epoch": 29.93387589013225,
      "grad_norm": 36.960811614990234,
      "learning_rate": 2.0066124109867752e-05,
      "loss": 1.7468,
      "step": 58850
    },
    {
      "epoch": 29.938962360122076,
      "grad_norm": 41.4086799621582,
      "learning_rate": 2.0061037639877926e-05,
      "loss": 1.8296,
      "step": 58860
    },
    {
      "epoch": 29.944048830111903,
      "grad_norm": 40.620155334472656,
      "learning_rate": 2.00559511698881e-05,
      "loss": 1.7706,
      "step": 58870
    },
    {
      "epoch": 29.94913530010173,
      "grad_norm": 42.01991271972656,
      "learning_rate": 2.0050864699898272e-05,
      "loss": 1.652,
      "step": 58880
    },
    {
      "epoch": 29.954221770091557,
      "grad_norm": 36.20235061645508,
      "learning_rate": 2.0045778229908442e-05,
      "loss": 1.8007,
      "step": 58890
    },
    {
      "epoch": 29.959308240081384,
      "grad_norm": 37.285980224609375,
      "learning_rate": 2.004069175991862e-05,
      "loss": 1.7381,
      "step": 58900
    },
    {
      "epoch": 29.96439471007121,
      "grad_norm": 35.538978576660156,
      "learning_rate": 2.0035605289928792e-05,
      "loss": 1.7143,
      "step": 58910
    },
    {
      "epoch": 29.969481180061038,
      "grad_norm": 30.074298858642578,
      "learning_rate": 2.0030518819938962e-05,
      "loss": 1.8396,
      "step": 58920
    },
    {
      "epoch": 29.974567650050865,
      "grad_norm": 37.820884704589844,
      "learning_rate": 2.0025432349949135e-05,
      "loss": 1.8458,
      "step": 58930
    },
    {
      "epoch": 29.979654120040692,
      "grad_norm": 45.65896987915039,
      "learning_rate": 2.002034587995931e-05,
      "loss": 1.815,
      "step": 58940
    },
    {
      "epoch": 29.98474059003052,
      "grad_norm": 35.08689880371094,
      "learning_rate": 2.0015259409969482e-05,
      "loss": 1.76,
      "step": 58950
    },
    {
      "epoch": 29.989827060020346,
      "grad_norm": 43.08736038208008,
      "learning_rate": 2.0010172939979655e-05,
      "loss": 1.7737,
      "step": 58960
    },
    {
      "epoch": 29.994913530010173,
      "grad_norm": 39.335914611816406,
      "learning_rate": 2.000508646998983e-05,
      "loss": 1.7597,
      "step": 58970
    },
    {
      "epoch": 30.0,
      "grad_norm": 43.52287673950195,
      "learning_rate": 2e-05,
      "loss": 1.7441,
      "step": 58980
    },
    {
      "epoch": 30.0,
      "eval_loss": 4.785391807556152,
      "eval_runtime": 2.6419,
      "eval_samples_per_second": 1050.395,
      "eval_steps_per_second": 131.347,
      "step": 58980
    },
    {
      "epoch": 30.005086469989827,
      "grad_norm": 43.142738342285156,
      "learning_rate": 1.999491353001017e-05,
      "loss": 1.7603,
      "step": 58990
    },
    {
      "epoch": 30.010172939979654,
      "grad_norm": 41.00224685668945,
      "learning_rate": 1.9989827060020348e-05,
      "loss": 1.6979,
      "step": 59000
    },
    {
      "epoch": 30.01525940996948,
      "grad_norm": 40.00889587402344,
      "learning_rate": 1.998474059003052e-05,
      "loss": 1.7733,
      "step": 59010
    },
    {
      "epoch": 30.020345879959308,
      "grad_norm": 38.28199005126953,
      "learning_rate": 1.997965412004069e-05,
      "loss": 1.8076,
      "step": 59020
    },
    {
      "epoch": 30.025432349949135,
      "grad_norm": 45.8785514831543,
      "learning_rate": 1.9974567650050865e-05,
      "loss": 1.7613,
      "step": 59030
    },
    {
      "epoch": 30.030518819938962,
      "grad_norm": 34.21559524536133,
      "learning_rate": 1.9969481180061038e-05,
      "loss": 1.7662,
      "step": 59040
    },
    {
      "epoch": 30.03560528992879,
      "grad_norm": 30.594844818115234,
      "learning_rate": 1.996439471007121e-05,
      "loss": 1.7886,
      "step": 59050
    },
    {
      "epoch": 30.040691759918616,
      "grad_norm": 38.055423736572266,
      "learning_rate": 1.9959308240081385e-05,
      "loss": 1.7761,
      "step": 59060
    },
    {
      "epoch": 30.045778229908443,
      "grad_norm": 29.981246948242188,
      "learning_rate": 1.9954221770091558e-05,
      "loss": 1.7578,
      "step": 59070
    },
    {
      "epoch": 30.05086469989827,
      "grad_norm": 40.00246810913086,
      "learning_rate": 1.994913530010173e-05,
      "loss": 1.6363,
      "step": 59080
    },
    {
      "epoch": 30.055951169888097,
      "grad_norm": 45.727455139160156,
      "learning_rate": 1.9944048830111904e-05,
      "loss": 1.7042,
      "step": 59090
    },
    {
      "epoch": 30.061037639877924,
      "grad_norm": 39.94389343261719,
      "learning_rate": 1.9938962360122078e-05,
      "loss": 1.6395,
      "step": 59100
    },
    {
      "epoch": 30.06612410986775,
      "grad_norm": 52.071475982666016,
      "learning_rate": 1.993387589013225e-05,
      "loss": 1.8129,
      "step": 59110
    },
    {
      "epoch": 30.071210579857578,
      "grad_norm": 32.82149887084961,
      "learning_rate": 1.992878942014242e-05,
      "loss": 1.8468,
      "step": 59120
    },
    {
      "epoch": 30.076297049847405,
      "grad_norm": 31.587974548339844,
      "learning_rate": 1.9923702950152594e-05,
      "loss": 1.7815,
      "step": 59130
    },
    {
      "epoch": 30.08138351983723,
      "grad_norm": 47.94892883300781,
      "learning_rate": 1.9918616480162767e-05,
      "loss": 1.8089,
      "step": 59140
    },
    {
      "epoch": 30.08646998982706,
      "grad_norm": 44.656803131103516,
      "learning_rate": 1.991353001017294e-05,
      "loss": 1.7984,
      "step": 59150
    },
    {
      "epoch": 30.091556459816886,
      "grad_norm": 31.640304565429688,
      "learning_rate": 1.9908443540183114e-05,
      "loss": 1.778,
      "step": 59160
    },
    {
      "epoch": 30.096642929806713,
      "grad_norm": 35.35164260864258,
      "learning_rate": 1.9903357070193287e-05,
      "loss": 1.8327,
      "step": 59170
    },
    {
      "epoch": 30.10172939979654,
      "grad_norm": 38.56229019165039,
      "learning_rate": 1.9898270600203457e-05,
      "loss": 1.7633,
      "step": 59180
    },
    {
      "epoch": 30.106815869786367,
      "grad_norm": 43.78031539916992,
      "learning_rate": 1.9893184130213634e-05,
      "loss": 1.7141,
      "step": 59190
    },
    {
      "epoch": 30.111902339776194,
      "grad_norm": 38.44843292236328,
      "learning_rate": 1.9888097660223807e-05,
      "loss": 1.7678,
      "step": 59200
    },
    {
      "epoch": 30.116988809766024,
      "grad_norm": 40.916133880615234,
      "learning_rate": 1.988301119023398e-05,
      "loss": 1.7286,
      "step": 59210
    },
    {
      "epoch": 30.12207527975585,
      "grad_norm": 35.37436294555664,
      "learning_rate": 1.987792472024415e-05,
      "loss": 1.7983,
      "step": 59220
    },
    {
      "epoch": 30.127161749745678,
      "grad_norm": 36.62480545043945,
      "learning_rate": 1.9872838250254324e-05,
      "loss": 1.7858,
      "step": 59230
    },
    {
      "epoch": 30.132248219735505,
      "grad_norm": 40.16572570800781,
      "learning_rate": 1.98677517802645e-05,
      "loss": 1.7326,
      "step": 59240
    },
    {
      "epoch": 30.137334689725332,
      "grad_norm": 34.62105178833008,
      "learning_rate": 1.986266531027467e-05,
      "loss": 1.7915,
      "step": 59250
    },
    {
      "epoch": 30.14242115971516,
      "grad_norm": 34.12543869018555,
      "learning_rate": 1.9857578840284843e-05,
      "loss": 1.8265,
      "step": 59260
    },
    {
      "epoch": 30.147507629704986,
      "grad_norm": 34.92311477661133,
      "learning_rate": 1.9852492370295017e-05,
      "loss": 1.8013,
      "step": 59270
    },
    {
      "epoch": 30.152594099694813,
      "grad_norm": 37.65456008911133,
      "learning_rate": 1.9847405900305187e-05,
      "loss": 1.775,
      "step": 59280
    },
    {
      "epoch": 30.15768056968464,
      "grad_norm": 36.084571838378906,
      "learning_rate": 1.9842319430315363e-05,
      "loss": 1.7362,
      "step": 59290
    },
    {
      "epoch": 30.162767039674467,
      "grad_norm": 35.06694412231445,
      "learning_rate": 1.9837232960325536e-05,
      "loss": 1.7743,
      "step": 59300
    },
    {
      "epoch": 30.167853509664294,
      "grad_norm": 45.4471435546875,
      "learning_rate": 1.9832146490335706e-05,
      "loss": 1.775,
      "step": 59310
    },
    {
      "epoch": 30.17293997965412,
      "grad_norm": 40.407806396484375,
      "learning_rate": 1.982706002034588e-05,
      "loss": 1.7498,
      "step": 59320
    },
    {
      "epoch": 30.178026449643948,
      "grad_norm": 35.81771469116211,
      "learning_rate": 1.9821973550356053e-05,
      "loss": 1.7997,
      "step": 59330
    },
    {
      "epoch": 30.183112919633775,
      "grad_norm": 46.099605560302734,
      "learning_rate": 1.9816887080366226e-05,
      "loss": 1.7725,
      "step": 59340
    },
    {
      "epoch": 30.188199389623602,
      "grad_norm": 34.956546783447266,
      "learning_rate": 1.98118006103764e-05,
      "loss": 1.7818,
      "step": 59350
    },
    {
      "epoch": 30.19328585961343,
      "grad_norm": 32.3110237121582,
      "learning_rate": 1.9806714140386573e-05,
      "loss": 1.736,
      "step": 59360
    },
    {
      "epoch": 30.198372329603256,
      "grad_norm": 37.439781188964844,
      "learning_rate": 1.9801627670396746e-05,
      "loss": 1.763,
      "step": 59370
    },
    {
      "epoch": 30.203458799593083,
      "grad_norm": 40.56238555908203,
      "learning_rate": 1.979654120040692e-05,
      "loss": 1.8014,
      "step": 59380
    },
    {
      "epoch": 30.20854526958291,
      "grad_norm": 49.249610900878906,
      "learning_rate": 1.9791454730417093e-05,
      "loss": 1.7671,
      "step": 59390
    },
    {
      "epoch": 30.213631739572737,
      "grad_norm": 38.217498779296875,
      "learning_rate": 1.9786368260427266e-05,
      "loss": 1.7907,
      "step": 59400
    },
    {
      "epoch": 30.218718209562564,
      "grad_norm": 38.08576202392578,
      "learning_rate": 1.9781281790437436e-05,
      "loss": 1.7878,
      "step": 59410
    },
    {
      "epoch": 30.22380467955239,
      "grad_norm": 37.37775802612305,
      "learning_rate": 1.977619532044761e-05,
      "loss": 1.6867,
      "step": 59420
    },
    {
      "epoch": 30.228891149542218,
      "grad_norm": 36.82076644897461,
      "learning_rate": 1.9771108850457782e-05,
      "loss": 1.7759,
      "step": 59430
    },
    {
      "epoch": 30.233977619532045,
      "grad_norm": 37.2093391418457,
      "learning_rate": 1.9766022380467956e-05,
      "loss": 1.7098,
      "step": 59440
    },
    {
      "epoch": 30.23906408952187,
      "grad_norm": 40.71281433105469,
      "learning_rate": 1.976093591047813e-05,
      "loss": 1.8282,
      "step": 59450
    },
    {
      "epoch": 30.2441505595117,
      "grad_norm": 28.83475685119629,
      "learning_rate": 1.9755849440488302e-05,
      "loss": 1.7366,
      "step": 59460
    },
    {
      "epoch": 30.249237029501526,
      "grad_norm": 36.42947769165039,
      "learning_rate": 1.9750762970498472e-05,
      "loss": 1.857,
      "step": 59470
    },
    {
      "epoch": 30.254323499491353,
      "grad_norm": 32.168983459472656,
      "learning_rate": 1.974567650050865e-05,
      "loss": 1.8427,
      "step": 59480
    },
    {
      "epoch": 30.25940996948118,
      "grad_norm": 43.93008804321289,
      "learning_rate": 1.9740590030518822e-05,
      "loss": 1.8343,
      "step": 59490
    },
    {
      "epoch": 30.264496439471007,
      "grad_norm": 31.467071533203125,
      "learning_rate": 1.9735503560528995e-05,
      "loss": 1.7735,
      "step": 59500
    },
    {
      "epoch": 30.269582909460834,
      "grad_norm": 44.07731246948242,
      "learning_rate": 1.9730417090539165e-05,
      "loss": 1.7858,
      "step": 59510
    },
    {
      "epoch": 30.27466937945066,
      "grad_norm": 39.812110900878906,
      "learning_rate": 1.972533062054934e-05,
      "loss": 1.7328,
      "step": 59520
    },
    {
      "epoch": 30.279755849440487,
      "grad_norm": 36.66896057128906,
      "learning_rate": 1.9720244150559515e-05,
      "loss": 1.7771,
      "step": 59530
    },
    {
      "epoch": 30.284842319430314,
      "grad_norm": 38.6700553894043,
      "learning_rate": 1.9715157680569685e-05,
      "loss": 1.8447,
      "step": 59540
    },
    {
      "epoch": 30.28992878942014,
      "grad_norm": 39.66515350341797,
      "learning_rate": 1.971007121057986e-05,
      "loss": 1.7313,
      "step": 59550
    },
    {
      "epoch": 30.29501525940997,
      "grad_norm": 44.664642333984375,
      "learning_rate": 1.970498474059003e-05,
      "loss": 1.6709,
      "step": 59560
    },
    {
      "epoch": 30.300101729399795,
      "grad_norm": 39.394405364990234,
      "learning_rate": 1.9699898270600205e-05,
      "loss": 1.7589,
      "step": 59570
    },
    {
      "epoch": 30.305188199389622,
      "grad_norm": 36.24317169189453,
      "learning_rate": 1.9694811800610378e-05,
      "loss": 1.7508,
      "step": 59580
    },
    {
      "epoch": 30.31027466937945,
      "grad_norm": 65.09384155273438,
      "learning_rate": 1.968972533062055e-05,
      "loss": 1.7716,
      "step": 59590
    },
    {
      "epoch": 30.315361139369276,
      "grad_norm": 31.367372512817383,
      "learning_rate": 1.968463886063072e-05,
      "loss": 1.8039,
      "step": 59600
    },
    {
      "epoch": 30.320447609359103,
      "grad_norm": 38.29567337036133,
      "learning_rate": 1.9679552390640895e-05,
      "loss": 1.7472,
      "step": 59610
    },
    {
      "epoch": 30.32553407934893,
      "grad_norm": 35.43337631225586,
      "learning_rate": 1.9674465920651068e-05,
      "loss": 1.8243,
      "step": 59620
    },
    {
      "epoch": 30.330620549338757,
      "grad_norm": 37.554412841796875,
      "learning_rate": 1.9669379450661245e-05,
      "loss": 1.7753,
      "step": 59630
    },
    {
      "epoch": 30.335707019328584,
      "grad_norm": 33.5817985534668,
      "learning_rate": 1.9664292980671415e-05,
      "loss": 1.7498,
      "step": 59640
    },
    {
      "epoch": 30.340793489318415,
      "grad_norm": 58.214176177978516,
      "learning_rate": 1.9659206510681588e-05,
      "loss": 1.8622,
      "step": 59650
    },
    {
      "epoch": 30.345879959308242,
      "grad_norm": 37.66242980957031,
      "learning_rate": 1.965412004069176e-05,
      "loss": 1.6983,
      "step": 59660
    },
    {
      "epoch": 30.35096642929807,
      "grad_norm": 36.792476654052734,
      "learning_rate": 1.9649033570701934e-05,
      "loss": 1.7739,
      "step": 59670
    },
    {
      "epoch": 30.356052899287896,
      "grad_norm": 44.570430755615234,
      "learning_rate": 1.9643947100712108e-05,
      "loss": 1.8029,
      "step": 59680
    },
    {
      "epoch": 30.361139369277723,
      "grad_norm": 38.79315185546875,
      "learning_rate": 1.963886063072228e-05,
      "loss": 1.7832,
      "step": 59690
    },
    {
      "epoch": 30.36622583926755,
      "grad_norm": 35.64857482910156,
      "learning_rate": 1.963377416073245e-05,
      "loss": 1.8334,
      "step": 59700
    },
    {
      "epoch": 30.371312309257377,
      "grad_norm": 49.288265228271484,
      "learning_rate": 1.9628687690742624e-05,
      "loss": 1.784,
      "step": 59710
    },
    {
      "epoch": 30.376398779247204,
      "grad_norm": 39.130130767822266,
      "learning_rate": 1.96236012207528e-05,
      "loss": 1.6908,
      "step": 59720
    },
    {
      "epoch": 30.38148524923703,
      "grad_norm": 37.233795166015625,
      "learning_rate": 1.961851475076297e-05,
      "loss": 1.821,
      "step": 59730
    },
    {
      "epoch": 30.386571719226858,
      "grad_norm": 42.184654235839844,
      "learning_rate": 1.9613428280773144e-05,
      "loss": 1.8608,
      "step": 59740
    },
    {
      "epoch": 30.391658189216685,
      "grad_norm": 35.25642395019531,
      "learning_rate": 1.9608341810783317e-05,
      "loss": 1.7521,
      "step": 59750
    },
    {
      "epoch": 30.39674465920651,
      "grad_norm": 42.022579193115234,
      "learning_rate": 1.960325534079349e-05,
      "loss": 1.7694,
      "step": 59760
    },
    {
      "epoch": 30.40183112919634,
      "grad_norm": 30.942407608032227,
      "learning_rate": 1.9598168870803664e-05,
      "loss": 1.8016,
      "step": 59770
    },
    {
      "epoch": 30.406917599186166,
      "grad_norm": 31.93199920654297,
      "learning_rate": 1.9593082400813837e-05,
      "loss": 1.7478,
      "step": 59780
    },
    {
      "epoch": 30.412004069175993,
      "grad_norm": 34.298336029052734,
      "learning_rate": 1.958799593082401e-05,
      "loss": 1.7481,
      "step": 59790
    },
    {
      "epoch": 30.41709053916582,
      "grad_norm": 33.81474685668945,
      "learning_rate": 1.958290946083418e-05,
      "loss": 1.7617,
      "step": 59800
    },
    {
      "epoch": 30.422177009155646,
      "grad_norm": 42.683956146240234,
      "learning_rate": 1.9577822990844354e-05,
      "loss": 1.6895,
      "step": 59810
    },
    {
      "epoch": 30.427263479145473,
      "grad_norm": 38.623905181884766,
      "learning_rate": 1.957273652085453e-05,
      "loss": 1.7714,
      "step": 59820
    },
    {
      "epoch": 30.4323499491353,
      "grad_norm": 45.2592658996582,
      "learning_rate": 1.95676500508647e-05,
      "loss": 1.7942,
      "step": 59830
    },
    {
      "epoch": 30.437436419125127,
      "grad_norm": 35.557071685791016,
      "learning_rate": 1.9562563580874873e-05,
      "loss": 1.7493,
      "step": 59840
    },
    {
      "epoch": 30.442522889114954,
      "grad_norm": 54.98461151123047,
      "learning_rate": 1.9557477110885047e-05,
      "loss": 1.7784,
      "step": 59850
    },
    {
      "epoch": 30.44760935910478,
      "grad_norm": 45.69265365600586,
      "learning_rate": 1.955239064089522e-05,
      "loss": 1.7169,
      "step": 59860
    },
    {
      "epoch": 30.45269582909461,
      "grad_norm": 39.62787628173828,
      "learning_rate": 1.9547304170905393e-05,
      "loss": 1.8075,
      "step": 59870
    },
    {
      "epoch": 30.457782299084435,
      "grad_norm": 39.39472579956055,
      "learning_rate": 1.9542217700915566e-05,
      "loss": 1.7795,
      "step": 59880
    },
    {
      "epoch": 30.462868769074262,
      "grad_norm": 35.89604949951172,
      "learning_rate": 1.953713123092574e-05,
      "loss": 1.7824,
      "step": 59890
    },
    {
      "epoch": 30.46795523906409,
      "grad_norm": 42.862213134765625,
      "learning_rate": 1.953204476093591e-05,
      "loss": 1.7301,
      "step": 59900
    },
    {
      "epoch": 30.473041709053916,
      "grad_norm": 35.55985641479492,
      "learning_rate": 1.9526958290946086e-05,
      "loss": 1.7453,
      "step": 59910
    },
    {
      "epoch": 30.478128179043743,
      "grad_norm": 41.61860656738281,
      "learning_rate": 1.952187182095626e-05,
      "loss": 1.7206,
      "step": 59920
    },
    {
      "epoch": 30.48321464903357,
      "grad_norm": 49.045555114746094,
      "learning_rate": 1.951678535096643e-05,
      "loss": 1.6883,
      "step": 59930
    },
    {
      "epoch": 30.488301119023397,
      "grad_norm": 41.5009651184082,
      "learning_rate": 1.9511698880976603e-05,
      "loss": 1.7677,
      "step": 59940
    },
    {
      "epoch": 30.493387589013224,
      "grad_norm": 39.225337982177734,
      "learning_rate": 1.9506612410986776e-05,
      "loss": 1.7032,
      "step": 59950
    },
    {
      "epoch": 30.49847405900305,
      "grad_norm": 40.350372314453125,
      "learning_rate": 1.950152594099695e-05,
      "loss": 1.8116,
      "step": 59960
    },
    {
      "epoch": 30.503560528992878,
      "grad_norm": 36.50239181518555,
      "learning_rate": 1.9496439471007123e-05,
      "loss": 1.7752,
      "step": 59970
    },
    {
      "epoch": 30.508646998982705,
      "grad_norm": 32.041053771972656,
      "learning_rate": 1.9491353001017296e-05,
      "loss": 1.7092,
      "step": 59980
    },
    {
      "epoch": 30.513733468972532,
      "grad_norm": 44.33567810058594,
      "learning_rate": 1.9486266531027466e-05,
      "loss": 1.6812,
      "step": 59990
    },
    {
      "epoch": 30.51881993896236,
      "grad_norm": 40.878292083740234,
      "learning_rate": 1.948118006103764e-05,
      "loss": 1.7359,
      "step": 60000
    },
    {
      "epoch": 30.523906408952186,
      "grad_norm": 40.20412063598633,
      "learning_rate": 1.9476093591047816e-05,
      "loss": 1.656,
      "step": 60010
    },
    {
      "epoch": 30.528992878942013,
      "grad_norm": 30.261573791503906,
      "learning_rate": 1.947100712105799e-05,
      "loss": 1.7751,
      "step": 60020
    },
    {
      "epoch": 30.53407934893184,
      "grad_norm": 43.51573181152344,
      "learning_rate": 1.946592065106816e-05,
      "loss": 1.7669,
      "step": 60030
    },
    {
      "epoch": 30.539165818921667,
      "grad_norm": 35.168113708496094,
      "learning_rate": 1.9460834181078332e-05,
      "loss": 1.6972,
      "step": 60040
    },
    {
      "epoch": 30.544252288911494,
      "grad_norm": 40.37608337402344,
      "learning_rate": 1.9455747711088506e-05,
      "loss": 1.7121,
      "step": 60050
    },
    {
      "epoch": 30.54933875890132,
      "grad_norm": 51.4852180480957,
      "learning_rate": 1.945066124109868e-05,
      "loss": 1.7657,
      "step": 60060
    },
    {
      "epoch": 30.554425228891148,
      "grad_norm": 39.775238037109375,
      "learning_rate": 1.9445574771108852e-05,
      "loss": 1.7601,
      "step": 60070
    },
    {
      "epoch": 30.559511698880975,
      "grad_norm": 42.047630310058594,
      "learning_rate": 1.9440488301119025e-05,
      "loss": 1.751,
      "step": 60080
    },
    {
      "epoch": 30.564598168870802,
      "grad_norm": 37.001121520996094,
      "learning_rate": 1.9435401831129195e-05,
      "loss": 1.7606,
      "step": 60090
    },
    {
      "epoch": 30.56968463886063,
      "grad_norm": 33.210838317871094,
      "learning_rate": 1.943031536113937e-05,
      "loss": 1.7374,
      "step": 60100
    },
    {
      "epoch": 30.57477110885046,
      "grad_norm": 42.22860336303711,
      "learning_rate": 1.9425228891149545e-05,
      "loss": 1.7426,
      "step": 60110
    },
    {
      "epoch": 30.579857578840286,
      "grad_norm": 49.06241989135742,
      "learning_rate": 1.9420142421159715e-05,
      "loss": 1.7638,
      "step": 60120
    },
    {
      "epoch": 30.584944048830113,
      "grad_norm": 42.332496643066406,
      "learning_rate": 1.941505595116989e-05,
      "loss": 1.7524,
      "step": 60130
    },
    {
      "epoch": 30.59003051881994,
      "grad_norm": 32.04609680175781,
      "learning_rate": 1.940996948118006e-05,
      "loss": 1.6918,
      "step": 60140
    },
    {
      "epoch": 30.595116988809767,
      "grad_norm": 43.2327880859375,
      "learning_rate": 1.9404883011190235e-05,
      "loss": 1.7784,
      "step": 60150
    },
    {
      "epoch": 30.600203458799594,
      "grad_norm": 47.47169494628906,
      "learning_rate": 1.9399796541200408e-05,
      "loss": 1.7292,
      "step": 60160
    },
    {
      "epoch": 30.60528992878942,
      "grad_norm": 39.74465560913086,
      "learning_rate": 1.939471007121058e-05,
      "loss": 1.7967,
      "step": 60170
    },
    {
      "epoch": 30.61037639877925,
      "grad_norm": 36.27477264404297,
      "learning_rate": 1.9389623601220755e-05,
      "loss": 1.811,
      "step": 60180
    },
    {
      "epoch": 30.615462868769075,
      "grad_norm": 47.052799224853516,
      "learning_rate": 1.9384537131230925e-05,
      "loss": 1.8087,
      "step": 60190
    },
    {
      "epoch": 30.620549338758902,
      "grad_norm": 35.07352828979492,
      "learning_rate": 1.93794506612411e-05,
      "loss": 1.8174,
      "step": 60200
    },
    {
      "epoch": 30.62563580874873,
      "grad_norm": 46.22844696044922,
      "learning_rate": 1.9374364191251275e-05,
      "loss": 1.749,
      "step": 60210
    },
    {
      "epoch": 30.630722278738556,
      "grad_norm": 38.459190368652344,
      "learning_rate": 1.9369277721261445e-05,
      "loss": 1.7826,
      "step": 60220
    },
    {
      "epoch": 30.635808748728383,
      "grad_norm": 41.50789260864258,
      "learning_rate": 1.9364191251271618e-05,
      "loss": 1.8255,
      "step": 60230
    },
    {
      "epoch": 30.64089521871821,
      "grad_norm": 41.75627136230469,
      "learning_rate": 1.935910478128179e-05,
      "loss": 1.7289,
      "step": 60240
    },
    {
      "epoch": 30.645981688708037,
      "grad_norm": 37.184303283691406,
      "learning_rate": 1.9354018311291964e-05,
      "loss": 1.7681,
      "step": 60250
    },
    {
      "epoch": 30.651068158697864,
      "grad_norm": 44.77875518798828,
      "learning_rate": 1.9348931841302138e-05,
      "loss": 1.7258,
      "step": 60260
    },
    {
      "epoch": 30.65615462868769,
      "grad_norm": 37.93216323852539,
      "learning_rate": 1.934384537131231e-05,
      "loss": 1.6958,
      "step": 60270
    },
    {
      "epoch": 30.661241098677518,
      "grad_norm": 53.974185943603516,
      "learning_rate": 1.933875890132248e-05,
      "loss": 1.8123,
      "step": 60280
    },
    {
      "epoch": 30.666327568667345,
      "grad_norm": 34.78876876831055,
      "learning_rate": 1.9333672431332654e-05,
      "loss": 1.6818,
      "step": 60290
    },
    {
      "epoch": 30.671414038657172,
      "grad_norm": 35.84954071044922,
      "learning_rate": 1.932858596134283e-05,
      "loss": 1.6761,
      "step": 60300
    },
    {
      "epoch": 30.676500508647,
      "grad_norm": 36.99813461303711,
      "learning_rate": 1.9323499491353004e-05,
      "loss": 1.8272,
      "step": 60310
    },
    {
      "epoch": 30.681586978636826,
      "grad_norm": 35.57119369506836,
      "learning_rate": 1.9318413021363174e-05,
      "loss": 1.7571,
      "step": 60320
    },
    {
      "epoch": 30.686673448626653,
      "grad_norm": 37.50864791870117,
      "learning_rate": 1.9313326551373347e-05,
      "loss": 1.7711,
      "step": 60330
    },
    {
      "epoch": 30.69175991861648,
      "grad_norm": 27.551340103149414,
      "learning_rate": 1.930824008138352e-05,
      "loss": 1.7803,
      "step": 60340
    },
    {
      "epoch": 30.696846388606307,
      "grad_norm": 36.808528900146484,
      "learning_rate": 1.9303153611393694e-05,
      "loss": 1.8069,
      "step": 60350
    },
    {
      "epoch": 30.701932858596134,
      "grad_norm": 38.15651321411133,
      "learning_rate": 1.9298067141403867e-05,
      "loss": 1.6666,
      "step": 60360
    },
    {
      "epoch": 30.70701932858596,
      "grad_norm": 46.03303527832031,
      "learning_rate": 1.929298067141404e-05,
      "loss": 1.676,
      "step": 60370
    },
    {
      "epoch": 30.712105798575788,
      "grad_norm": 41.98363494873047,
      "learning_rate": 1.928789420142421e-05,
      "loss": 1.6803,
      "step": 60380
    },
    {
      "epoch": 30.717192268565615,
      "grad_norm": 62.653297424316406,
      "learning_rate": 1.9282807731434387e-05,
      "loss": 1.7959,
      "step": 60390
    },
    {
      "epoch": 30.722278738555442,
      "grad_norm": 37.529388427734375,
      "learning_rate": 1.927772126144456e-05,
      "loss": 1.7449,
      "step": 60400
    },
    {
      "epoch": 30.72736520854527,
      "grad_norm": 29.34297752380371,
      "learning_rate": 1.927263479145473e-05,
      "loss": 1.8236,
      "step": 60410
    },
    {
      "epoch": 30.732451678535096,
      "grad_norm": 39.69228744506836,
      "learning_rate": 1.9267548321464903e-05,
      "loss": 1.7018,
      "step": 60420
    },
    {
      "epoch": 30.737538148524923,
      "grad_norm": 51.71860885620117,
      "learning_rate": 1.9262461851475077e-05,
      "loss": 1.7573,
      "step": 60430
    },
    {
      "epoch": 30.74262461851475,
      "grad_norm": 39.53652572631836,
      "learning_rate": 1.925737538148525e-05,
      "loss": 1.7367,
      "step": 60440
    },
    {
      "epoch": 30.747711088504577,
      "grad_norm": 40.642425537109375,
      "learning_rate": 1.9252288911495423e-05,
      "loss": 1.7099,
      "step": 60450
    },
    {
      "epoch": 30.752797558494404,
      "grad_norm": 40.963706970214844,
      "learning_rate": 1.9247202441505596e-05,
      "loss": 1.7336,
      "step": 60460
    },
    {
      "epoch": 30.75788402848423,
      "grad_norm": 38.54410934448242,
      "learning_rate": 1.924211597151577e-05,
      "loss": 1.7794,
      "step": 60470
    },
    {
      "epoch": 30.762970498474058,
      "grad_norm": 46.74848937988281,
      "learning_rate": 1.923702950152594e-05,
      "loss": 1.7428,
      "step": 60480
    },
    {
      "epoch": 30.768056968463885,
      "grad_norm": 31.14759635925293,
      "learning_rate": 1.9231943031536116e-05,
      "loss": 1.7416,
      "step": 60490
    },
    {
      "epoch": 30.77314343845371,
      "grad_norm": 41.02870559692383,
      "learning_rate": 1.922685656154629e-05,
      "loss": 1.7802,
      "step": 60500
    },
    {
      "epoch": 30.77822990844354,
      "grad_norm": 47.91111755371094,
      "learning_rate": 1.922177009155646e-05,
      "loss": 1.7349,
      "step": 60510
    },
    {
      "epoch": 30.783316378433366,
      "grad_norm": 41.82855987548828,
      "learning_rate": 1.9216683621566633e-05,
      "loss": 1.8237,
      "step": 60520
    },
    {
      "epoch": 30.788402848423193,
      "grad_norm": 33.79973220825195,
      "learning_rate": 1.9211597151576806e-05,
      "loss": 1.787,
      "step": 60530
    },
    {
      "epoch": 30.793489318413023,
      "grad_norm": 54.548301696777344,
      "learning_rate": 1.920651068158698e-05,
      "loss": 1.8194,
      "step": 60540
    },
    {
      "epoch": 30.79857578840285,
      "grad_norm": 31.354711532592773,
      "learning_rate": 1.9201424211597153e-05,
      "loss": 1.7632,
      "step": 60550
    },
    {
      "epoch": 30.803662258392677,
      "grad_norm": 33.36211013793945,
      "learning_rate": 1.9196337741607326e-05,
      "loss": 1.7611,
      "step": 60560
    },
    {
      "epoch": 30.808748728382504,
      "grad_norm": 34.82556915283203,
      "learning_rate": 1.91912512716175e-05,
      "loss": 1.7299,
      "step": 60570
    },
    {
      "epoch": 30.81383519837233,
      "grad_norm": 49.19993591308594,
      "learning_rate": 1.918616480162767e-05,
      "loss": 1.709,
      "step": 60580
    },
    {
      "epoch": 30.818921668362158,
      "grad_norm": 32.18923568725586,
      "learning_rate": 1.9181078331637846e-05,
      "loss": 1.8028,
      "step": 60590
    },
    {
      "epoch": 30.824008138351985,
      "grad_norm": 39.009151458740234,
      "learning_rate": 1.917599186164802e-05,
      "loss": 1.6717,
      "step": 60600
    },
    {
      "epoch": 30.829094608341812,
      "grad_norm": 45.147926330566406,
      "learning_rate": 1.917090539165819e-05,
      "loss": 1.7679,
      "step": 60610
    },
    {
      "epoch": 30.83418107833164,
      "grad_norm": 44.37213897705078,
      "learning_rate": 1.9165818921668362e-05,
      "loss": 1.7621,
      "step": 60620
    },
    {
      "epoch": 30.839267548321466,
      "grad_norm": 46.203548431396484,
      "learning_rate": 1.9160732451678536e-05,
      "loss": 1.7508,
      "step": 60630
    },
    {
      "epoch": 30.844354018311293,
      "grad_norm": 39.14014434814453,
      "learning_rate": 1.915564598168871e-05,
      "loss": 1.729,
      "step": 60640
    },
    {
      "epoch": 30.84944048830112,
      "grad_norm": 39.10991287231445,
      "learning_rate": 1.9150559511698882e-05,
      "loss": 1.7712,
      "step": 60650
    },
    {
      "epoch": 30.854526958290947,
      "grad_norm": 32.60209274291992,
      "learning_rate": 1.9145473041709055e-05,
      "loss": 1.7347,
      "step": 60660
    },
    {
      "epoch": 30.859613428280774,
      "grad_norm": 47.62681579589844,
      "learning_rate": 1.9140386571719225e-05,
      "loss": 1.7964,
      "step": 60670
    },
    {
      "epoch": 30.8646998982706,
      "grad_norm": 47.24256896972656,
      "learning_rate": 1.9135300101729402e-05,
      "loss": 1.7958,
      "step": 60680
    },
    {
      "epoch": 30.869786368260428,
      "grad_norm": 45.01774978637695,
      "learning_rate": 1.9130213631739575e-05,
      "loss": 1.7777,
      "step": 60690
    },
    {
      "epoch": 30.874872838250255,
      "grad_norm": 39.992557525634766,
      "learning_rate": 1.912512716174975e-05,
      "loss": 1.6968,
      "step": 60700
    },
    {
      "epoch": 30.879959308240082,
      "grad_norm": 40.256874084472656,
      "learning_rate": 1.912004069175992e-05,
      "loss": 1.7511,
      "step": 60710
    },
    {
      "epoch": 30.88504577822991,
      "grad_norm": 43.54792404174805,
      "learning_rate": 1.911495422177009e-05,
      "loss": 1.8287,
      "step": 60720
    },
    {
      "epoch": 30.890132248219736,
      "grad_norm": 38.805747985839844,
      "learning_rate": 1.9109867751780265e-05,
      "loss": 1.7865,
      "step": 60730
    },
    {
      "epoch": 30.895218718209563,
      "grad_norm": 34.465965270996094,
      "learning_rate": 1.9104781281790438e-05,
      "loss": 1.7794,
      "step": 60740
    },
    {
      "epoch": 30.90030518819939,
      "grad_norm": 30.745607376098633,
      "learning_rate": 1.909969481180061e-05,
      "loss": 1.6888,
      "step": 60750
    },
    {
      "epoch": 30.905391658189217,
      "grad_norm": 29.748233795166016,
      "learning_rate": 1.9094608341810785e-05,
      "loss": 1.7772,
      "step": 60760
    },
    {
      "epoch": 30.910478128179044,
      "grad_norm": 35.426544189453125,
      "learning_rate": 1.9089521871820955e-05,
      "loss": 1.7686,
      "step": 60770
    },
    {
      "epoch": 30.91556459816887,
      "grad_norm": 37.91679000854492,
      "learning_rate": 1.908443540183113e-05,
      "loss": 1.7743,
      "step": 60780
    },
    {
      "epoch": 30.920651068158698,
      "grad_norm": 35.785400390625,
      "learning_rate": 1.9079348931841305e-05,
      "loss": 1.7478,
      "step": 60790
    },
    {
      "epoch": 30.925737538148525,
      "grad_norm": 37.71248245239258,
      "learning_rate": 1.9074262461851475e-05,
      "loss": 1.688,
      "step": 60800
    },
    {
      "epoch": 30.93082400813835,
      "grad_norm": 48.39778518676758,
      "learning_rate": 1.9069175991861648e-05,
      "loss": 1.6928,
      "step": 60810
    },
    {
      "epoch": 30.93591047812818,
      "grad_norm": 35.212013244628906,
      "learning_rate": 1.906408952187182e-05,
      "loss": 1.7446,
      "step": 60820
    },
    {
      "epoch": 30.940996948118006,
      "grad_norm": 38.01749801635742,
      "learning_rate": 1.9059003051881998e-05,
      "loss": 1.7069,
      "step": 60830
    },
    {
      "epoch": 30.946083418107833,
      "grad_norm": 40.38789749145508,
      "learning_rate": 1.9053916581892168e-05,
      "loss": 1.7433,
      "step": 60840
    },
    {
      "epoch": 30.95116988809766,
      "grad_norm": 35.42866897583008,
      "learning_rate": 1.904883011190234e-05,
      "loss": 1.7358,
      "step": 60850
    },
    {
      "epoch": 30.956256358087487,
      "grad_norm": 32.76683807373047,
      "learning_rate": 1.9043743641912514e-05,
      "loss": 1.7643,
      "step": 60860
    },
    {
      "epoch": 30.961342828077314,
      "grad_norm": 39.823944091796875,
      "learning_rate": 1.9038657171922687e-05,
      "loss": 1.7036,
      "step": 60870
    },
    {
      "epoch": 30.96642929806714,
      "grad_norm": 40.419219970703125,
      "learning_rate": 1.903357070193286e-05,
      "loss": 1.7071,
      "step": 60880
    },
    {
      "epoch": 30.971515768056967,
      "grad_norm": 51.750404357910156,
      "learning_rate": 1.9028484231943034e-05,
      "loss": 1.7689,
      "step": 60890
    },
    {
      "epoch": 30.976602238046794,
      "grad_norm": 41.13185119628906,
      "learning_rate": 1.9023397761953204e-05,
      "loss": 1.8143,
      "step": 60900
    },
    {
      "epoch": 30.98168870803662,
      "grad_norm": 35.83097457885742,
      "learning_rate": 1.9018311291963377e-05,
      "loss": 1.7876,
      "step": 60910
    },
    {
      "epoch": 30.98677517802645,
      "grad_norm": 45.12562942504883,
      "learning_rate": 1.901322482197355e-05,
      "loss": 1.7506,
      "step": 60920
    },
    {
      "epoch": 30.991861648016275,
      "grad_norm": 43.17095947265625,
      "learning_rate": 1.9008138351983724e-05,
      "loss": 1.7817,
      "step": 60930
    },
    {
      "epoch": 30.996948118006102,
      "grad_norm": 36.13404846191406,
      "learning_rate": 1.9003051881993897e-05,
      "loss": 1.7762,
      "step": 60940
    },
    {
      "epoch": 31.0,
      "eval_loss": 4.798539638519287,
      "eval_runtime": 2.7358,
      "eval_samples_per_second": 1014.316,
      "eval_steps_per_second": 126.835,
      "step": 60946
    },
    {
      "epoch": 31.00203458799593,
      "grad_norm": 37.87378692626953,
      "learning_rate": 1.899796541200407e-05,
      "loss": 1.6914,
      "step": 60950
    },
    {
      "epoch": 31.007121057985756,
      "grad_norm": 35.47575759887695,
      "learning_rate": 1.899287894201424e-05,
      "loss": 1.8572,
      "step": 60960
    },
    {
      "epoch": 31.012207527975583,
      "grad_norm": 48.7138557434082,
      "learning_rate": 1.8987792472024417e-05,
      "loss": 1.8624,
      "step": 60970
    },
    {
      "epoch": 31.01729399796541,
      "grad_norm": 46.60110092163086,
      "learning_rate": 1.898270600203459e-05,
      "loss": 1.7155,
      "step": 60980
    },
    {
      "epoch": 31.022380467955237,
      "grad_norm": 32.9530143737793,
      "learning_rate": 1.8977619532044763e-05,
      "loss": 1.733,
      "step": 60990
    },
    {
      "epoch": 31.027466937945068,
      "grad_norm": 38.60734939575195,
      "learning_rate": 1.8972533062054933e-05,
      "loss": 1.7936,
      "step": 61000
    },
    {
      "epoch": 31.032553407934895,
      "grad_norm": 39.433528900146484,
      "learning_rate": 1.8967446592065107e-05,
      "loss": 1.6753,
      "step": 61010
    },
    {
      "epoch": 31.037639877924722,
      "grad_norm": 45.59014129638672,
      "learning_rate": 1.8962360122075283e-05,
      "loss": 1.8268,
      "step": 61020
    },
    {
      "epoch": 31.04272634791455,
      "grad_norm": 39.87051010131836,
      "learning_rate": 1.8957273652085453e-05,
      "loss": 1.7641,
      "step": 61030
    },
    {
      "epoch": 31.047812817904376,
      "grad_norm": 43.827571868896484,
      "learning_rate": 1.8952187182095627e-05,
      "loss": 1.8376,
      "step": 61040
    },
    {
      "epoch": 31.052899287894203,
      "grad_norm": 33.6673698425293,
      "learning_rate": 1.89471007121058e-05,
      "loss": 1.7433,
      "step": 61050
    },
    {
      "epoch": 31.05798575788403,
      "grad_norm": 35.142356872558594,
      "learning_rate": 1.894201424211597e-05,
      "loss": 1.8678,
      "step": 61060
    },
    {
      "epoch": 31.063072227873857,
      "grad_norm": 34.5307731628418,
      "learning_rate": 1.8936927772126146e-05,
      "loss": 1.7652,
      "step": 61070
    },
    {
      "epoch": 31.068158697863684,
      "grad_norm": 40.08327102661133,
      "learning_rate": 1.893184130213632e-05,
      "loss": 1.8008,
      "step": 61080
    },
    {
      "epoch": 31.07324516785351,
      "grad_norm": 49.735111236572266,
      "learning_rate": 1.892675483214649e-05,
      "loss": 1.8393,
      "step": 61090
    },
    {
      "epoch": 31.078331637843338,
      "grad_norm": 43.39450454711914,
      "learning_rate": 1.8921668362156663e-05,
      "loss": 1.8293,
      "step": 61100
    },
    {
      "epoch": 31.083418107833165,
      "grad_norm": 39.88137435913086,
      "learning_rate": 1.8916581892166836e-05,
      "loss": 1.7195,
      "step": 61110
    },
    {
      "epoch": 31.08850457782299,
      "grad_norm": 39.23460388183594,
      "learning_rate": 1.8911495422177013e-05,
      "loss": 1.6737,
      "step": 61120
    },
    {
      "epoch": 31.09359104781282,
      "grad_norm": 37.16865539550781,
      "learning_rate": 1.8906408952187183e-05,
      "loss": 1.6798,
      "step": 61130
    },
    {
      "epoch": 31.098677517802646,
      "grad_norm": 40.25748825073242,
      "learning_rate": 1.8901322482197356e-05,
      "loss": 1.7018,
      "step": 61140
    },
    {
      "epoch": 31.103763987792473,
      "grad_norm": 39.49522018432617,
      "learning_rate": 1.889623601220753e-05,
      "loss": 1.7004,
      "step": 61150
    },
    {
      "epoch": 31.1088504577823,
      "grad_norm": 41.76108169555664,
      "learning_rate": 1.8891149542217702e-05,
      "loss": 1.8675,
      "step": 61160
    },
    {
      "epoch": 31.113936927772126,
      "grad_norm": 43.14298629760742,
      "learning_rate": 1.8886063072227876e-05,
      "loss": 1.8008,
      "step": 61170
    },
    {
      "epoch": 31.119023397761953,
      "grad_norm": 40.70029067993164,
      "learning_rate": 1.888097660223805e-05,
      "loss": 1.8031,
      "step": 61180
    },
    {
      "epoch": 31.12410986775178,
      "grad_norm": 41.759037017822266,
      "learning_rate": 1.887589013224822e-05,
      "loss": 1.7274,
      "step": 61190
    },
    {
      "epoch": 31.129196337741607,
      "grad_norm": 31.688217163085938,
      "learning_rate": 1.8870803662258392e-05,
      "loss": 1.681,
      "step": 61200
    },
    {
      "epoch": 31.134282807731434,
      "grad_norm": 32.470420837402344,
      "learning_rate": 1.8865717192268566e-05,
      "loss": 1.728,
      "step": 61210
    },
    {
      "epoch": 31.13936927772126,
      "grad_norm": 39.810367584228516,
      "learning_rate": 1.886063072227874e-05,
      "loss": 1.6479,
      "step": 61220
    },
    {
      "epoch": 31.14445574771109,
      "grad_norm": 39.24315643310547,
      "learning_rate": 1.8855544252288912e-05,
      "loss": 1.794,
      "step": 61230
    },
    {
      "epoch": 31.149542217700915,
      "grad_norm": 39.86491394042969,
      "learning_rate": 1.8850457782299085e-05,
      "loss": 1.6633,
      "step": 61240
    },
    {
      "epoch": 31.154628687690742,
      "grad_norm": 39.807411193847656,
      "learning_rate": 1.884537131230926e-05,
      "loss": 1.7376,
      "step": 61250
    },
    {
      "epoch": 31.15971515768057,
      "grad_norm": 33.47908020019531,
      "learning_rate": 1.8840284842319432e-05,
      "loss": 1.6794,
      "step": 61260
    },
    {
      "epoch": 31.164801627670396,
      "grad_norm": 40.85910415649414,
      "learning_rate": 1.8835198372329605e-05,
      "loss": 1.655,
      "step": 61270
    },
    {
      "epoch": 31.169888097660223,
      "grad_norm": 30.76793098449707,
      "learning_rate": 1.883011190233978e-05,
      "loss": 1.8472,
      "step": 61280
    },
    {
      "epoch": 31.17497456765005,
      "grad_norm": 49.29948043823242,
      "learning_rate": 1.882502543234995e-05,
      "loss": 1.7239,
      "step": 61290
    },
    {
      "epoch": 31.180061037639877,
      "grad_norm": 31.662372589111328,
      "learning_rate": 1.881993896236012e-05,
      "loss": 1.6799,
      "step": 61300
    },
    {
      "epoch": 31.185147507629704,
      "grad_norm": 34.59944152832031,
      "learning_rate": 1.88148524923703e-05,
      "loss": 1.7706,
      "step": 61310
    },
    {
      "epoch": 31.19023397761953,
      "grad_norm": 43.701751708984375,
      "learning_rate": 1.8809766022380468e-05,
      "loss": 1.7225,
      "step": 61320
    },
    {
      "epoch": 31.195320447609358,
      "grad_norm": 37.85908508300781,
      "learning_rate": 1.880467955239064e-05,
      "loss": 1.7484,
      "step": 61330
    },
    {
      "epoch": 31.200406917599185,
      "grad_norm": 44.56134796142578,
      "learning_rate": 1.8799593082400815e-05,
      "loss": 1.6908,
      "step": 61340
    },
    {
      "epoch": 31.205493387589012,
      "grad_norm": 34.32136917114258,
      "learning_rate": 1.8794506612410988e-05,
      "loss": 1.7999,
      "step": 61350
    },
    {
      "epoch": 31.21057985757884,
      "grad_norm": 38.8282470703125,
      "learning_rate": 1.878942014242116e-05,
      "loss": 1.7714,
      "step": 61360
    },
    {
      "epoch": 31.215666327568666,
      "grad_norm": 44.38274383544922,
      "learning_rate": 1.8784333672431335e-05,
      "loss": 1.7338,
      "step": 61370
    },
    {
      "epoch": 31.220752797558493,
      "grad_norm": 40.52004623413086,
      "learning_rate": 1.8779247202441508e-05,
      "loss": 1.7279,
      "step": 61380
    },
    {
      "epoch": 31.22583926754832,
      "grad_norm": 37.510868072509766,
      "learning_rate": 1.8774160732451678e-05,
      "loss": 1.7765,
      "step": 61390
    },
    {
      "epoch": 31.230925737538147,
      "grad_norm": 39.045284271240234,
      "learning_rate": 1.876907426246185e-05,
      "loss": 1.748,
      "step": 61400
    },
    {
      "epoch": 31.236012207527974,
      "grad_norm": 48.3482551574707,
      "learning_rate": 1.8763987792472028e-05,
      "loss": 1.7794,
      "step": 61410
    },
    {
      "epoch": 31.2410986775178,
      "grad_norm": 33.95663833618164,
      "learning_rate": 1.8758901322482198e-05,
      "loss": 1.7627,
      "step": 61420
    },
    {
      "epoch": 31.246185147507628,
      "grad_norm": 40.447086334228516,
      "learning_rate": 1.875381485249237e-05,
      "loss": 1.6917,
      "step": 61430
    },
    {
      "epoch": 31.25127161749746,
      "grad_norm": 34.7506217956543,
      "learning_rate": 1.8748728382502544e-05,
      "loss": 1.7982,
      "step": 61440
    },
    {
      "epoch": 31.256358087487286,
      "grad_norm": 40.238346099853516,
      "learning_rate": 1.8743641912512718e-05,
      "loss": 1.7494,
      "step": 61450
    },
    {
      "epoch": 31.261444557477112,
      "grad_norm": 40.3388671875,
      "learning_rate": 1.873855544252289e-05,
      "loss": 1.7227,
      "step": 61460
    },
    {
      "epoch": 31.26653102746694,
      "grad_norm": 39.23129653930664,
      "learning_rate": 1.8733468972533064e-05,
      "loss": 1.7042,
      "step": 61470
    },
    {
      "epoch": 31.271617497456766,
      "grad_norm": 33.680179595947266,
      "learning_rate": 1.8728382502543234e-05,
      "loss": 1.773,
      "step": 61480
    },
    {
      "epoch": 31.276703967446593,
      "grad_norm": 39.998165130615234,
      "learning_rate": 1.8723296032553407e-05,
      "loss": 1.6852,
      "step": 61490
    },
    {
      "epoch": 31.28179043743642,
      "grad_norm": 51.4443244934082,
      "learning_rate": 1.8718209562563584e-05,
      "loss": 1.6957,
      "step": 61500
    },
    {
      "epoch": 31.286876907426247,
      "grad_norm": 33.10203170776367,
      "learning_rate": 1.8713123092573757e-05,
      "loss": 1.7201,
      "step": 61510
    },
    {
      "epoch": 31.291963377416074,
      "grad_norm": 40.41463851928711,
      "learning_rate": 1.8708036622583927e-05,
      "loss": 1.7376,
      "step": 61520
    },
    {
      "epoch": 31.2970498474059,
      "grad_norm": 33.933494567871094,
      "learning_rate": 1.87029501525941e-05,
      "loss": 1.6601,
      "step": 61530
    },
    {
      "epoch": 31.30213631739573,
      "grad_norm": 34.22336959838867,
      "learning_rate": 1.8697863682604274e-05,
      "loss": 1.7288,
      "step": 61540
    },
    {
      "epoch": 31.307222787385555,
      "grad_norm": 40.08305740356445,
      "learning_rate": 1.8692777212614447e-05,
      "loss": 1.6847,
      "step": 61550
    },
    {
      "epoch": 31.312309257375382,
      "grad_norm": 39.8088264465332,
      "learning_rate": 1.868769074262462e-05,
      "loss": 1.814,
      "step": 61560
    },
    {
      "epoch": 31.31739572736521,
      "grad_norm": 47.04790115356445,
      "learning_rate": 1.8682604272634793e-05,
      "loss": 1.7042,
      "step": 61570
    },
    {
      "epoch": 31.322482197355036,
      "grad_norm": 30.002710342407227,
      "learning_rate": 1.8677517802644963e-05,
      "loss": 1.6692,
      "step": 61580
    },
    {
      "epoch": 31.327568667344863,
      "grad_norm": 42.25471115112305,
      "learning_rate": 1.8672431332655137e-05,
      "loss": 1.6916,
      "step": 61590
    },
    {
      "epoch": 31.33265513733469,
      "grad_norm": 41.36444091796875,
      "learning_rate": 1.8667344862665313e-05,
      "loss": 1.7018,
      "step": 61600
    },
    {
      "epoch": 31.337741607324517,
      "grad_norm": 39.532569885253906,
      "learning_rate": 1.8662258392675483e-05,
      "loss": 1.7,
      "step": 61610
    },
    {
      "epoch": 31.342828077314344,
      "grad_norm": 40.704345703125,
      "learning_rate": 1.8657171922685657e-05,
      "loss": 1.7678,
      "step": 61620
    },
    {
      "epoch": 31.34791454730417,
      "grad_norm": 32.71677780151367,
      "learning_rate": 1.865208545269583e-05,
      "loss": 1.7944,
      "step": 61630
    },
    {
      "epoch": 31.353001017293998,
      "grad_norm": 36.05541229248047,
      "learning_rate": 1.8646998982706003e-05,
      "loss": 1.7225,
      "step": 61640
    },
    {
      "epoch": 31.358087487283825,
      "grad_norm": 36.381072998046875,
      "learning_rate": 1.8641912512716176e-05,
      "loss": 1.7091,
      "step": 61650
    },
    {
      "epoch": 31.363173957273652,
      "grad_norm": 45.17962646484375,
      "learning_rate": 1.863682604272635e-05,
      "loss": 1.7902,
      "step": 61660
    },
    {
      "epoch": 31.36826042726348,
      "grad_norm": 40.968265533447266,
      "learning_rate": 1.8631739572736523e-05,
      "loss": 1.6616,
      "step": 61670
    },
    {
      "epoch": 31.373346897253306,
      "grad_norm": 40.46133041381836,
      "learning_rate": 1.8626653102746693e-05,
      "loss": 1.6958,
      "step": 61680
    },
    {
      "epoch": 31.378433367243133,
      "grad_norm": 38.033897399902344,
      "learning_rate": 1.8621566632756866e-05,
      "loss": 1.7665,
      "step": 61690
    },
    {
      "epoch": 31.38351983723296,
      "grad_norm": 38.278663635253906,
      "learning_rate": 1.8616480162767043e-05,
      "loss": 1.7426,
      "step": 61700
    },
    {
      "epoch": 31.388606307222787,
      "grad_norm": 39.088653564453125,
      "learning_rate": 1.8611393692777213e-05,
      "loss": 1.68,
      "step": 61710
    },
    {
      "epoch": 31.393692777212614,
      "grad_norm": 29.798059463500977,
      "learning_rate": 1.8606307222787386e-05,
      "loss": 1.6988,
      "step": 61720
    },
    {
      "epoch": 31.39877924720244,
      "grad_norm": 43.92524337768555,
      "learning_rate": 1.860122075279756e-05,
      "loss": 1.7127,
      "step": 61730
    },
    {
      "epoch": 31.403865717192268,
      "grad_norm": 37.27103805541992,
      "learning_rate": 1.8596134282807733e-05,
      "loss": 1.8136,
      "step": 61740
    },
    {
      "epoch": 31.408952187182095,
      "grad_norm": 40.12039566040039,
      "learning_rate": 1.8591047812817906e-05,
      "loss": 1.5844,
      "step": 61750
    },
    {
      "epoch": 31.414038657171922,
      "grad_norm": 43.46427917480469,
      "learning_rate": 1.858596134282808e-05,
      "loss": 1.6796,
      "step": 61760
    },
    {
      "epoch": 31.41912512716175,
      "grad_norm": 40.280479431152344,
      "learning_rate": 1.858087487283825e-05,
      "loss": 1.7207,
      "step": 61770
    },
    {
      "epoch": 31.424211597151576,
      "grad_norm": 34.275909423828125,
      "learning_rate": 1.8575788402848422e-05,
      "loss": 1.7522,
      "step": 61780
    },
    {
      "epoch": 31.429298067141403,
      "grad_norm": 42.13941955566406,
      "learning_rate": 1.85707019328586e-05,
      "loss": 1.7392,
      "step": 61790
    },
    {
      "epoch": 31.43438453713123,
      "grad_norm": 43.21119689941406,
      "learning_rate": 1.8565615462868772e-05,
      "loss": 1.7232,
      "step": 61800
    },
    {
      "epoch": 31.439471007121057,
      "grad_norm": 40.1001091003418,
      "learning_rate": 1.8560528992878942e-05,
      "loss": 1.7413,
      "step": 61810
    },
    {
      "epoch": 31.444557477110884,
      "grad_norm": 33.33002853393555,
      "learning_rate": 1.8555442522889115e-05,
      "loss": 1.7569,
      "step": 61820
    },
    {
      "epoch": 31.44964394710071,
      "grad_norm": 35.93186950683594,
      "learning_rate": 1.855035605289929e-05,
      "loss": 1.7264,
      "step": 61830
    },
    {
      "epoch": 31.454730417090538,
      "grad_norm": 52.63235092163086,
      "learning_rate": 1.8545269582909462e-05,
      "loss": 1.743,
      "step": 61840
    },
    {
      "epoch": 31.459816887080365,
      "grad_norm": 31.582923889160156,
      "learning_rate": 1.8540183112919635e-05,
      "loss": 1.6804,
      "step": 61850
    },
    {
      "epoch": 31.46490335707019,
      "grad_norm": 32.34431076049805,
      "learning_rate": 1.853509664292981e-05,
      "loss": 1.7003,
      "step": 61860
    },
    {
      "epoch": 31.46998982706002,
      "grad_norm": 37.79499053955078,
      "learning_rate": 1.853001017293998e-05,
      "loss": 1.6748,
      "step": 61870
    },
    {
      "epoch": 31.475076297049846,
      "grad_norm": 39.042110443115234,
      "learning_rate": 1.852492370295015e-05,
      "loss": 1.6773,
      "step": 61880
    },
    {
      "epoch": 31.480162767039676,
      "grad_norm": 46.96318817138672,
      "learning_rate": 1.851983723296033e-05,
      "loss": 1.7294,
      "step": 61890
    },
    {
      "epoch": 31.485249237029503,
      "grad_norm": 32.113582611083984,
      "learning_rate": 1.8514750762970498e-05,
      "loss": 1.7402,
      "step": 61900
    },
    {
      "epoch": 31.49033570701933,
      "grad_norm": 42.624061584472656,
      "learning_rate": 1.850966429298067e-05,
      "loss": 1.7829,
      "step": 61910
    },
    {
      "epoch": 31.495422177009157,
      "grad_norm": 41.55781555175781,
      "learning_rate": 1.8504577822990845e-05,
      "loss": 1.6645,
      "step": 61920
    },
    {
      "epoch": 31.500508646998984,
      "grad_norm": 44.03754425048828,
      "learning_rate": 1.8499491353001018e-05,
      "loss": 1.6776,
      "step": 61930
    },
    {
      "epoch": 31.50559511698881,
      "grad_norm": 33.917842864990234,
      "learning_rate": 1.849440488301119e-05,
      "loss": 1.7615,
      "step": 61940
    },
    {
      "epoch": 31.510681586978638,
      "grad_norm": 37.9342041015625,
      "learning_rate": 1.8489318413021365e-05,
      "loss": 1.7699,
      "step": 61950
    },
    {
      "epoch": 31.515768056968465,
      "grad_norm": 37.76018524169922,
      "learning_rate": 1.8484231943031538e-05,
      "loss": 1.7118,
      "step": 61960
    },
    {
      "epoch": 31.520854526958292,
      "grad_norm": 56.01942443847656,
      "learning_rate": 1.8479145473041708e-05,
      "loss": 1.7277,
      "step": 61970
    },
    {
      "epoch": 31.52594099694812,
      "grad_norm": 45.953216552734375,
      "learning_rate": 1.8474059003051884e-05,
      "loss": 1.6689,
      "step": 61980
    },
    {
      "epoch": 31.531027466937946,
      "grad_norm": 40.0157470703125,
      "learning_rate": 1.8468972533062058e-05,
      "loss": 1.7163,
      "step": 61990
    },
    {
      "epoch": 31.536113936927773,
      "grad_norm": 39.39112854003906,
      "learning_rate": 1.8463886063072228e-05,
      "loss": 1.7081,
      "step": 62000
    },
    {
      "epoch": 31.5412004069176,
      "grad_norm": 37.29513931274414,
      "learning_rate": 1.84587995930824e-05,
      "loss": 1.7838,
      "step": 62010
    },
    {
      "epoch": 31.546286876907427,
      "grad_norm": 31.271413803100586,
      "learning_rate": 1.8453713123092574e-05,
      "loss": 1.6609,
      "step": 62020
    },
    {
      "epoch": 31.551373346897254,
      "grad_norm": 47.44449234008789,
      "learning_rate": 1.8448626653102748e-05,
      "loss": 1.6571,
      "step": 62030
    },
    {
      "epoch": 31.55645981688708,
      "grad_norm": 39.3445930480957,
      "learning_rate": 1.844354018311292e-05,
      "loss": 1.7492,
      "step": 62040
    },
    {
      "epoch": 31.561546286876908,
      "grad_norm": 47.84841537475586,
      "learning_rate": 1.8438453713123094e-05,
      "loss": 1.6712,
      "step": 62050
    },
    {
      "epoch": 31.566632756866735,
      "grad_norm": 35.51580810546875,
      "learning_rate": 1.8433367243133267e-05,
      "loss": 1.6326,
      "step": 62060
    },
    {
      "epoch": 31.571719226856562,
      "grad_norm": 33.3928108215332,
      "learning_rate": 1.8428280773143437e-05,
      "loss": 1.696,
      "step": 62070
    },
    {
      "epoch": 31.57680569684639,
      "grad_norm": 46.799461364746094,
      "learning_rate": 1.8423194303153614e-05,
      "loss": 1.7395,
      "step": 62080
    },
    {
      "epoch": 31.581892166836216,
      "grad_norm": 40.04927062988281,
      "learning_rate": 1.8418107833163787e-05,
      "loss": 1.7213,
      "step": 62090
    },
    {
      "epoch": 31.586978636826043,
      "grad_norm": 32.971961975097656,
      "learning_rate": 1.8413021363173957e-05,
      "loss": 1.7941,
      "step": 62100
    },
    {
      "epoch": 31.59206510681587,
      "grad_norm": 37.77688217163086,
      "learning_rate": 1.840793489318413e-05,
      "loss": 1.7408,
      "step": 62110
    },
    {
      "epoch": 31.597151576805697,
      "grad_norm": 37.55860137939453,
      "learning_rate": 1.8402848423194304e-05,
      "loss": 1.7293,
      "step": 62120
    },
    {
      "epoch": 31.602238046795524,
      "grad_norm": 33.96662902832031,
      "learning_rate": 1.8397761953204477e-05,
      "loss": 1.6711,
      "step": 62130
    },
    {
      "epoch": 31.60732451678535,
      "grad_norm": 44.6876335144043,
      "learning_rate": 1.839267548321465e-05,
      "loss": 1.7039,
      "step": 62140
    },
    {
      "epoch": 31.612410986775178,
      "grad_norm": 42.86520004272461,
      "learning_rate": 1.8387589013224823e-05,
      "loss": 1.7011,
      "step": 62150
    },
    {
      "epoch": 31.617497456765005,
      "grad_norm": 39.7850341796875,
      "learning_rate": 1.8382502543234993e-05,
      "loss": 1.7203,
      "step": 62160
    },
    {
      "epoch": 31.62258392675483,
      "grad_norm": 43.02821350097656,
      "learning_rate": 1.8377416073245167e-05,
      "loss": 1.7289,
      "step": 62170
    },
    {
      "epoch": 31.62767039674466,
      "grad_norm": 36.44362258911133,
      "learning_rate": 1.8372329603255343e-05,
      "loss": 1.7167,
      "step": 62180
    },
    {
      "epoch": 31.632756866734486,
      "grad_norm": 43.734012603759766,
      "learning_rate": 1.8367243133265517e-05,
      "loss": 1.7631,
      "step": 62190
    },
    {
      "epoch": 31.637843336724313,
      "grad_norm": 31.96537971496582,
      "learning_rate": 1.8362156663275687e-05,
      "loss": 1.785,
      "step": 62200
    },
    {
      "epoch": 31.64292980671414,
      "grad_norm": 36.43196105957031,
      "learning_rate": 1.835707019328586e-05,
      "loss": 1.7266,
      "step": 62210
    },
    {
      "epoch": 31.648016276703967,
      "grad_norm": 46.99606704711914,
      "learning_rate": 1.8351983723296033e-05,
      "loss": 1.7613,
      "step": 62220
    },
    {
      "epoch": 31.653102746693794,
      "grad_norm": 38.064552307128906,
      "learning_rate": 1.8346897253306206e-05,
      "loss": 1.847,
      "step": 62230
    },
    {
      "epoch": 31.65818921668362,
      "grad_norm": 32.91030502319336,
      "learning_rate": 1.834181078331638e-05,
      "loss": 1.7091,
      "step": 62240
    },
    {
      "epoch": 31.663275686673447,
      "grad_norm": 31.59612464904785,
      "learning_rate": 1.8336724313326553e-05,
      "loss": 1.7276,
      "step": 62250
    },
    {
      "epoch": 31.668362156663274,
      "grad_norm": 43.10361862182617,
      "learning_rate": 1.8331637843336723e-05,
      "loss": 1.744,
      "step": 62260
    },
    {
      "epoch": 31.6734486266531,
      "grad_norm": 37.2309455871582,
      "learning_rate": 1.83265513733469e-05,
      "loss": 1.7026,
      "step": 62270
    },
    {
      "epoch": 31.67853509664293,
      "grad_norm": 43.314815521240234,
      "learning_rate": 1.8321464903357073e-05,
      "loss": 1.7617,
      "step": 62280
    },
    {
      "epoch": 31.683621566632755,
      "grad_norm": 34.98461151123047,
      "learning_rate": 1.8316378433367243e-05,
      "loss": 1.7425,
      "step": 62290
    },
    {
      "epoch": 31.688708036622582,
      "grad_norm": 31.112220764160156,
      "learning_rate": 1.8311291963377416e-05,
      "loss": 1.713,
      "step": 62300
    },
    {
      "epoch": 31.69379450661241,
      "grad_norm": 55.21365737915039,
      "learning_rate": 1.830620549338759e-05,
      "loss": 1.7423,
      "step": 62310
    },
    {
      "epoch": 31.698880976602236,
      "grad_norm": 47.842529296875,
      "learning_rate": 1.8301119023397763e-05,
      "loss": 1.7354,
      "step": 62320
    },
    {
      "epoch": 31.703967446592067,
      "grad_norm": 37.84389877319336,
      "learning_rate": 1.8296032553407936e-05,
      "loss": 1.7745,
      "step": 62330
    },
    {
      "epoch": 31.709053916581894,
      "grad_norm": 33.01079177856445,
      "learning_rate": 1.829094608341811e-05,
      "loss": 1.7263,
      "step": 62340
    },
    {
      "epoch": 31.71414038657172,
      "grad_norm": 44.82138442993164,
      "learning_rate": 1.8285859613428282e-05,
      "loss": 1.7837,
      "step": 62350
    },
    {
      "epoch": 31.719226856561548,
      "grad_norm": 42.57996368408203,
      "learning_rate": 1.8280773143438452e-05,
      "loss": 1.769,
      "step": 62360
    },
    {
      "epoch": 31.724313326551375,
      "grad_norm": 40.825199127197266,
      "learning_rate": 1.827568667344863e-05,
      "loss": 1.6709,
      "step": 62370
    },
    {
      "epoch": 31.729399796541202,
      "grad_norm": 41.2891960144043,
      "learning_rate": 1.8270600203458802e-05,
      "loss": 1.7045,
      "step": 62380
    },
    {
      "epoch": 31.73448626653103,
      "grad_norm": 40.066856384277344,
      "learning_rate": 1.8265513733468972e-05,
      "loss": 1.7044,
      "step": 62390
    },
    {
      "epoch": 31.739572736520856,
      "grad_norm": 34.49386215209961,
      "learning_rate": 1.8260427263479145e-05,
      "loss": 1.7167,
      "step": 62400
    },
    {
      "epoch": 31.744659206510683,
      "grad_norm": 38.78734588623047,
      "learning_rate": 1.825534079348932e-05,
      "loss": 1.7968,
      "step": 62410
    },
    {
      "epoch": 31.74974567650051,
      "grad_norm": 33.40423583984375,
      "learning_rate": 1.8250254323499492e-05,
      "loss": 1.7167,
      "step": 62420
    },
    {
      "epoch": 31.754832146490337,
      "grad_norm": 49.30678939819336,
      "learning_rate": 1.8245167853509665e-05,
      "loss": 1.7833,
      "step": 62430
    },
    {
      "epoch": 31.759918616480164,
      "grad_norm": 35.14928436279297,
      "learning_rate": 1.824008138351984e-05,
      "loss": 1.7707,
      "step": 62440
    },
    {
      "epoch": 31.76500508646999,
      "grad_norm": 45.65630340576172,
      "learning_rate": 1.823499491353001e-05,
      "loss": 1.7729,
      "step": 62450
    },
    {
      "epoch": 31.770091556459818,
      "grad_norm": 36.59627151489258,
      "learning_rate": 1.8229908443540185e-05,
      "loss": 1.757,
      "step": 62460
    },
    {
      "epoch": 31.775178026449645,
      "grad_norm": 36.25947570800781,
      "learning_rate": 1.822482197355036e-05,
      "loss": 1.7184,
      "step": 62470
    },
    {
      "epoch": 31.78026449643947,
      "grad_norm": 36.96023941040039,
      "learning_rate": 1.821973550356053e-05,
      "loss": 1.75,
      "step": 62480
    },
    {
      "epoch": 31.7853509664293,
      "grad_norm": 32.19573211669922,
      "learning_rate": 1.82146490335707e-05,
      "loss": 1.7458,
      "step": 62490
    },
    {
      "epoch": 31.790437436419126,
      "grad_norm": 47.94792556762695,
      "learning_rate": 1.8209562563580875e-05,
      "loss": 1.7853,
      "step": 62500
    },
    {
      "epoch": 31.795523906408953,
      "grad_norm": 35.914493560791016,
      "learning_rate": 1.8204476093591048e-05,
      "loss": 1.8129,
      "step": 62510
    },
    {
      "epoch": 31.80061037639878,
      "grad_norm": 37.90879440307617,
      "learning_rate": 1.819938962360122e-05,
      "loss": 1.7302,
      "step": 62520
    },
    {
      "epoch": 31.805696846388607,
      "grad_norm": 37.14250183105469,
      "learning_rate": 1.8194303153611395e-05,
      "loss": 1.7275,
      "step": 62530
    },
    {
      "epoch": 31.810783316378433,
      "grad_norm": 50.87014389038086,
      "learning_rate": 1.8189216683621568e-05,
      "loss": 1.6954,
      "step": 62540
    },
    {
      "epoch": 31.81586978636826,
      "grad_norm": 34.12681198120117,
      "learning_rate": 1.8184130213631738e-05,
      "loss": 1.7391,
      "step": 62550
    },
    {
      "epoch": 31.820956256358087,
      "grad_norm": 38.72663497924805,
      "learning_rate": 1.8179043743641914e-05,
      "loss": 1.6284,
      "step": 62560
    },
    {
      "epoch": 31.826042726347914,
      "grad_norm": 34.559234619140625,
      "learning_rate": 1.8173957273652088e-05,
      "loss": 1.6113,
      "step": 62570
    },
    {
      "epoch": 31.83112919633774,
      "grad_norm": 51.38548278808594,
      "learning_rate": 1.8168870803662258e-05,
      "loss": 1.6598,
      "step": 62580
    },
    {
      "epoch": 31.83621566632757,
      "grad_norm": 37.2366943359375,
      "learning_rate": 1.816378433367243e-05,
      "loss": 1.7078,
      "step": 62590
    },
    {
      "epoch": 31.841302136317395,
      "grad_norm": 45.08460235595703,
      "learning_rate": 1.8158697863682604e-05,
      "loss": 1.7027,
      "step": 62600
    },
    {
      "epoch": 31.846388606307222,
      "grad_norm": 35.550113677978516,
      "learning_rate": 1.815361139369278e-05,
      "loss": 1.723,
      "step": 62610
    },
    {
      "epoch": 31.85147507629705,
      "grad_norm": 34.43916702270508,
      "learning_rate": 1.814852492370295e-05,
      "loss": 1.7367,
      "step": 62620
    },
    {
      "epoch": 31.856561546286876,
      "grad_norm": 38.629337310791016,
      "learning_rate": 1.8143438453713124e-05,
      "loss": 1.6548,
      "step": 62630
    },
    {
      "epoch": 31.861648016276703,
      "grad_norm": 40.102237701416016,
      "learning_rate": 1.8138351983723297e-05,
      "loss": 1.7163,
      "step": 62640
    },
    {
      "epoch": 31.86673448626653,
      "grad_norm": 40.09159469604492,
      "learning_rate": 1.8133265513733467e-05,
      "loss": 1.7225,
      "step": 62650
    },
    {
      "epoch": 31.871820956256357,
      "grad_norm": 42.07631301879883,
      "learning_rate": 1.8128179043743644e-05,
      "loss": 1.7991,
      "step": 62660
    },
    {
      "epoch": 31.876907426246184,
      "grad_norm": 59.22080993652344,
      "learning_rate": 1.8123092573753817e-05,
      "loss": 1.8072,
      "step": 62670
    },
    {
      "epoch": 31.88199389623601,
      "grad_norm": 38.30678176879883,
      "learning_rate": 1.8118006103763987e-05,
      "loss": 1.6583,
      "step": 62680
    },
    {
      "epoch": 31.887080366225838,
      "grad_norm": 32.18422317504883,
      "learning_rate": 1.811291963377416e-05,
      "loss": 1.8024,
      "step": 62690
    },
    {
      "epoch": 31.892166836215665,
      "grad_norm": 43.623050689697266,
      "learning_rate": 1.8107833163784334e-05,
      "loss": 1.7275,
      "step": 62700
    },
    {
      "epoch": 31.897253306205492,
      "grad_norm": 39.26604080200195,
      "learning_rate": 1.8102746693794507e-05,
      "loss": 1.6715,
      "step": 62710
    },
    {
      "epoch": 31.90233977619532,
      "grad_norm": 35.75481414794922,
      "learning_rate": 1.809766022380468e-05,
      "loss": 1.7031,
      "step": 62720
    },
    {
      "epoch": 31.907426246185146,
      "grad_norm": 42.408592224121094,
      "learning_rate": 1.8092573753814854e-05,
      "loss": 1.7038,
      "step": 62730
    },
    {
      "epoch": 31.912512716174973,
      "grad_norm": 48.70609664916992,
      "learning_rate": 1.8087487283825027e-05,
      "loss": 1.7204,
      "step": 62740
    },
    {
      "epoch": 31.9175991861648,
      "grad_norm": 38.25547409057617,
      "learning_rate": 1.80824008138352e-05,
      "loss": 1.7558,
      "step": 62750
    },
    {
      "epoch": 31.922685656154627,
      "grad_norm": 37.3637580871582,
      "learning_rate": 1.8077314343845373e-05,
      "loss": 1.7675,
      "step": 62760
    },
    {
      "epoch": 31.927772126144454,
      "grad_norm": 38.153865814208984,
      "learning_rate": 1.8072227873855547e-05,
      "loss": 1.7374,
      "step": 62770
    },
    {
      "epoch": 31.93285859613428,
      "grad_norm": 37.1760368347168,
      "learning_rate": 1.8067141403865717e-05,
      "loss": 1.7184,
      "step": 62780
    },
    {
      "epoch": 31.93794506612411,
      "grad_norm": 36.50288772583008,
      "learning_rate": 1.806205493387589e-05,
      "loss": 1.7235,
      "step": 62790
    },
    {
      "epoch": 31.94303153611394,
      "grad_norm": 35.680458068847656,
      "learning_rate": 1.8056968463886063e-05,
      "loss": 1.7613,
      "step": 62800
    },
    {
      "epoch": 31.948118006103766,
      "grad_norm": 42.92558288574219,
      "learning_rate": 1.8051881993896236e-05,
      "loss": 1.7017,
      "step": 62810
    },
    {
      "epoch": 31.953204476093592,
      "grad_norm": 40.90772247314453,
      "learning_rate": 1.804679552390641e-05,
      "loss": 1.7687,
      "step": 62820
    },
    {
      "epoch": 31.95829094608342,
      "grad_norm": 41.199462890625,
      "learning_rate": 1.8041709053916583e-05,
      "loss": 1.7617,
      "step": 62830
    },
    {
      "epoch": 31.963377416073246,
      "grad_norm": 40.047088623046875,
      "learning_rate": 1.8036622583926753e-05,
      "loss": 1.7018,
      "step": 62840
    },
    {
      "epoch": 31.968463886063073,
      "grad_norm": 42.55162811279297,
      "learning_rate": 1.803153611393693e-05,
      "loss": 1.7259,
      "step": 62850
    },
    {
      "epoch": 31.9735503560529,
      "grad_norm": 38.1729736328125,
      "learning_rate": 1.8026449643947103e-05,
      "loss": 1.6567,
      "step": 62860
    },
    {
      "epoch": 31.978636826042727,
      "grad_norm": 39.06232452392578,
      "learning_rate": 1.8021363173957276e-05,
      "loss": 1.7004,
      "step": 62870
    },
    {
      "epoch": 31.983723296032554,
      "grad_norm": 53.6194953918457,
      "learning_rate": 1.8016276703967446e-05,
      "loss": 1.7473,
      "step": 62880
    },
    {
      "epoch": 31.98880976602238,
      "grad_norm": 44.6927604675293,
      "learning_rate": 1.801119023397762e-05,
      "loss": 1.691,
      "step": 62890
    },
    {
      "epoch": 31.99389623601221,
      "grad_norm": 36.65610885620117,
      "learning_rate": 1.8006103763987796e-05,
      "loss": 1.6871,
      "step": 62900
    },
    {
      "epoch": 31.998982706002035,
      "grad_norm": 31.480915069580078,
      "learning_rate": 1.8001017293997966e-05,
      "loss": 1.7825,
      "step": 62910
    },
    {
      "epoch": 32.0,
      "eval_loss": 4.836447238922119,
      "eval_runtime": 2.7476,
      "eval_samples_per_second": 1009.959,
      "eval_steps_per_second": 126.29,
      "step": 62912
    },
    {
      "epoch": 32.00406917599186,
      "grad_norm": 40.21754455566406,
      "learning_rate": 1.799593082400814e-05,
      "loss": 1.7174,
      "step": 62920
    },
    {
      "epoch": 32.009155645981686,
      "grad_norm": 43.347328186035156,
      "learning_rate": 1.7990844354018312e-05,
      "loss": 1.6669,
      "step": 62930
    },
    {
      "epoch": 32.01424211597151,
      "grad_norm": 42.51255416870117,
      "learning_rate": 1.7985757884028486e-05,
      "loss": 1.7005,
      "step": 62940
    },
    {
      "epoch": 32.01932858596134,
      "grad_norm": 48.44271469116211,
      "learning_rate": 1.798067141403866e-05,
      "loss": 1.7474,
      "step": 62950
    },
    {
      "epoch": 32.02441505595117,
      "grad_norm": 42.64889907836914,
      "learning_rate": 1.7975584944048832e-05,
      "loss": 1.6871,
      "step": 62960
    },
    {
      "epoch": 32.029501525940994,
      "grad_norm": 42.50069808959961,
      "learning_rate": 1.7970498474059002e-05,
      "loss": 1.7345,
      "step": 62970
    },
    {
      "epoch": 32.03458799593082,
      "grad_norm": 43.83341979980469,
      "learning_rate": 1.7965412004069175e-05,
      "loss": 1.6789,
      "step": 62980
    },
    {
      "epoch": 32.03967446592065,
      "grad_norm": 35.03466033935547,
      "learning_rate": 1.796032553407935e-05,
      "loss": 1.7372,
      "step": 62990
    },
    {
      "epoch": 32.044760935910475,
      "grad_norm": 45.66886520385742,
      "learning_rate": 1.7955239064089525e-05,
      "loss": 1.8213,
      "step": 63000
    },
    {
      "epoch": 32.04984740590031,
      "grad_norm": 38.16459655761719,
      "learning_rate": 1.7950152594099695e-05,
      "loss": 1.6939,
      "step": 63010
    },
    {
      "epoch": 32.054933875890136,
      "grad_norm": 35.59281539916992,
      "learning_rate": 1.794506612410987e-05,
      "loss": 1.7465,
      "step": 63020
    },
    {
      "epoch": 32.06002034587996,
      "grad_norm": 39.14804458618164,
      "learning_rate": 1.7939979654120042e-05,
      "loss": 1.6686,
      "step": 63030
    },
    {
      "epoch": 32.06510681586979,
      "grad_norm": 45.27989959716797,
      "learning_rate": 1.7934893184130215e-05,
      "loss": 1.8092,
      "step": 63040
    },
    {
      "epoch": 32.07019328585962,
      "grad_norm": 49.13633346557617,
      "learning_rate": 1.792980671414039e-05,
      "loss": 1.7059,
      "step": 63050
    },
    {
      "epoch": 32.075279755849444,
      "grad_norm": 39.578453063964844,
      "learning_rate": 1.792472024415056e-05,
      "loss": 1.6548,
      "step": 63060
    },
    {
      "epoch": 32.08036622583927,
      "grad_norm": 45.811683654785156,
      "learning_rate": 1.791963377416073e-05,
      "loss": 1.6542,
      "step": 63070
    },
    {
      "epoch": 32.0854526958291,
      "grad_norm": 36.47154235839844,
      "learning_rate": 1.7914547304170905e-05,
      "loss": 1.7107,
      "step": 63080
    },
    {
      "epoch": 32.090539165818925,
      "grad_norm": 39.092586517333984,
      "learning_rate": 1.790946083418108e-05,
      "loss": 1.7112,
      "step": 63090
    },
    {
      "epoch": 32.09562563580875,
      "grad_norm": 49.943660736083984,
      "learning_rate": 1.790437436419125e-05,
      "loss": 1.6242,
      "step": 63100
    },
    {
      "epoch": 32.10071210579858,
      "grad_norm": 31.996774673461914,
      "learning_rate": 1.7899287894201425e-05,
      "loss": 1.7487,
      "step": 63110
    },
    {
      "epoch": 32.105798575788405,
      "grad_norm": 38.47140884399414,
      "learning_rate": 1.7894201424211598e-05,
      "loss": 1.7054,
      "step": 63120
    },
    {
      "epoch": 32.11088504577823,
      "grad_norm": 37.997703552246094,
      "learning_rate": 1.788911495422177e-05,
      "loss": 1.7909,
      "step": 63130
    },
    {
      "epoch": 32.11597151576806,
      "grad_norm": 36.3209228515625,
      "learning_rate": 1.7884028484231945e-05,
      "loss": 1.6483,
      "step": 63140
    },
    {
      "epoch": 32.121057985757886,
      "grad_norm": 35.77306365966797,
      "learning_rate": 1.7878942014242118e-05,
      "loss": 1.6776,
      "step": 63150
    },
    {
      "epoch": 32.12614445574771,
      "grad_norm": 36.4538688659668,
      "learning_rate": 1.787385554425229e-05,
      "loss": 1.7677,
      "step": 63160
    },
    {
      "epoch": 32.13123092573754,
      "grad_norm": 37.8176383972168,
      "learning_rate": 1.786876907426246e-05,
      "loss": 1.6636,
      "step": 63170
    },
    {
      "epoch": 32.13631739572737,
      "grad_norm": 52.39815139770508,
      "learning_rate": 1.7863682604272634e-05,
      "loss": 1.7507,
      "step": 63180
    },
    {
      "epoch": 32.141403865717194,
      "grad_norm": 35.71225357055664,
      "learning_rate": 1.785859613428281e-05,
      "loss": 1.7123,
      "step": 63190
    },
    {
      "epoch": 32.14649033570702,
      "grad_norm": 41.88285827636719,
      "learning_rate": 1.785350966429298e-05,
      "loss": 1.8329,
      "step": 63200
    },
    {
      "epoch": 32.15157680569685,
      "grad_norm": 29.985973358154297,
      "learning_rate": 1.7848423194303154e-05,
      "loss": 1.7386,
      "step": 63210
    },
    {
      "epoch": 32.156663275686675,
      "grad_norm": 42.230430603027344,
      "learning_rate": 1.7843336724313327e-05,
      "loss": 1.7279,
      "step": 63220
    },
    {
      "epoch": 32.1617497456765,
      "grad_norm": 40.80510330200195,
      "learning_rate": 1.78382502543235e-05,
      "loss": 1.7565,
      "step": 63230
    },
    {
      "epoch": 32.16683621566633,
      "grad_norm": 38.3403434753418,
      "learning_rate": 1.7833163784333674e-05,
      "loss": 1.6741,
      "step": 63240
    },
    {
      "epoch": 32.171922685656156,
      "grad_norm": 53.350791931152344,
      "learning_rate": 1.7828077314343847e-05,
      "loss": 1.7278,
      "step": 63250
    },
    {
      "epoch": 32.17700915564598,
      "grad_norm": 40.5131950378418,
      "learning_rate": 1.7822990844354017e-05,
      "loss": 1.6966,
      "step": 63260
    },
    {
      "epoch": 32.18209562563581,
      "grad_norm": 55.36127471923828,
      "learning_rate": 1.781790437436419e-05,
      "loss": 1.6934,
      "step": 63270
    },
    {
      "epoch": 32.18718209562564,
      "grad_norm": 38.40829086303711,
      "learning_rate": 1.7812817904374367e-05,
      "loss": 1.7332,
      "step": 63280
    },
    {
      "epoch": 32.192268565615464,
      "grad_norm": 38.037986755371094,
      "learning_rate": 1.780773143438454e-05,
      "loss": 1.7046,
      "step": 63290
    },
    {
      "epoch": 32.19735503560529,
      "grad_norm": 48.369693756103516,
      "learning_rate": 1.780264496439471e-05,
      "loss": 1.68,
      "step": 63300
    },
    {
      "epoch": 32.20244150559512,
      "grad_norm": 37.76565170288086,
      "learning_rate": 1.7797558494404884e-05,
      "loss": 1.7391,
      "step": 63310
    },
    {
      "epoch": 32.207527975584945,
      "grad_norm": 32.1522216796875,
      "learning_rate": 1.7792472024415057e-05,
      "loss": 1.8353,
      "step": 63320
    },
    {
      "epoch": 32.21261444557477,
      "grad_norm": 30.863323211669922,
      "learning_rate": 1.778738555442523e-05,
      "loss": 1.7436,
      "step": 63330
    },
    {
      "epoch": 32.2177009155646,
      "grad_norm": 33.026939392089844,
      "learning_rate": 1.7782299084435403e-05,
      "loss": 1.6827,
      "step": 63340
    },
    {
      "epoch": 32.222787385554426,
      "grad_norm": 39.07155227661133,
      "learning_rate": 1.7777212614445577e-05,
      "loss": 1.6876,
      "step": 63350
    },
    {
      "epoch": 32.22787385554425,
      "grad_norm": 36.90207290649414,
      "learning_rate": 1.7772126144455747e-05,
      "loss": 1.7228,
      "step": 63360
    },
    {
      "epoch": 32.23296032553408,
      "grad_norm": 50.799652099609375,
      "learning_rate": 1.776703967446592e-05,
      "loss": 1.7439,
      "step": 63370
    },
    {
      "epoch": 32.23804679552391,
      "grad_norm": 40.22282791137695,
      "learning_rate": 1.7761953204476096e-05,
      "loss": 1.7649,
      "step": 63380
    },
    {
      "epoch": 32.243133265513734,
      "grad_norm": 37.86624526977539,
      "learning_rate": 1.7756866734486266e-05,
      "loss": 1.7456,
      "step": 63390
    },
    {
      "epoch": 32.24821973550356,
      "grad_norm": 35.31111526489258,
      "learning_rate": 1.775178026449644e-05,
      "loss": 1.6986,
      "step": 63400
    },
    {
      "epoch": 32.25330620549339,
      "grad_norm": 35.012725830078125,
      "learning_rate": 1.7746693794506613e-05,
      "loss": 1.7535,
      "step": 63410
    },
    {
      "epoch": 32.258392675483215,
      "grad_norm": 33.997676849365234,
      "learning_rate": 1.7741607324516786e-05,
      "loss": 1.7357,
      "step": 63420
    },
    {
      "epoch": 32.26347914547304,
      "grad_norm": 39.09895324707031,
      "learning_rate": 1.773652085452696e-05,
      "loss": 1.6943,
      "step": 63430
    },
    {
      "epoch": 32.26856561546287,
      "grad_norm": 43.122257232666016,
      "learning_rate": 1.7731434384537133e-05,
      "loss": 1.7692,
      "step": 63440
    },
    {
      "epoch": 32.273652085452696,
      "grad_norm": 37.32654571533203,
      "learning_rate": 1.7726347914547306e-05,
      "loss": 1.6688,
      "step": 63450
    },
    {
      "epoch": 32.27873855544252,
      "grad_norm": 33.328426361083984,
      "learning_rate": 1.7721261444557476e-05,
      "loss": 1.7798,
      "step": 63460
    },
    {
      "epoch": 32.28382502543235,
      "grad_norm": 44.536075592041016,
      "learning_rate": 1.771617497456765e-05,
      "loss": 1.7705,
      "step": 63470
    },
    {
      "epoch": 32.28891149542218,
      "grad_norm": 40.31612014770508,
      "learning_rate": 1.7711088504577826e-05,
      "loss": 1.6703,
      "step": 63480
    },
    {
      "epoch": 32.293997965412004,
      "grad_norm": 42.50941848754883,
      "learning_rate": 1.7706002034587996e-05,
      "loss": 1.7033,
      "step": 63490
    },
    {
      "epoch": 32.29908443540183,
      "grad_norm": 44.458187103271484,
      "learning_rate": 1.770091556459817e-05,
      "loss": 1.7076,
      "step": 63500
    },
    {
      "epoch": 32.30417090539166,
      "grad_norm": 34.43963623046875,
      "learning_rate": 1.7695829094608342e-05,
      "loss": 1.705,
      "step": 63510
    },
    {
      "epoch": 32.309257375381485,
      "grad_norm": 47.03971481323242,
      "learning_rate": 1.7690742624618516e-05,
      "loss": 1.7058,
      "step": 63520
    },
    {
      "epoch": 32.31434384537131,
      "grad_norm": 49.956703186035156,
      "learning_rate": 1.768565615462869e-05,
      "loss": 1.6156,
      "step": 63530
    },
    {
      "epoch": 32.31943031536114,
      "grad_norm": 41.35780715942383,
      "learning_rate": 1.7680569684638862e-05,
      "loss": 1.7289,
      "step": 63540
    },
    {
      "epoch": 32.324516785350966,
      "grad_norm": 44.102054595947266,
      "learning_rate": 1.7675483214649035e-05,
      "loss": 1.64,
      "step": 63550
    },
    {
      "epoch": 32.32960325534079,
      "grad_norm": 38.232120513916016,
      "learning_rate": 1.7670396744659205e-05,
      "loss": 1.711,
      "step": 63560
    },
    {
      "epoch": 32.33468972533062,
      "grad_norm": 47.01249694824219,
      "learning_rate": 1.7665310274669382e-05,
      "loss": 1.6747,
      "step": 63570
    },
    {
      "epoch": 32.33977619532045,
      "grad_norm": 37.697086334228516,
      "learning_rate": 1.7660223804679555e-05,
      "loss": 1.7041,
      "step": 63580
    },
    {
      "epoch": 32.34486266531027,
      "grad_norm": 37.274654388427734,
      "learning_rate": 1.7655137334689725e-05,
      "loss": 1.6116,
      "step": 63590
    },
    {
      "epoch": 32.3499491353001,
      "grad_norm": 42.95335388183594,
      "learning_rate": 1.76500508646999e-05,
      "loss": 1.7821,
      "step": 63600
    },
    {
      "epoch": 32.35503560528993,
      "grad_norm": 36.55573272705078,
      "learning_rate": 1.7644964394710072e-05,
      "loss": 1.6621,
      "step": 63610
    },
    {
      "epoch": 32.360122075279754,
      "grad_norm": 54.289852142333984,
      "learning_rate": 1.7639877924720245e-05,
      "loss": 1.6579,
      "step": 63620
    },
    {
      "epoch": 32.36520854526958,
      "grad_norm": 42.948875427246094,
      "learning_rate": 1.763479145473042e-05,
      "loss": 1.7363,
      "step": 63630
    },
    {
      "epoch": 32.37029501525941,
      "grad_norm": 38.89794158935547,
      "learning_rate": 1.762970498474059e-05,
      "loss": 1.7379,
      "step": 63640
    },
    {
      "epoch": 32.375381485249235,
      "grad_norm": 37.243247985839844,
      "learning_rate": 1.762461851475076e-05,
      "loss": 1.7966,
      "step": 63650
    },
    {
      "epoch": 32.38046795523906,
      "grad_norm": 38.67348861694336,
      "learning_rate": 1.7619532044760935e-05,
      "loss": 1.6815,
      "step": 63660
    },
    {
      "epoch": 32.38555442522889,
      "grad_norm": 40.806644439697266,
      "learning_rate": 1.761444557477111e-05,
      "loss": 1.6932,
      "step": 63670
    },
    {
      "epoch": 32.390640895218716,
      "grad_norm": 42.13434982299805,
      "learning_rate": 1.7609359104781285e-05,
      "loss": 1.6831,
      "step": 63680
    },
    {
      "epoch": 32.39572736520854,
      "grad_norm": 40.64111328125,
      "learning_rate": 1.7604272634791455e-05,
      "loss": 1.747,
      "step": 63690
    },
    {
      "epoch": 32.40081383519837,
      "grad_norm": 34.8466911315918,
      "learning_rate": 1.7599186164801628e-05,
      "loss": 1.7444,
      "step": 63700
    },
    {
      "epoch": 32.4059003051882,
      "grad_norm": 46.04301452636719,
      "learning_rate": 1.75940996948118e-05,
      "loss": 1.6464,
      "step": 63710
    },
    {
      "epoch": 32.410986775178024,
      "grad_norm": 39.49855422973633,
      "learning_rate": 1.7589013224821975e-05,
      "loss": 1.6971,
      "step": 63720
    },
    {
      "epoch": 32.41607324516785,
      "grad_norm": 41.28902816772461,
      "learning_rate": 1.7583926754832148e-05,
      "loss": 1.7416,
      "step": 63730
    },
    {
      "epoch": 32.42115971515768,
      "grad_norm": 36.76936340332031,
      "learning_rate": 1.757884028484232e-05,
      "loss": 1.665,
      "step": 63740
    },
    {
      "epoch": 32.426246185147505,
      "grad_norm": 40.48114776611328,
      "learning_rate": 1.757375381485249e-05,
      "loss": 1.6456,
      "step": 63750
    },
    {
      "epoch": 32.43133265513733,
      "grad_norm": 39.82612609863281,
      "learning_rate": 1.7568667344862668e-05,
      "loss": 1.6304,
      "step": 63760
    },
    {
      "epoch": 32.43641912512716,
      "grad_norm": 33.22895431518555,
      "learning_rate": 1.756358087487284e-05,
      "loss": 1.689,
      "step": 63770
    },
    {
      "epoch": 32.441505595116986,
      "grad_norm": 33.84040451049805,
      "learning_rate": 1.755849440488301e-05,
      "loss": 1.7487,
      "step": 63780
    },
    {
      "epoch": 32.44659206510681,
      "grad_norm": 37.69711685180664,
      "learning_rate": 1.7553407934893184e-05,
      "loss": 1.6908,
      "step": 63790
    },
    {
      "epoch": 32.45167853509664,
      "grad_norm": 39.3973388671875,
      "learning_rate": 1.7548321464903357e-05,
      "loss": 1.7701,
      "step": 63800
    },
    {
      "epoch": 32.45676500508647,
      "grad_norm": 48.40687561035156,
      "learning_rate": 1.754323499491353e-05,
      "loss": 1.7374,
      "step": 63810
    },
    {
      "epoch": 32.461851475076294,
      "grad_norm": 62.84296798706055,
      "learning_rate": 1.7538148524923704e-05,
      "loss": 1.7812,
      "step": 63820
    },
    {
      "epoch": 32.46693794506612,
      "grad_norm": 52.946163177490234,
      "learning_rate": 1.7533062054933877e-05,
      "loss": 1.8002,
      "step": 63830
    },
    {
      "epoch": 32.47202441505595,
      "grad_norm": 46.73257064819336,
      "learning_rate": 1.752797558494405e-05,
      "loss": 1.773,
      "step": 63840
    },
    {
      "epoch": 32.477110885045775,
      "grad_norm": 43.347774505615234,
      "learning_rate": 1.752288911495422e-05,
      "loss": 1.7925,
      "step": 63850
    },
    {
      "epoch": 32.4821973550356,
      "grad_norm": 41.014102935791016,
      "learning_rate": 1.7517802644964397e-05,
      "loss": 1.701,
      "step": 63860
    },
    {
      "epoch": 32.48728382502543,
      "grad_norm": 38.030662536621094,
      "learning_rate": 1.751271617497457e-05,
      "loss": 1.7629,
      "step": 63870
    },
    {
      "epoch": 32.492370295015256,
      "grad_norm": 38.2750358581543,
      "learning_rate": 1.750762970498474e-05,
      "loss": 1.6301,
      "step": 63880
    },
    {
      "epoch": 32.49745676500508,
      "grad_norm": 39.985015869140625,
      "learning_rate": 1.7502543234994914e-05,
      "loss": 1.6332,
      "step": 63890
    },
    {
      "epoch": 32.50254323499492,
      "grad_norm": 34.52891159057617,
      "learning_rate": 1.7497456765005087e-05,
      "loss": 1.7576,
      "step": 63900
    },
    {
      "epoch": 32.507629704984744,
      "grad_norm": 33.602970123291016,
      "learning_rate": 1.749237029501526e-05,
      "loss": 1.7123,
      "step": 63910
    },
    {
      "epoch": 32.51271617497457,
      "grad_norm": 36.812232971191406,
      "learning_rate": 1.7487283825025433e-05,
      "loss": 1.6739,
      "step": 63920
    },
    {
      "epoch": 32.5178026449644,
      "grad_norm": 58.521053314208984,
      "learning_rate": 1.7482197355035607e-05,
      "loss": 1.7374,
      "step": 63930
    },
    {
      "epoch": 32.522889114954225,
      "grad_norm": 38.84894561767578,
      "learning_rate": 1.747711088504578e-05,
      "loss": 1.6736,
      "step": 63940
    },
    {
      "epoch": 32.52797558494405,
      "grad_norm": 33.3834228515625,
      "learning_rate": 1.747202441505595e-05,
      "loss": 1.7755,
      "step": 63950
    },
    {
      "epoch": 32.53306205493388,
      "grad_norm": 42.34242248535156,
      "learning_rate": 1.7466937945066126e-05,
      "loss": 1.7574,
      "step": 63960
    },
    {
      "epoch": 32.538148524923706,
      "grad_norm": 45.2096061706543,
      "learning_rate": 1.74618514750763e-05,
      "loss": 1.6941,
      "step": 63970
    },
    {
      "epoch": 32.54323499491353,
      "grad_norm": 38.4442024230957,
      "learning_rate": 1.745676500508647e-05,
      "loss": 1.6985,
      "step": 63980
    },
    {
      "epoch": 32.54832146490336,
      "grad_norm": 36.31980514526367,
      "learning_rate": 1.7451678535096643e-05,
      "loss": 1.6955,
      "step": 63990
    },
    {
      "epoch": 32.55340793489319,
      "grad_norm": 35.1077880859375,
      "learning_rate": 1.7446592065106816e-05,
      "loss": 1.6841,
      "step": 64000
    },
    {
      "epoch": 32.558494404883014,
      "grad_norm": 40.65294647216797,
      "learning_rate": 1.744150559511699e-05,
      "loss": 1.6743,
      "step": 64010
    },
    {
      "epoch": 32.56358087487284,
      "grad_norm": 52.80726623535156,
      "learning_rate": 1.7436419125127163e-05,
      "loss": 1.7239,
      "step": 64020
    },
    {
      "epoch": 32.56866734486267,
      "grad_norm": 47.38276672363281,
      "learning_rate": 1.7431332655137336e-05,
      "loss": 1.661,
      "step": 64030
    },
    {
      "epoch": 32.573753814852495,
      "grad_norm": 31.485239028930664,
      "learning_rate": 1.7426246185147506e-05,
      "loss": 1.6922,
      "step": 64040
    },
    {
      "epoch": 32.57884028484232,
      "grad_norm": 32.501277923583984,
      "learning_rate": 1.7421159715157683e-05,
      "loss": 1.662,
      "step": 64050
    },
    {
      "epoch": 32.58392675483215,
      "grad_norm": 59.74461364746094,
      "learning_rate": 1.7416073245167856e-05,
      "loss": 1.6203,
      "step": 64060
    },
    {
      "epoch": 32.589013224821976,
      "grad_norm": 40.13054656982422,
      "learning_rate": 1.7410986775178026e-05,
      "loss": 1.6799,
      "step": 64070
    },
    {
      "epoch": 32.5940996948118,
      "grad_norm": 40.177181243896484,
      "learning_rate": 1.74059003051882e-05,
      "loss": 1.7419,
      "step": 64080
    },
    {
      "epoch": 32.59918616480163,
      "grad_norm": 36.755210876464844,
      "learning_rate": 1.7400813835198372e-05,
      "loss": 1.7331,
      "step": 64090
    },
    {
      "epoch": 32.60427263479146,
      "grad_norm": 33.158748626708984,
      "learning_rate": 1.7395727365208546e-05,
      "loss": 1.7309,
      "step": 64100
    },
    {
      "epoch": 32.609359104781284,
      "grad_norm": 35.64463806152344,
      "learning_rate": 1.739064089521872e-05,
      "loss": 1.6992,
      "step": 64110
    },
    {
      "epoch": 32.61444557477111,
      "grad_norm": 32.7493782043457,
      "learning_rate": 1.7385554425228892e-05,
      "loss": 1.7275,
      "step": 64120
    },
    {
      "epoch": 32.61953204476094,
      "grad_norm": 31.270954132080078,
      "learning_rate": 1.7380467955239066e-05,
      "loss": 1.8627,
      "step": 64130
    },
    {
      "epoch": 32.624618514750765,
      "grad_norm": 45.09587097167969,
      "learning_rate": 1.7375381485249235e-05,
      "loss": 1.6808,
      "step": 64140
    },
    {
      "epoch": 32.62970498474059,
      "grad_norm": 38.086570739746094,
      "learning_rate": 1.7370295015259412e-05,
      "loss": 1.7352,
      "step": 64150
    },
    {
      "epoch": 32.63479145473042,
      "grad_norm": 33.681480407714844,
      "learning_rate": 1.7365208545269585e-05,
      "loss": 1.6733,
      "step": 64160
    },
    {
      "epoch": 32.639877924720246,
      "grad_norm": 43.833614349365234,
      "learning_rate": 1.7360122075279755e-05,
      "loss": 1.6987,
      "step": 64170
    },
    {
      "epoch": 32.64496439471007,
      "grad_norm": 36.679141998291016,
      "learning_rate": 1.735503560528993e-05,
      "loss": 1.7153,
      "step": 64180
    },
    {
      "epoch": 32.6500508646999,
      "grad_norm": 32.71443557739258,
      "learning_rate": 1.7349949135300102e-05,
      "loss": 1.7318,
      "step": 64190
    },
    {
      "epoch": 32.65513733468973,
      "grad_norm": 46.26139450073242,
      "learning_rate": 1.7344862665310275e-05,
      "loss": 1.735,
      "step": 64200
    },
    {
      "epoch": 32.66022380467955,
      "grad_norm": 37.39378356933594,
      "learning_rate": 1.733977619532045e-05,
      "loss": 1.7529,
      "step": 64210
    },
    {
      "epoch": 32.66531027466938,
      "grad_norm": 55.044368743896484,
      "learning_rate": 1.733468972533062e-05,
      "loss": 1.6835,
      "step": 64220
    },
    {
      "epoch": 32.67039674465921,
      "grad_norm": 42.21596145629883,
      "learning_rate": 1.7329603255340795e-05,
      "loss": 1.7458,
      "step": 64230
    },
    {
      "epoch": 32.675483214649034,
      "grad_norm": 45.157806396484375,
      "learning_rate": 1.7324516785350968e-05,
      "loss": 1.678,
      "step": 64240
    },
    {
      "epoch": 32.68056968463886,
      "grad_norm": 45.86687088012695,
      "learning_rate": 1.731943031536114e-05,
      "loss": 1.7557,
      "step": 64250
    },
    {
      "epoch": 32.68565615462869,
      "grad_norm": 41.76527786254883,
      "learning_rate": 1.7314343845371315e-05,
      "loss": 1.6681,
      "step": 64260
    },
    {
      "epoch": 32.690742624618515,
      "grad_norm": 37.92290496826172,
      "learning_rate": 1.7309257375381485e-05,
      "loss": 1.7723,
      "step": 64270
    },
    {
      "epoch": 32.69582909460834,
      "grad_norm": 34.49824523925781,
      "learning_rate": 1.7304170905391658e-05,
      "loss": 1.7217,
      "step": 64280
    },
    {
      "epoch": 32.70091556459817,
      "grad_norm": 42.48896408081055,
      "learning_rate": 1.729908443540183e-05,
      "loss": 1.6756,
      "step": 64290
    },
    {
      "epoch": 32.706002034587996,
      "grad_norm": 35.19803237915039,
      "learning_rate": 1.7293997965412005e-05,
      "loss": 1.6927,
      "step": 64300
    },
    {
      "epoch": 32.71108850457782,
      "grad_norm": 39.855445861816406,
      "learning_rate": 1.7288911495422178e-05,
      "loss": 1.7755,
      "step": 64310
    },
    {
      "epoch": 32.71617497456765,
      "grad_norm": 42.28372573852539,
      "learning_rate": 1.728382502543235e-05,
      "loss": 1.6704,
      "step": 64320
    },
    {
      "epoch": 32.72126144455748,
      "grad_norm": 34.38020324707031,
      "learning_rate": 1.727873855544252e-05,
      "loss": 1.694,
      "step": 64330
    },
    {
      "epoch": 32.726347914547304,
      "grad_norm": 31.438417434692383,
      "learning_rate": 1.7273652085452698e-05,
      "loss": 1.6934,
      "step": 64340
    },
    {
      "epoch": 32.73143438453713,
      "grad_norm": 40.54572677612305,
      "learning_rate": 1.726856561546287e-05,
      "loss": 1.6864,
      "step": 64350
    },
    {
      "epoch": 32.73652085452696,
      "grad_norm": 36.75595474243164,
      "learning_rate": 1.7263479145473044e-05,
      "loss": 1.6765,
      "step": 64360
    },
    {
      "epoch": 32.741607324516785,
      "grad_norm": 48.97713088989258,
      "learning_rate": 1.7258392675483214e-05,
      "loss": 1.7973,
      "step": 64370
    },
    {
      "epoch": 32.74669379450661,
      "grad_norm": 41.39543151855469,
      "learning_rate": 1.7253306205493387e-05,
      "loss": 1.7136,
      "step": 64380
    },
    {
      "epoch": 32.75178026449644,
      "grad_norm": 37.93674087524414,
      "learning_rate": 1.7248219735503564e-05,
      "loss": 1.6525,
      "step": 64390
    },
    {
      "epoch": 32.756866734486266,
      "grad_norm": 36.15715789794922,
      "learning_rate": 1.7243133265513734e-05,
      "loss": 1.6135,
      "step": 64400
    },
    {
      "epoch": 32.76195320447609,
      "grad_norm": 45.12229537963867,
      "learning_rate": 1.7238046795523907e-05,
      "loss": 1.6802,
      "step": 64410
    },
    {
      "epoch": 32.76703967446592,
      "grad_norm": 36.39902114868164,
      "learning_rate": 1.723296032553408e-05,
      "loss": 1.6929,
      "step": 64420
    },
    {
      "epoch": 32.77212614445575,
      "grad_norm": 42.8680305480957,
      "learning_rate": 1.722787385554425e-05,
      "loss": 1.7763,
      "step": 64430
    },
    {
      "epoch": 32.777212614445574,
      "grad_norm": 38.37797546386719,
      "learning_rate": 1.7222787385554427e-05,
      "loss": 1.7615,
      "step": 64440
    },
    {
      "epoch": 32.7822990844354,
      "grad_norm": 40.06218719482422,
      "learning_rate": 1.72177009155646e-05,
      "loss": 1.7537,
      "step": 64450
    },
    {
      "epoch": 32.78738555442523,
      "grad_norm": 50.8485221862793,
      "learning_rate": 1.721261444557477e-05,
      "loss": 1.7126,
      "step": 64460
    },
    {
      "epoch": 32.792472024415055,
      "grad_norm": 38.908470153808594,
      "learning_rate": 1.7207527975584944e-05,
      "loss": 1.6547,
      "step": 64470
    },
    {
      "epoch": 32.79755849440488,
      "grad_norm": 39.25341033935547,
      "learning_rate": 1.7202441505595117e-05,
      "loss": 1.7373,
      "step": 64480
    },
    {
      "epoch": 32.80264496439471,
      "grad_norm": 39.91978454589844,
      "learning_rate": 1.7197355035605293e-05,
      "loss": 1.8415,
      "step": 64490
    },
    {
      "epoch": 32.807731434384536,
      "grad_norm": 39.811702728271484,
      "learning_rate": 1.7192268565615463e-05,
      "loss": 1.75,
      "step": 64500
    },
    {
      "epoch": 32.81281790437436,
      "grad_norm": 39.31108856201172,
      "learning_rate": 1.7187182095625637e-05,
      "loss": 1.6578,
      "step": 64510
    },
    {
      "epoch": 32.81790437436419,
      "grad_norm": 31.668010711669922,
      "learning_rate": 1.718209562563581e-05,
      "loss": 1.6161,
      "step": 64520
    },
    {
      "epoch": 32.82299084435402,
      "grad_norm": 42.62882995605469,
      "learning_rate": 1.7177009155645983e-05,
      "loss": 1.7368,
      "step": 64530
    },
    {
      "epoch": 32.828077314343844,
      "grad_norm": 44.81184768676758,
      "learning_rate": 1.7171922685656156e-05,
      "loss": 1.6716,
      "step": 64540
    },
    {
      "epoch": 32.83316378433367,
      "grad_norm": 46.585941314697266,
      "learning_rate": 1.716683621566633e-05,
      "loss": 1.6405,
      "step": 64550
    },
    {
      "epoch": 32.8382502543235,
      "grad_norm": 43.2144660949707,
      "learning_rate": 1.71617497456765e-05,
      "loss": 1.6558,
      "step": 64560
    },
    {
      "epoch": 32.843336724313325,
      "grad_norm": 39.23708724975586,
      "learning_rate": 1.7156663275686673e-05,
      "loss": 1.7085,
      "step": 64570
    },
    {
      "epoch": 32.84842319430315,
      "grad_norm": 38.315208435058594,
      "learning_rate": 1.7151576805696846e-05,
      "loss": 1.6717,
      "step": 64580
    },
    {
      "epoch": 32.85350966429298,
      "grad_norm": 43.9091682434082,
      "learning_rate": 1.714649033570702e-05,
      "loss": 1.7438,
      "step": 64590
    },
    {
      "epoch": 32.858596134282806,
      "grad_norm": 39.626792907714844,
      "learning_rate": 1.7141403865717193e-05,
      "loss": 1.7079,
      "step": 64600
    },
    {
      "epoch": 32.86368260427263,
      "grad_norm": 42.123714447021484,
      "learning_rate": 1.7136317395727366e-05,
      "loss": 1.7279,
      "step": 64610
    },
    {
      "epoch": 32.86876907426246,
      "grad_norm": 36.49077606201172,
      "learning_rate": 1.713123092573754e-05,
      "loss": 1.7518,
      "step": 64620
    },
    {
      "epoch": 32.87385554425229,
      "grad_norm": 45.366477966308594,
      "learning_rate": 1.7126144455747713e-05,
      "loss": 1.7341,
      "step": 64630
    },
    {
      "epoch": 32.878942014242114,
      "grad_norm": 48.34497833251953,
      "learning_rate": 1.7121057985757886e-05,
      "loss": 1.6521,
      "step": 64640
    },
    {
      "epoch": 32.88402848423194,
      "grad_norm": 35.5858268737793,
      "learning_rate": 1.711597151576806e-05,
      "loss": 1.7538,
      "step": 64650
    },
    {
      "epoch": 32.88911495422177,
      "grad_norm": 45.833126068115234,
      "learning_rate": 1.711088504577823e-05,
      "loss": 1.7188,
      "step": 64660
    },
    {
      "epoch": 32.894201424211595,
      "grad_norm": 42.49862289428711,
      "learning_rate": 1.7105798575788402e-05,
      "loss": 1.6338,
      "step": 64670
    },
    {
      "epoch": 32.89928789420142,
      "grad_norm": 50.18728256225586,
      "learning_rate": 1.710071210579858e-05,
      "loss": 1.7162,
      "step": 64680
    },
    {
      "epoch": 32.90437436419125,
      "grad_norm": 32.61831283569336,
      "learning_rate": 1.709562563580875e-05,
      "loss": 1.7556,
      "step": 64690
    },
    {
      "epoch": 32.909460834181075,
      "grad_norm": 39.890106201171875,
      "learning_rate": 1.7090539165818922e-05,
      "loss": 1.6142,
      "step": 64700
    },
    {
      "epoch": 32.9145473041709,
      "grad_norm": 50.735904693603516,
      "learning_rate": 1.7085452695829096e-05,
      "loss": 1.6429,
      "step": 64710
    },
    {
      "epoch": 32.91963377416073,
      "grad_norm": 35.21809005737305,
      "learning_rate": 1.708036622583927e-05,
      "loss": 1.6997,
      "step": 64720
    },
    {
      "epoch": 32.924720244150556,
      "grad_norm": 40.60405731201172,
      "learning_rate": 1.7075279755849442e-05,
      "loss": 1.8034,
      "step": 64730
    },
    {
      "epoch": 32.92980671414038,
      "grad_norm": 41.48568344116211,
      "learning_rate": 1.7070193285859615e-05,
      "loss": 1.6645,
      "step": 64740
    },
    {
      "epoch": 32.93489318413021,
      "grad_norm": 39.409053802490234,
      "learning_rate": 1.706510681586979e-05,
      "loss": 1.6245,
      "step": 64750
    },
    {
      "epoch": 32.93997965412004,
      "grad_norm": 44.556453704833984,
      "learning_rate": 1.706002034587996e-05,
      "loss": 1.5823,
      "step": 64760
    },
    {
      "epoch": 32.945066124109864,
      "grad_norm": 35.45744705200195,
      "learning_rate": 1.7054933875890132e-05,
      "loss": 1.7327,
      "step": 64770
    },
    {
      "epoch": 32.95015259409969,
      "grad_norm": 36.333885192871094,
      "learning_rate": 1.704984740590031e-05,
      "loss": 1.6129,
      "step": 64780
    },
    {
      "epoch": 32.955239064089525,
      "grad_norm": 35.08552932739258,
      "learning_rate": 1.704476093591048e-05,
      "loss": 1.7515,
      "step": 64790
    },
    {
      "epoch": 32.96032553407935,
      "grad_norm": 38.662254333496094,
      "learning_rate": 1.703967446592065e-05,
      "loss": 1.6819,
      "step": 64800
    },
    {
      "epoch": 32.96541200406918,
      "grad_norm": 35.29121017456055,
      "learning_rate": 1.7034587995930825e-05,
      "loss": 1.7062,
      "step": 64810
    },
    {
      "epoch": 32.970498474059006,
      "grad_norm": 42.45071792602539,
      "learning_rate": 1.7029501525940998e-05,
      "loss": 1.7032,
      "step": 64820
    },
    {
      "epoch": 32.97558494404883,
      "grad_norm": 64.83971405029297,
      "learning_rate": 1.702441505595117e-05,
      "loss": 1.7302,
      "step": 64830
    },
    {
      "epoch": 32.98067141403866,
      "grad_norm": 39.24354553222656,
      "learning_rate": 1.7019328585961345e-05,
      "loss": 1.6843,
      "step": 64840
    },
    {
      "epoch": 32.98575788402849,
      "grad_norm": 36.202537536621094,
      "learning_rate": 1.7014242115971515e-05,
      "loss": 1.6684,
      "step": 64850
    },
    {
      "epoch": 32.990844354018314,
      "grad_norm": 36.96290969848633,
      "learning_rate": 1.7009155645981688e-05,
      "loss": 1.6973,
      "step": 64860
    },
    {
      "epoch": 32.99593082400814,
      "grad_norm": 42.138580322265625,
      "learning_rate": 1.7004069175991865e-05,
      "loss": 1.6315,
      "step": 64870
    },
    {
      "epoch": 33.0,
      "eval_loss": 4.878094673156738,
      "eval_runtime": 2.6767,
      "eval_samples_per_second": 1036.722,
      "eval_steps_per_second": 129.637,
      "step": 64878
    },
    {
      "epoch": 33.00101729399797,
      "grad_norm": 50.567047119140625,
      "learning_rate": 1.6998982706002035e-05,
      "loss": 1.7283,
      "step": 64880
    },
    {
      "epoch": 33.006103763987795,
      "grad_norm": 45.601539611816406,
      "learning_rate": 1.6993896236012208e-05,
      "loss": 1.6921,
      "step": 64890
    },
    {
      "epoch": 33.01119023397762,
      "grad_norm": 45.51931381225586,
      "learning_rate": 1.698880976602238e-05,
      "loss": 1.7242,
      "step": 64900
    },
    {
      "epoch": 33.01627670396745,
      "grad_norm": 42.64472198486328,
      "learning_rate": 1.6983723296032554e-05,
      "loss": 1.7755,
      "step": 64910
    },
    {
      "epoch": 33.021363173957276,
      "grad_norm": 38.771766662597656,
      "learning_rate": 1.6978636826042728e-05,
      "loss": 1.7109,
      "step": 64920
    },
    {
      "epoch": 33.0264496439471,
      "grad_norm": 47.385467529296875,
      "learning_rate": 1.69735503560529e-05,
      "loss": 1.6583,
      "step": 64930
    },
    {
      "epoch": 33.03153611393693,
      "grad_norm": 44.56050491333008,
      "learning_rate": 1.6968463886063074e-05,
      "loss": 1.5962,
      "step": 64940
    },
    {
      "epoch": 33.03662258392676,
      "grad_norm": 50.09805679321289,
      "learning_rate": 1.6963377416073244e-05,
      "loss": 1.6238,
      "step": 64950
    },
    {
      "epoch": 33.041709053916584,
      "grad_norm": 37.807228088378906,
      "learning_rate": 1.6958290946083417e-05,
      "loss": 1.7338,
      "step": 64960
    },
    {
      "epoch": 33.04679552390641,
      "grad_norm": 36.95539855957031,
      "learning_rate": 1.6953204476093594e-05,
      "loss": 1.6135,
      "step": 64970
    },
    {
      "epoch": 33.05188199389624,
      "grad_norm": 41.2508659362793,
      "learning_rate": 1.6948118006103764e-05,
      "loss": 1.6553,
      "step": 64980
    },
    {
      "epoch": 33.056968463886065,
      "grad_norm": 34.7153434753418,
      "learning_rate": 1.6943031536113937e-05,
      "loss": 1.6313,
      "step": 64990
    },
    {
      "epoch": 33.06205493387589,
      "grad_norm": 45.90495681762695,
      "learning_rate": 1.693794506612411e-05,
      "loss": 1.7431,
      "step": 65000
    },
    {
      "epoch": 33.06714140386572,
      "grad_norm": 49.22196578979492,
      "learning_rate": 1.6932858596134284e-05,
      "loss": 1.6186,
      "step": 65010
    },
    {
      "epoch": 33.072227873855546,
      "grad_norm": 37.30409240722656,
      "learning_rate": 1.6927772126144457e-05,
      "loss": 1.6208,
      "step": 65020
    },
    {
      "epoch": 33.07731434384537,
      "grad_norm": 36.49599838256836,
      "learning_rate": 1.692268565615463e-05,
      "loss": 1.6953,
      "step": 65030
    },
    {
      "epoch": 33.0824008138352,
      "grad_norm": 39.40779495239258,
      "learning_rate": 1.6917599186164804e-05,
      "loss": 1.7318,
      "step": 65040
    },
    {
      "epoch": 33.08748728382503,
      "grad_norm": 37.294395446777344,
      "learning_rate": 1.6912512716174974e-05,
      "loss": 1.6237,
      "step": 65050
    },
    {
      "epoch": 33.092573753814854,
      "grad_norm": 37.72502136230469,
      "learning_rate": 1.6907426246185147e-05,
      "loss": 1.7331,
      "step": 65060
    },
    {
      "epoch": 33.09766022380468,
      "grad_norm": 41.58969497680664,
      "learning_rate": 1.6902339776195323e-05,
      "loss": 1.6622,
      "step": 65070
    },
    {
      "epoch": 33.10274669379451,
      "grad_norm": 35.443214416503906,
      "learning_rate": 1.6897253306205493e-05,
      "loss": 1.7477,
      "step": 65080
    },
    {
      "epoch": 33.107833163784335,
      "grad_norm": 42.60002136230469,
      "learning_rate": 1.6892166836215667e-05,
      "loss": 1.6936,
      "step": 65090
    },
    {
      "epoch": 33.11291963377416,
      "grad_norm": 43.987728118896484,
      "learning_rate": 1.688708036622584e-05,
      "loss": 1.7702,
      "step": 65100
    },
    {
      "epoch": 33.11800610376399,
      "grad_norm": 34.99937438964844,
      "learning_rate": 1.6881993896236013e-05,
      "loss": 1.7593,
      "step": 65110
    },
    {
      "epoch": 33.123092573753816,
      "grad_norm": 45.88380813598633,
      "learning_rate": 1.6876907426246187e-05,
      "loss": 1.6817,
      "step": 65120
    },
    {
      "epoch": 33.12817904374364,
      "grad_norm": 42.596282958984375,
      "learning_rate": 1.687182095625636e-05,
      "loss": 1.6886,
      "step": 65130
    },
    {
      "epoch": 33.13326551373347,
      "grad_norm": 46.27248001098633,
      "learning_rate": 1.686673448626653e-05,
      "loss": 1.7227,
      "step": 65140
    },
    {
      "epoch": 33.1383519837233,
      "grad_norm": 36.30305862426758,
      "learning_rate": 1.6861648016276703e-05,
      "loss": 1.6838,
      "step": 65150
    },
    {
      "epoch": 33.143438453713124,
      "grad_norm": 40.6727180480957,
      "learning_rate": 1.685656154628688e-05,
      "loss": 1.712,
      "step": 65160
    },
    {
      "epoch": 33.14852492370295,
      "grad_norm": 35.10694122314453,
      "learning_rate": 1.6851475076297053e-05,
      "loss": 1.6667,
      "step": 65170
    },
    {
      "epoch": 33.15361139369278,
      "grad_norm": 40.249752044677734,
      "learning_rate": 1.6846388606307223e-05,
      "loss": 1.6597,
      "step": 65180
    },
    {
      "epoch": 33.158697863682605,
      "grad_norm": 40.74489212036133,
      "learning_rate": 1.6841302136317396e-05,
      "loss": 1.7162,
      "step": 65190
    },
    {
      "epoch": 33.16378433367243,
      "grad_norm": 46.88909149169922,
      "learning_rate": 1.683621566632757e-05,
      "loss": 1.7401,
      "step": 65200
    },
    {
      "epoch": 33.16887080366226,
      "grad_norm": 34.93980026245117,
      "learning_rate": 1.6831129196337743e-05,
      "loss": 1.7017,
      "step": 65210
    },
    {
      "epoch": 33.173957273652086,
      "grad_norm": 42.157806396484375,
      "learning_rate": 1.6826042726347916e-05,
      "loss": 1.6716,
      "step": 65220
    },
    {
      "epoch": 33.17904374364191,
      "grad_norm": 43.91655731201172,
      "learning_rate": 1.682095625635809e-05,
      "loss": 1.6441,
      "step": 65230
    },
    {
      "epoch": 33.18413021363174,
      "grad_norm": 38.78603744506836,
      "learning_rate": 1.681586978636826e-05,
      "loss": 1.7398,
      "step": 65240
    },
    {
      "epoch": 33.18921668362157,
      "grad_norm": 41.387184143066406,
      "learning_rate": 1.6810783316378432e-05,
      "loss": 1.7577,
      "step": 65250
    },
    {
      "epoch": 33.19430315361139,
      "grad_norm": 31.307939529418945,
      "learning_rate": 1.680569684638861e-05,
      "loss": 1.7101,
      "step": 65260
    },
    {
      "epoch": 33.19938962360122,
      "grad_norm": 39.4198112487793,
      "learning_rate": 1.680061037639878e-05,
      "loss": 1.6044,
      "step": 65270
    },
    {
      "epoch": 33.20447609359105,
      "grad_norm": 40.025508880615234,
      "learning_rate": 1.6795523906408952e-05,
      "loss": 1.6555,
      "step": 65280
    },
    {
      "epoch": 33.209562563580874,
      "grad_norm": 49.013431549072266,
      "learning_rate": 1.6790437436419126e-05,
      "loss": 1.7934,
      "step": 65290
    },
    {
      "epoch": 33.2146490335707,
      "grad_norm": 42.1064567565918,
      "learning_rate": 1.67853509664293e-05,
      "loss": 1.6899,
      "step": 65300
    },
    {
      "epoch": 33.21973550356053,
      "grad_norm": 33.21101379394531,
      "learning_rate": 1.6780264496439472e-05,
      "loss": 1.7128,
      "step": 65310
    },
    {
      "epoch": 33.224821973550355,
      "grad_norm": 43.9299201965332,
      "learning_rate": 1.6775178026449645e-05,
      "loss": 1.6687,
      "step": 65320
    },
    {
      "epoch": 33.22990844354018,
      "grad_norm": 44.66398620605469,
      "learning_rate": 1.677009155645982e-05,
      "loss": 1.6675,
      "step": 65330
    },
    {
      "epoch": 33.23499491353001,
      "grad_norm": 32.3648567199707,
      "learning_rate": 1.676500508646999e-05,
      "loss": 1.6686,
      "step": 65340
    },
    {
      "epoch": 33.240081383519836,
      "grad_norm": 43.598960876464844,
      "learning_rate": 1.6759918616480165e-05,
      "loss": 1.6845,
      "step": 65350
    },
    {
      "epoch": 33.24516785350966,
      "grad_norm": 52.66291809082031,
      "learning_rate": 1.675483214649034e-05,
      "loss": 1.6908,
      "step": 65360
    },
    {
      "epoch": 33.25025432349949,
      "grad_norm": 43.477622985839844,
      "learning_rate": 1.674974567650051e-05,
      "loss": 1.6666,
      "step": 65370
    },
    {
      "epoch": 33.25534079348932,
      "grad_norm": 33.80397033691406,
      "learning_rate": 1.674465920651068e-05,
      "loss": 1.701,
      "step": 65380
    },
    {
      "epoch": 33.260427263479144,
      "grad_norm": 34.77355194091797,
      "learning_rate": 1.6739572736520855e-05,
      "loss": 1.6874,
      "step": 65390
    },
    {
      "epoch": 33.26551373346897,
      "grad_norm": 53.62713623046875,
      "learning_rate": 1.6734486266531028e-05,
      "loss": 1.6705,
      "step": 65400
    },
    {
      "epoch": 33.2706002034588,
      "grad_norm": 34.492645263671875,
      "learning_rate": 1.67293997965412e-05,
      "loss": 1.6619,
      "step": 65410
    },
    {
      "epoch": 33.275686673448625,
      "grad_norm": 31.5157470703125,
      "learning_rate": 1.6724313326551375e-05,
      "loss": 1.642,
      "step": 65420
    },
    {
      "epoch": 33.28077314343845,
      "grad_norm": 43.66042709350586,
      "learning_rate": 1.6719226856561548e-05,
      "loss": 1.6883,
      "step": 65430
    },
    {
      "epoch": 33.28585961342828,
      "grad_norm": 38.63436508178711,
      "learning_rate": 1.6714140386571718e-05,
      "loss": 1.7456,
      "step": 65440
    },
    {
      "epoch": 33.290946083418106,
      "grad_norm": 36.29484176635742,
      "learning_rate": 1.6709053916581895e-05,
      "loss": 1.6506,
      "step": 65450
    },
    {
      "epoch": 33.29603255340793,
      "grad_norm": 56.65532302856445,
      "learning_rate": 1.6703967446592068e-05,
      "loss": 1.7839,
      "step": 65460
    },
    {
      "epoch": 33.30111902339776,
      "grad_norm": 39.94308853149414,
      "learning_rate": 1.6698880976602238e-05,
      "loss": 1.6742,
      "step": 65470
    },
    {
      "epoch": 33.30620549338759,
      "grad_norm": 50.348575592041016,
      "learning_rate": 1.669379450661241e-05,
      "loss": 1.7557,
      "step": 65480
    },
    {
      "epoch": 33.311291963377414,
      "grad_norm": 39.874691009521484,
      "learning_rate": 1.6688708036622584e-05,
      "loss": 1.7389,
      "step": 65490
    },
    {
      "epoch": 33.31637843336724,
      "grad_norm": 36.464508056640625,
      "learning_rate": 1.6683621566632758e-05,
      "loss": 1.695,
      "step": 65500
    },
    {
      "epoch": 33.32146490335707,
      "grad_norm": 41.891883850097656,
      "learning_rate": 1.667853509664293e-05,
      "loss": 1.6727,
      "step": 65510
    },
    {
      "epoch": 33.326551373346895,
      "grad_norm": 47.03420639038086,
      "learning_rate": 1.6673448626653104e-05,
      "loss": 1.6904,
      "step": 65520
    },
    {
      "epoch": 33.33163784333672,
      "grad_norm": 38.830142974853516,
      "learning_rate": 1.6668362156663274e-05,
      "loss": 1.7217,
      "step": 65530
    },
    {
      "epoch": 33.33672431332655,
      "grad_norm": 36.56773376464844,
      "learning_rate": 1.6663275686673447e-05,
      "loss": 1.7237,
      "step": 65540
    },
    {
      "epoch": 33.341810783316376,
      "grad_norm": 39.11456298828125,
      "learning_rate": 1.6658189216683624e-05,
      "loss": 1.714,
      "step": 65550
    },
    {
      "epoch": 33.3468972533062,
      "grad_norm": 39.496978759765625,
      "learning_rate": 1.6653102746693797e-05,
      "loss": 1.6562,
      "step": 65560
    },
    {
      "epoch": 33.35198372329603,
      "grad_norm": 49.21991729736328,
      "learning_rate": 1.6648016276703967e-05,
      "loss": 1.6972,
      "step": 65570
    },
    {
      "epoch": 33.35707019328586,
      "grad_norm": 34.30925369262695,
      "learning_rate": 1.664292980671414e-05,
      "loss": 1.7068,
      "step": 65580
    },
    {
      "epoch": 33.362156663275684,
      "grad_norm": 45.66422653198242,
      "learning_rate": 1.6637843336724314e-05,
      "loss": 1.6362,
      "step": 65590
    },
    {
      "epoch": 33.36724313326551,
      "grad_norm": 46.47806167602539,
      "learning_rate": 1.6632756866734487e-05,
      "loss": 1.7524,
      "step": 65600
    },
    {
      "epoch": 33.37232960325534,
      "grad_norm": 37.24106979370117,
      "learning_rate": 1.662767039674466e-05,
      "loss": 1.6857,
      "step": 65610
    },
    {
      "epoch": 33.377416073245165,
      "grad_norm": 35.88166809082031,
      "learning_rate": 1.6622583926754834e-05,
      "loss": 1.7861,
      "step": 65620
    },
    {
      "epoch": 33.38250254323499,
      "grad_norm": 39.369384765625,
      "learning_rate": 1.6617497456765004e-05,
      "loss": 1.6779,
      "step": 65630
    },
    {
      "epoch": 33.38758901322482,
      "grad_norm": 37.7976188659668,
      "learning_rate": 1.661241098677518e-05,
      "loss": 1.6894,
      "step": 65640
    },
    {
      "epoch": 33.392675483214646,
      "grad_norm": 36.069095611572266,
      "learning_rate": 1.6607324516785353e-05,
      "loss": 1.7054,
      "step": 65650
    },
    {
      "epoch": 33.39776195320447,
      "grad_norm": 38.1800537109375,
      "learning_rate": 1.6602238046795523e-05,
      "loss": 1.7364,
      "step": 65660
    },
    {
      "epoch": 33.4028484231943,
      "grad_norm": 39.08390808105469,
      "learning_rate": 1.6597151576805697e-05,
      "loss": 1.6591,
      "step": 65670
    },
    {
      "epoch": 33.407934893184134,
      "grad_norm": 37.07852554321289,
      "learning_rate": 1.659206510681587e-05,
      "loss": 1.6703,
      "step": 65680
    },
    {
      "epoch": 33.41302136317396,
      "grad_norm": 38.484375,
      "learning_rate": 1.6586978636826043e-05,
      "loss": 1.6668,
      "step": 65690
    },
    {
      "epoch": 33.41810783316379,
      "grad_norm": 43.29462814331055,
      "learning_rate": 1.6581892166836217e-05,
      "loss": 1.6187,
      "step": 65700
    },
    {
      "epoch": 33.423194303153615,
      "grad_norm": 35.44173049926758,
      "learning_rate": 1.657680569684639e-05,
      "loss": 1.7506,
      "step": 65710
    },
    {
      "epoch": 33.42828077314344,
      "grad_norm": 38.00233840942383,
      "learning_rate": 1.6571719226856563e-05,
      "loss": 1.6562,
      "step": 65720
    },
    {
      "epoch": 33.43336724313327,
      "grad_norm": 31.537511825561523,
      "learning_rate": 1.6566632756866733e-05,
      "loss": 1.6448,
      "step": 65730
    },
    {
      "epoch": 33.438453713123096,
      "grad_norm": 44.13615036010742,
      "learning_rate": 1.656154628687691e-05,
      "loss": 1.5923,
      "step": 65740
    },
    {
      "epoch": 33.44354018311292,
      "grad_norm": 35.614871978759766,
      "learning_rate": 1.6556459816887083e-05,
      "loss": 1.6808,
      "step": 65750
    },
    {
      "epoch": 33.44862665310275,
      "grad_norm": 59.818294525146484,
      "learning_rate": 1.6551373346897253e-05,
      "loss": 1.5789,
      "step": 65760
    },
    {
      "epoch": 33.45371312309258,
      "grad_norm": 42.71897888183594,
      "learning_rate": 1.6546286876907426e-05,
      "loss": 1.6507,
      "step": 65770
    },
    {
      "epoch": 33.458799593082404,
      "grad_norm": 28.705190658569336,
      "learning_rate": 1.65412004069176e-05,
      "loss": 1.7154,
      "step": 65780
    },
    {
      "epoch": 33.46388606307223,
      "grad_norm": 35.635860443115234,
      "learning_rate": 1.6536113936927773e-05,
      "loss": 1.6281,
      "step": 65790
    },
    {
      "epoch": 33.46897253306206,
      "grad_norm": 47.1968879699707,
      "learning_rate": 1.6531027466937946e-05,
      "loss": 1.7007,
      "step": 65800
    },
    {
      "epoch": 33.474059003051885,
      "grad_norm": 42.04478454589844,
      "learning_rate": 1.652594099694812e-05,
      "loss": 1.6261,
      "step": 65810
    },
    {
      "epoch": 33.47914547304171,
      "grad_norm": 43.62080001831055,
      "learning_rate": 1.652085452695829e-05,
      "loss": 1.6822,
      "step": 65820
    },
    {
      "epoch": 33.48423194303154,
      "grad_norm": 36.74695587158203,
      "learning_rate": 1.6515768056968466e-05,
      "loss": 1.7224,
      "step": 65830
    },
    {
      "epoch": 33.489318413021365,
      "grad_norm": 41.50972366333008,
      "learning_rate": 1.651068158697864e-05,
      "loss": 1.7387,
      "step": 65840
    },
    {
      "epoch": 33.49440488301119,
      "grad_norm": 43.98431396484375,
      "learning_rate": 1.6505595116988812e-05,
      "loss": 1.7328,
      "step": 65850
    },
    {
      "epoch": 33.49949135300102,
      "grad_norm": 41.233280181884766,
      "learning_rate": 1.6500508646998982e-05,
      "loss": 1.626,
      "step": 65860
    },
    {
      "epoch": 33.504577822990846,
      "grad_norm": 42.351802825927734,
      "learning_rate": 1.6495422177009156e-05,
      "loss": 1.7386,
      "step": 65870
    },
    {
      "epoch": 33.50966429298067,
      "grad_norm": 39.5511474609375,
      "learning_rate": 1.649033570701933e-05,
      "loss": 1.7263,
      "step": 65880
    },
    {
      "epoch": 33.5147507629705,
      "grad_norm": 39.3154296875,
      "learning_rate": 1.6485249237029502e-05,
      "loss": 1.665,
      "step": 65890
    },
    {
      "epoch": 33.51983723296033,
      "grad_norm": 47.51279067993164,
      "learning_rate": 1.6480162767039675e-05,
      "loss": 1.6513,
      "step": 65900
    },
    {
      "epoch": 33.524923702950154,
      "grad_norm": 40.575469970703125,
      "learning_rate": 1.647507629704985e-05,
      "loss": 1.6774,
      "step": 65910
    },
    {
      "epoch": 33.53001017293998,
      "grad_norm": 39.042789459228516,
      "learning_rate": 1.646998982706002e-05,
      "loss": 1.7368,
      "step": 65920
    },
    {
      "epoch": 33.53509664292981,
      "grad_norm": 37.70762252807617,
      "learning_rate": 1.6464903357070195e-05,
      "loss": 1.7087,
      "step": 65930
    },
    {
      "epoch": 33.540183112919635,
      "grad_norm": 50.34986877441406,
      "learning_rate": 1.645981688708037e-05,
      "loss": 1.7247,
      "step": 65940
    },
    {
      "epoch": 33.54526958290946,
      "grad_norm": 39.55373764038086,
      "learning_rate": 1.645473041709054e-05,
      "loss": 1.6438,
      "step": 65950
    },
    {
      "epoch": 33.55035605289929,
      "grad_norm": 43.325592041015625,
      "learning_rate": 1.644964394710071e-05,
      "loss": 1.6527,
      "step": 65960
    },
    {
      "epoch": 33.555442522889116,
      "grad_norm": 44.76728057861328,
      "learning_rate": 1.6444557477110885e-05,
      "loss": 1.713,
      "step": 65970
    },
    {
      "epoch": 33.56052899287894,
      "grad_norm": 29.465469360351562,
      "learning_rate": 1.643947100712106e-05,
      "loss": 1.6338,
      "step": 65980
    },
    {
      "epoch": 33.56561546286877,
      "grad_norm": 36.93544387817383,
      "learning_rate": 1.643438453713123e-05,
      "loss": 1.7776,
      "step": 65990
    },
    {
      "epoch": 33.5707019328586,
      "grad_norm": 43.742584228515625,
      "learning_rate": 1.6429298067141405e-05,
      "loss": 1.7135,
      "step": 66000
    },
    {
      "epoch": 33.575788402848424,
      "grad_norm": 41.0666389465332,
      "learning_rate": 1.6424211597151578e-05,
      "loss": 1.7216,
      "step": 66010
    },
    {
      "epoch": 33.58087487283825,
      "grad_norm": 36.60173416137695,
      "learning_rate": 1.6419125127161748e-05,
      "loss": 1.6829,
      "step": 66020
    },
    {
      "epoch": 33.58596134282808,
      "grad_norm": 34.536598205566406,
      "learning_rate": 1.6414038657171925e-05,
      "loss": 1.6691,
      "step": 66030
    },
    {
      "epoch": 33.591047812817905,
      "grad_norm": 37.00479507446289,
      "learning_rate": 1.6408952187182098e-05,
      "loss": 1.6278,
      "step": 66040
    },
    {
      "epoch": 33.59613428280773,
      "grad_norm": 45.58945083618164,
      "learning_rate": 1.6403865717192268e-05,
      "loss": 1.6717,
      "step": 66050
    },
    {
      "epoch": 33.60122075279756,
      "grad_norm": 42.89045715332031,
      "learning_rate": 1.639877924720244e-05,
      "loss": 1.6625,
      "step": 66060
    },
    {
      "epoch": 33.606307222787386,
      "grad_norm": 38.06751251220703,
      "learning_rate": 1.6393692777212614e-05,
      "loss": 1.702,
      "step": 66070
    },
    {
      "epoch": 33.61139369277721,
      "grad_norm": 38.3309211730957,
      "learning_rate": 1.6388606307222788e-05,
      "loss": 1.6051,
      "step": 66080
    },
    {
      "epoch": 33.61648016276704,
      "grad_norm": 42.24481201171875,
      "learning_rate": 1.638351983723296e-05,
      "loss": 1.7547,
      "step": 66090
    },
    {
      "epoch": 33.62156663275687,
      "grad_norm": 38.67842483520508,
      "learning_rate": 1.6378433367243134e-05,
      "loss": 1.8062,
      "step": 66100
    },
    {
      "epoch": 33.626653102746694,
      "grad_norm": 41.02858352661133,
      "learning_rate": 1.6373346897253308e-05,
      "loss": 1.6391,
      "step": 66110
    },
    {
      "epoch": 33.63173957273652,
      "grad_norm": 34.797855377197266,
      "learning_rate": 1.636826042726348e-05,
      "loss": 1.6236,
      "step": 66120
    },
    {
      "epoch": 33.63682604272635,
      "grad_norm": 40.06403350830078,
      "learning_rate": 1.6363173957273654e-05,
      "loss": 1.7063,
      "step": 66130
    },
    {
      "epoch": 33.641912512716175,
      "grad_norm": 36.55303192138672,
      "learning_rate": 1.6358087487283827e-05,
      "loss": 1.6121,
      "step": 66140
    },
    {
      "epoch": 33.646998982706,
      "grad_norm": 56.971527099609375,
      "learning_rate": 1.6353001017293997e-05,
      "loss": 1.6486,
      "step": 66150
    },
    {
      "epoch": 33.65208545269583,
      "grad_norm": 37.606075286865234,
      "learning_rate": 1.634791454730417e-05,
      "loss": 1.7252,
      "step": 66160
    },
    {
      "epoch": 33.657171922685656,
      "grad_norm": 40.3114128112793,
      "learning_rate": 1.6342828077314347e-05,
      "loss": 1.6349,
      "step": 66170
    },
    {
      "epoch": 33.66225839267548,
      "grad_norm": 34.980045318603516,
      "learning_rate": 1.6337741607324517e-05,
      "loss": 1.7234,
      "step": 66180
    },
    {
      "epoch": 33.66734486266531,
      "grad_norm": 39.109031677246094,
      "learning_rate": 1.633265513733469e-05,
      "loss": 1.5562,
      "step": 66190
    },
    {
      "epoch": 33.67243133265514,
      "grad_norm": 33.187782287597656,
      "learning_rate": 1.6327568667344864e-05,
      "loss": 1.5963,
      "step": 66200
    },
    {
      "epoch": 33.677517802644964,
      "grad_norm": 35.9639892578125,
      "learning_rate": 1.6322482197355034e-05,
      "loss": 1.7164,
      "step": 66210
    },
    {
      "epoch": 33.68260427263479,
      "grad_norm": 48.06353759765625,
      "learning_rate": 1.631739572736521e-05,
      "loss": 1.6875,
      "step": 66220
    },
    {
      "epoch": 33.68769074262462,
      "grad_norm": 44.78584671020508,
      "learning_rate": 1.6312309257375383e-05,
      "loss": 1.6379,
      "step": 66230
    },
    {
      "epoch": 33.692777212614445,
      "grad_norm": 37.553871154785156,
      "learning_rate": 1.6307222787385557e-05,
      "loss": 1.6411,
      "step": 66240
    },
    {
      "epoch": 33.69786368260427,
      "grad_norm": 37.46173095703125,
      "learning_rate": 1.6302136317395727e-05,
      "loss": 1.6641,
      "step": 66250
    },
    {
      "epoch": 33.7029501525941,
      "grad_norm": 44.94013977050781,
      "learning_rate": 1.62970498474059e-05,
      "loss": 1.6129,
      "step": 66260
    },
    {
      "epoch": 33.708036622583926,
      "grad_norm": 45.866554260253906,
      "learning_rate": 1.6291963377416077e-05,
      "loss": 1.7723,
      "step": 66270
    },
    {
      "epoch": 33.71312309257375,
      "grad_norm": 41.327938079833984,
      "learning_rate": 1.6286876907426247e-05,
      "loss": 1.738,
      "step": 66280
    },
    {
      "epoch": 33.71820956256358,
      "grad_norm": 41.67911148071289,
      "learning_rate": 1.628179043743642e-05,
      "loss": 1.6086,
      "step": 66290
    },
    {
      "epoch": 33.72329603255341,
      "grad_norm": 37.21437454223633,
      "learning_rate": 1.6276703967446593e-05,
      "loss": 1.619,
      "step": 66300
    },
    {
      "epoch": 33.728382502543234,
      "grad_norm": 38.388275146484375,
      "learning_rate": 1.6271617497456766e-05,
      "loss": 1.6697,
      "step": 66310
    },
    {
      "epoch": 33.73346897253306,
      "grad_norm": 49.15843200683594,
      "learning_rate": 1.626653102746694e-05,
      "loss": 1.6248,
      "step": 66320
    },
    {
      "epoch": 33.73855544252289,
      "grad_norm": 33.70553970336914,
      "learning_rate": 1.6261444557477113e-05,
      "loss": 1.7039,
      "step": 66330
    },
    {
      "epoch": 33.743641912512714,
      "grad_norm": 38.67939758300781,
      "learning_rate": 1.6256358087487283e-05,
      "loss": 1.7411,
      "step": 66340
    },
    {
      "epoch": 33.74872838250254,
      "grad_norm": 41.286170959472656,
      "learning_rate": 1.6251271617497456e-05,
      "loss": 1.7917,
      "step": 66350
    },
    {
      "epoch": 33.75381485249237,
      "grad_norm": 42.77887725830078,
      "learning_rate": 1.624618514750763e-05,
      "loss": 1.7154,
      "step": 66360
    },
    {
      "epoch": 33.758901322482195,
      "grad_norm": 33.086795806884766,
      "learning_rate": 1.6241098677517806e-05,
      "loss": 1.7118,
      "step": 66370
    },
    {
      "epoch": 33.76398779247202,
      "grad_norm": 39.94921875,
      "learning_rate": 1.6236012207527976e-05,
      "loss": 1.7444,
      "step": 66380
    },
    {
      "epoch": 33.76907426246185,
      "grad_norm": 38.19841003417969,
      "learning_rate": 1.623092573753815e-05,
      "loss": 1.7293,
      "step": 66390
    },
    {
      "epoch": 33.774160732451676,
      "grad_norm": 35.644920349121094,
      "learning_rate": 1.6225839267548323e-05,
      "loss": 1.7395,
      "step": 66400
    },
    {
      "epoch": 33.7792472024415,
      "grad_norm": 44.58189392089844,
      "learning_rate": 1.6220752797558496e-05,
      "loss": 1.7394,
      "step": 66410
    },
    {
      "epoch": 33.78433367243133,
      "grad_norm": 43.118404388427734,
      "learning_rate": 1.621566632756867e-05,
      "loss": 1.6936,
      "step": 66420
    },
    {
      "epoch": 33.78942014242116,
      "grad_norm": 31.423107147216797,
      "learning_rate": 1.6210579857578842e-05,
      "loss": 1.7153,
      "step": 66430
    },
    {
      "epoch": 33.794506612410984,
      "grad_norm": 33.05214309692383,
      "learning_rate": 1.6205493387589012e-05,
      "loss": 1.7074,
      "step": 66440
    },
    {
      "epoch": 33.79959308240081,
      "grad_norm": 41.54182434082031,
      "learning_rate": 1.6200406917599186e-05,
      "loss": 1.6666,
      "step": 66450
    },
    {
      "epoch": 33.80467955239064,
      "grad_norm": 39.63288497924805,
      "learning_rate": 1.6195320447609362e-05,
      "loss": 1.6013,
      "step": 66460
    },
    {
      "epoch": 33.809766022380465,
      "grad_norm": 39.68089294433594,
      "learning_rate": 1.6190233977619532e-05,
      "loss": 1.7257,
      "step": 66470
    },
    {
      "epoch": 33.81485249237029,
      "grad_norm": 39.97389602661133,
      "learning_rate": 1.6185147507629705e-05,
      "loss": 1.7138,
      "step": 66480
    },
    {
      "epoch": 33.81993896236012,
      "grad_norm": 42.953826904296875,
      "learning_rate": 1.618006103763988e-05,
      "loss": 1.6321,
      "step": 66490
    },
    {
      "epoch": 33.825025432349946,
      "grad_norm": 40.55851745605469,
      "learning_rate": 1.6174974567650052e-05,
      "loss": 1.7464,
      "step": 66500
    },
    {
      "epoch": 33.83011190233977,
      "grad_norm": 47.324974060058594,
      "learning_rate": 1.6169888097660225e-05,
      "loss": 1.6407,
      "step": 66510
    },
    {
      "epoch": 33.8351983723296,
      "grad_norm": 49.583404541015625,
      "learning_rate": 1.61648016276704e-05,
      "loss": 1.6569,
      "step": 66520
    },
    {
      "epoch": 33.84028484231943,
      "grad_norm": 42.203060150146484,
      "learning_rate": 1.6159715157680572e-05,
      "loss": 1.6596,
      "step": 66530
    },
    {
      "epoch": 33.845371312309254,
      "grad_norm": 57.17510986328125,
      "learning_rate": 1.615462868769074e-05,
      "loss": 1.643,
      "step": 66540
    },
    {
      "epoch": 33.85045778229908,
      "grad_norm": 38.3194465637207,
      "learning_rate": 1.6149542217700915e-05,
      "loss": 1.7396,
      "step": 66550
    },
    {
      "epoch": 33.85554425228891,
      "grad_norm": 34.4988899230957,
      "learning_rate": 1.614445574771109e-05,
      "loss": 1.7264,
      "step": 66560
    },
    {
      "epoch": 33.86063072227874,
      "grad_norm": 33.618682861328125,
      "learning_rate": 1.613936927772126e-05,
      "loss": 1.7232,
      "step": 66570
    },
    {
      "epoch": 33.86571719226856,
      "grad_norm": 39.305091857910156,
      "learning_rate": 1.6134282807731435e-05,
      "loss": 1.7336,
      "step": 66580
    },
    {
      "epoch": 33.870803662258396,
      "grad_norm": 44.91070556640625,
      "learning_rate": 1.6129196337741608e-05,
      "loss": 1.7051,
      "step": 66590
    },
    {
      "epoch": 33.87589013224822,
      "grad_norm": 32.8403205871582,
      "learning_rate": 1.612410986775178e-05,
      "loss": 1.6548,
      "step": 66600
    },
    {
      "epoch": 33.88097660223805,
      "grad_norm": 35.98191452026367,
      "learning_rate": 1.6119023397761955e-05,
      "loss": 1.6893,
      "step": 66610
    },
    {
      "epoch": 33.88606307222788,
      "grad_norm": 38.26158142089844,
      "learning_rate": 1.6113936927772128e-05,
      "loss": 1.6714,
      "step": 66620
    },
    {
      "epoch": 33.891149542217704,
      "grad_norm": 51.707427978515625,
      "learning_rate": 1.6108850457782298e-05,
      "loss": 1.7767,
      "step": 66630
    },
    {
      "epoch": 33.89623601220753,
      "grad_norm": 37.743995666503906,
      "learning_rate": 1.610376398779247e-05,
      "loss": 1.7136,
      "step": 66640
    },
    {
      "epoch": 33.90132248219736,
      "grad_norm": 35.04497528076172,
      "learning_rate": 1.6098677517802648e-05,
      "loss": 1.7485,
      "step": 66650
    },
    {
      "epoch": 33.906408952187185,
      "grad_norm": 34.200889587402344,
      "learning_rate": 1.609359104781282e-05,
      "loss": 1.7015,
      "step": 66660
    },
    {
      "epoch": 33.91149542217701,
      "grad_norm": 42.100494384765625,
      "learning_rate": 1.608850457782299e-05,
      "loss": 1.713,
      "step": 66670
    },
    {
      "epoch": 33.91658189216684,
      "grad_norm": 35.16596221923828,
      "learning_rate": 1.6083418107833164e-05,
      "loss": 1.5895,
      "step": 66680
    },
    {
      "epoch": 33.921668362156666,
      "grad_norm": 41.41745376586914,
      "learning_rate": 1.6078331637843338e-05,
      "loss": 1.6626,
      "step": 66690
    },
    {
      "epoch": 33.92675483214649,
      "grad_norm": 39.01633834838867,
      "learning_rate": 1.607324516785351e-05,
      "loss": 1.6708,
      "step": 66700
    },
    {
      "epoch": 33.93184130213632,
      "grad_norm": 38.56546401977539,
      "learning_rate": 1.6068158697863684e-05,
      "loss": 1.6884,
      "step": 66710
    },
    {
      "epoch": 33.93692777212615,
      "grad_norm": 35.953277587890625,
      "learning_rate": 1.6063072227873857e-05,
      "loss": 1.6345,
      "step": 66720
    },
    {
      "epoch": 33.942014242115974,
      "grad_norm": 32.02845764160156,
      "learning_rate": 1.6057985757884027e-05,
      "loss": 1.7143,
      "step": 66730
    },
    {
      "epoch": 33.9471007121058,
      "grad_norm": 52.22918701171875,
      "learning_rate": 1.60528992878942e-05,
      "loss": 1.6784,
      "step": 66740
    },
    {
      "epoch": 33.95218718209563,
      "grad_norm": 33.68070983886719,
      "learning_rate": 1.6047812817904377e-05,
      "loss": 1.724,
      "step": 66750
    },
    {
      "epoch": 33.957273652085455,
      "grad_norm": 40.23324203491211,
      "learning_rate": 1.6042726347914547e-05,
      "loss": 1.7332,
      "step": 66760
    },
    {
      "epoch": 33.96236012207528,
      "grad_norm": 31.574018478393555,
      "learning_rate": 1.603763987792472e-05,
      "loss": 1.6794,
      "step": 66770
    },
    {
      "epoch": 33.96744659206511,
      "grad_norm": 50.01007843017578,
      "learning_rate": 1.6032553407934894e-05,
      "loss": 1.6708,
      "step": 66780
    },
    {
      "epoch": 33.972533062054936,
      "grad_norm": 42.01715850830078,
      "learning_rate": 1.6027466937945067e-05,
      "loss": 1.7408,
      "step": 66790
    },
    {
      "epoch": 33.97761953204476,
      "grad_norm": 33.56958770751953,
      "learning_rate": 1.602238046795524e-05,
      "loss": 1.6241,
      "step": 66800
    },
    {
      "epoch": 33.98270600203459,
      "grad_norm": 54.185691833496094,
      "learning_rate": 1.6017293997965414e-05,
      "loss": 1.6154,
      "step": 66810
    },
    {
      "epoch": 33.98779247202442,
      "grad_norm": 38.272335052490234,
      "learning_rate": 1.6012207527975587e-05,
      "loss": 1.5969,
      "step": 66820
    },
    {
      "epoch": 33.992878942014244,
      "grad_norm": 34.20122528076172,
      "learning_rate": 1.6007121057985757e-05,
      "loss": 1.7147,
      "step": 66830
    },
    {
      "epoch": 33.99796541200407,
      "grad_norm": 48.808204650878906,
      "learning_rate": 1.600203458799593e-05,
      "loss": 1.636,
      "step": 66840
    },
    {
      "epoch": 34.0,
      "eval_loss": 4.899716854095459,
      "eval_runtime": 2.6934,
      "eval_samples_per_second": 1030.303,
      "eval_steps_per_second": 128.834,
      "step": 66844
    },
    {
      "epoch": 34.0030518819939,
      "grad_norm": 45.87662124633789,
      "learning_rate": 1.5996948118006107e-05,
      "loss": 1.7105,
      "step": 66850
    },
    {
      "epoch": 34.008138351983725,
      "grad_norm": 35.80006790161133,
      "learning_rate": 1.5991861648016277e-05,
      "loss": 1.6427,
      "step": 66860
    },
    {
      "epoch": 34.01322482197355,
      "grad_norm": 47.139766693115234,
      "learning_rate": 1.598677517802645e-05,
      "loss": 1.6368,
      "step": 66870
    },
    {
      "epoch": 34.01831129196338,
      "grad_norm": 33.40153884887695,
      "learning_rate": 1.5981688708036623e-05,
      "loss": 1.7116,
      "step": 66880
    },
    {
      "epoch": 34.023397761953206,
      "grad_norm": 35.727386474609375,
      "learning_rate": 1.5976602238046796e-05,
      "loss": 1.6857,
      "step": 66890
    },
    {
      "epoch": 34.02848423194303,
      "grad_norm": 44.653690338134766,
      "learning_rate": 1.597151576805697e-05,
      "loss": 1.5986,
      "step": 66900
    },
    {
      "epoch": 34.03357070193286,
      "grad_norm": 46.904754638671875,
      "learning_rate": 1.5966429298067143e-05,
      "loss": 1.7026,
      "step": 66910
    },
    {
      "epoch": 34.03865717192269,
      "grad_norm": 34.50074005126953,
      "learning_rate": 1.5961342828077316e-05,
      "loss": 1.688,
      "step": 66920
    },
    {
      "epoch": 34.04374364191251,
      "grad_norm": 41.77354049682617,
      "learning_rate": 1.5956256358087486e-05,
      "loss": 1.6792,
      "step": 66930
    },
    {
      "epoch": 34.04883011190234,
      "grad_norm": 34.60446548461914,
      "learning_rate": 1.5951169888097663e-05,
      "loss": 1.6558,
      "step": 66940
    },
    {
      "epoch": 34.05391658189217,
      "grad_norm": 39.089561462402344,
      "learning_rate": 1.5946083418107836e-05,
      "loss": 1.6723,
      "step": 66950
    },
    {
      "epoch": 34.059003051881994,
      "grad_norm": 31.690183639526367,
      "learning_rate": 1.5940996948118006e-05,
      "loss": 1.5408,
      "step": 66960
    },
    {
      "epoch": 34.06408952187182,
      "grad_norm": 40.17906951904297,
      "learning_rate": 1.593591047812818e-05,
      "loss": 1.6641,
      "step": 66970
    },
    {
      "epoch": 34.06917599186165,
      "grad_norm": 31.375259399414062,
      "learning_rate": 1.5930824008138353e-05,
      "loss": 1.6581,
      "step": 66980
    },
    {
      "epoch": 34.074262461851475,
      "grad_norm": 35.04364776611328,
      "learning_rate": 1.5925737538148526e-05,
      "loss": 1.7108,
      "step": 66990
    },
    {
      "epoch": 34.0793489318413,
      "grad_norm": 39.04237747192383,
      "learning_rate": 1.59206510681587e-05,
      "loss": 1.6655,
      "step": 67000
    },
    {
      "epoch": 34.08443540183113,
      "grad_norm": 38.12782669067383,
      "learning_rate": 1.5915564598168872e-05,
      "loss": 1.7066,
      "step": 67010
    },
    {
      "epoch": 34.089521871820956,
      "grad_norm": 40.10914611816406,
      "learning_rate": 1.5910478128179042e-05,
      "loss": 1.6511,
      "step": 67020
    },
    {
      "epoch": 34.09460834181078,
      "grad_norm": 49.077327728271484,
      "learning_rate": 1.5905391658189216e-05,
      "loss": 1.6084,
      "step": 67030
    },
    {
      "epoch": 34.09969481180061,
      "grad_norm": 43.57732009887695,
      "learning_rate": 1.5900305188199392e-05,
      "loss": 1.6907,
      "step": 67040
    },
    {
      "epoch": 34.10478128179044,
      "grad_norm": 38.825748443603516,
      "learning_rate": 1.5895218718209565e-05,
      "loss": 1.6774,
      "step": 67050
    },
    {
      "epoch": 34.109867751780264,
      "grad_norm": 41.283809661865234,
      "learning_rate": 1.5890132248219735e-05,
      "loss": 1.7269,
      "step": 67060
    },
    {
      "epoch": 34.11495422177009,
      "grad_norm": 45.797950744628906,
      "learning_rate": 1.588504577822991e-05,
      "loss": 1.6648,
      "step": 67070
    },
    {
      "epoch": 34.12004069175992,
      "grad_norm": 40.39281463623047,
      "learning_rate": 1.5879959308240082e-05,
      "loss": 1.6719,
      "step": 67080
    },
    {
      "epoch": 34.125127161749745,
      "grad_norm": 36.402278900146484,
      "learning_rate": 1.5874872838250255e-05,
      "loss": 1.6653,
      "step": 67090
    },
    {
      "epoch": 34.13021363173957,
      "grad_norm": 40.706695556640625,
      "learning_rate": 1.586978636826043e-05,
      "loss": 1.6284,
      "step": 67100
    },
    {
      "epoch": 34.1353001017294,
      "grad_norm": 38.81831359863281,
      "learning_rate": 1.5864699898270602e-05,
      "loss": 1.7122,
      "step": 67110
    },
    {
      "epoch": 34.140386571719226,
      "grad_norm": 40.37226867675781,
      "learning_rate": 1.585961342828077e-05,
      "loss": 1.6521,
      "step": 67120
    },
    {
      "epoch": 34.14547304170905,
      "grad_norm": 38.694236755371094,
      "learning_rate": 1.585452695829095e-05,
      "loss": 1.7079,
      "step": 67130
    },
    {
      "epoch": 34.15055951169888,
      "grad_norm": 42.705753326416016,
      "learning_rate": 1.584944048830112e-05,
      "loss": 1.6268,
      "step": 67140
    },
    {
      "epoch": 34.15564598168871,
      "grad_norm": 34.187156677246094,
      "learning_rate": 1.584435401831129e-05,
      "loss": 1.7539,
      "step": 67150
    },
    {
      "epoch": 34.160732451678534,
      "grad_norm": 35.00706481933594,
      "learning_rate": 1.5839267548321465e-05,
      "loss": 1.747,
      "step": 67160
    },
    {
      "epoch": 34.16581892166836,
      "grad_norm": 44.447166442871094,
      "learning_rate": 1.5834181078331638e-05,
      "loss": 1.6844,
      "step": 67170
    },
    {
      "epoch": 34.17090539165819,
      "grad_norm": 44.09883499145508,
      "learning_rate": 1.582909460834181e-05,
      "loss": 1.7128,
      "step": 67180
    },
    {
      "epoch": 34.175991861648015,
      "grad_norm": 39.479331970214844,
      "learning_rate": 1.5824008138351985e-05,
      "loss": 1.6584,
      "step": 67190
    },
    {
      "epoch": 34.18107833163784,
      "grad_norm": 52.055233001708984,
      "learning_rate": 1.5818921668362158e-05,
      "loss": 1.7776,
      "step": 67200
    },
    {
      "epoch": 34.18616480162767,
      "grad_norm": 40.12227249145508,
      "learning_rate": 1.581383519837233e-05,
      "loss": 1.7045,
      "step": 67210
    },
    {
      "epoch": 34.191251271617496,
      "grad_norm": 43.315731048583984,
      "learning_rate": 1.58087487283825e-05,
      "loss": 1.7873,
      "step": 67220
    },
    {
      "epoch": 34.19633774160732,
      "grad_norm": 50.86907196044922,
      "learning_rate": 1.5803662258392678e-05,
      "loss": 1.6812,
      "step": 67230
    },
    {
      "epoch": 34.20142421159715,
      "grad_norm": 35.5333366394043,
      "learning_rate": 1.579857578840285e-05,
      "loss": 1.7791,
      "step": 67240
    },
    {
      "epoch": 34.20651068158698,
      "grad_norm": 37.837890625,
      "learning_rate": 1.579348931841302e-05,
      "loss": 1.6906,
      "step": 67250
    },
    {
      "epoch": 34.211597151576804,
      "grad_norm": 45.22379684448242,
      "learning_rate": 1.5788402848423194e-05,
      "loss": 1.7314,
      "step": 67260
    },
    {
      "epoch": 34.21668362156663,
      "grad_norm": 55.854705810546875,
      "learning_rate": 1.5783316378433368e-05,
      "loss": 1.697,
      "step": 67270
    },
    {
      "epoch": 34.22177009155646,
      "grad_norm": 35.39427185058594,
      "learning_rate": 1.577822990844354e-05,
      "loss": 1.7113,
      "step": 67280
    },
    {
      "epoch": 34.226856561546285,
      "grad_norm": 37.02041244506836,
      "learning_rate": 1.5773143438453714e-05,
      "loss": 1.667,
      "step": 67290
    },
    {
      "epoch": 34.23194303153611,
      "grad_norm": 35.08157730102539,
      "learning_rate": 1.5768056968463887e-05,
      "loss": 1.6554,
      "step": 67300
    },
    {
      "epoch": 34.23702950152594,
      "grad_norm": 44.78850173950195,
      "learning_rate": 1.5762970498474057e-05,
      "loss": 1.6778,
      "step": 67310
    },
    {
      "epoch": 34.242115971515766,
      "grad_norm": 37.462799072265625,
      "learning_rate": 1.575788402848423e-05,
      "loss": 1.7488,
      "step": 67320
    },
    {
      "epoch": 34.24720244150559,
      "grad_norm": 45.307125091552734,
      "learning_rate": 1.5752797558494407e-05,
      "loss": 1.6552,
      "step": 67330
    },
    {
      "epoch": 34.25228891149542,
      "grad_norm": 43.547393798828125,
      "learning_rate": 1.574771108850458e-05,
      "loss": 1.6885,
      "step": 67340
    },
    {
      "epoch": 34.25737538148525,
      "grad_norm": 45.25539016723633,
      "learning_rate": 1.574262461851475e-05,
      "loss": 1.7061,
      "step": 67350
    },
    {
      "epoch": 34.262461851475074,
      "grad_norm": 50.907291412353516,
      "learning_rate": 1.5737538148524924e-05,
      "loss": 1.638,
      "step": 67360
    },
    {
      "epoch": 34.2675483214649,
      "grad_norm": 33.20884704589844,
      "learning_rate": 1.5732451678535097e-05,
      "loss": 1.6203,
      "step": 67370
    },
    {
      "epoch": 34.27263479145473,
      "grad_norm": 41.384517669677734,
      "learning_rate": 1.572736520854527e-05,
      "loss": 1.6566,
      "step": 67380
    },
    {
      "epoch": 34.277721261444555,
      "grad_norm": 35.271244049072266,
      "learning_rate": 1.5722278738555444e-05,
      "loss": 1.6412,
      "step": 67390
    },
    {
      "epoch": 34.28280773143438,
      "grad_norm": 40.9767951965332,
      "learning_rate": 1.5717192268565617e-05,
      "loss": 1.6892,
      "step": 67400
    },
    {
      "epoch": 34.28789420142421,
      "grad_norm": 40.00470733642578,
      "learning_rate": 1.5712105798575787e-05,
      "loss": 1.6412,
      "step": 67410
    },
    {
      "epoch": 34.292980671414035,
      "grad_norm": 46.27293395996094,
      "learning_rate": 1.5707019328585963e-05,
      "loss": 1.5747,
      "step": 67420
    },
    {
      "epoch": 34.29806714140386,
      "grad_norm": 33.29020309448242,
      "learning_rate": 1.5701932858596137e-05,
      "loss": 1.6273,
      "step": 67430
    },
    {
      "epoch": 34.30315361139369,
      "grad_norm": 41.95173263549805,
      "learning_rate": 1.5696846388606307e-05,
      "loss": 1.7485,
      "step": 67440
    },
    {
      "epoch": 34.308240081383516,
      "grad_norm": 29.971216201782227,
      "learning_rate": 1.569175991861648e-05,
      "loss": 1.6353,
      "step": 67450
    },
    {
      "epoch": 34.31332655137334,
      "grad_norm": 47.88552474975586,
      "learning_rate": 1.5686673448626653e-05,
      "loss": 1.6302,
      "step": 67460
    },
    {
      "epoch": 34.31841302136317,
      "grad_norm": 29.055105209350586,
      "learning_rate": 1.5681586978636826e-05,
      "loss": 1.7212,
      "step": 67470
    },
    {
      "epoch": 34.323499491353004,
      "grad_norm": 41.6639404296875,
      "learning_rate": 1.5676500508647e-05,
      "loss": 1.7065,
      "step": 67480
    },
    {
      "epoch": 34.32858596134283,
      "grad_norm": 36.12813949584961,
      "learning_rate": 1.5671414038657173e-05,
      "loss": 1.6849,
      "step": 67490
    },
    {
      "epoch": 34.33367243133266,
      "grad_norm": 41.43989562988281,
      "learning_rate": 1.5666327568667346e-05,
      "loss": 1.623,
      "step": 67500
    },
    {
      "epoch": 34.338758901322485,
      "grad_norm": 41.87031555175781,
      "learning_rate": 1.5661241098677516e-05,
      "loss": 1.6829,
      "step": 67510
    },
    {
      "epoch": 34.34384537131231,
      "grad_norm": 36.72190475463867,
      "learning_rate": 1.5656154628687693e-05,
      "loss": 1.6456,
      "step": 67520
    },
    {
      "epoch": 34.34893184130214,
      "grad_norm": 44.537330627441406,
      "learning_rate": 1.5651068158697866e-05,
      "loss": 1.7642,
      "step": 67530
    },
    {
      "epoch": 34.354018311291966,
      "grad_norm": 33.62192153930664,
      "learning_rate": 1.5645981688708036e-05,
      "loss": 1.664,
      "step": 67540
    },
    {
      "epoch": 34.35910478128179,
      "grad_norm": 49.33622741699219,
      "learning_rate": 1.564089521871821e-05,
      "loss": 1.6331,
      "step": 67550
    },
    {
      "epoch": 34.36419125127162,
      "grad_norm": 42.745094299316406,
      "learning_rate": 1.5635808748728383e-05,
      "loss": 1.6179,
      "step": 67560
    },
    {
      "epoch": 34.36927772126145,
      "grad_norm": 41.78816223144531,
      "learning_rate": 1.5630722278738556e-05,
      "loss": 1.7329,
      "step": 67570
    },
    {
      "epoch": 34.374364191251274,
      "grad_norm": 38.52616500854492,
      "learning_rate": 1.562563580874873e-05,
      "loss": 1.6187,
      "step": 67580
    },
    {
      "epoch": 34.3794506612411,
      "grad_norm": 56.16511535644531,
      "learning_rate": 1.5620549338758902e-05,
      "loss": 1.6858,
      "step": 67590
    },
    {
      "epoch": 34.38453713123093,
      "grad_norm": 49.03990173339844,
      "learning_rate": 1.5615462868769076e-05,
      "loss": 1.715,
      "step": 67600
    },
    {
      "epoch": 34.389623601220755,
      "grad_norm": 49.91615676879883,
      "learning_rate": 1.561037639877925e-05,
      "loss": 1.6979,
      "step": 67610
    },
    {
      "epoch": 34.39471007121058,
      "grad_norm": 37.38439178466797,
      "learning_rate": 1.5605289928789422e-05,
      "loss": 1.6217,
      "step": 67620
    },
    {
      "epoch": 34.39979654120041,
      "grad_norm": 43.4744873046875,
      "learning_rate": 1.5600203458799595e-05,
      "loss": 1.7375,
      "step": 67630
    },
    {
      "epoch": 34.404883011190236,
      "grad_norm": 39.23341751098633,
      "learning_rate": 1.5595116988809765e-05,
      "loss": 1.7079,
      "step": 67640
    },
    {
      "epoch": 34.40996948118006,
      "grad_norm": 35.156307220458984,
      "learning_rate": 1.559003051881994e-05,
      "loss": 1.6203,
      "step": 67650
    },
    {
      "epoch": 34.41505595116989,
      "grad_norm": 42.68776321411133,
      "learning_rate": 1.5584944048830112e-05,
      "loss": 1.6703,
      "step": 67660
    },
    {
      "epoch": 34.42014242115972,
      "grad_norm": 38.81101989746094,
      "learning_rate": 1.5579857578840285e-05,
      "loss": 1.6545,
      "step": 67670
    },
    {
      "epoch": 34.425228891149544,
      "grad_norm": 40.06007766723633,
      "learning_rate": 1.557477110885046e-05,
      "loss": 1.6196,
      "step": 67680
    },
    {
      "epoch": 34.43031536113937,
      "grad_norm": 46.40226745605469,
      "learning_rate": 1.5569684638860632e-05,
      "loss": 1.6895,
      "step": 67690
    },
    {
      "epoch": 34.4354018311292,
      "grad_norm": 35.750221252441406,
      "learning_rate": 1.55645981688708e-05,
      "loss": 1.7695,
      "step": 67700
    },
    {
      "epoch": 34.440488301119025,
      "grad_norm": 40.82941818237305,
      "learning_rate": 1.555951169888098e-05,
      "loss": 1.6295,
      "step": 67710
    },
    {
      "epoch": 34.44557477110885,
      "grad_norm": 34.990970611572266,
      "learning_rate": 1.555442522889115e-05,
      "loss": 1.7035,
      "step": 67720
    },
    {
      "epoch": 34.45066124109868,
      "grad_norm": 35.17975616455078,
      "learning_rate": 1.5549338758901325e-05,
      "loss": 1.6774,
      "step": 67730
    },
    {
      "epoch": 34.455747711088506,
      "grad_norm": 33.33604431152344,
      "learning_rate": 1.5544252288911495e-05,
      "loss": 1.6427,
      "step": 67740
    },
    {
      "epoch": 34.46083418107833,
      "grad_norm": 40.857112884521484,
      "learning_rate": 1.5539165818921668e-05,
      "loss": 1.6964,
      "step": 67750
    },
    {
      "epoch": 34.46592065106816,
      "grad_norm": 44.24462127685547,
      "learning_rate": 1.5534079348931845e-05,
      "loss": 1.6723,
      "step": 67760
    },
    {
      "epoch": 34.47100712105799,
      "grad_norm": 38.483612060546875,
      "learning_rate": 1.5528992878942015e-05,
      "loss": 1.6745,
      "step": 67770
    },
    {
      "epoch": 34.476093591047814,
      "grad_norm": 42.243621826171875,
      "learning_rate": 1.5523906408952188e-05,
      "loss": 1.5974,
      "step": 67780
    },
    {
      "epoch": 34.48118006103764,
      "grad_norm": 35.652870178222656,
      "learning_rate": 1.551881993896236e-05,
      "loss": 1.6649,
      "step": 67790
    },
    {
      "epoch": 34.48626653102747,
      "grad_norm": 55.179386138916016,
      "learning_rate": 1.551373346897253e-05,
      "loss": 1.5515,
      "step": 67800
    },
    {
      "epoch": 34.491353001017295,
      "grad_norm": 33.66626739501953,
      "learning_rate": 1.5508646998982708e-05,
      "loss": 1.6478,
      "step": 67810
    },
    {
      "epoch": 34.49643947100712,
      "grad_norm": 42.83509063720703,
      "learning_rate": 1.550356052899288e-05,
      "loss": 1.6597,
      "step": 67820
    },
    {
      "epoch": 34.50152594099695,
      "grad_norm": 51.78303527832031,
      "learning_rate": 1.549847405900305e-05,
      "loss": 1.7838,
      "step": 67830
    },
    {
      "epoch": 34.506612410986776,
      "grad_norm": 39.69502639770508,
      "learning_rate": 1.5493387589013224e-05,
      "loss": 1.6544,
      "step": 67840
    },
    {
      "epoch": 34.5116988809766,
      "grad_norm": 31.472597122192383,
      "learning_rate": 1.5488301119023398e-05,
      "loss": 1.5917,
      "step": 67850
    },
    {
      "epoch": 34.51678535096643,
      "grad_norm": 45.350120544433594,
      "learning_rate": 1.5483214649033574e-05,
      "loss": 1.6687,
      "step": 67860
    },
    {
      "epoch": 34.52187182095626,
      "grad_norm": 32.956703186035156,
      "learning_rate": 1.5478128179043744e-05,
      "loss": 1.7375,
      "step": 67870
    },
    {
      "epoch": 34.526958290946084,
      "grad_norm": 40.085227966308594,
      "learning_rate": 1.5473041709053917e-05,
      "loss": 1.649,
      "step": 67880
    },
    {
      "epoch": 34.53204476093591,
      "grad_norm": 36.14289474487305,
      "learning_rate": 1.546795523906409e-05,
      "loss": 1.7332,
      "step": 67890
    },
    {
      "epoch": 34.53713123092574,
      "grad_norm": 38.03523254394531,
      "learning_rate": 1.5462868769074264e-05,
      "loss": 1.74,
      "step": 67900
    },
    {
      "epoch": 34.542217700915565,
      "grad_norm": 48.60838317871094,
      "learning_rate": 1.5457782299084437e-05,
      "loss": 1.7464,
      "step": 67910
    },
    {
      "epoch": 34.54730417090539,
      "grad_norm": 38.51335906982422,
      "learning_rate": 1.545269582909461e-05,
      "loss": 1.6718,
      "step": 67920
    },
    {
      "epoch": 34.55239064089522,
      "grad_norm": 38.123016357421875,
      "learning_rate": 1.544760935910478e-05,
      "loss": 1.6371,
      "step": 67930
    },
    {
      "epoch": 34.557477110885046,
      "grad_norm": 43.0558967590332,
      "learning_rate": 1.5442522889114954e-05,
      "loss": 1.6556,
      "step": 67940
    },
    {
      "epoch": 34.56256358087487,
      "grad_norm": 37.211181640625,
      "learning_rate": 1.5437436419125127e-05,
      "loss": 1.7648,
      "step": 67950
    },
    {
      "epoch": 34.5676500508647,
      "grad_norm": 42.30058670043945,
      "learning_rate": 1.54323499491353e-05,
      "loss": 1.631,
      "step": 67960
    },
    {
      "epoch": 34.57273652085453,
      "grad_norm": 53.412357330322266,
      "learning_rate": 1.5427263479145474e-05,
      "loss": 1.6395,
      "step": 67970
    },
    {
      "epoch": 34.57782299084435,
      "grad_norm": 37.9547233581543,
      "learning_rate": 1.5422177009155647e-05,
      "loss": 1.6974,
      "step": 67980
    },
    {
      "epoch": 34.58290946083418,
      "grad_norm": 40.658180236816406,
      "learning_rate": 1.541709053916582e-05,
      "loss": 1.6324,
      "step": 67990
    },
    {
      "epoch": 34.58799593082401,
      "grad_norm": 40.723785400390625,
      "learning_rate": 1.5412004069175993e-05,
      "loss": 1.6126,
      "step": 68000
    },
    {
      "epoch": 34.593082400813834,
      "grad_norm": 44.40083694458008,
      "learning_rate": 1.5406917599186167e-05,
      "loss": 1.6509,
      "step": 68010
    },
    {
      "epoch": 34.59816887080366,
      "grad_norm": 49.32661437988281,
      "learning_rate": 1.540183112919634e-05,
      "loss": 1.665,
      "step": 68020
    },
    {
      "epoch": 34.60325534079349,
      "grad_norm": 45.34969711303711,
      "learning_rate": 1.539674465920651e-05,
      "loss": 1.6873,
      "step": 68030
    },
    {
      "epoch": 34.608341810783315,
      "grad_norm": 33.27268600463867,
      "learning_rate": 1.5391658189216683e-05,
      "loss": 1.6848,
      "step": 68040
    },
    {
      "epoch": 34.61342828077314,
      "grad_norm": 40.73270034790039,
      "learning_rate": 1.538657171922686e-05,
      "loss": 1.6617,
      "step": 68050
    },
    {
      "epoch": 34.61851475076297,
      "grad_norm": 35.37350082397461,
      "learning_rate": 1.538148524923703e-05,
      "loss": 1.6023,
      "step": 68060
    },
    {
      "epoch": 34.623601220752796,
      "grad_norm": 34.15386199951172,
      "learning_rate": 1.5376398779247203e-05,
      "loss": 1.6521,
      "step": 68070
    },
    {
      "epoch": 34.62868769074262,
      "grad_norm": 47.69395065307617,
      "learning_rate": 1.5371312309257376e-05,
      "loss": 1.7074,
      "step": 68080
    },
    {
      "epoch": 34.63377416073245,
      "grad_norm": 39.34418869018555,
      "learning_rate": 1.536622583926755e-05,
      "loss": 1.6488,
      "step": 68090
    },
    {
      "epoch": 34.63886063072228,
      "grad_norm": 52.81306838989258,
      "learning_rate": 1.5361139369277723e-05,
      "loss": 1.7175,
      "step": 68100
    },
    {
      "epoch": 34.643947100712104,
      "grad_norm": 37.39869689941406,
      "learning_rate": 1.5356052899287896e-05,
      "loss": 1.6661,
      "step": 68110
    },
    {
      "epoch": 34.64903357070193,
      "grad_norm": 47.61777114868164,
      "learning_rate": 1.5350966429298066e-05,
      "loss": 1.6777,
      "step": 68120
    },
    {
      "epoch": 34.65412004069176,
      "grad_norm": 49.772071838378906,
      "learning_rate": 1.534587995930824e-05,
      "loss": 1.6755,
      "step": 68130
    },
    {
      "epoch": 34.659206510681585,
      "grad_norm": 56.423370361328125,
      "learning_rate": 1.5340793489318413e-05,
      "loss": 1.6463,
      "step": 68140
    },
    {
      "epoch": 34.66429298067141,
      "grad_norm": 33.67693328857422,
      "learning_rate": 1.533570701932859e-05,
      "loss": 1.5822,
      "step": 68150
    },
    {
      "epoch": 34.66937945066124,
      "grad_norm": 39.39374542236328,
      "learning_rate": 1.533062054933876e-05,
      "loss": 1.6558,
      "step": 68160
    },
    {
      "epoch": 34.674465920651066,
      "grad_norm": 46.42409133911133,
      "learning_rate": 1.5325534079348932e-05,
      "loss": 1.5915,
      "step": 68170
    },
    {
      "epoch": 34.67955239064089,
      "grad_norm": 38.513916015625,
      "learning_rate": 1.5320447609359106e-05,
      "loss": 1.6103,
      "step": 68180
    },
    {
      "epoch": 34.68463886063072,
      "grad_norm": 37.25923156738281,
      "learning_rate": 1.531536113936928e-05,
      "loss": 1.7487,
      "step": 68190
    },
    {
      "epoch": 34.68972533062055,
      "grad_norm": 45.87030029296875,
      "learning_rate": 1.5310274669379452e-05,
      "loss": 1.6912,
      "step": 68200
    },
    {
      "epoch": 34.694811800610374,
      "grad_norm": 38.93106460571289,
      "learning_rate": 1.5305188199389626e-05,
      "loss": 1.7167,
      "step": 68210
    },
    {
      "epoch": 34.6998982706002,
      "grad_norm": 39.4211311340332,
      "learning_rate": 1.5300101729399795e-05,
      "loss": 1.6622,
      "step": 68220
    },
    {
      "epoch": 34.70498474059003,
      "grad_norm": 37.491546630859375,
      "learning_rate": 1.529501525940997e-05,
      "loss": 1.7007,
      "step": 68230
    },
    {
      "epoch": 34.710071210579855,
      "grad_norm": 57.92472457885742,
      "learning_rate": 1.5289928789420145e-05,
      "loss": 1.643,
      "step": 68240
    },
    {
      "epoch": 34.71515768056968,
      "grad_norm": 32.15880584716797,
      "learning_rate": 1.5284842319430315e-05,
      "loss": 1.7103,
      "step": 68250
    },
    {
      "epoch": 34.72024415055951,
      "grad_norm": 37.18538284301758,
      "learning_rate": 1.527975584944049e-05,
      "loss": 1.5665,
      "step": 68260
    },
    {
      "epoch": 34.725330620549336,
      "grad_norm": 47.91593933105469,
      "learning_rate": 1.5274669379450662e-05,
      "loss": 1.6398,
      "step": 68270
    },
    {
      "epoch": 34.73041709053916,
      "grad_norm": 42.365089416503906,
      "learning_rate": 1.5269582909460835e-05,
      "loss": 1.6653,
      "step": 68280
    },
    {
      "epoch": 34.73550356052899,
      "grad_norm": 41.39643478393555,
      "learning_rate": 1.526449643947101e-05,
      "loss": 1.755,
      "step": 68290
    },
    {
      "epoch": 34.74059003051882,
      "grad_norm": 49.82284927368164,
      "learning_rate": 1.525940996948118e-05,
      "loss": 1.6975,
      "step": 68300
    },
    {
      "epoch": 34.745676500508644,
      "grad_norm": 47.464935302734375,
      "learning_rate": 1.5254323499491355e-05,
      "loss": 1.6615,
      "step": 68310
    },
    {
      "epoch": 34.75076297049847,
      "grad_norm": 34.646575927734375,
      "learning_rate": 1.5249237029501527e-05,
      "loss": 1.6549,
      "step": 68320
    },
    {
      "epoch": 34.7558494404883,
      "grad_norm": 36.49760055541992,
      "learning_rate": 1.52441505595117e-05,
      "loss": 1.6274,
      "step": 68330
    },
    {
      "epoch": 34.760935910478125,
      "grad_norm": 32.545589447021484,
      "learning_rate": 1.5239064089521873e-05,
      "loss": 1.7106,
      "step": 68340
    },
    {
      "epoch": 34.76602238046795,
      "grad_norm": 38.213043212890625,
      "learning_rate": 1.5233977619532045e-05,
      "loss": 1.6902,
      "step": 68350
    },
    {
      "epoch": 34.77110885045778,
      "grad_norm": 38.255008697509766,
      "learning_rate": 1.5228891149542218e-05,
      "loss": 1.6457,
      "step": 68360
    },
    {
      "epoch": 34.77619532044761,
      "grad_norm": 41.18362045288086,
      "learning_rate": 1.5223804679552391e-05,
      "loss": 1.6108,
      "step": 68370
    },
    {
      "epoch": 34.78128179043744,
      "grad_norm": 41.892860412597656,
      "learning_rate": 1.5218718209562563e-05,
      "loss": 1.71,
      "step": 68380
    },
    {
      "epoch": 34.78636826042727,
      "grad_norm": 39.60723114013672,
      "learning_rate": 1.5213631739572736e-05,
      "loss": 1.6649,
      "step": 68390
    },
    {
      "epoch": 34.791454730417094,
      "grad_norm": 43.06098175048828,
      "learning_rate": 1.5208545269582911e-05,
      "loss": 1.6551,
      "step": 68400
    },
    {
      "epoch": 34.79654120040692,
      "grad_norm": 51.28156280517578,
      "learning_rate": 1.5203458799593084e-05,
      "loss": 1.5713,
      "step": 68410
    },
    {
      "epoch": 34.80162767039675,
      "grad_norm": 50.28483963012695,
      "learning_rate": 1.5198372329603256e-05,
      "loss": 1.6665,
      "step": 68420
    },
    {
      "epoch": 34.806714140386575,
      "grad_norm": 44.156585693359375,
      "learning_rate": 1.519328585961343e-05,
      "loss": 1.7018,
      "step": 68430
    },
    {
      "epoch": 34.8118006103764,
      "grad_norm": 39.61970901489258,
      "learning_rate": 1.5188199389623603e-05,
      "loss": 1.6345,
      "step": 68440
    },
    {
      "epoch": 34.81688708036623,
      "grad_norm": 40.47579574584961,
      "learning_rate": 1.5183112919633774e-05,
      "loss": 1.6438,
      "step": 68450
    },
    {
      "epoch": 34.821973550356056,
      "grad_norm": 57.57145690917969,
      "learning_rate": 1.5178026449643947e-05,
      "loss": 1.6333,
      "step": 68460
    },
    {
      "epoch": 34.82706002034588,
      "grad_norm": 42.1837272644043,
      "learning_rate": 1.5172939979654122e-05,
      "loss": 1.6538,
      "step": 68470
    },
    {
      "epoch": 34.83214649033571,
      "grad_norm": 36.08503723144531,
      "learning_rate": 1.5167853509664292e-05,
      "loss": 1.6094,
      "step": 68480
    },
    {
      "epoch": 34.83723296032554,
      "grad_norm": 57.16765213012695,
      "learning_rate": 1.5162767039674467e-05,
      "loss": 1.6095,
      "step": 68490
    },
    {
      "epoch": 34.842319430315364,
      "grad_norm": 52.70363998413086,
      "learning_rate": 1.515768056968464e-05,
      "loss": 1.6827,
      "step": 68500
    },
    {
      "epoch": 34.84740590030519,
      "grad_norm": 41.456016540527344,
      "learning_rate": 1.5152594099694812e-05,
      "loss": 1.7024,
      "step": 68510
    },
    {
      "epoch": 34.85249237029502,
      "grad_norm": 61.639427185058594,
      "learning_rate": 1.5147507629704985e-05,
      "loss": 1.6314,
      "step": 68520
    },
    {
      "epoch": 34.857578840284845,
      "grad_norm": 43.95915985107422,
      "learning_rate": 1.5142421159715159e-05,
      "loss": 1.7461,
      "step": 68530
    },
    {
      "epoch": 34.86266531027467,
      "grad_norm": 36.06349563598633,
      "learning_rate": 1.5137334689725332e-05,
      "loss": 1.6231,
      "step": 68540
    },
    {
      "epoch": 34.8677517802645,
      "grad_norm": 39.50010681152344,
      "learning_rate": 1.5132248219735504e-05,
      "loss": 1.6192,
      "step": 68550
    },
    {
      "epoch": 34.872838250254325,
      "grad_norm": 38.69561767578125,
      "learning_rate": 1.5127161749745677e-05,
      "loss": 1.6927,
      "step": 68560
    },
    {
      "epoch": 34.87792472024415,
      "grad_norm": 44.758506774902344,
      "learning_rate": 1.5122075279755852e-05,
      "loss": 1.64,
      "step": 68570
    },
    {
      "epoch": 34.88301119023398,
      "grad_norm": 39.856239318847656,
      "learning_rate": 1.5116988809766022e-05,
      "loss": 1.7039,
      "step": 68580
    },
    {
      "epoch": 34.888097660223806,
      "grad_norm": 38.895748138427734,
      "learning_rate": 1.5111902339776197e-05,
      "loss": 1.6046,
      "step": 68590
    },
    {
      "epoch": 34.89318413021363,
      "grad_norm": 50.08353042602539,
      "learning_rate": 1.510681586978637e-05,
      "loss": 1.7056,
      "step": 68600
    },
    {
      "epoch": 34.89827060020346,
      "grad_norm": 39.55626678466797,
      "learning_rate": 1.5101729399796542e-05,
      "loss": 1.7258,
      "step": 68610
    },
    {
      "epoch": 34.90335707019329,
      "grad_norm": 45.08948516845703,
      "learning_rate": 1.5096642929806715e-05,
      "loss": 1.6291,
      "step": 68620
    },
    {
      "epoch": 34.908443540183114,
      "grad_norm": 38.205238342285156,
      "learning_rate": 1.5091556459816888e-05,
      "loss": 1.6214,
      "step": 68630
    },
    {
      "epoch": 34.91353001017294,
      "grad_norm": 37.84638214111328,
      "learning_rate": 1.508646998982706e-05,
      "loss": 1.5787,
      "step": 68640
    },
    {
      "epoch": 34.91861648016277,
      "grad_norm": 55.52997970581055,
      "learning_rate": 1.5081383519837233e-05,
      "loss": 1.6461,
      "step": 68650
    },
    {
      "epoch": 34.923702950152595,
      "grad_norm": 36.58466720581055,
      "learning_rate": 1.5076297049847408e-05,
      "loss": 1.5232,
      "step": 68660
    },
    {
      "epoch": 34.92878942014242,
      "grad_norm": 41.92445373535156,
      "learning_rate": 1.5071210579857581e-05,
      "loss": 1.6464,
      "step": 68670
    },
    {
      "epoch": 34.93387589013225,
      "grad_norm": 45.310604095458984,
      "learning_rate": 1.5066124109867751e-05,
      "loss": 1.7651,
      "step": 68680
    },
    {
      "epoch": 34.938962360122076,
      "grad_norm": 35.94341278076172,
      "learning_rate": 1.5061037639877926e-05,
      "loss": 1.6568,
      "step": 68690
    },
    {
      "epoch": 34.9440488301119,
      "grad_norm": 43.999935150146484,
      "learning_rate": 1.50559511698881e-05,
      "loss": 1.681,
      "step": 68700
    },
    {
      "epoch": 34.94913530010173,
      "grad_norm": 42.42660903930664,
      "learning_rate": 1.5050864699898271e-05,
      "loss": 1.7829,
      "step": 68710
    },
    {
      "epoch": 34.95422177009156,
      "grad_norm": 36.32636260986328,
      "learning_rate": 1.5045778229908444e-05,
      "loss": 1.7366,
      "step": 68720
    },
    {
      "epoch": 34.959308240081384,
      "grad_norm": 43.32636642456055,
      "learning_rate": 1.5040691759918618e-05,
      "loss": 1.6509,
      "step": 68730
    },
    {
      "epoch": 34.96439471007121,
      "grad_norm": 34.818416595458984,
      "learning_rate": 1.5035605289928789e-05,
      "loss": 1.6298,
      "step": 68740
    },
    {
      "epoch": 34.96948118006104,
      "grad_norm": 46.91139602661133,
      "learning_rate": 1.5030518819938962e-05,
      "loss": 1.7002,
      "step": 68750
    },
    {
      "epoch": 34.974567650050865,
      "grad_norm": 36.54978942871094,
      "learning_rate": 1.5025432349949137e-05,
      "loss": 1.687,
      "step": 68760
    },
    {
      "epoch": 34.97965412004069,
      "grad_norm": 44.729862213134766,
      "learning_rate": 1.5020345879959307e-05,
      "loss": 1.5935,
      "step": 68770
    },
    {
      "epoch": 34.98474059003052,
      "grad_norm": 37.55766677856445,
      "learning_rate": 1.5015259409969482e-05,
      "loss": 1.706,
      "step": 68780
    },
    {
      "epoch": 34.989827060020346,
      "grad_norm": 34.673397064208984,
      "learning_rate": 1.5010172939979656e-05,
      "loss": 1.556,
      "step": 68790
    },
    {
      "epoch": 34.99491353001017,
      "grad_norm": 36.956844329833984,
      "learning_rate": 1.5005086469989829e-05,
      "loss": 1.6637,
      "step": 68800
    },
    {
      "epoch": 35.0,
      "grad_norm": 46.19990539550781,
      "learning_rate": 1.5e-05,
      "loss": 1.7051,
      "step": 68810
    },
    {
      "epoch": 35.0,
      "eval_loss": 4.943520545959473,
      "eval_runtime": 2.7271,
      "eval_samples_per_second": 1017.568,
      "eval_steps_per_second": 127.242,
      "step": 68810
    },
    {
      "epoch": 35.00508646998983,
      "grad_norm": 54.87370300292969,
      "learning_rate": 1.4994913530010174e-05,
      "loss": 1.6635,
      "step": 68820
    },
    {
      "epoch": 35.010172939979654,
      "grad_norm": 40.3487434387207,
      "learning_rate": 1.4989827060020347e-05,
      "loss": 1.7072,
      "step": 68830
    },
    {
      "epoch": 35.01525940996948,
      "grad_norm": 45.37821578979492,
      "learning_rate": 1.4984740590030519e-05,
      "loss": 1.6073,
      "step": 68840
    },
    {
      "epoch": 35.02034587995931,
      "grad_norm": 42.912532806396484,
      "learning_rate": 1.4979654120040692e-05,
      "loss": 1.5307,
      "step": 68850
    },
    {
      "epoch": 35.025432349949135,
      "grad_norm": 35.88697814941406,
      "learning_rate": 1.4974567650050867e-05,
      "loss": 1.6865,
      "step": 68860
    },
    {
      "epoch": 35.03051881993896,
      "grad_norm": 38.084476470947266,
      "learning_rate": 1.4969481180061037e-05,
      "loss": 1.6522,
      "step": 68870
    },
    {
      "epoch": 35.03560528992879,
      "grad_norm": 44.40262985229492,
      "learning_rate": 1.4964394710071212e-05,
      "loss": 1.6331,
      "step": 68880
    },
    {
      "epoch": 35.040691759918616,
      "grad_norm": 38.85377502441406,
      "learning_rate": 1.4959308240081385e-05,
      "loss": 1.6452,
      "step": 68890
    },
    {
      "epoch": 35.04577822990844,
      "grad_norm": 41.194828033447266,
      "learning_rate": 1.4954221770091557e-05,
      "loss": 1.8119,
      "step": 68900
    },
    {
      "epoch": 35.05086469989827,
      "grad_norm": 36.18115997314453,
      "learning_rate": 1.494913530010173e-05,
      "loss": 1.657,
      "step": 68910
    },
    {
      "epoch": 35.0559511698881,
      "grad_norm": 39.56422424316406,
      "learning_rate": 1.4944048830111903e-05,
      "loss": 1.6711,
      "step": 68920
    },
    {
      "epoch": 35.061037639877924,
      "grad_norm": 42.905277252197266,
      "learning_rate": 1.4938962360122075e-05,
      "loss": 1.66,
      "step": 68930
    },
    {
      "epoch": 35.06612410986775,
      "grad_norm": 49.32353591918945,
      "learning_rate": 1.4933875890132248e-05,
      "loss": 1.7022,
      "step": 68940
    },
    {
      "epoch": 35.07121057985758,
      "grad_norm": 34.91532897949219,
      "learning_rate": 1.4928789420142423e-05,
      "loss": 1.6799,
      "step": 68950
    },
    {
      "epoch": 35.076297049847405,
      "grad_norm": 39.01618957519531,
      "learning_rate": 1.4923702950152596e-05,
      "loss": 1.6373,
      "step": 68960
    },
    {
      "epoch": 35.08138351983723,
      "grad_norm": 45.129638671875,
      "learning_rate": 1.4918616480162768e-05,
      "loss": 1.5779,
      "step": 68970
    },
    {
      "epoch": 35.08646998982706,
      "grad_norm": 35.25495147705078,
      "learning_rate": 1.4913530010172941e-05,
      "loss": 1.7047,
      "step": 68980
    },
    {
      "epoch": 35.091556459816886,
      "grad_norm": 46.418052673339844,
      "learning_rate": 1.4908443540183114e-05,
      "loss": 1.7319,
      "step": 68990
    },
    {
      "epoch": 35.09664292980671,
      "grad_norm": 42.11519241333008,
      "learning_rate": 1.4903357070193286e-05,
      "loss": 1.6637,
      "step": 69000
    },
    {
      "epoch": 35.10172939979654,
      "grad_norm": 43.14596939086914,
      "learning_rate": 1.489827060020346e-05,
      "loss": 1.6792,
      "step": 69010
    },
    {
      "epoch": 35.10681586978637,
      "grad_norm": 34.77446746826172,
      "learning_rate": 1.4893184130213633e-05,
      "loss": 1.6385,
      "step": 69020
    },
    {
      "epoch": 35.111902339776194,
      "grad_norm": 40.54256820678711,
      "learning_rate": 1.4888097660223804e-05,
      "loss": 1.6576,
      "step": 69030
    },
    {
      "epoch": 35.11698880976602,
      "grad_norm": 35.09758758544922,
      "learning_rate": 1.4883011190233977e-05,
      "loss": 1.6088,
      "step": 69040
    },
    {
      "epoch": 35.12207527975585,
      "grad_norm": 54.68547058105469,
      "learning_rate": 1.4877924720244152e-05,
      "loss": 1.6084,
      "step": 69050
    },
    {
      "epoch": 35.127161749745675,
      "grad_norm": 35.393760681152344,
      "learning_rate": 1.4872838250254322e-05,
      "loss": 1.6543,
      "step": 69060
    },
    {
      "epoch": 35.1322482197355,
      "grad_norm": 30.041522979736328,
      "learning_rate": 1.4867751780264497e-05,
      "loss": 1.591,
      "step": 69070
    },
    {
      "epoch": 35.13733468972533,
      "grad_norm": 46.5186653137207,
      "learning_rate": 1.486266531027467e-05,
      "loss": 1.5781,
      "step": 69080
    },
    {
      "epoch": 35.142421159715155,
      "grad_norm": 46.66154861450195,
      "learning_rate": 1.4857578840284844e-05,
      "loss": 1.6275,
      "step": 69090
    },
    {
      "epoch": 35.14750762970498,
      "grad_norm": 45.19977951049805,
      "learning_rate": 1.4852492370295015e-05,
      "loss": 1.6644,
      "step": 69100
    },
    {
      "epoch": 35.15259409969481,
      "grad_norm": 35.634559631347656,
      "learning_rate": 1.4847405900305189e-05,
      "loss": 1.667,
      "step": 69110
    },
    {
      "epoch": 35.157680569684636,
      "grad_norm": 44.75748062133789,
      "learning_rate": 1.4842319430315364e-05,
      "loss": 1.6795,
      "step": 69120
    },
    {
      "epoch": 35.16276703967446,
      "grad_norm": 42.36014175415039,
      "learning_rate": 1.4837232960325534e-05,
      "loss": 1.6569,
      "step": 69130
    },
    {
      "epoch": 35.16785350966429,
      "grad_norm": 34.00132751464844,
      "learning_rate": 1.4832146490335709e-05,
      "loss": 1.6322,
      "step": 69140
    },
    {
      "epoch": 35.17293997965412,
      "grad_norm": 37.11098098754883,
      "learning_rate": 1.4827060020345882e-05,
      "loss": 1.64,
      "step": 69150
    },
    {
      "epoch": 35.178026449643944,
      "grad_norm": 38.205318450927734,
      "learning_rate": 1.4821973550356052e-05,
      "loss": 1.6066,
      "step": 69160
    },
    {
      "epoch": 35.18311291963377,
      "grad_norm": 38.17192459106445,
      "learning_rate": 1.4816887080366227e-05,
      "loss": 1.6404,
      "step": 69170
    },
    {
      "epoch": 35.1881993896236,
      "grad_norm": 45.42030715942383,
      "learning_rate": 1.48118006103764e-05,
      "loss": 1.6658,
      "step": 69180
    },
    {
      "epoch": 35.193285859613425,
      "grad_norm": 36.47441101074219,
      "learning_rate": 1.4806714140386572e-05,
      "loss": 1.6529,
      "step": 69190
    },
    {
      "epoch": 35.19837232960325,
      "grad_norm": 35.11872100830078,
      "learning_rate": 1.4801627670396745e-05,
      "loss": 1.7054,
      "step": 69200
    },
    {
      "epoch": 35.20345879959308,
      "grad_norm": 44.71160125732422,
      "learning_rate": 1.4796541200406918e-05,
      "loss": 1.6235,
      "step": 69210
    },
    {
      "epoch": 35.208545269582906,
      "grad_norm": 38.86326599121094,
      "learning_rate": 1.4791454730417093e-05,
      "loss": 1.6483,
      "step": 69220
    },
    {
      "epoch": 35.21363173957273,
      "grad_norm": 44.04426956176758,
      "learning_rate": 1.4786368260427263e-05,
      "loss": 1.6166,
      "step": 69230
    },
    {
      "epoch": 35.21871820956256,
      "grad_norm": 41.307350158691406,
      "learning_rate": 1.4781281790437438e-05,
      "loss": 1.7457,
      "step": 69240
    },
    {
      "epoch": 35.22380467955239,
      "grad_norm": 40.46470642089844,
      "learning_rate": 1.4776195320447611e-05,
      "loss": 1.7535,
      "step": 69250
    },
    {
      "epoch": 35.22889114954222,
      "grad_norm": 37.90481185913086,
      "learning_rate": 1.4771108850457783e-05,
      "loss": 1.7154,
      "step": 69260
    },
    {
      "epoch": 35.23397761953205,
      "grad_norm": 39.48377227783203,
      "learning_rate": 1.4766022380467956e-05,
      "loss": 1.5677,
      "step": 69270
    },
    {
      "epoch": 35.239064089521875,
      "grad_norm": 41.96367645263672,
      "learning_rate": 1.476093591047813e-05,
      "loss": 1.602,
      "step": 69280
    },
    {
      "epoch": 35.2441505595117,
      "grad_norm": 37.00951385498047,
      "learning_rate": 1.4755849440488301e-05,
      "loss": 1.7048,
      "step": 69290
    },
    {
      "epoch": 35.24923702950153,
      "grad_norm": 44.96951675415039,
      "learning_rate": 1.4750762970498474e-05,
      "loss": 1.6063,
      "step": 69300
    },
    {
      "epoch": 35.254323499491356,
      "grad_norm": 43.18570327758789,
      "learning_rate": 1.4745676500508648e-05,
      "loss": 1.6154,
      "step": 69310
    },
    {
      "epoch": 35.25940996948118,
      "grad_norm": 43.04894256591797,
      "learning_rate": 1.4740590030518819e-05,
      "loss": 1.6112,
      "step": 69320
    },
    {
      "epoch": 35.26449643947101,
      "grad_norm": 43.464622497558594,
      "learning_rate": 1.4735503560528992e-05,
      "loss": 1.665,
      "step": 69330
    },
    {
      "epoch": 35.26958290946084,
      "grad_norm": 42.85947036743164,
      "learning_rate": 1.4730417090539167e-05,
      "loss": 1.6301,
      "step": 69340
    },
    {
      "epoch": 35.274669379450664,
      "grad_norm": 46.3797607421875,
      "learning_rate": 1.472533062054934e-05,
      "loss": 1.6583,
      "step": 69350
    },
    {
      "epoch": 35.27975584944049,
      "grad_norm": 46.3231201171875,
      "learning_rate": 1.4720244150559512e-05,
      "loss": 1.6389,
      "step": 69360
    },
    {
      "epoch": 35.28484231943032,
      "grad_norm": 33.23310852050781,
      "learning_rate": 1.4715157680569686e-05,
      "loss": 1.6052,
      "step": 69370
    },
    {
      "epoch": 35.289928789420145,
      "grad_norm": 42.214622497558594,
      "learning_rate": 1.4710071210579859e-05,
      "loss": 1.675,
      "step": 69380
    },
    {
      "epoch": 35.29501525940997,
      "grad_norm": 37.89232635498047,
      "learning_rate": 1.470498474059003e-05,
      "loss": 1.5997,
      "step": 69390
    },
    {
      "epoch": 35.3001017293998,
      "grad_norm": 38.50933074951172,
      "learning_rate": 1.4699898270600204e-05,
      "loss": 1.6945,
      "step": 69400
    },
    {
      "epoch": 35.305188199389626,
      "grad_norm": 37.09083938598633,
      "learning_rate": 1.4694811800610379e-05,
      "loss": 1.6108,
      "step": 69410
    },
    {
      "epoch": 35.31027466937945,
      "grad_norm": 45.842803955078125,
      "learning_rate": 1.4689725330620549e-05,
      "loss": 1.5911,
      "step": 69420
    },
    {
      "epoch": 35.31536113936928,
      "grad_norm": 35.97726058959961,
      "learning_rate": 1.4684638860630724e-05,
      "loss": 1.5989,
      "step": 69430
    },
    {
      "epoch": 35.32044760935911,
      "grad_norm": 37.624610900878906,
      "learning_rate": 1.4679552390640897e-05,
      "loss": 1.6185,
      "step": 69440
    },
    {
      "epoch": 35.325534079348934,
      "grad_norm": 38.25499725341797,
      "learning_rate": 1.4674465920651068e-05,
      "loss": 1.6407,
      "step": 69450
    },
    {
      "epoch": 35.33062054933876,
      "grad_norm": 39.26506805419922,
      "learning_rate": 1.4669379450661242e-05,
      "loss": 1.6465,
      "step": 69460
    },
    {
      "epoch": 35.33570701932859,
      "grad_norm": 38.887081146240234,
      "learning_rate": 1.4664292980671415e-05,
      "loss": 1.6596,
      "step": 69470
    },
    {
      "epoch": 35.340793489318415,
      "grad_norm": 49.90298843383789,
      "learning_rate": 1.4659206510681588e-05,
      "loss": 1.7101,
      "step": 69480
    },
    {
      "epoch": 35.34587995930824,
      "grad_norm": 45.98070526123047,
      "learning_rate": 1.465412004069176e-05,
      "loss": 1.6702,
      "step": 69490
    },
    {
      "epoch": 35.35096642929807,
      "grad_norm": 46.322933197021484,
      "learning_rate": 1.4649033570701933e-05,
      "loss": 1.6035,
      "step": 69500
    },
    {
      "epoch": 35.356052899287896,
      "grad_norm": 41.74613952636719,
      "learning_rate": 1.4643947100712108e-05,
      "loss": 1.744,
      "step": 69510
    },
    {
      "epoch": 35.36113936927772,
      "grad_norm": 38.37361145019531,
      "learning_rate": 1.4638860630722278e-05,
      "loss": 1.6927,
      "step": 69520
    },
    {
      "epoch": 35.36622583926755,
      "grad_norm": 39.48590087890625,
      "learning_rate": 1.4633774160732453e-05,
      "loss": 1.6731,
      "step": 69530
    },
    {
      "epoch": 35.37131230925738,
      "grad_norm": 46.13682556152344,
      "learning_rate": 1.4628687690742626e-05,
      "loss": 1.7145,
      "step": 69540
    },
    {
      "epoch": 35.376398779247204,
      "grad_norm": 45.71565628051758,
      "learning_rate": 1.4623601220752798e-05,
      "loss": 1.5976,
      "step": 69550
    },
    {
      "epoch": 35.38148524923703,
      "grad_norm": 40.042762756347656,
      "learning_rate": 1.4618514750762971e-05,
      "loss": 1.5971,
      "step": 69560
    },
    {
      "epoch": 35.38657171922686,
      "grad_norm": 36.8421630859375,
      "learning_rate": 1.4613428280773144e-05,
      "loss": 1.6613,
      "step": 69570
    },
    {
      "epoch": 35.391658189216685,
      "grad_norm": 41.077762603759766,
      "learning_rate": 1.4608341810783316e-05,
      "loss": 1.5927,
      "step": 69580
    },
    {
      "epoch": 35.39674465920651,
      "grad_norm": 35.89622497558594,
      "learning_rate": 1.460325534079349e-05,
      "loss": 1.643,
      "step": 69590
    },
    {
      "epoch": 35.40183112919634,
      "grad_norm": 38.76737594604492,
      "learning_rate": 1.4598168870803664e-05,
      "loss": 1.6575,
      "step": 69600
    },
    {
      "epoch": 35.406917599186166,
      "grad_norm": 33.57174301147461,
      "learning_rate": 1.4593082400813838e-05,
      "loss": 1.6577,
      "step": 69610
    },
    {
      "epoch": 35.41200406917599,
      "grad_norm": 43.575714111328125,
      "learning_rate": 1.4587995930824009e-05,
      "loss": 1.7025,
      "step": 69620
    },
    {
      "epoch": 35.41709053916582,
      "grad_norm": 38.03117752075195,
      "learning_rate": 1.4582909460834182e-05,
      "loss": 1.6809,
      "step": 69630
    },
    {
      "epoch": 35.42217700915565,
      "grad_norm": 29.648569107055664,
      "learning_rate": 1.4577822990844356e-05,
      "loss": 1.666,
      "step": 69640
    },
    {
      "epoch": 35.42726347914547,
      "grad_norm": 43.101627349853516,
      "learning_rate": 1.4572736520854527e-05,
      "loss": 1.6381,
      "step": 69650
    },
    {
      "epoch": 35.4323499491353,
      "grad_norm": 46.461708068847656,
      "learning_rate": 1.45676500508647e-05,
      "loss": 1.5854,
      "step": 69660
    },
    {
      "epoch": 35.43743641912513,
      "grad_norm": 44.61823272705078,
      "learning_rate": 1.4562563580874874e-05,
      "loss": 1.7489,
      "step": 69670
    },
    {
      "epoch": 35.442522889114954,
      "grad_norm": 46.4610481262207,
      "learning_rate": 1.4557477110885045e-05,
      "loss": 1.6639,
      "step": 69680
    },
    {
      "epoch": 35.44760935910478,
      "grad_norm": 39.83421325683594,
      "learning_rate": 1.4552390640895219e-05,
      "loss": 1.6537,
      "step": 69690
    },
    {
      "epoch": 35.45269582909461,
      "grad_norm": 41.315460205078125,
      "learning_rate": 1.4547304170905394e-05,
      "loss": 1.6458,
      "step": 69700
    },
    {
      "epoch": 35.457782299084435,
      "grad_norm": 37.563323974609375,
      "learning_rate": 1.4542217700915564e-05,
      "loss": 1.8012,
      "step": 69710
    },
    {
      "epoch": 35.46286876907426,
      "grad_norm": 31.163917541503906,
      "learning_rate": 1.4537131230925739e-05,
      "loss": 1.5967,
      "step": 69720
    },
    {
      "epoch": 35.46795523906409,
      "grad_norm": 51.36937713623047,
      "learning_rate": 1.4532044760935912e-05,
      "loss": 1.6958,
      "step": 69730
    },
    {
      "epoch": 35.473041709053916,
      "grad_norm": 36.431243896484375,
      "learning_rate": 1.4526958290946083e-05,
      "loss": 1.7053,
      "step": 69740
    },
    {
      "epoch": 35.47812817904374,
      "grad_norm": 45.93281936645508,
      "learning_rate": 1.4521871820956257e-05,
      "loss": 1.6286,
      "step": 69750
    },
    {
      "epoch": 35.48321464903357,
      "grad_norm": 41.98479461669922,
      "learning_rate": 1.451678535096643e-05,
      "loss": 1.6066,
      "step": 69760
    },
    {
      "epoch": 35.4883011190234,
      "grad_norm": 44.620941162109375,
      "learning_rate": 1.4511698880976605e-05,
      "loss": 1.6488,
      "step": 69770
    },
    {
      "epoch": 35.493387589013224,
      "grad_norm": 53.523780822753906,
      "learning_rate": 1.4506612410986775e-05,
      "loss": 1.6292,
      "step": 69780
    },
    {
      "epoch": 35.49847405900305,
      "grad_norm": 40.25602722167969,
      "learning_rate": 1.4501525940996948e-05,
      "loss": 1.642,
      "step": 69790
    },
    {
      "epoch": 35.50356052899288,
      "grad_norm": 33.242923736572266,
      "learning_rate": 1.4496439471007123e-05,
      "loss": 1.6177,
      "step": 69800
    },
    {
      "epoch": 35.508646998982705,
      "grad_norm": 36.385986328125,
      "learning_rate": 1.4491353001017293e-05,
      "loss": 1.6399,
      "step": 69810
    },
    {
      "epoch": 35.51373346897253,
      "grad_norm": 44.46218490600586,
      "learning_rate": 1.4486266531027468e-05,
      "loss": 1.6274,
      "step": 69820
    },
    {
      "epoch": 35.51881993896236,
      "grad_norm": 34.29412078857422,
      "learning_rate": 1.4481180061037641e-05,
      "loss": 1.6603,
      "step": 69830
    },
    {
      "epoch": 35.523906408952186,
      "grad_norm": 32.59537124633789,
      "learning_rate": 1.4476093591047813e-05,
      "loss": 1.6461,
      "step": 69840
    },
    {
      "epoch": 35.52899287894201,
      "grad_norm": 36.121315002441406,
      "learning_rate": 1.4471007121057986e-05,
      "loss": 1.5841,
      "step": 69850
    },
    {
      "epoch": 35.53407934893184,
      "grad_norm": 45.93290710449219,
      "learning_rate": 1.446592065106816e-05,
      "loss": 1.5893,
      "step": 69860
    },
    {
      "epoch": 35.53916581892167,
      "grad_norm": 41.12601089477539,
      "learning_rate": 1.4460834181078331e-05,
      "loss": 1.687,
      "step": 69870
    },
    {
      "epoch": 35.544252288911494,
      "grad_norm": 41.88776779174805,
      "learning_rate": 1.4455747711088504e-05,
      "loss": 1.7547,
      "step": 69880
    },
    {
      "epoch": 35.54933875890132,
      "grad_norm": 48.032196044921875,
      "learning_rate": 1.445066124109868e-05,
      "loss": 1.5973,
      "step": 69890
    },
    {
      "epoch": 35.55442522889115,
      "grad_norm": 40.01027297973633,
      "learning_rate": 1.4445574771108853e-05,
      "loss": 1.5644,
      "step": 69900
    },
    {
      "epoch": 35.559511698880975,
      "grad_norm": 42.89093017578125,
      "learning_rate": 1.4440488301119024e-05,
      "loss": 1.5925,
      "step": 69910
    },
    {
      "epoch": 35.5645981688708,
      "grad_norm": 35.63488006591797,
      "learning_rate": 1.4435401831129197e-05,
      "loss": 1.7088,
      "step": 69920
    },
    {
      "epoch": 35.56968463886063,
      "grad_norm": 47.418155670166016,
      "learning_rate": 1.443031536113937e-05,
      "loss": 1.5854,
      "step": 69930
    },
    {
      "epoch": 35.574771108850456,
      "grad_norm": 41.63956832885742,
      "learning_rate": 1.4425228891149542e-05,
      "loss": 1.7376,
      "step": 69940
    },
    {
      "epoch": 35.57985757884028,
      "grad_norm": 37.785762786865234,
      "learning_rate": 1.4420142421159716e-05,
      "loss": 1.6209,
      "step": 69950
    },
    {
      "epoch": 35.58494404883011,
      "grad_norm": 39.80085754394531,
      "learning_rate": 1.4415055951169889e-05,
      "loss": 1.653,
      "step": 69960
    },
    {
      "epoch": 35.59003051881994,
      "grad_norm": 35.42292022705078,
      "learning_rate": 1.440996948118006e-05,
      "loss": 1.7406,
      "step": 69970
    },
    {
      "epoch": 35.595116988809764,
      "grad_norm": 47.48982238769531,
      "learning_rate": 1.4404883011190234e-05,
      "loss": 1.5749,
      "step": 69980
    },
    {
      "epoch": 35.60020345879959,
      "grad_norm": 36.99208068847656,
      "learning_rate": 1.4399796541200409e-05,
      "loss": 1.6862,
      "step": 69990
    },
    {
      "epoch": 35.60528992878942,
      "grad_norm": 41.89916229248047,
      "learning_rate": 1.4394710071210579e-05,
      "loss": 1.6404,
      "step": 70000
    },
    {
      "epoch": 35.610376398779245,
      "grad_norm": 35.01428985595703,
      "learning_rate": 1.4389623601220754e-05,
      "loss": 1.6542,
      "step": 70010
    },
    {
      "epoch": 35.61546286876907,
      "grad_norm": 48.51655960083008,
      "learning_rate": 1.4384537131230927e-05,
      "loss": 1.5787,
      "step": 70020
    },
    {
      "epoch": 35.6205493387589,
      "grad_norm": 38.270957946777344,
      "learning_rate": 1.43794506612411e-05,
      "loss": 1.6343,
      "step": 70030
    },
    {
      "epoch": 35.625635808748726,
      "grad_norm": 32.590362548828125,
      "learning_rate": 1.4374364191251272e-05,
      "loss": 1.625,
      "step": 70040
    },
    {
      "epoch": 35.63072227873855,
      "grad_norm": 41.01915740966797,
      "learning_rate": 1.4369277721261445e-05,
      "loss": 1.5868,
      "step": 70050
    },
    {
      "epoch": 35.63580874872838,
      "grad_norm": 33.20170593261719,
      "learning_rate": 1.436419125127162e-05,
      "loss": 1.5818,
      "step": 70060
    },
    {
      "epoch": 35.64089521871821,
      "grad_norm": 45.91522216796875,
      "learning_rate": 1.435910478128179e-05,
      "loss": 1.6208,
      "step": 70070
    },
    {
      "epoch": 35.645981688708034,
      "grad_norm": 41.27333450317383,
      "learning_rate": 1.4354018311291965e-05,
      "loss": 1.6994,
      "step": 70080
    },
    {
      "epoch": 35.65106815869786,
      "grad_norm": 47.94691467285156,
      "learning_rate": 1.4348931841302138e-05,
      "loss": 1.5846,
      "step": 70090
    },
    {
      "epoch": 35.65615462868769,
      "grad_norm": 38.74552917480469,
      "learning_rate": 1.434384537131231e-05,
      "loss": 1.6126,
      "step": 70100
    },
    {
      "epoch": 35.661241098677515,
      "grad_norm": 52.833194732666016,
      "learning_rate": 1.4338758901322483e-05,
      "loss": 1.6115,
      "step": 70110
    },
    {
      "epoch": 35.66632756866734,
      "grad_norm": 47.4422492980957,
      "learning_rate": 1.4333672431332656e-05,
      "loss": 1.6839,
      "step": 70120
    },
    {
      "epoch": 35.67141403865717,
      "grad_norm": 51.64756393432617,
      "learning_rate": 1.4328585961342828e-05,
      "loss": 1.5846,
      "step": 70130
    },
    {
      "epoch": 35.676500508646996,
      "grad_norm": 39.66306686401367,
      "learning_rate": 1.4323499491353001e-05,
      "loss": 1.6637,
      "step": 70140
    },
    {
      "epoch": 35.68158697863683,
      "grad_norm": 42.85317611694336,
      "learning_rate": 1.4318413021363174e-05,
      "loss": 1.6886,
      "step": 70150
    },
    {
      "epoch": 35.68667344862666,
      "grad_norm": 36.694000244140625,
      "learning_rate": 1.431332655137335e-05,
      "loss": 1.6029,
      "step": 70160
    },
    {
      "epoch": 35.691759918616484,
      "grad_norm": 49.820369720458984,
      "learning_rate": 1.430824008138352e-05,
      "loss": 1.6578,
      "step": 70170
    },
    {
      "epoch": 35.69684638860631,
      "grad_norm": 38.093746185302734,
      "learning_rate": 1.4303153611393694e-05,
      "loss": 1.6102,
      "step": 70180
    },
    {
      "epoch": 35.70193285859614,
      "grad_norm": 46.46092224121094,
      "learning_rate": 1.4298067141403868e-05,
      "loss": 1.5881,
      "step": 70190
    },
    {
      "epoch": 35.707019328585965,
      "grad_norm": 39.72966384887695,
      "learning_rate": 1.4292980671414039e-05,
      "loss": 1.6243,
      "step": 70200
    },
    {
      "epoch": 35.71210579857579,
      "grad_norm": 37.8441047668457,
      "learning_rate": 1.4287894201424212e-05,
      "loss": 1.5855,
      "step": 70210
    },
    {
      "epoch": 35.71719226856562,
      "grad_norm": 40.21946716308594,
      "learning_rate": 1.4282807731434386e-05,
      "loss": 1.5488,
      "step": 70220
    },
    {
      "epoch": 35.722278738555445,
      "grad_norm": 40.41574478149414,
      "learning_rate": 1.4277721261444557e-05,
      "loss": 1.7293,
      "step": 70230
    },
    {
      "epoch": 35.72736520854527,
      "grad_norm": 38.91071319580078,
      "learning_rate": 1.427263479145473e-05,
      "loss": 1.6302,
      "step": 70240
    },
    {
      "epoch": 35.7324516785351,
      "grad_norm": 44.562992095947266,
      "learning_rate": 1.4267548321464906e-05,
      "loss": 1.633,
      "step": 70250
    },
    {
      "epoch": 35.737538148524926,
      "grad_norm": 43.949893951416016,
      "learning_rate": 1.4262461851475075e-05,
      "loss": 1.7059,
      "step": 70260
    },
    {
      "epoch": 35.74262461851475,
      "grad_norm": 35.9266357421875,
      "learning_rate": 1.425737538148525e-05,
      "loss": 1.6633,
      "step": 70270
    },
    {
      "epoch": 35.74771108850458,
      "grad_norm": 42.08327865600586,
      "learning_rate": 1.4252288911495424e-05,
      "loss": 1.7165,
      "step": 70280
    },
    {
      "epoch": 35.75279755849441,
      "grad_norm": 46.297523498535156,
      "learning_rate": 1.4247202441505597e-05,
      "loss": 1.7567,
      "step": 70290
    },
    {
      "epoch": 35.757884028484234,
      "grad_norm": 51.65607833862305,
      "learning_rate": 1.4242115971515769e-05,
      "loss": 1.6079,
      "step": 70300
    },
    {
      "epoch": 35.76297049847406,
      "grad_norm": 38.77080535888672,
      "learning_rate": 1.4237029501525942e-05,
      "loss": 1.6365,
      "step": 70310
    },
    {
      "epoch": 35.76805696846389,
      "grad_norm": 47.166015625,
      "learning_rate": 1.4231943031536115e-05,
      "loss": 1.5552,
      "step": 70320
    },
    {
      "epoch": 35.773143438453715,
      "grad_norm": 36.973392486572266,
      "learning_rate": 1.4226856561546287e-05,
      "loss": 1.6754,
      "step": 70330
    },
    {
      "epoch": 35.77822990844354,
      "grad_norm": 38.59065628051758,
      "learning_rate": 1.422177009155646e-05,
      "loss": 1.699,
      "step": 70340
    },
    {
      "epoch": 35.78331637843337,
      "grad_norm": 38.66366958618164,
      "learning_rate": 1.4216683621566635e-05,
      "loss": 1.5603,
      "step": 70350
    },
    {
      "epoch": 35.788402848423196,
      "grad_norm": 34.76042938232422,
      "learning_rate": 1.4211597151576805e-05,
      "loss": 1.6452,
      "step": 70360
    },
    {
      "epoch": 35.79348931841302,
      "grad_norm": 38.96957015991211,
      "learning_rate": 1.420651068158698e-05,
      "loss": 1.6454,
      "step": 70370
    },
    {
      "epoch": 35.79857578840285,
      "grad_norm": 39.587581634521484,
      "learning_rate": 1.4201424211597153e-05,
      "loss": 1.6397,
      "step": 70380
    },
    {
      "epoch": 35.80366225839268,
      "grad_norm": 38.81454086303711,
      "learning_rate": 1.4196337741607325e-05,
      "loss": 1.6591,
      "step": 70390
    },
    {
      "epoch": 35.808748728382504,
      "grad_norm": 35.2063102722168,
      "learning_rate": 1.4191251271617498e-05,
      "loss": 1.6332,
      "step": 70400
    },
    {
      "epoch": 35.81383519837233,
      "grad_norm": 38.520904541015625,
      "learning_rate": 1.4186164801627671e-05,
      "loss": 1.7416,
      "step": 70410
    },
    {
      "epoch": 35.81892166836216,
      "grad_norm": 45.90047073364258,
      "learning_rate": 1.4181078331637846e-05,
      "loss": 1.7171,
      "step": 70420
    },
    {
      "epoch": 35.824008138351985,
      "grad_norm": 39.9986457824707,
      "learning_rate": 1.4175991861648016e-05,
      "loss": 1.645,
      "step": 70430
    },
    {
      "epoch": 35.82909460834181,
      "grad_norm": 40.287742614746094,
      "learning_rate": 1.417090539165819e-05,
      "loss": 1.6738,
      "step": 70440
    },
    {
      "epoch": 35.83418107833164,
      "grad_norm": 41.2918815612793,
      "learning_rate": 1.4165818921668364e-05,
      "loss": 1.6719,
      "step": 70450
    },
    {
      "epoch": 35.839267548321466,
      "grad_norm": 38.983062744140625,
      "learning_rate": 1.4160732451678534e-05,
      "loss": 1.7408,
      "step": 70460
    },
    {
      "epoch": 35.84435401831129,
      "grad_norm": 40.71845245361328,
      "learning_rate": 1.415564598168871e-05,
      "loss": 1.6239,
      "step": 70470
    },
    {
      "epoch": 35.84944048830112,
      "grad_norm": 34.066673278808594,
      "learning_rate": 1.4150559511698883e-05,
      "loss": 1.651,
      "step": 70480
    },
    {
      "epoch": 35.85452695829095,
      "grad_norm": 46.489688873291016,
      "learning_rate": 1.4145473041709054e-05,
      "loss": 1.7117,
      "step": 70490
    },
    {
      "epoch": 35.859613428280774,
      "grad_norm": 39.62286376953125,
      "learning_rate": 1.4140386571719227e-05,
      "loss": 1.6615,
      "step": 70500
    },
    {
      "epoch": 35.8646998982706,
      "grad_norm": 41.35847473144531,
      "learning_rate": 1.41353001017294e-05,
      "loss": 1.591,
      "step": 70510
    },
    {
      "epoch": 35.86978636826043,
      "grad_norm": 43.98480224609375,
      "learning_rate": 1.4130213631739572e-05,
      "loss": 1.7209,
      "step": 70520
    },
    {
      "epoch": 35.874872838250255,
      "grad_norm": 47.079288482666016,
      "learning_rate": 1.4125127161749746e-05,
      "loss": 1.7249,
      "step": 70530
    },
    {
      "epoch": 35.87995930824008,
      "grad_norm": 41.31741714477539,
      "learning_rate": 1.412004069175992e-05,
      "loss": 1.6191,
      "step": 70540
    },
    {
      "epoch": 35.88504577822991,
      "grad_norm": 46.75971221923828,
      "learning_rate": 1.411495422177009e-05,
      "loss": 1.7124,
      "step": 70550
    },
    {
      "epoch": 35.890132248219736,
      "grad_norm": 45.0808219909668,
      "learning_rate": 1.4109867751780265e-05,
      "loss": 1.6225,
      "step": 70560
    },
    {
      "epoch": 35.89521871820956,
      "grad_norm": 37.557838439941406,
      "learning_rate": 1.4104781281790439e-05,
      "loss": 1.5541,
      "step": 70570
    },
    {
      "epoch": 35.90030518819939,
      "grad_norm": 36.749664306640625,
      "learning_rate": 1.4099694811800612e-05,
      "loss": 1.6297,
      "step": 70580
    },
    {
      "epoch": 35.90539165818922,
      "grad_norm": 38.84033203125,
      "learning_rate": 1.4094608341810784e-05,
      "loss": 1.6475,
      "step": 70590
    },
    {
      "epoch": 35.910478128179044,
      "grad_norm": 39.09642028808594,
      "learning_rate": 1.4089521871820957e-05,
      "loss": 1.6806,
      "step": 70600
    },
    {
      "epoch": 35.91556459816887,
      "grad_norm": 38.102928161621094,
      "learning_rate": 1.408443540183113e-05,
      "loss": 1.6643,
      "step": 70610
    },
    {
      "epoch": 35.9206510681587,
      "grad_norm": 49.02290725708008,
      "learning_rate": 1.4079348931841302e-05,
      "loss": 1.6672,
      "step": 70620
    },
    {
      "epoch": 35.925737538148525,
      "grad_norm": 43.680580139160156,
      "learning_rate": 1.4074262461851475e-05,
      "loss": 1.7246,
      "step": 70630
    },
    {
      "epoch": 35.93082400813835,
      "grad_norm": 44.090545654296875,
      "learning_rate": 1.406917599186165e-05,
      "loss": 1.6748,
      "step": 70640
    },
    {
      "epoch": 35.93591047812818,
      "grad_norm": 49.33859634399414,
      "learning_rate": 1.406408952187182e-05,
      "loss": 1.7095,
      "step": 70650
    },
    {
      "epoch": 35.940996948118006,
      "grad_norm": 37.82427978515625,
      "learning_rate": 1.4059003051881995e-05,
      "loss": 1.654,
      "step": 70660
    },
    {
      "epoch": 35.94608341810783,
      "grad_norm": 40.19102096557617,
      "learning_rate": 1.4053916581892168e-05,
      "loss": 1.6761,
      "step": 70670
    },
    {
      "epoch": 35.95116988809766,
      "grad_norm": 43.93472671508789,
      "learning_rate": 1.404883011190234e-05,
      "loss": 1.588,
      "step": 70680
    },
    {
      "epoch": 35.95625635808749,
      "grad_norm": 52.08327865600586,
      "learning_rate": 1.4043743641912513e-05,
      "loss": 1.6197,
      "step": 70690
    },
    {
      "epoch": 35.96134282807731,
      "grad_norm": 39.60028076171875,
      "learning_rate": 1.4038657171922686e-05,
      "loss": 1.6435,
      "step": 70700
    },
    {
      "epoch": 35.96642929806714,
      "grad_norm": 33.15586471557617,
      "learning_rate": 1.4033570701932861e-05,
      "loss": 1.7522,
      "step": 70710
    },
    {
      "epoch": 35.97151576805697,
      "grad_norm": 35.38144302368164,
      "learning_rate": 1.4028484231943031e-05,
      "loss": 1.6428,
      "step": 70720
    },
    {
      "epoch": 35.976602238046794,
      "grad_norm": 41.677616119384766,
      "learning_rate": 1.4023397761953206e-05,
      "loss": 1.6014,
      "step": 70730
    },
    {
      "epoch": 35.98168870803662,
      "grad_norm": 36.6861572265625,
      "learning_rate": 1.401831129196338e-05,
      "loss": 1.729,
      "step": 70740
    },
    {
      "epoch": 35.98677517802645,
      "grad_norm": 42.1634635925293,
      "learning_rate": 1.4013224821973551e-05,
      "loss": 1.6559,
      "step": 70750
    },
    {
      "epoch": 35.991861648016275,
      "grad_norm": 39.75774002075195,
      "learning_rate": 1.4008138351983724e-05,
      "loss": 1.5981,
      "step": 70760
    },
    {
      "epoch": 35.9969481180061,
      "grad_norm": 39.44660949707031,
      "learning_rate": 1.4003051881993898e-05,
      "loss": 1.6827,
      "step": 70770
    },
    {
      "epoch": 36.0,
      "eval_loss": 4.953289031982422,
      "eval_runtime": 2.6597,
      "eval_samples_per_second": 1043.337,
      "eval_steps_per_second": 130.464,
      "step": 70776
    },
    {
      "epoch": 36.00203458799593,
      "grad_norm": 39.73049545288086,
      "learning_rate": 1.3997965412004069e-05,
      "loss": 1.6845,
      "step": 70780
    },
    {
      "epoch": 36.007121057985756,
      "grad_norm": 44.687320709228516,
      "learning_rate": 1.3992878942014242e-05,
      "loss": 1.6311,
      "step": 70790
    },
    {
      "epoch": 36.01220752797558,
      "grad_norm": 43.39567184448242,
      "learning_rate": 1.3987792472024416e-05,
      "loss": 1.6162,
      "step": 70800
    },
    {
      "epoch": 36.01729399796541,
      "grad_norm": 40.46115493774414,
      "learning_rate": 1.3982706002034587e-05,
      "loss": 1.6828,
      "step": 70810
    },
    {
      "epoch": 36.02238046795524,
      "grad_norm": 49.136837005615234,
      "learning_rate": 1.397761953204476e-05,
      "loss": 1.6667,
      "step": 70820
    },
    {
      "epoch": 36.027466937945064,
      "grad_norm": 41.13984298706055,
      "learning_rate": 1.3972533062054936e-05,
      "loss": 1.5579,
      "step": 70830
    },
    {
      "epoch": 36.03255340793489,
      "grad_norm": 49.147518157958984,
      "learning_rate": 1.3967446592065109e-05,
      "loss": 1.6747,
      "step": 70840
    },
    {
      "epoch": 36.03763987792472,
      "grad_norm": 32.24686050415039,
      "learning_rate": 1.396236012207528e-05,
      "loss": 1.7208,
      "step": 70850
    },
    {
      "epoch": 36.042726347914545,
      "grad_norm": 45.32366943359375,
      "learning_rate": 1.3957273652085454e-05,
      "loss": 1.6964,
      "step": 70860
    },
    {
      "epoch": 36.04781281790437,
      "grad_norm": 38.74018859863281,
      "learning_rate": 1.3952187182095627e-05,
      "loss": 1.5186,
      "step": 70870
    },
    {
      "epoch": 36.0528992878942,
      "grad_norm": 33.539451599121094,
      "learning_rate": 1.3947100712105799e-05,
      "loss": 1.5606,
      "step": 70880
    },
    {
      "epoch": 36.057985757884026,
      "grad_norm": 41.0234375,
      "learning_rate": 1.3942014242115972e-05,
      "loss": 1.5696,
      "step": 70890
    },
    {
      "epoch": 36.06307222787385,
      "grad_norm": 37.242218017578125,
      "learning_rate": 1.3936927772126147e-05,
      "loss": 1.5269,
      "step": 70900
    },
    {
      "epoch": 36.06815869786368,
      "grad_norm": 36.83184814453125,
      "learning_rate": 1.3931841302136317e-05,
      "loss": 1.5528,
      "step": 70910
    },
    {
      "epoch": 36.07324516785351,
      "grad_norm": 41.174957275390625,
      "learning_rate": 1.392675483214649e-05,
      "loss": 1.6444,
      "step": 70920
    },
    {
      "epoch": 36.078331637843334,
      "grad_norm": 40.372493743896484,
      "learning_rate": 1.3921668362156665e-05,
      "loss": 1.7003,
      "step": 70930
    },
    {
      "epoch": 36.08341810783316,
      "grad_norm": 46.34299850463867,
      "learning_rate": 1.3916581892166835e-05,
      "loss": 1.7194,
      "step": 70940
    },
    {
      "epoch": 36.08850457782299,
      "grad_norm": 43.71303176879883,
      "learning_rate": 1.391149542217701e-05,
      "loss": 1.6176,
      "step": 70950
    },
    {
      "epoch": 36.093591047812815,
      "grad_norm": 47.49433898925781,
      "learning_rate": 1.3906408952187183e-05,
      "loss": 1.6354,
      "step": 70960
    },
    {
      "epoch": 36.09867751780264,
      "grad_norm": 41.522464752197266,
      "learning_rate": 1.3901322482197356e-05,
      "loss": 1.6904,
      "step": 70970
    },
    {
      "epoch": 36.10376398779247,
      "grad_norm": 44.305702209472656,
      "learning_rate": 1.3896236012207528e-05,
      "loss": 1.6431,
      "step": 70980
    },
    {
      "epoch": 36.108850457782296,
      "grad_norm": 38.49235534667969,
      "learning_rate": 1.3891149542217701e-05,
      "loss": 1.5916,
      "step": 70990
    },
    {
      "epoch": 36.11393692777212,
      "grad_norm": 38.37462615966797,
      "learning_rate": 1.3886063072227876e-05,
      "loss": 1.6587,
      "step": 71000
    },
    {
      "epoch": 36.11902339776195,
      "grad_norm": 38.71110916137695,
      "learning_rate": 1.3880976602238046e-05,
      "loss": 1.6487,
      "step": 71010
    },
    {
      "epoch": 36.12410986775178,
      "grad_norm": 43.47015380859375,
      "learning_rate": 1.3875890132248221e-05,
      "loss": 1.5981,
      "step": 71020
    },
    {
      "epoch": 36.129196337741604,
      "grad_norm": 37.40078353881836,
      "learning_rate": 1.3870803662258394e-05,
      "loss": 1.5551,
      "step": 71030
    },
    {
      "epoch": 36.13428280773143,
      "grad_norm": 46.03862380981445,
      "learning_rate": 1.3865717192268566e-05,
      "loss": 1.6146,
      "step": 71040
    },
    {
      "epoch": 36.139369277721265,
      "grad_norm": 38.78623580932617,
      "learning_rate": 1.386063072227874e-05,
      "loss": 1.7127,
      "step": 71050
    },
    {
      "epoch": 36.14445574771109,
      "grad_norm": 32.48851776123047,
      "learning_rate": 1.3855544252288913e-05,
      "loss": 1.6325,
      "step": 71060
    },
    {
      "epoch": 36.14954221770092,
      "grad_norm": 39.07832336425781,
      "learning_rate": 1.3850457782299084e-05,
      "loss": 1.6109,
      "step": 71070
    },
    {
      "epoch": 36.154628687690746,
      "grad_norm": 44.503299713134766,
      "learning_rate": 1.3845371312309257e-05,
      "loss": 1.6777,
      "step": 71080
    },
    {
      "epoch": 36.15971515768057,
      "grad_norm": 43.406681060791016,
      "learning_rate": 1.384028484231943e-05,
      "loss": 1.6917,
      "step": 71090
    },
    {
      "epoch": 36.1648016276704,
      "grad_norm": 45.956207275390625,
      "learning_rate": 1.3835198372329606e-05,
      "loss": 1.6261,
      "step": 71100
    },
    {
      "epoch": 36.16988809766023,
      "grad_norm": 45.64931869506836,
      "learning_rate": 1.3830111902339776e-05,
      "loss": 1.5388,
      "step": 71110
    },
    {
      "epoch": 36.174974567650054,
      "grad_norm": 36.289066314697266,
      "learning_rate": 1.382502543234995e-05,
      "loss": 1.6103,
      "step": 71120
    },
    {
      "epoch": 36.18006103763988,
      "grad_norm": 47.30488967895508,
      "learning_rate": 1.3819938962360124e-05,
      "loss": 1.6582,
      "step": 71130
    },
    {
      "epoch": 36.18514750762971,
      "grad_norm": 49.845420837402344,
      "learning_rate": 1.3814852492370295e-05,
      "loss": 1.6162,
      "step": 71140
    },
    {
      "epoch": 36.190233977619535,
      "grad_norm": 38.14981460571289,
      "learning_rate": 1.3809766022380469e-05,
      "loss": 1.5904,
      "step": 71150
    },
    {
      "epoch": 36.19532044760936,
      "grad_norm": 42.33839797973633,
      "learning_rate": 1.3804679552390642e-05,
      "loss": 1.7166,
      "step": 71160
    },
    {
      "epoch": 36.20040691759919,
      "grad_norm": 37.09162139892578,
      "learning_rate": 1.3799593082400814e-05,
      "loss": 1.5717,
      "step": 71170
    },
    {
      "epoch": 36.205493387589016,
      "grad_norm": 52.70500183105469,
      "learning_rate": 1.3794506612410987e-05,
      "loss": 1.648,
      "step": 71180
    },
    {
      "epoch": 36.21057985757884,
      "grad_norm": 43.55546951293945,
      "learning_rate": 1.3789420142421162e-05,
      "loss": 1.6416,
      "step": 71190
    },
    {
      "epoch": 36.21566632756867,
      "grad_norm": 44.02481460571289,
      "learning_rate": 1.3784333672431332e-05,
      "loss": 1.6747,
      "step": 71200
    },
    {
      "epoch": 36.2207527975585,
      "grad_norm": 41.73788833618164,
      "learning_rate": 1.3779247202441507e-05,
      "loss": 1.6148,
      "step": 71210
    },
    {
      "epoch": 36.225839267548324,
      "grad_norm": 38.71111297607422,
      "learning_rate": 1.377416073245168e-05,
      "loss": 1.7323,
      "step": 71220
    },
    {
      "epoch": 36.23092573753815,
      "grad_norm": 38.61824417114258,
      "learning_rate": 1.3769074262461853e-05,
      "loss": 1.6306,
      "step": 71230
    },
    {
      "epoch": 36.23601220752798,
      "grad_norm": 38.86931228637695,
      "learning_rate": 1.3763987792472025e-05,
      "loss": 1.6293,
      "step": 71240
    },
    {
      "epoch": 36.241098677517805,
      "grad_norm": 41.3193473815918,
      "learning_rate": 1.3758901322482198e-05,
      "loss": 1.6509,
      "step": 71250
    },
    {
      "epoch": 36.24618514750763,
      "grad_norm": 37.0525016784668,
      "learning_rate": 1.3753814852492371e-05,
      "loss": 1.652,
      "step": 71260
    },
    {
      "epoch": 36.25127161749746,
      "grad_norm": 35.12188720703125,
      "learning_rate": 1.3748728382502543e-05,
      "loss": 1.6808,
      "step": 71270
    },
    {
      "epoch": 36.256358087487286,
      "grad_norm": 70.94646453857422,
      "learning_rate": 1.3743641912512716e-05,
      "loss": 1.6124,
      "step": 71280
    },
    {
      "epoch": 36.26144455747711,
      "grad_norm": 44.42073440551758,
      "learning_rate": 1.3738555442522891e-05,
      "loss": 1.6472,
      "step": 71290
    },
    {
      "epoch": 36.26653102746694,
      "grad_norm": 47.79201126098633,
      "learning_rate": 1.3733468972533061e-05,
      "loss": 1.6747,
      "step": 71300
    },
    {
      "epoch": 36.271617497456766,
      "grad_norm": 36.914588928222656,
      "learning_rate": 1.3728382502543236e-05,
      "loss": 1.7495,
      "step": 71310
    },
    {
      "epoch": 36.27670396744659,
      "grad_norm": 39.565704345703125,
      "learning_rate": 1.372329603255341e-05,
      "loss": 1.6804,
      "step": 71320
    },
    {
      "epoch": 36.28179043743642,
      "grad_norm": 43.752845764160156,
      "learning_rate": 1.3718209562563581e-05,
      "loss": 1.6781,
      "step": 71330
    },
    {
      "epoch": 36.28687690742625,
      "grad_norm": 45.95707321166992,
      "learning_rate": 1.3713123092573754e-05,
      "loss": 1.5435,
      "step": 71340
    },
    {
      "epoch": 36.291963377416074,
      "grad_norm": 35.97141647338867,
      "learning_rate": 1.3708036622583928e-05,
      "loss": 1.5871,
      "step": 71350
    },
    {
      "epoch": 36.2970498474059,
      "grad_norm": 44.771263122558594,
      "learning_rate": 1.3702950152594099e-05,
      "loss": 1.5866,
      "step": 71360
    },
    {
      "epoch": 36.30213631739573,
      "grad_norm": 38.205726623535156,
      "learning_rate": 1.3697863682604272e-05,
      "loss": 1.5908,
      "step": 71370
    },
    {
      "epoch": 36.307222787385555,
      "grad_norm": 36.95338439941406,
      "learning_rate": 1.3692777212614447e-05,
      "loss": 1.6379,
      "step": 71380
    },
    {
      "epoch": 36.31230925737538,
      "grad_norm": 40.06455993652344,
      "learning_rate": 1.368769074262462e-05,
      "loss": 1.6732,
      "step": 71390
    },
    {
      "epoch": 36.31739572736521,
      "grad_norm": 30.814157485961914,
      "learning_rate": 1.368260427263479e-05,
      "loss": 1.7052,
      "step": 71400
    },
    {
      "epoch": 36.322482197355036,
      "grad_norm": 34.441802978515625,
      "learning_rate": 1.3677517802644966e-05,
      "loss": 1.6611,
      "step": 71410
    },
    {
      "epoch": 36.32756866734486,
      "grad_norm": 61.024147033691406,
      "learning_rate": 1.3672431332655139e-05,
      "loss": 1.6013,
      "step": 71420
    },
    {
      "epoch": 36.33265513733469,
      "grad_norm": 42.781349182128906,
      "learning_rate": 1.366734486266531e-05,
      "loss": 1.6155,
      "step": 71430
    },
    {
      "epoch": 36.33774160732452,
      "grad_norm": 36.33604431152344,
      "learning_rate": 1.3662258392675484e-05,
      "loss": 1.6178,
      "step": 71440
    },
    {
      "epoch": 36.342828077314344,
      "grad_norm": 35.41183090209961,
      "learning_rate": 1.3657171922685657e-05,
      "loss": 1.5711,
      "step": 71450
    },
    {
      "epoch": 36.34791454730417,
      "grad_norm": 51.83332061767578,
      "learning_rate": 1.3652085452695829e-05,
      "loss": 1.6788,
      "step": 71460
    },
    {
      "epoch": 36.353001017294,
      "grad_norm": 46.12962341308594,
      "learning_rate": 1.3646998982706002e-05,
      "loss": 1.6328,
      "step": 71470
    },
    {
      "epoch": 36.358087487283825,
      "grad_norm": 42.31990432739258,
      "learning_rate": 1.3641912512716177e-05,
      "loss": 1.5884,
      "step": 71480
    },
    {
      "epoch": 36.36317395727365,
      "grad_norm": 48.2877311706543,
      "learning_rate": 1.3636826042726347e-05,
      "loss": 1.5615,
      "step": 71490
    },
    {
      "epoch": 36.36826042726348,
      "grad_norm": 44.74916458129883,
      "learning_rate": 1.3631739572736522e-05,
      "loss": 1.6741,
      "step": 71500
    },
    {
      "epoch": 36.373346897253306,
      "grad_norm": 43.268375396728516,
      "learning_rate": 1.3626653102746695e-05,
      "loss": 1.6252,
      "step": 71510
    },
    {
      "epoch": 36.37843336724313,
      "grad_norm": 47.676700592041016,
      "learning_rate": 1.3621566632756868e-05,
      "loss": 1.6425,
      "step": 71520
    },
    {
      "epoch": 36.38351983723296,
      "grad_norm": 45.14793395996094,
      "learning_rate": 1.361648016276704e-05,
      "loss": 1.6011,
      "step": 71530
    },
    {
      "epoch": 36.38860630722279,
      "grad_norm": 48.1458625793457,
      "learning_rate": 1.3611393692777213e-05,
      "loss": 1.6152,
      "step": 71540
    },
    {
      "epoch": 36.393692777212614,
      "grad_norm": 38.841827392578125,
      "learning_rate": 1.3606307222787388e-05,
      "loss": 1.6503,
      "step": 71550
    },
    {
      "epoch": 36.39877924720244,
      "grad_norm": 40.44013595581055,
      "learning_rate": 1.3601220752797558e-05,
      "loss": 1.7137,
      "step": 71560
    },
    {
      "epoch": 36.40386571719227,
      "grad_norm": 36.85496520996094,
      "learning_rate": 1.3596134282807731e-05,
      "loss": 1.686,
      "step": 71570
    },
    {
      "epoch": 36.408952187182095,
      "grad_norm": 38.14197540283203,
      "learning_rate": 1.3591047812817906e-05,
      "loss": 1.7154,
      "step": 71580
    },
    {
      "epoch": 36.41403865717192,
      "grad_norm": 50.74812316894531,
      "learning_rate": 1.3585961342828076e-05,
      "loss": 1.6206,
      "step": 71590
    },
    {
      "epoch": 36.41912512716175,
      "grad_norm": 57.17900848388672,
      "learning_rate": 1.3580874872838251e-05,
      "loss": 1.6305,
      "step": 71600
    },
    {
      "epoch": 36.424211597151576,
      "grad_norm": 37.45317840576172,
      "learning_rate": 1.3575788402848424e-05,
      "loss": 1.6815,
      "step": 71610
    },
    {
      "epoch": 36.4292980671414,
      "grad_norm": 36.342220306396484,
      "learning_rate": 1.3570701932858596e-05,
      "loss": 1.5586,
      "step": 71620
    },
    {
      "epoch": 36.43438453713123,
      "grad_norm": 38.342613220214844,
      "learning_rate": 1.356561546286877e-05,
      "loss": 1.6902,
      "step": 71630
    },
    {
      "epoch": 36.43947100712106,
      "grad_norm": 33.09157180786133,
      "learning_rate": 1.3560528992878943e-05,
      "loss": 1.5745,
      "step": 71640
    },
    {
      "epoch": 36.444557477110884,
      "grad_norm": 44.46973419189453,
      "learning_rate": 1.3555442522889118e-05,
      "loss": 1.4886,
      "step": 71650
    },
    {
      "epoch": 36.44964394710071,
      "grad_norm": 37.510929107666016,
      "learning_rate": 1.3550356052899287e-05,
      "loss": 1.6081,
      "step": 71660
    },
    {
      "epoch": 36.45473041709054,
      "grad_norm": 37.827491760253906,
      "learning_rate": 1.3545269582909462e-05,
      "loss": 1.651,
      "step": 71670
    },
    {
      "epoch": 36.459816887080365,
      "grad_norm": 44.80080032348633,
      "learning_rate": 1.3540183112919636e-05,
      "loss": 1.6225,
      "step": 71680
    },
    {
      "epoch": 36.46490335707019,
      "grad_norm": 37.0173454284668,
      "learning_rate": 1.3535096642929807e-05,
      "loss": 1.6872,
      "step": 71690
    },
    {
      "epoch": 36.46998982706002,
      "grad_norm": 41.63009262084961,
      "learning_rate": 1.353001017293998e-05,
      "loss": 1.6376,
      "step": 71700
    },
    {
      "epoch": 36.475076297049846,
      "grad_norm": 50.999855041503906,
      "learning_rate": 1.3524923702950154e-05,
      "loss": 1.6791,
      "step": 71710
    },
    {
      "epoch": 36.48016276703967,
      "grad_norm": 38.38811492919922,
      "learning_rate": 1.3519837232960325e-05,
      "loss": 1.5943,
      "step": 71720
    },
    {
      "epoch": 36.4852492370295,
      "grad_norm": 39.94442367553711,
      "learning_rate": 1.3514750762970499e-05,
      "loss": 1.5949,
      "step": 71730
    },
    {
      "epoch": 36.49033570701933,
      "grad_norm": 50.85819625854492,
      "learning_rate": 1.3509664292980672e-05,
      "loss": 1.665,
      "step": 71740
    },
    {
      "epoch": 36.495422177009154,
      "grad_norm": 36.90959548950195,
      "learning_rate": 1.3504577822990844e-05,
      "loss": 1.6887,
      "step": 71750
    },
    {
      "epoch": 36.50050864699898,
      "grad_norm": 40.99778747558594,
      "learning_rate": 1.3499491353001017e-05,
      "loss": 1.6305,
      "step": 71760
    },
    {
      "epoch": 36.50559511698881,
      "grad_norm": 42.97018814086914,
      "learning_rate": 1.3494404883011192e-05,
      "loss": 1.6207,
      "step": 71770
    },
    {
      "epoch": 36.510681586978635,
      "grad_norm": 39.051856994628906,
      "learning_rate": 1.3489318413021365e-05,
      "loss": 1.6514,
      "step": 71780
    },
    {
      "epoch": 36.51576805696846,
      "grad_norm": 37.997772216796875,
      "learning_rate": 1.3484231943031537e-05,
      "loss": 1.736,
      "step": 71790
    },
    {
      "epoch": 36.52085452695829,
      "grad_norm": 47.195037841796875,
      "learning_rate": 1.347914547304171e-05,
      "loss": 1.6595,
      "step": 71800
    },
    {
      "epoch": 36.525940996948115,
      "grad_norm": 37.7617073059082,
      "learning_rate": 1.3474059003051883e-05,
      "loss": 1.5549,
      "step": 71810
    },
    {
      "epoch": 36.53102746693794,
      "grad_norm": 41.07319259643555,
      "learning_rate": 1.3468972533062055e-05,
      "loss": 1.55,
      "step": 71820
    },
    {
      "epoch": 36.53611393692777,
      "grad_norm": 47.890960693359375,
      "learning_rate": 1.3463886063072228e-05,
      "loss": 1.5881,
      "step": 71830
    },
    {
      "epoch": 36.541200406917596,
      "grad_norm": 34.23369216918945,
      "learning_rate": 1.3458799593082403e-05,
      "loss": 1.6284,
      "step": 71840
    },
    {
      "epoch": 36.54628687690742,
      "grad_norm": 38.19130325317383,
      "learning_rate": 1.3453713123092573e-05,
      "loss": 1.7119,
      "step": 71850
    },
    {
      "epoch": 36.55137334689725,
      "grad_norm": 39.3839225769043,
      "learning_rate": 1.3448626653102748e-05,
      "loss": 1.7302,
      "step": 71860
    },
    {
      "epoch": 36.55645981688708,
      "grad_norm": 47.78285598754883,
      "learning_rate": 1.3443540183112921e-05,
      "loss": 1.601,
      "step": 71870
    },
    {
      "epoch": 36.561546286876904,
      "grad_norm": 37.425926208496094,
      "learning_rate": 1.3438453713123093e-05,
      "loss": 1.6244,
      "step": 71880
    },
    {
      "epoch": 36.56663275686673,
      "grad_norm": 40.06748580932617,
      "learning_rate": 1.3433367243133266e-05,
      "loss": 1.6396,
      "step": 71890
    },
    {
      "epoch": 36.57171922685656,
      "grad_norm": 38.815494537353516,
      "learning_rate": 1.342828077314344e-05,
      "loss": 1.6495,
      "step": 71900
    },
    {
      "epoch": 36.576805696846385,
      "grad_norm": 33.05866241455078,
      "learning_rate": 1.3423194303153613e-05,
      "loss": 1.6746,
      "step": 71910
    },
    {
      "epoch": 36.58189216683621,
      "grad_norm": 57.85558319091797,
      "learning_rate": 1.3418107833163784e-05,
      "loss": 1.6411,
      "step": 71920
    },
    {
      "epoch": 36.58697863682604,
      "grad_norm": 36.09203338623047,
      "learning_rate": 1.3413021363173958e-05,
      "loss": 1.6142,
      "step": 71930
    },
    {
      "epoch": 36.592065106815866,
      "grad_norm": 39.78847122192383,
      "learning_rate": 1.3407934893184133e-05,
      "loss": 1.5784,
      "step": 71940
    },
    {
      "epoch": 36.5971515768057,
      "grad_norm": 51.07615661621094,
      "learning_rate": 1.3402848423194302e-05,
      "loss": 1.5409,
      "step": 71950
    },
    {
      "epoch": 36.60223804679553,
      "grad_norm": 37.13230514526367,
      "learning_rate": 1.3397761953204477e-05,
      "loss": 1.6572,
      "step": 71960
    },
    {
      "epoch": 36.607324516785354,
      "grad_norm": 41.8342170715332,
      "learning_rate": 1.339267548321465e-05,
      "loss": 1.6522,
      "step": 71970
    },
    {
      "epoch": 36.61241098677518,
      "grad_norm": 44.73118591308594,
      "learning_rate": 1.3387589013224822e-05,
      "loss": 1.6774,
      "step": 71980
    },
    {
      "epoch": 36.61749745676501,
      "grad_norm": 60.80864715576172,
      "learning_rate": 1.3382502543234996e-05,
      "loss": 1.63,
      "step": 71990
    },
    {
      "epoch": 36.622583926754835,
      "grad_norm": 59.544334411621094,
      "learning_rate": 1.3377416073245169e-05,
      "loss": 1.7009,
      "step": 72000
    },
    {
      "epoch": 36.62767039674466,
      "grad_norm": 53.974525451660156,
      "learning_rate": 1.337232960325534e-05,
      "loss": 1.6077,
      "step": 72010
    },
    {
      "epoch": 36.63275686673449,
      "grad_norm": 30.820310592651367,
      "learning_rate": 1.3367243133265514e-05,
      "loss": 1.6266,
      "step": 72020
    },
    {
      "epoch": 36.637843336724316,
      "grad_norm": 35.80066680908203,
      "learning_rate": 1.3362156663275689e-05,
      "loss": 1.5381,
      "step": 72030
    },
    {
      "epoch": 36.64292980671414,
      "grad_norm": 43.857444763183594,
      "learning_rate": 1.3357070193285862e-05,
      "loss": 1.6502,
      "step": 72040
    },
    {
      "epoch": 36.64801627670397,
      "grad_norm": 46.87094497680664,
      "learning_rate": 1.3351983723296032e-05,
      "loss": 1.7015,
      "step": 72050
    },
    {
      "epoch": 36.6531027466938,
      "grad_norm": 38.04680252075195,
      "learning_rate": 1.3346897253306207e-05,
      "loss": 1.608,
      "step": 72060
    },
    {
      "epoch": 36.658189216683624,
      "grad_norm": 38.708412170410156,
      "learning_rate": 1.334181078331638e-05,
      "loss": 1.6277,
      "step": 72070
    },
    {
      "epoch": 36.66327568667345,
      "grad_norm": 46.17795944213867,
      "learning_rate": 1.3336724313326552e-05,
      "loss": 1.6813,
      "step": 72080
    },
    {
      "epoch": 36.66836215666328,
      "grad_norm": 37.65537643432617,
      "learning_rate": 1.3331637843336725e-05,
      "loss": 1.6397,
      "step": 72090
    },
    {
      "epoch": 36.673448626653105,
      "grad_norm": 45.487239837646484,
      "learning_rate": 1.3326551373346898e-05,
      "loss": 1.6008,
      "step": 72100
    },
    {
      "epoch": 36.67853509664293,
      "grad_norm": 33.65102005004883,
      "learning_rate": 1.332146490335707e-05,
      "loss": 1.6493,
      "step": 72110
    },
    {
      "epoch": 36.68362156663276,
      "grad_norm": 40.24658966064453,
      "learning_rate": 1.3316378433367243e-05,
      "loss": 1.6727,
      "step": 72120
    },
    {
      "epoch": 36.688708036622586,
      "grad_norm": 62.882808685302734,
      "learning_rate": 1.3311291963377418e-05,
      "loss": 1.6474,
      "step": 72130
    },
    {
      "epoch": 36.69379450661241,
      "grad_norm": 39.914894104003906,
      "learning_rate": 1.3306205493387588e-05,
      "loss": 1.7016,
      "step": 72140
    },
    {
      "epoch": 36.69888097660224,
      "grad_norm": 43.10697555541992,
      "learning_rate": 1.3301119023397763e-05,
      "loss": 1.6028,
      "step": 72150
    },
    {
      "epoch": 36.70396744659207,
      "grad_norm": 58.851341247558594,
      "learning_rate": 1.3296032553407936e-05,
      "loss": 1.6165,
      "step": 72160
    },
    {
      "epoch": 36.709053916581894,
      "grad_norm": 40.00071716308594,
      "learning_rate": 1.3290946083418108e-05,
      "loss": 1.6576,
      "step": 72170
    },
    {
      "epoch": 36.71414038657172,
      "grad_norm": 36.174312591552734,
      "learning_rate": 1.3285859613428281e-05,
      "loss": 1.6033,
      "step": 72180
    },
    {
      "epoch": 36.71922685656155,
      "grad_norm": 40.76277542114258,
      "learning_rate": 1.3280773143438454e-05,
      "loss": 1.6139,
      "step": 72190
    },
    {
      "epoch": 36.724313326551375,
      "grad_norm": 40.6497688293457,
      "learning_rate": 1.3275686673448628e-05,
      "loss": 1.6841,
      "step": 72200
    },
    {
      "epoch": 36.7293997965412,
      "grad_norm": 44.1322021484375,
      "learning_rate": 1.32706002034588e-05,
      "loss": 1.613,
      "step": 72210
    },
    {
      "epoch": 36.73448626653103,
      "grad_norm": 41.49617004394531,
      "learning_rate": 1.3265513733468973e-05,
      "loss": 1.534,
      "step": 72220
    },
    {
      "epoch": 36.739572736520856,
      "grad_norm": 62.06939697265625,
      "learning_rate": 1.3260427263479148e-05,
      "loss": 1.6172,
      "step": 72230
    },
    {
      "epoch": 36.74465920651068,
      "grad_norm": 36.71087646484375,
      "learning_rate": 1.3255340793489317e-05,
      "loss": 1.5954,
      "step": 72240
    },
    {
      "epoch": 36.74974567650051,
      "grad_norm": 49.81232452392578,
      "learning_rate": 1.3250254323499492e-05,
      "loss": 1.7186,
      "step": 72250
    },
    {
      "epoch": 36.75483214649034,
      "grad_norm": 42.03662109375,
      "learning_rate": 1.3245167853509666e-05,
      "loss": 1.7015,
      "step": 72260
    },
    {
      "epoch": 36.759918616480164,
      "grad_norm": 33.92269515991211,
      "learning_rate": 1.3240081383519837e-05,
      "loss": 1.6066,
      "step": 72270
    },
    {
      "epoch": 36.76500508646999,
      "grad_norm": 37.51481628417969,
      "learning_rate": 1.323499491353001e-05,
      "loss": 1.6835,
      "step": 72280
    },
    {
      "epoch": 36.77009155645982,
      "grad_norm": 37.78004455566406,
      "learning_rate": 1.3229908443540184e-05,
      "loss": 1.6656,
      "step": 72290
    },
    {
      "epoch": 36.775178026449645,
      "grad_norm": 36.44202423095703,
      "learning_rate": 1.3224821973550355e-05,
      "loss": 1.649,
      "step": 72300
    },
    {
      "epoch": 36.78026449643947,
      "grad_norm": 42.62039566040039,
      "learning_rate": 1.3219735503560529e-05,
      "loss": 1.5544,
      "step": 72310
    },
    {
      "epoch": 36.7853509664293,
      "grad_norm": 37.46831130981445,
      "learning_rate": 1.3214649033570704e-05,
      "loss": 1.6769,
      "step": 72320
    },
    {
      "epoch": 36.790437436419126,
      "grad_norm": 37.370582580566406,
      "learning_rate": 1.3209562563580877e-05,
      "loss": 1.6334,
      "step": 72330
    },
    {
      "epoch": 36.79552390640895,
      "grad_norm": 45.35578918457031,
      "learning_rate": 1.3204476093591049e-05,
      "loss": 1.5625,
      "step": 72340
    },
    {
      "epoch": 36.80061037639878,
      "grad_norm": 35.32511520385742,
      "learning_rate": 1.3199389623601222e-05,
      "loss": 1.6185,
      "step": 72350
    },
    {
      "epoch": 36.80569684638861,
      "grad_norm": 42.30464553833008,
      "learning_rate": 1.3194303153611395e-05,
      "loss": 1.6195,
      "step": 72360
    },
    {
      "epoch": 36.81078331637843,
      "grad_norm": 31.084714889526367,
      "learning_rate": 1.3189216683621567e-05,
      "loss": 1.5827,
      "step": 72370
    },
    {
      "epoch": 36.81586978636826,
      "grad_norm": 35.346221923828125,
      "learning_rate": 1.318413021363174e-05,
      "loss": 1.6343,
      "step": 72380
    },
    {
      "epoch": 36.82095625635809,
      "grad_norm": 42.014102935791016,
      "learning_rate": 1.3179043743641913e-05,
      "loss": 1.5901,
      "step": 72390
    },
    {
      "epoch": 36.826042726347914,
      "grad_norm": 40.448211669921875,
      "learning_rate": 1.3173957273652085e-05,
      "loss": 1.5848,
      "step": 72400
    },
    {
      "epoch": 36.83112919633774,
      "grad_norm": 41.154396057128906,
      "learning_rate": 1.3168870803662258e-05,
      "loss": 1.6405,
      "step": 72410
    },
    {
      "epoch": 36.83621566632757,
      "grad_norm": 37.253543853759766,
      "learning_rate": 1.3163784333672433e-05,
      "loss": 1.5549,
      "step": 72420
    },
    {
      "epoch": 36.841302136317395,
      "grad_norm": 37.049827575683594,
      "learning_rate": 1.3158697863682603e-05,
      "loss": 1.6673,
      "step": 72430
    },
    {
      "epoch": 36.84638860630722,
      "grad_norm": 48.357627868652344,
      "learning_rate": 1.3153611393692778e-05,
      "loss": 1.6805,
      "step": 72440
    },
    {
      "epoch": 36.85147507629705,
      "grad_norm": 41.6434211730957,
      "learning_rate": 1.3148524923702951e-05,
      "loss": 1.6109,
      "step": 72450
    },
    {
      "epoch": 36.856561546286876,
      "grad_norm": 33.25988006591797,
      "learning_rate": 1.3143438453713125e-05,
      "loss": 1.644,
      "step": 72460
    },
    {
      "epoch": 36.8616480162767,
      "grad_norm": 42.96487808227539,
      "learning_rate": 1.3138351983723296e-05,
      "loss": 1.5947,
      "step": 72470
    },
    {
      "epoch": 36.86673448626653,
      "grad_norm": 41.960693359375,
      "learning_rate": 1.313326551373347e-05,
      "loss": 1.6326,
      "step": 72480
    },
    {
      "epoch": 36.87182095625636,
      "grad_norm": 39.60935592651367,
      "learning_rate": 1.3128179043743644e-05,
      "loss": 1.6632,
      "step": 72490
    },
    {
      "epoch": 36.876907426246184,
      "grad_norm": 45.309940338134766,
      "learning_rate": 1.3123092573753814e-05,
      "loss": 1.5851,
      "step": 72500
    },
    {
      "epoch": 36.88199389623601,
      "grad_norm": 37.82000732421875,
      "learning_rate": 1.311800610376399e-05,
      "loss": 1.5907,
      "step": 72510
    },
    {
      "epoch": 36.88708036622584,
      "grad_norm": 56.91529846191406,
      "learning_rate": 1.3112919633774163e-05,
      "loss": 1.6421,
      "step": 72520
    },
    {
      "epoch": 36.892166836215665,
      "grad_norm": 46.15895462036133,
      "learning_rate": 1.3107833163784332e-05,
      "loss": 1.6765,
      "step": 72530
    },
    {
      "epoch": 36.89725330620549,
      "grad_norm": 39.6531867980957,
      "learning_rate": 1.3102746693794507e-05,
      "loss": 1.6129,
      "step": 72540
    },
    {
      "epoch": 36.90233977619532,
      "grad_norm": 43.47589874267578,
      "learning_rate": 1.309766022380468e-05,
      "loss": 1.5971,
      "step": 72550
    },
    {
      "epoch": 36.907426246185146,
      "grad_norm": 39.931121826171875,
      "learning_rate": 1.3092573753814852e-05,
      "loss": 1.5442,
      "step": 72560
    },
    {
      "epoch": 36.91251271617497,
      "grad_norm": 50.21316909790039,
      "learning_rate": 1.3087487283825026e-05,
      "loss": 1.5893,
      "step": 72570
    },
    {
      "epoch": 36.9175991861648,
      "grad_norm": 39.99455261230469,
      "learning_rate": 1.3082400813835199e-05,
      "loss": 1.6163,
      "step": 72580
    },
    {
      "epoch": 36.92268565615463,
      "grad_norm": 34.46239471435547,
      "learning_rate": 1.3077314343845374e-05,
      "loss": 1.6803,
      "step": 72590
    },
    {
      "epoch": 36.927772126144454,
      "grad_norm": 33.70522689819336,
      "learning_rate": 1.3072227873855544e-05,
      "loss": 1.5894,
      "step": 72600
    },
    {
      "epoch": 36.93285859613428,
      "grad_norm": 36.332820892333984,
      "learning_rate": 1.3067141403865719e-05,
      "loss": 1.6675,
      "step": 72610
    },
    {
      "epoch": 36.93794506612411,
      "grad_norm": 42.917537689208984,
      "learning_rate": 1.3062054933875892e-05,
      "loss": 1.5812,
      "step": 72620
    },
    {
      "epoch": 36.943031536113935,
      "grad_norm": 38.6495475769043,
      "learning_rate": 1.3056968463886064e-05,
      "loss": 1.5789,
      "step": 72630
    },
    {
      "epoch": 36.94811800610376,
      "grad_norm": 38.120574951171875,
      "learning_rate": 1.3051881993896237e-05,
      "loss": 1.6745,
      "step": 72640
    },
    {
      "epoch": 36.95320447609359,
      "grad_norm": 33.946598052978516,
      "learning_rate": 1.304679552390641e-05,
      "loss": 1.5462,
      "step": 72650
    },
    {
      "epoch": 36.958290946083416,
      "grad_norm": 52.446380615234375,
      "learning_rate": 1.3041709053916582e-05,
      "loss": 1.7039,
      "step": 72660
    },
    {
      "epoch": 36.96337741607324,
      "grad_norm": 35.104068756103516,
      "learning_rate": 1.3036622583926755e-05,
      "loss": 1.5996,
      "step": 72670
    },
    {
      "epoch": 36.96846388606307,
      "grad_norm": 39.28935623168945,
      "learning_rate": 1.3031536113936928e-05,
      "loss": 1.5211,
      "step": 72680
    },
    {
      "epoch": 36.9735503560529,
      "grad_norm": 36.54427719116211,
      "learning_rate": 1.30264496439471e-05,
      "loss": 1.738,
      "step": 72690
    },
    {
      "epoch": 36.978636826042724,
      "grad_norm": 54.164398193359375,
      "learning_rate": 1.3021363173957273e-05,
      "loss": 1.6024,
      "step": 72700
    },
    {
      "epoch": 36.98372329603255,
      "grad_norm": 38.819950103759766,
      "learning_rate": 1.3016276703967448e-05,
      "loss": 1.7318,
      "step": 72710
    },
    {
      "epoch": 36.98880976602238,
      "grad_norm": 45.385581970214844,
      "learning_rate": 1.3011190233977621e-05,
      "loss": 1.5993,
      "step": 72720
    },
    {
      "epoch": 36.993896236012205,
      "grad_norm": 34.83588790893555,
      "learning_rate": 1.3006103763987793e-05,
      "loss": 1.6261,
      "step": 72730
    },
    {
      "epoch": 36.99898270600203,
      "grad_norm": 36.8228759765625,
      "learning_rate": 1.3001017293997966e-05,
      "loss": 1.6645,
      "step": 72740
    },
    {
      "epoch": 37.0,
      "eval_loss": 4.967173099517822,
      "eval_runtime": 2.6762,
      "eval_samples_per_second": 1036.927,
      "eval_steps_per_second": 129.663,
      "step": 72742
    },
    {
      "epoch": 37.00406917599186,
      "grad_norm": 36.1478157043457,
      "learning_rate": 1.299593082400814e-05,
      "loss": 1.6161,
      "step": 72750
    },
    {
      "epoch": 37.009155645981686,
      "grad_norm": 40.50597381591797,
      "learning_rate": 1.2990844354018311e-05,
      "loss": 1.6859,
      "step": 72760
    },
    {
      "epoch": 37.01424211597151,
      "grad_norm": 66.90547943115234,
      "learning_rate": 1.2985757884028484e-05,
      "loss": 1.703,
      "step": 72770
    },
    {
      "epoch": 37.01932858596134,
      "grad_norm": 42.906009674072266,
      "learning_rate": 1.298067141403866e-05,
      "loss": 1.5499,
      "step": 72780
    },
    {
      "epoch": 37.02441505595117,
      "grad_norm": 40.114234924316406,
      "learning_rate": 1.297558494404883e-05,
      "loss": 1.6379,
      "step": 72790
    },
    {
      "epoch": 37.029501525940994,
      "grad_norm": 39.87284469604492,
      "learning_rate": 1.2970498474059004e-05,
      "loss": 1.6207,
      "step": 72800
    },
    {
      "epoch": 37.03458799593082,
      "grad_norm": 40.363380432128906,
      "learning_rate": 1.2965412004069178e-05,
      "loss": 1.5107,
      "step": 72810
    },
    {
      "epoch": 37.03967446592065,
      "grad_norm": 33.222782135009766,
      "learning_rate": 1.2960325534079349e-05,
      "loss": 1.5866,
      "step": 72820
    },
    {
      "epoch": 37.044760935910475,
      "grad_norm": 45.24863052368164,
      "learning_rate": 1.2955239064089522e-05,
      "loss": 1.6612,
      "step": 72830
    },
    {
      "epoch": 37.04984740590031,
      "grad_norm": 45.37287139892578,
      "learning_rate": 1.2950152594099696e-05,
      "loss": 1.6628,
      "step": 72840
    },
    {
      "epoch": 37.054933875890136,
      "grad_norm": 33.82986831665039,
      "learning_rate": 1.2945066124109869e-05,
      "loss": 1.6678,
      "step": 72850
    },
    {
      "epoch": 37.06002034587996,
      "grad_norm": 29.278764724731445,
      "learning_rate": 1.293997965412004e-05,
      "loss": 1.5633,
      "step": 72860
    },
    {
      "epoch": 37.06510681586979,
      "grad_norm": 35.931522369384766,
      "learning_rate": 1.2934893184130214e-05,
      "loss": 1.61,
      "step": 72870
    },
    {
      "epoch": 37.07019328585962,
      "grad_norm": 35.01615524291992,
      "learning_rate": 1.2929806714140389e-05,
      "loss": 1.6102,
      "step": 72880
    },
    {
      "epoch": 37.075279755849444,
      "grad_norm": 45.01306915283203,
      "learning_rate": 1.2924720244150559e-05,
      "loss": 1.6617,
      "step": 72890
    },
    {
      "epoch": 37.08036622583927,
      "grad_norm": 41.6917839050293,
      "learning_rate": 1.2919633774160734e-05,
      "loss": 1.5889,
      "step": 72900
    },
    {
      "epoch": 37.0854526958291,
      "grad_norm": 38.49075698852539,
      "learning_rate": 1.2914547304170907e-05,
      "loss": 1.6329,
      "step": 72910
    },
    {
      "epoch": 37.090539165818925,
      "grad_norm": 39.861167907714844,
      "learning_rate": 1.2909460834181079e-05,
      "loss": 1.601,
      "step": 72920
    },
    {
      "epoch": 37.09562563580875,
      "grad_norm": 51.00206756591797,
      "learning_rate": 1.2904374364191252e-05,
      "loss": 1.6526,
      "step": 72930
    },
    {
      "epoch": 37.10071210579858,
      "grad_norm": 38.92417526245117,
      "learning_rate": 1.2899287894201425e-05,
      "loss": 1.6454,
      "step": 72940
    },
    {
      "epoch": 37.105798575788405,
      "grad_norm": 40.368106842041016,
      "learning_rate": 1.2894201424211597e-05,
      "loss": 1.5955,
      "step": 72950
    },
    {
      "epoch": 37.11088504577823,
      "grad_norm": 36.230987548828125,
      "learning_rate": 1.288911495422177e-05,
      "loss": 1.5825,
      "step": 72960
    },
    {
      "epoch": 37.11597151576806,
      "grad_norm": 34.03650665283203,
      "learning_rate": 1.2884028484231945e-05,
      "loss": 1.546,
      "step": 72970
    },
    {
      "epoch": 37.121057985757886,
      "grad_norm": 45.70956802368164,
      "learning_rate": 1.2878942014242115e-05,
      "loss": 1.5552,
      "step": 72980
    },
    {
      "epoch": 37.12614445574771,
      "grad_norm": 41.48259353637695,
      "learning_rate": 1.287385554425229e-05,
      "loss": 1.6577,
      "step": 72990
    },
    {
      "epoch": 37.13123092573754,
      "grad_norm": 44.48984146118164,
      "learning_rate": 1.2868769074262463e-05,
      "loss": 1.5329,
      "step": 73000
    },
    {
      "epoch": 37.13631739572737,
      "grad_norm": 50.47970199584961,
      "learning_rate": 1.2863682604272636e-05,
      "loss": 1.6153,
      "step": 73010
    },
    {
      "epoch": 37.141403865717194,
      "grad_norm": 37.139488220214844,
      "learning_rate": 1.2858596134282808e-05,
      "loss": 1.6114,
      "step": 73020
    },
    {
      "epoch": 37.14649033570702,
      "grad_norm": 64.03185272216797,
      "learning_rate": 1.2853509664292981e-05,
      "loss": 1.6905,
      "step": 73030
    },
    {
      "epoch": 37.15157680569685,
      "grad_norm": 51.59733963012695,
      "learning_rate": 1.2848423194303155e-05,
      "loss": 1.6423,
      "step": 73040
    },
    {
      "epoch": 37.156663275686675,
      "grad_norm": 41.37636184692383,
      "learning_rate": 1.2843336724313326e-05,
      "loss": 1.6704,
      "step": 73050
    },
    {
      "epoch": 37.1617497456765,
      "grad_norm": 45.01645278930664,
      "learning_rate": 1.28382502543235e-05,
      "loss": 1.6089,
      "step": 73060
    },
    {
      "epoch": 37.16683621566633,
      "grad_norm": 54.297420501708984,
      "learning_rate": 1.2833163784333674e-05,
      "loss": 1.6427,
      "step": 73070
    },
    {
      "epoch": 37.171922685656156,
      "grad_norm": 37.88594055175781,
      "learning_rate": 1.2828077314343844e-05,
      "loss": 1.6822,
      "step": 73080
    },
    {
      "epoch": 37.17700915564598,
      "grad_norm": 41.661930084228516,
      "learning_rate": 1.282299084435402e-05,
      "loss": 1.6074,
      "step": 73090
    },
    {
      "epoch": 37.18209562563581,
      "grad_norm": 40.41027069091797,
      "learning_rate": 1.2817904374364193e-05,
      "loss": 1.483,
      "step": 73100
    },
    {
      "epoch": 37.18718209562564,
      "grad_norm": 38.2757453918457,
      "learning_rate": 1.2812817904374364e-05,
      "loss": 1.5664,
      "step": 73110
    },
    {
      "epoch": 37.192268565615464,
      "grad_norm": 44.18657684326172,
      "learning_rate": 1.2807731434384537e-05,
      "loss": 1.6998,
      "step": 73120
    },
    {
      "epoch": 37.19735503560529,
      "grad_norm": 38.36994552612305,
      "learning_rate": 1.280264496439471e-05,
      "loss": 1.6238,
      "step": 73130
    },
    {
      "epoch": 37.20244150559512,
      "grad_norm": 43.582462310791016,
      "learning_rate": 1.2797558494404886e-05,
      "loss": 1.6973,
      "step": 73140
    },
    {
      "epoch": 37.207527975584945,
      "grad_norm": 36.87731170654297,
      "learning_rate": 1.2792472024415056e-05,
      "loss": 1.573,
      "step": 73150
    },
    {
      "epoch": 37.21261444557477,
      "grad_norm": 41.544864654541016,
      "learning_rate": 1.278738555442523e-05,
      "loss": 1.6225,
      "step": 73160
    },
    {
      "epoch": 37.2177009155646,
      "grad_norm": 41.61436080932617,
      "learning_rate": 1.2782299084435404e-05,
      "loss": 1.6813,
      "step": 73170
    },
    {
      "epoch": 37.222787385554426,
      "grad_norm": 42.21348571777344,
      "learning_rate": 1.2777212614445574e-05,
      "loss": 1.6897,
      "step": 73180
    },
    {
      "epoch": 37.22787385554425,
      "grad_norm": 36.889808654785156,
      "learning_rate": 1.2772126144455749e-05,
      "loss": 1.5735,
      "step": 73190
    },
    {
      "epoch": 37.23296032553408,
      "grad_norm": 40.88454055786133,
      "learning_rate": 1.2767039674465922e-05,
      "loss": 1.6613,
      "step": 73200
    },
    {
      "epoch": 37.23804679552391,
      "grad_norm": 47.51590347290039,
      "learning_rate": 1.2761953204476094e-05,
      "loss": 1.6241,
      "step": 73210
    },
    {
      "epoch": 37.243133265513734,
      "grad_norm": 42.01093292236328,
      "learning_rate": 1.2756866734486267e-05,
      "loss": 1.5366,
      "step": 73220
    },
    {
      "epoch": 37.24821973550356,
      "grad_norm": 34.7319221496582,
      "learning_rate": 1.275178026449644e-05,
      "loss": 1.5524,
      "step": 73230
    },
    {
      "epoch": 37.25330620549339,
      "grad_norm": 39.25798034667969,
      "learning_rate": 1.2746693794506612e-05,
      "loss": 1.6174,
      "step": 73240
    },
    {
      "epoch": 37.258392675483215,
      "grad_norm": 38.33924102783203,
      "learning_rate": 1.2741607324516785e-05,
      "loss": 1.6115,
      "step": 73250
    },
    {
      "epoch": 37.26347914547304,
      "grad_norm": 36.90431213378906,
      "learning_rate": 1.273652085452696e-05,
      "loss": 1.5544,
      "step": 73260
    },
    {
      "epoch": 37.26856561546287,
      "grad_norm": 41.02583312988281,
      "learning_rate": 1.2731434384537133e-05,
      "loss": 1.6796,
      "step": 73270
    },
    {
      "epoch": 37.273652085452696,
      "grad_norm": 46.96139144897461,
      "learning_rate": 1.2726347914547305e-05,
      "loss": 1.6416,
      "step": 73280
    },
    {
      "epoch": 37.27873855544252,
      "grad_norm": 47.198978424072266,
      "learning_rate": 1.2721261444557478e-05,
      "loss": 1.5869,
      "step": 73290
    },
    {
      "epoch": 37.28382502543235,
      "grad_norm": 44.56840515136719,
      "learning_rate": 1.2716174974567651e-05,
      "loss": 1.5991,
      "step": 73300
    },
    {
      "epoch": 37.28891149542218,
      "grad_norm": 43.60308074951172,
      "learning_rate": 1.2711088504577823e-05,
      "loss": 1.5248,
      "step": 73310
    },
    {
      "epoch": 37.293997965412004,
      "grad_norm": 40.645206451416016,
      "learning_rate": 1.2706002034587996e-05,
      "loss": 1.5399,
      "step": 73320
    },
    {
      "epoch": 37.29908443540183,
      "grad_norm": 38.12184524536133,
      "learning_rate": 1.270091556459817e-05,
      "loss": 1.6145,
      "step": 73330
    },
    {
      "epoch": 37.30417090539166,
      "grad_norm": 39.2384033203125,
      "learning_rate": 1.2695829094608341e-05,
      "loss": 1.5858,
      "step": 73340
    },
    {
      "epoch": 37.309257375381485,
      "grad_norm": 35.04637145996094,
      "learning_rate": 1.2690742624618514e-05,
      "loss": 1.6065,
      "step": 73350
    },
    {
      "epoch": 37.31434384537131,
      "grad_norm": 40.49055099487305,
      "learning_rate": 1.268565615462869e-05,
      "loss": 1.5958,
      "step": 73360
    },
    {
      "epoch": 37.31943031536114,
      "grad_norm": 44.78056335449219,
      "learning_rate": 1.268056968463886e-05,
      "loss": 1.5803,
      "step": 73370
    },
    {
      "epoch": 37.324516785350966,
      "grad_norm": 43.46205139160156,
      "learning_rate": 1.2675483214649034e-05,
      "loss": 1.6015,
      "step": 73380
    },
    {
      "epoch": 37.32960325534079,
      "grad_norm": 38.490638732910156,
      "learning_rate": 1.2670396744659208e-05,
      "loss": 1.5891,
      "step": 73390
    },
    {
      "epoch": 37.33468972533062,
      "grad_norm": 42.898502349853516,
      "learning_rate": 1.266531027466938e-05,
      "loss": 1.6167,
      "step": 73400
    },
    {
      "epoch": 37.33977619532045,
      "grad_norm": 32.16356658935547,
      "learning_rate": 1.2660223804679552e-05,
      "loss": 1.5028,
      "step": 73410
    },
    {
      "epoch": 37.34486266531027,
      "grad_norm": 33.58768844604492,
      "learning_rate": 1.2655137334689726e-05,
      "loss": 1.6595,
      "step": 73420
    },
    {
      "epoch": 37.3499491353001,
      "grad_norm": 39.08272933959961,
      "learning_rate": 1.26500508646999e-05,
      "loss": 1.5868,
      "step": 73430
    },
    {
      "epoch": 37.35503560528993,
      "grad_norm": 35.16404724121094,
      "learning_rate": 1.264496439471007e-05,
      "loss": 1.5066,
      "step": 73440
    },
    {
      "epoch": 37.360122075279754,
      "grad_norm": 49.780879974365234,
      "learning_rate": 1.2639877924720246e-05,
      "loss": 1.5655,
      "step": 73450
    },
    {
      "epoch": 37.36520854526958,
      "grad_norm": 48.28700256347656,
      "learning_rate": 1.2634791454730419e-05,
      "loss": 1.5921,
      "step": 73460
    },
    {
      "epoch": 37.37029501525941,
      "grad_norm": 41.63703918457031,
      "learning_rate": 1.262970498474059e-05,
      "loss": 1.6045,
      "step": 73470
    },
    {
      "epoch": 37.375381485249235,
      "grad_norm": 46.622440338134766,
      "learning_rate": 1.2624618514750764e-05,
      "loss": 1.5254,
      "step": 73480
    },
    {
      "epoch": 37.38046795523906,
      "grad_norm": 47.55093765258789,
      "learning_rate": 1.2619532044760937e-05,
      "loss": 1.6158,
      "step": 73490
    },
    {
      "epoch": 37.38555442522889,
      "grad_norm": 36.68797302246094,
      "learning_rate": 1.2614445574771109e-05,
      "loss": 1.6969,
      "step": 73500
    },
    {
      "epoch": 37.390640895218716,
      "grad_norm": 51.13479232788086,
      "learning_rate": 1.2609359104781282e-05,
      "loss": 1.6346,
      "step": 73510
    },
    {
      "epoch": 37.39572736520854,
      "grad_norm": 43.02939224243164,
      "learning_rate": 1.2604272634791455e-05,
      "loss": 1.7279,
      "step": 73520
    },
    {
      "epoch": 37.40081383519837,
      "grad_norm": 41.4635009765625,
      "learning_rate": 1.259918616480163e-05,
      "loss": 1.6082,
      "step": 73530
    },
    {
      "epoch": 37.4059003051882,
      "grad_norm": 40.03154373168945,
      "learning_rate": 1.25940996948118e-05,
      "loss": 1.6804,
      "step": 73540
    },
    {
      "epoch": 37.410986775178024,
      "grad_norm": 41.66228103637695,
      "learning_rate": 1.2589013224821975e-05,
      "loss": 1.5583,
      "step": 73550
    },
    {
      "epoch": 37.41607324516785,
      "grad_norm": 35.98115158081055,
      "learning_rate": 1.2583926754832148e-05,
      "loss": 1.5909,
      "step": 73560
    },
    {
      "epoch": 37.42115971515768,
      "grad_norm": 44.60823440551758,
      "learning_rate": 1.257884028484232e-05,
      "loss": 1.6302,
      "step": 73570
    },
    {
      "epoch": 37.426246185147505,
      "grad_norm": 36.959930419921875,
      "learning_rate": 1.2573753814852493e-05,
      "loss": 1.6136,
      "step": 73580
    },
    {
      "epoch": 37.43133265513733,
      "grad_norm": 38.99269485473633,
      "learning_rate": 1.2568667344862666e-05,
      "loss": 1.5602,
      "step": 73590
    },
    {
      "epoch": 37.43641912512716,
      "grad_norm": 37.296295166015625,
      "learning_rate": 1.2563580874872838e-05,
      "loss": 1.5945,
      "step": 73600
    },
    {
      "epoch": 37.441505595116986,
      "grad_norm": 45.62160873413086,
      "learning_rate": 1.2558494404883011e-05,
      "loss": 1.6171,
      "step": 73610
    },
    {
      "epoch": 37.44659206510681,
      "grad_norm": 49.16944885253906,
      "learning_rate": 1.2553407934893186e-05,
      "loss": 1.6314,
      "step": 73620
    },
    {
      "epoch": 37.45167853509664,
      "grad_norm": 29.944448471069336,
      "learning_rate": 1.2548321464903356e-05,
      "loss": 1.6301,
      "step": 73630
    },
    {
      "epoch": 37.45676500508647,
      "grad_norm": 39.865989685058594,
      "learning_rate": 1.2543234994913531e-05,
      "loss": 1.6448,
      "step": 73640
    },
    {
      "epoch": 37.461851475076294,
      "grad_norm": 44.895538330078125,
      "learning_rate": 1.2538148524923704e-05,
      "loss": 1.6352,
      "step": 73650
    },
    {
      "epoch": 37.46693794506612,
      "grad_norm": 32.74005889892578,
      "learning_rate": 1.2533062054933878e-05,
      "loss": 1.5077,
      "step": 73660
    },
    {
      "epoch": 37.47202441505595,
      "grad_norm": 46.09595489501953,
      "learning_rate": 1.252797558494405e-05,
      "loss": 1.6136,
      "step": 73670
    },
    {
      "epoch": 37.477110885045775,
      "grad_norm": 37.96636199951172,
      "learning_rate": 1.2522889114954223e-05,
      "loss": 1.5982,
      "step": 73680
    },
    {
      "epoch": 37.4821973550356,
      "grad_norm": 38.180599212646484,
      "learning_rate": 1.2517802644964396e-05,
      "loss": 1.6048,
      "step": 73690
    },
    {
      "epoch": 37.48728382502543,
      "grad_norm": 43.839508056640625,
      "learning_rate": 1.2512716174974567e-05,
      "loss": 1.7674,
      "step": 73700
    },
    {
      "epoch": 37.492370295015256,
      "grad_norm": 44.73663330078125,
      "learning_rate": 1.250762970498474e-05,
      "loss": 1.6472,
      "step": 73710
    },
    {
      "epoch": 37.49745676500508,
      "grad_norm": 45.94024658203125,
      "learning_rate": 1.2502543234994916e-05,
      "loss": 1.6492,
      "step": 73720
    },
    {
      "epoch": 37.50254323499492,
      "grad_norm": 40.40624237060547,
      "learning_rate": 1.2497456765005087e-05,
      "loss": 1.5824,
      "step": 73730
    },
    {
      "epoch": 37.507629704984744,
      "grad_norm": 41.361637115478516,
      "learning_rate": 1.249237029501526e-05,
      "loss": 1.6564,
      "step": 73740
    },
    {
      "epoch": 37.51271617497457,
      "grad_norm": 47.54484558105469,
      "learning_rate": 1.2487283825025432e-05,
      "loss": 1.6331,
      "step": 73750
    },
    {
      "epoch": 37.5178026449644,
      "grad_norm": 39.74543762207031,
      "learning_rate": 1.2482197355035605e-05,
      "loss": 1.6027,
      "step": 73760
    },
    {
      "epoch": 37.522889114954225,
      "grad_norm": 40.080360412597656,
      "learning_rate": 1.2477110885045779e-05,
      "loss": 1.6033,
      "step": 73770
    },
    {
      "epoch": 37.52797558494405,
      "grad_norm": 43.87593460083008,
      "learning_rate": 1.2472024415055952e-05,
      "loss": 1.6743,
      "step": 73780
    },
    {
      "epoch": 37.53306205493388,
      "grad_norm": 50.992774963378906,
      "learning_rate": 1.2466937945066125e-05,
      "loss": 1.6221,
      "step": 73790
    },
    {
      "epoch": 37.538148524923706,
      "grad_norm": 47.465484619140625,
      "learning_rate": 1.2461851475076297e-05,
      "loss": 1.5807,
      "step": 73800
    },
    {
      "epoch": 37.54323499491353,
      "grad_norm": 45.092811584472656,
      "learning_rate": 1.245676500508647e-05,
      "loss": 1.5849,
      "step": 73810
    },
    {
      "epoch": 37.54832146490336,
      "grad_norm": 45.874637603759766,
      "learning_rate": 1.2451678535096643e-05,
      "loss": 1.5897,
      "step": 73820
    },
    {
      "epoch": 37.55340793489319,
      "grad_norm": 39.255645751953125,
      "learning_rate": 1.2446592065106817e-05,
      "loss": 1.6255,
      "step": 73830
    },
    {
      "epoch": 37.558494404883014,
      "grad_norm": 47.06639099121094,
      "learning_rate": 1.244150559511699e-05,
      "loss": 1.5648,
      "step": 73840
    },
    {
      "epoch": 37.56358087487284,
      "grad_norm": 47.25898361206055,
      "learning_rate": 1.2436419125127162e-05,
      "loss": 1.5604,
      "step": 73850
    },
    {
      "epoch": 37.56866734486267,
      "grad_norm": 42.7463493347168,
      "learning_rate": 1.2431332655137337e-05,
      "loss": 1.5883,
      "step": 73860
    },
    {
      "epoch": 37.573753814852495,
      "grad_norm": 35.922508239746094,
      "learning_rate": 1.2426246185147508e-05,
      "loss": 1.6009,
      "step": 73870
    },
    {
      "epoch": 37.57884028484232,
      "grad_norm": 39.098167419433594,
      "learning_rate": 1.2421159715157681e-05,
      "loss": 1.614,
      "step": 73880
    },
    {
      "epoch": 37.58392675483215,
      "grad_norm": 33.69533157348633,
      "learning_rate": 1.2416073245167855e-05,
      "loss": 1.6514,
      "step": 73890
    },
    {
      "epoch": 37.589013224821976,
      "grad_norm": 42.73847198486328,
      "learning_rate": 1.2410986775178026e-05,
      "loss": 1.651,
      "step": 73900
    },
    {
      "epoch": 37.5940996948118,
      "grad_norm": 36.577178955078125,
      "learning_rate": 1.2405900305188201e-05,
      "loss": 1.6212,
      "step": 73910
    },
    {
      "epoch": 37.59918616480163,
      "grad_norm": 49.976016998291016,
      "learning_rate": 1.2400813835198373e-05,
      "loss": 1.5954,
      "step": 73920
    },
    {
      "epoch": 37.60427263479146,
      "grad_norm": 47.63438415527344,
      "learning_rate": 1.2395727365208546e-05,
      "loss": 1.6349,
      "step": 73930
    },
    {
      "epoch": 37.609359104781284,
      "grad_norm": 46.00154113769531,
      "learning_rate": 1.239064089521872e-05,
      "loss": 1.6361,
      "step": 73940
    },
    {
      "epoch": 37.61444557477111,
      "grad_norm": 43.857032775878906,
      "learning_rate": 1.2385554425228891e-05,
      "loss": 1.5628,
      "step": 73950
    },
    {
      "epoch": 37.61953204476094,
      "grad_norm": 33.58747863769531,
      "learning_rate": 1.2380467955239064e-05,
      "loss": 1.6454,
      "step": 73960
    },
    {
      "epoch": 37.624618514750765,
      "grad_norm": 43.664676666259766,
      "learning_rate": 1.2375381485249238e-05,
      "loss": 1.5402,
      "step": 73970
    },
    {
      "epoch": 37.62970498474059,
      "grad_norm": 40.869747161865234,
      "learning_rate": 1.237029501525941e-05,
      "loss": 1.5123,
      "step": 73980
    },
    {
      "epoch": 37.63479145473042,
      "grad_norm": 33.872703552246094,
      "learning_rate": 1.2365208545269584e-05,
      "loss": 1.6024,
      "step": 73990
    },
    {
      "epoch": 37.639877924720246,
      "grad_norm": 41.592498779296875,
      "learning_rate": 1.2360122075279756e-05,
      "loss": 1.6494,
      "step": 74000
    },
    {
      "epoch": 37.64496439471007,
      "grad_norm": 45.35739517211914,
      "learning_rate": 1.2355035605289929e-05,
      "loss": 1.6081,
      "step": 74010
    },
    {
      "epoch": 37.6500508646999,
      "grad_norm": 56.961124420166016,
      "learning_rate": 1.2349949135300102e-05,
      "loss": 1.6873,
      "step": 74020
    },
    {
      "epoch": 37.65513733468973,
      "grad_norm": 42.086891174316406,
      "learning_rate": 1.2344862665310276e-05,
      "loss": 1.6897,
      "step": 74030
    },
    {
      "epoch": 37.66022380467955,
      "grad_norm": 34.11526107788086,
      "learning_rate": 1.2339776195320449e-05,
      "loss": 1.6076,
      "step": 74040
    },
    {
      "epoch": 37.66531027466938,
      "grad_norm": 45.31251907348633,
      "learning_rate": 1.233468972533062e-05,
      "loss": 1.5652,
      "step": 74050
    },
    {
      "epoch": 37.67039674465921,
      "grad_norm": 42.09197235107422,
      "learning_rate": 1.2329603255340794e-05,
      "loss": 1.5872,
      "step": 74060
    },
    {
      "epoch": 37.675483214649034,
      "grad_norm": 54.630210876464844,
      "learning_rate": 1.2324516785350967e-05,
      "loss": 1.5629,
      "step": 74070
    },
    {
      "epoch": 37.68056968463886,
      "grad_norm": 44.94890213012695,
      "learning_rate": 1.231943031536114e-05,
      "loss": 1.5568,
      "step": 74080
    },
    {
      "epoch": 37.68565615462869,
      "grad_norm": 46.536773681640625,
      "learning_rate": 1.2314343845371312e-05,
      "loss": 1.6925,
      "step": 74090
    },
    {
      "epoch": 37.690742624618515,
      "grad_norm": 45.777591705322266,
      "learning_rate": 1.2309257375381487e-05,
      "loss": 1.6722,
      "step": 74100
    },
    {
      "epoch": 37.69582909460834,
      "grad_norm": 49.54364013671875,
      "learning_rate": 1.2304170905391658e-05,
      "loss": 1.6135,
      "step": 74110
    },
    {
      "epoch": 37.70091556459817,
      "grad_norm": 40.90702438354492,
      "learning_rate": 1.2299084435401832e-05,
      "loss": 1.6325,
      "step": 74120
    },
    {
      "epoch": 37.706002034587996,
      "grad_norm": 44.133724212646484,
      "learning_rate": 1.2293997965412005e-05,
      "loss": 1.6715,
      "step": 74130
    },
    {
      "epoch": 37.71108850457782,
      "grad_norm": 45.15467834472656,
      "learning_rate": 1.2288911495422177e-05,
      "loss": 1.6879,
      "step": 74140
    },
    {
      "epoch": 37.71617497456765,
      "grad_norm": 34.14387130737305,
      "learning_rate": 1.2283825025432352e-05,
      "loss": 1.6532,
      "step": 74150
    },
    {
      "epoch": 37.72126144455748,
      "grad_norm": 46.82944107055664,
      "learning_rate": 1.2278738555442523e-05,
      "loss": 1.5589,
      "step": 74160
    },
    {
      "epoch": 37.726347914547304,
      "grad_norm": 33.317073822021484,
      "learning_rate": 1.2273652085452696e-05,
      "loss": 1.6182,
      "step": 74170
    },
    {
      "epoch": 37.73143438453713,
      "grad_norm": 36.37649154663086,
      "learning_rate": 1.226856561546287e-05,
      "loss": 1.5869,
      "step": 74180
    },
    {
      "epoch": 37.73652085452696,
      "grad_norm": 45.06214904785156,
      "learning_rate": 1.2263479145473041e-05,
      "loss": 1.5941,
      "step": 74190
    },
    {
      "epoch": 37.741607324516785,
      "grad_norm": 34.70845413208008,
      "learning_rate": 1.2258392675483216e-05,
      "loss": 1.688,
      "step": 74200
    },
    {
      "epoch": 37.74669379450661,
      "grad_norm": 36.288856506347656,
      "learning_rate": 1.2253306205493388e-05,
      "loss": 1.6093,
      "step": 74210
    },
    {
      "epoch": 37.75178026449644,
      "grad_norm": 41.8344841003418,
      "learning_rate": 1.2248219735503561e-05,
      "loss": 1.6055,
      "step": 74220
    },
    {
      "epoch": 37.756866734486266,
      "grad_norm": 38.25897979736328,
      "learning_rate": 1.2243133265513734e-05,
      "loss": 1.6107,
      "step": 74230
    },
    {
      "epoch": 37.76195320447609,
      "grad_norm": 51.72967529296875,
      "learning_rate": 1.2238046795523906e-05,
      "loss": 1.5827,
      "step": 74240
    },
    {
      "epoch": 37.76703967446592,
      "grad_norm": 50.283817291259766,
      "learning_rate": 1.2232960325534081e-05,
      "loss": 1.5321,
      "step": 74250
    },
    {
      "epoch": 37.77212614445575,
      "grad_norm": 51.352928161621094,
      "learning_rate": 1.2227873855544253e-05,
      "loss": 1.6534,
      "step": 74260
    },
    {
      "epoch": 37.777212614445574,
      "grad_norm": 35.29439163208008,
      "learning_rate": 1.2222787385554426e-05,
      "loss": 1.56,
      "step": 74270
    },
    {
      "epoch": 37.7822990844354,
      "grad_norm": 49.470130920410156,
      "learning_rate": 1.2217700915564599e-05,
      "loss": 1.5713,
      "step": 74280
    },
    {
      "epoch": 37.78738555442523,
      "grad_norm": 32.5138053894043,
      "learning_rate": 1.221261444557477e-05,
      "loss": 1.6089,
      "step": 74290
    },
    {
      "epoch": 37.792472024415055,
      "grad_norm": 39.10755157470703,
      "learning_rate": 1.2207527975584946e-05,
      "loss": 1.6049,
      "step": 74300
    },
    {
      "epoch": 37.79755849440488,
      "grad_norm": 48.841773986816406,
      "learning_rate": 1.2202441505595117e-05,
      "loss": 1.5897,
      "step": 74310
    },
    {
      "epoch": 37.80264496439471,
      "grad_norm": 48.55065155029297,
      "learning_rate": 1.219735503560529e-05,
      "loss": 1.6165,
      "step": 74320
    },
    {
      "epoch": 37.807731434384536,
      "grad_norm": 45.86091995239258,
      "learning_rate": 1.2192268565615464e-05,
      "loss": 1.4918,
      "step": 74330
    },
    {
      "epoch": 37.81281790437436,
      "grad_norm": 33.180328369140625,
      "learning_rate": 1.2187182095625637e-05,
      "loss": 1.6832,
      "step": 74340
    },
    {
      "epoch": 37.81790437436419,
      "grad_norm": 44.08403015136719,
      "learning_rate": 1.2182095625635809e-05,
      "loss": 1.5946,
      "step": 74350
    },
    {
      "epoch": 37.82299084435402,
      "grad_norm": 41.02058410644531,
      "learning_rate": 1.2177009155645982e-05,
      "loss": 1.6044,
      "step": 74360
    },
    {
      "epoch": 37.828077314343844,
      "grad_norm": 31.67705535888672,
      "learning_rate": 1.2171922685656155e-05,
      "loss": 1.5902,
      "step": 74370
    },
    {
      "epoch": 37.83316378433367,
      "grad_norm": 42.75541687011719,
      "learning_rate": 1.2166836215666329e-05,
      "loss": 1.5548,
      "step": 74380
    },
    {
      "epoch": 37.8382502543235,
      "grad_norm": 50.839332580566406,
      "learning_rate": 1.2161749745676502e-05,
      "loss": 1.6612,
      "step": 74390
    },
    {
      "epoch": 37.843336724313325,
      "grad_norm": 48.6254997253418,
      "learning_rate": 1.2156663275686673e-05,
      "loss": 1.5948,
      "step": 74400
    },
    {
      "epoch": 37.84842319430315,
      "grad_norm": 48.70280838012695,
      "learning_rate": 1.2151576805696847e-05,
      "loss": 1.7038,
      "step": 74410
    },
    {
      "epoch": 37.85350966429298,
      "grad_norm": 53.06844711303711,
      "learning_rate": 1.214649033570702e-05,
      "loss": 1.6358,
      "step": 74420
    },
    {
      "epoch": 37.858596134282806,
      "grad_norm": 43.93128204345703,
      "learning_rate": 1.2141403865717192e-05,
      "loss": 1.6088,
      "step": 74430
    },
    {
      "epoch": 37.86368260427263,
      "grad_norm": 41.17925262451172,
      "learning_rate": 1.2136317395727367e-05,
      "loss": 1.6419,
      "step": 74440
    },
    {
      "epoch": 37.86876907426246,
      "grad_norm": 47.854190826416016,
      "learning_rate": 1.2131230925737538e-05,
      "loss": 1.5631,
      "step": 74450
    },
    {
      "epoch": 37.87385554425229,
      "grad_norm": 37.11492919921875,
      "learning_rate": 1.2126144455747711e-05,
      "loss": 1.6254,
      "step": 74460
    },
    {
      "epoch": 37.878942014242114,
      "grad_norm": 41.30191421508789,
      "learning_rate": 1.2121057985757885e-05,
      "loss": 1.5983,
      "step": 74470
    },
    {
      "epoch": 37.88402848423194,
      "grad_norm": 37.95949935913086,
      "learning_rate": 1.2115971515768056e-05,
      "loss": 1.5287,
      "step": 74480
    },
    {
      "epoch": 37.88911495422177,
      "grad_norm": 38.21548080444336,
      "learning_rate": 1.2110885045778231e-05,
      "loss": 1.6018,
      "step": 74490
    },
    {
      "epoch": 37.894201424211595,
      "grad_norm": 41.259761810302734,
      "learning_rate": 1.2105798575788403e-05,
      "loss": 1.6107,
      "step": 74500
    },
    {
      "epoch": 37.89928789420142,
      "grad_norm": 31.995922088623047,
      "learning_rate": 1.2100712105798578e-05,
      "loss": 1.5456,
      "step": 74510
    },
    {
      "epoch": 37.90437436419125,
      "grad_norm": 37.595672607421875,
      "learning_rate": 1.209562563580875e-05,
      "loss": 1.6054,
      "step": 74520
    },
    {
      "epoch": 37.909460834181075,
      "grad_norm": 40.591468811035156,
      "learning_rate": 1.2090539165818923e-05,
      "loss": 1.6111,
      "step": 74530
    },
    {
      "epoch": 37.9145473041709,
      "grad_norm": 40.95817565917969,
      "learning_rate": 1.2085452695829096e-05,
      "loss": 1.6222,
      "step": 74540
    },
    {
      "epoch": 37.91963377416073,
      "grad_norm": 37.59220504760742,
      "learning_rate": 1.2080366225839268e-05,
      "loss": 1.5825,
      "step": 74550
    },
    {
      "epoch": 37.924720244150556,
      "grad_norm": 42.30667495727539,
      "learning_rate": 1.207527975584944e-05,
      "loss": 1.5867,
      "step": 74560
    },
    {
      "epoch": 37.92980671414038,
      "grad_norm": 37.84349822998047,
      "learning_rate": 1.2070193285859614e-05,
      "loss": 1.5408,
      "step": 74570
    },
    {
      "epoch": 37.93489318413021,
      "grad_norm": 38.90564727783203,
      "learning_rate": 1.2065106815869787e-05,
      "loss": 1.6647,
      "step": 74580
    },
    {
      "epoch": 37.93997965412004,
      "grad_norm": 52.073265075683594,
      "learning_rate": 1.206002034587996e-05,
      "loss": 1.5955,
      "step": 74590
    },
    {
      "epoch": 37.945066124109864,
      "grad_norm": 47.06399917602539,
      "learning_rate": 1.2054933875890132e-05,
      "loss": 1.5349,
      "step": 74600
    },
    {
      "epoch": 37.95015259409969,
      "grad_norm": 36.85879898071289,
      "learning_rate": 1.2049847405900306e-05,
      "loss": 1.5387,
      "step": 74610
    },
    {
      "epoch": 37.955239064089525,
      "grad_norm": 52.96724319458008,
      "learning_rate": 1.2044760935910479e-05,
      "loss": 1.6438,
      "step": 74620
    },
    {
      "epoch": 37.96032553407935,
      "grad_norm": 42.26615905761719,
      "learning_rate": 1.2039674465920652e-05,
      "loss": 1.6035,
      "step": 74630
    },
    {
      "epoch": 37.96541200406918,
      "grad_norm": 51.374996185302734,
      "learning_rate": 1.2034587995930825e-05,
      "loss": 1.5918,
      "step": 74640
    },
    {
      "epoch": 37.970498474059006,
      "grad_norm": 36.19390869140625,
      "learning_rate": 1.2029501525940997e-05,
      "loss": 1.5723,
      "step": 74650
    },
    {
      "epoch": 37.97558494404883,
      "grad_norm": 40.83635711669922,
      "learning_rate": 1.202441505595117e-05,
      "loss": 1.55,
      "step": 74660
    },
    {
      "epoch": 37.98067141403866,
      "grad_norm": 46.166481018066406,
      "learning_rate": 1.2019328585961344e-05,
      "loss": 1.7006,
      "step": 74670
    },
    {
      "epoch": 37.98575788402849,
      "grad_norm": 35.04807662963867,
      "learning_rate": 1.2014242115971517e-05,
      "loss": 1.5948,
      "step": 74680
    },
    {
      "epoch": 37.990844354018314,
      "grad_norm": 38.76961898803711,
      "learning_rate": 1.2009155645981688e-05,
      "loss": 1.6127,
      "step": 74690
    },
    {
      "epoch": 37.99593082400814,
      "grad_norm": 41.64048385620117,
      "learning_rate": 1.2004069175991862e-05,
      "loss": 1.5564,
      "step": 74700
    },
    {
      "epoch": 38.0,
      "eval_loss": 5.011714935302734,
      "eval_runtime": 2.6086,
      "eval_samples_per_second": 1063.801,
      "eval_steps_per_second": 133.023,
      "step": 74708
    },
    {
      "epoch": 38.00101729399797,
      "grad_norm": 37.816436767578125,
      "learning_rate": 1.1998982706002035e-05,
      "loss": 1.5661,
      "step": 74710
    },
    {
      "epoch": 38.006103763987795,
      "grad_norm": 33.59271240234375,
      "learning_rate": 1.1993896236012208e-05,
      "loss": 1.6988,
      "step": 74720
    },
    {
      "epoch": 38.01119023397762,
      "grad_norm": 36.91947555541992,
      "learning_rate": 1.1988809766022382e-05,
      "loss": 1.5606,
      "step": 74730
    },
    {
      "epoch": 38.01627670396745,
      "grad_norm": 34.24559020996094,
      "learning_rate": 1.1983723296032553e-05,
      "loss": 1.6889,
      "step": 74740
    },
    {
      "epoch": 38.021363173957276,
      "grad_norm": 39.05890655517578,
      "learning_rate": 1.1978636826042728e-05,
      "loss": 1.6761,
      "step": 74750
    },
    {
      "epoch": 38.0264496439471,
      "grad_norm": 56.62370300292969,
      "learning_rate": 1.19735503560529e-05,
      "loss": 1.5897,
      "step": 74760
    },
    {
      "epoch": 38.03153611393693,
      "grad_norm": 31.497724533081055,
      "learning_rate": 1.1968463886063073e-05,
      "loss": 1.6033,
      "step": 74770
    },
    {
      "epoch": 38.03662258392676,
      "grad_norm": 45.51273727416992,
      "learning_rate": 1.1963377416073246e-05,
      "loss": 1.5634,
      "step": 74780
    },
    {
      "epoch": 38.041709053916584,
      "grad_norm": 44.097747802734375,
      "learning_rate": 1.1958290946083418e-05,
      "loss": 1.5845,
      "step": 74790
    },
    {
      "epoch": 38.04679552390641,
      "grad_norm": 35.30734634399414,
      "learning_rate": 1.1953204476093593e-05,
      "loss": 1.5999,
      "step": 74800
    },
    {
      "epoch": 38.05188199389624,
      "grad_norm": 38.50794982910156,
      "learning_rate": 1.1948118006103764e-05,
      "loss": 1.6038,
      "step": 74810
    },
    {
      "epoch": 38.056968463886065,
      "grad_norm": 41.929195404052734,
      "learning_rate": 1.1943031536113938e-05,
      "loss": 1.6153,
      "step": 74820
    },
    {
      "epoch": 38.06205493387589,
      "grad_norm": 47.193843841552734,
      "learning_rate": 1.1937945066124111e-05,
      "loss": 1.5038,
      "step": 74830
    },
    {
      "epoch": 38.06714140386572,
      "grad_norm": 50.156883239746094,
      "learning_rate": 1.1932858596134283e-05,
      "loss": 1.5515,
      "step": 74840
    },
    {
      "epoch": 38.072227873855546,
      "grad_norm": 48.76032638549805,
      "learning_rate": 1.1927772126144458e-05,
      "loss": 1.501,
      "step": 74850
    },
    {
      "epoch": 38.07731434384537,
      "grad_norm": 37.93177032470703,
      "learning_rate": 1.1922685656154629e-05,
      "loss": 1.6111,
      "step": 74860
    },
    {
      "epoch": 38.0824008138352,
      "grad_norm": 39.808250427246094,
      "learning_rate": 1.1917599186164802e-05,
      "loss": 1.567,
      "step": 74870
    },
    {
      "epoch": 38.08748728382503,
      "grad_norm": 48.55940246582031,
      "learning_rate": 1.1912512716174976e-05,
      "loss": 1.5305,
      "step": 74880
    },
    {
      "epoch": 38.092573753814854,
      "grad_norm": 36.327232360839844,
      "learning_rate": 1.1907426246185147e-05,
      "loss": 1.6034,
      "step": 74890
    },
    {
      "epoch": 38.09766022380468,
      "grad_norm": 47.60039138793945,
      "learning_rate": 1.190233977619532e-05,
      "loss": 1.5922,
      "step": 74900
    },
    {
      "epoch": 38.10274669379451,
      "grad_norm": 37.45424270629883,
      "learning_rate": 1.1897253306205494e-05,
      "loss": 1.6174,
      "step": 74910
    },
    {
      "epoch": 38.107833163784335,
      "grad_norm": 47.450923919677734,
      "learning_rate": 1.1892166836215667e-05,
      "loss": 1.6071,
      "step": 74920
    },
    {
      "epoch": 38.11291963377416,
      "grad_norm": 41.61305618286133,
      "learning_rate": 1.188708036622584e-05,
      "loss": 1.6255,
      "step": 74930
    },
    {
      "epoch": 38.11800610376399,
      "grad_norm": 40.96990966796875,
      "learning_rate": 1.1881993896236012e-05,
      "loss": 1.6825,
      "step": 74940
    },
    {
      "epoch": 38.123092573753816,
      "grad_norm": 46.97806167602539,
      "learning_rate": 1.1876907426246185e-05,
      "loss": 1.504,
      "step": 74950
    },
    {
      "epoch": 38.12817904374364,
      "grad_norm": 38.53700637817383,
      "learning_rate": 1.1871820956256359e-05,
      "loss": 1.5769,
      "step": 74960
    },
    {
      "epoch": 38.13326551373347,
      "grad_norm": 41.965789794921875,
      "learning_rate": 1.1866734486266532e-05,
      "loss": 1.5939,
      "step": 74970
    },
    {
      "epoch": 38.1383519837233,
      "grad_norm": 35.907737731933594,
      "learning_rate": 1.1861648016276705e-05,
      "loss": 1.6119,
      "step": 74980
    },
    {
      "epoch": 38.143438453713124,
      "grad_norm": 35.575103759765625,
      "learning_rate": 1.1856561546286878e-05,
      "loss": 1.6404,
      "step": 74990
    },
    {
      "epoch": 38.14852492370295,
      "grad_norm": 41.72859191894531,
      "learning_rate": 1.185147507629705e-05,
      "loss": 1.6373,
      "step": 75000
    },
    {
      "epoch": 38.15361139369278,
      "grad_norm": 40.00801467895508,
      "learning_rate": 1.1846388606307223e-05,
      "loss": 1.5473,
      "step": 75010
    },
    {
      "epoch": 38.158697863682605,
      "grad_norm": 51.12883377075195,
      "learning_rate": 1.1841302136317397e-05,
      "loss": 1.6514,
      "step": 75020
    },
    {
      "epoch": 38.16378433367243,
      "grad_norm": 42.617042541503906,
      "learning_rate": 1.1836215666327568e-05,
      "loss": 1.564,
      "step": 75030
    },
    {
      "epoch": 38.16887080366226,
      "grad_norm": 42.603294372558594,
      "learning_rate": 1.1831129196337743e-05,
      "loss": 1.6449,
      "step": 75040
    },
    {
      "epoch": 38.173957273652086,
      "grad_norm": 31.391538619995117,
      "learning_rate": 1.1826042726347915e-05,
      "loss": 1.5629,
      "step": 75050
    },
    {
      "epoch": 38.17904374364191,
      "grad_norm": 35.8919563293457,
      "learning_rate": 1.1820956256358088e-05,
      "loss": 1.758,
      "step": 75060
    },
    {
      "epoch": 38.18413021363174,
      "grad_norm": 43.37397003173828,
      "learning_rate": 1.1815869786368261e-05,
      "loss": 1.6299,
      "step": 75070
    },
    {
      "epoch": 38.18921668362157,
      "grad_norm": 46.1848030090332,
      "learning_rate": 1.1810783316378433e-05,
      "loss": 1.5465,
      "step": 75080
    },
    {
      "epoch": 38.19430315361139,
      "grad_norm": 36.96296310424805,
      "learning_rate": 1.1805696846388608e-05,
      "loss": 1.5337,
      "step": 75090
    },
    {
      "epoch": 38.19938962360122,
      "grad_norm": 43.7075309753418,
      "learning_rate": 1.180061037639878e-05,
      "loss": 1.543,
      "step": 75100
    },
    {
      "epoch": 38.20447609359105,
      "grad_norm": 35.93132781982422,
      "learning_rate": 1.1795523906408953e-05,
      "loss": 1.5249,
      "step": 75110
    },
    {
      "epoch": 38.209562563580874,
      "grad_norm": 40.96036911010742,
      "learning_rate": 1.1790437436419126e-05,
      "loss": 1.6298,
      "step": 75120
    },
    {
      "epoch": 38.2146490335707,
      "grad_norm": 40.950538635253906,
      "learning_rate": 1.1785350966429298e-05,
      "loss": 1.5493,
      "step": 75130
    },
    {
      "epoch": 38.21973550356053,
      "grad_norm": 39.87623596191406,
      "learning_rate": 1.1780264496439473e-05,
      "loss": 1.5768,
      "step": 75140
    },
    {
      "epoch": 38.224821973550355,
      "grad_norm": 36.30400466918945,
      "learning_rate": 1.1775178026449644e-05,
      "loss": 1.6156,
      "step": 75150
    },
    {
      "epoch": 38.22990844354018,
      "grad_norm": 38.76736068725586,
      "learning_rate": 1.1770091556459817e-05,
      "loss": 1.5815,
      "step": 75160
    },
    {
      "epoch": 38.23499491353001,
      "grad_norm": 33.06690979003906,
      "learning_rate": 1.176500508646999e-05,
      "loss": 1.5278,
      "step": 75170
    },
    {
      "epoch": 38.240081383519836,
      "grad_norm": 55.43980407714844,
      "learning_rate": 1.1759918616480162e-05,
      "loss": 1.5921,
      "step": 75180
    },
    {
      "epoch": 38.24516785350966,
      "grad_norm": 39.64323043823242,
      "learning_rate": 1.1754832146490337e-05,
      "loss": 1.5748,
      "step": 75190
    },
    {
      "epoch": 38.25025432349949,
      "grad_norm": 40.33828353881836,
      "learning_rate": 1.1749745676500509e-05,
      "loss": 1.5713,
      "step": 75200
    },
    {
      "epoch": 38.25534079348932,
      "grad_norm": 31.66002655029297,
      "learning_rate": 1.1744659206510682e-05,
      "loss": 1.5164,
      "step": 75210
    },
    {
      "epoch": 38.260427263479144,
      "grad_norm": 41.590576171875,
      "learning_rate": 1.1739572736520855e-05,
      "loss": 1.6223,
      "step": 75220
    },
    {
      "epoch": 38.26551373346897,
      "grad_norm": 57.39423370361328,
      "learning_rate": 1.1734486266531029e-05,
      "loss": 1.5897,
      "step": 75230
    },
    {
      "epoch": 38.2706002034588,
      "grad_norm": 34.73782730102539,
      "learning_rate": 1.17293997965412e-05,
      "loss": 1.5644,
      "step": 75240
    },
    {
      "epoch": 38.275686673448625,
      "grad_norm": 33.03127670288086,
      "learning_rate": 1.1724313326551374e-05,
      "loss": 1.5515,
      "step": 75250
    },
    {
      "epoch": 38.28077314343845,
      "grad_norm": 40.532859802246094,
      "learning_rate": 1.1719226856561547e-05,
      "loss": 1.5335,
      "step": 75260
    },
    {
      "epoch": 38.28585961342828,
      "grad_norm": 41.9079704284668,
      "learning_rate": 1.171414038657172e-05,
      "loss": 1.5777,
      "step": 75270
    },
    {
      "epoch": 38.290946083418106,
      "grad_norm": 48.751895904541016,
      "learning_rate": 1.1709053916581893e-05,
      "loss": 1.5729,
      "step": 75280
    },
    {
      "epoch": 38.29603255340793,
      "grad_norm": 39.13304901123047,
      "learning_rate": 1.1703967446592065e-05,
      "loss": 1.5265,
      "step": 75290
    },
    {
      "epoch": 38.30111902339776,
      "grad_norm": 47.95229721069336,
      "learning_rate": 1.1698880976602238e-05,
      "loss": 1.5545,
      "step": 75300
    },
    {
      "epoch": 38.30620549338759,
      "grad_norm": 41.575157165527344,
      "learning_rate": 1.1693794506612412e-05,
      "loss": 1.627,
      "step": 75310
    },
    {
      "epoch": 38.311291963377414,
      "grad_norm": 37.72721862792969,
      "learning_rate": 1.1688708036622585e-05,
      "loss": 1.5422,
      "step": 75320
    },
    {
      "epoch": 38.31637843336724,
      "grad_norm": 37.54133987426758,
      "learning_rate": 1.1683621566632758e-05,
      "loss": 1.5425,
      "step": 75330
    },
    {
      "epoch": 38.32146490335707,
      "grad_norm": 44.1240119934082,
      "learning_rate": 1.167853509664293e-05,
      "loss": 1.5851,
      "step": 75340
    },
    {
      "epoch": 38.326551373346895,
      "grad_norm": 39.24785232543945,
      "learning_rate": 1.1673448626653103e-05,
      "loss": 1.5859,
      "step": 75350
    },
    {
      "epoch": 38.33163784333672,
      "grad_norm": 47.35560989379883,
      "learning_rate": 1.1668362156663276e-05,
      "loss": 1.6296,
      "step": 75360
    },
    {
      "epoch": 38.33672431332655,
      "grad_norm": 35.01995086669922,
      "learning_rate": 1.1663275686673448e-05,
      "loss": 1.6316,
      "step": 75370
    },
    {
      "epoch": 38.341810783316376,
      "grad_norm": 36.651153564453125,
      "learning_rate": 1.1658189216683623e-05,
      "loss": 1.6939,
      "step": 75380
    },
    {
      "epoch": 38.3468972533062,
      "grad_norm": 47.77214431762695,
      "learning_rate": 1.1653102746693794e-05,
      "loss": 1.6683,
      "step": 75390
    },
    {
      "epoch": 38.35198372329603,
      "grad_norm": 43.76722717285156,
      "learning_rate": 1.164801627670397e-05,
      "loss": 1.6354,
      "step": 75400
    },
    {
      "epoch": 38.35707019328586,
      "grad_norm": 43.402442932128906,
      "learning_rate": 1.1642929806714141e-05,
      "loss": 1.6313,
      "step": 75410
    },
    {
      "epoch": 38.362156663275684,
      "grad_norm": 33.01436996459961,
      "learning_rate": 1.1637843336724313e-05,
      "loss": 1.5879,
      "step": 75420
    },
    {
      "epoch": 38.36724313326551,
      "grad_norm": 46.4126091003418,
      "learning_rate": 1.1632756866734488e-05,
      "loss": 1.5692,
      "step": 75430
    },
    {
      "epoch": 38.37232960325534,
      "grad_norm": 48.26210021972656,
      "learning_rate": 1.1627670396744659e-05,
      "loss": 1.5491,
      "step": 75440
    },
    {
      "epoch": 38.377416073245165,
      "grad_norm": 39.764320373535156,
      "learning_rate": 1.1622583926754834e-05,
      "loss": 1.634,
      "step": 75450
    },
    {
      "epoch": 38.38250254323499,
      "grad_norm": 33.95600128173828,
      "learning_rate": 1.1617497456765006e-05,
      "loss": 1.4502,
      "step": 75460
    },
    {
      "epoch": 38.38758901322482,
      "grad_norm": 40.911537170410156,
      "learning_rate": 1.1612410986775179e-05,
      "loss": 1.7001,
      "step": 75470
    },
    {
      "epoch": 38.392675483214646,
      "grad_norm": 42.638153076171875,
      "learning_rate": 1.1607324516785352e-05,
      "loss": 1.6048,
      "step": 75480
    },
    {
      "epoch": 38.39776195320447,
      "grad_norm": 38.973846435546875,
      "learning_rate": 1.1602238046795524e-05,
      "loss": 1.504,
      "step": 75490
    },
    {
      "epoch": 38.4028484231943,
      "grad_norm": 38.98676300048828,
      "learning_rate": 1.1597151576805697e-05,
      "loss": 1.6333,
      "step": 75500
    },
    {
      "epoch": 38.407934893184134,
      "grad_norm": 42.42227554321289,
      "learning_rate": 1.159206510681587e-05,
      "loss": 1.6526,
      "step": 75510
    },
    {
      "epoch": 38.41302136317396,
      "grad_norm": 38.031272888183594,
      "learning_rate": 1.1586978636826044e-05,
      "loss": 1.6359,
      "step": 75520
    },
    {
      "epoch": 38.41810783316379,
      "grad_norm": 39.49171829223633,
      "learning_rate": 1.1581892166836217e-05,
      "loss": 1.5323,
      "step": 75530
    },
    {
      "epoch": 38.423194303153615,
      "grad_norm": 41.71683883666992,
      "learning_rate": 1.1576805696846389e-05,
      "loss": 1.5028,
      "step": 75540
    },
    {
      "epoch": 38.42828077314344,
      "grad_norm": 43.77770233154297,
      "learning_rate": 1.1571719226856562e-05,
      "loss": 1.5223,
      "step": 75550
    },
    {
      "epoch": 38.43336724313327,
      "grad_norm": 42.208030700683594,
      "learning_rate": 1.1566632756866735e-05,
      "loss": 1.5751,
      "step": 75560
    },
    {
      "epoch": 38.438453713123096,
      "grad_norm": 42.223209381103516,
      "learning_rate": 1.1561546286876908e-05,
      "loss": 1.5789,
      "step": 75570
    },
    {
      "epoch": 38.44354018311292,
      "grad_norm": 47.27695083618164,
      "learning_rate": 1.155645981688708e-05,
      "loss": 1.5786,
      "step": 75580
    },
    {
      "epoch": 38.44862665310275,
      "grad_norm": 52.78607177734375,
      "learning_rate": 1.1551373346897253e-05,
      "loss": 1.5806,
      "step": 75590
    },
    {
      "epoch": 38.45371312309258,
      "grad_norm": 42.152374267578125,
      "learning_rate": 1.1546286876907427e-05,
      "loss": 1.6451,
      "step": 75600
    },
    {
      "epoch": 38.458799593082404,
      "grad_norm": 31.399629592895508,
      "learning_rate": 1.15412004069176e-05,
      "loss": 1.5461,
      "step": 75610
    },
    {
      "epoch": 38.46388606307223,
      "grad_norm": 44.90974044799805,
      "learning_rate": 1.1536113936927773e-05,
      "loss": 1.6042,
      "step": 75620
    },
    {
      "epoch": 38.46897253306206,
      "grad_norm": 43.473087310791016,
      "learning_rate": 1.1531027466937945e-05,
      "loss": 1.5737,
      "step": 75630
    },
    {
      "epoch": 38.474059003051885,
      "grad_norm": 39.80834197998047,
      "learning_rate": 1.152594099694812e-05,
      "loss": 1.6225,
      "step": 75640
    },
    {
      "epoch": 38.47914547304171,
      "grad_norm": 47.828609466552734,
      "learning_rate": 1.1520854526958291e-05,
      "loss": 1.6514,
      "step": 75650
    },
    {
      "epoch": 38.48423194303154,
      "grad_norm": 41.519004821777344,
      "learning_rate": 1.1515768056968465e-05,
      "loss": 1.6,
      "step": 75660
    },
    {
      "epoch": 38.489318413021365,
      "grad_norm": 42.13949203491211,
      "learning_rate": 1.1510681586978638e-05,
      "loss": 1.6767,
      "step": 75670
    },
    {
      "epoch": 38.49440488301119,
      "grad_norm": 40.76814270019531,
      "learning_rate": 1.150559511698881e-05,
      "loss": 1.6864,
      "step": 75680
    },
    {
      "epoch": 38.49949135300102,
      "grad_norm": 42.01241683959961,
      "learning_rate": 1.1500508646998984e-05,
      "loss": 1.6541,
      "step": 75690
    },
    {
      "epoch": 38.504577822990846,
      "grad_norm": 29.976886749267578,
      "learning_rate": 1.1495422177009156e-05,
      "loss": 1.6194,
      "step": 75700
    },
    {
      "epoch": 38.50966429298067,
      "grad_norm": 38.26206588745117,
      "learning_rate": 1.149033570701933e-05,
      "loss": 1.5535,
      "step": 75710
    },
    {
      "epoch": 38.5147507629705,
      "grad_norm": 44.85446548461914,
      "learning_rate": 1.1485249237029503e-05,
      "loss": 1.6034,
      "step": 75720
    },
    {
      "epoch": 38.51983723296033,
      "grad_norm": 38.650299072265625,
      "learning_rate": 1.1480162767039674e-05,
      "loss": 1.5925,
      "step": 75730
    },
    {
      "epoch": 38.524923702950154,
      "grad_norm": 38.175254821777344,
      "learning_rate": 1.1475076297049849e-05,
      "loss": 1.6514,
      "step": 75740
    },
    {
      "epoch": 38.53001017293998,
      "grad_norm": 33.1063232421875,
      "learning_rate": 1.146998982706002e-05,
      "loss": 1.6315,
      "step": 75750
    },
    {
      "epoch": 38.53509664292981,
      "grad_norm": 49.66758346557617,
      "learning_rate": 1.1464903357070194e-05,
      "loss": 1.639,
      "step": 75760
    },
    {
      "epoch": 38.540183112919635,
      "grad_norm": 45.77804946899414,
      "learning_rate": 1.1459816887080367e-05,
      "loss": 1.6472,
      "step": 75770
    },
    {
      "epoch": 38.54526958290946,
      "grad_norm": 41.41360092163086,
      "learning_rate": 1.1454730417090539e-05,
      "loss": 1.5347,
      "step": 75780
    },
    {
      "epoch": 38.55035605289929,
      "grad_norm": 36.0936393737793,
      "learning_rate": 1.1449643947100714e-05,
      "loss": 1.6229,
      "step": 75790
    },
    {
      "epoch": 38.555442522889116,
      "grad_norm": 44.63642883300781,
      "learning_rate": 1.1444557477110885e-05,
      "loss": 1.6114,
      "step": 75800
    },
    {
      "epoch": 38.56052899287894,
      "grad_norm": 40.40666961669922,
      "learning_rate": 1.1439471007121059e-05,
      "loss": 1.6198,
      "step": 75810
    },
    {
      "epoch": 38.56561546286877,
      "grad_norm": 47.33206558227539,
      "learning_rate": 1.1434384537131232e-05,
      "loss": 1.5984,
      "step": 75820
    },
    {
      "epoch": 38.5707019328586,
      "grad_norm": 33.92261505126953,
      "learning_rate": 1.1429298067141404e-05,
      "loss": 1.5966,
      "step": 75830
    },
    {
      "epoch": 38.575788402848424,
      "grad_norm": 44.93219757080078,
      "learning_rate": 1.1424211597151577e-05,
      "loss": 1.5882,
      "step": 75840
    },
    {
      "epoch": 38.58087487283825,
      "grad_norm": 47.97832107543945,
      "learning_rate": 1.141912512716175e-05,
      "loss": 1.6315,
      "step": 75850
    },
    {
      "epoch": 38.58596134282808,
      "grad_norm": 39.138999938964844,
      "learning_rate": 1.1414038657171923e-05,
      "loss": 1.541,
      "step": 75860
    },
    {
      "epoch": 38.591047812817905,
      "grad_norm": 55.8618278503418,
      "learning_rate": 1.1408952187182097e-05,
      "loss": 1.6782,
      "step": 75870
    },
    {
      "epoch": 38.59613428280773,
      "grad_norm": 49.60963821411133,
      "learning_rate": 1.140386571719227e-05,
      "loss": 1.6792,
      "step": 75880
    },
    {
      "epoch": 38.60122075279756,
      "grad_norm": 51.230560302734375,
      "learning_rate": 1.1398779247202442e-05,
      "loss": 1.6248,
      "step": 75890
    },
    {
      "epoch": 38.606307222787386,
      "grad_norm": 33.696048736572266,
      "learning_rate": 1.1393692777212615e-05,
      "loss": 1.6431,
      "step": 75900
    },
    {
      "epoch": 38.61139369277721,
      "grad_norm": 36.85986328125,
      "learning_rate": 1.1388606307222788e-05,
      "loss": 1.5918,
      "step": 75910
    },
    {
      "epoch": 38.61648016276704,
      "grad_norm": 43.7498664855957,
      "learning_rate": 1.1383519837232961e-05,
      "loss": 1.5701,
      "step": 75920
    },
    {
      "epoch": 38.62156663275687,
      "grad_norm": 43.650508880615234,
      "learning_rate": 1.1378433367243135e-05,
      "loss": 1.6317,
      "step": 75930
    },
    {
      "epoch": 38.626653102746694,
      "grad_norm": 55.1641960144043,
      "learning_rate": 1.1373346897253306e-05,
      "loss": 1.6446,
      "step": 75940
    },
    {
      "epoch": 38.63173957273652,
      "grad_norm": 40.17182540893555,
      "learning_rate": 1.136826042726348e-05,
      "loss": 1.6717,
      "step": 75950
    },
    {
      "epoch": 38.63682604272635,
      "grad_norm": 35.033504486083984,
      "learning_rate": 1.1363173957273653e-05,
      "loss": 1.6653,
      "step": 75960
    },
    {
      "epoch": 38.641912512716175,
      "grad_norm": 49.936946868896484,
      "learning_rate": 1.1358087487283824e-05,
      "loss": 1.6468,
      "step": 75970
    },
    {
      "epoch": 38.646998982706,
      "grad_norm": 38.775634765625,
      "learning_rate": 1.1353001017294e-05,
      "loss": 1.7102,
      "step": 75980
    },
    {
      "epoch": 38.65208545269583,
      "grad_norm": 46.430782318115234,
      "learning_rate": 1.1347914547304171e-05,
      "loss": 1.6348,
      "step": 75990
    },
    {
      "epoch": 38.657171922685656,
      "grad_norm": 45.55025863647461,
      "learning_rate": 1.1342828077314344e-05,
      "loss": 1.6128,
      "step": 76000
    },
    {
      "epoch": 38.66225839267548,
      "grad_norm": 49.41818618774414,
      "learning_rate": 1.1337741607324518e-05,
      "loss": 1.5578,
      "step": 76010
    },
    {
      "epoch": 38.66734486266531,
      "grad_norm": 42.85789489746094,
      "learning_rate": 1.1332655137334689e-05,
      "loss": 1.5866,
      "step": 76020
    },
    {
      "epoch": 38.67243133265514,
      "grad_norm": 54.76715850830078,
      "learning_rate": 1.1327568667344864e-05,
      "loss": 1.6565,
      "step": 76030
    },
    {
      "epoch": 38.677517802644964,
      "grad_norm": 36.56215286254883,
      "learning_rate": 1.1322482197355036e-05,
      "loss": 1.4729,
      "step": 76040
    },
    {
      "epoch": 38.68260427263479,
      "grad_norm": 56.920108795166016,
      "learning_rate": 1.1317395727365209e-05,
      "loss": 1.6349,
      "step": 76050
    },
    {
      "epoch": 38.68769074262462,
      "grad_norm": 31.45806884765625,
      "learning_rate": 1.1312309257375382e-05,
      "loss": 1.57,
      "step": 76060
    },
    {
      "epoch": 38.692777212614445,
      "grad_norm": 42.74226379394531,
      "learning_rate": 1.1307222787385554e-05,
      "loss": 1.5727,
      "step": 76070
    },
    {
      "epoch": 38.69786368260427,
      "grad_norm": 42.40981674194336,
      "learning_rate": 1.1302136317395729e-05,
      "loss": 1.5625,
      "step": 76080
    },
    {
      "epoch": 38.7029501525941,
      "grad_norm": 37.298545837402344,
      "learning_rate": 1.12970498474059e-05,
      "loss": 1.616,
      "step": 76090
    },
    {
      "epoch": 38.708036622583926,
      "grad_norm": 43.465885162353516,
      "learning_rate": 1.1291963377416074e-05,
      "loss": 1.6,
      "step": 76100
    },
    {
      "epoch": 38.71312309257375,
      "grad_norm": 52.5372428894043,
      "learning_rate": 1.1286876907426247e-05,
      "loss": 1.6241,
      "step": 76110
    },
    {
      "epoch": 38.71820956256358,
      "grad_norm": 51.51036834716797,
      "learning_rate": 1.128179043743642e-05,
      "loss": 1.706,
      "step": 76120
    },
    {
      "epoch": 38.72329603255341,
      "grad_norm": 38.98320770263672,
      "learning_rate": 1.1276703967446594e-05,
      "loss": 1.679,
      "step": 76130
    },
    {
      "epoch": 38.728382502543234,
      "grad_norm": 33.745426177978516,
      "learning_rate": 1.1271617497456765e-05,
      "loss": 1.5941,
      "step": 76140
    },
    {
      "epoch": 38.73346897253306,
      "grad_norm": 43.19212341308594,
      "learning_rate": 1.1266531027466938e-05,
      "loss": 1.5779,
      "step": 76150
    },
    {
      "epoch": 38.73855544252289,
      "grad_norm": 51.341121673583984,
      "learning_rate": 1.1261444557477112e-05,
      "loss": 1.6486,
      "step": 76160
    },
    {
      "epoch": 38.743641912512714,
      "grad_norm": 42.919864654541016,
      "learning_rate": 1.1256358087487285e-05,
      "loss": 1.6546,
      "step": 76170
    },
    {
      "epoch": 38.74872838250254,
      "grad_norm": 45.978702545166016,
      "learning_rate": 1.1251271617497457e-05,
      "loss": 1.4976,
      "step": 76180
    },
    {
      "epoch": 38.75381485249237,
      "grad_norm": 45.680999755859375,
      "learning_rate": 1.124618514750763e-05,
      "loss": 1.6614,
      "step": 76190
    },
    {
      "epoch": 38.758901322482195,
      "grad_norm": 44.44786834716797,
      "learning_rate": 1.1241098677517803e-05,
      "loss": 1.6283,
      "step": 76200
    },
    {
      "epoch": 38.76398779247202,
      "grad_norm": 35.713924407958984,
      "learning_rate": 1.1236012207527976e-05,
      "loss": 1.684,
      "step": 76210
    },
    {
      "epoch": 38.76907426246185,
      "grad_norm": 38.96531295776367,
      "learning_rate": 1.123092573753815e-05,
      "loss": 1.6023,
      "step": 76220
    },
    {
      "epoch": 38.774160732451676,
      "grad_norm": 40.38938522338867,
      "learning_rate": 1.1225839267548321e-05,
      "loss": 1.6265,
      "step": 76230
    },
    {
      "epoch": 38.7792472024415,
      "grad_norm": 41.2269287109375,
      "learning_rate": 1.1220752797558495e-05,
      "loss": 1.5525,
      "step": 76240
    },
    {
      "epoch": 38.78433367243133,
      "grad_norm": 38.347557067871094,
      "learning_rate": 1.1215666327568668e-05,
      "loss": 1.6306,
      "step": 76250
    },
    {
      "epoch": 38.78942014242116,
      "grad_norm": 39.6376953125,
      "learning_rate": 1.1210579857578841e-05,
      "loss": 1.6669,
      "step": 76260
    },
    {
      "epoch": 38.794506612410984,
      "grad_norm": 53.66864013671875,
      "learning_rate": 1.1205493387589014e-05,
      "loss": 1.5618,
      "step": 76270
    },
    {
      "epoch": 38.79959308240081,
      "grad_norm": 33.24836730957031,
      "learning_rate": 1.1200406917599186e-05,
      "loss": 1.5889,
      "step": 76280
    },
    {
      "epoch": 38.80467955239064,
      "grad_norm": 37.10639190673828,
      "learning_rate": 1.1195320447609361e-05,
      "loss": 1.5945,
      "step": 76290
    },
    {
      "epoch": 38.809766022380465,
      "grad_norm": 36.74982833862305,
      "learning_rate": 1.1190233977619533e-05,
      "loss": 1.6901,
      "step": 76300
    },
    {
      "epoch": 38.81485249237029,
      "grad_norm": 37.675758361816406,
      "learning_rate": 1.1185147507629704e-05,
      "loss": 1.6214,
      "step": 76310
    },
    {
      "epoch": 38.81993896236012,
      "grad_norm": 38.400962829589844,
      "learning_rate": 1.1180061037639879e-05,
      "loss": 1.5881,
      "step": 76320
    },
    {
      "epoch": 38.825025432349946,
      "grad_norm": 47.849884033203125,
      "learning_rate": 1.117497456765005e-05,
      "loss": 1.6084,
      "step": 76330
    },
    {
      "epoch": 38.83011190233977,
      "grad_norm": 43.285499572753906,
      "learning_rate": 1.1169888097660226e-05,
      "loss": 1.6597,
      "step": 76340
    },
    {
      "epoch": 38.8351983723296,
      "grad_norm": 34.24153518676758,
      "learning_rate": 1.1164801627670397e-05,
      "loss": 1.6146,
      "step": 76350
    },
    {
      "epoch": 38.84028484231943,
      "grad_norm": 42.13951873779297,
      "learning_rate": 1.115971515768057e-05,
      "loss": 1.522,
      "step": 76360
    },
    {
      "epoch": 38.845371312309254,
      "grad_norm": 42.804595947265625,
      "learning_rate": 1.1154628687690744e-05,
      "loss": 1.4805,
      "step": 76370
    },
    {
      "epoch": 38.85045778229908,
      "grad_norm": 34.15324401855469,
      "learning_rate": 1.1149542217700915e-05,
      "loss": 1.6452,
      "step": 76380
    },
    {
      "epoch": 38.85554425228891,
      "grad_norm": 47.36363983154297,
      "learning_rate": 1.1144455747711089e-05,
      "loss": 1.5719,
      "step": 76390
    },
    {
      "epoch": 38.86063072227874,
      "grad_norm": 37.43044662475586,
      "learning_rate": 1.1139369277721262e-05,
      "loss": 1.5404,
      "step": 76400
    },
    {
      "epoch": 38.86571719226856,
      "grad_norm": 38.97904968261719,
      "learning_rate": 1.1134282807731435e-05,
      "loss": 1.5426,
      "step": 76410
    },
    {
      "epoch": 38.870803662258396,
      "grad_norm": 41.24862289428711,
      "learning_rate": 1.1129196337741609e-05,
      "loss": 1.7076,
      "step": 76420
    },
    {
      "epoch": 38.87589013224822,
      "grad_norm": 47.2458381652832,
      "learning_rate": 1.112410986775178e-05,
      "loss": 1.6142,
      "step": 76430
    },
    {
      "epoch": 38.88097660223805,
      "grad_norm": 45.73484420776367,
      "learning_rate": 1.1119023397761953e-05,
      "loss": 1.4972,
      "step": 76440
    },
    {
      "epoch": 38.88606307222788,
      "grad_norm": 39.740516662597656,
      "learning_rate": 1.1113936927772127e-05,
      "loss": 1.5421,
      "step": 76450
    },
    {
      "epoch": 38.891149542217704,
      "grad_norm": 32.83855056762695,
      "learning_rate": 1.11088504577823e-05,
      "loss": 1.5767,
      "step": 76460
    },
    {
      "epoch": 38.89623601220753,
      "grad_norm": 39.44954299926758,
      "learning_rate": 1.1103763987792473e-05,
      "loss": 1.487,
      "step": 76470
    },
    {
      "epoch": 38.90132248219736,
      "grad_norm": 38.171634674072266,
      "learning_rate": 1.1098677517802645e-05,
      "loss": 1.5664,
      "step": 76480
    },
    {
      "epoch": 38.906408952187185,
      "grad_norm": 46.154449462890625,
      "learning_rate": 1.1093591047812818e-05,
      "loss": 1.5906,
      "step": 76490
    },
    {
      "epoch": 38.91149542217701,
      "grad_norm": 47.08890914916992,
      "learning_rate": 1.1088504577822991e-05,
      "loss": 1.5843,
      "step": 76500
    },
    {
      "epoch": 38.91658189216684,
      "grad_norm": 51.53684616088867,
      "learning_rate": 1.1083418107833165e-05,
      "loss": 1.6019,
      "step": 76510
    },
    {
      "epoch": 38.921668362156666,
      "grad_norm": 49.37654495239258,
      "learning_rate": 1.1078331637843336e-05,
      "loss": 1.5249,
      "step": 76520
    },
    {
      "epoch": 38.92675483214649,
      "grad_norm": 36.478492736816406,
      "learning_rate": 1.1073245167853511e-05,
      "loss": 1.663,
      "step": 76530
    },
    {
      "epoch": 38.93184130213632,
      "grad_norm": 50.877323150634766,
      "learning_rate": 1.1068158697863683e-05,
      "loss": 1.5649,
      "step": 76540
    },
    {
      "epoch": 38.93692777212615,
      "grad_norm": 35.259056091308594,
      "learning_rate": 1.1063072227873856e-05,
      "loss": 1.5688,
      "step": 76550
    },
    {
      "epoch": 38.942014242115974,
      "grad_norm": 38.258304595947266,
      "learning_rate": 1.105798575788403e-05,
      "loss": 1.5145,
      "step": 76560
    },
    {
      "epoch": 38.9471007121058,
      "grad_norm": 34.04860305786133,
      "learning_rate": 1.1052899287894201e-05,
      "loss": 1.597,
      "step": 76570
    },
    {
      "epoch": 38.95218718209563,
      "grad_norm": 49.58124542236328,
      "learning_rate": 1.1047812817904376e-05,
      "loss": 1.6582,
      "step": 76580
    },
    {
      "epoch": 38.957273652085455,
      "grad_norm": 40.794925689697266,
      "learning_rate": 1.1042726347914548e-05,
      "loss": 1.592,
      "step": 76590
    },
    {
      "epoch": 38.96236012207528,
      "grad_norm": 60.369075775146484,
      "learning_rate": 1.103763987792472e-05,
      "loss": 1.6274,
      "step": 76600
    },
    {
      "epoch": 38.96744659206511,
      "grad_norm": 47.6906623840332,
      "learning_rate": 1.1032553407934894e-05,
      "loss": 1.5959,
      "step": 76610
    },
    {
      "epoch": 38.972533062054936,
      "grad_norm": 42.15781021118164,
      "learning_rate": 1.1027466937945066e-05,
      "loss": 1.5732,
      "step": 76620
    },
    {
      "epoch": 38.97761953204476,
      "grad_norm": 41.30836486816406,
      "learning_rate": 1.102238046795524e-05,
      "loss": 1.6312,
      "step": 76630
    },
    {
      "epoch": 38.98270600203459,
      "grad_norm": 42.75885009765625,
      "learning_rate": 1.1017293997965412e-05,
      "loss": 1.6473,
      "step": 76640
    },
    {
      "epoch": 38.98779247202442,
      "grad_norm": 43.79951858520508,
      "learning_rate": 1.1012207527975586e-05,
      "loss": 1.5697,
      "step": 76650
    },
    {
      "epoch": 38.992878942014244,
      "grad_norm": 39.81681823730469,
      "learning_rate": 1.1007121057985759e-05,
      "loss": 1.6001,
      "step": 76660
    },
    {
      "epoch": 38.99796541200407,
      "grad_norm": 33.656044006347656,
      "learning_rate": 1.100203458799593e-05,
      "loss": 1.5459,
      "step": 76670
    },
    {
      "epoch": 39.0,
      "eval_loss": 5.025351524353027,
      "eval_runtime": 2.635,
      "eval_samples_per_second": 1053.147,
      "eval_steps_per_second": 131.691,
      "step": 76674
    },
    {
      "epoch": 39.0030518819939,
      "grad_norm": 40.75369644165039,
      "learning_rate": 1.0996948118006105e-05,
      "loss": 1.6218,
      "step": 76680
    },
    {
      "epoch": 39.008138351983725,
      "grad_norm": 47.82950210571289,
      "learning_rate": 1.0991861648016277e-05,
      "loss": 1.6698,
      "step": 76690
    },
    {
      "epoch": 39.01322482197355,
      "grad_norm": 41.208072662353516,
      "learning_rate": 1.098677517802645e-05,
      "loss": 1.5962,
      "step": 76700
    },
    {
      "epoch": 39.01831129196338,
      "grad_norm": 37.74123001098633,
      "learning_rate": 1.0981688708036624e-05,
      "loss": 1.6209,
      "step": 76710
    },
    {
      "epoch": 39.023397761953206,
      "grad_norm": 42.56153106689453,
      "learning_rate": 1.0976602238046795e-05,
      "loss": 1.5235,
      "step": 76720
    },
    {
      "epoch": 39.02848423194303,
      "grad_norm": 41.610538482666016,
      "learning_rate": 1.097151576805697e-05,
      "loss": 1.6079,
      "step": 76730
    },
    {
      "epoch": 39.03357070193286,
      "grad_norm": 30.826784133911133,
      "learning_rate": 1.0966429298067142e-05,
      "loss": 1.6122,
      "step": 76740
    },
    {
      "epoch": 39.03865717192269,
      "grad_norm": 36.07972717285156,
      "learning_rate": 1.0961342828077315e-05,
      "loss": 1.5521,
      "step": 76750
    },
    {
      "epoch": 39.04374364191251,
      "grad_norm": 38.564369201660156,
      "learning_rate": 1.0956256358087488e-05,
      "loss": 1.5855,
      "step": 76760
    },
    {
      "epoch": 39.04883011190234,
      "grad_norm": 34.65121078491211,
      "learning_rate": 1.0951169888097662e-05,
      "loss": 1.6094,
      "step": 76770
    },
    {
      "epoch": 39.05391658189217,
      "grad_norm": 58.203773498535156,
      "learning_rate": 1.0946083418107833e-05,
      "loss": 1.6655,
      "step": 76780
    },
    {
      "epoch": 39.059003051881994,
      "grad_norm": 46.30563735961914,
      "learning_rate": 1.0940996948118006e-05,
      "loss": 1.4508,
      "step": 76790
    },
    {
      "epoch": 39.06408952187182,
      "grad_norm": 32.52949142456055,
      "learning_rate": 1.093591047812818e-05,
      "loss": 1.56,
      "step": 76800
    },
    {
      "epoch": 39.06917599186165,
      "grad_norm": 49.95745849609375,
      "learning_rate": 1.0930824008138353e-05,
      "loss": 1.5411,
      "step": 76810
    },
    {
      "epoch": 39.074262461851475,
      "grad_norm": 41.899776458740234,
      "learning_rate": 1.0925737538148526e-05,
      "loss": 1.5985,
      "step": 76820
    },
    {
      "epoch": 39.0793489318413,
      "grad_norm": 48.3337516784668,
      "learning_rate": 1.0920651068158698e-05,
      "loss": 1.5451,
      "step": 76830
    },
    {
      "epoch": 39.08443540183113,
      "grad_norm": 46.32369613647461,
      "learning_rate": 1.0915564598168871e-05,
      "loss": 1.5668,
      "step": 76840
    },
    {
      "epoch": 39.089521871820956,
      "grad_norm": 41.09483337402344,
      "learning_rate": 1.0910478128179044e-05,
      "loss": 1.5515,
      "step": 76850
    },
    {
      "epoch": 39.09460834181078,
      "grad_norm": 41.58115768432617,
      "learning_rate": 1.0905391658189216e-05,
      "loss": 1.5716,
      "step": 76860
    },
    {
      "epoch": 39.09969481180061,
      "grad_norm": 36.171905517578125,
      "learning_rate": 1.0900305188199391e-05,
      "loss": 1.6296,
      "step": 76870
    },
    {
      "epoch": 39.10478128179044,
      "grad_norm": 43.40092086791992,
      "learning_rate": 1.0895218718209563e-05,
      "loss": 1.6075,
      "step": 76880
    },
    {
      "epoch": 39.109867751780264,
      "grad_norm": 51.03685760498047,
      "learning_rate": 1.0890132248219736e-05,
      "loss": 1.617,
      "step": 76890
    },
    {
      "epoch": 39.11495422177009,
      "grad_norm": 38.293888092041016,
      "learning_rate": 1.0885045778229909e-05,
      "loss": 1.5516,
      "step": 76900
    },
    {
      "epoch": 39.12004069175992,
      "grad_norm": 38.16972732543945,
      "learning_rate": 1.087995930824008e-05,
      "loss": 1.6283,
      "step": 76910
    },
    {
      "epoch": 39.125127161749745,
      "grad_norm": 36.50310516357422,
      "learning_rate": 1.0874872838250256e-05,
      "loss": 1.6139,
      "step": 76920
    },
    {
      "epoch": 39.13021363173957,
      "grad_norm": 40.665287017822266,
      "learning_rate": 1.0869786368260427e-05,
      "loss": 1.5196,
      "step": 76930
    },
    {
      "epoch": 39.1353001017294,
      "grad_norm": 43.905208587646484,
      "learning_rate": 1.08646998982706e-05,
      "loss": 1.5668,
      "step": 76940
    },
    {
      "epoch": 39.140386571719226,
      "grad_norm": 41.441890716552734,
      "learning_rate": 1.0859613428280774e-05,
      "loss": 1.54,
      "step": 76950
    },
    {
      "epoch": 39.14547304170905,
      "grad_norm": 44.55491638183594,
      "learning_rate": 1.0854526958290945e-05,
      "loss": 1.6061,
      "step": 76960
    },
    {
      "epoch": 39.15055951169888,
      "grad_norm": 39.69295120239258,
      "learning_rate": 1.084944048830112e-05,
      "loss": 1.5352,
      "step": 76970
    },
    {
      "epoch": 39.15564598168871,
      "grad_norm": 30.74776268005371,
      "learning_rate": 1.0844354018311292e-05,
      "loss": 1.5617,
      "step": 76980
    },
    {
      "epoch": 39.160732451678534,
      "grad_norm": 40.034446716308594,
      "learning_rate": 1.0839267548321465e-05,
      "loss": 1.5332,
      "step": 76990
    },
    {
      "epoch": 39.16581892166836,
      "grad_norm": 44.49816131591797,
      "learning_rate": 1.0834181078331639e-05,
      "loss": 1.6192,
      "step": 77000
    },
    {
      "epoch": 39.17090539165819,
      "grad_norm": 40.52965545654297,
      "learning_rate": 1.0829094608341812e-05,
      "loss": 1.5391,
      "step": 77010
    },
    {
      "epoch": 39.175991861648015,
      "grad_norm": 45.45493698120117,
      "learning_rate": 1.0824008138351985e-05,
      "loss": 1.5995,
      "step": 77020
    },
    {
      "epoch": 39.18107833163784,
      "grad_norm": 51.2938232421875,
      "learning_rate": 1.0818921668362157e-05,
      "loss": 1.5805,
      "step": 77030
    },
    {
      "epoch": 39.18616480162767,
      "grad_norm": 46.42593002319336,
      "learning_rate": 1.081383519837233e-05,
      "loss": 1.5772,
      "step": 77040
    },
    {
      "epoch": 39.191251271617496,
      "grad_norm": 40.237998962402344,
      "learning_rate": 1.0808748728382503e-05,
      "loss": 1.5809,
      "step": 77050
    },
    {
      "epoch": 39.19633774160732,
      "grad_norm": 40.92538070678711,
      "learning_rate": 1.0803662258392677e-05,
      "loss": 1.5831,
      "step": 77060
    },
    {
      "epoch": 39.20142421159715,
      "grad_norm": 42.12969970703125,
      "learning_rate": 1.079857578840285e-05,
      "loss": 1.5826,
      "step": 77070
    },
    {
      "epoch": 39.20651068158698,
      "grad_norm": 41.54624557495117,
      "learning_rate": 1.0793489318413021e-05,
      "loss": 1.6786,
      "step": 77080
    },
    {
      "epoch": 39.211597151576804,
      "grad_norm": 39.78565216064453,
      "learning_rate": 1.0788402848423195e-05,
      "loss": 1.5754,
      "step": 77090
    },
    {
      "epoch": 39.21668362156663,
      "grad_norm": 39.692623138427734,
      "learning_rate": 1.0783316378433368e-05,
      "loss": 1.5529,
      "step": 77100
    },
    {
      "epoch": 39.22177009155646,
      "grad_norm": 44.93321228027344,
      "learning_rate": 1.0778229908443541e-05,
      "loss": 1.5635,
      "step": 77110
    },
    {
      "epoch": 39.226856561546285,
      "grad_norm": 36.41349411010742,
      "learning_rate": 1.0773143438453713e-05,
      "loss": 1.5431,
      "step": 77120
    },
    {
      "epoch": 39.23194303153611,
      "grad_norm": 41.76921463012695,
      "learning_rate": 1.0768056968463886e-05,
      "loss": 1.6935,
      "step": 77130
    },
    {
      "epoch": 39.23702950152594,
      "grad_norm": 46.02833557128906,
      "learning_rate": 1.076297049847406e-05,
      "loss": 1.6028,
      "step": 77140
    },
    {
      "epoch": 39.242115971515766,
      "grad_norm": 40.60550308227539,
      "learning_rate": 1.0757884028484233e-05,
      "loss": 1.6155,
      "step": 77150
    },
    {
      "epoch": 39.24720244150559,
      "grad_norm": 33.19868850708008,
      "learning_rate": 1.0752797558494406e-05,
      "loss": 1.6177,
      "step": 77160
    },
    {
      "epoch": 39.25228891149542,
      "grad_norm": 49.07931137084961,
      "learning_rate": 1.0747711088504578e-05,
      "loss": 1.6182,
      "step": 77170
    },
    {
      "epoch": 39.25737538148525,
      "grad_norm": 34.275856018066406,
      "learning_rate": 1.074262461851475e-05,
      "loss": 1.5406,
      "step": 77180
    },
    {
      "epoch": 39.262461851475074,
      "grad_norm": 34.18421936035156,
      "learning_rate": 1.0737538148524924e-05,
      "loss": 1.5449,
      "step": 77190
    },
    {
      "epoch": 39.2675483214649,
      "grad_norm": 35.72234344482422,
      "learning_rate": 1.0732451678535096e-05,
      "loss": 1.5757,
      "step": 77200
    },
    {
      "epoch": 39.27263479145473,
      "grad_norm": 39.416725158691406,
      "learning_rate": 1.072736520854527e-05,
      "loss": 1.5665,
      "step": 77210
    },
    {
      "epoch": 39.277721261444555,
      "grad_norm": 40.51279830932617,
      "learning_rate": 1.0722278738555442e-05,
      "loss": 1.6267,
      "step": 77220
    },
    {
      "epoch": 39.28280773143438,
      "grad_norm": 40.4322395324707,
      "learning_rate": 1.0717192268565617e-05,
      "loss": 1.5234,
      "step": 77230
    },
    {
      "epoch": 39.28789420142421,
      "grad_norm": 39.33919906616211,
      "learning_rate": 1.0712105798575789e-05,
      "loss": 1.3969,
      "step": 77240
    },
    {
      "epoch": 39.292980671414035,
      "grad_norm": 43.94609832763672,
      "learning_rate": 1.0707019328585962e-05,
      "loss": 1.5252,
      "step": 77250
    },
    {
      "epoch": 39.29806714140386,
      "grad_norm": 36.67831802368164,
      "learning_rate": 1.0701932858596135e-05,
      "loss": 1.5952,
      "step": 77260
    },
    {
      "epoch": 39.30315361139369,
      "grad_norm": 47.511451721191406,
      "learning_rate": 1.0696846388606307e-05,
      "loss": 1.6001,
      "step": 77270
    },
    {
      "epoch": 39.308240081383516,
      "grad_norm": 34.64072036743164,
      "learning_rate": 1.0691759918616482e-05,
      "loss": 1.5336,
      "step": 77280
    },
    {
      "epoch": 39.31332655137334,
      "grad_norm": 36.52079772949219,
      "learning_rate": 1.0686673448626654e-05,
      "loss": 1.5366,
      "step": 77290
    },
    {
      "epoch": 39.31841302136317,
      "grad_norm": 39.23582458496094,
      "learning_rate": 1.0681586978636827e-05,
      "loss": 1.6499,
      "step": 77300
    },
    {
      "epoch": 39.323499491353004,
      "grad_norm": 51.181312561035156,
      "learning_rate": 1.0676500508647e-05,
      "loss": 1.5065,
      "step": 77310
    },
    {
      "epoch": 39.32858596134283,
      "grad_norm": 47.16530227661133,
      "learning_rate": 1.0671414038657172e-05,
      "loss": 1.5478,
      "step": 77320
    },
    {
      "epoch": 39.33367243133266,
      "grad_norm": 34.18317794799805,
      "learning_rate": 1.0666327568667345e-05,
      "loss": 1.5362,
      "step": 77330
    },
    {
      "epoch": 39.338758901322485,
      "grad_norm": 45.01045608520508,
      "learning_rate": 1.0661241098677518e-05,
      "loss": 1.5689,
      "step": 77340
    },
    {
      "epoch": 39.34384537131231,
      "grad_norm": 51.87807846069336,
      "learning_rate": 1.0656154628687692e-05,
      "loss": 1.5779,
      "step": 77350
    },
    {
      "epoch": 39.34893184130214,
      "grad_norm": 38.744808197021484,
      "learning_rate": 1.0651068158697865e-05,
      "loss": 1.5517,
      "step": 77360
    },
    {
      "epoch": 39.354018311291966,
      "grad_norm": 47.101966857910156,
      "learning_rate": 1.0645981688708036e-05,
      "loss": 1.5854,
      "step": 77370
    },
    {
      "epoch": 39.35910478128179,
      "grad_norm": 41.67083740234375,
      "learning_rate": 1.064089521871821e-05,
      "loss": 1.5056,
      "step": 77380
    },
    {
      "epoch": 39.36419125127162,
      "grad_norm": 41.0189094543457,
      "learning_rate": 1.0635808748728383e-05,
      "loss": 1.7084,
      "step": 77390
    },
    {
      "epoch": 39.36927772126145,
      "grad_norm": 39.74211120605469,
      "learning_rate": 1.0630722278738556e-05,
      "loss": 1.5933,
      "step": 77400
    },
    {
      "epoch": 39.374364191251274,
      "grad_norm": 40.778133392333984,
      "learning_rate": 1.062563580874873e-05,
      "loss": 1.5871,
      "step": 77410
    },
    {
      "epoch": 39.3794506612411,
      "grad_norm": 40.6410026550293,
      "learning_rate": 1.0620549338758901e-05,
      "loss": 1.6215,
      "step": 77420
    },
    {
      "epoch": 39.38453713123093,
      "grad_norm": 43.398006439208984,
      "learning_rate": 1.0615462868769074e-05,
      "loss": 1.5373,
      "step": 77430
    },
    {
      "epoch": 39.389623601220755,
      "grad_norm": 42.08988571166992,
      "learning_rate": 1.0610376398779248e-05,
      "loss": 1.6583,
      "step": 77440
    },
    {
      "epoch": 39.39471007121058,
      "grad_norm": 46.94026565551758,
      "learning_rate": 1.0605289928789421e-05,
      "loss": 1.6052,
      "step": 77450
    },
    {
      "epoch": 39.39979654120041,
      "grad_norm": 45.80576705932617,
      "learning_rate": 1.0600203458799593e-05,
      "loss": 1.6502,
      "step": 77460
    },
    {
      "epoch": 39.404883011190236,
      "grad_norm": 43.6396369934082,
      "learning_rate": 1.0595116988809768e-05,
      "loss": 1.5877,
      "step": 77470
    },
    {
      "epoch": 39.40996948118006,
      "grad_norm": 44.919986724853516,
      "learning_rate": 1.0590030518819939e-05,
      "loss": 1.5743,
      "step": 77480
    },
    {
      "epoch": 39.41505595116989,
      "grad_norm": 35.430397033691406,
      "learning_rate": 1.0584944048830112e-05,
      "loss": 1.5369,
      "step": 77490
    },
    {
      "epoch": 39.42014242115972,
      "grad_norm": 43.11275863647461,
      "learning_rate": 1.0579857578840286e-05,
      "loss": 1.5654,
      "step": 77500
    },
    {
      "epoch": 39.425228891149544,
      "grad_norm": 43.316341400146484,
      "learning_rate": 1.0574771108850457e-05,
      "loss": 1.5386,
      "step": 77510
    },
    {
      "epoch": 39.43031536113937,
      "grad_norm": 37.284358978271484,
      "learning_rate": 1.0569684638860632e-05,
      "loss": 1.5995,
      "step": 77520
    },
    {
      "epoch": 39.4354018311292,
      "grad_norm": 46.38119888305664,
      "learning_rate": 1.0564598168870804e-05,
      "loss": 1.6583,
      "step": 77530
    },
    {
      "epoch": 39.440488301119025,
      "grad_norm": 38.02951431274414,
      "learning_rate": 1.0559511698880977e-05,
      "loss": 1.5506,
      "step": 77540
    },
    {
      "epoch": 39.44557477110885,
      "grad_norm": 36.43539047241211,
      "learning_rate": 1.055442522889115e-05,
      "loss": 1.6022,
      "step": 77550
    },
    {
      "epoch": 39.45066124109868,
      "grad_norm": 36.51170349121094,
      "learning_rate": 1.0549338758901322e-05,
      "loss": 1.5258,
      "step": 77560
    },
    {
      "epoch": 39.455747711088506,
      "grad_norm": 42.95235061645508,
      "learning_rate": 1.0544252288911497e-05,
      "loss": 1.6685,
      "step": 77570
    },
    {
      "epoch": 39.46083418107833,
      "grad_norm": 49.22543716430664,
      "learning_rate": 1.0539165818921669e-05,
      "loss": 1.5561,
      "step": 77580
    },
    {
      "epoch": 39.46592065106816,
      "grad_norm": 36.69774627685547,
      "learning_rate": 1.0534079348931842e-05,
      "loss": 1.5946,
      "step": 77590
    },
    {
      "epoch": 39.47100712105799,
      "grad_norm": 39.881412506103516,
      "learning_rate": 1.0528992878942015e-05,
      "loss": 1.6146,
      "step": 77600
    },
    {
      "epoch": 39.476093591047814,
      "grad_norm": 32.97622299194336,
      "learning_rate": 1.0523906408952187e-05,
      "loss": 1.5909,
      "step": 77610
    },
    {
      "epoch": 39.48118006103764,
      "grad_norm": 30.588014602661133,
      "learning_rate": 1.0518819938962362e-05,
      "loss": 1.5101,
      "step": 77620
    },
    {
      "epoch": 39.48626653102747,
      "grad_norm": 42.89614486694336,
      "learning_rate": 1.0513733468972533e-05,
      "loss": 1.6479,
      "step": 77630
    },
    {
      "epoch": 39.491353001017295,
      "grad_norm": 44.255027770996094,
      "learning_rate": 1.0508646998982707e-05,
      "loss": 1.5155,
      "step": 77640
    },
    {
      "epoch": 39.49643947100712,
      "grad_norm": 34.02046585083008,
      "learning_rate": 1.050356052899288e-05,
      "loss": 1.5789,
      "step": 77650
    },
    {
      "epoch": 39.50152594099695,
      "grad_norm": 37.479652404785156,
      "learning_rate": 1.0498474059003053e-05,
      "loss": 1.5712,
      "step": 77660
    },
    {
      "epoch": 39.506612410986776,
      "grad_norm": 40.278018951416016,
      "learning_rate": 1.0493387589013225e-05,
      "loss": 1.5563,
      "step": 77670
    },
    {
      "epoch": 39.5116988809766,
      "grad_norm": 47.61825180053711,
      "learning_rate": 1.0488301119023398e-05,
      "loss": 1.6106,
      "step": 77680
    },
    {
      "epoch": 39.51678535096643,
      "grad_norm": 46.00697708129883,
      "learning_rate": 1.0483214649033571e-05,
      "loss": 1.6398,
      "step": 77690
    },
    {
      "epoch": 39.52187182095626,
      "grad_norm": 37.81709671020508,
      "learning_rate": 1.0478128179043745e-05,
      "loss": 1.6351,
      "step": 77700
    },
    {
      "epoch": 39.526958290946084,
      "grad_norm": 45.82717514038086,
      "learning_rate": 1.0473041709053918e-05,
      "loss": 1.585,
      "step": 77710
    },
    {
      "epoch": 39.53204476093591,
      "grad_norm": 34.469146728515625,
      "learning_rate": 1.046795523906409e-05,
      "loss": 1.6456,
      "step": 77720
    },
    {
      "epoch": 39.53713123092574,
      "grad_norm": 39.734676361083984,
      "learning_rate": 1.0462868769074263e-05,
      "loss": 1.5996,
      "step": 77730
    },
    {
      "epoch": 39.542217700915565,
      "grad_norm": 36.99065399169922,
      "learning_rate": 1.0457782299084436e-05,
      "loss": 1.6154,
      "step": 77740
    },
    {
      "epoch": 39.54730417090539,
      "grad_norm": 45.792938232421875,
      "learning_rate": 1.045269582909461e-05,
      "loss": 1.5807,
      "step": 77750
    },
    {
      "epoch": 39.55239064089522,
      "grad_norm": 45.4600830078125,
      "learning_rate": 1.0447609359104783e-05,
      "loss": 1.5626,
      "step": 77760
    },
    {
      "epoch": 39.557477110885046,
      "grad_norm": 40.89320755004883,
      "learning_rate": 1.0442522889114954e-05,
      "loss": 1.6222,
      "step": 77770
    },
    {
      "epoch": 39.56256358087487,
      "grad_norm": 48.41841506958008,
      "learning_rate": 1.0437436419125127e-05,
      "loss": 1.6538,
      "step": 77780
    },
    {
      "epoch": 39.5676500508647,
      "grad_norm": 41.478084564208984,
      "learning_rate": 1.04323499491353e-05,
      "loss": 1.5834,
      "step": 77790
    },
    {
      "epoch": 39.57273652085453,
      "grad_norm": 44.268245697021484,
      "learning_rate": 1.0427263479145472e-05,
      "loss": 1.5824,
      "step": 77800
    },
    {
      "epoch": 39.57782299084435,
      "grad_norm": 46.4952507019043,
      "learning_rate": 1.0422177009155647e-05,
      "loss": 1.5513,
      "step": 77810
    },
    {
      "epoch": 39.58290946083418,
      "grad_norm": 52.486839294433594,
      "learning_rate": 1.0417090539165819e-05,
      "loss": 1.5811,
      "step": 77820
    },
    {
      "epoch": 39.58799593082401,
      "grad_norm": 39.929996490478516,
      "learning_rate": 1.0412004069175992e-05,
      "loss": 1.6299,
      "step": 77830
    },
    {
      "epoch": 39.593082400813834,
      "grad_norm": 41.98783874511719,
      "learning_rate": 1.0406917599186165e-05,
      "loss": 1.5617,
      "step": 77840
    },
    {
      "epoch": 39.59816887080366,
      "grad_norm": 32.8205680847168,
      "learning_rate": 1.0401831129196337e-05,
      "loss": 1.5324,
      "step": 77850
    },
    {
      "epoch": 39.60325534079349,
      "grad_norm": 39.99701690673828,
      "learning_rate": 1.0396744659206512e-05,
      "loss": 1.6364,
      "step": 77860
    },
    {
      "epoch": 39.608341810783315,
      "grad_norm": 33.50992965698242,
      "learning_rate": 1.0391658189216684e-05,
      "loss": 1.591,
      "step": 77870
    },
    {
      "epoch": 39.61342828077314,
      "grad_norm": 42.34897994995117,
      "learning_rate": 1.0386571719226859e-05,
      "loss": 1.5865,
      "step": 77880
    },
    {
      "epoch": 39.61851475076297,
      "grad_norm": 36.11015319824219,
      "learning_rate": 1.038148524923703e-05,
      "loss": 1.5463,
      "step": 77890
    },
    {
      "epoch": 39.623601220752796,
      "grad_norm": 47.770118713378906,
      "learning_rate": 1.0376398779247203e-05,
      "loss": 1.6201,
      "step": 77900
    },
    {
      "epoch": 39.62868769074262,
      "grad_norm": 32.520267486572266,
      "learning_rate": 1.0371312309257377e-05,
      "loss": 1.643,
      "step": 77910
    },
    {
      "epoch": 39.63377416073245,
      "grad_norm": 44.767086029052734,
      "learning_rate": 1.0366225839267548e-05,
      "loss": 1.5465,
      "step": 77920
    },
    {
      "epoch": 39.63886063072228,
      "grad_norm": 37.455657958984375,
      "learning_rate": 1.0361139369277722e-05,
      "loss": 1.5672,
      "step": 77930
    },
    {
      "epoch": 39.643947100712104,
      "grad_norm": 36.32118225097656,
      "learning_rate": 1.0356052899287895e-05,
      "loss": 1.5692,
      "step": 77940
    },
    {
      "epoch": 39.64903357070193,
      "grad_norm": 40.5372314453125,
      "learning_rate": 1.0350966429298068e-05,
      "loss": 1.6273,
      "step": 77950
    },
    {
      "epoch": 39.65412004069176,
      "grad_norm": 38.830841064453125,
      "learning_rate": 1.0345879959308241e-05,
      "loss": 1.6926,
      "step": 77960
    },
    {
      "epoch": 39.659206510681585,
      "grad_norm": 38.338470458984375,
      "learning_rate": 1.0340793489318413e-05,
      "loss": 1.5653,
      "step": 77970
    },
    {
      "epoch": 39.66429298067141,
      "grad_norm": 40.240379333496094,
      "learning_rate": 1.0335707019328586e-05,
      "loss": 1.6026,
      "step": 77980
    },
    {
      "epoch": 39.66937945066124,
      "grad_norm": 47.619876861572266,
      "learning_rate": 1.033062054933876e-05,
      "loss": 1.619,
      "step": 77990
    },
    {
      "epoch": 39.674465920651066,
      "grad_norm": 36.919925689697266,
      "learning_rate": 1.0325534079348933e-05,
      "loss": 1.6094,
      "step": 78000
    },
    {
      "epoch": 39.67955239064089,
      "grad_norm": 40.75956726074219,
      "learning_rate": 1.0320447609359104e-05,
      "loss": 1.6062,
      "step": 78010
    },
    {
      "epoch": 39.68463886063072,
      "grad_norm": 36.088253021240234,
      "learning_rate": 1.0315361139369278e-05,
      "loss": 1.5919,
      "step": 78020
    },
    {
      "epoch": 39.68972533062055,
      "grad_norm": 42.974830627441406,
      "learning_rate": 1.0310274669379451e-05,
      "loss": 1.6066,
      "step": 78030
    },
    {
      "epoch": 39.694811800610374,
      "grad_norm": 41.94123840332031,
      "learning_rate": 1.0305188199389624e-05,
      "loss": 1.5847,
      "step": 78040
    },
    {
      "epoch": 39.6998982706002,
      "grad_norm": 49.3862190246582,
      "learning_rate": 1.0300101729399798e-05,
      "loss": 1.5732,
      "step": 78050
    },
    {
      "epoch": 39.70498474059003,
      "grad_norm": 48.472225189208984,
      "learning_rate": 1.0295015259409969e-05,
      "loss": 1.5354,
      "step": 78060
    },
    {
      "epoch": 39.710071210579855,
      "grad_norm": 34.04933547973633,
      "learning_rate": 1.0289928789420142e-05,
      "loss": 1.5284,
      "step": 78070
    },
    {
      "epoch": 39.71515768056968,
      "grad_norm": 40.00394821166992,
      "learning_rate": 1.0284842319430316e-05,
      "loss": 1.5843,
      "step": 78080
    },
    {
      "epoch": 39.72024415055951,
      "grad_norm": 42.19152069091797,
      "learning_rate": 1.0279755849440489e-05,
      "loss": 1.5555,
      "step": 78090
    },
    {
      "epoch": 39.725330620549336,
      "grad_norm": 39.36515426635742,
      "learning_rate": 1.0274669379450662e-05,
      "loss": 1.5587,
      "step": 78100
    },
    {
      "epoch": 39.73041709053916,
      "grad_norm": 34.63554382324219,
      "learning_rate": 1.0269582909460834e-05,
      "loss": 1.5193,
      "step": 78110
    },
    {
      "epoch": 39.73550356052899,
      "grad_norm": 42.84455490112305,
      "learning_rate": 1.0264496439471009e-05,
      "loss": 1.5403,
      "step": 78120
    },
    {
      "epoch": 39.74059003051882,
      "grad_norm": 35.42494583129883,
      "learning_rate": 1.025940996948118e-05,
      "loss": 1.6204,
      "step": 78130
    },
    {
      "epoch": 39.745676500508644,
      "grad_norm": 44.03135299682617,
      "learning_rate": 1.0254323499491354e-05,
      "loss": 1.5409,
      "step": 78140
    },
    {
      "epoch": 39.75076297049847,
      "grad_norm": 29.016376495361328,
      "learning_rate": 1.0249237029501527e-05,
      "loss": 1.5746,
      "step": 78150
    },
    {
      "epoch": 39.7558494404883,
      "grad_norm": 45.84712219238281,
      "learning_rate": 1.0244150559511699e-05,
      "loss": 1.5546,
      "step": 78160
    },
    {
      "epoch": 39.760935910478125,
      "grad_norm": 41.40532302856445,
      "learning_rate": 1.0239064089521874e-05,
      "loss": 1.5621,
      "step": 78170
    },
    {
      "epoch": 39.76602238046795,
      "grad_norm": 48.41156768798828,
      "learning_rate": 1.0233977619532045e-05,
      "loss": 1.5449,
      "step": 78180
    },
    {
      "epoch": 39.77110885045778,
      "grad_norm": 51.21541976928711,
      "learning_rate": 1.0228891149542218e-05,
      "loss": 1.6335,
      "step": 78190
    },
    {
      "epoch": 39.77619532044761,
      "grad_norm": 45.139366149902344,
      "learning_rate": 1.0223804679552392e-05,
      "loss": 1.624,
      "step": 78200
    },
    {
      "epoch": 39.78128179043744,
      "grad_norm": 28.591684341430664,
      "learning_rate": 1.0218718209562563e-05,
      "loss": 1.6968,
      "step": 78210
    },
    {
      "epoch": 39.78636826042727,
      "grad_norm": 43.89127731323242,
      "learning_rate": 1.0213631739572738e-05,
      "loss": 1.6383,
      "step": 78220
    },
    {
      "epoch": 39.791454730417094,
      "grad_norm": 47.39406967163086,
      "learning_rate": 1.020854526958291e-05,
      "loss": 1.5726,
      "step": 78230
    },
    {
      "epoch": 39.79654120040692,
      "grad_norm": 40.621604919433594,
      "learning_rate": 1.0203458799593083e-05,
      "loss": 1.6176,
      "step": 78240
    },
    {
      "epoch": 39.80162767039675,
      "grad_norm": 46.65509033203125,
      "learning_rate": 1.0198372329603256e-05,
      "loss": 1.5552,
      "step": 78250
    },
    {
      "epoch": 39.806714140386575,
      "grad_norm": 47.970001220703125,
      "learning_rate": 1.0193285859613428e-05,
      "loss": 1.4734,
      "step": 78260
    },
    {
      "epoch": 39.8118006103764,
      "grad_norm": 45.190494537353516,
      "learning_rate": 1.0188199389623601e-05,
      "loss": 1.6086,
      "step": 78270
    },
    {
      "epoch": 39.81688708036623,
      "grad_norm": 38.08155059814453,
      "learning_rate": 1.0183112919633775e-05,
      "loss": 1.5372,
      "step": 78280
    },
    {
      "epoch": 39.821973550356056,
      "grad_norm": 29.075410842895508,
      "learning_rate": 1.0178026449643948e-05,
      "loss": 1.5848,
      "step": 78290
    },
    {
      "epoch": 39.82706002034588,
      "grad_norm": 40.30674743652344,
      "learning_rate": 1.0172939979654121e-05,
      "loss": 1.5638,
      "step": 78300
    },
    {
      "epoch": 39.83214649033571,
      "grad_norm": 51.419029235839844,
      "learning_rate": 1.0167853509664293e-05,
      "loss": 1.5834,
      "step": 78310
    },
    {
      "epoch": 39.83723296032554,
      "grad_norm": 54.758270263671875,
      "learning_rate": 1.0162767039674466e-05,
      "loss": 1.6339,
      "step": 78320
    },
    {
      "epoch": 39.842319430315364,
      "grad_norm": 32.353912353515625,
      "learning_rate": 1.015768056968464e-05,
      "loss": 1.4834,
      "step": 78330
    },
    {
      "epoch": 39.84740590030519,
      "grad_norm": 38.801998138427734,
      "learning_rate": 1.0152594099694813e-05,
      "loss": 1.5901,
      "step": 78340
    },
    {
      "epoch": 39.85249237029502,
      "grad_norm": 39.69216537475586,
      "learning_rate": 1.0147507629704986e-05,
      "loss": 1.5387,
      "step": 78350
    },
    {
      "epoch": 39.857578840284845,
      "grad_norm": 44.965049743652344,
      "learning_rate": 1.0142421159715159e-05,
      "loss": 1.5843,
      "step": 78360
    },
    {
      "epoch": 39.86266531027467,
      "grad_norm": 46.64297103881836,
      "learning_rate": 1.013733468972533e-05,
      "loss": 1.5551,
      "step": 78370
    },
    {
      "epoch": 39.8677517802645,
      "grad_norm": 37.812931060791016,
      "learning_rate": 1.0132248219735504e-05,
      "loss": 1.5038,
      "step": 78380
    },
    {
      "epoch": 39.872838250254325,
      "grad_norm": 51.008506774902344,
      "learning_rate": 1.0127161749745677e-05,
      "loss": 1.5065,
      "step": 78390
    },
    {
      "epoch": 39.87792472024415,
      "grad_norm": 40.01517868041992,
      "learning_rate": 1.0122075279755849e-05,
      "loss": 1.5198,
      "step": 78400
    },
    {
      "epoch": 39.88301119023398,
      "grad_norm": 47.05185317993164,
      "learning_rate": 1.0116988809766024e-05,
      "loss": 1.5796,
      "step": 78410
    },
    {
      "epoch": 39.888097660223806,
      "grad_norm": 38.879127502441406,
      "learning_rate": 1.0111902339776195e-05,
      "loss": 1.6124,
      "step": 78420
    },
    {
      "epoch": 39.89318413021363,
      "grad_norm": 35.02560806274414,
      "learning_rate": 1.0106815869786369e-05,
      "loss": 1.5917,
      "step": 78430
    },
    {
      "epoch": 39.89827060020346,
      "grad_norm": 41.70290756225586,
      "learning_rate": 1.0101729399796542e-05,
      "loss": 1.6026,
      "step": 78440
    },
    {
      "epoch": 39.90335707019329,
      "grad_norm": 33.577049255371094,
      "learning_rate": 1.0096642929806714e-05,
      "loss": 1.5986,
      "step": 78450
    },
    {
      "epoch": 39.908443540183114,
      "grad_norm": 39.20917892456055,
      "learning_rate": 1.0091556459816889e-05,
      "loss": 1.6167,
      "step": 78460
    },
    {
      "epoch": 39.91353001017294,
      "grad_norm": 53.11636734008789,
      "learning_rate": 1.008646998982706e-05,
      "loss": 1.683,
      "step": 78470
    },
    {
      "epoch": 39.91861648016277,
      "grad_norm": 42.34482192993164,
      "learning_rate": 1.0081383519837233e-05,
      "loss": 1.5867,
      "step": 78480
    },
    {
      "epoch": 39.923702950152595,
      "grad_norm": 47.62617874145508,
      "learning_rate": 1.0076297049847407e-05,
      "loss": 1.5353,
      "step": 78490
    },
    {
      "epoch": 39.92878942014242,
      "grad_norm": 49.10483932495117,
      "learning_rate": 1.0071210579857578e-05,
      "loss": 1.6411,
      "step": 78500
    },
    {
      "epoch": 39.93387589013225,
      "grad_norm": 40.702484130859375,
      "learning_rate": 1.0066124109867753e-05,
      "loss": 1.5915,
      "step": 78510
    },
    {
      "epoch": 39.938962360122076,
      "grad_norm": 52.37916564941406,
      "learning_rate": 1.0061037639877925e-05,
      "loss": 1.5011,
      "step": 78520
    },
    {
      "epoch": 39.9440488301119,
      "grad_norm": 36.80418014526367,
      "learning_rate": 1.0055951169888098e-05,
      "loss": 1.6387,
      "step": 78530
    },
    {
      "epoch": 39.94913530010173,
      "grad_norm": 35.748695373535156,
      "learning_rate": 1.0050864699898271e-05,
      "loss": 1.669,
      "step": 78540
    },
    {
      "epoch": 39.95422177009156,
      "grad_norm": 40.004371643066406,
      "learning_rate": 1.0045778229908443e-05,
      "loss": 1.5499,
      "step": 78550
    },
    {
      "epoch": 39.959308240081384,
      "grad_norm": 35.74735641479492,
      "learning_rate": 1.0040691759918618e-05,
      "loss": 1.4845,
      "step": 78560
    },
    {
      "epoch": 39.96439471007121,
      "grad_norm": 39.24626541137695,
      "learning_rate": 1.003560528992879e-05,
      "loss": 1.5561,
      "step": 78570
    },
    {
      "epoch": 39.96948118006104,
      "grad_norm": 38.04024887084961,
      "learning_rate": 1.0030518819938963e-05,
      "loss": 1.6061,
      "step": 78580
    },
    {
      "epoch": 39.974567650050865,
      "grad_norm": 58.326175689697266,
      "learning_rate": 1.0025432349949136e-05,
      "loss": 1.6685,
      "step": 78590
    },
    {
      "epoch": 39.97965412004069,
      "grad_norm": 39.99003219604492,
      "learning_rate": 1.002034587995931e-05,
      "loss": 1.5314,
      "step": 78600
    },
    {
      "epoch": 39.98474059003052,
      "grad_norm": 41.513031005859375,
      "learning_rate": 1.0015259409969481e-05,
      "loss": 1.5809,
      "step": 78610
    },
    {
      "epoch": 39.989827060020346,
      "grad_norm": 45.40687561035156,
      "learning_rate": 1.0010172939979654e-05,
      "loss": 1.6515,
      "step": 78620
    },
    {
      "epoch": 39.99491353001017,
      "grad_norm": 46.283382415771484,
      "learning_rate": 1.0005086469989828e-05,
      "loss": 1.5675,
      "step": 78630
    },
    {
      "epoch": 40.0,
      "grad_norm": 44.72971725463867,
      "learning_rate": 1e-05,
      "loss": 1.5615,
      "step": 78640
    },
    {
      "epoch": 40.0,
      "eval_loss": 5.043919563293457,
      "eval_runtime": 2.6161,
      "eval_samples_per_second": 1060.758,
      "eval_steps_per_second": 132.642,
      "step": 78640
    },
    {
      "epoch": 40.00508646998983,
      "grad_norm": 40.788291931152344,
      "learning_rate": 9.994913530010174e-06,
      "loss": 1.5687,
      "step": 78650
    },
    {
      "epoch": 40.010172939979654,
      "grad_norm": 48.71665573120117,
      "learning_rate": 9.989827060020346e-06,
      "loss": 1.538,
      "step": 78660
    },
    {
      "epoch": 40.01525940996948,
      "grad_norm": 39.77970504760742,
      "learning_rate": 9.984740590030519e-06,
      "loss": 1.5978,
      "step": 78670
    },
    {
      "epoch": 40.02034587995931,
      "grad_norm": 36.65420913696289,
      "learning_rate": 9.979654120040692e-06,
      "loss": 1.6134,
      "step": 78680
    },
    {
      "epoch": 40.025432349949135,
      "grad_norm": 48.06401062011719,
      "learning_rate": 9.974567650050866e-06,
      "loss": 1.4979,
      "step": 78690
    },
    {
      "epoch": 40.03051881993896,
      "grad_norm": 47.72291564941406,
      "learning_rate": 9.969481180061039e-06,
      "loss": 1.5928,
      "step": 78700
    },
    {
      "epoch": 40.03560528992879,
      "grad_norm": 47.25580596923828,
      "learning_rate": 9.96439471007121e-06,
      "loss": 1.4752,
      "step": 78710
    },
    {
      "epoch": 40.040691759918616,
      "grad_norm": 39.81888198852539,
      "learning_rate": 9.959308240081384e-06,
      "loss": 1.5469,
      "step": 78720
    },
    {
      "epoch": 40.04577822990844,
      "grad_norm": 56.26752471923828,
      "learning_rate": 9.954221770091557e-06,
      "loss": 1.6781,
      "step": 78730
    },
    {
      "epoch": 40.05086469989827,
      "grad_norm": 41.45897674560547,
      "learning_rate": 9.949135300101729e-06,
      "loss": 1.55,
      "step": 78740
    },
    {
      "epoch": 40.0559511698881,
      "grad_norm": 43.414222717285156,
      "learning_rate": 9.944048830111904e-06,
      "loss": 1.5639,
      "step": 78750
    },
    {
      "epoch": 40.061037639877924,
      "grad_norm": 50.656681060791016,
      "learning_rate": 9.938962360122075e-06,
      "loss": 1.5884,
      "step": 78760
    },
    {
      "epoch": 40.06612410986775,
      "grad_norm": 36.188663482666016,
      "learning_rate": 9.93387589013225e-06,
      "loss": 1.5492,
      "step": 78770
    },
    {
      "epoch": 40.07121057985758,
      "grad_norm": 39.79277801513672,
      "learning_rate": 9.928789420142422e-06,
      "loss": 1.5419,
      "step": 78780
    },
    {
      "epoch": 40.076297049847405,
      "grad_norm": 34.909423828125,
      "learning_rate": 9.923702950152593e-06,
      "loss": 1.4957,
      "step": 78790
    },
    {
      "epoch": 40.08138351983723,
      "grad_norm": 32.27947235107422,
      "learning_rate": 9.918616480162768e-06,
      "loss": 1.6301,
      "step": 78800
    },
    {
      "epoch": 40.08646998982706,
      "grad_norm": 38.62812423706055,
      "learning_rate": 9.91353001017294e-06,
      "loss": 1.6399,
      "step": 78810
    },
    {
      "epoch": 40.091556459816886,
      "grad_norm": 37.60779571533203,
      "learning_rate": 9.908443540183113e-06,
      "loss": 1.5017,
      "step": 78820
    },
    {
      "epoch": 40.09664292980671,
      "grad_norm": 37.70895004272461,
      "learning_rate": 9.903357070193286e-06,
      "loss": 1.5222,
      "step": 78830
    },
    {
      "epoch": 40.10172939979654,
      "grad_norm": 41.62104797363281,
      "learning_rate": 9.89827060020346e-06,
      "loss": 1.542,
      "step": 78840
    },
    {
      "epoch": 40.10681586978637,
      "grad_norm": 50.28086471557617,
      "learning_rate": 9.893184130213633e-06,
      "loss": 1.6223,
      "step": 78850
    },
    {
      "epoch": 40.111902339776194,
      "grad_norm": 48.72166061401367,
      "learning_rate": 9.888097660223805e-06,
      "loss": 1.5503,
      "step": 78860
    },
    {
      "epoch": 40.11698880976602,
      "grad_norm": 38.9811897277832,
      "learning_rate": 9.883011190233978e-06,
      "loss": 1.6375,
      "step": 78870
    },
    {
      "epoch": 40.12207527975585,
      "grad_norm": 40.771236419677734,
      "learning_rate": 9.877924720244151e-06,
      "loss": 1.6037,
      "step": 78880
    },
    {
      "epoch": 40.127161749745675,
      "grad_norm": 39.46018981933594,
      "learning_rate": 9.872838250254324e-06,
      "loss": 1.5326,
      "step": 78890
    },
    {
      "epoch": 40.1322482197355,
      "grad_norm": 36.951908111572266,
      "learning_rate": 9.867751780264498e-06,
      "loss": 1.6209,
      "step": 78900
    },
    {
      "epoch": 40.13733468972533,
      "grad_norm": 35.56687545776367,
      "learning_rate": 9.86266531027467e-06,
      "loss": 1.617,
      "step": 78910
    },
    {
      "epoch": 40.142421159715155,
      "grad_norm": 47.95799255371094,
      "learning_rate": 9.857578840284843e-06,
      "loss": 1.6649,
      "step": 78920
    },
    {
      "epoch": 40.14750762970498,
      "grad_norm": 40.354732513427734,
      "learning_rate": 9.852492370295016e-06,
      "loss": 1.6113,
      "step": 78930
    },
    {
      "epoch": 40.15259409969481,
      "grad_norm": 43.781410217285156,
      "learning_rate": 9.847405900305189e-06,
      "loss": 1.6158,
      "step": 78940
    },
    {
      "epoch": 40.157680569684636,
      "grad_norm": 35.4255256652832,
      "learning_rate": 9.84231943031536e-06,
      "loss": 1.6082,
      "step": 78950
    },
    {
      "epoch": 40.16276703967446,
      "grad_norm": 35.861427307128906,
      "learning_rate": 9.837232960325534e-06,
      "loss": 1.5992,
      "step": 78960
    },
    {
      "epoch": 40.16785350966429,
      "grad_norm": 37.01142120361328,
      "learning_rate": 9.832146490335707e-06,
      "loss": 1.6123,
      "step": 78970
    },
    {
      "epoch": 40.17293997965412,
      "grad_norm": 57.57001876831055,
      "learning_rate": 9.82706002034588e-06,
      "loss": 1.6335,
      "step": 78980
    },
    {
      "epoch": 40.178026449643944,
      "grad_norm": 37.98649597167969,
      "learning_rate": 9.821973550356054e-06,
      "loss": 1.5202,
      "step": 78990
    },
    {
      "epoch": 40.18311291963377,
      "grad_norm": 38.23836135864258,
      "learning_rate": 9.816887080366225e-06,
      "loss": 1.5546,
      "step": 79000
    },
    {
      "epoch": 40.1881993896236,
      "grad_norm": 39.452392578125,
      "learning_rate": 9.8118006103764e-06,
      "loss": 1.4422,
      "step": 79010
    },
    {
      "epoch": 40.193285859613425,
      "grad_norm": 49.09275436401367,
      "learning_rate": 9.806714140386572e-06,
      "loss": 1.5607,
      "step": 79020
    },
    {
      "epoch": 40.19837232960325,
      "grad_norm": 41.282737731933594,
      "learning_rate": 9.801627670396745e-06,
      "loss": 1.54,
      "step": 79030
    },
    {
      "epoch": 40.20345879959308,
      "grad_norm": 41.014076232910156,
      "learning_rate": 9.796541200406919e-06,
      "loss": 1.6092,
      "step": 79040
    },
    {
      "epoch": 40.208545269582906,
      "grad_norm": 31.1401424407959,
      "learning_rate": 9.79145473041709e-06,
      "loss": 1.5361,
      "step": 79050
    },
    {
      "epoch": 40.21363173957273,
      "grad_norm": 39.957679748535156,
      "learning_rate": 9.786368260427265e-06,
      "loss": 1.5641,
      "step": 79060
    },
    {
      "epoch": 40.21871820956256,
      "grad_norm": 40.37009811401367,
      "learning_rate": 9.781281790437437e-06,
      "loss": 1.5736,
      "step": 79070
    },
    {
      "epoch": 40.22380467955239,
      "grad_norm": 47.915836334228516,
      "learning_rate": 9.77619532044761e-06,
      "loss": 1.5578,
      "step": 79080
    },
    {
      "epoch": 40.22889114954222,
      "grad_norm": 48.3565673828125,
      "learning_rate": 9.771108850457783e-06,
      "loss": 1.5391,
      "step": 79090
    },
    {
      "epoch": 40.23397761953205,
      "grad_norm": 37.41348648071289,
      "learning_rate": 9.766022380467955e-06,
      "loss": 1.5918,
      "step": 79100
    },
    {
      "epoch": 40.239064089521875,
      "grad_norm": 47.24856185913086,
      "learning_rate": 9.76093591047813e-06,
      "loss": 1.5949,
      "step": 79110
    },
    {
      "epoch": 40.2441505595117,
      "grad_norm": 43.00944900512695,
      "learning_rate": 9.755849440488301e-06,
      "loss": 1.605,
      "step": 79120
    },
    {
      "epoch": 40.24923702950153,
      "grad_norm": 38.021366119384766,
      "learning_rate": 9.750762970498475e-06,
      "loss": 1.5267,
      "step": 79130
    },
    {
      "epoch": 40.254323499491356,
      "grad_norm": 37.47697448730469,
      "learning_rate": 9.745676500508648e-06,
      "loss": 1.6053,
      "step": 79140
    },
    {
      "epoch": 40.25940996948118,
      "grad_norm": 43.260093688964844,
      "learning_rate": 9.74059003051882e-06,
      "loss": 1.521,
      "step": 79150
    },
    {
      "epoch": 40.26449643947101,
      "grad_norm": 42.65390396118164,
      "learning_rate": 9.735503560528995e-06,
      "loss": 1.5554,
      "step": 79160
    },
    {
      "epoch": 40.26958290946084,
      "grad_norm": 40.3005256652832,
      "learning_rate": 9.730417090539166e-06,
      "loss": 1.5586,
      "step": 79170
    },
    {
      "epoch": 40.274669379450664,
      "grad_norm": 32.85943603515625,
      "learning_rate": 9.72533062054934e-06,
      "loss": 1.5702,
      "step": 79180
    },
    {
      "epoch": 40.27975584944049,
      "grad_norm": 46.02510070800781,
      "learning_rate": 9.720244150559513e-06,
      "loss": 1.4551,
      "step": 79190
    },
    {
      "epoch": 40.28484231943032,
      "grad_norm": 36.38162612915039,
      "learning_rate": 9.715157680569684e-06,
      "loss": 1.5409,
      "step": 79200
    },
    {
      "epoch": 40.289928789420145,
      "grad_norm": 39.80763626098633,
      "learning_rate": 9.710071210579858e-06,
      "loss": 1.5149,
      "step": 79210
    },
    {
      "epoch": 40.29501525940997,
      "grad_norm": 44.71854782104492,
      "learning_rate": 9.70498474059003e-06,
      "loss": 1.5024,
      "step": 79220
    },
    {
      "epoch": 40.3001017293998,
      "grad_norm": 37.44927215576172,
      "learning_rate": 9.699898270600204e-06,
      "loss": 1.6246,
      "step": 79230
    },
    {
      "epoch": 40.305188199389626,
      "grad_norm": 41.00318908691406,
      "learning_rate": 9.694811800610377e-06,
      "loss": 1.6242,
      "step": 79240
    },
    {
      "epoch": 40.31027466937945,
      "grad_norm": 41.3819694519043,
      "learning_rate": 9.68972533062055e-06,
      "loss": 1.6026,
      "step": 79250
    },
    {
      "epoch": 40.31536113936928,
      "grad_norm": 48.02158737182617,
      "learning_rate": 9.684638860630722e-06,
      "loss": 1.6576,
      "step": 79260
    },
    {
      "epoch": 40.32044760935911,
      "grad_norm": 42.61494064331055,
      "learning_rate": 9.679552390640896e-06,
      "loss": 1.5338,
      "step": 79270
    },
    {
      "epoch": 40.325534079348934,
      "grad_norm": 42.994239807128906,
      "learning_rate": 9.674465920651069e-06,
      "loss": 1.5729,
      "step": 79280
    },
    {
      "epoch": 40.33062054933876,
      "grad_norm": 39.35600280761719,
      "learning_rate": 9.66937945066124e-06,
      "loss": 1.4969,
      "step": 79290
    },
    {
      "epoch": 40.33570701932859,
      "grad_norm": 36.4035758972168,
      "learning_rate": 9.664292980671415e-06,
      "loss": 1.5352,
      "step": 79300
    },
    {
      "epoch": 40.340793489318415,
      "grad_norm": 38.15452575683594,
      "learning_rate": 9.659206510681587e-06,
      "loss": 1.6137,
      "step": 79310
    },
    {
      "epoch": 40.34587995930824,
      "grad_norm": 34.25519943237305,
      "learning_rate": 9.65412004069176e-06,
      "loss": 1.5825,
      "step": 79320
    },
    {
      "epoch": 40.35096642929807,
      "grad_norm": 33.78300476074219,
      "learning_rate": 9.649033570701934e-06,
      "loss": 1.6275,
      "step": 79330
    },
    {
      "epoch": 40.356052899287896,
      "grad_norm": 37.34946060180664,
      "learning_rate": 9.643947100712105e-06,
      "loss": 1.5264,
      "step": 79340
    },
    {
      "epoch": 40.36113936927772,
      "grad_norm": 38.80309295654297,
      "learning_rate": 9.63886063072228e-06,
      "loss": 1.5347,
      "step": 79350
    },
    {
      "epoch": 40.36622583926755,
      "grad_norm": 47.548072814941406,
      "learning_rate": 9.633774160732452e-06,
      "loss": 1.5334,
      "step": 79360
    },
    {
      "epoch": 40.37131230925738,
      "grad_norm": 48.923561096191406,
      "learning_rate": 9.628687690742625e-06,
      "loss": 1.5721,
      "step": 79370
    },
    {
      "epoch": 40.376398779247204,
      "grad_norm": 44.140953063964844,
      "learning_rate": 9.623601220752798e-06,
      "loss": 1.5486,
      "step": 79380
    },
    {
      "epoch": 40.38148524923703,
      "grad_norm": 44.023189544677734,
      "learning_rate": 9.61851475076297e-06,
      "loss": 1.5093,
      "step": 79390
    },
    {
      "epoch": 40.38657171922686,
      "grad_norm": 44.29025650024414,
      "learning_rate": 9.613428280773145e-06,
      "loss": 1.5863,
      "step": 79400
    },
    {
      "epoch": 40.391658189216685,
      "grad_norm": 40.96570587158203,
      "learning_rate": 9.608341810783316e-06,
      "loss": 1.6054,
      "step": 79410
    },
    {
      "epoch": 40.39674465920651,
      "grad_norm": 47.76359558105469,
      "learning_rate": 9.60325534079349e-06,
      "loss": 1.6373,
      "step": 79420
    },
    {
      "epoch": 40.40183112919634,
      "grad_norm": 43.311031341552734,
      "learning_rate": 9.598168870803663e-06,
      "loss": 1.7104,
      "step": 79430
    },
    {
      "epoch": 40.406917599186166,
      "grad_norm": 53.027530670166016,
      "learning_rate": 9.593082400813835e-06,
      "loss": 1.5064,
      "step": 79440
    },
    {
      "epoch": 40.41200406917599,
      "grad_norm": 39.329689025878906,
      "learning_rate": 9.58799593082401e-06,
      "loss": 1.5776,
      "step": 79450
    },
    {
      "epoch": 40.41709053916582,
      "grad_norm": 36.729434967041016,
      "learning_rate": 9.582909460834181e-06,
      "loss": 1.5434,
      "step": 79460
    },
    {
      "epoch": 40.42217700915565,
      "grad_norm": 34.206146240234375,
      "learning_rate": 9.577822990844354e-06,
      "loss": 1.5662,
      "step": 79470
    },
    {
      "epoch": 40.42726347914547,
      "grad_norm": 37.62567138671875,
      "learning_rate": 9.572736520854528e-06,
      "loss": 1.5376,
      "step": 79480
    },
    {
      "epoch": 40.4323499491353,
      "grad_norm": 48.92930221557617,
      "learning_rate": 9.567650050864701e-06,
      "loss": 1.5376,
      "step": 79490
    },
    {
      "epoch": 40.43743641912513,
      "grad_norm": 37.87398910522461,
      "learning_rate": 9.562563580874874e-06,
      "loss": 1.5952,
      "step": 79500
    },
    {
      "epoch": 40.442522889114954,
      "grad_norm": 46.91330337524414,
      "learning_rate": 9.557477110885046e-06,
      "loss": 1.5831,
      "step": 79510
    },
    {
      "epoch": 40.44760935910478,
      "grad_norm": 43.148258209228516,
      "learning_rate": 9.552390640895219e-06,
      "loss": 1.5369,
      "step": 79520
    },
    {
      "epoch": 40.45269582909461,
      "grad_norm": 34.5606803894043,
      "learning_rate": 9.547304170905392e-06,
      "loss": 1.6884,
      "step": 79530
    },
    {
      "epoch": 40.457782299084435,
      "grad_norm": 41.31463623046875,
      "learning_rate": 9.542217700915566e-06,
      "loss": 1.5846,
      "step": 79540
    },
    {
      "epoch": 40.46286876907426,
      "grad_norm": 41.21721267700195,
      "learning_rate": 9.537131230925737e-06,
      "loss": 1.5554,
      "step": 79550
    },
    {
      "epoch": 40.46795523906409,
      "grad_norm": 32.93405532836914,
      "learning_rate": 9.53204476093591e-06,
      "loss": 1.5474,
      "step": 79560
    },
    {
      "epoch": 40.473041709053916,
      "grad_norm": 44.88018035888672,
      "learning_rate": 9.526958290946084e-06,
      "loss": 1.4824,
      "step": 79570
    },
    {
      "epoch": 40.47812817904374,
      "grad_norm": 46.94881057739258,
      "learning_rate": 9.521871820956257e-06,
      "loss": 1.6061,
      "step": 79580
    },
    {
      "epoch": 40.48321464903357,
      "grad_norm": 42.786537170410156,
      "learning_rate": 9.51678535096643e-06,
      "loss": 1.6588,
      "step": 79590
    },
    {
      "epoch": 40.4883011190234,
      "grad_norm": 51.213417053222656,
      "learning_rate": 9.511698880976602e-06,
      "loss": 1.6156,
      "step": 79600
    },
    {
      "epoch": 40.493387589013224,
      "grad_norm": 38.12222671508789,
      "learning_rate": 9.506612410986775e-06,
      "loss": 1.6014,
      "step": 79610
    },
    {
      "epoch": 40.49847405900305,
      "grad_norm": 64.80534362792969,
      "learning_rate": 9.501525940996949e-06,
      "loss": 1.6078,
      "step": 79620
    },
    {
      "epoch": 40.50356052899288,
      "grad_norm": 40.20853042602539,
      "learning_rate": 9.49643947100712e-06,
      "loss": 1.5445,
      "step": 79630
    },
    {
      "epoch": 40.508646998982705,
      "grad_norm": 41.939388275146484,
      "learning_rate": 9.491353001017295e-06,
      "loss": 1.5628,
      "step": 79640
    },
    {
      "epoch": 40.51373346897253,
      "grad_norm": 40.187171936035156,
      "learning_rate": 9.486266531027467e-06,
      "loss": 1.5269,
      "step": 79650
    },
    {
      "epoch": 40.51881993896236,
      "grad_norm": 38.7816162109375,
      "learning_rate": 9.481180061037642e-06,
      "loss": 1.5995,
      "step": 79660
    },
    {
      "epoch": 40.523906408952186,
      "grad_norm": 39.05488586425781,
      "learning_rate": 9.476093591047813e-06,
      "loss": 1.5722,
      "step": 79670
    },
    {
      "epoch": 40.52899287894201,
      "grad_norm": 42.65768814086914,
      "learning_rate": 9.471007121057985e-06,
      "loss": 1.5735,
      "step": 79680
    },
    {
      "epoch": 40.53407934893184,
      "grad_norm": 37.19706726074219,
      "learning_rate": 9.46592065106816e-06,
      "loss": 1.55,
      "step": 79690
    },
    {
      "epoch": 40.53916581892167,
      "grad_norm": 36.54077911376953,
      "learning_rate": 9.460834181078331e-06,
      "loss": 1.6558,
      "step": 79700
    },
    {
      "epoch": 40.544252288911494,
      "grad_norm": 38.60912322998047,
      "learning_rate": 9.455747711088506e-06,
      "loss": 1.5617,
      "step": 79710
    },
    {
      "epoch": 40.54933875890132,
      "grad_norm": 42.05973815917969,
      "learning_rate": 9.450661241098678e-06,
      "loss": 1.515,
      "step": 79720
    },
    {
      "epoch": 40.55442522889115,
      "grad_norm": 41.64387130737305,
      "learning_rate": 9.445574771108851e-06,
      "loss": 1.6532,
      "step": 79730
    },
    {
      "epoch": 40.559511698880975,
      "grad_norm": 43.217315673828125,
      "learning_rate": 9.440488301119025e-06,
      "loss": 1.642,
      "step": 79740
    },
    {
      "epoch": 40.5645981688708,
      "grad_norm": 39.60394287109375,
      "learning_rate": 9.435401831129196e-06,
      "loss": 1.581,
      "step": 79750
    },
    {
      "epoch": 40.56968463886063,
      "grad_norm": 41.02766036987305,
      "learning_rate": 9.43031536113937e-06,
      "loss": 1.5363,
      "step": 79760
    },
    {
      "epoch": 40.574771108850456,
      "grad_norm": 60.76530838012695,
      "learning_rate": 9.425228891149543e-06,
      "loss": 1.5098,
      "step": 79770
    },
    {
      "epoch": 40.57985757884028,
      "grad_norm": 39.43804168701172,
      "learning_rate": 9.420142421159716e-06,
      "loss": 1.5123,
      "step": 79780
    },
    {
      "epoch": 40.58494404883011,
      "grad_norm": 28.590984344482422,
      "learning_rate": 9.41505595116989e-06,
      "loss": 1.5286,
      "step": 79790
    },
    {
      "epoch": 40.59003051881994,
      "grad_norm": 47.60231018066406,
      "learning_rate": 9.40996948118006e-06,
      "loss": 1.4595,
      "step": 79800
    },
    {
      "epoch": 40.595116988809764,
      "grad_norm": 39.269527435302734,
      "learning_rate": 9.404883011190234e-06,
      "loss": 1.5177,
      "step": 79810
    },
    {
      "epoch": 40.60020345879959,
      "grad_norm": 47.31953430175781,
      "learning_rate": 9.399796541200407e-06,
      "loss": 1.6161,
      "step": 79820
    },
    {
      "epoch": 40.60528992878942,
      "grad_norm": 43.176395416259766,
      "learning_rate": 9.39471007121058e-06,
      "loss": 1.5908,
      "step": 79830
    },
    {
      "epoch": 40.610376398779245,
      "grad_norm": 43.07217025756836,
      "learning_rate": 9.389623601220754e-06,
      "loss": 1.5296,
      "step": 79840
    },
    {
      "epoch": 40.61546286876907,
      "grad_norm": 40.461151123046875,
      "learning_rate": 9.384537131230926e-06,
      "loss": 1.6416,
      "step": 79850
    },
    {
      "epoch": 40.6205493387589,
      "grad_norm": 41.907920837402344,
      "learning_rate": 9.379450661241099e-06,
      "loss": 1.6094,
      "step": 79860
    },
    {
      "epoch": 40.625635808748726,
      "grad_norm": 50.091365814208984,
      "learning_rate": 9.374364191251272e-06,
      "loss": 1.5239,
      "step": 79870
    },
    {
      "epoch": 40.63072227873855,
      "grad_norm": 36.8677978515625,
      "learning_rate": 9.369277721261445e-06,
      "loss": 1.5834,
      "step": 79880
    },
    {
      "epoch": 40.63580874872838,
      "grad_norm": 39.99707794189453,
      "learning_rate": 9.364191251271617e-06,
      "loss": 1.5691,
      "step": 79890
    },
    {
      "epoch": 40.64089521871821,
      "grad_norm": 35.642051696777344,
      "learning_rate": 9.359104781281792e-06,
      "loss": 1.5046,
      "step": 79900
    },
    {
      "epoch": 40.645981688708034,
      "grad_norm": 38.83357620239258,
      "learning_rate": 9.354018311291964e-06,
      "loss": 1.63,
      "step": 79910
    },
    {
      "epoch": 40.65106815869786,
      "grad_norm": 33.17497634887695,
      "learning_rate": 9.348931841302137e-06,
      "loss": 1.5642,
      "step": 79920
    },
    {
      "epoch": 40.65615462868769,
      "grad_norm": 51.56596755981445,
      "learning_rate": 9.34384537131231e-06,
      "loss": 1.5143,
      "step": 79930
    },
    {
      "epoch": 40.661241098677515,
      "grad_norm": 33.8050537109375,
      "learning_rate": 9.338758901322482e-06,
      "loss": 1.5771,
      "step": 79940
    },
    {
      "epoch": 40.66632756866734,
      "grad_norm": 50.496028900146484,
      "learning_rate": 9.333672431332657e-06,
      "loss": 1.6176,
      "step": 79950
    },
    {
      "epoch": 40.67141403865717,
      "grad_norm": 42.69752502441406,
      "learning_rate": 9.328585961342828e-06,
      "loss": 1.5714,
      "step": 79960
    },
    {
      "epoch": 40.676500508646996,
      "grad_norm": 38.282737731933594,
      "learning_rate": 9.323499491353002e-06,
      "loss": 1.6683,
      "step": 79970
    },
    {
      "epoch": 40.68158697863683,
      "grad_norm": 45.512996673583984,
      "learning_rate": 9.318413021363175e-06,
      "loss": 1.5968,
      "step": 79980
    },
    {
      "epoch": 40.68667344862666,
      "grad_norm": 35.04356002807617,
      "learning_rate": 9.313326551373346e-06,
      "loss": 1.567,
      "step": 79990
    },
    {
      "epoch": 40.691759918616484,
      "grad_norm": 46.497039794921875,
      "learning_rate": 9.308240081383521e-06,
      "loss": 1.6417,
      "step": 80000
    },
    {
      "epoch": 40.69684638860631,
      "grad_norm": 45.32913589477539,
      "learning_rate": 9.303153611393693e-06,
      "loss": 1.559,
      "step": 80010
    },
    {
      "epoch": 40.70193285859614,
      "grad_norm": 46.210227966308594,
      "learning_rate": 9.298067141403866e-06,
      "loss": 1.5959,
      "step": 80020
    },
    {
      "epoch": 40.707019328585965,
      "grad_norm": 37.88307189941406,
      "learning_rate": 9.29298067141404e-06,
      "loss": 1.5514,
      "step": 80030
    },
    {
      "epoch": 40.71210579857579,
      "grad_norm": 35.628604888916016,
      "learning_rate": 9.287894201424211e-06,
      "loss": 1.6255,
      "step": 80040
    },
    {
      "epoch": 40.71719226856562,
      "grad_norm": 40.1923828125,
      "learning_rate": 9.282807731434386e-06,
      "loss": 1.5702,
      "step": 80050
    },
    {
      "epoch": 40.722278738555445,
      "grad_norm": 46.038265228271484,
      "learning_rate": 9.277721261444558e-06,
      "loss": 1.5986,
      "step": 80060
    },
    {
      "epoch": 40.72736520854527,
      "grad_norm": 48.142372131347656,
      "learning_rate": 9.272634791454731e-06,
      "loss": 1.564,
      "step": 80070
    },
    {
      "epoch": 40.7324516785351,
      "grad_norm": 41.43010711669922,
      "learning_rate": 9.267548321464904e-06,
      "loss": 1.6289,
      "step": 80080
    },
    {
      "epoch": 40.737538148524926,
      "grad_norm": 38.97130584716797,
      "learning_rate": 9.262461851475076e-06,
      "loss": 1.4967,
      "step": 80090
    },
    {
      "epoch": 40.74262461851475,
      "grad_norm": 41.19704055786133,
      "learning_rate": 9.257375381485249e-06,
      "loss": 1.5502,
      "step": 80100
    },
    {
      "epoch": 40.74771108850458,
      "grad_norm": 51.01530075073242,
      "learning_rate": 9.252288911495422e-06,
      "loss": 1.6566,
      "step": 80110
    },
    {
      "epoch": 40.75279755849441,
      "grad_norm": 49.32563400268555,
      "learning_rate": 9.247202441505596e-06,
      "loss": 1.5206,
      "step": 80120
    },
    {
      "epoch": 40.757884028484234,
      "grad_norm": 44.74566650390625,
      "learning_rate": 9.242115971515769e-06,
      "loss": 1.528,
      "step": 80130
    },
    {
      "epoch": 40.76297049847406,
      "grad_norm": 42.71686935424805,
      "learning_rate": 9.237029501525942e-06,
      "loss": 1.573,
      "step": 80140
    },
    {
      "epoch": 40.76805696846389,
      "grad_norm": 38.503238677978516,
      "learning_rate": 9.231943031536114e-06,
      "loss": 1.5449,
      "step": 80150
    },
    {
      "epoch": 40.773143438453715,
      "grad_norm": 36.630088806152344,
      "learning_rate": 9.226856561546287e-06,
      "loss": 1.5316,
      "step": 80160
    },
    {
      "epoch": 40.77822990844354,
      "grad_norm": 49.32474136352539,
      "learning_rate": 9.22177009155646e-06,
      "loss": 1.5786,
      "step": 80170
    },
    {
      "epoch": 40.78331637843337,
      "grad_norm": 48.948753356933594,
      "learning_rate": 9.216683621566634e-06,
      "loss": 1.5841,
      "step": 80180
    },
    {
      "epoch": 40.788402848423196,
      "grad_norm": 40.719051361083984,
      "learning_rate": 9.211597151576807e-06,
      "loss": 1.5234,
      "step": 80190
    },
    {
      "epoch": 40.79348931841302,
      "grad_norm": 45.005306243896484,
      "learning_rate": 9.206510681586979e-06,
      "loss": 1.5702,
      "step": 80200
    },
    {
      "epoch": 40.79857578840285,
      "grad_norm": 44.38568878173828,
      "learning_rate": 9.201424211597152e-06,
      "loss": 1.6375,
      "step": 80210
    },
    {
      "epoch": 40.80366225839268,
      "grad_norm": 45.460670471191406,
      "learning_rate": 9.196337741607325e-06,
      "loss": 1.6092,
      "step": 80220
    },
    {
      "epoch": 40.808748728382504,
      "grad_norm": 38.80934143066406,
      "learning_rate": 9.191251271617497e-06,
      "loss": 1.5753,
      "step": 80230
    },
    {
      "epoch": 40.81383519837233,
      "grad_norm": 38.315975189208984,
      "learning_rate": 9.186164801627672e-06,
      "loss": 1.4478,
      "step": 80240
    },
    {
      "epoch": 40.81892166836216,
      "grad_norm": 41.71610641479492,
      "learning_rate": 9.181078331637843e-06,
      "loss": 1.5206,
      "step": 80250
    },
    {
      "epoch": 40.824008138351985,
      "grad_norm": 42.3734130859375,
      "learning_rate": 9.175991861648017e-06,
      "loss": 1.5188,
      "step": 80260
    },
    {
      "epoch": 40.82909460834181,
      "grad_norm": 51.49477767944336,
      "learning_rate": 9.17090539165819e-06,
      "loss": 1.5417,
      "step": 80270
    },
    {
      "epoch": 40.83418107833164,
      "grad_norm": 45.25847244262695,
      "learning_rate": 9.165818921668361e-06,
      "loss": 1.5482,
      "step": 80280
    },
    {
      "epoch": 40.839267548321466,
      "grad_norm": 46.871761322021484,
      "learning_rate": 9.160732451678536e-06,
      "loss": 1.5835,
      "step": 80290
    },
    {
      "epoch": 40.84435401831129,
      "grad_norm": 35.374794006347656,
      "learning_rate": 9.155645981688708e-06,
      "loss": 1.5325,
      "step": 80300
    },
    {
      "epoch": 40.84944048830112,
      "grad_norm": 40.93598937988281,
      "learning_rate": 9.150559511698881e-06,
      "loss": 1.5348,
      "step": 80310
    },
    {
      "epoch": 40.85452695829095,
      "grad_norm": 43.60966873168945,
      "learning_rate": 9.145473041709055e-06,
      "loss": 1.6751,
      "step": 80320
    },
    {
      "epoch": 40.859613428280774,
      "grad_norm": 45.35927200317383,
      "learning_rate": 9.140386571719226e-06,
      "loss": 1.5106,
      "step": 80330
    },
    {
      "epoch": 40.8646998982706,
      "grad_norm": 42.127479553222656,
      "learning_rate": 9.135300101729401e-06,
      "loss": 1.7051,
      "step": 80340
    },
    {
      "epoch": 40.86978636826043,
      "grad_norm": 36.38832473754883,
      "learning_rate": 9.130213631739573e-06,
      "loss": 1.4275,
      "step": 80350
    },
    {
      "epoch": 40.874872838250255,
      "grad_norm": 47.435489654541016,
      "learning_rate": 9.125127161749746e-06,
      "loss": 1.6513,
      "step": 80360
    },
    {
      "epoch": 40.87995930824008,
      "grad_norm": 37.75640106201172,
      "learning_rate": 9.12004069175992e-06,
      "loss": 1.541,
      "step": 80370
    },
    {
      "epoch": 40.88504577822991,
      "grad_norm": 33.80109786987305,
      "learning_rate": 9.114954221770093e-06,
      "loss": 1.5561,
      "step": 80380
    },
    {
      "epoch": 40.890132248219736,
      "grad_norm": 41.835845947265625,
      "learning_rate": 9.109867751780266e-06,
      "loss": 1.6027,
      "step": 80390
    },
    {
      "epoch": 40.89521871820956,
      "grad_norm": 44.5565299987793,
      "learning_rate": 9.104781281790437e-06,
      "loss": 1.61,
      "step": 80400
    },
    {
      "epoch": 40.90030518819939,
      "grad_norm": 35.73784255981445,
      "learning_rate": 9.09969481180061e-06,
      "loss": 1.4848,
      "step": 80410
    },
    {
      "epoch": 40.90539165818922,
      "grad_norm": 40.247718811035156,
      "learning_rate": 9.094608341810784e-06,
      "loss": 1.5242,
      "step": 80420
    },
    {
      "epoch": 40.910478128179044,
      "grad_norm": 34.57329177856445,
      "learning_rate": 9.089521871820957e-06,
      "loss": 1.5461,
      "step": 80430
    },
    {
      "epoch": 40.91556459816887,
      "grad_norm": 42.096717834472656,
      "learning_rate": 9.084435401831129e-06,
      "loss": 1.5802,
      "step": 80440
    },
    {
      "epoch": 40.9206510681587,
      "grad_norm": 41.61739730834961,
      "learning_rate": 9.079348931841302e-06,
      "loss": 1.5533,
      "step": 80450
    },
    {
      "epoch": 40.925737538148525,
      "grad_norm": 49.80772018432617,
      "learning_rate": 9.074262461851475e-06,
      "loss": 1.4764,
      "step": 80460
    },
    {
      "epoch": 40.93082400813835,
      "grad_norm": 37.725887298583984,
      "learning_rate": 9.069175991861649e-06,
      "loss": 1.5646,
      "step": 80470
    },
    {
      "epoch": 40.93591047812818,
      "grad_norm": 41.5239143371582,
      "learning_rate": 9.064089521871822e-06,
      "loss": 1.5535,
      "step": 80480
    },
    {
      "epoch": 40.940996948118006,
      "grad_norm": 40.6110954284668,
      "learning_rate": 9.059003051881994e-06,
      "loss": 1.5896,
      "step": 80490
    },
    {
      "epoch": 40.94608341810783,
      "grad_norm": 38.8178596496582,
      "learning_rate": 9.053916581892167e-06,
      "loss": 1.6167,
      "step": 80500
    },
    {
      "epoch": 40.95116988809766,
      "grad_norm": 38.54920959472656,
      "learning_rate": 9.04883011190234e-06,
      "loss": 1.5669,
      "step": 80510
    },
    {
      "epoch": 40.95625635808749,
      "grad_norm": 56.65824508666992,
      "learning_rate": 9.043743641912513e-06,
      "loss": 1.5527,
      "step": 80520
    },
    {
      "epoch": 40.96134282807731,
      "grad_norm": 44.64398193359375,
      "learning_rate": 9.038657171922687e-06,
      "loss": 1.5468,
      "step": 80530
    },
    {
      "epoch": 40.96642929806714,
      "grad_norm": 38.047393798828125,
      "learning_rate": 9.033570701932858e-06,
      "loss": 1.6075,
      "step": 80540
    },
    {
      "epoch": 40.97151576805697,
      "grad_norm": 48.14852523803711,
      "learning_rate": 9.028484231943032e-06,
      "loss": 1.6,
      "step": 80550
    },
    {
      "epoch": 40.976602238046794,
      "grad_norm": 40.74940872192383,
      "learning_rate": 9.023397761953205e-06,
      "loss": 1.523,
      "step": 80560
    },
    {
      "epoch": 40.98168870803662,
      "grad_norm": 42.723411560058594,
      "learning_rate": 9.018311291963376e-06,
      "loss": 1.5409,
      "step": 80570
    },
    {
      "epoch": 40.98677517802645,
      "grad_norm": 34.14844512939453,
      "learning_rate": 9.013224821973551e-06,
      "loss": 1.5941,
      "step": 80580
    },
    {
      "epoch": 40.991861648016275,
      "grad_norm": 56.7311897277832,
      "learning_rate": 9.008138351983723e-06,
      "loss": 1.6384,
      "step": 80590
    },
    {
      "epoch": 40.9969481180061,
      "grad_norm": 42.61274719238281,
      "learning_rate": 9.003051881993898e-06,
      "loss": 1.5908,
      "step": 80600
    },
    {
      "epoch": 41.0,
      "eval_loss": 5.059598445892334,
      "eval_runtime": 2.7328,
      "eval_samples_per_second": 1015.434,
      "eval_steps_per_second": 126.975,
      "step": 80606
    },
    {
      "epoch": 41.00203458799593,
      "grad_norm": 48.003379821777344,
      "learning_rate": 8.99796541200407e-06,
      "loss": 1.5677,
      "step": 80610
    },
    {
      "epoch": 41.007121057985756,
      "grad_norm": 46.999385833740234,
      "learning_rate": 8.992878942014243e-06,
      "loss": 1.5847,
      "step": 80620
    },
    {
      "epoch": 41.01220752797558,
      "grad_norm": 46.052799224853516,
      "learning_rate": 8.987792472024416e-06,
      "loss": 1.6308,
      "step": 80630
    },
    {
      "epoch": 41.01729399796541,
      "grad_norm": 28.593942642211914,
      "learning_rate": 8.982706002034588e-06,
      "loss": 1.5569,
      "step": 80640
    },
    {
      "epoch": 41.02238046795524,
      "grad_norm": 46.74417495727539,
      "learning_rate": 8.977619532044763e-06,
      "loss": 1.5965,
      "step": 80650
    },
    {
      "epoch": 41.027466937945064,
      "grad_norm": 48.19754409790039,
      "learning_rate": 8.972533062054934e-06,
      "loss": 1.4712,
      "step": 80660
    },
    {
      "epoch": 41.03255340793489,
      "grad_norm": 46.8657112121582,
      "learning_rate": 8.967446592065108e-06,
      "loss": 1.5492,
      "step": 80670
    },
    {
      "epoch": 41.03763987792472,
      "grad_norm": 41.57094192504883,
      "learning_rate": 8.96236012207528e-06,
      "loss": 1.4274,
      "step": 80680
    },
    {
      "epoch": 41.042726347914545,
      "grad_norm": 47.04475402832031,
      "learning_rate": 8.957273652085452e-06,
      "loss": 1.499,
      "step": 80690
    },
    {
      "epoch": 41.04781281790437,
      "grad_norm": 37.69841766357422,
      "learning_rate": 8.952187182095626e-06,
      "loss": 1.5057,
      "step": 80700
    },
    {
      "epoch": 41.0528992878942,
      "grad_norm": 38.14751434326172,
      "learning_rate": 8.947100712105799e-06,
      "loss": 1.4763,
      "step": 80710
    },
    {
      "epoch": 41.057985757884026,
      "grad_norm": 42.16005325317383,
      "learning_rate": 8.942014242115972e-06,
      "loss": 1.6165,
      "step": 80720
    },
    {
      "epoch": 41.06307222787385,
      "grad_norm": 42.69465255737305,
      "learning_rate": 8.936927772126146e-06,
      "loss": 1.5468,
      "step": 80730
    },
    {
      "epoch": 41.06815869786368,
      "grad_norm": 45.00988006591797,
      "learning_rate": 8.931841302136317e-06,
      "loss": 1.5648,
      "step": 80740
    },
    {
      "epoch": 41.07324516785351,
      "grad_norm": 35.20820617675781,
      "learning_rate": 8.92675483214649e-06,
      "loss": 1.5306,
      "step": 80750
    },
    {
      "epoch": 41.078331637843334,
      "grad_norm": 40.565364837646484,
      "learning_rate": 8.921668362156664e-06,
      "loss": 1.5065,
      "step": 80760
    },
    {
      "epoch": 41.08341810783316,
      "grad_norm": 37.20143508911133,
      "learning_rate": 8.916581892166837e-06,
      "loss": 1.5617,
      "step": 80770
    },
    {
      "epoch": 41.08850457782299,
      "grad_norm": 42.98430633544922,
      "learning_rate": 8.911495422177009e-06,
      "loss": 1.5436,
      "step": 80780
    },
    {
      "epoch": 41.093591047812815,
      "grad_norm": 53.346527099609375,
      "learning_rate": 8.906408952187184e-06,
      "loss": 1.5871,
      "step": 80790
    },
    {
      "epoch": 41.09867751780264,
      "grad_norm": 42.874778747558594,
      "learning_rate": 8.901322482197355e-06,
      "loss": 1.5852,
      "step": 80800
    },
    {
      "epoch": 41.10376398779247,
      "grad_norm": 36.76434326171875,
      "learning_rate": 8.896236012207528e-06,
      "loss": 1.6046,
      "step": 80810
    },
    {
      "epoch": 41.108850457782296,
      "grad_norm": 44.28581237792969,
      "learning_rate": 8.891149542217702e-06,
      "loss": 1.526,
      "step": 80820
    },
    {
      "epoch": 41.11393692777212,
      "grad_norm": 44.739784240722656,
      "learning_rate": 8.886063072227873e-06,
      "loss": 1.5517,
      "step": 80830
    },
    {
      "epoch": 41.11902339776195,
      "grad_norm": 47.70432662963867,
      "learning_rate": 8.880976602238048e-06,
      "loss": 1.5555,
      "step": 80840
    },
    {
      "epoch": 41.12410986775178,
      "grad_norm": 51.77901840209961,
      "learning_rate": 8.87589013224822e-06,
      "loss": 1.6055,
      "step": 80850
    },
    {
      "epoch": 41.129196337741604,
      "grad_norm": 41.424381256103516,
      "learning_rate": 8.870803662258393e-06,
      "loss": 1.538,
      "step": 80860
    },
    {
      "epoch": 41.13428280773143,
      "grad_norm": 42.744815826416016,
      "learning_rate": 8.865717192268566e-06,
      "loss": 1.5968,
      "step": 80870
    },
    {
      "epoch": 41.139369277721265,
      "grad_norm": 47.14893341064453,
      "learning_rate": 8.860630722278738e-06,
      "loss": 1.5175,
      "step": 80880
    },
    {
      "epoch": 41.14445574771109,
      "grad_norm": 46.757938385009766,
      "learning_rate": 8.855544252288913e-06,
      "loss": 1.5155,
      "step": 80890
    },
    {
      "epoch": 41.14954221770092,
      "grad_norm": 42.84789276123047,
      "learning_rate": 8.850457782299085e-06,
      "loss": 1.5112,
      "step": 80900
    },
    {
      "epoch": 41.154628687690746,
      "grad_norm": 38.95518493652344,
      "learning_rate": 8.845371312309258e-06,
      "loss": 1.5181,
      "step": 80910
    },
    {
      "epoch": 41.15971515768057,
      "grad_norm": 49.50120162963867,
      "learning_rate": 8.840284842319431e-06,
      "loss": 1.5611,
      "step": 80920
    },
    {
      "epoch": 41.1648016276704,
      "grad_norm": 36.666160583496094,
      "learning_rate": 8.835198372329603e-06,
      "loss": 1.65,
      "step": 80930
    },
    {
      "epoch": 41.16988809766023,
      "grad_norm": 41.3534049987793,
      "learning_rate": 8.830111902339778e-06,
      "loss": 1.6486,
      "step": 80940
    },
    {
      "epoch": 41.174974567650054,
      "grad_norm": 40.66511917114258,
      "learning_rate": 8.82502543234995e-06,
      "loss": 1.5736,
      "step": 80950
    },
    {
      "epoch": 41.18006103763988,
      "grad_norm": 52.59016036987305,
      "learning_rate": 8.819938962360123e-06,
      "loss": 1.5816,
      "step": 80960
    },
    {
      "epoch": 41.18514750762971,
      "grad_norm": 36.904327392578125,
      "learning_rate": 8.814852492370296e-06,
      "loss": 1.5818,
      "step": 80970
    },
    {
      "epoch": 41.190233977619535,
      "grad_norm": 39.45343017578125,
      "learning_rate": 8.809766022380467e-06,
      "loss": 1.5263,
      "step": 80980
    },
    {
      "epoch": 41.19532044760936,
      "grad_norm": 52.88304138183594,
      "learning_rate": 8.804679552390642e-06,
      "loss": 1.5145,
      "step": 80990
    },
    {
      "epoch": 41.20040691759919,
      "grad_norm": 46.091102600097656,
      "learning_rate": 8.799593082400814e-06,
      "loss": 1.5001,
      "step": 81000
    },
    {
      "epoch": 41.205493387589016,
      "grad_norm": 44.37775421142578,
      "learning_rate": 8.794506612410987e-06,
      "loss": 1.6367,
      "step": 81010
    },
    {
      "epoch": 41.21057985757884,
      "grad_norm": 37.4136962890625,
      "learning_rate": 8.78942014242116e-06,
      "loss": 1.5055,
      "step": 81020
    },
    {
      "epoch": 41.21566632756867,
      "grad_norm": 52.29645538330078,
      "learning_rate": 8.784333672431334e-06,
      "loss": 1.6481,
      "step": 81030
    },
    {
      "epoch": 41.2207527975585,
      "grad_norm": 41.185523986816406,
      "learning_rate": 8.779247202441505e-06,
      "loss": 1.585,
      "step": 81040
    },
    {
      "epoch": 41.225839267548324,
      "grad_norm": 36.75491714477539,
      "learning_rate": 8.774160732451679e-06,
      "loss": 1.5683,
      "step": 81050
    },
    {
      "epoch": 41.23092573753815,
      "grad_norm": 44.19133758544922,
      "learning_rate": 8.769074262461852e-06,
      "loss": 1.5416,
      "step": 81060
    },
    {
      "epoch": 41.23601220752798,
      "grad_norm": 38.52980422973633,
      "learning_rate": 8.763987792472025e-06,
      "loss": 1.5649,
      "step": 81070
    },
    {
      "epoch": 41.241098677517805,
      "grad_norm": 52.05467224121094,
      "learning_rate": 8.758901322482199e-06,
      "loss": 1.5197,
      "step": 81080
    },
    {
      "epoch": 41.24618514750763,
      "grad_norm": 40.55896759033203,
      "learning_rate": 8.75381485249237e-06,
      "loss": 1.5906,
      "step": 81090
    },
    {
      "epoch": 41.25127161749746,
      "grad_norm": 41.440303802490234,
      "learning_rate": 8.748728382502543e-06,
      "loss": 1.6059,
      "step": 81100
    },
    {
      "epoch": 41.256358087487286,
      "grad_norm": 41.89322280883789,
      "learning_rate": 8.743641912512717e-06,
      "loss": 1.5186,
      "step": 81110
    },
    {
      "epoch": 41.26144455747711,
      "grad_norm": 45.92658233642578,
      "learning_rate": 8.73855544252289e-06,
      "loss": 1.5806,
      "step": 81120
    },
    {
      "epoch": 41.26653102746694,
      "grad_norm": 38.54423904418945,
      "learning_rate": 8.733468972533063e-06,
      "loss": 1.5118,
      "step": 81130
    },
    {
      "epoch": 41.271617497456766,
      "grad_norm": 44.354095458984375,
      "learning_rate": 8.728382502543235e-06,
      "loss": 1.5437,
      "step": 81140
    },
    {
      "epoch": 41.27670396744659,
      "grad_norm": 37.812679290771484,
      "learning_rate": 8.723296032553408e-06,
      "loss": 1.5647,
      "step": 81150
    },
    {
      "epoch": 41.28179043743642,
      "grad_norm": 40.897151947021484,
      "learning_rate": 8.718209562563581e-06,
      "loss": 1.5463,
      "step": 81160
    },
    {
      "epoch": 41.28687690742625,
      "grad_norm": 42.94014358520508,
      "learning_rate": 8.713123092573753e-06,
      "loss": 1.5583,
      "step": 81170
    },
    {
      "epoch": 41.291963377416074,
      "grad_norm": 46.184959411621094,
      "learning_rate": 8.708036622583928e-06,
      "loss": 1.5247,
      "step": 81180
    },
    {
      "epoch": 41.2970498474059,
      "grad_norm": 52.725772857666016,
      "learning_rate": 8.7029501525941e-06,
      "loss": 1.5564,
      "step": 81190
    },
    {
      "epoch": 41.30213631739573,
      "grad_norm": 36.125919342041016,
      "learning_rate": 8.697863682604273e-06,
      "loss": 1.577,
      "step": 81200
    },
    {
      "epoch": 41.307222787385555,
      "grad_norm": 38.77497863769531,
      "learning_rate": 8.692777212614446e-06,
      "loss": 1.532,
      "step": 81210
    },
    {
      "epoch": 41.31230925737538,
      "grad_norm": 38.12825393676758,
      "learning_rate": 8.687690742624618e-06,
      "loss": 1.6046,
      "step": 81220
    },
    {
      "epoch": 41.31739572736521,
      "grad_norm": 41.690670013427734,
      "learning_rate": 8.682604272634793e-06,
      "loss": 1.5468,
      "step": 81230
    },
    {
      "epoch": 41.322482197355036,
      "grad_norm": 46.24967956542969,
      "learning_rate": 8.677517802644964e-06,
      "loss": 1.504,
      "step": 81240
    },
    {
      "epoch": 41.32756866734486,
      "grad_norm": 44.949180603027344,
      "learning_rate": 8.672431332655138e-06,
      "loss": 1.5737,
      "step": 81250
    },
    {
      "epoch": 41.33265513733469,
      "grad_norm": 51.50136947631836,
      "learning_rate": 8.66734486266531e-06,
      "loss": 1.4763,
      "step": 81260
    },
    {
      "epoch": 41.33774160732452,
      "grad_norm": 40.19854736328125,
      "learning_rate": 8.662258392675484e-06,
      "loss": 1.5654,
      "step": 81270
    },
    {
      "epoch": 41.342828077314344,
      "grad_norm": 37.3369140625,
      "learning_rate": 8.657171922685657e-06,
      "loss": 1.4976,
      "step": 81280
    },
    {
      "epoch": 41.34791454730417,
      "grad_norm": 45.74330139160156,
      "learning_rate": 8.652085452695829e-06,
      "loss": 1.5581,
      "step": 81290
    },
    {
      "epoch": 41.353001017294,
      "grad_norm": 37.25826644897461,
      "learning_rate": 8.646998982706002e-06,
      "loss": 1.6127,
      "step": 81300
    },
    {
      "epoch": 41.358087487283825,
      "grad_norm": 43.0673942565918,
      "learning_rate": 8.641912512716176e-06,
      "loss": 1.568,
      "step": 81310
    },
    {
      "epoch": 41.36317395727365,
      "grad_norm": 39.08877182006836,
      "learning_rate": 8.636826042726349e-06,
      "loss": 1.4921,
      "step": 81320
    },
    {
      "epoch": 41.36826042726348,
      "grad_norm": 36.33067321777344,
      "learning_rate": 8.631739572736522e-06,
      "loss": 1.683,
      "step": 81330
    },
    {
      "epoch": 41.373346897253306,
      "grad_norm": 36.051700592041016,
      "learning_rate": 8.626653102746694e-06,
      "loss": 1.5551,
      "step": 81340
    },
    {
      "epoch": 41.37843336724313,
      "grad_norm": 45.91065216064453,
      "learning_rate": 8.621566632756867e-06,
      "loss": 1.4983,
      "step": 81350
    },
    {
      "epoch": 41.38351983723296,
      "grad_norm": 34.655792236328125,
      "learning_rate": 8.61648016276704e-06,
      "loss": 1.6407,
      "step": 81360
    },
    {
      "epoch": 41.38860630722279,
      "grad_norm": 44.89165115356445,
      "learning_rate": 8.611393692777214e-06,
      "loss": 1.578,
      "step": 81370
    },
    {
      "epoch": 41.393692777212614,
      "grad_norm": 61.68633270263672,
      "learning_rate": 8.606307222787385e-06,
      "loss": 1.5892,
      "step": 81380
    },
    {
      "epoch": 41.39877924720244,
      "grad_norm": 43.849693298339844,
      "learning_rate": 8.601220752797558e-06,
      "loss": 1.5564,
      "step": 81390
    },
    {
      "epoch": 41.40386571719227,
      "grad_norm": 37.6843376159668,
      "learning_rate": 8.596134282807732e-06,
      "loss": 1.5509,
      "step": 81400
    },
    {
      "epoch": 41.408952187182095,
      "grad_norm": 45.785911560058594,
      "learning_rate": 8.591047812817905e-06,
      "loss": 1.6563,
      "step": 81410
    },
    {
      "epoch": 41.41403865717192,
      "grad_norm": 44.505428314208984,
      "learning_rate": 8.585961342828078e-06,
      "loss": 1.6071,
      "step": 81420
    },
    {
      "epoch": 41.41912512716175,
      "grad_norm": 36.16102981567383,
      "learning_rate": 8.58087487283825e-06,
      "loss": 1.5189,
      "step": 81430
    },
    {
      "epoch": 41.424211597151576,
      "grad_norm": 51.139434814453125,
      "learning_rate": 8.575788402848423e-06,
      "loss": 1.6116,
      "step": 81440
    },
    {
      "epoch": 41.4292980671414,
      "grad_norm": 42.44548034667969,
      "learning_rate": 8.570701932858596e-06,
      "loss": 1.5597,
      "step": 81450
    },
    {
      "epoch": 41.43438453713123,
      "grad_norm": 33.573909759521484,
      "learning_rate": 8.56561546286877e-06,
      "loss": 1.5766,
      "step": 81460
    },
    {
      "epoch": 41.43947100712106,
      "grad_norm": 48.589935302734375,
      "learning_rate": 8.560528992878943e-06,
      "loss": 1.5835,
      "step": 81470
    },
    {
      "epoch": 41.444557477110884,
      "grad_norm": 45.11563491821289,
      "learning_rate": 8.555442522889115e-06,
      "loss": 1.5516,
      "step": 81480
    },
    {
      "epoch": 41.44964394710071,
      "grad_norm": 48.52946090698242,
      "learning_rate": 8.55035605289929e-06,
      "loss": 1.5207,
      "step": 81490
    },
    {
      "epoch": 41.45473041709054,
      "grad_norm": 43.77190399169922,
      "learning_rate": 8.545269582909461e-06,
      "loss": 1.5986,
      "step": 81500
    },
    {
      "epoch": 41.459816887080365,
      "grad_norm": 38.597434997558594,
      "learning_rate": 8.540183112919634e-06,
      "loss": 1.5601,
      "step": 81510
    },
    {
      "epoch": 41.46490335707019,
      "grad_norm": 35.02536392211914,
      "learning_rate": 8.535096642929808e-06,
      "loss": 1.5494,
      "step": 81520
    },
    {
      "epoch": 41.46998982706002,
      "grad_norm": 43.488460540771484,
      "learning_rate": 8.53001017293998e-06,
      "loss": 1.5494,
      "step": 81530
    },
    {
      "epoch": 41.475076297049846,
      "grad_norm": 53.25355911254883,
      "learning_rate": 8.524923702950154e-06,
      "loss": 1.4915,
      "step": 81540
    },
    {
      "epoch": 41.48016276703967,
      "grad_norm": 32.14024353027344,
      "learning_rate": 8.519837232960326e-06,
      "loss": 1.58,
      "step": 81550
    },
    {
      "epoch": 41.4852492370295,
      "grad_norm": 42.88435363769531,
      "learning_rate": 8.514750762970499e-06,
      "loss": 1.6047,
      "step": 81560
    },
    {
      "epoch": 41.49033570701933,
      "grad_norm": 42.60768127441406,
      "learning_rate": 8.509664292980672e-06,
      "loss": 1.5652,
      "step": 81570
    },
    {
      "epoch": 41.495422177009154,
      "grad_norm": 34.91180419921875,
      "learning_rate": 8.504577822990844e-06,
      "loss": 1.6215,
      "step": 81580
    },
    {
      "epoch": 41.50050864699898,
      "grad_norm": 41.62919235229492,
      "learning_rate": 8.499491353001017e-06,
      "loss": 1.5914,
      "step": 81590
    },
    {
      "epoch": 41.50559511698881,
      "grad_norm": 41.775062561035156,
      "learning_rate": 8.49440488301119e-06,
      "loss": 1.6261,
      "step": 81600
    },
    {
      "epoch": 41.510681586978635,
      "grad_norm": 36.712703704833984,
      "learning_rate": 8.489318413021364e-06,
      "loss": 1.5371,
      "step": 81610
    },
    {
      "epoch": 41.51576805696846,
      "grad_norm": 46.498077392578125,
      "learning_rate": 8.484231943031537e-06,
      "loss": 1.6375,
      "step": 81620
    },
    {
      "epoch": 41.52085452695829,
      "grad_norm": 47.74623107910156,
      "learning_rate": 8.479145473041709e-06,
      "loss": 1.5508,
      "step": 81630
    },
    {
      "epoch": 41.525940996948115,
      "grad_norm": 48.26372146606445,
      "learning_rate": 8.474059003051882e-06,
      "loss": 1.6308,
      "step": 81640
    },
    {
      "epoch": 41.53102746693794,
      "grad_norm": 36.69176483154297,
      "learning_rate": 8.468972533062055e-06,
      "loss": 1.5427,
      "step": 81650
    },
    {
      "epoch": 41.53611393692777,
      "grad_norm": 45.20292282104492,
      "learning_rate": 8.463886063072229e-06,
      "loss": 1.5286,
      "step": 81660
    },
    {
      "epoch": 41.541200406917596,
      "grad_norm": 52.46417999267578,
      "learning_rate": 8.458799593082402e-06,
      "loss": 1.6818,
      "step": 81670
    },
    {
      "epoch": 41.54628687690742,
      "grad_norm": 31.107994079589844,
      "learning_rate": 8.453713123092573e-06,
      "loss": 1.531,
      "step": 81680
    },
    {
      "epoch": 41.55137334689725,
      "grad_norm": 35.37496566772461,
      "learning_rate": 8.448626653102747e-06,
      "loss": 1.5365,
      "step": 81690
    },
    {
      "epoch": 41.55645981688708,
      "grad_norm": 41.60728454589844,
      "learning_rate": 8.44354018311292e-06,
      "loss": 1.6243,
      "step": 81700
    },
    {
      "epoch": 41.561546286876904,
      "grad_norm": 45.229618072509766,
      "learning_rate": 8.438453713123093e-06,
      "loss": 1.5349,
      "step": 81710
    },
    {
      "epoch": 41.56663275686673,
      "grad_norm": 30.358415603637695,
      "learning_rate": 8.433367243133265e-06,
      "loss": 1.6172,
      "step": 81720
    },
    {
      "epoch": 41.57171922685656,
      "grad_norm": 37.59270095825195,
      "learning_rate": 8.42828077314344e-06,
      "loss": 1.5447,
      "step": 81730
    },
    {
      "epoch": 41.576805696846385,
      "grad_norm": 41.199607849121094,
      "learning_rate": 8.423194303153611e-06,
      "loss": 1.5627,
      "step": 81740
    },
    {
      "epoch": 41.58189216683621,
      "grad_norm": 46.543426513671875,
      "learning_rate": 8.418107833163785e-06,
      "loss": 1.5383,
      "step": 81750
    },
    {
      "epoch": 41.58697863682604,
      "grad_norm": 42.23293685913086,
      "learning_rate": 8.413021363173958e-06,
      "loss": 1.6176,
      "step": 81760
    },
    {
      "epoch": 41.592065106815866,
      "grad_norm": 50.59817123413086,
      "learning_rate": 8.40793489318413e-06,
      "loss": 1.5222,
      "step": 81770
    },
    {
      "epoch": 41.5971515768057,
      "grad_norm": 42.51648712158203,
      "learning_rate": 8.402848423194305e-06,
      "loss": 1.5831,
      "step": 81780
    },
    {
      "epoch": 41.60223804679553,
      "grad_norm": 45.36902618408203,
      "learning_rate": 8.397761953204476e-06,
      "loss": 1.5065,
      "step": 81790
    },
    {
      "epoch": 41.607324516785354,
      "grad_norm": 41.75386428833008,
      "learning_rate": 8.39267548321465e-06,
      "loss": 1.4545,
      "step": 81800
    },
    {
      "epoch": 41.61241098677518,
      "grad_norm": 42.54149627685547,
      "learning_rate": 8.387589013224823e-06,
      "loss": 1.531,
      "step": 81810
    },
    {
      "epoch": 41.61749745676501,
      "grad_norm": 44.20979690551758,
      "learning_rate": 8.382502543234994e-06,
      "loss": 1.5483,
      "step": 81820
    },
    {
      "epoch": 41.622583926754835,
      "grad_norm": 38.712669372558594,
      "learning_rate": 8.37741607324517e-06,
      "loss": 1.6945,
      "step": 81830
    },
    {
      "epoch": 41.62767039674466,
      "grad_norm": 51.72773361206055,
      "learning_rate": 8.37232960325534e-06,
      "loss": 1.5703,
      "step": 81840
    },
    {
      "epoch": 41.63275686673449,
      "grad_norm": 34.414794921875,
      "learning_rate": 8.367243133265514e-06,
      "loss": 1.5455,
      "step": 81850
    },
    {
      "epoch": 41.637843336724316,
      "grad_norm": 39.46544647216797,
      "learning_rate": 8.362156663275687e-06,
      "loss": 1.4405,
      "step": 81860
    },
    {
      "epoch": 41.64292980671414,
      "grad_norm": 47.77854537963867,
      "learning_rate": 8.357070193285859e-06,
      "loss": 1.594,
      "step": 81870
    },
    {
      "epoch": 41.64801627670397,
      "grad_norm": 51.971107482910156,
      "learning_rate": 8.351983723296034e-06,
      "loss": 1.4991,
      "step": 81880
    },
    {
      "epoch": 41.6531027466938,
      "grad_norm": 39.675235748291016,
      "learning_rate": 8.346897253306206e-06,
      "loss": 1.5122,
      "step": 81890
    },
    {
      "epoch": 41.658189216683624,
      "grad_norm": 42.9295539855957,
      "learning_rate": 8.341810783316379e-06,
      "loss": 1.507,
      "step": 81900
    },
    {
      "epoch": 41.66327568667345,
      "grad_norm": 37.83671951293945,
      "learning_rate": 8.336724313326552e-06,
      "loss": 1.5631,
      "step": 81910
    },
    {
      "epoch": 41.66836215666328,
      "grad_norm": 44.810760498046875,
      "learning_rate": 8.331637843336724e-06,
      "loss": 1.5278,
      "step": 81920
    },
    {
      "epoch": 41.673448626653105,
      "grad_norm": 41.322669982910156,
      "learning_rate": 8.326551373346899e-06,
      "loss": 1.5173,
      "step": 81930
    },
    {
      "epoch": 41.67853509664293,
      "grad_norm": 45.93415451049805,
      "learning_rate": 8.32146490335707e-06,
      "loss": 1.5875,
      "step": 81940
    },
    {
      "epoch": 41.68362156663276,
      "grad_norm": 50.123191833496094,
      "learning_rate": 8.316378433367244e-06,
      "loss": 1.5209,
      "step": 81950
    },
    {
      "epoch": 41.688708036622586,
      "grad_norm": 40.52768325805664,
      "learning_rate": 8.311291963377417e-06,
      "loss": 1.6181,
      "step": 81960
    },
    {
      "epoch": 41.69379450661241,
      "grad_norm": 38.39455032348633,
      "learning_rate": 8.30620549338759e-06,
      "loss": 1.6196,
      "step": 81970
    },
    {
      "epoch": 41.69888097660224,
      "grad_norm": 40.4317626953125,
      "learning_rate": 8.301119023397762e-06,
      "loss": 1.5247,
      "step": 81980
    },
    {
      "epoch": 41.70396744659207,
      "grad_norm": 40.60233688354492,
      "learning_rate": 8.296032553407935e-06,
      "loss": 1.5239,
      "step": 81990
    },
    {
      "epoch": 41.709053916581894,
      "grad_norm": 48.90507888793945,
      "learning_rate": 8.290946083418108e-06,
      "loss": 1.5685,
      "step": 82000
    },
    {
      "epoch": 41.71414038657172,
      "grad_norm": 39.1539421081543,
      "learning_rate": 8.285859613428282e-06,
      "loss": 1.4884,
      "step": 82010
    },
    {
      "epoch": 41.71922685656155,
      "grad_norm": 39.68061447143555,
      "learning_rate": 8.280773143438455e-06,
      "loss": 1.4657,
      "step": 82020
    },
    {
      "epoch": 41.724313326551375,
      "grad_norm": 39.83312225341797,
      "learning_rate": 8.275686673448626e-06,
      "loss": 1.474,
      "step": 82030
    },
    {
      "epoch": 41.7293997965412,
      "grad_norm": 45.17652893066406,
      "learning_rate": 8.2706002034588e-06,
      "loss": 1.5729,
      "step": 82040
    },
    {
      "epoch": 41.73448626653103,
      "grad_norm": 41.70677185058594,
      "learning_rate": 8.265513733468973e-06,
      "loss": 1.5641,
      "step": 82050
    },
    {
      "epoch": 41.739572736520856,
      "grad_norm": 39.02257537841797,
      "learning_rate": 8.260427263479145e-06,
      "loss": 1.5926,
      "step": 82060
    },
    {
      "epoch": 41.74465920651068,
      "grad_norm": 39.72727966308594,
      "learning_rate": 8.25534079348932e-06,
      "loss": 1.5057,
      "step": 82070
    },
    {
      "epoch": 41.74974567650051,
      "grad_norm": 40.049346923828125,
      "learning_rate": 8.250254323499491e-06,
      "loss": 1.5475,
      "step": 82080
    },
    {
      "epoch": 41.75483214649034,
      "grad_norm": 36.279937744140625,
      "learning_rate": 8.245167853509664e-06,
      "loss": 1.5469,
      "step": 82090
    },
    {
      "epoch": 41.759918616480164,
      "grad_norm": 36.38920593261719,
      "learning_rate": 8.240081383519838e-06,
      "loss": 1.5394,
      "step": 82100
    },
    {
      "epoch": 41.76500508646999,
      "grad_norm": 33.43804168701172,
      "learning_rate": 8.23499491353001e-06,
      "loss": 1.5784,
      "step": 82110
    },
    {
      "epoch": 41.77009155645982,
      "grad_norm": 39.96536636352539,
      "learning_rate": 8.229908443540184e-06,
      "loss": 1.5651,
      "step": 82120
    },
    {
      "epoch": 41.775178026449645,
      "grad_norm": 39.34296798706055,
      "learning_rate": 8.224821973550356e-06,
      "loss": 1.5571,
      "step": 82130
    },
    {
      "epoch": 41.78026449643947,
      "grad_norm": 41.340187072753906,
      "learning_rate": 8.21973550356053e-06,
      "loss": 1.5998,
      "step": 82140
    },
    {
      "epoch": 41.7853509664293,
      "grad_norm": 46.842254638671875,
      "learning_rate": 8.214649033570702e-06,
      "loss": 1.5147,
      "step": 82150
    },
    {
      "epoch": 41.790437436419126,
      "grad_norm": 42.28704071044922,
      "learning_rate": 8.209562563580874e-06,
      "loss": 1.6134,
      "step": 82160
    },
    {
      "epoch": 41.79552390640895,
      "grad_norm": 48.21788787841797,
      "learning_rate": 8.204476093591049e-06,
      "loss": 1.6157,
      "step": 82170
    },
    {
      "epoch": 41.80061037639878,
      "grad_norm": 44.7103271484375,
      "learning_rate": 8.19938962360122e-06,
      "loss": 1.5412,
      "step": 82180
    },
    {
      "epoch": 41.80569684638861,
      "grad_norm": 55.44484329223633,
      "learning_rate": 8.194303153611394e-06,
      "loss": 1.5637,
      "step": 82190
    },
    {
      "epoch": 41.81078331637843,
      "grad_norm": 47.152339935302734,
      "learning_rate": 8.189216683621567e-06,
      "loss": 1.5613,
      "step": 82200
    },
    {
      "epoch": 41.81586978636826,
      "grad_norm": 46.9583854675293,
      "learning_rate": 8.18413021363174e-06,
      "loss": 1.5587,
      "step": 82210
    },
    {
      "epoch": 41.82095625635809,
      "grad_norm": 37.51890563964844,
      "learning_rate": 8.179043743641914e-06,
      "loss": 1.5769,
      "step": 82220
    },
    {
      "epoch": 41.826042726347914,
      "grad_norm": 37.979496002197266,
      "learning_rate": 8.173957273652085e-06,
      "loss": 1.5187,
      "step": 82230
    },
    {
      "epoch": 41.83112919633774,
      "grad_norm": 42.891605377197266,
      "learning_rate": 8.168870803662259e-06,
      "loss": 1.5552,
      "step": 82240
    },
    {
      "epoch": 41.83621566632757,
      "grad_norm": 45.864444732666016,
      "learning_rate": 8.163784333672432e-06,
      "loss": 1.5331,
      "step": 82250
    },
    {
      "epoch": 41.841302136317395,
      "grad_norm": 38.835548400878906,
      "learning_rate": 8.158697863682605e-06,
      "loss": 1.5388,
      "step": 82260
    },
    {
      "epoch": 41.84638860630722,
      "grad_norm": 40.2126350402832,
      "learning_rate": 8.153611393692778e-06,
      "loss": 1.5612,
      "step": 82270
    },
    {
      "epoch": 41.85147507629705,
      "grad_norm": 41.45305252075195,
      "learning_rate": 8.14852492370295e-06,
      "loss": 1.5514,
      "step": 82280
    },
    {
      "epoch": 41.856561546286876,
      "grad_norm": 45.95368194580078,
      "learning_rate": 8.143438453713123e-06,
      "loss": 1.5501,
      "step": 82290
    },
    {
      "epoch": 41.8616480162767,
      "grad_norm": 41.993900299072266,
      "learning_rate": 8.138351983723297e-06,
      "loss": 1.446,
      "step": 82300
    },
    {
      "epoch": 41.86673448626653,
      "grad_norm": 42.77568435668945,
      "learning_rate": 8.13326551373347e-06,
      "loss": 1.5653,
      "step": 82310
    },
    {
      "epoch": 41.87182095625636,
      "grad_norm": 41.56568908691406,
      "learning_rate": 8.128179043743641e-06,
      "loss": 1.5864,
      "step": 82320
    },
    {
      "epoch": 41.876907426246184,
      "grad_norm": 36.16567611694336,
      "learning_rate": 8.123092573753815e-06,
      "loss": 1.6355,
      "step": 82330
    },
    {
      "epoch": 41.88199389623601,
      "grad_norm": 44.73285675048828,
      "learning_rate": 8.118006103763988e-06,
      "loss": 1.5498,
      "step": 82340
    },
    {
      "epoch": 41.88708036622584,
      "grad_norm": 45.96996307373047,
      "learning_rate": 8.112919633774161e-06,
      "loss": 1.5963,
      "step": 82350
    },
    {
      "epoch": 41.892166836215665,
      "grad_norm": 43.622806549072266,
      "learning_rate": 8.107833163784335e-06,
      "loss": 1.5824,
      "step": 82360
    },
    {
      "epoch": 41.89725330620549,
      "grad_norm": 35.879486083984375,
      "learning_rate": 8.102746693794506e-06,
      "loss": 1.5457,
      "step": 82370
    },
    {
      "epoch": 41.90233977619532,
      "grad_norm": 39.9002799987793,
      "learning_rate": 8.097660223804681e-06,
      "loss": 1.6251,
      "step": 82380
    },
    {
      "epoch": 41.907426246185146,
      "grad_norm": 32.25070571899414,
      "learning_rate": 8.092573753814853e-06,
      "loss": 1.5176,
      "step": 82390
    },
    {
      "epoch": 41.91251271617497,
      "grad_norm": 37.05244445800781,
      "learning_rate": 8.087487283825026e-06,
      "loss": 1.5022,
      "step": 82400
    },
    {
      "epoch": 41.9175991861648,
      "grad_norm": 44.20551681518555,
      "learning_rate": 8.0824008138352e-06,
      "loss": 1.5353,
      "step": 82410
    },
    {
      "epoch": 41.92268565615463,
      "grad_norm": 44.310909271240234,
      "learning_rate": 8.07731434384537e-06,
      "loss": 1.566,
      "step": 82420
    },
    {
      "epoch": 41.927772126144454,
      "grad_norm": 58.1402587890625,
      "learning_rate": 8.072227873855546e-06,
      "loss": 1.566,
      "step": 82430
    },
    {
      "epoch": 41.93285859613428,
      "grad_norm": 39.40578079223633,
      "learning_rate": 8.067141403865717e-06,
      "loss": 1.5685,
      "step": 82440
    },
    {
      "epoch": 41.93794506612411,
      "grad_norm": 38.85390853881836,
      "learning_rate": 8.06205493387589e-06,
      "loss": 1.6694,
      "step": 82450
    },
    {
      "epoch": 41.943031536113935,
      "grad_norm": 46.19053268432617,
      "learning_rate": 8.056968463886064e-06,
      "loss": 1.6115,
      "step": 82460
    },
    {
      "epoch": 41.94811800610376,
      "grad_norm": 44.539432525634766,
      "learning_rate": 8.051881993896236e-06,
      "loss": 1.5335,
      "step": 82470
    },
    {
      "epoch": 41.95320447609359,
      "grad_norm": 43.39771270751953,
      "learning_rate": 8.04679552390641e-06,
      "loss": 1.6182,
      "step": 82480
    },
    {
      "epoch": 41.958290946083416,
      "grad_norm": 51.50468444824219,
      "learning_rate": 8.041709053916582e-06,
      "loss": 1.5722,
      "step": 82490
    },
    {
      "epoch": 41.96337741607324,
      "grad_norm": 34.88353729248047,
      "learning_rate": 8.036622583926755e-06,
      "loss": 1.6055,
      "step": 82500
    },
    {
      "epoch": 41.96846388606307,
      "grad_norm": 34.42996597290039,
      "learning_rate": 8.031536113936929e-06,
      "loss": 1.6319,
      "step": 82510
    },
    {
      "epoch": 41.9735503560529,
      "grad_norm": 47.29375457763672,
      "learning_rate": 8.0264496439471e-06,
      "loss": 1.4694,
      "step": 82520
    },
    {
      "epoch": 41.978636826042724,
      "grad_norm": 37.196407318115234,
      "learning_rate": 8.021363173957274e-06,
      "loss": 1.614,
      "step": 82530
    },
    {
      "epoch": 41.98372329603255,
      "grad_norm": 50.38880920410156,
      "learning_rate": 8.016276703967447e-06,
      "loss": 1.5892,
      "step": 82540
    },
    {
      "epoch": 41.98880976602238,
      "grad_norm": 39.252593994140625,
      "learning_rate": 8.01119023397762e-06,
      "loss": 1.5576,
      "step": 82550
    },
    {
      "epoch": 41.993896236012205,
      "grad_norm": 48.82537078857422,
      "learning_rate": 8.006103763987793e-06,
      "loss": 1.4512,
      "step": 82560
    },
    {
      "epoch": 41.99898270600203,
      "grad_norm": 44.51234817504883,
      "learning_rate": 8.001017293997965e-06,
      "loss": 1.6033,
      "step": 82570
    },
    {
      "epoch": 42.0,
      "eval_loss": 5.072673320770264,
      "eval_runtime": 2.6573,
      "eval_samples_per_second": 1044.301,
      "eval_steps_per_second": 130.585,
      "step": 82572
    },
    {
      "epoch": 42.00406917599186,
      "grad_norm": 39.949581146240234,
      "learning_rate": 7.995930824008138e-06,
      "loss": 1.5483,
      "step": 82580
    },
    {
      "epoch": 42.009155645981686,
      "grad_norm": 34.24357604980469,
      "learning_rate": 7.990844354018312e-06,
      "loss": 1.5357,
      "step": 82590
    },
    {
      "epoch": 42.01424211597151,
      "grad_norm": 43.85334396362305,
      "learning_rate": 7.985757884028485e-06,
      "loss": 1.5017,
      "step": 82600
    },
    {
      "epoch": 42.01932858596134,
      "grad_norm": 52.70582580566406,
      "learning_rate": 7.980671414038658e-06,
      "loss": 1.5627,
      "step": 82610
    },
    {
      "epoch": 42.02441505595117,
      "grad_norm": 40.359130859375,
      "learning_rate": 7.975584944048831e-06,
      "loss": 1.6083,
      "step": 82620
    },
    {
      "epoch": 42.029501525940994,
      "grad_norm": 39.105804443359375,
      "learning_rate": 7.970498474059003e-06,
      "loss": 1.5609,
      "step": 82630
    },
    {
      "epoch": 42.03458799593082,
      "grad_norm": 33.540470123291016,
      "learning_rate": 7.965412004069176e-06,
      "loss": 1.5456,
      "step": 82640
    },
    {
      "epoch": 42.03967446592065,
      "grad_norm": 34.972694396972656,
      "learning_rate": 7.96032553407935e-06,
      "loss": 1.5229,
      "step": 82650
    },
    {
      "epoch": 42.044760935910475,
      "grad_norm": 39.628353118896484,
      "learning_rate": 7.955239064089521e-06,
      "loss": 1.5308,
      "step": 82660
    },
    {
      "epoch": 42.04984740590031,
      "grad_norm": 39.60731506347656,
      "learning_rate": 7.950152594099696e-06,
      "loss": 1.5049,
      "step": 82670
    },
    {
      "epoch": 42.054933875890136,
      "grad_norm": 46.85502243041992,
      "learning_rate": 7.945066124109868e-06,
      "loss": 1.5463,
      "step": 82680
    },
    {
      "epoch": 42.06002034587996,
      "grad_norm": 39.42681884765625,
      "learning_rate": 7.939979654120041e-06,
      "loss": 1.6405,
      "step": 82690
    },
    {
      "epoch": 42.06510681586979,
      "grad_norm": 37.10885238647461,
      "learning_rate": 7.934893184130214e-06,
      "loss": 1.4205,
      "step": 82700
    },
    {
      "epoch": 42.07019328585962,
      "grad_norm": 43.00165939331055,
      "learning_rate": 7.929806714140386e-06,
      "loss": 1.5441,
      "step": 82710
    },
    {
      "epoch": 42.075279755849444,
      "grad_norm": 46.49220275878906,
      "learning_rate": 7.92472024415056e-06,
      "loss": 1.5117,
      "step": 82720
    },
    {
      "epoch": 42.08036622583927,
      "grad_norm": 31.753889083862305,
      "learning_rate": 7.919633774160732e-06,
      "loss": 1.5995,
      "step": 82730
    },
    {
      "epoch": 42.0854526958291,
      "grad_norm": 47.106903076171875,
      "learning_rate": 7.914547304170906e-06,
      "loss": 1.573,
      "step": 82740
    },
    {
      "epoch": 42.090539165818925,
      "grad_norm": 48.484703063964844,
      "learning_rate": 7.909460834181079e-06,
      "loss": 1.5638,
      "step": 82750
    },
    {
      "epoch": 42.09562563580875,
      "grad_norm": 37.160457611083984,
      "learning_rate": 7.90437436419125e-06,
      "loss": 1.5358,
      "step": 82760
    },
    {
      "epoch": 42.10071210579858,
      "grad_norm": 32.19341278076172,
      "learning_rate": 7.899287894201426e-06,
      "loss": 1.6329,
      "step": 82770
    },
    {
      "epoch": 42.105798575788405,
      "grad_norm": 39.270042419433594,
      "learning_rate": 7.894201424211597e-06,
      "loss": 1.5036,
      "step": 82780
    },
    {
      "epoch": 42.11088504577823,
      "grad_norm": 39.71515655517578,
      "learning_rate": 7.88911495422177e-06,
      "loss": 1.5246,
      "step": 82790
    },
    {
      "epoch": 42.11597151576806,
      "grad_norm": 40.73680877685547,
      "learning_rate": 7.884028484231944e-06,
      "loss": 1.5875,
      "step": 82800
    },
    {
      "epoch": 42.121057985757886,
      "grad_norm": 39.50635528564453,
      "learning_rate": 7.878942014242115e-06,
      "loss": 1.6141,
      "step": 82810
    },
    {
      "epoch": 42.12614445574771,
      "grad_norm": 36.34025192260742,
      "learning_rate": 7.87385554425229e-06,
      "loss": 1.538,
      "step": 82820
    },
    {
      "epoch": 42.13123092573754,
      "grad_norm": 51.44961929321289,
      "learning_rate": 7.868769074262462e-06,
      "loss": 1.5334,
      "step": 82830
    },
    {
      "epoch": 42.13631739572737,
      "grad_norm": 45.19158172607422,
      "learning_rate": 7.863682604272635e-06,
      "loss": 1.4655,
      "step": 82840
    },
    {
      "epoch": 42.141403865717194,
      "grad_norm": 39.369136810302734,
      "learning_rate": 7.858596134282808e-06,
      "loss": 1.6295,
      "step": 82850
    },
    {
      "epoch": 42.14649033570702,
      "grad_norm": 41.29875564575195,
      "learning_rate": 7.853509664292982e-06,
      "loss": 1.5147,
      "step": 82860
    },
    {
      "epoch": 42.15157680569685,
      "grad_norm": 45.552040100097656,
      "learning_rate": 7.848423194303153e-06,
      "loss": 1.5816,
      "step": 82870
    },
    {
      "epoch": 42.156663275686675,
      "grad_norm": 40.00476837158203,
      "learning_rate": 7.843336724313327e-06,
      "loss": 1.5782,
      "step": 82880
    },
    {
      "epoch": 42.1617497456765,
      "grad_norm": 44.69422149658203,
      "learning_rate": 7.8382502543235e-06,
      "loss": 1.5828,
      "step": 82890
    },
    {
      "epoch": 42.16683621566633,
      "grad_norm": 58.3773078918457,
      "learning_rate": 7.833163784333673e-06,
      "loss": 1.5107,
      "step": 82900
    },
    {
      "epoch": 42.171922685656156,
      "grad_norm": 50.441917419433594,
      "learning_rate": 7.828077314343846e-06,
      "loss": 1.6557,
      "step": 82910
    },
    {
      "epoch": 42.17700915564598,
      "grad_norm": 40.22209167480469,
      "learning_rate": 7.822990844354018e-06,
      "loss": 1.5239,
      "step": 82920
    },
    {
      "epoch": 42.18209562563581,
      "grad_norm": 47.776790618896484,
      "learning_rate": 7.817904374364191e-06,
      "loss": 1.5096,
      "step": 82930
    },
    {
      "epoch": 42.18718209562564,
      "grad_norm": 42.668766021728516,
      "learning_rate": 7.812817904374365e-06,
      "loss": 1.53,
      "step": 82940
    },
    {
      "epoch": 42.192268565615464,
      "grad_norm": 27.957971572875977,
      "learning_rate": 7.807731434384538e-06,
      "loss": 1.5914,
      "step": 82950
    },
    {
      "epoch": 42.19735503560529,
      "grad_norm": 42.87211608886719,
      "learning_rate": 7.802644964394711e-06,
      "loss": 1.5488,
      "step": 82960
    },
    {
      "epoch": 42.20244150559512,
      "grad_norm": 49.98469924926758,
      "learning_rate": 7.797558494404883e-06,
      "loss": 1.4728,
      "step": 82970
    },
    {
      "epoch": 42.207527975584945,
      "grad_norm": 41.116329193115234,
      "learning_rate": 7.792472024415056e-06,
      "loss": 1.5399,
      "step": 82980
    },
    {
      "epoch": 42.21261444557477,
      "grad_norm": 38.96058654785156,
      "learning_rate": 7.78738555442523e-06,
      "loss": 1.5668,
      "step": 82990
    },
    {
      "epoch": 42.2177009155646,
      "grad_norm": 36.960147857666016,
      "learning_rate": 7.7822990844354e-06,
      "loss": 1.5535,
      "step": 83000
    },
    {
      "epoch": 42.222787385554426,
      "grad_norm": 35.0135383605957,
      "learning_rate": 7.777212614445576e-06,
      "loss": 1.5178,
      "step": 83010
    },
    {
      "epoch": 42.22787385554425,
      "grad_norm": 37.959632873535156,
      "learning_rate": 7.772126144455747e-06,
      "loss": 1.5127,
      "step": 83020
    },
    {
      "epoch": 42.23296032553408,
      "grad_norm": 54.02543258666992,
      "learning_rate": 7.767039674465922e-06,
      "loss": 1.5445,
      "step": 83030
    },
    {
      "epoch": 42.23804679552391,
      "grad_norm": 34.49626922607422,
      "learning_rate": 7.761953204476094e-06,
      "loss": 1.5439,
      "step": 83040
    },
    {
      "epoch": 42.243133265513734,
      "grad_norm": 39.751953125,
      "learning_rate": 7.756866734486266e-06,
      "loss": 1.5147,
      "step": 83050
    },
    {
      "epoch": 42.24821973550356,
      "grad_norm": 42.134864807128906,
      "learning_rate": 7.75178026449644e-06,
      "loss": 1.5161,
      "step": 83060
    },
    {
      "epoch": 42.25330620549339,
      "grad_norm": 45.00381088256836,
      "learning_rate": 7.746693794506612e-06,
      "loss": 1.5588,
      "step": 83070
    },
    {
      "epoch": 42.258392675483215,
      "grad_norm": 33.11455154418945,
      "learning_rate": 7.741607324516787e-06,
      "loss": 1.457,
      "step": 83080
    },
    {
      "epoch": 42.26347914547304,
      "grad_norm": 48.609649658203125,
      "learning_rate": 7.736520854526959e-06,
      "loss": 1.5482,
      "step": 83090
    },
    {
      "epoch": 42.26856561546287,
      "grad_norm": 46.74691390991211,
      "learning_rate": 7.731434384537132e-06,
      "loss": 1.5084,
      "step": 83100
    },
    {
      "epoch": 42.273652085452696,
      "grad_norm": 41.98337936401367,
      "learning_rate": 7.726347914547305e-06,
      "loss": 1.5432,
      "step": 83110
    },
    {
      "epoch": 42.27873855544252,
      "grad_norm": 40.252716064453125,
      "learning_rate": 7.721261444557477e-06,
      "loss": 1.5262,
      "step": 83120
    },
    {
      "epoch": 42.28382502543235,
      "grad_norm": 43.469486236572266,
      "learning_rate": 7.71617497456765e-06,
      "loss": 1.5232,
      "step": 83130
    },
    {
      "epoch": 42.28891149542218,
      "grad_norm": 32.95331954956055,
      "learning_rate": 7.711088504577823e-06,
      "loss": 1.5575,
      "step": 83140
    },
    {
      "epoch": 42.293997965412004,
      "grad_norm": 47.22357177734375,
      "learning_rate": 7.706002034587997e-06,
      "loss": 1.5152,
      "step": 83150
    },
    {
      "epoch": 42.29908443540183,
      "grad_norm": 52.28546142578125,
      "learning_rate": 7.70091556459817e-06,
      "loss": 1.5289,
      "step": 83160
    },
    {
      "epoch": 42.30417090539166,
      "grad_norm": 45.618831634521484,
      "learning_rate": 7.695829094608342e-06,
      "loss": 1.4852,
      "step": 83170
    },
    {
      "epoch": 42.309257375381485,
      "grad_norm": 40.172691345214844,
      "learning_rate": 7.690742624618515e-06,
      "loss": 1.5922,
      "step": 83180
    },
    {
      "epoch": 42.31434384537131,
      "grad_norm": 39.14166259765625,
      "learning_rate": 7.685656154628688e-06,
      "loss": 1.6213,
      "step": 83190
    },
    {
      "epoch": 42.31943031536114,
      "grad_norm": 40.116634368896484,
      "learning_rate": 7.680569684638861e-06,
      "loss": 1.5426,
      "step": 83200
    },
    {
      "epoch": 42.324516785350966,
      "grad_norm": 41.229549407958984,
      "learning_rate": 7.675483214649033e-06,
      "loss": 1.5236,
      "step": 83210
    },
    {
      "epoch": 42.32960325534079,
      "grad_norm": 38.900856018066406,
      "learning_rate": 7.670396744659206e-06,
      "loss": 1.5904,
      "step": 83220
    },
    {
      "epoch": 42.33468972533062,
      "grad_norm": 41.94743728637695,
      "learning_rate": 7.66531027466938e-06,
      "loss": 1.5122,
      "step": 83230
    },
    {
      "epoch": 42.33977619532045,
      "grad_norm": 44.03579330444336,
      "learning_rate": 7.660223804679553e-06,
      "loss": 1.5565,
      "step": 83240
    },
    {
      "epoch": 42.34486266531027,
      "grad_norm": 37.68389129638672,
      "learning_rate": 7.655137334689726e-06,
      "loss": 1.5161,
      "step": 83250
    },
    {
      "epoch": 42.3499491353001,
      "grad_norm": 35.23731994628906,
      "learning_rate": 7.650050864699898e-06,
      "loss": 1.5905,
      "step": 83260
    },
    {
      "epoch": 42.35503560528993,
      "grad_norm": 43.857845306396484,
      "learning_rate": 7.644964394710073e-06,
      "loss": 1.4979,
      "step": 83270
    },
    {
      "epoch": 42.360122075279754,
      "grad_norm": 53.82266616821289,
      "learning_rate": 7.639877924720244e-06,
      "loss": 1.4441,
      "step": 83280
    },
    {
      "epoch": 42.36520854526958,
      "grad_norm": 47.29783248901367,
      "learning_rate": 7.634791454730418e-06,
      "loss": 1.5454,
      "step": 83290
    },
    {
      "epoch": 42.37029501525941,
      "grad_norm": 40.485538482666016,
      "learning_rate": 7.62970498474059e-06,
      "loss": 1.5466,
      "step": 83300
    },
    {
      "epoch": 42.375381485249235,
      "grad_norm": 37.670509338378906,
      "learning_rate": 7.624618514750763e-06,
      "loss": 1.5543,
      "step": 83310
    },
    {
      "epoch": 42.38046795523906,
      "grad_norm": 43.836971282958984,
      "learning_rate": 7.6195320447609365e-06,
      "loss": 1.5683,
      "step": 83320
    },
    {
      "epoch": 42.38555442522889,
      "grad_norm": 43.868865966796875,
      "learning_rate": 7.614445574771109e-06,
      "loss": 1.5199,
      "step": 83330
    },
    {
      "epoch": 42.390640895218716,
      "grad_norm": 33.64653015136719,
      "learning_rate": 7.609359104781281e-06,
      "loss": 1.6036,
      "step": 83340
    },
    {
      "epoch": 42.39572736520854,
      "grad_norm": 41.64340591430664,
      "learning_rate": 7.6042726347914555e-06,
      "loss": 1.5849,
      "step": 83350
    },
    {
      "epoch": 42.40081383519837,
      "grad_norm": 45.4994010925293,
      "learning_rate": 7.599186164801628e-06,
      "loss": 1.5805,
      "step": 83360
    },
    {
      "epoch": 42.4059003051882,
      "grad_norm": 39.00516891479492,
      "learning_rate": 7.594099694811801e-06,
      "loss": 1.4903,
      "step": 83370
    },
    {
      "epoch": 42.410986775178024,
      "grad_norm": 43.917320251464844,
      "learning_rate": 7.589013224821974e-06,
      "loss": 1.5112,
      "step": 83380
    },
    {
      "epoch": 42.41607324516785,
      "grad_norm": 39.780975341796875,
      "learning_rate": 7.583926754832146e-06,
      "loss": 1.546,
      "step": 83390
    },
    {
      "epoch": 42.42115971515768,
      "grad_norm": 49.05664825439453,
      "learning_rate": 7.57884028484232e-06,
      "loss": 1.5056,
      "step": 83400
    },
    {
      "epoch": 42.426246185147505,
      "grad_norm": 45.67436981201172,
      "learning_rate": 7.573753814852493e-06,
      "loss": 1.5902,
      "step": 83410
    },
    {
      "epoch": 42.43133265513733,
      "grad_norm": 53.88312530517578,
      "learning_rate": 7.568667344862666e-06,
      "loss": 1.5296,
      "step": 83420
    },
    {
      "epoch": 42.43641912512716,
      "grad_norm": 39.457664489746094,
      "learning_rate": 7.563580874872838e-06,
      "loss": 1.5131,
      "step": 83430
    },
    {
      "epoch": 42.441505595116986,
      "grad_norm": 41.18168640136719,
      "learning_rate": 7.558494404883011e-06,
      "loss": 1.6012,
      "step": 83440
    },
    {
      "epoch": 42.44659206510681,
      "grad_norm": 37.771636962890625,
      "learning_rate": 7.553407934893185e-06,
      "loss": 1.4934,
      "step": 83450
    },
    {
      "epoch": 42.45167853509664,
      "grad_norm": 41.10318374633789,
      "learning_rate": 7.548321464903357e-06,
      "loss": 1.5178,
      "step": 83460
    },
    {
      "epoch": 42.45676500508647,
      "grad_norm": 43.831905364990234,
      "learning_rate": 7.54323499491353e-06,
      "loss": 1.5622,
      "step": 83470
    },
    {
      "epoch": 42.461851475076294,
      "grad_norm": 30.373044967651367,
      "learning_rate": 7.538148524923704e-06,
      "loss": 1.5493,
      "step": 83480
    },
    {
      "epoch": 42.46693794506612,
      "grad_norm": 43.512428283691406,
      "learning_rate": 7.5330620549338756e-06,
      "loss": 1.5817,
      "step": 83490
    },
    {
      "epoch": 42.47202441505595,
      "grad_norm": 37.813316345214844,
      "learning_rate": 7.52797558494405e-06,
      "loss": 1.5336,
      "step": 83500
    },
    {
      "epoch": 42.477110885045775,
      "grad_norm": 49.46356964111328,
      "learning_rate": 7.522889114954222e-06,
      "loss": 1.5153,
      "step": 83510
    },
    {
      "epoch": 42.4821973550356,
      "grad_norm": 36.106483459472656,
      "learning_rate": 7.5178026449643946e-06,
      "loss": 1.6232,
      "step": 83520
    },
    {
      "epoch": 42.48728382502543,
      "grad_norm": 44.509883880615234,
      "learning_rate": 7.512716174974569e-06,
      "loss": 1.5375,
      "step": 83530
    },
    {
      "epoch": 42.492370295015256,
      "grad_norm": 52.22216033935547,
      "learning_rate": 7.507629704984741e-06,
      "loss": 1.5931,
      "step": 83540
    },
    {
      "epoch": 42.49745676500508,
      "grad_norm": 45.509620666503906,
      "learning_rate": 7.502543234994914e-06,
      "loss": 1.4904,
      "step": 83550
    },
    {
      "epoch": 42.50254323499492,
      "grad_norm": 48.18702697753906,
      "learning_rate": 7.497456765005087e-06,
      "loss": 1.5258,
      "step": 83560
    },
    {
      "epoch": 42.507629704984744,
      "grad_norm": 39.38345718383789,
      "learning_rate": 7.492370295015259e-06,
      "loss": 1.5548,
      "step": 83570
    },
    {
      "epoch": 42.51271617497457,
      "grad_norm": 46.509178161621094,
      "learning_rate": 7.487283825025433e-06,
      "loss": 1.5134,
      "step": 83580
    },
    {
      "epoch": 42.5178026449644,
      "grad_norm": 45.52068328857422,
      "learning_rate": 7.482197355035606e-06,
      "loss": 1.5479,
      "step": 83590
    },
    {
      "epoch": 42.522889114954225,
      "grad_norm": 41.50217056274414,
      "learning_rate": 7.477110885045778e-06,
      "loss": 1.594,
      "step": 83600
    },
    {
      "epoch": 42.52797558494405,
      "grad_norm": 43.888919830322266,
      "learning_rate": 7.4720244150559515e-06,
      "loss": 1.6032,
      "step": 83610
    },
    {
      "epoch": 42.53306205493388,
      "grad_norm": 40.69417953491211,
      "learning_rate": 7.466937945066124e-06,
      "loss": 1.6251,
      "step": 83620
    },
    {
      "epoch": 42.538148524923706,
      "grad_norm": 36.82678985595703,
      "learning_rate": 7.461851475076298e-06,
      "loss": 1.6552,
      "step": 83630
    },
    {
      "epoch": 42.54323499491353,
      "grad_norm": 36.39635467529297,
      "learning_rate": 7.4567650050864705e-06,
      "loss": 1.4807,
      "step": 83640
    },
    {
      "epoch": 42.54832146490336,
      "grad_norm": 49.93054962158203,
      "learning_rate": 7.451678535096643e-06,
      "loss": 1.4974,
      "step": 83650
    },
    {
      "epoch": 42.55340793489319,
      "grad_norm": 53.94316101074219,
      "learning_rate": 7.446592065106816e-06,
      "loss": 1.6233,
      "step": 83660
    },
    {
      "epoch": 42.558494404883014,
      "grad_norm": 48.09429931640625,
      "learning_rate": 7.441505595116989e-06,
      "loss": 1.4807,
      "step": 83670
    },
    {
      "epoch": 42.56358087487284,
      "grad_norm": 41.86105728149414,
      "learning_rate": 7.436419125127161e-06,
      "loss": 1.5531,
      "step": 83680
    },
    {
      "epoch": 42.56866734486267,
      "grad_norm": 43.17734146118164,
      "learning_rate": 7.431332655137335e-06,
      "loss": 1.5218,
      "step": 83690
    },
    {
      "epoch": 42.573753814852495,
      "grad_norm": 42.374000549316406,
      "learning_rate": 7.426246185147508e-06,
      "loss": 1.5897,
      "step": 83700
    },
    {
      "epoch": 42.57884028484232,
      "grad_norm": 39.62931442260742,
      "learning_rate": 7.421159715157682e-06,
      "loss": 1.5985,
      "step": 83710
    },
    {
      "epoch": 42.58392675483215,
      "grad_norm": 44.247650146484375,
      "learning_rate": 7.416073245167854e-06,
      "loss": 1.5578,
      "step": 83720
    },
    {
      "epoch": 42.589013224821976,
      "grad_norm": 50.40778350830078,
      "learning_rate": 7.410986775178026e-06,
      "loss": 1.4764,
      "step": 83730
    },
    {
      "epoch": 42.5940996948118,
      "grad_norm": 33.38008499145508,
      "learning_rate": 7.4059003051882e-06,
      "loss": 1.4955,
      "step": 83740
    },
    {
      "epoch": 42.59918616480163,
      "grad_norm": 40.52408218383789,
      "learning_rate": 7.400813835198372e-06,
      "loss": 1.5952,
      "step": 83750
    },
    {
      "epoch": 42.60427263479146,
      "grad_norm": 40.24757385253906,
      "learning_rate": 7.3957273652085465e-06,
      "loss": 1.4989,
      "step": 83760
    },
    {
      "epoch": 42.609359104781284,
      "grad_norm": 37.370269775390625,
      "learning_rate": 7.390640895218719e-06,
      "loss": 1.503,
      "step": 83770
    },
    {
      "epoch": 42.61444557477111,
      "grad_norm": 43.670135498046875,
      "learning_rate": 7.385554425228891e-06,
      "loss": 1.5541,
      "step": 83780
    },
    {
      "epoch": 42.61953204476094,
      "grad_norm": 43.35026550292969,
      "learning_rate": 7.380467955239065e-06,
      "loss": 1.5869,
      "step": 83790
    },
    {
      "epoch": 42.624618514750765,
      "grad_norm": 59.816253662109375,
      "learning_rate": 7.375381485249237e-06,
      "loss": 1.5442,
      "step": 83800
    },
    {
      "epoch": 42.62970498474059,
      "grad_norm": 41.96116638183594,
      "learning_rate": 7.3702950152594096e-06,
      "loss": 1.5568,
      "step": 83810
    },
    {
      "epoch": 42.63479145473042,
      "grad_norm": 38.81584930419922,
      "learning_rate": 7.365208545269584e-06,
      "loss": 1.5238,
      "step": 83820
    },
    {
      "epoch": 42.639877924720246,
      "grad_norm": 35.589271545410156,
      "learning_rate": 7.360122075279756e-06,
      "loss": 1.4851,
      "step": 83830
    },
    {
      "epoch": 42.64496439471007,
      "grad_norm": 32.658138275146484,
      "learning_rate": 7.355035605289929e-06,
      "loss": 1.5456,
      "step": 83840
    },
    {
      "epoch": 42.6500508646999,
      "grad_norm": 45.3802604675293,
      "learning_rate": 7.349949135300102e-06,
      "loss": 1.6241,
      "step": 83850
    },
    {
      "epoch": 42.65513733468973,
      "grad_norm": 40.168800354003906,
      "learning_rate": 7.344862665310274e-06,
      "loss": 1.5212,
      "step": 83860
    },
    {
      "epoch": 42.66022380467955,
      "grad_norm": 50.88619613647461,
      "learning_rate": 7.339776195320448e-06,
      "loss": 1.5437,
      "step": 83870
    },
    {
      "epoch": 42.66531027466938,
      "grad_norm": 43.893985748291016,
      "learning_rate": 7.334689725330621e-06,
      "loss": 1.46,
      "step": 83880
    },
    {
      "epoch": 42.67039674465921,
      "grad_norm": 42.03965377807617,
      "learning_rate": 7.329603255340794e-06,
      "loss": 1.4933,
      "step": 83890
    },
    {
      "epoch": 42.675483214649034,
      "grad_norm": 41.89316177368164,
      "learning_rate": 7.3245167853509665e-06,
      "loss": 1.6018,
      "step": 83900
    },
    {
      "epoch": 42.68056968463886,
      "grad_norm": 44.56852722167969,
      "learning_rate": 7.319430315361139e-06,
      "loss": 1.5603,
      "step": 83910
    },
    {
      "epoch": 42.68565615462869,
      "grad_norm": 42.031822204589844,
      "learning_rate": 7.314343845371313e-06,
      "loss": 1.5602,
      "step": 83920
    },
    {
      "epoch": 42.690742624618515,
      "grad_norm": 33.667335510253906,
      "learning_rate": 7.3092573753814855e-06,
      "loss": 1.5312,
      "step": 83930
    },
    {
      "epoch": 42.69582909460834,
      "grad_norm": 42.45746994018555,
      "learning_rate": 7.304170905391658e-06,
      "loss": 1.4829,
      "step": 83940
    },
    {
      "epoch": 42.70091556459817,
      "grad_norm": 42.265403747558594,
      "learning_rate": 7.299084435401832e-06,
      "loss": 1.4298,
      "step": 83950
    },
    {
      "epoch": 42.706002034587996,
      "grad_norm": 43.17839050292969,
      "learning_rate": 7.2939979654120045e-06,
      "loss": 1.442,
      "step": 83960
    },
    {
      "epoch": 42.71108850457782,
      "grad_norm": 55.715885162353516,
      "learning_rate": 7.288911495422178e-06,
      "loss": 1.5351,
      "step": 83970
    },
    {
      "epoch": 42.71617497456765,
      "grad_norm": 46.225345611572266,
      "learning_rate": 7.28382502543235e-06,
      "loss": 1.6084,
      "step": 83980
    },
    {
      "epoch": 42.72126144455748,
      "grad_norm": 46.17626190185547,
      "learning_rate": 7.278738555442523e-06,
      "loss": 1.5297,
      "step": 83990
    },
    {
      "epoch": 42.726347914547304,
      "grad_norm": 46.02534484863281,
      "learning_rate": 7.273652085452697e-06,
      "loss": 1.4985,
      "step": 84000
    },
    {
      "epoch": 42.73143438453713,
      "grad_norm": 51.06165313720703,
      "learning_rate": 7.268565615462869e-06,
      "loss": 1.4828,
      "step": 84010
    },
    {
      "epoch": 42.73652085452696,
      "grad_norm": 37.11720275878906,
      "learning_rate": 7.263479145473042e-06,
      "loss": 1.5735,
      "step": 84020
    },
    {
      "epoch": 42.741607324516785,
      "grad_norm": 38.10348892211914,
      "learning_rate": 7.258392675483215e-06,
      "loss": 1.5468,
      "step": 84030
    },
    {
      "epoch": 42.74669379450661,
      "grad_norm": 41.65531921386719,
      "learning_rate": 7.253306205493387e-06,
      "loss": 1.5278,
      "step": 84040
    },
    {
      "epoch": 42.75178026449644,
      "grad_norm": 40.6494140625,
      "learning_rate": 7.2482197355035615e-06,
      "loss": 1.5621,
      "step": 84050
    },
    {
      "epoch": 42.756866734486266,
      "grad_norm": 37.26569747924805,
      "learning_rate": 7.243133265513734e-06,
      "loss": 1.5737,
      "step": 84060
    },
    {
      "epoch": 42.76195320447609,
      "grad_norm": 44.68105697631836,
      "learning_rate": 7.238046795523906e-06,
      "loss": 1.5386,
      "step": 84070
    },
    {
      "epoch": 42.76703967446592,
      "grad_norm": 47.93449401855469,
      "learning_rate": 7.23296032553408e-06,
      "loss": 1.5065,
      "step": 84080
    },
    {
      "epoch": 42.77212614445575,
      "grad_norm": 39.27980422973633,
      "learning_rate": 7.227873855544252e-06,
      "loss": 1.5016,
      "step": 84090
    },
    {
      "epoch": 42.777212614445574,
      "grad_norm": 42.2762451171875,
      "learning_rate": 7.222787385554426e-06,
      "loss": 1.5739,
      "step": 84100
    },
    {
      "epoch": 42.7822990844354,
      "grad_norm": 42.03364181518555,
      "learning_rate": 7.217700915564599e-06,
      "loss": 1.5315,
      "step": 84110
    },
    {
      "epoch": 42.78738555442523,
      "grad_norm": 43.485164642333984,
      "learning_rate": 7.212614445574771e-06,
      "loss": 1.5247,
      "step": 84120
    },
    {
      "epoch": 42.792472024415055,
      "grad_norm": 40.90016174316406,
      "learning_rate": 7.207527975584944e-06,
      "loss": 1.6606,
      "step": 84130
    },
    {
      "epoch": 42.79755849440488,
      "grad_norm": 41.88010787963867,
      "learning_rate": 7.202441505595117e-06,
      "loss": 1.5454,
      "step": 84140
    },
    {
      "epoch": 42.80264496439471,
      "grad_norm": 53.39183807373047,
      "learning_rate": 7.197355035605289e-06,
      "loss": 1.446,
      "step": 84150
    },
    {
      "epoch": 42.807731434384536,
      "grad_norm": 46.81516647338867,
      "learning_rate": 7.192268565615463e-06,
      "loss": 1.5343,
      "step": 84160
    },
    {
      "epoch": 42.81281790437436,
      "grad_norm": 38.107948303222656,
      "learning_rate": 7.187182095625636e-06,
      "loss": 1.4623,
      "step": 84170
    },
    {
      "epoch": 42.81790437436419,
      "grad_norm": 48.79246520996094,
      "learning_rate": 7.18209562563581e-06,
      "loss": 1.641,
      "step": 84180
    },
    {
      "epoch": 42.82299084435402,
      "grad_norm": 41.22488784790039,
      "learning_rate": 7.177009155645982e-06,
      "loss": 1.6226,
      "step": 84190
    },
    {
      "epoch": 42.828077314343844,
      "grad_norm": 38.0609245300293,
      "learning_rate": 7.171922685656155e-06,
      "loss": 1.5408,
      "step": 84200
    },
    {
      "epoch": 42.83316378433367,
      "grad_norm": 47.279945373535156,
      "learning_rate": 7.166836215666328e-06,
      "loss": 1.6296,
      "step": 84210
    },
    {
      "epoch": 42.8382502543235,
      "grad_norm": 34.766944885253906,
      "learning_rate": 7.1617497456765005e-06,
      "loss": 1.5727,
      "step": 84220
    },
    {
      "epoch": 42.843336724313325,
      "grad_norm": 41.74155044555664,
      "learning_rate": 7.156663275686675e-06,
      "loss": 1.4831,
      "step": 84230
    },
    {
      "epoch": 42.84842319430315,
      "grad_norm": 47.791175842285156,
      "learning_rate": 7.151576805696847e-06,
      "loss": 1.5763,
      "step": 84240
    },
    {
      "epoch": 42.85350966429298,
      "grad_norm": 45.25194549560547,
      "learning_rate": 7.1464903357070195e-06,
      "loss": 1.5044,
      "step": 84250
    },
    {
      "epoch": 42.858596134282806,
      "grad_norm": 42.279544830322266,
      "learning_rate": 7.141403865717193e-06,
      "loss": 1.4881,
      "step": 84260
    },
    {
      "epoch": 42.86368260427263,
      "grad_norm": 52.84447479248047,
      "learning_rate": 7.136317395727365e-06,
      "loss": 1.6257,
      "step": 84270
    },
    {
      "epoch": 42.86876907426246,
      "grad_norm": 44.69827651977539,
      "learning_rate": 7.131230925737538e-06,
      "loss": 1.5985,
      "step": 84280
    },
    {
      "epoch": 42.87385554425229,
      "grad_norm": 46.28339767456055,
      "learning_rate": 7.126144455747712e-06,
      "loss": 1.5886,
      "step": 84290
    },
    {
      "epoch": 42.878942014242114,
      "grad_norm": 44.71991729736328,
      "learning_rate": 7.121057985757884e-06,
      "loss": 1.5399,
      "step": 84300
    },
    {
      "epoch": 42.88402848423194,
      "grad_norm": 48.40507125854492,
      "learning_rate": 7.1159715157680575e-06,
      "loss": 1.5392,
      "step": 84310
    },
    {
      "epoch": 42.88911495422177,
      "grad_norm": 39.8342399597168,
      "learning_rate": 7.11088504577823e-06,
      "loss": 1.4917,
      "step": 84320
    },
    {
      "epoch": 42.894201424211595,
      "grad_norm": 39.21328353881836,
      "learning_rate": 7.105798575788402e-06,
      "loss": 1.5337,
      "step": 84330
    },
    {
      "epoch": 42.89928789420142,
      "grad_norm": 33.24087142944336,
      "learning_rate": 7.1007121057985765e-06,
      "loss": 1.5656,
      "step": 84340
    },
    {
      "epoch": 42.90437436419125,
      "grad_norm": 47.25678634643555,
      "learning_rate": 7.095625635808749e-06,
      "loss": 1.6194,
      "step": 84350
    },
    {
      "epoch": 42.909460834181075,
      "grad_norm": 38.89152908325195,
      "learning_rate": 7.090539165818923e-06,
      "loss": 1.5677,
      "step": 84360
    },
    {
      "epoch": 42.9145473041709,
      "grad_norm": 41.32731628417969,
      "learning_rate": 7.085452695829095e-06,
      "loss": 1.5713,
      "step": 84370
    },
    {
      "epoch": 42.91963377416073,
      "grad_norm": 54.06919479370117,
      "learning_rate": 7.080366225839267e-06,
      "loss": 1.5278,
      "step": 84380
    },
    {
      "epoch": 42.924720244150556,
      "grad_norm": 43.28741455078125,
      "learning_rate": 7.075279755849441e-06,
      "loss": 1.4965,
      "step": 84390
    },
    {
      "epoch": 42.92980671414038,
      "grad_norm": 41.910064697265625,
      "learning_rate": 7.070193285859614e-06,
      "loss": 1.5129,
      "step": 84400
    },
    {
      "epoch": 42.93489318413021,
      "grad_norm": 36.78085708618164,
      "learning_rate": 7.065106815869786e-06,
      "loss": 1.4669,
      "step": 84410
    },
    {
      "epoch": 42.93997965412004,
      "grad_norm": 38.60860061645508,
      "learning_rate": 7.06002034587996e-06,
      "loss": 1.5755,
      "step": 84420
    },
    {
      "epoch": 42.945066124109864,
      "grad_norm": 31.249160766601562,
      "learning_rate": 7.054933875890133e-06,
      "loss": 1.5863,
      "step": 84430
    },
    {
      "epoch": 42.95015259409969,
      "grad_norm": 52.44765853881836,
      "learning_rate": 7.049847405900306e-06,
      "loss": 1.5434,
      "step": 84440
    },
    {
      "epoch": 42.955239064089525,
      "grad_norm": 48.91876220703125,
      "learning_rate": 7.044760935910478e-06,
      "loss": 1.5049,
      "step": 84450
    },
    {
      "epoch": 42.96032553407935,
      "grad_norm": 41.22586441040039,
      "learning_rate": 7.039674465920651e-06,
      "loss": 1.5779,
      "step": 84460
    },
    {
      "epoch": 42.96541200406918,
      "grad_norm": 44.71295166015625,
      "learning_rate": 7.034587995930825e-06,
      "loss": 1.5015,
      "step": 84470
    },
    {
      "epoch": 42.970498474059006,
      "grad_norm": 39.276161193847656,
      "learning_rate": 7.029501525940997e-06,
      "loss": 1.5415,
      "step": 84480
    },
    {
      "epoch": 42.97558494404883,
      "grad_norm": 45.977821350097656,
      "learning_rate": 7.02441505595117e-06,
      "loss": 1.6452,
      "step": 84490
    },
    {
      "epoch": 42.98067141403866,
      "grad_norm": 41.87889099121094,
      "learning_rate": 7.019328585961343e-06,
      "loss": 1.5421,
      "step": 84500
    },
    {
      "epoch": 42.98575788402849,
      "grad_norm": 38.830501556396484,
      "learning_rate": 7.0142421159715156e-06,
      "loss": 1.6168,
      "step": 84510
    },
    {
      "epoch": 42.990844354018314,
      "grad_norm": 35.53289794921875,
      "learning_rate": 7.00915564598169e-06,
      "loss": 1.5001,
      "step": 84520
    },
    {
      "epoch": 42.99593082400814,
      "grad_norm": 43.25929641723633,
      "learning_rate": 7.004069175991862e-06,
      "loss": 1.4512,
      "step": 84530
    },
    {
      "epoch": 43.0,
      "eval_loss": 5.094310760498047,
      "eval_runtime": 2.7066,
      "eval_samples_per_second": 1025.275,
      "eval_steps_per_second": 128.206,
      "step": 84538
    },
    {
      "epoch": 43.00101729399797,
      "grad_norm": 42.92605209350586,
      "learning_rate": 6.9989827060020346e-06,
      "loss": 1.6316,
      "step": 84540
    },
    {
      "epoch": 43.006103763987795,
      "grad_norm": 34.08183670043945,
      "learning_rate": 6.993896236012208e-06,
      "loss": 1.6065,
      "step": 84550
    },
    {
      "epoch": 43.01119023397762,
      "grad_norm": 36.24444580078125,
      "learning_rate": 6.98880976602238e-06,
      "loss": 1.4703,
      "step": 84560
    },
    {
      "epoch": 43.01627670396745,
      "grad_norm": 33.63100814819336,
      "learning_rate": 6.983723296032554e-06,
      "loss": 1.5497,
      "step": 84570
    },
    {
      "epoch": 43.021363173957276,
      "grad_norm": 59.34022521972656,
      "learning_rate": 6.978636826042727e-06,
      "loss": 1.5532,
      "step": 84580
    },
    {
      "epoch": 43.0264496439471,
      "grad_norm": 42.8057975769043,
      "learning_rate": 6.973550356052899e-06,
      "loss": 1.5163,
      "step": 84590
    },
    {
      "epoch": 43.03153611393693,
      "grad_norm": 66.6258773803711,
      "learning_rate": 6.968463886063073e-06,
      "loss": 1.5758,
      "step": 84600
    },
    {
      "epoch": 43.03662258392676,
      "grad_norm": 43.6239013671875,
      "learning_rate": 6.963377416073245e-06,
      "loss": 1.5374,
      "step": 84610
    },
    {
      "epoch": 43.041709053916584,
      "grad_norm": 39.62413787841797,
      "learning_rate": 6.958290946083417e-06,
      "loss": 1.5274,
      "step": 84620
    },
    {
      "epoch": 43.04679552390641,
      "grad_norm": 36.35110855102539,
      "learning_rate": 6.9532044760935915e-06,
      "loss": 1.5548,
      "step": 84630
    },
    {
      "epoch": 43.05188199389624,
      "grad_norm": 39.23524475097656,
      "learning_rate": 6.948118006103764e-06,
      "loss": 1.5808,
      "step": 84640
    },
    {
      "epoch": 43.056968463886065,
      "grad_norm": 40.34479904174805,
      "learning_rate": 6.943031536113938e-06,
      "loss": 1.613,
      "step": 84650
    },
    {
      "epoch": 43.06205493387589,
      "grad_norm": 36.95936965942383,
      "learning_rate": 6.9379450661241105e-06,
      "loss": 1.611,
      "step": 84660
    },
    {
      "epoch": 43.06714140386572,
      "grad_norm": 42.136199951171875,
      "learning_rate": 6.932858596134283e-06,
      "loss": 1.4695,
      "step": 84670
    },
    {
      "epoch": 43.072227873855546,
      "grad_norm": 41.06361389160156,
      "learning_rate": 6.927772126144456e-06,
      "loss": 1.5506,
      "step": 84680
    },
    {
      "epoch": 43.07731434384537,
      "grad_norm": 56.423282623291016,
      "learning_rate": 6.922685656154629e-06,
      "loss": 1.4828,
      "step": 84690
    },
    {
      "epoch": 43.0824008138352,
      "grad_norm": 39.45036315917969,
      "learning_rate": 6.917599186164803e-06,
      "loss": 1.5497,
      "step": 84700
    },
    {
      "epoch": 43.08748728382503,
      "grad_norm": 45.14849090576172,
      "learning_rate": 6.912512716174975e-06,
      "loss": 1.5129,
      "step": 84710
    },
    {
      "epoch": 43.092573753814854,
      "grad_norm": 39.33010482788086,
      "learning_rate": 6.907426246185148e-06,
      "loss": 1.4579,
      "step": 84720
    },
    {
      "epoch": 43.09766022380468,
      "grad_norm": 42.07780456542969,
      "learning_rate": 6.902339776195321e-06,
      "loss": 1.5186,
      "step": 84730
    },
    {
      "epoch": 43.10274669379451,
      "grad_norm": 38.222469329833984,
      "learning_rate": 6.897253306205493e-06,
      "loss": 1.5616,
      "step": 84740
    },
    {
      "epoch": 43.107833163784335,
      "grad_norm": 56.816680908203125,
      "learning_rate": 6.892166836215666e-06,
      "loss": 1.5314,
      "step": 84750
    },
    {
      "epoch": 43.11291963377416,
      "grad_norm": 38.113548278808594,
      "learning_rate": 6.88708036622584e-06,
      "loss": 1.5616,
      "step": 84760
    },
    {
      "epoch": 43.11800610376399,
      "grad_norm": 43.461368560791016,
      "learning_rate": 6.881993896236012e-06,
      "loss": 1.4578,
      "step": 84770
    },
    {
      "epoch": 43.123092573753816,
      "grad_norm": 45.97154235839844,
      "learning_rate": 6.876907426246186e-06,
      "loss": 1.5074,
      "step": 84780
    },
    {
      "epoch": 43.12817904374364,
      "grad_norm": 63.11545181274414,
      "learning_rate": 6.871820956256358e-06,
      "loss": 1.5293,
      "step": 84790
    },
    {
      "epoch": 43.13326551373347,
      "grad_norm": 42.94584274291992,
      "learning_rate": 6.8667344862665306e-06,
      "loss": 1.5725,
      "step": 84800
    },
    {
      "epoch": 43.1383519837233,
      "grad_norm": 43.913330078125,
      "learning_rate": 6.861648016276705e-06,
      "loss": 1.4992,
      "step": 84810
    },
    {
      "epoch": 43.143438453713124,
      "grad_norm": 44.64737319946289,
      "learning_rate": 6.856561546286877e-06,
      "loss": 1.5402,
      "step": 84820
    },
    {
      "epoch": 43.14852492370295,
      "grad_norm": 43.87466812133789,
      "learning_rate": 6.8514750762970496e-06,
      "loss": 1.5109,
      "step": 84830
    },
    {
      "epoch": 43.15361139369278,
      "grad_norm": 36.59852600097656,
      "learning_rate": 6.846388606307224e-06,
      "loss": 1.4314,
      "step": 84840
    },
    {
      "epoch": 43.158697863682605,
      "grad_norm": 45.50019836425781,
      "learning_rate": 6.841302136317395e-06,
      "loss": 1.5674,
      "step": 84850
    },
    {
      "epoch": 43.16378433367243,
      "grad_norm": 50.44705581665039,
      "learning_rate": 6.836215666327569e-06,
      "loss": 1.5222,
      "step": 84860
    },
    {
      "epoch": 43.16887080366226,
      "grad_norm": 37.989532470703125,
      "learning_rate": 6.831129196337742e-06,
      "loss": 1.4975,
      "step": 84870
    },
    {
      "epoch": 43.173957273652086,
      "grad_norm": 38.80923843383789,
      "learning_rate": 6.826042726347914e-06,
      "loss": 1.4575,
      "step": 84880
    },
    {
      "epoch": 43.17904374364191,
      "grad_norm": 40.16115951538086,
      "learning_rate": 6.820956256358088e-06,
      "loss": 1.5533,
      "step": 84890
    },
    {
      "epoch": 43.18413021363174,
      "grad_norm": 34.65386199951172,
      "learning_rate": 6.815869786368261e-06,
      "loss": 1.5731,
      "step": 84900
    },
    {
      "epoch": 43.18921668362157,
      "grad_norm": 38.1529655456543,
      "learning_rate": 6.810783316378434e-06,
      "loss": 1.5555,
      "step": 84910
    },
    {
      "epoch": 43.19430315361139,
      "grad_norm": 43.57187271118164,
      "learning_rate": 6.8056968463886065e-06,
      "loss": 1.4633,
      "step": 84920
    },
    {
      "epoch": 43.19938962360122,
      "grad_norm": 33.20855712890625,
      "learning_rate": 6.800610376398779e-06,
      "loss": 1.5412,
      "step": 84930
    },
    {
      "epoch": 43.20447609359105,
      "grad_norm": 45.75139236450195,
      "learning_rate": 6.795523906408953e-06,
      "loss": 1.5558,
      "step": 84940
    },
    {
      "epoch": 43.209562563580874,
      "grad_norm": 39.912288665771484,
      "learning_rate": 6.7904374364191255e-06,
      "loss": 1.545,
      "step": 84950
    },
    {
      "epoch": 43.2146490335707,
      "grad_norm": 41.1648063659668,
      "learning_rate": 6.785350966429298e-06,
      "loss": 1.5405,
      "step": 84960
    },
    {
      "epoch": 43.21973550356053,
      "grad_norm": 56.19810485839844,
      "learning_rate": 6.780264496439471e-06,
      "loss": 1.5586,
      "step": 84970
    },
    {
      "epoch": 43.224821973550355,
      "grad_norm": 43.17401885986328,
      "learning_rate": 6.775178026449644e-06,
      "loss": 1.5571,
      "step": 84980
    },
    {
      "epoch": 43.22990844354018,
      "grad_norm": 44.93336486816406,
      "learning_rate": 6.770091556459818e-06,
      "loss": 1.5022,
      "step": 84990
    },
    {
      "epoch": 43.23499491353001,
      "grad_norm": 44.85803985595703,
      "learning_rate": 6.76500508646999e-06,
      "loss": 1.5872,
      "step": 85000
    },
    {
      "epoch": 43.240081383519836,
      "grad_norm": 40.38915252685547,
      "learning_rate": 6.759918616480163e-06,
      "loss": 1.5651,
      "step": 85010
    },
    {
      "epoch": 43.24516785350966,
      "grad_norm": 44.563472747802734,
      "learning_rate": 6.754832146490336e-06,
      "loss": 1.4226,
      "step": 85020
    },
    {
      "epoch": 43.25025432349949,
      "grad_norm": 36.60685729980469,
      "learning_rate": 6.749745676500508e-06,
      "loss": 1.5569,
      "step": 85030
    },
    {
      "epoch": 43.25534079348932,
      "grad_norm": 44.312095642089844,
      "learning_rate": 6.7446592065106825e-06,
      "loss": 1.5333,
      "step": 85040
    },
    {
      "epoch": 43.260427263479144,
      "grad_norm": 43.76537322998047,
      "learning_rate": 6.739572736520855e-06,
      "loss": 1.5812,
      "step": 85050
    },
    {
      "epoch": 43.26551373346897,
      "grad_norm": 40.55573272705078,
      "learning_rate": 6.734486266531027e-06,
      "loss": 1.5437,
      "step": 85060
    },
    {
      "epoch": 43.2706002034588,
      "grad_norm": 46.144840240478516,
      "learning_rate": 6.7293997965412015e-06,
      "loss": 1.5606,
      "step": 85070
    },
    {
      "epoch": 43.275686673448625,
      "grad_norm": 48.35071563720703,
      "learning_rate": 6.724313326551374e-06,
      "loss": 1.5994,
      "step": 85080
    },
    {
      "epoch": 43.28077314343845,
      "grad_norm": 47.18782424926758,
      "learning_rate": 6.719226856561546e-06,
      "loss": 1.6074,
      "step": 85090
    },
    {
      "epoch": 43.28585961342828,
      "grad_norm": 40.00077438354492,
      "learning_rate": 6.71414038657172e-06,
      "loss": 1.4709,
      "step": 85100
    },
    {
      "epoch": 43.290946083418106,
      "grad_norm": 36.273616790771484,
      "learning_rate": 6.709053916581892e-06,
      "loss": 1.4947,
      "step": 85110
    },
    {
      "epoch": 43.29603255340793,
      "grad_norm": 42.0883903503418,
      "learning_rate": 6.703967446592066e-06,
      "loss": 1.6295,
      "step": 85120
    },
    {
      "epoch": 43.30111902339776,
      "grad_norm": 38.282657623291016,
      "learning_rate": 6.698880976602239e-06,
      "loss": 1.5018,
      "step": 85130
    },
    {
      "epoch": 43.30620549338759,
      "grad_norm": 40.63232421875,
      "learning_rate": 6.693794506612411e-06,
      "loss": 1.5811,
      "step": 85140
    },
    {
      "epoch": 43.311291963377414,
      "grad_norm": 44.3790397644043,
      "learning_rate": 6.688708036622584e-06,
      "loss": 1.4985,
      "step": 85150
    },
    {
      "epoch": 43.31637843336724,
      "grad_norm": 45.10044860839844,
      "learning_rate": 6.683621566632757e-06,
      "loss": 1.5275,
      "step": 85160
    },
    {
      "epoch": 43.32146490335707,
      "grad_norm": 39.22914505004883,
      "learning_rate": 6.678535096642931e-06,
      "loss": 1.5239,
      "step": 85170
    },
    {
      "epoch": 43.326551373346895,
      "grad_norm": 42.711334228515625,
      "learning_rate": 6.673448626653103e-06,
      "loss": 1.4505,
      "step": 85180
    },
    {
      "epoch": 43.33163784333672,
      "grad_norm": 36.77305221557617,
      "learning_rate": 6.668362156663276e-06,
      "loss": 1.62,
      "step": 85190
    },
    {
      "epoch": 43.33672431332655,
      "grad_norm": 33.417137145996094,
      "learning_rate": 6.663275686673449e-06,
      "loss": 1.5442,
      "step": 85200
    },
    {
      "epoch": 43.341810783316376,
      "grad_norm": 37.24927520751953,
      "learning_rate": 6.6581892166836216e-06,
      "loss": 1.5038,
      "step": 85210
    },
    {
      "epoch": 43.3468972533062,
      "grad_norm": 49.177032470703125,
      "learning_rate": 6.653102746693794e-06,
      "loss": 1.5605,
      "step": 85220
    },
    {
      "epoch": 43.35198372329603,
      "grad_norm": 41.03819274902344,
      "learning_rate": 6.648016276703968e-06,
      "loss": 1.5006,
      "step": 85230
    },
    {
      "epoch": 43.35707019328586,
      "grad_norm": 57.161376953125,
      "learning_rate": 6.6429298067141405e-06,
      "loss": 1.5383,
      "step": 85240
    },
    {
      "epoch": 43.362156663275684,
      "grad_norm": 44.335575103759766,
      "learning_rate": 6.637843336724314e-06,
      "loss": 1.5728,
      "step": 85250
    },
    {
      "epoch": 43.36724313326551,
      "grad_norm": 39.62765121459961,
      "learning_rate": 6.632756866734486e-06,
      "loss": 1.5928,
      "step": 85260
    },
    {
      "epoch": 43.37232960325534,
      "grad_norm": 33.80820083618164,
      "learning_rate": 6.627670396744659e-06,
      "loss": 1.5,
      "step": 85270
    },
    {
      "epoch": 43.377416073245165,
      "grad_norm": 57.83929443359375,
      "learning_rate": 6.622583926754833e-06,
      "loss": 1.5334,
      "step": 85280
    },
    {
      "epoch": 43.38250254323499,
      "grad_norm": 48.7432746887207,
      "learning_rate": 6.617497456765005e-06,
      "loss": 1.5761,
      "step": 85290
    },
    {
      "epoch": 43.38758901322482,
      "grad_norm": 48.286376953125,
      "learning_rate": 6.612410986775178e-06,
      "loss": 1.68,
      "step": 85300
    },
    {
      "epoch": 43.392675483214646,
      "grad_norm": 41.3564453125,
      "learning_rate": 6.607324516785352e-06,
      "loss": 1.5461,
      "step": 85310
    },
    {
      "epoch": 43.39776195320447,
      "grad_norm": 45.689510345458984,
      "learning_rate": 6.602238046795524e-06,
      "loss": 1.4484,
      "step": 85320
    },
    {
      "epoch": 43.4028484231943,
      "grad_norm": 37.691593170166016,
      "learning_rate": 6.5971515768056975e-06,
      "loss": 1.4544,
      "step": 85330
    },
    {
      "epoch": 43.407934893184134,
      "grad_norm": 43.646846771240234,
      "learning_rate": 6.59206510681587e-06,
      "loss": 1.5688,
      "step": 85340
    },
    {
      "epoch": 43.41302136317396,
      "grad_norm": 57.230411529541016,
      "learning_rate": 6.586978636826042e-06,
      "loss": 1.548,
      "step": 85350
    },
    {
      "epoch": 43.41810783316379,
      "grad_norm": 41.665157318115234,
      "learning_rate": 6.5818921668362165e-06,
      "loss": 1.5526,
      "step": 85360
    },
    {
      "epoch": 43.423194303153615,
      "grad_norm": 40.42289733886719,
      "learning_rate": 6.576805696846389e-06,
      "loss": 1.6134,
      "step": 85370
    },
    {
      "epoch": 43.42828077314344,
      "grad_norm": 39.197566986083984,
      "learning_rate": 6.571719226856562e-06,
      "loss": 1.5659,
      "step": 85380
    },
    {
      "epoch": 43.43336724313327,
      "grad_norm": 38.221527099609375,
      "learning_rate": 6.566632756866735e-06,
      "loss": 1.5202,
      "step": 85390
    },
    {
      "epoch": 43.438453713123096,
      "grad_norm": 38.39778137207031,
      "learning_rate": 6.561546286876907e-06,
      "loss": 1.4447,
      "step": 85400
    },
    {
      "epoch": 43.44354018311292,
      "grad_norm": 53.53643035888672,
      "learning_rate": 6.556459816887081e-06,
      "loss": 1.5065,
      "step": 85410
    },
    {
      "epoch": 43.44862665310275,
      "grad_norm": 47.07681655883789,
      "learning_rate": 6.551373346897254e-06,
      "loss": 1.5501,
      "step": 85420
    },
    {
      "epoch": 43.45371312309258,
      "grad_norm": 46.94389343261719,
      "learning_rate": 6.546286876907426e-06,
      "loss": 1.503,
      "step": 85430
    },
    {
      "epoch": 43.458799593082404,
      "grad_norm": 56.53196716308594,
      "learning_rate": 6.541200406917599e-06,
      "loss": 1.5434,
      "step": 85440
    },
    {
      "epoch": 43.46388606307223,
      "grad_norm": 45.555381774902344,
      "learning_rate": 6.536113936927772e-06,
      "loss": 1.5873,
      "step": 85450
    },
    {
      "epoch": 43.46897253306206,
      "grad_norm": 36.377838134765625,
      "learning_rate": 6.531027466937946e-06,
      "loss": 1.4969,
      "step": 85460
    },
    {
      "epoch": 43.474059003051885,
      "grad_norm": 42.00220489501953,
      "learning_rate": 6.525940996948118e-06,
      "loss": 1.4844,
      "step": 85470
    },
    {
      "epoch": 43.47914547304171,
      "grad_norm": 47.71523666381836,
      "learning_rate": 6.520854526958291e-06,
      "loss": 1.5931,
      "step": 85480
    },
    {
      "epoch": 43.48423194303154,
      "grad_norm": 46.162567138671875,
      "learning_rate": 6.515768056968464e-06,
      "loss": 1.5711,
      "step": 85490
    },
    {
      "epoch": 43.489318413021365,
      "grad_norm": 39.40809631347656,
      "learning_rate": 6.5106815869786366e-06,
      "loss": 1.5377,
      "step": 85500
    },
    {
      "epoch": 43.49440488301119,
      "grad_norm": 34.813594818115234,
      "learning_rate": 6.505595116988811e-06,
      "loss": 1.5939,
      "step": 85510
    },
    {
      "epoch": 43.49949135300102,
      "grad_norm": 42.84215545654297,
      "learning_rate": 6.500508646998983e-06,
      "loss": 1.5745,
      "step": 85520
    },
    {
      "epoch": 43.504577822990846,
      "grad_norm": 41.91014099121094,
      "learning_rate": 6.4954221770091556e-06,
      "loss": 1.5271,
      "step": 85530
    },
    {
      "epoch": 43.50966429298067,
      "grad_norm": 41.33305740356445,
      "learning_rate": 6.49033570701933e-06,
      "loss": 1.4716,
      "step": 85540
    },
    {
      "epoch": 43.5147507629705,
      "grad_norm": 50.61893081665039,
      "learning_rate": 6.485249237029502e-06,
      "loss": 1.5064,
      "step": 85550
    },
    {
      "epoch": 43.51983723296033,
      "grad_norm": 47.26038360595703,
      "learning_rate": 6.4801627670396746e-06,
      "loss": 1.6086,
      "step": 85560
    },
    {
      "epoch": 43.524923702950154,
      "grad_norm": 35.693912506103516,
      "learning_rate": 6.475076297049848e-06,
      "loss": 1.4118,
      "step": 85570
    },
    {
      "epoch": 43.53001017293998,
      "grad_norm": 41.760982513427734,
      "learning_rate": 6.46998982706002e-06,
      "loss": 1.5077,
      "step": 85580
    },
    {
      "epoch": 43.53509664292981,
      "grad_norm": 39.53513717651367,
      "learning_rate": 6.464903357070194e-06,
      "loss": 1.5635,
      "step": 85590
    },
    {
      "epoch": 43.540183112919635,
      "grad_norm": 43.86415100097656,
      "learning_rate": 6.459816887080367e-06,
      "loss": 1.5153,
      "step": 85600
    },
    {
      "epoch": 43.54526958290946,
      "grad_norm": 41.8851432800293,
      "learning_rate": 6.454730417090539e-06,
      "loss": 1.4639,
      "step": 85610
    },
    {
      "epoch": 43.55035605289929,
      "grad_norm": 45.76576232910156,
      "learning_rate": 6.4496439471007125e-06,
      "loss": 1.4638,
      "step": 85620
    },
    {
      "epoch": 43.555442522889116,
      "grad_norm": 44.33628463745117,
      "learning_rate": 6.444557477110885e-06,
      "loss": 1.5813,
      "step": 85630
    },
    {
      "epoch": 43.56052899287894,
      "grad_norm": 50.19352340698242,
      "learning_rate": 6.439471007121057e-06,
      "loss": 1.4776,
      "step": 85640
    },
    {
      "epoch": 43.56561546286877,
      "grad_norm": 34.26003646850586,
      "learning_rate": 6.4343845371312315e-06,
      "loss": 1.6258,
      "step": 85650
    },
    {
      "epoch": 43.5707019328586,
      "grad_norm": 42.35901641845703,
      "learning_rate": 6.429298067141404e-06,
      "loss": 1.4768,
      "step": 85660
    },
    {
      "epoch": 43.575788402848424,
      "grad_norm": 41.55825424194336,
      "learning_rate": 6.424211597151577e-06,
      "loss": 1.4799,
      "step": 85670
    },
    {
      "epoch": 43.58087487283825,
      "grad_norm": 43.48603439331055,
      "learning_rate": 6.41912512716175e-06,
      "loss": 1.5478,
      "step": 85680
    },
    {
      "epoch": 43.58596134282808,
      "grad_norm": 47.62461853027344,
      "learning_rate": 6.414038657171922e-06,
      "loss": 1.5335,
      "step": 85690
    },
    {
      "epoch": 43.591047812817905,
      "grad_norm": 44.66522979736328,
      "learning_rate": 6.408952187182096e-06,
      "loss": 1.4459,
      "step": 85700
    },
    {
      "epoch": 43.59613428280773,
      "grad_norm": 50.78290939331055,
      "learning_rate": 6.403865717192269e-06,
      "loss": 1.5285,
      "step": 85710
    },
    {
      "epoch": 43.60122075279756,
      "grad_norm": 40.49698257446289,
      "learning_rate": 6.398779247202443e-06,
      "loss": 1.4608,
      "step": 85720
    },
    {
      "epoch": 43.606307222787386,
      "grad_norm": 46.39724349975586,
      "learning_rate": 6.393692777212615e-06,
      "loss": 1.5286,
      "step": 85730
    },
    {
      "epoch": 43.61139369277721,
      "grad_norm": 37.145973205566406,
      "learning_rate": 6.388606307222787e-06,
      "loss": 1.5351,
      "step": 85740
    },
    {
      "epoch": 43.61648016276704,
      "grad_norm": 42.1186408996582,
      "learning_rate": 6.383519837232961e-06,
      "loss": 1.526,
      "step": 85750
    },
    {
      "epoch": 43.62156663275687,
      "grad_norm": 54.54994583129883,
      "learning_rate": 6.378433367243133e-06,
      "loss": 1.5129,
      "step": 85760
    },
    {
      "epoch": 43.626653102746694,
      "grad_norm": 43.14699172973633,
      "learning_rate": 6.373346897253306e-06,
      "loss": 1.5483,
      "step": 85770
    },
    {
      "epoch": 43.63173957273652,
      "grad_norm": 37.166595458984375,
      "learning_rate": 6.36826042726348e-06,
      "loss": 1.555,
      "step": 85780
    },
    {
      "epoch": 43.63682604272635,
      "grad_norm": 38.25063705444336,
      "learning_rate": 6.363173957273652e-06,
      "loss": 1.5032,
      "step": 85790
    },
    {
      "epoch": 43.641912512716175,
      "grad_norm": 47.19970703125,
      "learning_rate": 6.358087487283826e-06,
      "loss": 1.5451,
      "step": 85800
    },
    {
      "epoch": 43.646998982706,
      "grad_norm": 43.46842575073242,
      "learning_rate": 6.353001017293998e-06,
      "loss": 1.5781,
      "step": 85810
    },
    {
      "epoch": 43.65208545269583,
      "grad_norm": 39.433815002441406,
      "learning_rate": 6.3479145473041706e-06,
      "loss": 1.5106,
      "step": 85820
    },
    {
      "epoch": 43.657171922685656,
      "grad_norm": 36.32673645019531,
      "learning_rate": 6.342828077314345e-06,
      "loss": 1.5554,
      "step": 85830
    },
    {
      "epoch": 43.66225839267548,
      "grad_norm": 45.838531494140625,
      "learning_rate": 6.337741607324517e-06,
      "loss": 1.5864,
      "step": 85840
    },
    {
      "epoch": 43.66734486266531,
      "grad_norm": 37.74919509887695,
      "learning_rate": 6.33265513733469e-06,
      "loss": 1.4972,
      "step": 85850
    },
    {
      "epoch": 43.67243133265514,
      "grad_norm": 41.28832244873047,
      "learning_rate": 6.327568667344863e-06,
      "loss": 1.5049,
      "step": 85860
    },
    {
      "epoch": 43.677517802644964,
      "grad_norm": 50.29219055175781,
      "learning_rate": 6.322482197355035e-06,
      "loss": 1.6143,
      "step": 85870
    },
    {
      "epoch": 43.68260427263479,
      "grad_norm": 35.40570068359375,
      "learning_rate": 6.317395727365209e-06,
      "loss": 1.5158,
      "step": 85880
    },
    {
      "epoch": 43.68769074262462,
      "grad_norm": 39.785362243652344,
      "learning_rate": 6.312309257375382e-06,
      "loss": 1.5916,
      "step": 85890
    },
    {
      "epoch": 43.692777212614445,
      "grad_norm": 46.43782424926758,
      "learning_rate": 6.307222787385554e-06,
      "loss": 1.6146,
      "step": 85900
    },
    {
      "epoch": 43.69786368260427,
      "grad_norm": 36.234535217285156,
      "learning_rate": 6.3021363173957276e-06,
      "loss": 1.5448,
      "step": 85910
    },
    {
      "epoch": 43.7029501525941,
      "grad_norm": 44.121070861816406,
      "learning_rate": 6.2970498474059e-06,
      "loss": 1.5528,
      "step": 85920
    },
    {
      "epoch": 43.708036622583926,
      "grad_norm": 42.80986785888672,
      "learning_rate": 6.291963377416074e-06,
      "loss": 1.5907,
      "step": 85930
    },
    {
      "epoch": 43.71312309257375,
      "grad_norm": 44.18974304199219,
      "learning_rate": 6.2868769074262465e-06,
      "loss": 1.5493,
      "step": 85940
    },
    {
      "epoch": 43.71820956256358,
      "grad_norm": 43.268890380859375,
      "learning_rate": 6.281790437436419e-06,
      "loss": 1.4911,
      "step": 85950
    },
    {
      "epoch": 43.72329603255341,
      "grad_norm": 42.35202407836914,
      "learning_rate": 6.276703967446593e-06,
      "loss": 1.5548,
      "step": 85960
    },
    {
      "epoch": 43.728382502543234,
      "grad_norm": 50.383522033691406,
      "learning_rate": 6.2716174974567655e-06,
      "loss": 1.6551,
      "step": 85970
    },
    {
      "epoch": 43.73346897253306,
      "grad_norm": 33.16896438598633,
      "learning_rate": 6.266531027466939e-06,
      "loss": 1.6208,
      "step": 85980
    },
    {
      "epoch": 43.73855544252289,
      "grad_norm": 43.09815216064453,
      "learning_rate": 6.261444557477111e-06,
      "loss": 1.5182,
      "step": 85990
    },
    {
      "epoch": 43.743641912512714,
      "grad_norm": 44.2044792175293,
      "learning_rate": 6.256358087487284e-06,
      "loss": 1.5318,
      "step": 86000
    },
    {
      "epoch": 43.74872838250254,
      "grad_norm": 37.48063659667969,
      "learning_rate": 6.251271617497458e-06,
      "loss": 1.5531,
      "step": 86010
    },
    {
      "epoch": 43.75381485249237,
      "grad_norm": 48.63420867919922,
      "learning_rate": 6.24618514750763e-06,
      "loss": 1.4805,
      "step": 86020
    },
    {
      "epoch": 43.758901322482195,
      "grad_norm": 39.41790771484375,
      "learning_rate": 6.241098677517803e-06,
      "loss": 1.5422,
      "step": 86030
    },
    {
      "epoch": 43.76398779247202,
      "grad_norm": 40.86454391479492,
      "learning_rate": 6.236012207527976e-06,
      "loss": 1.4616,
      "step": 86040
    },
    {
      "epoch": 43.76907426246185,
      "grad_norm": 37.94633865356445,
      "learning_rate": 6.230925737538148e-06,
      "loss": 1.5818,
      "step": 86050
    },
    {
      "epoch": 43.774160732451676,
      "grad_norm": 47.76114273071289,
      "learning_rate": 6.225839267548322e-06,
      "loss": 1.6326,
      "step": 86060
    },
    {
      "epoch": 43.7792472024415,
      "grad_norm": 40.80635452270508,
      "learning_rate": 6.220752797558495e-06,
      "loss": 1.4955,
      "step": 86070
    },
    {
      "epoch": 43.78433367243133,
      "grad_norm": 46.19228744506836,
      "learning_rate": 6.215666327568668e-06,
      "loss": 1.4558,
      "step": 86080
    },
    {
      "epoch": 43.78942014242116,
      "grad_norm": 39.01066207885742,
      "learning_rate": 6.210579857578841e-06,
      "loss": 1.5715,
      "step": 86090
    },
    {
      "epoch": 43.794506612410984,
      "grad_norm": 41.21754455566406,
      "learning_rate": 6.205493387589013e-06,
      "loss": 1.5374,
      "step": 86100
    },
    {
      "epoch": 43.79959308240081,
      "grad_norm": 39.37613296508789,
      "learning_rate": 6.200406917599186e-06,
      "loss": 1.5589,
      "step": 86110
    },
    {
      "epoch": 43.80467955239064,
      "grad_norm": 44.14287185668945,
      "learning_rate": 6.19532044760936e-06,
      "loss": 1.543,
      "step": 86120
    },
    {
      "epoch": 43.809766022380465,
      "grad_norm": 39.44422149658203,
      "learning_rate": 6.190233977619532e-06,
      "loss": 1.4722,
      "step": 86130
    },
    {
      "epoch": 43.81485249237029,
      "grad_norm": 48.891441345214844,
      "learning_rate": 6.185147507629705e-06,
      "loss": 1.4895,
      "step": 86140
    },
    {
      "epoch": 43.81993896236012,
      "grad_norm": 51.07663345336914,
      "learning_rate": 6.180061037639878e-06,
      "loss": 1.5315,
      "step": 86150
    },
    {
      "epoch": 43.825025432349946,
      "grad_norm": 49.02037811279297,
      "learning_rate": 6.174974567650051e-06,
      "loss": 1.5221,
      "step": 86160
    },
    {
      "epoch": 43.83011190233977,
      "grad_norm": 40.96922302246094,
      "learning_rate": 6.169888097660224e-06,
      "loss": 1.5364,
      "step": 86170
    },
    {
      "epoch": 43.8351983723296,
      "grad_norm": 39.948570251464844,
      "learning_rate": 6.164801627670397e-06,
      "loss": 1.497,
      "step": 86180
    },
    {
      "epoch": 43.84028484231943,
      "grad_norm": 44.00096130371094,
      "learning_rate": 6.15971515768057e-06,
      "loss": 1.5515,
      "step": 86190
    },
    {
      "epoch": 43.845371312309254,
      "grad_norm": 41.217918395996094,
      "learning_rate": 6.154628687690743e-06,
      "loss": 1.5357,
      "step": 86200
    },
    {
      "epoch": 43.85045778229908,
      "grad_norm": 56.368492126464844,
      "learning_rate": 6.149542217700916e-06,
      "loss": 1.5749,
      "step": 86210
    },
    {
      "epoch": 43.85554425228891,
      "grad_norm": 39.953453063964844,
      "learning_rate": 6.144455747711088e-06,
      "loss": 1.4616,
      "step": 86220
    },
    {
      "epoch": 43.86063072227874,
      "grad_norm": 45.24760437011719,
      "learning_rate": 6.1393692777212616e-06,
      "loss": 1.5671,
      "step": 86230
    },
    {
      "epoch": 43.86571719226856,
      "grad_norm": 44.54902648925781,
      "learning_rate": 6.134282807731435e-06,
      "loss": 1.4192,
      "step": 86240
    },
    {
      "epoch": 43.870803662258396,
      "grad_norm": 43.5598258972168,
      "learning_rate": 6.129196337741608e-06,
      "loss": 1.5454,
      "step": 86250
    },
    {
      "epoch": 43.87589013224822,
      "grad_norm": 44.31193542480469,
      "learning_rate": 6.1241098677517806e-06,
      "loss": 1.5509,
      "step": 86260
    },
    {
      "epoch": 43.88097660223805,
      "grad_norm": 37.479759216308594,
      "learning_rate": 6.119023397761953e-06,
      "loss": 1.4943,
      "step": 86270
    },
    {
      "epoch": 43.88606307222788,
      "grad_norm": 52.85475540161133,
      "learning_rate": 6.113936927772126e-06,
      "loss": 1.4491,
      "step": 86280
    },
    {
      "epoch": 43.891149542217704,
      "grad_norm": 54.81576156616211,
      "learning_rate": 6.1088504577822995e-06,
      "loss": 1.4999,
      "step": 86290
    },
    {
      "epoch": 43.89623601220753,
      "grad_norm": 43.3090934753418,
      "learning_rate": 6.103763987792473e-06,
      "loss": 1.4751,
      "step": 86300
    },
    {
      "epoch": 43.90132248219736,
      "grad_norm": 38.440738677978516,
      "learning_rate": 6.098677517802645e-06,
      "loss": 1.4702,
      "step": 86310
    },
    {
      "epoch": 43.906408952187185,
      "grad_norm": 45.28884506225586,
      "learning_rate": 6.0935910478128185e-06,
      "loss": 1.5606,
      "step": 86320
    },
    {
      "epoch": 43.91149542217701,
      "grad_norm": 52.48822021484375,
      "learning_rate": 6.088504577822991e-06,
      "loss": 1.4777,
      "step": 86330
    },
    {
      "epoch": 43.91658189216684,
      "grad_norm": 50.63727951049805,
      "learning_rate": 6.083418107833164e-06,
      "loss": 1.617,
      "step": 86340
    },
    {
      "epoch": 43.921668362156666,
      "grad_norm": 34.18146514892578,
      "learning_rate": 6.078331637843337e-06,
      "loss": 1.617,
      "step": 86350
    },
    {
      "epoch": 43.92675483214649,
      "grad_norm": 40.4159049987793,
      "learning_rate": 6.07324516785351e-06,
      "loss": 1.6457,
      "step": 86360
    },
    {
      "epoch": 43.93184130213632,
      "grad_norm": 42.315818786621094,
      "learning_rate": 6.068158697863683e-06,
      "loss": 1.502,
      "step": 86370
    },
    {
      "epoch": 43.93692777212615,
      "grad_norm": 48.384544372558594,
      "learning_rate": 6.063072227873856e-06,
      "loss": 1.5443,
      "step": 86380
    },
    {
      "epoch": 43.942014242115974,
      "grad_norm": 38.47340774536133,
      "learning_rate": 6.057985757884028e-06,
      "loss": 1.6175,
      "step": 86390
    },
    {
      "epoch": 43.9471007121058,
      "grad_norm": 34.583744049072266,
      "learning_rate": 6.052899287894201e-06,
      "loss": 1.4796,
      "step": 86400
    },
    {
      "epoch": 43.95218718209563,
      "grad_norm": 44.487892150878906,
      "learning_rate": 6.047812817904375e-06,
      "loss": 1.4382,
      "step": 86410
    },
    {
      "epoch": 43.957273652085455,
      "grad_norm": 40.29996871948242,
      "learning_rate": 6.042726347914548e-06,
      "loss": 1.5574,
      "step": 86420
    },
    {
      "epoch": 43.96236012207528,
      "grad_norm": 41.72974395751953,
      "learning_rate": 6.03763987792472e-06,
      "loss": 1.5178,
      "step": 86430
    },
    {
      "epoch": 43.96744659206511,
      "grad_norm": 47.72896194458008,
      "learning_rate": 6.032553407934894e-06,
      "loss": 1.5822,
      "step": 86440
    },
    {
      "epoch": 43.972533062054936,
      "grad_norm": 37.31395721435547,
      "learning_rate": 6.027466937945066e-06,
      "loss": 1.5308,
      "step": 86450
    },
    {
      "epoch": 43.97761953204476,
      "grad_norm": 41.42243194580078,
      "learning_rate": 6.022380467955239e-06,
      "loss": 1.4768,
      "step": 86460
    },
    {
      "epoch": 43.98270600203459,
      "grad_norm": 42.29304885864258,
      "learning_rate": 6.017293997965413e-06,
      "loss": 1.4823,
      "step": 86470
    },
    {
      "epoch": 43.98779247202442,
      "grad_norm": 40.2358283996582,
      "learning_rate": 6.012207527975585e-06,
      "loss": 1.5289,
      "step": 86480
    },
    {
      "epoch": 43.992878942014244,
      "grad_norm": 50.263397216796875,
      "learning_rate": 6.007121057985758e-06,
      "loss": 1.4777,
      "step": 86490
    },
    {
      "epoch": 43.99796541200407,
      "grad_norm": 41.99710464477539,
      "learning_rate": 6.002034587995931e-06,
      "loss": 1.5282,
      "step": 86500
    },
    {
      "epoch": 44.0,
      "eval_loss": 5.113205432891846,
      "eval_runtime": 2.7034,
      "eval_samples_per_second": 1026.467,
      "eval_steps_per_second": 128.355,
      "step": 86504
    },
    {
      "epoch": 44.0030518819939,
      "grad_norm": 45.99089813232422,
      "learning_rate": 5.996948118006104e-06,
      "loss": 1.5582,
      "step": 86510
    },
    {
      "epoch": 44.008138351983725,
      "grad_norm": 33.371395111083984,
      "learning_rate": 5.9918616480162766e-06,
      "loss": 1.538,
      "step": 86520
    },
    {
      "epoch": 44.01322482197355,
      "grad_norm": 43.552520751953125,
      "learning_rate": 5.98677517802645e-06,
      "loss": 1.501,
      "step": 86530
    },
    {
      "epoch": 44.01831129196338,
      "grad_norm": 40.54119873046875,
      "learning_rate": 5.981688708036623e-06,
      "loss": 1.562,
      "step": 86540
    },
    {
      "epoch": 44.023397761953206,
      "grad_norm": 51.853660583496094,
      "learning_rate": 5.976602238046796e-06,
      "loss": 1.5554,
      "step": 86550
    },
    {
      "epoch": 44.02848423194303,
      "grad_norm": 52.07240676879883,
      "learning_rate": 5.971515768056969e-06,
      "loss": 1.5362,
      "step": 86560
    },
    {
      "epoch": 44.03357070193286,
      "grad_norm": 52.872840881347656,
      "learning_rate": 5.966429298067141e-06,
      "loss": 1.5544,
      "step": 86570
    },
    {
      "epoch": 44.03865717192269,
      "grad_norm": 35.000247955322266,
      "learning_rate": 5.9613428280773146e-06,
      "loss": 1.5681,
      "step": 86580
    },
    {
      "epoch": 44.04374364191251,
      "grad_norm": 39.670562744140625,
      "learning_rate": 5.956256358087488e-06,
      "loss": 1.5139,
      "step": 86590
    },
    {
      "epoch": 44.04883011190234,
      "grad_norm": 54.243133544921875,
      "learning_rate": 5.95116988809766e-06,
      "loss": 1.5558,
      "step": 86600
    },
    {
      "epoch": 44.05391658189217,
      "grad_norm": 38.76536178588867,
      "learning_rate": 5.9460834181078335e-06,
      "loss": 1.5543,
      "step": 86610
    },
    {
      "epoch": 44.059003051881994,
      "grad_norm": 43.21581268310547,
      "learning_rate": 5.940996948118006e-06,
      "loss": 1.5692,
      "step": 86620
    },
    {
      "epoch": 44.06408952187182,
      "grad_norm": 38.702842712402344,
      "learning_rate": 5.935910478128179e-06,
      "loss": 1.4755,
      "step": 86630
    },
    {
      "epoch": 44.06917599186165,
      "grad_norm": 45.32529830932617,
      "learning_rate": 5.9308240081383525e-06,
      "loss": 1.5073,
      "step": 86640
    },
    {
      "epoch": 44.074262461851475,
      "grad_norm": 51.96444320678711,
      "learning_rate": 5.925737538148525e-06,
      "loss": 1.6055,
      "step": 86650
    },
    {
      "epoch": 44.0793489318413,
      "grad_norm": 53.2107048034668,
      "learning_rate": 5.920651068158698e-06,
      "loss": 1.5423,
      "step": 86660
    },
    {
      "epoch": 44.08443540183113,
      "grad_norm": 42.960411071777344,
      "learning_rate": 5.9155645981688715e-06,
      "loss": 1.5444,
      "step": 86670
    },
    {
      "epoch": 44.089521871820956,
      "grad_norm": 36.79694366455078,
      "learning_rate": 5.910478128179044e-06,
      "loss": 1.4837,
      "step": 86680
    },
    {
      "epoch": 44.09460834181078,
      "grad_norm": 40.69112777709961,
      "learning_rate": 5.905391658189216e-06,
      "loss": 1.4755,
      "step": 86690
    },
    {
      "epoch": 44.09969481180061,
      "grad_norm": 38.1114616394043,
      "learning_rate": 5.90030518819939e-06,
      "loss": 1.5232,
      "step": 86700
    },
    {
      "epoch": 44.10478128179044,
      "grad_norm": 44.09449768066406,
      "learning_rate": 5.895218718209563e-06,
      "loss": 1.4951,
      "step": 86710
    },
    {
      "epoch": 44.109867751780264,
      "grad_norm": 44.158329010009766,
      "learning_rate": 5.890132248219736e-06,
      "loss": 1.5189,
      "step": 86720
    },
    {
      "epoch": 44.11495422177009,
      "grad_norm": 35.28575134277344,
      "learning_rate": 5.885045778229909e-06,
      "loss": 1.5208,
      "step": 86730
    },
    {
      "epoch": 44.12004069175992,
      "grad_norm": 42.82830047607422,
      "learning_rate": 5.879959308240081e-06,
      "loss": 1.4744,
      "step": 86740
    },
    {
      "epoch": 44.125127161749745,
      "grad_norm": 39.197601318359375,
      "learning_rate": 5.874872838250254e-06,
      "loss": 1.4822,
      "step": 86750
    },
    {
      "epoch": 44.13021363173957,
      "grad_norm": 37.78278350830078,
      "learning_rate": 5.869786368260428e-06,
      "loss": 1.55,
      "step": 86760
    },
    {
      "epoch": 44.1353001017294,
      "grad_norm": 45.5598030090332,
      "learning_rate": 5.8646998982706e-06,
      "loss": 1.4821,
      "step": 86770
    },
    {
      "epoch": 44.140386571719226,
      "grad_norm": 39.753135681152344,
      "learning_rate": 5.859613428280773e-06,
      "loss": 1.4648,
      "step": 86780
    },
    {
      "epoch": 44.14547304170905,
      "grad_norm": 43.339515686035156,
      "learning_rate": 5.854526958290947e-06,
      "loss": 1.5758,
      "step": 86790
    },
    {
      "epoch": 44.15055951169888,
      "grad_norm": 41.76919174194336,
      "learning_rate": 5.849440488301119e-06,
      "loss": 1.522,
      "step": 86800
    },
    {
      "epoch": 44.15564598168871,
      "grad_norm": 36.31974411010742,
      "learning_rate": 5.844354018311292e-06,
      "loss": 1.5388,
      "step": 86810
    },
    {
      "epoch": 44.160732451678534,
      "grad_norm": 45.32482147216797,
      "learning_rate": 5.839267548321465e-06,
      "loss": 1.5569,
      "step": 86820
    },
    {
      "epoch": 44.16581892166836,
      "grad_norm": 41.76974105834961,
      "learning_rate": 5.834181078331638e-06,
      "loss": 1.5334,
      "step": 86830
    },
    {
      "epoch": 44.17090539165819,
      "grad_norm": 39.52693176269531,
      "learning_rate": 5.829094608341811e-06,
      "loss": 1.4981,
      "step": 86840
    },
    {
      "epoch": 44.175991861648015,
      "grad_norm": 44.68505096435547,
      "learning_rate": 5.824008138351985e-06,
      "loss": 1.5058,
      "step": 86850
    },
    {
      "epoch": 44.18107833163784,
      "grad_norm": 55.2398567199707,
      "learning_rate": 5.818921668362156e-06,
      "loss": 1.4903,
      "step": 86860
    },
    {
      "epoch": 44.18616480162767,
      "grad_norm": 43.86407470703125,
      "learning_rate": 5.8138351983723296e-06,
      "loss": 1.474,
      "step": 86870
    },
    {
      "epoch": 44.191251271617496,
      "grad_norm": 39.13399887084961,
      "learning_rate": 5.808748728382503e-06,
      "loss": 1.5518,
      "step": 86880
    },
    {
      "epoch": 44.19633774160732,
      "grad_norm": 46.53506088256836,
      "learning_rate": 5.803662258392676e-06,
      "loss": 1.5502,
      "step": 86890
    },
    {
      "epoch": 44.20142421159715,
      "grad_norm": 36.68197250366211,
      "learning_rate": 5.7985757884028486e-06,
      "loss": 1.5063,
      "step": 86900
    },
    {
      "epoch": 44.20651068158698,
      "grad_norm": 58.35795974731445,
      "learning_rate": 5.793489318413022e-06,
      "loss": 1.5693,
      "step": 86910
    },
    {
      "epoch": 44.211597151576804,
      "grad_norm": 40.78373718261719,
      "learning_rate": 5.788402848423194e-06,
      "loss": 1.5427,
      "step": 86920
    },
    {
      "epoch": 44.21668362156663,
      "grad_norm": 39.268856048583984,
      "learning_rate": 5.7833163784333676e-06,
      "loss": 1.4879,
      "step": 86930
    },
    {
      "epoch": 44.22177009155646,
      "grad_norm": 48.44450378417969,
      "learning_rate": 5.77822990844354e-06,
      "loss": 1.5237,
      "step": 86940
    },
    {
      "epoch": 44.226856561546285,
      "grad_norm": 47.48917007446289,
      "learning_rate": 5.773143438453713e-06,
      "loss": 1.6025,
      "step": 86950
    },
    {
      "epoch": 44.23194303153611,
      "grad_norm": 41.38256072998047,
      "learning_rate": 5.7680569684638865e-06,
      "loss": 1.5057,
      "step": 86960
    },
    {
      "epoch": 44.23702950152594,
      "grad_norm": 43.305908203125,
      "learning_rate": 5.76297049847406e-06,
      "loss": 1.5293,
      "step": 86970
    },
    {
      "epoch": 44.242115971515766,
      "grad_norm": 36.85192108154297,
      "learning_rate": 5.757884028484232e-06,
      "loss": 1.532,
      "step": 86980
    },
    {
      "epoch": 44.24720244150559,
      "grad_norm": 42.397056579589844,
      "learning_rate": 5.752797558494405e-06,
      "loss": 1.5258,
      "step": 86990
    },
    {
      "epoch": 44.25228891149542,
      "grad_norm": 37.93321228027344,
      "learning_rate": 5.747711088504578e-06,
      "loss": 1.5208,
      "step": 87000
    },
    {
      "epoch": 44.25737538148525,
      "grad_norm": 48.02176284790039,
      "learning_rate": 5.742624618514751e-06,
      "loss": 1.5235,
      "step": 87010
    },
    {
      "epoch": 44.262461851475074,
      "grad_norm": 49.87272262573242,
      "learning_rate": 5.7375381485249245e-06,
      "loss": 1.5581,
      "step": 87020
    },
    {
      "epoch": 44.2675483214649,
      "grad_norm": 46.86779022216797,
      "learning_rate": 5.732451678535097e-06,
      "loss": 1.5238,
      "step": 87030
    },
    {
      "epoch": 44.27263479145473,
      "grad_norm": 42.54743576049805,
      "learning_rate": 5.727365208545269e-06,
      "loss": 1.5285,
      "step": 87040
    },
    {
      "epoch": 44.277721261444555,
      "grad_norm": 36.4118766784668,
      "learning_rate": 5.722278738555443e-06,
      "loss": 1.5229,
      "step": 87050
    },
    {
      "epoch": 44.28280773143438,
      "grad_norm": 40.28772735595703,
      "learning_rate": 5.717192268565616e-06,
      "loss": 1.4703,
      "step": 87060
    },
    {
      "epoch": 44.28789420142421,
      "grad_norm": 45.30169677734375,
      "learning_rate": 5.712105798575788e-06,
      "loss": 1.4392,
      "step": 87070
    },
    {
      "epoch": 44.292980671414035,
      "grad_norm": 36.58491897583008,
      "learning_rate": 5.707019328585962e-06,
      "loss": 1.5119,
      "step": 87080
    },
    {
      "epoch": 44.29806714140386,
      "grad_norm": 40.562252044677734,
      "learning_rate": 5.701932858596135e-06,
      "loss": 1.4938,
      "step": 87090
    },
    {
      "epoch": 44.30315361139369,
      "grad_norm": 41.34088134765625,
      "learning_rate": 5.696846388606307e-06,
      "loss": 1.437,
      "step": 87100
    },
    {
      "epoch": 44.308240081383516,
      "grad_norm": 34.19093322753906,
      "learning_rate": 5.691759918616481e-06,
      "loss": 1.4779,
      "step": 87110
    },
    {
      "epoch": 44.31332655137334,
      "grad_norm": 41.77174758911133,
      "learning_rate": 5.686673448626653e-06,
      "loss": 1.4994,
      "step": 87120
    },
    {
      "epoch": 44.31841302136317,
      "grad_norm": 39.75888442993164,
      "learning_rate": 5.681586978636826e-06,
      "loss": 1.5385,
      "step": 87130
    },
    {
      "epoch": 44.323499491353004,
      "grad_norm": 50.83283233642578,
      "learning_rate": 5.676500508647e-06,
      "loss": 1.4434,
      "step": 87140
    },
    {
      "epoch": 44.32858596134283,
      "grad_norm": 39.68117904663086,
      "learning_rate": 5.671414038657172e-06,
      "loss": 1.5962,
      "step": 87150
    },
    {
      "epoch": 44.33367243133266,
      "grad_norm": 49.933563232421875,
      "learning_rate": 5.6663275686673446e-06,
      "loss": 1.5301,
      "step": 87160
    },
    {
      "epoch": 44.338758901322485,
      "grad_norm": 42.25938415527344,
      "learning_rate": 5.661241098677518e-06,
      "loss": 1.5296,
      "step": 87170
    },
    {
      "epoch": 44.34384537131231,
      "grad_norm": 48.882667541503906,
      "learning_rate": 5.656154628687691e-06,
      "loss": 1.5291,
      "step": 87180
    },
    {
      "epoch": 44.34893184130214,
      "grad_norm": 48.078269958496094,
      "learning_rate": 5.651068158697864e-06,
      "loss": 1.5466,
      "step": 87190
    },
    {
      "epoch": 44.354018311291966,
      "grad_norm": 47.36149978637695,
      "learning_rate": 5.645981688708037e-06,
      "loss": 1.5708,
      "step": 87200
    },
    {
      "epoch": 44.35910478128179,
      "grad_norm": 36.84480667114258,
      "learning_rate": 5.64089521871821e-06,
      "loss": 1.4295,
      "step": 87210
    },
    {
      "epoch": 44.36419125127162,
      "grad_norm": 40.19779586791992,
      "learning_rate": 5.6358087487283826e-06,
      "loss": 1.5469,
      "step": 87220
    },
    {
      "epoch": 44.36927772126145,
      "grad_norm": 38.496307373046875,
      "learning_rate": 5.630722278738556e-06,
      "loss": 1.5695,
      "step": 87230
    },
    {
      "epoch": 44.374364191251274,
      "grad_norm": 37.13945388793945,
      "learning_rate": 5.625635808748728e-06,
      "loss": 1.5291,
      "step": 87240
    },
    {
      "epoch": 44.3794506612411,
      "grad_norm": 46.41157150268555,
      "learning_rate": 5.6205493387589016e-06,
      "loss": 1.6371,
      "step": 87250
    },
    {
      "epoch": 44.38453713123093,
      "grad_norm": 31.314472198486328,
      "learning_rate": 5.615462868769075e-06,
      "loss": 1.5001,
      "step": 87260
    },
    {
      "epoch": 44.389623601220755,
      "grad_norm": 37.273563385009766,
      "learning_rate": 5.610376398779247e-06,
      "loss": 1.5099,
      "step": 87270
    },
    {
      "epoch": 44.39471007121058,
      "grad_norm": 49.41252136230469,
      "learning_rate": 5.6052899287894206e-06,
      "loss": 1.4883,
      "step": 87280
    },
    {
      "epoch": 44.39979654120041,
      "grad_norm": 49.50080871582031,
      "learning_rate": 5.600203458799593e-06,
      "loss": 1.5303,
      "step": 87290
    },
    {
      "epoch": 44.404883011190236,
      "grad_norm": 42.06966781616211,
      "learning_rate": 5.595116988809766e-06,
      "loss": 1.5026,
      "step": 87300
    },
    {
      "epoch": 44.40996948118006,
      "grad_norm": 39.372276306152344,
      "learning_rate": 5.5900305188199395e-06,
      "loss": 1.5106,
      "step": 87310
    },
    {
      "epoch": 44.41505595116989,
      "grad_norm": 50.50760269165039,
      "learning_rate": 5.584944048830113e-06,
      "loss": 1.6038,
      "step": 87320
    },
    {
      "epoch": 44.42014242115972,
      "grad_norm": 57.54548263549805,
      "learning_rate": 5.579857578840285e-06,
      "loss": 1.5437,
      "step": 87330
    },
    {
      "epoch": 44.425228891149544,
      "grad_norm": 37.799049377441406,
      "learning_rate": 5.574771108850458e-06,
      "loss": 1.4913,
      "step": 87340
    },
    {
      "epoch": 44.43031536113937,
      "grad_norm": 36.861759185791016,
      "learning_rate": 5.569684638860631e-06,
      "loss": 1.5984,
      "step": 87350
    },
    {
      "epoch": 44.4354018311292,
      "grad_norm": 42.696861267089844,
      "learning_rate": 5.564598168870804e-06,
      "loss": 1.5582,
      "step": 87360
    },
    {
      "epoch": 44.440488301119025,
      "grad_norm": 36.30614471435547,
      "learning_rate": 5.559511698880977e-06,
      "loss": 1.4711,
      "step": 87370
    },
    {
      "epoch": 44.44557477110885,
      "grad_norm": 41.20140838623047,
      "learning_rate": 5.55442522889115e-06,
      "loss": 1.4992,
      "step": 87380
    },
    {
      "epoch": 44.45066124109868,
      "grad_norm": 49.584556579589844,
      "learning_rate": 5.549338758901322e-06,
      "loss": 1.5047,
      "step": 87390
    },
    {
      "epoch": 44.455747711088506,
      "grad_norm": 42.37450408935547,
      "learning_rate": 5.544252288911496e-06,
      "loss": 1.553,
      "step": 87400
    },
    {
      "epoch": 44.46083418107833,
      "grad_norm": 35.52458572387695,
      "learning_rate": 5.539165818921668e-06,
      "loss": 1.4055,
      "step": 87410
    },
    {
      "epoch": 44.46592065106816,
      "grad_norm": 48.612632751464844,
      "learning_rate": 5.534079348931841e-06,
      "loss": 1.5611,
      "step": 87420
    },
    {
      "epoch": 44.47100712105799,
      "grad_norm": 51.73784255981445,
      "learning_rate": 5.528992878942015e-06,
      "loss": 1.5337,
      "step": 87430
    },
    {
      "epoch": 44.476093591047814,
      "grad_norm": 41.30232620239258,
      "learning_rate": 5.523906408952188e-06,
      "loss": 1.5406,
      "step": 87440
    },
    {
      "epoch": 44.48118006103764,
      "grad_norm": 38.19368362426758,
      "learning_rate": 5.51881993896236e-06,
      "loss": 1.6537,
      "step": 87450
    },
    {
      "epoch": 44.48626653102747,
      "grad_norm": 47.5050163269043,
      "learning_rate": 5.513733468972533e-06,
      "loss": 1.5178,
      "step": 87460
    },
    {
      "epoch": 44.491353001017295,
      "grad_norm": 36.024314880371094,
      "learning_rate": 5.508646998982706e-06,
      "loss": 1.5541,
      "step": 87470
    },
    {
      "epoch": 44.49643947100712,
      "grad_norm": 41.72972106933594,
      "learning_rate": 5.503560528992879e-06,
      "loss": 1.5215,
      "step": 87480
    },
    {
      "epoch": 44.50152594099695,
      "grad_norm": 54.515621185302734,
      "learning_rate": 5.498474059003053e-06,
      "loss": 1.5281,
      "step": 87490
    },
    {
      "epoch": 44.506612410986776,
      "grad_norm": 43.665985107421875,
      "learning_rate": 5.493387589013225e-06,
      "loss": 1.6472,
      "step": 87500
    },
    {
      "epoch": 44.5116988809766,
      "grad_norm": 43.062896728515625,
      "learning_rate": 5.4883011190233976e-06,
      "loss": 1.4797,
      "step": 87510
    },
    {
      "epoch": 44.51678535096643,
      "grad_norm": 38.975181579589844,
      "learning_rate": 5.483214649033571e-06,
      "loss": 1.5067,
      "step": 87520
    },
    {
      "epoch": 44.52187182095626,
      "grad_norm": 38.06362533569336,
      "learning_rate": 5.478128179043744e-06,
      "loss": 1.5573,
      "step": 87530
    },
    {
      "epoch": 44.526958290946084,
      "grad_norm": 43.30840301513672,
      "learning_rate": 5.4730417090539166e-06,
      "loss": 1.5802,
      "step": 87540
    },
    {
      "epoch": 44.53204476093591,
      "grad_norm": 42.671653747558594,
      "learning_rate": 5.46795523906409e-06,
      "loss": 1.4942,
      "step": 87550
    },
    {
      "epoch": 44.53713123092574,
      "grad_norm": 35.41117858886719,
      "learning_rate": 5.462868769074263e-06,
      "loss": 1.5453,
      "step": 87560
    },
    {
      "epoch": 44.542217700915565,
      "grad_norm": 41.13655090332031,
      "learning_rate": 5.4577822990844356e-06,
      "loss": 1.4297,
      "step": 87570
    },
    {
      "epoch": 44.54730417090539,
      "grad_norm": 43.845985412597656,
      "learning_rate": 5.452695829094608e-06,
      "loss": 1.5641,
      "step": 87580
    },
    {
      "epoch": 44.55239064089522,
      "grad_norm": 45.73919677734375,
      "learning_rate": 5.447609359104781e-06,
      "loss": 1.5144,
      "step": 87590
    },
    {
      "epoch": 44.557477110885046,
      "grad_norm": 43.146095275878906,
      "learning_rate": 5.4425228891149546e-06,
      "loss": 1.5338,
      "step": 87600
    },
    {
      "epoch": 44.56256358087487,
      "grad_norm": 39.69234848022461,
      "learning_rate": 5.437436419125128e-06,
      "loss": 1.5527,
      "step": 87610
    },
    {
      "epoch": 44.5676500508647,
      "grad_norm": 42.9564094543457,
      "learning_rate": 5.4323499491353e-06,
      "loss": 1.4263,
      "step": 87620
    },
    {
      "epoch": 44.57273652085453,
      "grad_norm": 44.24296569824219,
      "learning_rate": 5.427263479145473e-06,
      "loss": 1.5501,
      "step": 87630
    },
    {
      "epoch": 44.57782299084435,
      "grad_norm": 37.9853630065918,
      "learning_rate": 5.422177009155646e-06,
      "loss": 1.5181,
      "step": 87640
    },
    {
      "epoch": 44.58290946083418,
      "grad_norm": 41.222774505615234,
      "learning_rate": 5.417090539165819e-06,
      "loss": 1.5297,
      "step": 87650
    },
    {
      "epoch": 44.58799593082401,
      "grad_norm": 45.16448974609375,
      "learning_rate": 5.4120040691759925e-06,
      "loss": 1.4462,
      "step": 87660
    },
    {
      "epoch": 44.593082400813834,
      "grad_norm": 40.71355438232422,
      "learning_rate": 5.406917599186165e-06,
      "loss": 1.4046,
      "step": 87670
    },
    {
      "epoch": 44.59816887080366,
      "grad_norm": 43.131324768066406,
      "learning_rate": 5.401831129196338e-06,
      "loss": 1.4855,
      "step": 87680
    },
    {
      "epoch": 44.60325534079349,
      "grad_norm": 37.13282775878906,
      "learning_rate": 5.396744659206511e-06,
      "loss": 1.5976,
      "step": 87690
    },
    {
      "epoch": 44.608341810783315,
      "grad_norm": 37.09953308105469,
      "learning_rate": 5.391658189216684e-06,
      "loss": 1.4781,
      "step": 87700
    },
    {
      "epoch": 44.61342828077314,
      "grad_norm": 42.86589431762695,
      "learning_rate": 5.386571719226856e-06,
      "loss": 1.4897,
      "step": 87710
    },
    {
      "epoch": 44.61851475076297,
      "grad_norm": 35.88092803955078,
      "learning_rate": 5.38148524923703e-06,
      "loss": 1.5536,
      "step": 87720
    },
    {
      "epoch": 44.623601220752796,
      "grad_norm": 43.005409240722656,
      "learning_rate": 5.376398779247203e-06,
      "loss": 1.5488,
      "step": 87730
    },
    {
      "epoch": 44.62868769074262,
      "grad_norm": 42.04304504394531,
      "learning_rate": 5.371312309257375e-06,
      "loss": 1.5008,
      "step": 87740
    },
    {
      "epoch": 44.63377416073245,
      "grad_norm": 34.40863037109375,
      "learning_rate": 5.366225839267548e-06,
      "loss": 1.5504,
      "step": 87750
    },
    {
      "epoch": 44.63886063072228,
      "grad_norm": 48.97463607788086,
      "learning_rate": 5.361139369277721e-06,
      "loss": 1.555,
      "step": 87760
    },
    {
      "epoch": 44.643947100712104,
      "grad_norm": 40.254554748535156,
      "learning_rate": 5.356052899287894e-06,
      "loss": 1.5038,
      "step": 87770
    },
    {
      "epoch": 44.64903357070193,
      "grad_norm": 53.15716552734375,
      "learning_rate": 5.350966429298068e-06,
      "loss": 1.4889,
      "step": 87780
    },
    {
      "epoch": 44.65412004069176,
      "grad_norm": 37.85044860839844,
      "learning_rate": 5.345879959308241e-06,
      "loss": 1.5861,
      "step": 87790
    },
    {
      "epoch": 44.659206510681585,
      "grad_norm": 31.705228805541992,
      "learning_rate": 5.340793489318413e-06,
      "loss": 1.578,
      "step": 87800
    },
    {
      "epoch": 44.66429298067141,
      "grad_norm": 34.960880279541016,
      "learning_rate": 5.335707019328586e-06,
      "loss": 1.5318,
      "step": 87810
    },
    {
      "epoch": 44.66937945066124,
      "grad_norm": 38.543033599853516,
      "learning_rate": 5.330620549338759e-06,
      "loss": 1.5557,
      "step": 87820
    },
    {
      "epoch": 44.674465920651066,
      "grad_norm": 45.90802764892578,
      "learning_rate": 5.325534079348932e-06,
      "loss": 1.4648,
      "step": 87830
    },
    {
      "epoch": 44.67955239064089,
      "grad_norm": 47.92131042480469,
      "learning_rate": 5.320447609359105e-06,
      "loss": 1.4981,
      "step": 87840
    },
    {
      "epoch": 44.68463886063072,
      "grad_norm": 38.313533782958984,
      "learning_rate": 5.315361139369278e-06,
      "loss": 1.5632,
      "step": 87850
    },
    {
      "epoch": 44.68972533062055,
      "grad_norm": 37.13953399658203,
      "learning_rate": 5.3102746693794506e-06,
      "loss": 1.5958,
      "step": 87860
    },
    {
      "epoch": 44.694811800610374,
      "grad_norm": 44.57814025878906,
      "learning_rate": 5.305188199389624e-06,
      "loss": 1.5476,
      "step": 87870
    },
    {
      "epoch": 44.6998982706002,
      "grad_norm": 48.0518913269043,
      "learning_rate": 5.300101729399796e-06,
      "loss": 1.5167,
      "step": 87880
    },
    {
      "epoch": 44.70498474059003,
      "grad_norm": 39.709510803222656,
      "learning_rate": 5.2950152594099696e-06,
      "loss": 1.5146,
      "step": 87890
    },
    {
      "epoch": 44.710071210579855,
      "grad_norm": 36.608882904052734,
      "learning_rate": 5.289928789420143e-06,
      "loss": 1.5609,
      "step": 87900
    },
    {
      "epoch": 44.71515768056968,
      "grad_norm": 45.72115707397461,
      "learning_rate": 5.284842319430316e-06,
      "loss": 1.5555,
      "step": 87910
    },
    {
      "epoch": 44.72024415055951,
      "grad_norm": 45.64609146118164,
      "learning_rate": 5.2797558494404886e-06,
      "loss": 1.4604,
      "step": 87920
    },
    {
      "epoch": 44.725330620549336,
      "grad_norm": 42.45969009399414,
      "learning_rate": 5.274669379450661e-06,
      "loss": 1.5553,
      "step": 87930
    },
    {
      "epoch": 44.73041709053916,
      "grad_norm": 42.156673431396484,
      "learning_rate": 5.269582909460834e-06,
      "loss": 1.5192,
      "step": 87940
    },
    {
      "epoch": 44.73550356052899,
      "grad_norm": 36.46780776977539,
      "learning_rate": 5.2644964394710076e-06,
      "loss": 1.4955,
      "step": 87950
    },
    {
      "epoch": 44.74059003051882,
      "grad_norm": 42.107154846191406,
      "learning_rate": 5.259409969481181e-06,
      "loss": 1.5519,
      "step": 87960
    },
    {
      "epoch": 44.745676500508644,
      "grad_norm": 50.41670227050781,
      "learning_rate": 5.254323499491353e-06,
      "loss": 1.4997,
      "step": 87970
    },
    {
      "epoch": 44.75076297049847,
      "grad_norm": 45.67921829223633,
      "learning_rate": 5.2492370295015265e-06,
      "loss": 1.487,
      "step": 87980
    },
    {
      "epoch": 44.7558494404883,
      "grad_norm": 40.673160552978516,
      "learning_rate": 5.244150559511699e-06,
      "loss": 1.6178,
      "step": 87990
    },
    {
      "epoch": 44.760935910478125,
      "grad_norm": 41.117801666259766,
      "learning_rate": 5.239064089521872e-06,
      "loss": 1.528,
      "step": 88000
    },
    {
      "epoch": 44.76602238046795,
      "grad_norm": 48.26901626586914,
      "learning_rate": 5.233977619532045e-06,
      "loss": 1.5582,
      "step": 88010
    },
    {
      "epoch": 44.77110885045778,
      "grad_norm": 59.79054260253906,
      "learning_rate": 5.228891149542218e-06,
      "loss": 1.4708,
      "step": 88020
    },
    {
      "epoch": 44.77619532044761,
      "grad_norm": 46.9425048828125,
      "learning_rate": 5.223804679552391e-06,
      "loss": 1.5303,
      "step": 88030
    },
    {
      "epoch": 44.78128179043744,
      "grad_norm": 49.993404388427734,
      "learning_rate": 5.218718209562564e-06,
      "loss": 1.5246,
      "step": 88040
    },
    {
      "epoch": 44.78636826042727,
      "grad_norm": 38.457332611083984,
      "learning_rate": 5.213631739572736e-06,
      "loss": 1.5236,
      "step": 88050
    },
    {
      "epoch": 44.791454730417094,
      "grad_norm": 39.01847839355469,
      "learning_rate": 5.208545269582909e-06,
      "loss": 1.5421,
      "step": 88060
    },
    {
      "epoch": 44.79654120040692,
      "grad_norm": 39.95354080200195,
      "learning_rate": 5.203458799593083e-06,
      "loss": 1.5382,
      "step": 88070
    },
    {
      "epoch": 44.80162767039675,
      "grad_norm": 49.519622802734375,
      "learning_rate": 5.198372329603256e-06,
      "loss": 1.5633,
      "step": 88080
    },
    {
      "epoch": 44.806714140386575,
      "grad_norm": 39.73836898803711,
      "learning_rate": 5.193285859613429e-06,
      "loss": 1.5871,
      "step": 88090
    },
    {
      "epoch": 44.8118006103764,
      "grad_norm": 49.49146270751953,
      "learning_rate": 5.188199389623602e-06,
      "loss": 1.481,
      "step": 88100
    },
    {
      "epoch": 44.81688708036623,
      "grad_norm": 36.86838912963867,
      "learning_rate": 5.183112919633774e-06,
      "loss": 1.6228,
      "step": 88110
    },
    {
      "epoch": 44.821973550356056,
      "grad_norm": 40.60741424560547,
      "learning_rate": 5.178026449643947e-06,
      "loss": 1.4482,
      "step": 88120
    },
    {
      "epoch": 44.82706002034588,
      "grad_norm": 38.26103591918945,
      "learning_rate": 5.172939979654121e-06,
      "loss": 1.5376,
      "step": 88130
    },
    {
      "epoch": 44.83214649033571,
      "grad_norm": 46.759979248046875,
      "learning_rate": 5.167853509664293e-06,
      "loss": 1.5003,
      "step": 88140
    },
    {
      "epoch": 44.83723296032554,
      "grad_norm": 47.752017974853516,
      "learning_rate": 5.162767039674466e-06,
      "loss": 1.4988,
      "step": 88150
    },
    {
      "epoch": 44.842319430315364,
      "grad_norm": 48.8634033203125,
      "learning_rate": 5.157680569684639e-06,
      "loss": 1.5709,
      "step": 88160
    },
    {
      "epoch": 44.84740590030519,
      "grad_norm": 38.31473922729492,
      "learning_rate": 5.152594099694812e-06,
      "loss": 1.5452,
      "step": 88170
    },
    {
      "epoch": 44.85249237029502,
      "grad_norm": 52.22101593017578,
      "learning_rate": 5.1475076297049846e-06,
      "loss": 1.498,
      "step": 88180
    },
    {
      "epoch": 44.857578840284845,
      "grad_norm": 43.47050476074219,
      "learning_rate": 5.142421159715158e-06,
      "loss": 1.5511,
      "step": 88190
    },
    {
      "epoch": 44.86266531027467,
      "grad_norm": 41.093238830566406,
      "learning_rate": 5.137334689725331e-06,
      "loss": 1.5156,
      "step": 88200
    },
    {
      "epoch": 44.8677517802645,
      "grad_norm": 53.0775146484375,
      "learning_rate": 5.132248219735504e-06,
      "loss": 1.5965,
      "step": 88210
    },
    {
      "epoch": 44.872838250254325,
      "grad_norm": 41.47314453125,
      "learning_rate": 5.127161749745677e-06,
      "loss": 1.5001,
      "step": 88220
    },
    {
      "epoch": 44.87792472024415,
      "grad_norm": 56.18439865112305,
      "learning_rate": 5.122075279755849e-06,
      "loss": 1.4464,
      "step": 88230
    },
    {
      "epoch": 44.88301119023398,
      "grad_norm": 43.91520690917969,
      "learning_rate": 5.1169888097660226e-06,
      "loss": 1.5472,
      "step": 88240
    },
    {
      "epoch": 44.888097660223806,
      "grad_norm": 43.843414306640625,
      "learning_rate": 5.111902339776196e-06,
      "loss": 1.5468,
      "step": 88250
    },
    {
      "epoch": 44.89318413021363,
      "grad_norm": 44.19247817993164,
      "learning_rate": 5.106815869786369e-06,
      "loss": 1.4971,
      "step": 88260
    },
    {
      "epoch": 44.89827060020346,
      "grad_norm": 46.119850158691406,
      "learning_rate": 5.1017293997965416e-06,
      "loss": 1.5876,
      "step": 88270
    },
    {
      "epoch": 44.90335707019329,
      "grad_norm": 39.402923583984375,
      "learning_rate": 5.096642929806714e-06,
      "loss": 1.5143,
      "step": 88280
    },
    {
      "epoch": 44.908443540183114,
      "grad_norm": 54.83552169799805,
      "learning_rate": 5.091556459816887e-06,
      "loss": 1.5244,
      "step": 88290
    },
    {
      "epoch": 44.91353001017294,
      "grad_norm": 38.856998443603516,
      "learning_rate": 5.0864699898270606e-06,
      "loss": 1.5174,
      "step": 88300
    },
    {
      "epoch": 44.91861648016277,
      "grad_norm": 48.851173400878906,
      "learning_rate": 5.081383519837233e-06,
      "loss": 1.5023,
      "step": 88310
    },
    {
      "epoch": 44.923702950152595,
      "grad_norm": 35.31443405151367,
      "learning_rate": 5.076297049847406e-06,
      "loss": 1.4875,
      "step": 88320
    },
    {
      "epoch": 44.92878942014242,
      "grad_norm": 47.32018280029297,
      "learning_rate": 5.0712105798575795e-06,
      "loss": 1.4563,
      "step": 88330
    },
    {
      "epoch": 44.93387589013225,
      "grad_norm": 45.421043395996094,
      "learning_rate": 5.066124109867752e-06,
      "loss": 1.5051,
      "step": 88340
    },
    {
      "epoch": 44.938962360122076,
      "grad_norm": 47.40475082397461,
      "learning_rate": 5.061037639877924e-06,
      "loss": 1.5852,
      "step": 88350
    },
    {
      "epoch": 44.9440488301119,
      "grad_norm": 31.771408081054688,
      "learning_rate": 5.055951169888098e-06,
      "loss": 1.4707,
      "step": 88360
    },
    {
      "epoch": 44.94913530010173,
      "grad_norm": 52.7017822265625,
      "learning_rate": 5.050864699898271e-06,
      "loss": 1.5725,
      "step": 88370
    },
    {
      "epoch": 44.95422177009156,
      "grad_norm": 36.410831451416016,
      "learning_rate": 5.045778229908444e-06,
      "loss": 1.6131,
      "step": 88380
    },
    {
      "epoch": 44.959308240081384,
      "grad_norm": 47.96660614013672,
      "learning_rate": 5.040691759918617e-06,
      "loss": 1.5217,
      "step": 88390
    },
    {
      "epoch": 44.96439471007121,
      "grad_norm": 45.338592529296875,
      "learning_rate": 5.035605289928789e-06,
      "loss": 1.6378,
      "step": 88400
    },
    {
      "epoch": 44.96948118006104,
      "grad_norm": 36.359375,
      "learning_rate": 5.030518819938962e-06,
      "loss": 1.514,
      "step": 88410
    },
    {
      "epoch": 44.974567650050865,
      "grad_norm": 37.152732849121094,
      "learning_rate": 5.025432349949136e-06,
      "loss": 1.5318,
      "step": 88420
    },
    {
      "epoch": 44.97965412004069,
      "grad_norm": 42.41383743286133,
      "learning_rate": 5.020345879959309e-06,
      "loss": 1.4876,
      "step": 88430
    },
    {
      "epoch": 44.98474059003052,
      "grad_norm": 40.46651077270508,
      "learning_rate": 5.015259409969481e-06,
      "loss": 1.5453,
      "step": 88440
    },
    {
      "epoch": 44.989827060020346,
      "grad_norm": 40.456565856933594,
      "learning_rate": 5.010172939979655e-06,
      "loss": 1.5835,
      "step": 88450
    },
    {
      "epoch": 44.99491353001017,
      "grad_norm": 33.49571228027344,
      "learning_rate": 5.005086469989827e-06,
      "loss": 1.5349,
      "step": 88460
    },
    {
      "epoch": 45.0,
      "grad_norm": 55.00492858886719,
      "learning_rate": 5e-06,
      "loss": 1.5252,
      "step": 88470
    },
    {
      "epoch": 45.0,
      "eval_loss": 5.106524467468262,
      "eval_runtime": 2.8102,
      "eval_samples_per_second": 987.489,
      "eval_steps_per_second": 123.481,
      "step": 88470
    },
    {
      "epoch": 45.00508646998983,
      "grad_norm": 48.19247817993164,
      "learning_rate": 4.994913530010173e-06,
      "loss": 1.4236,
      "step": 88480
    },
    {
      "epoch": 45.010172939979654,
      "grad_norm": 45.099552154541016,
      "learning_rate": 4.989827060020346e-06,
      "loss": 1.4777,
      "step": 88490
    },
    {
      "epoch": 45.01525940996948,
      "grad_norm": 43.67214584350586,
      "learning_rate": 4.984740590030519e-06,
      "loss": 1.588,
      "step": 88500
    },
    {
      "epoch": 45.02034587995931,
      "grad_norm": 46.15288162231445,
      "learning_rate": 4.979654120040692e-06,
      "loss": 1.5331,
      "step": 88510
    },
    {
      "epoch": 45.025432349949135,
      "grad_norm": 41.39405059814453,
      "learning_rate": 4.974567650050864e-06,
      "loss": 1.4723,
      "step": 88520
    },
    {
      "epoch": 45.03051881993896,
      "grad_norm": 36.788204193115234,
      "learning_rate": 4.9694811800610376e-06,
      "loss": 1.5199,
      "step": 88530
    },
    {
      "epoch": 45.03560528992879,
      "grad_norm": 40.81499099731445,
      "learning_rate": 4.964394710071211e-06,
      "loss": 1.5043,
      "step": 88540
    },
    {
      "epoch": 45.040691759918616,
      "grad_norm": 34.3900260925293,
      "learning_rate": 4.959308240081384e-06,
      "loss": 1.554,
      "step": 88550
    },
    {
      "epoch": 45.04577822990844,
      "grad_norm": 39.43939208984375,
      "learning_rate": 4.9542217700915566e-06,
      "loss": 1.6029,
      "step": 88560
    },
    {
      "epoch": 45.05086469989827,
      "grad_norm": 41.64844512939453,
      "learning_rate": 4.94913530010173e-06,
      "loss": 1.4528,
      "step": 88570
    },
    {
      "epoch": 45.0559511698881,
      "grad_norm": 37.01781463623047,
      "learning_rate": 4.944048830111902e-06,
      "loss": 1.5307,
      "step": 88580
    },
    {
      "epoch": 45.061037639877924,
      "grad_norm": 47.476810455322266,
      "learning_rate": 4.9389623601220756e-06,
      "loss": 1.5208,
      "step": 88590
    },
    {
      "epoch": 45.06612410986775,
      "grad_norm": 40.92911148071289,
      "learning_rate": 4.933875890132249e-06,
      "loss": 1.4738,
      "step": 88600
    },
    {
      "epoch": 45.07121057985758,
      "grad_norm": 42.446693420410156,
      "learning_rate": 4.928789420142421e-06,
      "loss": 1.4432,
      "step": 88610
    },
    {
      "epoch": 45.076297049847405,
      "grad_norm": 53.458343505859375,
      "learning_rate": 4.9237029501525946e-06,
      "loss": 1.5145,
      "step": 88620
    },
    {
      "epoch": 45.08138351983723,
      "grad_norm": 32.34455490112305,
      "learning_rate": 4.918616480162767e-06,
      "loss": 1.527,
      "step": 88630
    },
    {
      "epoch": 45.08646998982706,
      "grad_norm": 40.805335998535156,
      "learning_rate": 4.91353001017294e-06,
      "loss": 1.4511,
      "step": 88640
    },
    {
      "epoch": 45.091556459816886,
      "grad_norm": 43.126014709472656,
      "learning_rate": 4.908443540183113e-06,
      "loss": 1.503,
      "step": 88650
    },
    {
      "epoch": 45.09664292980671,
      "grad_norm": 47.015960693359375,
      "learning_rate": 4.903357070193286e-06,
      "loss": 1.5372,
      "step": 88660
    },
    {
      "epoch": 45.10172939979654,
      "grad_norm": 39.342002868652344,
      "learning_rate": 4.898270600203459e-06,
      "loss": 1.546,
      "step": 88670
    },
    {
      "epoch": 45.10681586978637,
      "grad_norm": 40.80619812011719,
      "learning_rate": 4.8931841302136325e-06,
      "loss": 1.5408,
      "step": 88680
    },
    {
      "epoch": 45.111902339776194,
      "grad_norm": 43.67615509033203,
      "learning_rate": 4.888097660223805e-06,
      "loss": 1.4932,
      "step": 88690
    },
    {
      "epoch": 45.11698880976602,
      "grad_norm": 40.57148361206055,
      "learning_rate": 4.883011190233977e-06,
      "loss": 1.4999,
      "step": 88700
    },
    {
      "epoch": 45.12207527975585,
      "grad_norm": 43.36944580078125,
      "learning_rate": 4.877924720244151e-06,
      "loss": 1.5281,
      "step": 88710
    },
    {
      "epoch": 45.127161749745675,
      "grad_norm": 42.856300354003906,
      "learning_rate": 4.872838250254324e-06,
      "loss": 1.5473,
      "step": 88720
    },
    {
      "epoch": 45.1322482197355,
      "grad_norm": 37.20988845825195,
      "learning_rate": 4.867751780264497e-06,
      "loss": 1.5626,
      "step": 88730
    },
    {
      "epoch": 45.13733468972533,
      "grad_norm": 44.75261688232422,
      "learning_rate": 4.86266531027467e-06,
      "loss": 1.5461,
      "step": 88740
    },
    {
      "epoch": 45.142421159715155,
      "grad_norm": 52.88767623901367,
      "learning_rate": 4.857578840284842e-06,
      "loss": 1.666,
      "step": 88750
    },
    {
      "epoch": 45.14750762970498,
      "grad_norm": 50.280338287353516,
      "learning_rate": 4.852492370295015e-06,
      "loss": 1.5073,
      "step": 88760
    },
    {
      "epoch": 45.15259409969481,
      "grad_norm": 48.4744758605957,
      "learning_rate": 4.847405900305189e-06,
      "loss": 1.435,
      "step": 88770
    },
    {
      "epoch": 45.157680569684636,
      "grad_norm": 41.952945709228516,
      "learning_rate": 4.842319430315361e-06,
      "loss": 1.512,
      "step": 88780
    },
    {
      "epoch": 45.16276703967446,
      "grad_norm": 41.6141242980957,
      "learning_rate": 4.837232960325534e-06,
      "loss": 1.4886,
      "step": 88790
    },
    {
      "epoch": 45.16785350966429,
      "grad_norm": 44.11463165283203,
      "learning_rate": 4.832146490335708e-06,
      "loss": 1.6126,
      "step": 88800
    },
    {
      "epoch": 45.17293997965412,
      "grad_norm": 50.12845230102539,
      "learning_rate": 4.82706002034588e-06,
      "loss": 1.5005,
      "step": 88810
    },
    {
      "epoch": 45.178026449643944,
      "grad_norm": 42.289894104003906,
      "learning_rate": 4.8219735503560526e-06,
      "loss": 1.5178,
      "step": 88820
    },
    {
      "epoch": 45.18311291963377,
      "grad_norm": 36.92329788208008,
      "learning_rate": 4.816887080366226e-06,
      "loss": 1.4574,
      "step": 88830
    },
    {
      "epoch": 45.1881993896236,
      "grad_norm": 39.616729736328125,
      "learning_rate": 4.811800610376399e-06,
      "loss": 1.4448,
      "step": 88840
    },
    {
      "epoch": 45.193285859613425,
      "grad_norm": 41.267417907714844,
      "learning_rate": 4.806714140386572e-06,
      "loss": 1.4408,
      "step": 88850
    },
    {
      "epoch": 45.19837232960325,
      "grad_norm": 40.831539154052734,
      "learning_rate": 4.801627670396745e-06,
      "loss": 1.5225,
      "step": 88860
    },
    {
      "epoch": 45.20345879959308,
      "grad_norm": 35.98619842529297,
      "learning_rate": 4.796541200406917e-06,
      "loss": 1.6062,
      "step": 88870
    },
    {
      "epoch": 45.208545269582906,
      "grad_norm": 48.97635269165039,
      "learning_rate": 4.7914547304170906e-06,
      "loss": 1.5758,
      "step": 88880
    },
    {
      "epoch": 45.21363173957273,
      "grad_norm": 47.23432159423828,
      "learning_rate": 4.786368260427264e-06,
      "loss": 1.5625,
      "step": 88890
    },
    {
      "epoch": 45.21871820956256,
      "grad_norm": 34.5593147277832,
      "learning_rate": 4.781281790437437e-06,
      "loss": 1.5049,
      "step": 88900
    },
    {
      "epoch": 45.22380467955239,
      "grad_norm": 41.2221794128418,
      "learning_rate": 4.7761953204476096e-06,
      "loss": 1.5671,
      "step": 88910
    },
    {
      "epoch": 45.22889114954222,
      "grad_norm": 38.99591827392578,
      "learning_rate": 4.771108850457783e-06,
      "loss": 1.5224,
      "step": 88920
    },
    {
      "epoch": 45.23397761953205,
      "grad_norm": 37.702144622802734,
      "learning_rate": 4.766022380467955e-06,
      "loss": 1.579,
      "step": 88930
    },
    {
      "epoch": 45.239064089521875,
      "grad_norm": 43.90187454223633,
      "learning_rate": 4.7609359104781286e-06,
      "loss": 1.4724,
      "step": 88940
    },
    {
      "epoch": 45.2441505595117,
      "grad_norm": 47.4887580871582,
      "learning_rate": 4.755849440488301e-06,
      "loss": 1.5748,
      "step": 88950
    },
    {
      "epoch": 45.24923702950153,
      "grad_norm": 35.91958999633789,
      "learning_rate": 4.750762970498474e-06,
      "loss": 1.546,
      "step": 88960
    },
    {
      "epoch": 45.254323499491356,
      "grad_norm": 39.32896041870117,
      "learning_rate": 4.7456765005086476e-06,
      "loss": 1.5701,
      "step": 88970
    },
    {
      "epoch": 45.25940996948118,
      "grad_norm": 38.25385665893555,
      "learning_rate": 4.740590030518821e-06,
      "loss": 1.4825,
      "step": 88980
    },
    {
      "epoch": 45.26449643947101,
      "grad_norm": 38.20780944824219,
      "learning_rate": 4.735503560528992e-06,
      "loss": 1.5225,
      "step": 88990
    },
    {
      "epoch": 45.26958290946084,
      "grad_norm": 59.35506057739258,
      "learning_rate": 4.730417090539166e-06,
      "loss": 1.5176,
      "step": 89000
    },
    {
      "epoch": 45.274669379450664,
      "grad_norm": 40.44156265258789,
      "learning_rate": 4.725330620549339e-06,
      "loss": 1.5003,
      "step": 89010
    },
    {
      "epoch": 45.27975584944049,
      "grad_norm": 38.99219512939453,
      "learning_rate": 4.720244150559512e-06,
      "loss": 1.4866,
      "step": 89020
    },
    {
      "epoch": 45.28484231943032,
      "grad_norm": 34.248985290527344,
      "learning_rate": 4.715157680569685e-06,
      "loss": 1.5677,
      "step": 89030
    },
    {
      "epoch": 45.289928789420145,
      "grad_norm": 47.753482818603516,
      "learning_rate": 4.710071210579858e-06,
      "loss": 1.5722,
      "step": 89040
    },
    {
      "epoch": 45.29501525940997,
      "grad_norm": 40.713844299316406,
      "learning_rate": 4.70498474059003e-06,
      "loss": 1.4283,
      "step": 89050
    },
    {
      "epoch": 45.3001017293998,
      "grad_norm": 36.9896125793457,
      "learning_rate": 4.699898270600204e-06,
      "loss": 1.5641,
      "step": 89060
    },
    {
      "epoch": 45.305188199389626,
      "grad_norm": 35.11143493652344,
      "learning_rate": 4.694811800610377e-06,
      "loss": 1.5293,
      "step": 89070
    },
    {
      "epoch": 45.31027466937945,
      "grad_norm": 47.25868225097656,
      "learning_rate": 4.689725330620549e-06,
      "loss": 1.4838,
      "step": 89080
    },
    {
      "epoch": 45.31536113936928,
      "grad_norm": 44.58767318725586,
      "learning_rate": 4.684638860630723e-06,
      "loss": 1.4811,
      "step": 89090
    },
    {
      "epoch": 45.32044760935911,
      "grad_norm": 39.144615173339844,
      "learning_rate": 4.679552390640896e-06,
      "loss": 1.4564,
      "step": 89100
    },
    {
      "epoch": 45.325534079348934,
      "grad_norm": 34.52791976928711,
      "learning_rate": 4.674465920651068e-06,
      "loss": 1.51,
      "step": 89110
    },
    {
      "epoch": 45.33062054933876,
      "grad_norm": 51.65948486328125,
      "learning_rate": 4.669379450661241e-06,
      "loss": 1.526,
      "step": 89120
    },
    {
      "epoch": 45.33570701932859,
      "grad_norm": 41.214351654052734,
      "learning_rate": 4.664292980671414e-06,
      "loss": 1.629,
      "step": 89130
    },
    {
      "epoch": 45.340793489318415,
      "grad_norm": 58.550777435302734,
      "learning_rate": 4.659206510681587e-06,
      "loss": 1.5247,
      "step": 89140
    },
    {
      "epoch": 45.34587995930824,
      "grad_norm": 36.12446212768555,
      "learning_rate": 4.654120040691761e-06,
      "loss": 1.5318,
      "step": 89150
    },
    {
      "epoch": 45.35096642929807,
      "grad_norm": 37.37991714477539,
      "learning_rate": 4.649033570701933e-06,
      "loss": 1.4987,
      "step": 89160
    },
    {
      "epoch": 45.356052899287896,
      "grad_norm": 40.99116516113281,
      "learning_rate": 4.6439471007121056e-06,
      "loss": 1.4737,
      "step": 89170
    },
    {
      "epoch": 45.36113936927772,
      "grad_norm": 41.48716354370117,
      "learning_rate": 4.638860630722279e-06,
      "loss": 1.5051,
      "step": 89180
    },
    {
      "epoch": 45.36622583926755,
      "grad_norm": 39.0294303894043,
      "learning_rate": 4.633774160732452e-06,
      "loss": 1.4677,
      "step": 89190
    },
    {
      "epoch": 45.37131230925738,
      "grad_norm": 39.55520248413086,
      "learning_rate": 4.6286876907426246e-06,
      "loss": 1.5387,
      "step": 89200
    },
    {
      "epoch": 45.376398779247204,
      "grad_norm": 43.325897216796875,
      "learning_rate": 4.623601220752798e-06,
      "loss": 1.4977,
      "step": 89210
    },
    {
      "epoch": 45.38148524923703,
      "grad_norm": 56.03025436401367,
      "learning_rate": 4.618514750762971e-06,
      "loss": 1.5281,
      "step": 89220
    },
    {
      "epoch": 45.38657171922686,
      "grad_norm": 45.954978942871094,
      "learning_rate": 4.6134282807731436e-06,
      "loss": 1.4618,
      "step": 89230
    },
    {
      "epoch": 45.391658189216685,
      "grad_norm": 45.309818267822266,
      "learning_rate": 4.608341810783317e-06,
      "loss": 1.4707,
      "step": 89240
    },
    {
      "epoch": 45.39674465920651,
      "grad_norm": 40.16799545288086,
      "learning_rate": 4.603255340793489e-06,
      "loss": 1.562,
      "step": 89250
    },
    {
      "epoch": 45.40183112919634,
      "grad_norm": 50.36984634399414,
      "learning_rate": 4.5981688708036626e-06,
      "loss": 1.5728,
      "step": 89260
    },
    {
      "epoch": 45.406917599186166,
      "grad_norm": 56.16654586791992,
      "learning_rate": 4.593082400813836e-06,
      "loss": 1.5238,
      "step": 89270
    },
    {
      "epoch": 45.41200406917599,
      "grad_norm": 41.77314758300781,
      "learning_rate": 4.587995930824008e-06,
      "loss": 1.4893,
      "step": 89280
    },
    {
      "epoch": 45.41709053916582,
      "grad_norm": 54.739566802978516,
      "learning_rate": 4.582909460834181e-06,
      "loss": 1.5609,
      "step": 89290
    },
    {
      "epoch": 45.42217700915565,
      "grad_norm": 42.668766021728516,
      "learning_rate": 4.577822990844354e-06,
      "loss": 1.4933,
      "step": 89300
    },
    {
      "epoch": 45.42726347914547,
      "grad_norm": 42.98647689819336,
      "learning_rate": 4.572736520854527e-06,
      "loss": 1.5544,
      "step": 89310
    },
    {
      "epoch": 45.4323499491353,
      "grad_norm": 39.13016128540039,
      "learning_rate": 4.5676500508647006e-06,
      "loss": 1.5723,
      "step": 89320
    },
    {
      "epoch": 45.43743641912513,
      "grad_norm": 37.029598236083984,
      "learning_rate": 4.562563580874873e-06,
      "loss": 1.4452,
      "step": 89330
    },
    {
      "epoch": 45.442522889114954,
      "grad_norm": 51.867462158203125,
      "learning_rate": 4.557477110885046e-06,
      "loss": 1.4932,
      "step": 89340
    },
    {
      "epoch": 45.44760935910478,
      "grad_norm": 44.408504486083984,
      "learning_rate": 4.552390640895219e-06,
      "loss": 1.5224,
      "step": 89350
    },
    {
      "epoch": 45.45269582909461,
      "grad_norm": 45.78398132324219,
      "learning_rate": 4.547304170905392e-06,
      "loss": 1.5499,
      "step": 89360
    },
    {
      "epoch": 45.457782299084435,
      "grad_norm": 45.48887634277344,
      "learning_rate": 4.542217700915564e-06,
      "loss": 1.5273,
      "step": 89370
    },
    {
      "epoch": 45.46286876907426,
      "grad_norm": 54.18081283569336,
      "learning_rate": 4.537131230925738e-06,
      "loss": 1.4953,
      "step": 89380
    },
    {
      "epoch": 45.46795523906409,
      "grad_norm": 46.25358200073242,
      "learning_rate": 4.532044760935911e-06,
      "loss": 1.4516,
      "step": 89390
    },
    {
      "epoch": 45.473041709053916,
      "grad_norm": 41.40422821044922,
      "learning_rate": 4.526958290946083e-06,
      "loss": 1.558,
      "step": 89400
    },
    {
      "epoch": 45.47812817904374,
      "grad_norm": 35.21223068237305,
      "learning_rate": 4.521871820956257e-06,
      "loss": 1.5697,
      "step": 89410
    },
    {
      "epoch": 45.48321464903357,
      "grad_norm": 35.21393966674805,
      "learning_rate": 4.516785350966429e-06,
      "loss": 1.4752,
      "step": 89420
    },
    {
      "epoch": 45.4883011190234,
      "grad_norm": 37.562782287597656,
      "learning_rate": 4.511698880976602e-06,
      "loss": 1.4784,
      "step": 89430
    },
    {
      "epoch": 45.493387589013224,
      "grad_norm": 42.286251068115234,
      "learning_rate": 4.506612410986776e-06,
      "loss": 1.4246,
      "step": 89440
    },
    {
      "epoch": 45.49847405900305,
      "grad_norm": 44.72853088378906,
      "learning_rate": 4.501525940996949e-06,
      "loss": 1.5449,
      "step": 89450
    },
    {
      "epoch": 45.50356052899288,
      "grad_norm": 43.08238983154297,
      "learning_rate": 4.496439471007121e-06,
      "loss": 1.4769,
      "step": 89460
    },
    {
      "epoch": 45.508646998982705,
      "grad_norm": 40.60537338256836,
      "learning_rate": 4.491353001017294e-06,
      "loss": 1.4411,
      "step": 89470
    },
    {
      "epoch": 45.51373346897253,
      "grad_norm": 43.21757125854492,
      "learning_rate": 4.486266531027467e-06,
      "loss": 1.5098,
      "step": 89480
    },
    {
      "epoch": 45.51881993896236,
      "grad_norm": 38.82209777832031,
      "learning_rate": 4.48118006103764e-06,
      "loss": 1.5238,
      "step": 89490
    },
    {
      "epoch": 45.523906408952186,
      "grad_norm": 45.31983184814453,
      "learning_rate": 4.476093591047813e-06,
      "loss": 1.4329,
      "step": 89500
    },
    {
      "epoch": 45.52899287894201,
      "grad_norm": 46.154273986816406,
      "learning_rate": 4.471007121057986e-06,
      "loss": 1.4667,
      "step": 89510
    },
    {
      "epoch": 45.53407934893184,
      "grad_norm": 35.452545166015625,
      "learning_rate": 4.4659206510681586e-06,
      "loss": 1.5477,
      "step": 89520
    },
    {
      "epoch": 45.53916581892167,
      "grad_norm": 35.90202331542969,
      "learning_rate": 4.460834181078332e-06,
      "loss": 1.5416,
      "step": 89530
    },
    {
      "epoch": 45.544252288911494,
      "grad_norm": 56.39915466308594,
      "learning_rate": 4.455747711088504e-06,
      "loss": 1.5614,
      "step": 89540
    },
    {
      "epoch": 45.54933875890132,
      "grad_norm": 34.68206024169922,
      "learning_rate": 4.4506612410986776e-06,
      "loss": 1.6003,
      "step": 89550
    },
    {
      "epoch": 45.55442522889115,
      "grad_norm": 41.66783142089844,
      "learning_rate": 4.445574771108851e-06,
      "loss": 1.5783,
      "step": 89560
    },
    {
      "epoch": 45.559511698880975,
      "grad_norm": 35.12575912475586,
      "learning_rate": 4.440488301119024e-06,
      "loss": 1.5835,
      "step": 89570
    },
    {
      "epoch": 45.5645981688708,
      "grad_norm": 42.56321334838867,
      "learning_rate": 4.4354018311291966e-06,
      "loss": 1.4782,
      "step": 89580
    },
    {
      "epoch": 45.56968463886063,
      "grad_norm": 43.93128204345703,
      "learning_rate": 4.430315361139369e-06,
      "loss": 1.5535,
      "step": 89590
    },
    {
      "epoch": 45.574771108850456,
      "grad_norm": 39.11119842529297,
      "learning_rate": 4.425228891149542e-06,
      "loss": 1.5393,
      "step": 89600
    },
    {
      "epoch": 45.57985757884028,
      "grad_norm": 46.13337707519531,
      "learning_rate": 4.4201424211597156e-06,
      "loss": 1.4787,
      "step": 89610
    },
    {
      "epoch": 45.58494404883011,
      "grad_norm": 47.52382278442383,
      "learning_rate": 4.415055951169889e-06,
      "loss": 1.5334,
      "step": 89620
    },
    {
      "epoch": 45.59003051881994,
      "grad_norm": 37.39694595336914,
      "learning_rate": 4.409969481180061e-06,
      "loss": 1.4344,
      "step": 89630
    },
    {
      "epoch": 45.595116988809764,
      "grad_norm": 40.702388763427734,
      "learning_rate": 4.404883011190234e-06,
      "loss": 1.5208,
      "step": 89640
    },
    {
      "epoch": 45.60020345879959,
      "grad_norm": 36.98946762084961,
      "learning_rate": 4.399796541200407e-06,
      "loss": 1.5573,
      "step": 89650
    },
    {
      "epoch": 45.60528992878942,
      "grad_norm": 46.939640045166016,
      "learning_rate": 4.39471007121058e-06,
      "loss": 1.5569,
      "step": 89660
    },
    {
      "epoch": 45.610376398779245,
      "grad_norm": 33.73761749267578,
      "learning_rate": 4.389623601220753e-06,
      "loss": 1.4719,
      "step": 89670
    },
    {
      "epoch": 45.61546286876907,
      "grad_norm": 38.9638786315918,
      "learning_rate": 4.384537131230926e-06,
      "loss": 1.4932,
      "step": 89680
    },
    {
      "epoch": 45.6205493387589,
      "grad_norm": 50.063133239746094,
      "learning_rate": 4.379450661241099e-06,
      "loss": 1.5269,
      "step": 89690
    },
    {
      "epoch": 45.625635808748726,
      "grad_norm": 39.104496002197266,
      "learning_rate": 4.374364191251272e-06,
      "loss": 1.5028,
      "step": 89700
    },
    {
      "epoch": 45.63072227873855,
      "grad_norm": 39.026336669921875,
      "learning_rate": 4.369277721261445e-06,
      "loss": 1.4294,
      "step": 89710
    },
    {
      "epoch": 45.63580874872838,
      "grad_norm": 46.4050407409668,
      "learning_rate": 4.364191251271617e-06,
      "loss": 1.4489,
      "step": 89720
    },
    {
      "epoch": 45.64089521871821,
      "grad_norm": 37.626808166503906,
      "learning_rate": 4.359104781281791e-06,
      "loss": 1.4981,
      "step": 89730
    },
    {
      "epoch": 45.645981688708034,
      "grad_norm": 41.231624603271484,
      "learning_rate": 4.354018311291964e-06,
      "loss": 1.5933,
      "step": 89740
    },
    {
      "epoch": 45.65106815869786,
      "grad_norm": 39.75242233276367,
      "learning_rate": 4.348931841302136e-06,
      "loss": 1.4475,
      "step": 89750
    },
    {
      "epoch": 45.65615462868769,
      "grad_norm": 36.779693603515625,
      "learning_rate": 4.343845371312309e-06,
      "loss": 1.5838,
      "step": 89760
    },
    {
      "epoch": 45.661241098677515,
      "grad_norm": 42.111602783203125,
      "learning_rate": 4.338758901322482e-06,
      "loss": 1.5138,
      "step": 89770
    },
    {
      "epoch": 45.66632756866734,
      "grad_norm": 39.91050338745117,
      "learning_rate": 4.333672431332655e-06,
      "loss": 1.5313,
      "step": 89780
    },
    {
      "epoch": 45.67141403865717,
      "grad_norm": 39.18871307373047,
      "learning_rate": 4.328585961342829e-06,
      "loss": 1.5204,
      "step": 89790
    },
    {
      "epoch": 45.676500508646996,
      "grad_norm": 44.25078201293945,
      "learning_rate": 4.323499491353001e-06,
      "loss": 1.4685,
      "step": 89800
    },
    {
      "epoch": 45.68158697863683,
      "grad_norm": 53.00474548339844,
      "learning_rate": 4.318413021363174e-06,
      "loss": 1.5023,
      "step": 89810
    },
    {
      "epoch": 45.68667344862666,
      "grad_norm": 34.749507904052734,
      "learning_rate": 4.313326551373347e-06,
      "loss": 1.4671,
      "step": 89820
    },
    {
      "epoch": 45.691759918616484,
      "grad_norm": 38.597660064697266,
      "learning_rate": 4.30824008138352e-06,
      "loss": 1.5416,
      "step": 89830
    },
    {
      "epoch": 45.69684638860631,
      "grad_norm": 38.476356506347656,
      "learning_rate": 4.3031536113936926e-06,
      "loss": 1.4849,
      "step": 89840
    },
    {
      "epoch": 45.70193285859614,
      "grad_norm": 44.69059753417969,
      "learning_rate": 4.298067141403866e-06,
      "loss": 1.5477,
      "step": 89850
    },
    {
      "epoch": 45.707019328585965,
      "grad_norm": 36.50756072998047,
      "learning_rate": 4.292980671414039e-06,
      "loss": 1.5075,
      "step": 89860
    },
    {
      "epoch": 45.71210579857579,
      "grad_norm": 36.24103546142578,
      "learning_rate": 4.2878942014242116e-06,
      "loss": 1.5519,
      "step": 89870
    },
    {
      "epoch": 45.71719226856562,
      "grad_norm": 52.34610366821289,
      "learning_rate": 4.282807731434385e-06,
      "loss": 1.505,
      "step": 89880
    },
    {
      "epoch": 45.722278738555445,
      "grad_norm": 37.521060943603516,
      "learning_rate": 4.277721261444557e-06,
      "loss": 1.5886,
      "step": 89890
    },
    {
      "epoch": 45.72736520854527,
      "grad_norm": 45.13834762573242,
      "learning_rate": 4.2726347914547306e-06,
      "loss": 1.5501,
      "step": 89900
    },
    {
      "epoch": 45.7324516785351,
      "grad_norm": 60.50930404663086,
      "learning_rate": 4.267548321464904e-06,
      "loss": 1.5258,
      "step": 89910
    },
    {
      "epoch": 45.737538148524926,
      "grad_norm": 39.799991607666016,
      "learning_rate": 4.262461851475077e-06,
      "loss": 1.505,
      "step": 89920
    },
    {
      "epoch": 45.74262461851475,
      "grad_norm": 46.322532653808594,
      "learning_rate": 4.2573753814852496e-06,
      "loss": 1.661,
      "step": 89930
    },
    {
      "epoch": 45.74771108850458,
      "grad_norm": 46.548397064208984,
      "learning_rate": 4.252288911495422e-06,
      "loss": 1.5884,
      "step": 89940
    },
    {
      "epoch": 45.75279755849441,
      "grad_norm": 41.671024322509766,
      "learning_rate": 4.247202441505595e-06,
      "loss": 1.5084,
      "step": 89950
    },
    {
      "epoch": 45.757884028484234,
      "grad_norm": 38.234493255615234,
      "learning_rate": 4.2421159715157686e-06,
      "loss": 1.4912,
      "step": 89960
    },
    {
      "epoch": 45.76297049847406,
      "grad_norm": 38.076900482177734,
      "learning_rate": 4.237029501525941e-06,
      "loss": 1.5343,
      "step": 89970
    },
    {
      "epoch": 45.76805696846389,
      "grad_norm": 51.03750228881836,
      "learning_rate": 4.231943031536114e-06,
      "loss": 1.5256,
      "step": 89980
    },
    {
      "epoch": 45.773143438453715,
      "grad_norm": 45.68101119995117,
      "learning_rate": 4.226856561546287e-06,
      "loss": 1.5044,
      "step": 89990
    },
    {
      "epoch": 45.77822990844354,
      "grad_norm": 36.25133514404297,
      "learning_rate": 4.22177009155646e-06,
      "loss": 1.5433,
      "step": 90000
    },
    {
      "epoch": 45.78331637843337,
      "grad_norm": 46.873287200927734,
      "learning_rate": 4.216683621566632e-06,
      "loss": 1.4785,
      "step": 90010
    },
    {
      "epoch": 45.788402848423196,
      "grad_norm": 42.203365325927734,
      "learning_rate": 4.211597151576806e-06,
      "loss": 1.5244,
      "step": 90020
    },
    {
      "epoch": 45.79348931841302,
      "grad_norm": 39.317169189453125,
      "learning_rate": 4.206510681586979e-06,
      "loss": 1.5703,
      "step": 90030
    },
    {
      "epoch": 45.79857578840285,
      "grad_norm": 45.05990982055664,
      "learning_rate": 4.201424211597152e-06,
      "loss": 1.5041,
      "step": 90040
    },
    {
      "epoch": 45.80366225839268,
      "grad_norm": 57.07390594482422,
      "learning_rate": 4.196337741607325e-06,
      "loss": 1.5152,
      "step": 90050
    },
    {
      "epoch": 45.808748728382504,
      "grad_norm": 33.40603256225586,
      "learning_rate": 4.191251271617497e-06,
      "loss": 1.5294,
      "step": 90060
    },
    {
      "epoch": 45.81383519837233,
      "grad_norm": 40.172203063964844,
      "learning_rate": 4.18616480162767e-06,
      "loss": 1.5213,
      "step": 90070
    },
    {
      "epoch": 45.81892166836216,
      "grad_norm": 51.155277252197266,
      "learning_rate": 4.181078331637844e-06,
      "loss": 1.5358,
      "step": 90080
    },
    {
      "epoch": 45.824008138351985,
      "grad_norm": 36.739036560058594,
      "learning_rate": 4.175991861648017e-06,
      "loss": 1.5415,
      "step": 90090
    },
    {
      "epoch": 45.82909460834181,
      "grad_norm": 46.36400604248047,
      "learning_rate": 4.170905391658189e-06,
      "loss": 1.567,
      "step": 90100
    },
    {
      "epoch": 45.83418107833164,
      "grad_norm": 50.122188568115234,
      "learning_rate": 4.165818921668362e-06,
      "loss": 1.5142,
      "step": 90110
    },
    {
      "epoch": 45.839267548321466,
      "grad_norm": 37.928138732910156,
      "learning_rate": 4.160732451678535e-06,
      "loss": 1.4987,
      "step": 90120
    },
    {
      "epoch": 45.84435401831129,
      "grad_norm": 41.68071365356445,
      "learning_rate": 4.155645981688708e-06,
      "loss": 1.5596,
      "step": 90130
    },
    {
      "epoch": 45.84944048830112,
      "grad_norm": 49.89850616455078,
      "learning_rate": 4.150559511698881e-06,
      "loss": 1.4996,
      "step": 90140
    },
    {
      "epoch": 45.85452695829095,
      "grad_norm": 41.59352111816406,
      "learning_rate": 4.145473041709054e-06,
      "loss": 1.4829,
      "step": 90150
    },
    {
      "epoch": 45.859613428280774,
      "grad_norm": 37.02594757080078,
      "learning_rate": 4.140386571719227e-06,
      "loss": 1.4758,
      "step": 90160
    },
    {
      "epoch": 45.8646998982706,
      "grad_norm": 40.362064361572266,
      "learning_rate": 4.1353001017294e-06,
      "loss": 1.4973,
      "step": 90170
    },
    {
      "epoch": 45.86978636826043,
      "grad_norm": 44.400299072265625,
      "learning_rate": 4.130213631739572e-06,
      "loss": 1.476,
      "step": 90180
    },
    {
      "epoch": 45.874872838250255,
      "grad_norm": 49.01984786987305,
      "learning_rate": 4.1251271617497456e-06,
      "loss": 1.5535,
      "step": 90190
    },
    {
      "epoch": 45.87995930824008,
      "grad_norm": 39.695674896240234,
      "learning_rate": 4.120040691759919e-06,
      "loss": 1.4544,
      "step": 90200
    },
    {
      "epoch": 45.88504577822991,
      "grad_norm": 36.784019470214844,
      "learning_rate": 4.114954221770092e-06,
      "loss": 1.5023,
      "step": 90210
    },
    {
      "epoch": 45.890132248219736,
      "grad_norm": 42.28992462158203,
      "learning_rate": 4.109867751780265e-06,
      "loss": 1.5673,
      "step": 90220
    },
    {
      "epoch": 45.89521871820956,
      "grad_norm": 35.260807037353516,
      "learning_rate": 4.104781281790437e-06,
      "loss": 1.4616,
      "step": 90230
    },
    {
      "epoch": 45.90030518819939,
      "grad_norm": 42.826576232910156,
      "learning_rate": 4.09969481180061e-06,
      "loss": 1.5414,
      "step": 90240
    },
    {
      "epoch": 45.90539165818922,
      "grad_norm": 50.14356231689453,
      "learning_rate": 4.0946083418107836e-06,
      "loss": 1.4286,
      "step": 90250
    },
    {
      "epoch": 45.910478128179044,
      "grad_norm": 44.541141510009766,
      "learning_rate": 4.089521871820957e-06,
      "loss": 1.575,
      "step": 90260
    },
    {
      "epoch": 45.91556459816887,
      "grad_norm": 37.43323516845703,
      "learning_rate": 4.084435401831129e-06,
      "loss": 1.5477,
      "step": 90270
    },
    {
      "epoch": 45.9206510681587,
      "grad_norm": 44.49801254272461,
      "learning_rate": 4.0793489318413026e-06,
      "loss": 1.4348,
      "step": 90280
    },
    {
      "epoch": 45.925737538148525,
      "grad_norm": 40.729698181152344,
      "learning_rate": 4.074262461851475e-06,
      "loss": 1.4749,
      "step": 90290
    },
    {
      "epoch": 45.93082400813835,
      "grad_norm": 48.14573287963867,
      "learning_rate": 4.069175991861648e-06,
      "loss": 1.4486,
      "step": 90300
    },
    {
      "epoch": 45.93591047812818,
      "grad_norm": 48.068511962890625,
      "learning_rate": 4.064089521871821e-06,
      "loss": 1.5055,
      "step": 90310
    },
    {
      "epoch": 45.940996948118006,
      "grad_norm": 41.5312385559082,
      "learning_rate": 4.059003051881994e-06,
      "loss": 1.5426,
      "step": 90320
    },
    {
      "epoch": 45.94608341810783,
      "grad_norm": 45.7509880065918,
      "learning_rate": 4.053916581892167e-06,
      "loss": 1.5603,
      "step": 90330
    },
    {
      "epoch": 45.95116988809766,
      "grad_norm": 43.36585998535156,
      "learning_rate": 4.0488301119023406e-06,
      "loss": 1.6268,
      "step": 90340
    },
    {
      "epoch": 45.95625635808749,
      "grad_norm": 36.94413375854492,
      "learning_rate": 4.043743641912513e-06,
      "loss": 1.4238,
      "step": 90350
    },
    {
      "epoch": 45.96134282807731,
      "grad_norm": 32.46429443359375,
      "learning_rate": 4.038657171922685e-06,
      "loss": 1.5187,
      "step": 90360
    },
    {
      "epoch": 45.96642929806714,
      "grad_norm": 36.00517272949219,
      "learning_rate": 4.033570701932859e-06,
      "loss": 1.5064,
      "step": 90370
    },
    {
      "epoch": 45.97151576805697,
      "grad_norm": 40.22174835205078,
      "learning_rate": 4.028484231943032e-06,
      "loss": 1.5353,
      "step": 90380
    },
    {
      "epoch": 45.976602238046794,
      "grad_norm": 43.69636535644531,
      "learning_rate": 4.023397761953205e-06,
      "loss": 1.5472,
      "step": 90390
    },
    {
      "epoch": 45.98168870803662,
      "grad_norm": 44.0807991027832,
      "learning_rate": 4.018311291963378e-06,
      "loss": 1.6221,
      "step": 90400
    },
    {
      "epoch": 45.98677517802645,
      "grad_norm": 44.380897521972656,
      "learning_rate": 4.01322482197355e-06,
      "loss": 1.4468,
      "step": 90410
    },
    {
      "epoch": 45.991861648016275,
      "grad_norm": 35.208988189697266,
      "learning_rate": 4.008138351983723e-06,
      "loss": 1.601,
      "step": 90420
    },
    {
      "epoch": 45.9969481180061,
      "grad_norm": 44.70931625366211,
      "learning_rate": 4.003051881993897e-06,
      "loss": 1.5739,
      "step": 90430
    },
    {
      "epoch": 46.0,
      "eval_loss": 5.120180606842041,
      "eval_runtime": 2.7339,
      "eval_samples_per_second": 1015.033,
      "eval_steps_per_second": 126.925,
      "step": 90436
    },
    {
      "epoch": 46.00203458799593,
      "grad_norm": 43.6629638671875,
      "learning_rate": 3.997965412004069e-06,
      "loss": 1.4419,
      "step": 90440
    },
    {
      "epoch": 46.007121057985756,
      "grad_norm": 47.648193359375,
      "learning_rate": 3.992878942014242e-06,
      "loss": 1.4901,
      "step": 90450
    },
    {
      "epoch": 46.01220752797558,
      "grad_norm": 34.47688674926758,
      "learning_rate": 3.987792472024416e-06,
      "loss": 1.5488,
      "step": 90460
    },
    {
      "epoch": 46.01729399796541,
      "grad_norm": 39.513851165771484,
      "learning_rate": 3.982706002034588e-06,
      "loss": 1.5265,
      "step": 90470
    },
    {
      "epoch": 46.02238046795524,
      "grad_norm": 52.00571060180664,
      "learning_rate": 3.9776195320447606e-06,
      "loss": 1.5075,
      "step": 90480
    },
    {
      "epoch": 46.027466937945064,
      "grad_norm": 35.55179977416992,
      "learning_rate": 3.972533062054934e-06,
      "loss": 1.4899,
      "step": 90490
    },
    {
      "epoch": 46.03255340793489,
      "grad_norm": 41.85688781738281,
      "learning_rate": 3.967446592065107e-06,
      "loss": 1.5048,
      "step": 90500
    },
    {
      "epoch": 46.03763987792472,
      "grad_norm": 39.07832717895508,
      "learning_rate": 3.96236012207528e-06,
      "loss": 1.4924,
      "step": 90510
    },
    {
      "epoch": 46.042726347914545,
      "grad_norm": 45.45063781738281,
      "learning_rate": 3.957273652085453e-06,
      "loss": 1.6036,
      "step": 90520
    },
    {
      "epoch": 46.04781281790437,
      "grad_norm": 38.611289978027344,
      "learning_rate": 3.952187182095625e-06,
      "loss": 1.616,
      "step": 90530
    },
    {
      "epoch": 46.0528992878942,
      "grad_norm": 38.598106384277344,
      "learning_rate": 3.9471007121057986e-06,
      "loss": 1.4925,
      "step": 90540
    },
    {
      "epoch": 46.057985757884026,
      "grad_norm": 40.645355224609375,
      "learning_rate": 3.942014242115972e-06,
      "loss": 1.458,
      "step": 90550
    },
    {
      "epoch": 46.06307222787385,
      "grad_norm": 33.61643981933594,
      "learning_rate": 3.936927772126145e-06,
      "loss": 1.4558,
      "step": 90560
    },
    {
      "epoch": 46.06815869786368,
      "grad_norm": 37.175880432128906,
      "learning_rate": 3.9318413021363176e-06,
      "loss": 1.5396,
      "step": 90570
    },
    {
      "epoch": 46.07324516785351,
      "grad_norm": 41.10649871826172,
      "learning_rate": 3.926754832146491e-06,
      "loss": 1.5697,
      "step": 90580
    },
    {
      "epoch": 46.078331637843334,
      "grad_norm": 41.77470016479492,
      "learning_rate": 3.921668362156663e-06,
      "loss": 1.4395,
      "step": 90590
    },
    {
      "epoch": 46.08341810783316,
      "grad_norm": 52.726253509521484,
      "learning_rate": 3.9165818921668366e-06,
      "loss": 1.4981,
      "step": 90600
    },
    {
      "epoch": 46.08850457782299,
      "grad_norm": 45.55174255371094,
      "learning_rate": 3.911495422177009e-06,
      "loss": 1.5026,
      "step": 90610
    },
    {
      "epoch": 46.093591047812815,
      "grad_norm": 31.135047912597656,
      "learning_rate": 3.906408952187182e-06,
      "loss": 1.5163,
      "step": 90620
    },
    {
      "epoch": 46.09867751780264,
      "grad_norm": 37.152198791503906,
      "learning_rate": 3.9013224821973556e-06,
      "loss": 1.5786,
      "step": 90630
    },
    {
      "epoch": 46.10376398779247,
      "grad_norm": 40.17341232299805,
      "learning_rate": 3.896236012207528e-06,
      "loss": 1.5363,
      "step": 90640
    },
    {
      "epoch": 46.108850457782296,
      "grad_norm": 35.99798583984375,
      "learning_rate": 3.8911495422177e-06,
      "loss": 1.5616,
      "step": 90650
    },
    {
      "epoch": 46.11393692777212,
      "grad_norm": 41.03359603881836,
      "learning_rate": 3.886063072227874e-06,
      "loss": 1.4521,
      "step": 90660
    },
    {
      "epoch": 46.11902339776195,
      "grad_norm": 39.3094482421875,
      "learning_rate": 3.880976602238047e-06,
      "loss": 1.5261,
      "step": 90670
    },
    {
      "epoch": 46.12410986775178,
      "grad_norm": 41.16441345214844,
      "learning_rate": 3.87589013224822e-06,
      "loss": 1.5236,
      "step": 90680
    },
    {
      "epoch": 46.129196337741604,
      "grad_norm": 33.849403381347656,
      "learning_rate": 3.8708036622583935e-06,
      "loss": 1.4664,
      "step": 90690
    },
    {
      "epoch": 46.13428280773143,
      "grad_norm": 40.49634552001953,
      "learning_rate": 3.865717192268566e-06,
      "loss": 1.5615,
      "step": 90700
    },
    {
      "epoch": 46.139369277721265,
      "grad_norm": 38.93764877319336,
      "learning_rate": 3.860630722278738e-06,
      "loss": 1.539,
      "step": 90710
    },
    {
      "epoch": 46.14445574771109,
      "grad_norm": 48.15792465209961,
      "learning_rate": 3.855544252288912e-06,
      "loss": 1.4905,
      "step": 90720
    },
    {
      "epoch": 46.14954221770092,
      "grad_norm": 52.97344207763672,
      "learning_rate": 3.850457782299085e-06,
      "loss": 1.4911,
      "step": 90730
    },
    {
      "epoch": 46.154628687690746,
      "grad_norm": 44.8564338684082,
      "learning_rate": 3.845371312309257e-06,
      "loss": 1.4602,
      "step": 90740
    },
    {
      "epoch": 46.15971515768057,
      "grad_norm": 38.3801155090332,
      "learning_rate": 3.840284842319431e-06,
      "loss": 1.516,
      "step": 90750
    },
    {
      "epoch": 46.1648016276704,
      "grad_norm": 50.14921188354492,
      "learning_rate": 3.835198372329603e-06,
      "loss": 1.5086,
      "step": 90760
    },
    {
      "epoch": 46.16988809766023,
      "grad_norm": 42.91413116455078,
      "learning_rate": 3.830111902339776e-06,
      "loss": 1.5438,
      "step": 90770
    },
    {
      "epoch": 46.174974567650054,
      "grad_norm": 39.784358978271484,
      "learning_rate": 3.825025432349949e-06,
      "loss": 1.5822,
      "step": 90780
    },
    {
      "epoch": 46.18006103763988,
      "grad_norm": 38.6088752746582,
      "learning_rate": 3.819938962360122e-06,
      "loss": 1.4834,
      "step": 90790
    },
    {
      "epoch": 46.18514750762971,
      "grad_norm": 43.773902893066406,
      "learning_rate": 3.814852492370295e-06,
      "loss": 1.4434,
      "step": 90800
    },
    {
      "epoch": 46.190233977619535,
      "grad_norm": 47.2109375,
      "learning_rate": 3.8097660223804683e-06,
      "loss": 1.4491,
      "step": 90810
    },
    {
      "epoch": 46.19532044760936,
      "grad_norm": 36.44492721557617,
      "learning_rate": 3.8046795523906407e-06,
      "loss": 1.5556,
      "step": 90820
    },
    {
      "epoch": 46.20040691759919,
      "grad_norm": 47.046966552734375,
      "learning_rate": 3.799593082400814e-06,
      "loss": 1.5385,
      "step": 90830
    },
    {
      "epoch": 46.205493387589016,
      "grad_norm": 42.00392532348633,
      "learning_rate": 3.794506612410987e-06,
      "loss": 1.5467,
      "step": 90840
    },
    {
      "epoch": 46.21057985757884,
      "grad_norm": 37.477169036865234,
      "learning_rate": 3.78942014242116e-06,
      "loss": 1.5699,
      "step": 90850
    },
    {
      "epoch": 46.21566632756867,
      "grad_norm": 45.03186798095703,
      "learning_rate": 3.784333672431333e-06,
      "loss": 1.4282,
      "step": 90860
    },
    {
      "epoch": 46.2207527975585,
      "grad_norm": 46.3511848449707,
      "learning_rate": 3.7792472024415054e-06,
      "loss": 1.5324,
      "step": 90870
    },
    {
      "epoch": 46.225839267548324,
      "grad_norm": 43.001129150390625,
      "learning_rate": 3.7741607324516787e-06,
      "loss": 1.4916,
      "step": 90880
    },
    {
      "epoch": 46.23092573753815,
      "grad_norm": 45.76777648925781,
      "learning_rate": 3.769074262461852e-06,
      "loss": 1.5791,
      "step": 90890
    },
    {
      "epoch": 46.23601220752798,
      "grad_norm": 45.0885124206543,
      "learning_rate": 3.763987792472025e-06,
      "loss": 1.571,
      "step": 90900
    },
    {
      "epoch": 46.241098677517805,
      "grad_norm": 46.54854965209961,
      "learning_rate": 3.7589013224821973e-06,
      "loss": 1.5254,
      "step": 90910
    },
    {
      "epoch": 46.24618514750763,
      "grad_norm": 38.14885711669922,
      "learning_rate": 3.7538148524923706e-06,
      "loss": 1.5528,
      "step": 90920
    },
    {
      "epoch": 46.25127161749746,
      "grad_norm": 49.85270309448242,
      "learning_rate": 3.7487283825025434e-06,
      "loss": 1.5834,
      "step": 90930
    },
    {
      "epoch": 46.256358087487286,
      "grad_norm": 49.95622253417969,
      "learning_rate": 3.7436419125127167e-06,
      "loss": 1.4353,
      "step": 90940
    },
    {
      "epoch": 46.26144455747711,
      "grad_norm": 39.4868049621582,
      "learning_rate": 3.738555442522889e-06,
      "loss": 1.4077,
      "step": 90950
    },
    {
      "epoch": 46.26653102746694,
      "grad_norm": 54.99723815917969,
      "learning_rate": 3.733468972533062e-06,
      "loss": 1.5616,
      "step": 90960
    },
    {
      "epoch": 46.271617497456766,
      "grad_norm": 38.974571228027344,
      "learning_rate": 3.7283825025432353e-06,
      "loss": 1.582,
      "step": 90970
    },
    {
      "epoch": 46.27670396744659,
      "grad_norm": 47.561222076416016,
      "learning_rate": 3.723296032553408e-06,
      "loss": 1.5525,
      "step": 90980
    },
    {
      "epoch": 46.28179043743642,
      "grad_norm": 40.61567306518555,
      "learning_rate": 3.7182095625635806e-06,
      "loss": 1.4495,
      "step": 90990
    },
    {
      "epoch": 46.28687690742625,
      "grad_norm": 32.570674896240234,
      "learning_rate": 3.713123092573754e-06,
      "loss": 1.4834,
      "step": 91000
    },
    {
      "epoch": 46.291963377416074,
      "grad_norm": 36.77402114868164,
      "learning_rate": 3.708036622583927e-06,
      "loss": 1.5191,
      "step": 91010
    },
    {
      "epoch": 46.2970498474059,
      "grad_norm": 56.843055725097656,
      "learning_rate": 3.7029501525941e-06,
      "loss": 1.4953,
      "step": 91020
    },
    {
      "epoch": 46.30213631739573,
      "grad_norm": 45.666839599609375,
      "learning_rate": 3.6978636826042733e-06,
      "loss": 1.4745,
      "step": 91030
    },
    {
      "epoch": 46.307222787385555,
      "grad_norm": 39.32803726196289,
      "learning_rate": 3.6927772126144457e-06,
      "loss": 1.4541,
      "step": 91040
    },
    {
      "epoch": 46.31230925737538,
      "grad_norm": 42.3009033203125,
      "learning_rate": 3.6876907426246186e-06,
      "loss": 1.5326,
      "step": 91050
    },
    {
      "epoch": 46.31739572736521,
      "grad_norm": 48.35640335083008,
      "learning_rate": 3.682604272634792e-06,
      "loss": 1.5052,
      "step": 91060
    },
    {
      "epoch": 46.322482197355036,
      "grad_norm": 40.456573486328125,
      "learning_rate": 3.6775178026449647e-06,
      "loss": 1.5105,
      "step": 91070
    },
    {
      "epoch": 46.32756866734486,
      "grad_norm": 46.31370544433594,
      "learning_rate": 3.672431332655137e-06,
      "loss": 1.5868,
      "step": 91080
    },
    {
      "epoch": 46.33265513733469,
      "grad_norm": 37.699012756347656,
      "learning_rate": 3.6673448626653104e-06,
      "loss": 1.476,
      "step": 91090
    },
    {
      "epoch": 46.33774160732452,
      "grad_norm": 35.63047790527344,
      "learning_rate": 3.6622583926754833e-06,
      "loss": 1.5031,
      "step": 91100
    },
    {
      "epoch": 46.342828077314344,
      "grad_norm": 45.81058120727539,
      "learning_rate": 3.6571719226856566e-06,
      "loss": 1.5433,
      "step": 91110
    },
    {
      "epoch": 46.34791454730417,
      "grad_norm": 38.74165725708008,
      "learning_rate": 3.652085452695829e-06,
      "loss": 1.595,
      "step": 91120
    },
    {
      "epoch": 46.353001017294,
      "grad_norm": 35.40967559814453,
      "learning_rate": 3.6469989827060023e-06,
      "loss": 1.6121,
      "step": 91130
    },
    {
      "epoch": 46.358087487283825,
      "grad_norm": 41.926490783691406,
      "learning_rate": 3.641912512716175e-06,
      "loss": 1.5542,
      "step": 91140
    },
    {
      "epoch": 46.36317395727365,
      "grad_norm": 39.65419006347656,
      "learning_rate": 3.6368260427263484e-06,
      "loss": 1.5463,
      "step": 91150
    },
    {
      "epoch": 46.36826042726348,
      "grad_norm": 44.671363830566406,
      "learning_rate": 3.631739572736521e-06,
      "loss": 1.4635,
      "step": 91160
    },
    {
      "epoch": 46.373346897253306,
      "grad_norm": 39.035892486572266,
      "learning_rate": 3.6266531027466937e-06,
      "loss": 1.5215,
      "step": 91170
    },
    {
      "epoch": 46.37843336724313,
      "grad_norm": 41.66495132446289,
      "learning_rate": 3.621566632756867e-06,
      "loss": 1.4805,
      "step": 91180
    },
    {
      "epoch": 46.38351983723296,
      "grad_norm": 50.43741226196289,
      "learning_rate": 3.61648016276704e-06,
      "loss": 1.4723,
      "step": 91190
    },
    {
      "epoch": 46.38860630722279,
      "grad_norm": 45.705570220947266,
      "learning_rate": 3.611393692777213e-06,
      "loss": 1.5013,
      "step": 91200
    },
    {
      "epoch": 46.393692777212614,
      "grad_norm": 36.116188049316406,
      "learning_rate": 3.6063072227873856e-06,
      "loss": 1.5301,
      "step": 91210
    },
    {
      "epoch": 46.39877924720244,
      "grad_norm": 32.30317687988281,
      "learning_rate": 3.6012207527975584e-06,
      "loss": 1.528,
      "step": 91220
    },
    {
      "epoch": 46.40386571719227,
      "grad_norm": 44.63063430786133,
      "learning_rate": 3.5961342828077317e-06,
      "loss": 1.5221,
      "step": 91230
    },
    {
      "epoch": 46.408952187182095,
      "grad_norm": 41.9024772644043,
      "learning_rate": 3.591047812817905e-06,
      "loss": 1.6597,
      "step": 91240
    },
    {
      "epoch": 46.41403865717192,
      "grad_norm": 42.45714569091797,
      "learning_rate": 3.5859613428280774e-06,
      "loss": 1.4345,
      "step": 91250
    },
    {
      "epoch": 46.41912512716175,
      "grad_norm": 47.3886833190918,
      "learning_rate": 3.5808748728382503e-06,
      "loss": 1.5327,
      "step": 91260
    },
    {
      "epoch": 46.424211597151576,
      "grad_norm": 41.05226135253906,
      "learning_rate": 3.5757884028484236e-06,
      "loss": 1.4737,
      "step": 91270
    },
    {
      "epoch": 46.4292980671414,
      "grad_norm": 41.94992446899414,
      "learning_rate": 3.5707019328585964e-06,
      "loss": 1.518,
      "step": 91280
    },
    {
      "epoch": 46.43438453713123,
      "grad_norm": 40.04872131347656,
      "learning_rate": 3.565615462868769e-06,
      "loss": 1.4751,
      "step": 91290
    },
    {
      "epoch": 46.43947100712106,
      "grad_norm": 52.89006042480469,
      "learning_rate": 3.560528992878942e-06,
      "loss": 1.5001,
      "step": 91300
    },
    {
      "epoch": 46.444557477110884,
      "grad_norm": 39.94792938232422,
      "learning_rate": 3.555442522889115e-06,
      "loss": 1.5032,
      "step": 91310
    },
    {
      "epoch": 46.44964394710071,
      "grad_norm": 41.19922637939453,
      "learning_rate": 3.5503560528992883e-06,
      "loss": 1.5358,
      "step": 91320
    },
    {
      "epoch": 46.45473041709054,
      "grad_norm": 48.21778106689453,
      "learning_rate": 3.5452695829094616e-06,
      "loss": 1.5495,
      "step": 91330
    },
    {
      "epoch": 46.459816887080365,
      "grad_norm": 40.82645797729492,
      "learning_rate": 3.5401831129196336e-06,
      "loss": 1.4792,
      "step": 91340
    },
    {
      "epoch": 46.46490335707019,
      "grad_norm": 37.55769348144531,
      "learning_rate": 3.535096642929807e-06,
      "loss": 1.4598,
      "step": 91350
    },
    {
      "epoch": 46.46998982706002,
      "grad_norm": 41.65703582763672,
      "learning_rate": 3.53001017293998e-06,
      "loss": 1.5568,
      "step": 91360
    },
    {
      "epoch": 46.475076297049846,
      "grad_norm": 39.16838455200195,
      "learning_rate": 3.524923702950153e-06,
      "loss": 1.5839,
      "step": 91370
    },
    {
      "epoch": 46.48016276703967,
      "grad_norm": 39.64093780517578,
      "learning_rate": 3.5198372329603254e-06,
      "loss": 1.5578,
      "step": 91380
    },
    {
      "epoch": 46.4852492370295,
      "grad_norm": 36.523475646972656,
      "learning_rate": 3.5147507629704987e-06,
      "loss": 1.5522,
      "step": 91390
    },
    {
      "epoch": 46.49033570701933,
      "grad_norm": 39.7872314453125,
      "learning_rate": 3.5096642929806716e-06,
      "loss": 1.5614,
      "step": 91400
    },
    {
      "epoch": 46.495422177009154,
      "grad_norm": 56.32766342163086,
      "learning_rate": 3.504577822990845e-06,
      "loss": 1.5806,
      "step": 91410
    },
    {
      "epoch": 46.50050864699898,
      "grad_norm": 39.03767776489258,
      "learning_rate": 3.4994913530010173e-06,
      "loss": 1.5016,
      "step": 91420
    },
    {
      "epoch": 46.50559511698881,
      "grad_norm": 39.08223342895508,
      "learning_rate": 3.49440488301119e-06,
      "loss": 1.5953,
      "step": 91430
    },
    {
      "epoch": 46.510681586978635,
      "grad_norm": 40.183990478515625,
      "learning_rate": 3.4893184130213634e-06,
      "loss": 1.4217,
      "step": 91440
    },
    {
      "epoch": 46.51576805696846,
      "grad_norm": 37.63376235961914,
      "learning_rate": 3.4842319430315367e-06,
      "loss": 1.5378,
      "step": 91450
    },
    {
      "epoch": 46.52085452695829,
      "grad_norm": 35.04747772216797,
      "learning_rate": 3.4791454730417087e-06,
      "loss": 1.5201,
      "step": 91460
    },
    {
      "epoch": 46.525940996948115,
      "grad_norm": 57.44929885864258,
      "learning_rate": 3.474059003051882e-06,
      "loss": 1.5137,
      "step": 91470
    },
    {
      "epoch": 46.53102746693794,
      "grad_norm": 41.45407485961914,
      "learning_rate": 3.4689725330620553e-06,
      "loss": 1.564,
      "step": 91480
    },
    {
      "epoch": 46.53611393692777,
      "grad_norm": 37.922698974609375,
      "learning_rate": 3.463886063072228e-06,
      "loss": 1.53,
      "step": 91490
    },
    {
      "epoch": 46.541200406917596,
      "grad_norm": 51.00185775756836,
      "learning_rate": 3.4587995930824014e-06,
      "loss": 1.4883,
      "step": 91500
    },
    {
      "epoch": 46.54628687690742,
      "grad_norm": 39.904991149902344,
      "learning_rate": 3.453713123092574e-06,
      "loss": 1.4359,
      "step": 91510
    },
    {
      "epoch": 46.55137334689725,
      "grad_norm": 55.42058563232422,
      "learning_rate": 3.4486266531027467e-06,
      "loss": 1.5129,
      "step": 91520
    },
    {
      "epoch": 46.55645981688708,
      "grad_norm": 44.28315353393555,
      "learning_rate": 3.44354018311292e-06,
      "loss": 1.4976,
      "step": 91530
    },
    {
      "epoch": 46.561546286876904,
      "grad_norm": 35.7014274597168,
      "learning_rate": 3.438453713123093e-06,
      "loss": 1.5356,
      "step": 91540
    },
    {
      "epoch": 46.56663275686673,
      "grad_norm": 46.35686111450195,
      "learning_rate": 3.4333672431332653e-06,
      "loss": 1.493,
      "step": 91550
    },
    {
      "epoch": 46.57171922685656,
      "grad_norm": 37.416656494140625,
      "learning_rate": 3.4282807731434386e-06,
      "loss": 1.4856,
      "step": 91560
    },
    {
      "epoch": 46.576805696846385,
      "grad_norm": 43.13886260986328,
      "learning_rate": 3.423194303153612e-06,
      "loss": 1.6091,
      "step": 91570
    },
    {
      "epoch": 46.58189216683621,
      "grad_norm": 35.260101318359375,
      "learning_rate": 3.4181078331637847e-06,
      "loss": 1.5049,
      "step": 91580
    },
    {
      "epoch": 46.58697863682604,
      "grad_norm": 50.7891845703125,
      "learning_rate": 3.413021363173957e-06,
      "loss": 1.5443,
      "step": 91590
    },
    {
      "epoch": 46.592065106815866,
      "grad_norm": 51.489524841308594,
      "learning_rate": 3.4079348931841304e-06,
      "loss": 1.5059,
      "step": 91600
    },
    {
      "epoch": 46.5971515768057,
      "grad_norm": 41.6096305847168,
      "learning_rate": 3.4028484231943033e-06,
      "loss": 1.4527,
      "step": 91610
    },
    {
      "epoch": 46.60223804679553,
      "grad_norm": 39.41971969604492,
      "learning_rate": 3.3977619532044766e-06,
      "loss": 1.4416,
      "step": 91620
    },
    {
      "epoch": 46.607324516785354,
      "grad_norm": 45.29721450805664,
      "learning_rate": 3.392675483214649e-06,
      "loss": 1.5207,
      "step": 91630
    },
    {
      "epoch": 46.61241098677518,
      "grad_norm": 43.771724700927734,
      "learning_rate": 3.387589013224822e-06,
      "loss": 1.5164,
      "step": 91640
    },
    {
      "epoch": 46.61749745676501,
      "grad_norm": 45.71156311035156,
      "learning_rate": 3.382502543234995e-06,
      "loss": 1.5765,
      "step": 91650
    },
    {
      "epoch": 46.622583926754835,
      "grad_norm": 57.3823356628418,
      "learning_rate": 3.377416073245168e-06,
      "loss": 1.5745,
      "step": 91660
    },
    {
      "epoch": 46.62767039674466,
      "grad_norm": 43.1251220703125,
      "learning_rate": 3.3723296032553413e-06,
      "loss": 1.4847,
      "step": 91670
    },
    {
      "epoch": 46.63275686673449,
      "grad_norm": 45.673011779785156,
      "learning_rate": 3.3672431332655137e-06,
      "loss": 1.5979,
      "step": 91680
    },
    {
      "epoch": 46.637843336724316,
      "grad_norm": 42.988433837890625,
      "learning_rate": 3.362156663275687e-06,
      "loss": 1.4816,
      "step": 91690
    },
    {
      "epoch": 46.64292980671414,
      "grad_norm": 49.3460693359375,
      "learning_rate": 3.35707019328586e-06,
      "loss": 1.4869,
      "step": 91700
    },
    {
      "epoch": 46.64801627670397,
      "grad_norm": 46.135154724121094,
      "learning_rate": 3.351983723296033e-06,
      "loss": 1.4195,
      "step": 91710
    },
    {
      "epoch": 46.6531027466938,
      "grad_norm": 38.70075607299805,
      "learning_rate": 3.3468972533062056e-06,
      "loss": 1.4969,
      "step": 91720
    },
    {
      "epoch": 46.658189216683624,
      "grad_norm": 47.945037841796875,
      "learning_rate": 3.3418107833163784e-06,
      "loss": 1.5334,
      "step": 91730
    },
    {
      "epoch": 46.66327568667345,
      "grad_norm": 41.466880798339844,
      "learning_rate": 3.3367243133265517e-06,
      "loss": 1.5495,
      "step": 91740
    },
    {
      "epoch": 46.66836215666328,
      "grad_norm": 40.33152770996094,
      "learning_rate": 3.3316378433367246e-06,
      "loss": 1.5552,
      "step": 91750
    },
    {
      "epoch": 46.673448626653105,
      "grad_norm": 43.8655891418457,
      "learning_rate": 3.326551373346897e-06,
      "loss": 1.5306,
      "step": 91760
    },
    {
      "epoch": 46.67853509664293,
      "grad_norm": 44.94731521606445,
      "learning_rate": 3.3214649033570703e-06,
      "loss": 1.4278,
      "step": 91770
    },
    {
      "epoch": 46.68362156663276,
      "grad_norm": 36.503360748291016,
      "learning_rate": 3.316378433367243e-06,
      "loss": 1.5424,
      "step": 91780
    },
    {
      "epoch": 46.688708036622586,
      "grad_norm": 42.428592681884766,
      "learning_rate": 3.3112919633774164e-06,
      "loss": 1.5052,
      "step": 91790
    },
    {
      "epoch": 46.69379450661241,
      "grad_norm": 50.20441436767578,
      "learning_rate": 3.306205493387589e-06,
      "loss": 1.4364,
      "step": 91800
    },
    {
      "epoch": 46.69888097660224,
      "grad_norm": 48.9525032043457,
      "learning_rate": 3.301119023397762e-06,
      "loss": 1.4313,
      "step": 91810
    },
    {
      "epoch": 46.70396744659207,
      "grad_norm": 35.560855865478516,
      "learning_rate": 3.296032553407935e-06,
      "loss": 1.4278,
      "step": 91820
    },
    {
      "epoch": 46.709053916581894,
      "grad_norm": 41.995643615722656,
      "learning_rate": 3.2909460834181083e-06,
      "loss": 1.488,
      "step": 91830
    },
    {
      "epoch": 46.71414038657172,
      "grad_norm": 46.30108642578125,
      "learning_rate": 3.285859613428281e-06,
      "loss": 1.5658,
      "step": 91840
    },
    {
      "epoch": 46.71922685656155,
      "grad_norm": 38.172725677490234,
      "learning_rate": 3.2807731434384536e-06,
      "loss": 1.5179,
      "step": 91850
    },
    {
      "epoch": 46.724313326551375,
      "grad_norm": 44.91112518310547,
      "learning_rate": 3.275686673448627e-06,
      "loss": 1.4919,
      "step": 91860
    },
    {
      "epoch": 46.7293997965412,
      "grad_norm": 39.61918640136719,
      "learning_rate": 3.2706002034587997e-06,
      "loss": 1.5306,
      "step": 91870
    },
    {
      "epoch": 46.73448626653103,
      "grad_norm": 42.57400894165039,
      "learning_rate": 3.265513733468973e-06,
      "loss": 1.4374,
      "step": 91880
    },
    {
      "epoch": 46.739572736520856,
      "grad_norm": 45.22875213623047,
      "learning_rate": 3.2604272634791454e-06,
      "loss": 1.5383,
      "step": 91890
    },
    {
      "epoch": 46.74465920651068,
      "grad_norm": 40.45534896850586,
      "learning_rate": 3.2553407934893183e-06,
      "loss": 1.5802,
      "step": 91900
    },
    {
      "epoch": 46.74974567650051,
      "grad_norm": 51.76003646850586,
      "learning_rate": 3.2502543234994916e-06,
      "loss": 1.531,
      "step": 91910
    },
    {
      "epoch": 46.75483214649034,
      "grad_norm": 53.655643463134766,
      "learning_rate": 3.245167853509665e-06,
      "loss": 1.4642,
      "step": 91920
    },
    {
      "epoch": 46.759918616480164,
      "grad_norm": 47.53974151611328,
      "learning_rate": 3.2400813835198373e-06,
      "loss": 1.5126,
      "step": 91930
    },
    {
      "epoch": 46.76500508646999,
      "grad_norm": 37.2698860168457,
      "learning_rate": 3.23499491353001e-06,
      "loss": 1.5555,
      "step": 91940
    },
    {
      "epoch": 46.77009155645982,
      "grad_norm": 45.63935470581055,
      "learning_rate": 3.2299084435401834e-06,
      "loss": 1.4792,
      "step": 91950
    },
    {
      "epoch": 46.775178026449645,
      "grad_norm": 40.31132125854492,
      "learning_rate": 3.2248219735503563e-06,
      "loss": 1.6096,
      "step": 91960
    },
    {
      "epoch": 46.78026449643947,
      "grad_norm": 44.23741149902344,
      "learning_rate": 3.2197355035605287e-06,
      "loss": 1.4459,
      "step": 91970
    },
    {
      "epoch": 46.7853509664293,
      "grad_norm": 35.78767776489258,
      "learning_rate": 3.214649033570702e-06,
      "loss": 1.5155,
      "step": 91980
    },
    {
      "epoch": 46.790437436419126,
      "grad_norm": 41.261783599853516,
      "learning_rate": 3.209562563580875e-06,
      "loss": 1.5055,
      "step": 91990
    },
    {
      "epoch": 46.79552390640895,
      "grad_norm": 60.60747146606445,
      "learning_rate": 3.204476093591048e-06,
      "loss": 1.4422,
      "step": 92000
    },
    {
      "epoch": 46.80061037639878,
      "grad_norm": 46.591251373291016,
      "learning_rate": 3.1993896236012214e-06,
      "loss": 1.456,
      "step": 92010
    },
    {
      "epoch": 46.80569684638861,
      "grad_norm": 41.86826705932617,
      "learning_rate": 3.1943031536113934e-06,
      "loss": 1.4713,
      "step": 92020
    },
    {
      "epoch": 46.81078331637843,
      "grad_norm": 41.807926177978516,
      "learning_rate": 3.1892166836215667e-06,
      "loss": 1.4805,
      "step": 92030
    },
    {
      "epoch": 46.81586978636826,
      "grad_norm": 37.601104736328125,
      "learning_rate": 3.18413021363174e-06,
      "loss": 1.5392,
      "step": 92040
    },
    {
      "epoch": 46.82095625635809,
      "grad_norm": 46.282142639160156,
      "learning_rate": 3.179043743641913e-06,
      "loss": 1.57,
      "step": 92050
    },
    {
      "epoch": 46.826042726347914,
      "grad_norm": 39.364356994628906,
      "learning_rate": 3.1739572736520853e-06,
      "loss": 1.5073,
      "step": 92060
    },
    {
      "epoch": 46.83112919633774,
      "grad_norm": 39.397090911865234,
      "learning_rate": 3.1688708036622586e-06,
      "loss": 1.4607,
      "step": 92070
    },
    {
      "epoch": 46.83621566632757,
      "grad_norm": 41.762969970703125,
      "learning_rate": 3.1637843336724314e-06,
      "loss": 1.4639,
      "step": 92080
    },
    {
      "epoch": 46.841302136317395,
      "grad_norm": 43.02061080932617,
      "learning_rate": 3.1586978636826047e-06,
      "loss": 1.5453,
      "step": 92090
    },
    {
      "epoch": 46.84638860630722,
      "grad_norm": 44.15068054199219,
      "learning_rate": 3.153611393692777e-06,
      "loss": 1.4597,
      "step": 92100
    },
    {
      "epoch": 46.85147507629705,
      "grad_norm": 46.24961471557617,
      "learning_rate": 3.14852492370295e-06,
      "loss": 1.5002,
      "step": 92110
    },
    {
      "epoch": 46.856561546286876,
      "grad_norm": 52.16416549682617,
      "learning_rate": 3.1434384537131233e-06,
      "loss": 1.4669,
      "step": 92120
    },
    {
      "epoch": 46.8616480162767,
      "grad_norm": 48.80875778198242,
      "learning_rate": 3.1383519837232966e-06,
      "loss": 1.5112,
      "step": 92130
    },
    {
      "epoch": 46.86673448626653,
      "grad_norm": 43.92155075073242,
      "learning_rate": 3.1332655137334694e-06,
      "loss": 1.4538,
      "step": 92140
    },
    {
      "epoch": 46.87182095625636,
      "grad_norm": 41.71023941040039,
      "learning_rate": 3.128179043743642e-06,
      "loss": 1.5322,
      "step": 92150
    },
    {
      "epoch": 46.876907426246184,
      "grad_norm": 45.32436752319336,
      "learning_rate": 3.123092573753815e-06,
      "loss": 1.4456,
      "step": 92160
    },
    {
      "epoch": 46.88199389623601,
      "grad_norm": 38.010833740234375,
      "learning_rate": 3.118006103763988e-06,
      "loss": 1.569,
      "step": 92170
    },
    {
      "epoch": 46.88708036622584,
      "grad_norm": 40.30180740356445,
      "learning_rate": 3.112919633774161e-06,
      "loss": 1.5486,
      "step": 92180
    },
    {
      "epoch": 46.892166836215665,
      "grad_norm": 47.405364990234375,
      "learning_rate": 3.107833163784334e-06,
      "loss": 1.4933,
      "step": 92190
    },
    {
      "epoch": 46.89725330620549,
      "grad_norm": 46.633792877197266,
      "learning_rate": 3.1027466937945066e-06,
      "loss": 1.5308,
      "step": 92200
    },
    {
      "epoch": 46.90233977619532,
      "grad_norm": 37.6888313293457,
      "learning_rate": 3.09766022380468e-06,
      "loss": 1.5316,
      "step": 92210
    },
    {
      "epoch": 46.907426246185146,
      "grad_norm": 40.21061706542969,
      "learning_rate": 3.0925737538148527e-06,
      "loss": 1.6212,
      "step": 92220
    },
    {
      "epoch": 46.91251271617497,
      "grad_norm": 40.99703598022461,
      "learning_rate": 3.0874872838250256e-06,
      "loss": 1.5342,
      "step": 92230
    },
    {
      "epoch": 46.9175991861648,
      "grad_norm": 38.413089752197266,
      "learning_rate": 3.0824008138351984e-06,
      "loss": 1.4857,
      "step": 92240
    },
    {
      "epoch": 46.92268565615463,
      "grad_norm": 52.961769104003906,
      "learning_rate": 3.0773143438453717e-06,
      "loss": 1.4711,
      "step": 92250
    },
    {
      "epoch": 46.927772126144454,
      "grad_norm": 34.088722229003906,
      "learning_rate": 3.072227873855544e-06,
      "loss": 1.5436,
      "step": 92260
    },
    {
      "epoch": 46.93285859613428,
      "grad_norm": 37.754737854003906,
      "learning_rate": 3.0671414038657174e-06,
      "loss": 1.5339,
      "step": 92270
    },
    {
      "epoch": 46.93794506612411,
      "grad_norm": 41.21103286743164,
      "learning_rate": 3.0620549338758903e-06,
      "loss": 1.5001,
      "step": 92280
    },
    {
      "epoch": 46.943031536113935,
      "grad_norm": 45.17170333862305,
      "learning_rate": 3.056968463886063e-06,
      "loss": 1.4441,
      "step": 92290
    },
    {
      "epoch": 46.94811800610376,
      "grad_norm": 44.17101287841797,
      "learning_rate": 3.0518819938962364e-06,
      "loss": 1.4959,
      "step": 92300
    },
    {
      "epoch": 46.95320447609359,
      "grad_norm": 43.33843994140625,
      "learning_rate": 3.0467955239064093e-06,
      "loss": 1.4799,
      "step": 92310
    },
    {
      "epoch": 46.958290946083416,
      "grad_norm": 38.58918380737305,
      "learning_rate": 3.041709053916582e-06,
      "loss": 1.5162,
      "step": 92320
    },
    {
      "epoch": 46.96337741607324,
      "grad_norm": 35.291648864746094,
      "learning_rate": 3.036622583926755e-06,
      "loss": 1.5104,
      "step": 92330
    },
    {
      "epoch": 46.96846388606307,
      "grad_norm": 48.6464958190918,
      "learning_rate": 3.031536113936928e-06,
      "loss": 1.5413,
      "step": 92340
    },
    {
      "epoch": 46.9735503560529,
      "grad_norm": 39.31510925292969,
      "learning_rate": 3.0264496439471007e-06,
      "loss": 1.5032,
      "step": 92350
    },
    {
      "epoch": 46.978636826042724,
      "grad_norm": 36.988922119140625,
      "learning_rate": 3.021363173957274e-06,
      "loss": 1.4952,
      "step": 92360
    },
    {
      "epoch": 46.98372329603255,
      "grad_norm": 45.420169830322266,
      "learning_rate": 3.016276703967447e-06,
      "loss": 1.5727,
      "step": 92370
    },
    {
      "epoch": 46.98880976602238,
      "grad_norm": 40.592288970947266,
      "learning_rate": 3.0111902339776197e-06,
      "loss": 1.4147,
      "step": 92380
    },
    {
      "epoch": 46.993896236012205,
      "grad_norm": 33.92420196533203,
      "learning_rate": 3.0061037639877926e-06,
      "loss": 1.5688,
      "step": 92390
    },
    {
      "epoch": 46.99898270600203,
      "grad_norm": 41.460384368896484,
      "learning_rate": 3.0010172939979654e-06,
      "loss": 1.5244,
      "step": 92400
    },
    {
      "epoch": 47.0,
      "eval_loss": 5.128779888153076,
      "eval_runtime": 2.7111,
      "eval_samples_per_second": 1023.576,
      "eval_steps_per_second": 127.993,
      "step": 92402
    },
    {
      "epoch": 47.00406917599186,
      "grad_norm": 44.70136260986328,
      "learning_rate": 2.9959308240081383e-06,
      "loss": 1.496,
      "step": 92410
    },
    {
      "epoch": 47.009155645981686,
      "grad_norm": 34.83604431152344,
      "learning_rate": 2.9908443540183116e-06,
      "loss": 1.5714,
      "step": 92420
    },
    {
      "epoch": 47.01424211597151,
      "grad_norm": 38.14518737792969,
      "learning_rate": 2.9857578840284844e-06,
      "loss": 1.5878,
      "step": 92430
    },
    {
      "epoch": 47.01932858596134,
      "grad_norm": 46.53135299682617,
      "learning_rate": 2.9806714140386573e-06,
      "loss": 1.5238,
      "step": 92440
    },
    {
      "epoch": 47.02441505595117,
      "grad_norm": 38.58788299560547,
      "learning_rate": 2.97558494404883e-06,
      "loss": 1.5686,
      "step": 92450
    },
    {
      "epoch": 47.029501525940994,
      "grad_norm": 46.70903015136719,
      "learning_rate": 2.970498474059003e-06,
      "loss": 1.4537,
      "step": 92460
    },
    {
      "epoch": 47.03458799593082,
      "grad_norm": 48.61833190917969,
      "learning_rate": 2.9654120040691763e-06,
      "loss": 1.521,
      "step": 92470
    },
    {
      "epoch": 47.03967446592065,
      "grad_norm": 48.28695297241211,
      "learning_rate": 2.960325534079349e-06,
      "loss": 1.55,
      "step": 92480
    },
    {
      "epoch": 47.044760935910475,
      "grad_norm": 48.387351989746094,
      "learning_rate": 2.955239064089522e-06,
      "loss": 1.5227,
      "step": 92490
    },
    {
      "epoch": 47.04984740590031,
      "grad_norm": 40.76081085205078,
      "learning_rate": 2.950152594099695e-06,
      "loss": 1.4986,
      "step": 92500
    },
    {
      "epoch": 47.054933875890136,
      "grad_norm": 44.93098831176758,
      "learning_rate": 2.945066124109868e-06,
      "loss": 1.4895,
      "step": 92510
    },
    {
      "epoch": 47.06002034587996,
      "grad_norm": 39.383766174316406,
      "learning_rate": 2.9399796541200406e-06,
      "loss": 1.5748,
      "step": 92520
    },
    {
      "epoch": 47.06510681586979,
      "grad_norm": 34.75510025024414,
      "learning_rate": 2.934893184130214e-06,
      "loss": 1.4578,
      "step": 92530
    },
    {
      "epoch": 47.07019328585962,
      "grad_norm": 39.503055572509766,
      "learning_rate": 2.9298067141403867e-06,
      "loss": 1.5011,
      "step": 92540
    },
    {
      "epoch": 47.075279755849444,
      "grad_norm": 39.11096954345703,
      "learning_rate": 2.9247202441505596e-06,
      "loss": 1.4965,
      "step": 92550
    },
    {
      "epoch": 47.08036622583927,
      "grad_norm": 43.6878547668457,
      "learning_rate": 2.9196337741607324e-06,
      "loss": 1.5169,
      "step": 92560
    },
    {
      "epoch": 47.0854526958291,
      "grad_norm": 41.2558479309082,
      "learning_rate": 2.9145473041709057e-06,
      "loss": 1.4959,
      "step": 92570
    },
    {
      "epoch": 47.090539165818925,
      "grad_norm": 46.64643096923828,
      "learning_rate": 2.909460834181078e-06,
      "loss": 1.5047,
      "step": 92580
    },
    {
      "epoch": 47.09562563580875,
      "grad_norm": 36.628334045410156,
      "learning_rate": 2.9043743641912514e-06,
      "loss": 1.5022,
      "step": 92590
    },
    {
      "epoch": 47.10071210579858,
      "grad_norm": 45.07879638671875,
      "learning_rate": 2.8992878942014243e-06,
      "loss": 1.5141,
      "step": 92600
    },
    {
      "epoch": 47.105798575788405,
      "grad_norm": 38.017826080322266,
      "learning_rate": 2.894201424211597e-06,
      "loss": 1.4175,
      "step": 92610
    },
    {
      "epoch": 47.11088504577823,
      "grad_norm": 44.285888671875,
      "learning_rate": 2.88911495422177e-06,
      "loss": 1.4552,
      "step": 92620
    },
    {
      "epoch": 47.11597151576806,
      "grad_norm": 55.33749771118164,
      "learning_rate": 2.8840284842319433e-06,
      "loss": 1.5384,
      "step": 92630
    },
    {
      "epoch": 47.121057985757886,
      "grad_norm": 41.08788299560547,
      "learning_rate": 2.878942014242116e-06,
      "loss": 1.4727,
      "step": 92640
    },
    {
      "epoch": 47.12614445574771,
      "grad_norm": 40.37324142456055,
      "learning_rate": 2.873855544252289e-06,
      "loss": 1.4113,
      "step": 92650
    },
    {
      "epoch": 47.13123092573754,
      "grad_norm": 47.727169036865234,
      "learning_rate": 2.8687690742624623e-06,
      "loss": 1.539,
      "step": 92660
    },
    {
      "epoch": 47.13631739572737,
      "grad_norm": 55.8167610168457,
      "learning_rate": 2.8636826042726347e-06,
      "loss": 1.5432,
      "step": 92670
    },
    {
      "epoch": 47.141403865717194,
      "grad_norm": 55.91301345825195,
      "learning_rate": 2.858596134282808e-06,
      "loss": 1.5727,
      "step": 92680
    },
    {
      "epoch": 47.14649033570702,
      "grad_norm": 36.38865661621094,
      "learning_rate": 2.853509664292981e-06,
      "loss": 1.4869,
      "step": 92690
    },
    {
      "epoch": 47.15157680569685,
      "grad_norm": 38.407039642333984,
      "learning_rate": 2.8484231943031537e-06,
      "loss": 1.5157,
      "step": 92700
    },
    {
      "epoch": 47.156663275686675,
      "grad_norm": 39.09317398071289,
      "learning_rate": 2.8433367243133266e-06,
      "loss": 1.5331,
      "step": 92710
    },
    {
      "epoch": 47.1617497456765,
      "grad_norm": 49.468780517578125,
      "learning_rate": 2.8382502543235e-06,
      "loss": 1.5225,
      "step": 92720
    },
    {
      "epoch": 47.16683621566633,
      "grad_norm": 47.7297477722168,
      "learning_rate": 2.8331637843336723e-06,
      "loss": 1.4133,
      "step": 92730
    },
    {
      "epoch": 47.171922685656156,
      "grad_norm": 46.133888244628906,
      "learning_rate": 2.8280773143438456e-06,
      "loss": 1.4915,
      "step": 92740
    },
    {
      "epoch": 47.17700915564598,
      "grad_norm": 46.26949691772461,
      "learning_rate": 2.8229908443540184e-06,
      "loss": 1.5939,
      "step": 92750
    },
    {
      "epoch": 47.18209562563581,
      "grad_norm": 44.431846618652344,
      "learning_rate": 2.8179043743641913e-06,
      "loss": 1.5283,
      "step": 92760
    },
    {
      "epoch": 47.18718209562564,
      "grad_norm": 37.49402618408203,
      "learning_rate": 2.812817904374364e-06,
      "loss": 1.5196,
      "step": 92770
    },
    {
      "epoch": 47.192268565615464,
      "grad_norm": 45.849063873291016,
      "learning_rate": 2.8077314343845374e-06,
      "loss": 1.5849,
      "step": 92780
    },
    {
      "epoch": 47.19735503560529,
      "grad_norm": 39.47105407714844,
      "learning_rate": 2.8026449643947103e-06,
      "loss": 1.5154,
      "step": 92790
    },
    {
      "epoch": 47.20244150559512,
      "grad_norm": 45.75829315185547,
      "learning_rate": 2.797558494404883e-06,
      "loss": 1.4701,
      "step": 92800
    },
    {
      "epoch": 47.207527975584945,
      "grad_norm": 44.086483001708984,
      "learning_rate": 2.7924720244150564e-06,
      "loss": 1.538,
      "step": 92810
    },
    {
      "epoch": 47.21261444557477,
      "grad_norm": 37.85213851928711,
      "learning_rate": 2.787385554425229e-06,
      "loss": 1.6225,
      "step": 92820
    },
    {
      "epoch": 47.2177009155646,
      "grad_norm": 40.76707077026367,
      "learning_rate": 2.782299084435402e-06,
      "loss": 1.5819,
      "step": 92830
    },
    {
      "epoch": 47.222787385554426,
      "grad_norm": 46.653717041015625,
      "learning_rate": 2.777212614445575e-06,
      "loss": 1.4878,
      "step": 92840
    },
    {
      "epoch": 47.22787385554425,
      "grad_norm": 38.773651123046875,
      "learning_rate": 2.772126144455748e-06,
      "loss": 1.5255,
      "step": 92850
    },
    {
      "epoch": 47.23296032553408,
      "grad_norm": 44.637760162353516,
      "learning_rate": 2.7670396744659207e-06,
      "loss": 1.4434,
      "step": 92860
    },
    {
      "epoch": 47.23804679552391,
      "grad_norm": 36.96111297607422,
      "learning_rate": 2.761953204476094e-06,
      "loss": 1.4739,
      "step": 92870
    },
    {
      "epoch": 47.243133265513734,
      "grad_norm": 41.93818664550781,
      "learning_rate": 2.7568667344862664e-06,
      "loss": 1.503,
      "step": 92880
    },
    {
      "epoch": 47.24821973550356,
      "grad_norm": 46.84939956665039,
      "learning_rate": 2.7517802644964397e-06,
      "loss": 1.5594,
      "step": 92890
    },
    {
      "epoch": 47.25330620549339,
      "grad_norm": 47.43837356567383,
      "learning_rate": 2.7466937945066126e-06,
      "loss": 1.4968,
      "step": 92900
    },
    {
      "epoch": 47.258392675483215,
      "grad_norm": 44.231082916259766,
      "learning_rate": 2.7416073245167854e-06,
      "loss": 1.4973,
      "step": 92910
    },
    {
      "epoch": 47.26347914547304,
      "grad_norm": 35.95551681518555,
      "learning_rate": 2.7365208545269583e-06,
      "loss": 1.4913,
      "step": 92920
    },
    {
      "epoch": 47.26856561546287,
      "grad_norm": 43.931461334228516,
      "learning_rate": 2.7314343845371316e-06,
      "loss": 1.4843,
      "step": 92930
    },
    {
      "epoch": 47.273652085452696,
      "grad_norm": 43.25667953491211,
      "learning_rate": 2.726347914547304e-06,
      "loss": 1.5252,
      "step": 92940
    },
    {
      "epoch": 47.27873855544252,
      "grad_norm": 40.59513473510742,
      "learning_rate": 2.7212614445574773e-06,
      "loss": 1.4339,
      "step": 92950
    },
    {
      "epoch": 47.28382502543235,
      "grad_norm": 42.21417236328125,
      "learning_rate": 2.71617497456765e-06,
      "loss": 1.5813,
      "step": 92960
    },
    {
      "epoch": 47.28891149542218,
      "grad_norm": 41.04456329345703,
      "learning_rate": 2.711088504577823e-06,
      "loss": 1.4938,
      "step": 92970
    },
    {
      "epoch": 47.293997965412004,
      "grad_norm": 35.31217575073242,
      "learning_rate": 2.7060020345879963e-06,
      "loss": 1.5803,
      "step": 92980
    },
    {
      "epoch": 47.29908443540183,
      "grad_norm": 54.03731155395508,
      "learning_rate": 2.700915564598169e-06,
      "loss": 1.5162,
      "step": 92990
    },
    {
      "epoch": 47.30417090539166,
      "grad_norm": 41.49789810180664,
      "learning_rate": 2.695829094608342e-06,
      "loss": 1.5142,
      "step": 93000
    },
    {
      "epoch": 47.309257375381485,
      "grad_norm": 49.26539611816406,
      "learning_rate": 2.690742624618515e-06,
      "loss": 1.5568,
      "step": 93010
    },
    {
      "epoch": 47.31434384537131,
      "grad_norm": 41.95469665527344,
      "learning_rate": 2.6856561546286877e-06,
      "loss": 1.4656,
      "step": 93020
    },
    {
      "epoch": 47.31943031536114,
      "grad_norm": 43.64715576171875,
      "learning_rate": 2.6805696846388606e-06,
      "loss": 1.5223,
      "step": 93030
    },
    {
      "epoch": 47.324516785350966,
      "grad_norm": 54.78709030151367,
      "learning_rate": 2.675483214649034e-06,
      "loss": 1.5514,
      "step": 93040
    },
    {
      "epoch": 47.32960325534079,
      "grad_norm": 43.922096252441406,
      "learning_rate": 2.6703967446592067e-06,
      "loss": 1.5547,
      "step": 93050
    },
    {
      "epoch": 47.33468972533062,
      "grad_norm": 38.750099182128906,
      "learning_rate": 2.6653102746693796e-06,
      "loss": 1.6123,
      "step": 93060
    },
    {
      "epoch": 47.33977619532045,
      "grad_norm": 50.69618606567383,
      "learning_rate": 2.6602238046795524e-06,
      "loss": 1.4593,
      "step": 93070
    },
    {
      "epoch": 47.34486266531027,
      "grad_norm": 48.236305236816406,
      "learning_rate": 2.6551373346897253e-06,
      "loss": 1.5383,
      "step": 93080
    },
    {
      "epoch": 47.3499491353001,
      "grad_norm": 36.23395919799805,
      "learning_rate": 2.650050864699898e-06,
      "loss": 1.5075,
      "step": 93090
    },
    {
      "epoch": 47.35503560528993,
      "grad_norm": 37.00498962402344,
      "learning_rate": 2.6449643947100714e-06,
      "loss": 1.4504,
      "step": 93100
    },
    {
      "epoch": 47.360122075279754,
      "grad_norm": 40.316139221191406,
      "learning_rate": 2.6398779247202443e-06,
      "loss": 1.5548,
      "step": 93110
    },
    {
      "epoch": 47.36520854526958,
      "grad_norm": 39.79369354248047,
      "learning_rate": 2.634791454730417e-06,
      "loss": 1.4809,
      "step": 93120
    },
    {
      "epoch": 47.37029501525941,
      "grad_norm": 47.20327377319336,
      "learning_rate": 2.6297049847405904e-06,
      "loss": 1.4972,
      "step": 93130
    },
    {
      "epoch": 47.375381485249235,
      "grad_norm": 37.700958251953125,
      "learning_rate": 2.6246185147507633e-06,
      "loss": 1.5047,
      "step": 93140
    },
    {
      "epoch": 47.38046795523906,
      "grad_norm": 52.72771453857422,
      "learning_rate": 2.619532044760936e-06,
      "loss": 1.4948,
      "step": 93150
    },
    {
      "epoch": 47.38555442522889,
      "grad_norm": 42.03061294555664,
      "learning_rate": 2.614445574771109e-06,
      "loss": 1.4545,
      "step": 93160
    },
    {
      "epoch": 47.390640895218716,
      "grad_norm": 48.02322769165039,
      "learning_rate": 2.609359104781282e-06,
      "loss": 1.5097,
      "step": 93170
    },
    {
      "epoch": 47.39572736520854,
      "grad_norm": 49.21272659301758,
      "learning_rate": 2.6042726347914547e-06,
      "loss": 1.5152,
      "step": 93180
    },
    {
      "epoch": 47.40081383519837,
      "grad_norm": 45.79226303100586,
      "learning_rate": 2.599186164801628e-06,
      "loss": 1.4993,
      "step": 93190
    },
    {
      "epoch": 47.4059003051882,
      "grad_norm": 37.52225112915039,
      "learning_rate": 2.594099694811801e-06,
      "loss": 1.5686,
      "step": 93200
    },
    {
      "epoch": 47.410986775178024,
      "grad_norm": 42.30246353149414,
      "learning_rate": 2.5890132248219737e-06,
      "loss": 1.5453,
      "step": 93210
    },
    {
      "epoch": 47.41607324516785,
      "grad_norm": 39.69456481933594,
      "learning_rate": 2.5839267548321466e-06,
      "loss": 1.5081,
      "step": 93220
    },
    {
      "epoch": 47.42115971515768,
      "grad_norm": 36.276180267333984,
      "learning_rate": 2.5788402848423194e-06,
      "loss": 1.4397,
      "step": 93230
    },
    {
      "epoch": 47.426246185147505,
      "grad_norm": 39.77678680419922,
      "learning_rate": 2.5737538148524923e-06,
      "loss": 1.4845,
      "step": 93240
    },
    {
      "epoch": 47.43133265513733,
      "grad_norm": 39.479984283447266,
      "learning_rate": 2.5686673448626656e-06,
      "loss": 1.5525,
      "step": 93250
    },
    {
      "epoch": 47.43641912512716,
      "grad_norm": 41.27796936035156,
      "learning_rate": 2.5635808748728384e-06,
      "loss": 1.5274,
      "step": 93260
    },
    {
      "epoch": 47.441505595116986,
      "grad_norm": 44.92918395996094,
      "learning_rate": 2.5584944048830113e-06,
      "loss": 1.4639,
      "step": 93270
    },
    {
      "epoch": 47.44659206510681,
      "grad_norm": 45.589881896972656,
      "learning_rate": 2.5534079348931846e-06,
      "loss": 1.4464,
      "step": 93280
    },
    {
      "epoch": 47.45167853509664,
      "grad_norm": 45.79493713378906,
      "learning_rate": 2.548321464903357e-06,
      "loss": 1.5111,
      "step": 93290
    },
    {
      "epoch": 47.45676500508647,
      "grad_norm": 43.66863250732422,
      "learning_rate": 2.5432349949135303e-06,
      "loss": 1.501,
      "step": 93300
    },
    {
      "epoch": 47.461851475076294,
      "grad_norm": 39.823909759521484,
      "learning_rate": 2.538148524923703e-06,
      "loss": 1.4579,
      "step": 93310
    },
    {
      "epoch": 47.46693794506612,
      "grad_norm": 40.8309440612793,
      "learning_rate": 2.533062054933876e-06,
      "loss": 1.5035,
      "step": 93320
    },
    {
      "epoch": 47.47202441505595,
      "grad_norm": 45.53833770751953,
      "learning_rate": 2.527975584944049e-06,
      "loss": 1.4999,
      "step": 93330
    },
    {
      "epoch": 47.477110885045775,
      "grad_norm": 45.86545181274414,
      "learning_rate": 2.522889114954222e-06,
      "loss": 1.4612,
      "step": 93340
    },
    {
      "epoch": 47.4821973550356,
      "grad_norm": 40.022830963134766,
      "learning_rate": 2.5178026449643946e-06,
      "loss": 1.5203,
      "step": 93350
    },
    {
      "epoch": 47.48728382502543,
      "grad_norm": 44.79206085205078,
      "learning_rate": 2.512716174974568e-06,
      "loss": 1.591,
      "step": 93360
    },
    {
      "epoch": 47.492370295015256,
      "grad_norm": 51.02528381347656,
      "learning_rate": 2.5076297049847407e-06,
      "loss": 1.5355,
      "step": 93370
    },
    {
      "epoch": 47.49745676500508,
      "grad_norm": 44.20241165161133,
      "learning_rate": 2.5025432349949136e-06,
      "loss": 1.5242,
      "step": 93380
    },
    {
      "epoch": 47.50254323499492,
      "grad_norm": 38.867820739746094,
      "learning_rate": 2.4974567650050864e-06,
      "loss": 1.4967,
      "step": 93390
    },
    {
      "epoch": 47.507629704984744,
      "grad_norm": 37.58338928222656,
      "learning_rate": 2.4923702950152597e-06,
      "loss": 1.4992,
      "step": 93400
    },
    {
      "epoch": 47.51271617497457,
      "grad_norm": 34.364837646484375,
      "learning_rate": 2.487283825025432e-06,
      "loss": 1.5256,
      "step": 93410
    },
    {
      "epoch": 47.5178026449644,
      "grad_norm": 49.961734771728516,
      "learning_rate": 2.4821973550356054e-06,
      "loss": 1.5124,
      "step": 93420
    },
    {
      "epoch": 47.522889114954225,
      "grad_norm": 46.353172302246094,
      "learning_rate": 2.4771108850457783e-06,
      "loss": 1.5113,
      "step": 93430
    },
    {
      "epoch": 47.52797558494405,
      "grad_norm": 39.225589752197266,
      "learning_rate": 2.472024415055951e-06,
      "loss": 1.5981,
      "step": 93440
    },
    {
      "epoch": 47.53306205493388,
      "grad_norm": 42.509918212890625,
      "learning_rate": 2.4669379450661244e-06,
      "loss": 1.5234,
      "step": 93450
    },
    {
      "epoch": 47.538148524923706,
      "grad_norm": 46.79204177856445,
      "learning_rate": 2.4618514750762973e-06,
      "loss": 1.5116,
      "step": 93460
    },
    {
      "epoch": 47.54323499491353,
      "grad_norm": 41.967926025390625,
      "learning_rate": 2.45676500508647e-06,
      "loss": 1.4946,
      "step": 93470
    },
    {
      "epoch": 47.54832146490336,
      "grad_norm": 38.22476577758789,
      "learning_rate": 2.451678535096643e-06,
      "loss": 1.4923,
      "step": 93480
    },
    {
      "epoch": 47.55340793489319,
      "grad_norm": 36.92522048950195,
      "learning_rate": 2.4465920651068163e-06,
      "loss": 1.4536,
      "step": 93490
    },
    {
      "epoch": 47.558494404883014,
      "grad_norm": 47.0152473449707,
      "learning_rate": 2.4415055951169887e-06,
      "loss": 1.4974,
      "step": 93500
    },
    {
      "epoch": 47.56358087487284,
      "grad_norm": 46.449005126953125,
      "learning_rate": 2.436419125127162e-06,
      "loss": 1.4629,
      "step": 93510
    },
    {
      "epoch": 47.56866734486267,
      "grad_norm": 37.61407470703125,
      "learning_rate": 2.431332655137335e-06,
      "loss": 1.5185,
      "step": 93520
    },
    {
      "epoch": 47.573753814852495,
      "grad_norm": 38.26049041748047,
      "learning_rate": 2.4262461851475077e-06,
      "loss": 1.5585,
      "step": 93530
    },
    {
      "epoch": 47.57884028484232,
      "grad_norm": 37.21469497680664,
      "learning_rate": 2.4211597151576806e-06,
      "loss": 1.4912,
      "step": 93540
    },
    {
      "epoch": 47.58392675483215,
      "grad_norm": 47.9929084777832,
      "learning_rate": 2.416073245167854e-06,
      "loss": 1.4913,
      "step": 93550
    },
    {
      "epoch": 47.589013224821976,
      "grad_norm": 40.61515808105469,
      "learning_rate": 2.4109867751780263e-06,
      "loss": 1.4957,
      "step": 93560
    },
    {
      "epoch": 47.5940996948118,
      "grad_norm": 38.649635314941406,
      "learning_rate": 2.4059003051881996e-06,
      "loss": 1.5318,
      "step": 93570
    },
    {
      "epoch": 47.59918616480163,
      "grad_norm": 42.23223876953125,
      "learning_rate": 2.4008138351983724e-06,
      "loss": 1.5466,
      "step": 93580
    },
    {
      "epoch": 47.60427263479146,
      "grad_norm": 42.63998794555664,
      "learning_rate": 2.3957273652085453e-06,
      "loss": 1.557,
      "step": 93590
    },
    {
      "epoch": 47.609359104781284,
      "grad_norm": 43.74321365356445,
      "learning_rate": 2.3906408952187186e-06,
      "loss": 1.4688,
      "step": 93600
    },
    {
      "epoch": 47.61444557477111,
      "grad_norm": 51.240234375,
      "learning_rate": 2.3855544252288914e-06,
      "loss": 1.4412,
      "step": 93610
    },
    {
      "epoch": 47.61953204476094,
      "grad_norm": 52.428794860839844,
      "learning_rate": 2.3804679552390643e-06,
      "loss": 1.4462,
      "step": 93620
    },
    {
      "epoch": 47.624618514750765,
      "grad_norm": 40.840431213378906,
      "learning_rate": 2.375381485249237e-06,
      "loss": 1.4466,
      "step": 93630
    },
    {
      "epoch": 47.62970498474059,
      "grad_norm": 38.10345458984375,
      "learning_rate": 2.3702950152594104e-06,
      "loss": 1.469,
      "step": 93640
    },
    {
      "epoch": 47.63479145473042,
      "grad_norm": 48.59545135498047,
      "learning_rate": 2.365208545269583e-06,
      "loss": 1.5391,
      "step": 93650
    },
    {
      "epoch": 47.639877924720246,
      "grad_norm": 46.790130615234375,
      "learning_rate": 2.360122075279756e-06,
      "loss": 1.4774,
      "step": 93660
    },
    {
      "epoch": 47.64496439471007,
      "grad_norm": 44.297523498535156,
      "learning_rate": 2.355035605289929e-06,
      "loss": 1.5362,
      "step": 93670
    },
    {
      "epoch": 47.6500508646999,
      "grad_norm": 45.34919738769531,
      "learning_rate": 2.349949135300102e-06,
      "loss": 1.49,
      "step": 93680
    },
    {
      "epoch": 47.65513733468973,
      "grad_norm": 36.6839714050293,
      "learning_rate": 2.3448626653102747e-06,
      "loss": 1.4172,
      "step": 93690
    },
    {
      "epoch": 47.66022380467955,
      "grad_norm": 46.13703155517578,
      "learning_rate": 2.339776195320448e-06,
      "loss": 1.4883,
      "step": 93700
    },
    {
      "epoch": 47.66531027466938,
      "grad_norm": 40.623199462890625,
      "learning_rate": 2.3346897253306204e-06,
      "loss": 1.5325,
      "step": 93710
    },
    {
      "epoch": 47.67039674465921,
      "grad_norm": 44.07675552368164,
      "learning_rate": 2.3296032553407937e-06,
      "loss": 1.4479,
      "step": 93720
    },
    {
      "epoch": 47.675483214649034,
      "grad_norm": 45.86684799194336,
      "learning_rate": 2.3245167853509666e-06,
      "loss": 1.5034,
      "step": 93730
    },
    {
      "epoch": 47.68056968463886,
      "grad_norm": 39.0511474609375,
      "learning_rate": 2.3194303153611394e-06,
      "loss": 1.4155,
      "step": 93740
    },
    {
      "epoch": 47.68565615462869,
      "grad_norm": 34.12748718261719,
      "learning_rate": 2.3143438453713123e-06,
      "loss": 1.4767,
      "step": 93750
    },
    {
      "epoch": 47.690742624618515,
      "grad_norm": 53.38546371459961,
      "learning_rate": 2.3092573753814856e-06,
      "loss": 1.5286,
      "step": 93760
    },
    {
      "epoch": 47.69582909460834,
      "grad_norm": 34.65660858154297,
      "learning_rate": 2.3041709053916584e-06,
      "loss": 1.4412,
      "step": 93770
    },
    {
      "epoch": 47.70091556459817,
      "grad_norm": 55.1549072265625,
      "learning_rate": 2.2990844354018313e-06,
      "loss": 1.5573,
      "step": 93780
    },
    {
      "epoch": 47.706002034587996,
      "grad_norm": 39.38185119628906,
      "learning_rate": 2.293997965412004e-06,
      "loss": 1.5084,
      "step": 93790
    },
    {
      "epoch": 47.71108850457782,
      "grad_norm": 41.03104782104492,
      "learning_rate": 2.288911495422177e-06,
      "loss": 1.4564,
      "step": 93800
    },
    {
      "epoch": 47.71617497456765,
      "grad_norm": 41.430328369140625,
      "learning_rate": 2.2838250254323503e-06,
      "loss": 1.4445,
      "step": 93810
    },
    {
      "epoch": 47.72126144455748,
      "grad_norm": 34.73393630981445,
      "learning_rate": 2.278738555442523e-06,
      "loss": 1.5212,
      "step": 93820
    },
    {
      "epoch": 47.726347914547304,
      "grad_norm": 43.43172073364258,
      "learning_rate": 2.273652085452696e-06,
      "loss": 1.4965,
      "step": 93830
    },
    {
      "epoch": 47.73143438453713,
      "grad_norm": 48.84751510620117,
      "learning_rate": 2.268565615462869e-06,
      "loss": 1.4997,
      "step": 93840
    },
    {
      "epoch": 47.73652085452696,
      "grad_norm": 37.97139358520508,
      "learning_rate": 2.2634791454730417e-06,
      "loss": 1.5276,
      "step": 93850
    },
    {
      "epoch": 47.741607324516785,
      "grad_norm": 36.89325714111328,
      "learning_rate": 2.2583926754832146e-06,
      "loss": 1.4721,
      "step": 93860
    },
    {
      "epoch": 47.74669379450661,
      "grad_norm": 41.57592010498047,
      "learning_rate": 2.253306205493388e-06,
      "loss": 1.5813,
      "step": 93870
    },
    {
      "epoch": 47.75178026449644,
      "grad_norm": 41.762794494628906,
      "learning_rate": 2.2482197355035607e-06,
      "loss": 1.5301,
      "step": 93880
    },
    {
      "epoch": 47.756866734486266,
      "grad_norm": 42.01737976074219,
      "learning_rate": 2.2431332655137336e-06,
      "loss": 1.5426,
      "step": 93890
    },
    {
      "epoch": 47.76195320447609,
      "grad_norm": 46.39301681518555,
      "learning_rate": 2.2380467955239064e-06,
      "loss": 1.4405,
      "step": 93900
    },
    {
      "epoch": 47.76703967446592,
      "grad_norm": 32.7653923034668,
      "learning_rate": 2.2329603255340793e-06,
      "loss": 1.4837,
      "step": 93910
    },
    {
      "epoch": 47.77212614445575,
      "grad_norm": 34.31373977661133,
      "learning_rate": 2.227873855544252e-06,
      "loss": 1.4919,
      "step": 93920
    },
    {
      "epoch": 47.777212614445574,
      "grad_norm": 39.979801177978516,
      "learning_rate": 2.2227873855544254e-06,
      "loss": 1.4801,
      "step": 93930
    },
    {
      "epoch": 47.7822990844354,
      "grad_norm": 42.972782135009766,
      "learning_rate": 2.2177009155645983e-06,
      "loss": 1.5269,
      "step": 93940
    },
    {
      "epoch": 47.78738555442523,
      "grad_norm": 47.252685546875,
      "learning_rate": 2.212614445574771e-06,
      "loss": 1.5174,
      "step": 93950
    },
    {
      "epoch": 47.792472024415055,
      "grad_norm": 46.998287200927734,
      "learning_rate": 2.2075279755849444e-06,
      "loss": 1.5257,
      "step": 93960
    },
    {
      "epoch": 47.79755849440488,
      "grad_norm": 50.74472427368164,
      "learning_rate": 2.202441505595117e-06,
      "loss": 1.569,
      "step": 93970
    },
    {
      "epoch": 47.80264496439471,
      "grad_norm": 43.876766204833984,
      "learning_rate": 2.19735503560529e-06,
      "loss": 1.5573,
      "step": 93980
    },
    {
      "epoch": 47.807731434384536,
      "grad_norm": 38.70625686645508,
      "learning_rate": 2.192268565615463e-06,
      "loss": 1.5128,
      "step": 93990
    },
    {
      "epoch": 47.81281790437436,
      "grad_norm": 46.32863235473633,
      "learning_rate": 2.187182095625636e-06,
      "loss": 1.5297,
      "step": 94000
    },
    {
      "epoch": 47.81790437436419,
      "grad_norm": 42.81539535522461,
      "learning_rate": 2.1820956256358087e-06,
      "loss": 1.4776,
      "step": 94010
    },
    {
      "epoch": 47.82299084435402,
      "grad_norm": 49.67715835571289,
      "learning_rate": 2.177009155645982e-06,
      "loss": 1.4768,
      "step": 94020
    },
    {
      "epoch": 47.828077314343844,
      "grad_norm": 41.88240051269531,
      "learning_rate": 2.1719226856561544e-06,
      "loss": 1.4948,
      "step": 94030
    },
    {
      "epoch": 47.83316378433367,
      "grad_norm": 40.52986145019531,
      "learning_rate": 2.1668362156663277e-06,
      "loss": 1.5182,
      "step": 94040
    },
    {
      "epoch": 47.8382502543235,
      "grad_norm": 43.1014289855957,
      "learning_rate": 2.1617497456765006e-06,
      "loss": 1.4944,
      "step": 94050
    },
    {
      "epoch": 47.843336724313325,
      "grad_norm": 38.839630126953125,
      "learning_rate": 2.1566632756866734e-06,
      "loss": 1.4223,
      "step": 94060
    },
    {
      "epoch": 47.84842319430315,
      "grad_norm": 33.25129699707031,
      "learning_rate": 2.1515768056968463e-06,
      "loss": 1.5237,
      "step": 94070
    },
    {
      "epoch": 47.85350966429298,
      "grad_norm": 53.56483840942383,
      "learning_rate": 2.1464903357070196e-06,
      "loss": 1.4901,
      "step": 94080
    },
    {
      "epoch": 47.858596134282806,
      "grad_norm": 39.12882614135742,
      "learning_rate": 2.1414038657171924e-06,
      "loss": 1.5636,
      "step": 94090
    },
    {
      "epoch": 47.86368260427263,
      "grad_norm": 36.119808197021484,
      "learning_rate": 2.1363173957273653e-06,
      "loss": 1.4548,
      "step": 94100
    },
    {
      "epoch": 47.86876907426246,
      "grad_norm": 53.82025146484375,
      "learning_rate": 2.1312309257375386e-06,
      "loss": 1.5168,
      "step": 94110
    },
    {
      "epoch": 47.87385554425229,
      "grad_norm": 35.87590026855469,
      "learning_rate": 2.126144455747711e-06,
      "loss": 1.4797,
      "step": 94120
    },
    {
      "epoch": 47.878942014242114,
      "grad_norm": 45.252681732177734,
      "learning_rate": 2.1210579857578843e-06,
      "loss": 1.5227,
      "step": 94130
    },
    {
      "epoch": 47.88402848423194,
      "grad_norm": 38.537818908691406,
      "learning_rate": 2.115971515768057e-06,
      "loss": 1.4712,
      "step": 94140
    },
    {
      "epoch": 47.88911495422177,
      "grad_norm": 44.44694137573242,
      "learning_rate": 2.11088504577823e-06,
      "loss": 1.5051,
      "step": 94150
    },
    {
      "epoch": 47.894201424211595,
      "grad_norm": 32.96002960205078,
      "learning_rate": 2.105798575788403e-06,
      "loss": 1.5086,
      "step": 94160
    },
    {
      "epoch": 47.89928789420142,
      "grad_norm": 46.18561935424805,
      "learning_rate": 2.100712105798576e-06,
      "loss": 1.5578,
      "step": 94170
    },
    {
      "epoch": 47.90437436419125,
      "grad_norm": 45.52171325683594,
      "learning_rate": 2.0956256358087486e-06,
      "loss": 1.4978,
      "step": 94180
    },
    {
      "epoch": 47.909460834181075,
      "grad_norm": 36.26266098022461,
      "learning_rate": 2.090539165818922e-06,
      "loss": 1.4935,
      "step": 94190
    },
    {
      "epoch": 47.9145473041709,
      "grad_norm": 49.956207275390625,
      "learning_rate": 2.0854526958290947e-06,
      "loss": 1.4364,
      "step": 94200
    },
    {
      "epoch": 47.91963377416073,
      "grad_norm": 36.7790641784668,
      "learning_rate": 2.0803662258392676e-06,
      "loss": 1.4529,
      "step": 94210
    },
    {
      "epoch": 47.924720244150556,
      "grad_norm": 38.970420837402344,
      "learning_rate": 2.0752797558494404e-06,
      "loss": 1.493,
      "step": 94220
    },
    {
      "epoch": 47.92980671414038,
      "grad_norm": 56.83871078491211,
      "learning_rate": 2.0701932858596137e-06,
      "loss": 1.4628,
      "step": 94230
    },
    {
      "epoch": 47.93489318413021,
      "grad_norm": 40.94285583496094,
      "learning_rate": 2.065106815869786e-06,
      "loss": 1.557,
      "step": 94240
    },
    {
      "epoch": 47.93997965412004,
      "grad_norm": 43.15511703491211,
      "learning_rate": 2.0600203458799594e-06,
      "loss": 1.4649,
      "step": 94250
    },
    {
      "epoch": 47.945066124109864,
      "grad_norm": 41.44207763671875,
      "learning_rate": 2.0549338758901327e-06,
      "loss": 1.5503,
      "step": 94260
    },
    {
      "epoch": 47.95015259409969,
      "grad_norm": 31.860437393188477,
      "learning_rate": 2.049847405900305e-06,
      "loss": 1.4358,
      "step": 94270
    },
    {
      "epoch": 47.955239064089525,
      "grad_norm": 35.96294403076172,
      "learning_rate": 2.0447609359104784e-06,
      "loss": 1.4728,
      "step": 94280
    },
    {
      "epoch": 47.96032553407935,
      "grad_norm": 41.752037048339844,
      "learning_rate": 2.0396744659206513e-06,
      "loss": 1.378,
      "step": 94290
    },
    {
      "epoch": 47.96541200406918,
      "grad_norm": 50.60121536254883,
      "learning_rate": 2.034587995930824e-06,
      "loss": 1.4909,
      "step": 94300
    },
    {
      "epoch": 47.970498474059006,
      "grad_norm": 36.817474365234375,
      "learning_rate": 2.029501525940997e-06,
      "loss": 1.5766,
      "step": 94310
    },
    {
      "epoch": 47.97558494404883,
      "grad_norm": 36.190696716308594,
      "learning_rate": 2.0244150559511703e-06,
      "loss": 1.4123,
      "step": 94320
    },
    {
      "epoch": 47.98067141403866,
      "grad_norm": 50.40776443481445,
      "learning_rate": 2.0193285859613427e-06,
      "loss": 1.59,
      "step": 94330
    },
    {
      "epoch": 47.98575788402849,
      "grad_norm": 37.0706901550293,
      "learning_rate": 2.014242115971516e-06,
      "loss": 1.5395,
      "step": 94340
    },
    {
      "epoch": 47.990844354018314,
      "grad_norm": 52.7075080871582,
      "learning_rate": 2.009155645981689e-06,
      "loss": 1.547,
      "step": 94350
    },
    {
      "epoch": 47.99593082400814,
      "grad_norm": 34.745758056640625,
      "learning_rate": 2.0040691759918617e-06,
      "loss": 1.5732,
      "step": 94360
    },
    {
      "epoch": 48.0,
      "eval_loss": 5.127310276031494,
      "eval_runtime": 2.6379,
      "eval_samples_per_second": 1051.956,
      "eval_steps_per_second": 131.542,
      "step": 94368
    },
    {
      "epoch": 48.00101729399797,
      "grad_norm": 37.744014739990234,
      "learning_rate": 1.9989827060020346e-06,
      "loss": 1.4654,
      "step": 94370
    },
    {
      "epoch": 48.006103763987795,
      "grad_norm": 49.93959045410156,
      "learning_rate": 1.993896236012208e-06,
      "loss": 1.4656,
      "step": 94380
    },
    {
      "epoch": 48.01119023397762,
      "grad_norm": 45.943756103515625,
      "learning_rate": 1.9888097660223803e-06,
      "loss": 1.5182,
      "step": 94390
    },
    {
      "epoch": 48.01627670396745,
      "grad_norm": 43.4254264831543,
      "learning_rate": 1.9837232960325536e-06,
      "loss": 1.4303,
      "step": 94400
    },
    {
      "epoch": 48.021363173957276,
      "grad_norm": 42.4273567199707,
      "learning_rate": 1.9786368260427264e-06,
      "loss": 1.439,
      "step": 94410
    },
    {
      "epoch": 48.0264496439471,
      "grad_norm": 32.1690559387207,
      "learning_rate": 1.9735503560528993e-06,
      "loss": 1.4325,
      "step": 94420
    },
    {
      "epoch": 48.03153611393693,
      "grad_norm": 51.60405349731445,
      "learning_rate": 1.9684638860630726e-06,
      "loss": 1.5101,
      "step": 94430
    },
    {
      "epoch": 48.03662258392676,
      "grad_norm": 45.56248092651367,
      "learning_rate": 1.9633774160732454e-06,
      "loss": 1.436,
      "step": 94440
    },
    {
      "epoch": 48.041709053916584,
      "grad_norm": 42.62778091430664,
      "learning_rate": 1.9582909460834183e-06,
      "loss": 1.5023,
      "step": 94450
    },
    {
      "epoch": 48.04679552390641,
      "grad_norm": 44.39722442626953,
      "learning_rate": 1.953204476093591e-06,
      "loss": 1.4133,
      "step": 94460
    },
    {
      "epoch": 48.05188199389624,
      "grad_norm": 57.74488830566406,
      "learning_rate": 1.948118006103764e-06,
      "loss": 1.5052,
      "step": 94470
    },
    {
      "epoch": 48.056968463886065,
      "grad_norm": 55.570220947265625,
      "learning_rate": 1.943031536113937e-06,
      "loss": 1.472,
      "step": 94480
    },
    {
      "epoch": 48.06205493387589,
      "grad_norm": 47.63843536376953,
      "learning_rate": 1.93794506612411e-06,
      "loss": 1.4192,
      "step": 94490
    },
    {
      "epoch": 48.06714140386572,
      "grad_norm": 38.1879997253418,
      "learning_rate": 1.932858596134283e-06,
      "loss": 1.5411,
      "step": 94500
    },
    {
      "epoch": 48.072227873855546,
      "grad_norm": 40.80592727661133,
      "learning_rate": 1.927772126144456e-06,
      "loss": 1.6277,
      "step": 94510
    },
    {
      "epoch": 48.07731434384537,
      "grad_norm": 36.4716796875,
      "learning_rate": 1.9226856561546287e-06,
      "loss": 1.4131,
      "step": 94520
    },
    {
      "epoch": 48.0824008138352,
      "grad_norm": 42.121524810791016,
      "learning_rate": 1.9175991861648016e-06,
      "loss": 1.5366,
      "step": 94530
    },
    {
      "epoch": 48.08748728382503,
      "grad_norm": 36.256126403808594,
      "learning_rate": 1.9125127161749744e-06,
      "loss": 1.4571,
      "step": 94540
    },
    {
      "epoch": 48.092573753814854,
      "grad_norm": 38.989967346191406,
      "learning_rate": 1.9074262461851477e-06,
      "loss": 1.4667,
      "step": 94550
    },
    {
      "epoch": 48.09766022380468,
      "grad_norm": 41.45096206665039,
      "learning_rate": 1.9023397761953204e-06,
      "loss": 1.4828,
      "step": 94560
    },
    {
      "epoch": 48.10274669379451,
      "grad_norm": 34.50444030761719,
      "learning_rate": 1.8972533062054934e-06,
      "loss": 1.6007,
      "step": 94570
    },
    {
      "epoch": 48.107833163784335,
      "grad_norm": 50.5111198425293,
      "learning_rate": 1.8921668362156665e-06,
      "loss": 1.5488,
      "step": 94580
    },
    {
      "epoch": 48.11291963377416,
      "grad_norm": 44.7133674621582,
      "learning_rate": 1.8870803662258394e-06,
      "loss": 1.5,
      "step": 94590
    },
    {
      "epoch": 48.11800610376399,
      "grad_norm": 57.13365936279297,
      "learning_rate": 1.8819938962360124e-06,
      "loss": 1.4848,
      "step": 94600
    },
    {
      "epoch": 48.123092573753816,
      "grad_norm": 36.37705612182617,
      "learning_rate": 1.8769074262461853e-06,
      "loss": 1.5711,
      "step": 94610
    },
    {
      "epoch": 48.12817904374364,
      "grad_norm": 36.22310256958008,
      "learning_rate": 1.8718209562563583e-06,
      "loss": 1.5441,
      "step": 94620
    },
    {
      "epoch": 48.13326551373347,
      "grad_norm": 44.70492935180664,
      "learning_rate": 1.866734486266531e-06,
      "loss": 1.415,
      "step": 94630
    },
    {
      "epoch": 48.1383519837233,
      "grad_norm": 51.12418746948242,
      "learning_rate": 1.861648016276704e-06,
      "loss": 1.5045,
      "step": 94640
    },
    {
      "epoch": 48.143438453713124,
      "grad_norm": 41.07468795776367,
      "learning_rate": 1.856561546286877e-06,
      "loss": 1.4378,
      "step": 94650
    },
    {
      "epoch": 48.14852492370295,
      "grad_norm": 46.8634147644043,
      "learning_rate": 1.85147507629705e-06,
      "loss": 1.5555,
      "step": 94660
    },
    {
      "epoch": 48.15361139369278,
      "grad_norm": 44.29704284667969,
      "learning_rate": 1.8463886063072229e-06,
      "loss": 1.5005,
      "step": 94670
    },
    {
      "epoch": 48.158697863682605,
      "grad_norm": 42.428680419921875,
      "learning_rate": 1.841302136317396e-06,
      "loss": 1.5609,
      "step": 94680
    },
    {
      "epoch": 48.16378433367243,
      "grad_norm": 43.57793426513672,
      "learning_rate": 1.8362156663275686e-06,
      "loss": 1.5495,
      "step": 94690
    },
    {
      "epoch": 48.16887080366226,
      "grad_norm": 59.30847930908203,
      "learning_rate": 1.8311291963377416e-06,
      "loss": 1.5123,
      "step": 94700
    },
    {
      "epoch": 48.173957273652086,
      "grad_norm": 52.177040100097656,
      "learning_rate": 1.8260427263479145e-06,
      "loss": 1.4554,
      "step": 94710
    },
    {
      "epoch": 48.17904374364191,
      "grad_norm": 36.27828598022461,
      "learning_rate": 1.8209562563580876e-06,
      "loss": 1.5408,
      "step": 94720
    },
    {
      "epoch": 48.18413021363174,
      "grad_norm": 33.98855972290039,
      "learning_rate": 1.8158697863682604e-06,
      "loss": 1.55,
      "step": 94730
    },
    {
      "epoch": 48.18921668362157,
      "grad_norm": 37.95167541503906,
      "learning_rate": 1.8107833163784335e-06,
      "loss": 1.5714,
      "step": 94740
    },
    {
      "epoch": 48.19430315361139,
      "grad_norm": 33.446815490722656,
      "learning_rate": 1.8056968463886066e-06,
      "loss": 1.5843,
      "step": 94750
    },
    {
      "epoch": 48.19938962360122,
      "grad_norm": 39.76475143432617,
      "learning_rate": 1.8006103763987792e-06,
      "loss": 1.4263,
      "step": 94760
    },
    {
      "epoch": 48.20447609359105,
      "grad_norm": 47.47726821899414,
      "learning_rate": 1.7955239064089525e-06,
      "loss": 1.5365,
      "step": 94770
    },
    {
      "epoch": 48.209562563580874,
      "grad_norm": 39.25736999511719,
      "learning_rate": 1.7904374364191251e-06,
      "loss": 1.4654,
      "step": 94780
    },
    {
      "epoch": 48.2146490335707,
      "grad_norm": 32.4736328125,
      "learning_rate": 1.7853509664292982e-06,
      "loss": 1.5117,
      "step": 94790
    },
    {
      "epoch": 48.21973550356053,
      "grad_norm": 60.63739776611328,
      "learning_rate": 1.780264496439471e-06,
      "loss": 1.5101,
      "step": 94800
    },
    {
      "epoch": 48.224821973550355,
      "grad_norm": 51.83326721191406,
      "learning_rate": 1.7751780264496441e-06,
      "loss": 1.4942,
      "step": 94810
    },
    {
      "epoch": 48.22990844354018,
      "grad_norm": 42.25494384765625,
      "learning_rate": 1.7700915564598168e-06,
      "loss": 1.511,
      "step": 94820
    },
    {
      "epoch": 48.23499491353001,
      "grad_norm": 39.299434661865234,
      "learning_rate": 1.76500508646999e-06,
      "loss": 1.5486,
      "step": 94830
    },
    {
      "epoch": 48.240081383519836,
      "grad_norm": 46.12508010864258,
      "learning_rate": 1.7599186164801627e-06,
      "loss": 1.4471,
      "step": 94840
    },
    {
      "epoch": 48.24516785350966,
      "grad_norm": 49.31126403808594,
      "learning_rate": 1.7548321464903358e-06,
      "loss": 1.4569,
      "step": 94850
    },
    {
      "epoch": 48.25025432349949,
      "grad_norm": 48.96062088012695,
      "learning_rate": 1.7497456765005086e-06,
      "loss": 1.512,
      "step": 94860
    },
    {
      "epoch": 48.25534079348932,
      "grad_norm": 44.25242233276367,
      "learning_rate": 1.7446592065106817e-06,
      "loss": 1.5092,
      "step": 94870
    },
    {
      "epoch": 48.260427263479144,
      "grad_norm": 41.92031478881836,
      "learning_rate": 1.7395727365208544e-06,
      "loss": 1.6031,
      "step": 94880
    },
    {
      "epoch": 48.26551373346897,
      "grad_norm": 40.22431564331055,
      "learning_rate": 1.7344862665310276e-06,
      "loss": 1.4384,
      "step": 94890
    },
    {
      "epoch": 48.2706002034588,
      "grad_norm": 36.017738342285156,
      "learning_rate": 1.7293997965412007e-06,
      "loss": 1.4691,
      "step": 94900
    },
    {
      "epoch": 48.275686673448625,
      "grad_norm": 37.57862854003906,
      "learning_rate": 1.7243133265513734e-06,
      "loss": 1.4824,
      "step": 94910
    },
    {
      "epoch": 48.28077314343845,
      "grad_norm": 62.260379791259766,
      "learning_rate": 1.7192268565615464e-06,
      "loss": 1.4878,
      "step": 94920
    },
    {
      "epoch": 48.28585961342828,
      "grad_norm": 48.321590423583984,
      "learning_rate": 1.7141403865717193e-06,
      "loss": 1.4366,
      "step": 94930
    },
    {
      "epoch": 48.290946083418106,
      "grad_norm": 37.454917907714844,
      "learning_rate": 1.7090539165818923e-06,
      "loss": 1.4817,
      "step": 94940
    },
    {
      "epoch": 48.29603255340793,
      "grad_norm": 46.08327102661133,
      "learning_rate": 1.7039674465920652e-06,
      "loss": 1.5392,
      "step": 94950
    },
    {
      "epoch": 48.30111902339776,
      "grad_norm": 36.81001281738281,
      "learning_rate": 1.6988809766022383e-06,
      "loss": 1.4892,
      "step": 94960
    },
    {
      "epoch": 48.30620549338759,
      "grad_norm": 38.12235641479492,
      "learning_rate": 1.693794506612411e-06,
      "loss": 1.4873,
      "step": 94970
    },
    {
      "epoch": 48.311291963377414,
      "grad_norm": 44.31707763671875,
      "learning_rate": 1.688708036622584e-06,
      "loss": 1.5178,
      "step": 94980
    },
    {
      "epoch": 48.31637843336724,
      "grad_norm": 65.66929626464844,
      "learning_rate": 1.6836215666327569e-06,
      "loss": 1.4848,
      "step": 94990
    },
    {
      "epoch": 48.32146490335707,
      "grad_norm": 35.68056869506836,
      "learning_rate": 1.67853509664293e-06,
      "loss": 1.5946,
      "step": 95000
    },
    {
      "epoch": 48.326551373346895,
      "grad_norm": 43.25088119506836,
      "learning_rate": 1.6734486266531028e-06,
      "loss": 1.4551,
      "step": 95010
    },
    {
      "epoch": 48.33163784333672,
      "grad_norm": 49.64729309082031,
      "learning_rate": 1.6683621566632759e-06,
      "loss": 1.5227,
      "step": 95020
    },
    {
      "epoch": 48.33672431332655,
      "grad_norm": 42.99211502075195,
      "learning_rate": 1.6632756866734485e-06,
      "loss": 1.5126,
      "step": 95030
    },
    {
      "epoch": 48.341810783316376,
      "grad_norm": 44.17335891723633,
      "learning_rate": 1.6581892166836216e-06,
      "loss": 1.52,
      "step": 95040
    },
    {
      "epoch": 48.3468972533062,
      "grad_norm": 46.05385208129883,
      "learning_rate": 1.6531027466937944e-06,
      "loss": 1.4809,
      "step": 95050
    },
    {
      "epoch": 48.35198372329603,
      "grad_norm": 43.79286575317383,
      "learning_rate": 1.6480162767039675e-06,
      "loss": 1.5149,
      "step": 95060
    },
    {
      "epoch": 48.35707019328586,
      "grad_norm": 41.755313873291016,
      "learning_rate": 1.6429298067141406e-06,
      "loss": 1.5194,
      "step": 95070
    },
    {
      "epoch": 48.362156663275684,
      "grad_norm": 39.25022506713867,
      "learning_rate": 1.6378433367243134e-06,
      "loss": 1.4362,
      "step": 95080
    },
    {
      "epoch": 48.36724313326551,
      "grad_norm": 38.20582580566406,
      "learning_rate": 1.6327568667344865e-06,
      "loss": 1.5139,
      "step": 95090
    },
    {
      "epoch": 48.37232960325534,
      "grad_norm": 45.46407699584961,
      "learning_rate": 1.6276703967446591e-06,
      "loss": 1.4081,
      "step": 95100
    },
    {
      "epoch": 48.377416073245165,
      "grad_norm": 42.36026382446289,
      "learning_rate": 1.6225839267548324e-06,
      "loss": 1.5102,
      "step": 95110
    },
    {
      "epoch": 48.38250254323499,
      "grad_norm": 40.142032623291016,
      "learning_rate": 1.617497456765005e-06,
      "loss": 1.4875,
      "step": 95120
    },
    {
      "epoch": 48.38758901322482,
      "grad_norm": 44.362117767333984,
      "learning_rate": 1.6124109867751781e-06,
      "loss": 1.4809,
      "step": 95130
    },
    {
      "epoch": 48.392675483214646,
      "grad_norm": 45.72627639770508,
      "learning_rate": 1.607324516785351e-06,
      "loss": 1.5113,
      "step": 95140
    },
    {
      "epoch": 48.39776195320447,
      "grad_norm": 35.31900405883789,
      "learning_rate": 1.602238046795524e-06,
      "loss": 1.4735,
      "step": 95150
    },
    {
      "epoch": 48.4028484231943,
      "grad_norm": 38.04146194458008,
      "learning_rate": 1.5971515768056967e-06,
      "loss": 1.5777,
      "step": 95160
    },
    {
      "epoch": 48.407934893184134,
      "grad_norm": 37.69730758666992,
      "learning_rate": 1.59206510681587e-06,
      "loss": 1.4017,
      "step": 95170
    },
    {
      "epoch": 48.41302136317396,
      "grad_norm": 44.484893798828125,
      "learning_rate": 1.5869786368260426e-06,
      "loss": 1.4648,
      "step": 95180
    },
    {
      "epoch": 48.41810783316379,
      "grad_norm": 34.836727142333984,
      "learning_rate": 1.5818921668362157e-06,
      "loss": 1.44,
      "step": 95190
    },
    {
      "epoch": 48.423194303153615,
      "grad_norm": 36.685665130615234,
      "learning_rate": 1.5768056968463886e-06,
      "loss": 1.4873,
      "step": 95200
    },
    {
      "epoch": 48.42828077314344,
      "grad_norm": 58.75645065307617,
      "learning_rate": 1.5717192268565616e-06,
      "loss": 1.5309,
      "step": 95210
    },
    {
      "epoch": 48.43336724313327,
      "grad_norm": 49.267906188964844,
      "learning_rate": 1.5666327568667347e-06,
      "loss": 1.4976,
      "step": 95220
    },
    {
      "epoch": 48.438453713123096,
      "grad_norm": 35.297767639160156,
      "learning_rate": 1.5615462868769076e-06,
      "loss": 1.5522,
      "step": 95230
    },
    {
      "epoch": 48.44354018311292,
      "grad_norm": 49.800376892089844,
      "learning_rate": 1.5564598168870804e-06,
      "loss": 1.5366,
      "step": 95240
    },
    {
      "epoch": 48.44862665310275,
      "grad_norm": 45.20812225341797,
      "learning_rate": 1.5513733468972533e-06,
      "loss": 1.4663,
      "step": 95250
    },
    {
      "epoch": 48.45371312309258,
      "grad_norm": 46.870792388916016,
      "learning_rate": 1.5462868769074264e-06,
      "loss": 1.5065,
      "step": 95260
    },
    {
      "epoch": 48.458799593082404,
      "grad_norm": 36.27977752685547,
      "learning_rate": 1.5412004069175992e-06,
      "loss": 1.5305,
      "step": 95270
    },
    {
      "epoch": 48.46388606307223,
      "grad_norm": 40.905860900878906,
      "learning_rate": 1.536113936927772e-06,
      "loss": 1.5618,
      "step": 95280
    },
    {
      "epoch": 48.46897253306206,
      "grad_norm": 35.910762786865234,
      "learning_rate": 1.5310274669379451e-06,
      "loss": 1.4784,
      "step": 95290
    },
    {
      "epoch": 48.474059003051885,
      "grad_norm": 39.21137619018555,
      "learning_rate": 1.5259409969481182e-06,
      "loss": 1.4802,
      "step": 95300
    },
    {
      "epoch": 48.47914547304171,
      "grad_norm": 38.115753173828125,
      "learning_rate": 1.520854526958291e-06,
      "loss": 1.4911,
      "step": 95310
    },
    {
      "epoch": 48.48423194303154,
      "grad_norm": 48.978816986083984,
      "learning_rate": 1.515768056968464e-06,
      "loss": 1.5598,
      "step": 95320
    },
    {
      "epoch": 48.489318413021365,
      "grad_norm": 43.290260314941406,
      "learning_rate": 1.510681586978637e-06,
      "loss": 1.5603,
      "step": 95330
    },
    {
      "epoch": 48.49440488301119,
      "grad_norm": 40.90224838256836,
      "learning_rate": 1.5055951169888099e-06,
      "loss": 1.422,
      "step": 95340
    },
    {
      "epoch": 48.49949135300102,
      "grad_norm": 50.882530212402344,
      "learning_rate": 1.5005086469989827e-06,
      "loss": 1.5403,
      "step": 95350
    },
    {
      "epoch": 48.504577822990846,
      "grad_norm": 39.0517463684082,
      "learning_rate": 1.4954221770091558e-06,
      "loss": 1.4663,
      "step": 95360
    },
    {
      "epoch": 48.50966429298067,
      "grad_norm": 36.32625961303711,
      "learning_rate": 1.4903357070193286e-06,
      "loss": 1.5454,
      "step": 95370
    },
    {
      "epoch": 48.5147507629705,
      "grad_norm": 42.43733215332031,
      "learning_rate": 1.4852492370295015e-06,
      "loss": 1.4878,
      "step": 95380
    },
    {
      "epoch": 48.51983723296033,
      "grad_norm": 42.09489822387695,
      "learning_rate": 1.4801627670396746e-06,
      "loss": 1.5034,
      "step": 95390
    },
    {
      "epoch": 48.524923702950154,
      "grad_norm": 42.964988708496094,
      "learning_rate": 1.4750762970498474e-06,
      "loss": 1.5587,
      "step": 95400
    },
    {
      "epoch": 48.53001017293998,
      "grad_norm": 35.31109619140625,
      "learning_rate": 1.4699898270600203e-06,
      "loss": 1.5715,
      "step": 95410
    },
    {
      "epoch": 48.53509664292981,
      "grad_norm": 37.13463592529297,
      "learning_rate": 1.4649033570701934e-06,
      "loss": 1.4774,
      "step": 95420
    },
    {
      "epoch": 48.540183112919635,
      "grad_norm": 36.99674606323242,
      "learning_rate": 1.4598168870803662e-06,
      "loss": 1.4313,
      "step": 95430
    },
    {
      "epoch": 48.54526958290946,
      "grad_norm": 40.20819091796875,
      "learning_rate": 1.454730417090539e-06,
      "loss": 1.4577,
      "step": 95440
    },
    {
      "epoch": 48.55035605289929,
      "grad_norm": 42.62046432495117,
      "learning_rate": 1.4496439471007121e-06,
      "loss": 1.4527,
      "step": 95450
    },
    {
      "epoch": 48.555442522889116,
      "grad_norm": 42.74146270751953,
      "learning_rate": 1.444557477110885e-06,
      "loss": 1.5622,
      "step": 95460
    },
    {
      "epoch": 48.56052899287894,
      "grad_norm": 41.29769515991211,
      "learning_rate": 1.439471007121058e-06,
      "loss": 1.4163,
      "step": 95470
    },
    {
      "epoch": 48.56561546286877,
      "grad_norm": 38.6207160949707,
      "learning_rate": 1.4343845371312311e-06,
      "loss": 1.5407,
      "step": 95480
    },
    {
      "epoch": 48.5707019328586,
      "grad_norm": 33.008853912353516,
      "learning_rate": 1.429298067141404e-06,
      "loss": 1.5514,
      "step": 95490
    },
    {
      "epoch": 48.575788402848424,
      "grad_norm": 48.41147994995117,
      "learning_rate": 1.4242115971515769e-06,
      "loss": 1.5976,
      "step": 95500
    },
    {
      "epoch": 48.58087487283825,
      "grad_norm": 43.51947021484375,
      "learning_rate": 1.41912512716175e-06,
      "loss": 1.5077,
      "step": 95510
    },
    {
      "epoch": 48.58596134282808,
      "grad_norm": 45.62651443481445,
      "learning_rate": 1.4140386571719228e-06,
      "loss": 1.4792,
      "step": 95520
    },
    {
      "epoch": 48.591047812817905,
      "grad_norm": 44.116031646728516,
      "learning_rate": 1.4089521871820956e-06,
      "loss": 1.5629,
      "step": 95530
    },
    {
      "epoch": 48.59613428280773,
      "grad_norm": 44.769309997558594,
      "learning_rate": 1.4038657171922687e-06,
      "loss": 1.4473,
      "step": 95540
    },
    {
      "epoch": 48.60122075279756,
      "grad_norm": 40.23503494262695,
      "learning_rate": 1.3987792472024416e-06,
      "loss": 1.5121,
      "step": 95550
    },
    {
      "epoch": 48.606307222787386,
      "grad_norm": 34.361549377441406,
      "learning_rate": 1.3936927772126144e-06,
      "loss": 1.4461,
      "step": 95560
    },
    {
      "epoch": 48.61139369277721,
      "grad_norm": 37.83910369873047,
      "learning_rate": 1.3886063072227875e-06,
      "loss": 1.512,
      "step": 95570
    },
    {
      "epoch": 48.61648016276704,
      "grad_norm": 46.00741195678711,
      "learning_rate": 1.3835198372329604e-06,
      "loss": 1.4606,
      "step": 95580
    },
    {
      "epoch": 48.62156663275687,
      "grad_norm": 36.118648529052734,
      "learning_rate": 1.3784333672431332e-06,
      "loss": 1.5119,
      "step": 95590
    },
    {
      "epoch": 48.626653102746694,
      "grad_norm": 46.15801239013672,
      "learning_rate": 1.3733468972533063e-06,
      "loss": 1.4529,
      "step": 95600
    },
    {
      "epoch": 48.63173957273652,
      "grad_norm": 36.186275482177734,
      "learning_rate": 1.3682604272634791e-06,
      "loss": 1.4632,
      "step": 95610
    },
    {
      "epoch": 48.63682604272635,
      "grad_norm": 44.252479553222656,
      "learning_rate": 1.363173957273652e-06,
      "loss": 1.4667,
      "step": 95620
    },
    {
      "epoch": 48.641912512716175,
      "grad_norm": 40.0183219909668,
      "learning_rate": 1.358087487283825e-06,
      "loss": 1.4918,
      "step": 95630
    },
    {
      "epoch": 48.646998982706,
      "grad_norm": 38.683738708496094,
      "learning_rate": 1.3530010172939981e-06,
      "loss": 1.5892,
      "step": 95640
    },
    {
      "epoch": 48.65208545269583,
      "grad_norm": 42.90885925292969,
      "learning_rate": 1.347914547304171e-06,
      "loss": 1.4575,
      "step": 95650
    },
    {
      "epoch": 48.657171922685656,
      "grad_norm": 37.31248474121094,
      "learning_rate": 1.3428280773143439e-06,
      "loss": 1.5001,
      "step": 95660
    },
    {
      "epoch": 48.66225839267548,
      "grad_norm": 43.36505126953125,
      "learning_rate": 1.337741607324517e-06,
      "loss": 1.4832,
      "step": 95670
    },
    {
      "epoch": 48.66734486266531,
      "grad_norm": 44.139739990234375,
      "learning_rate": 1.3326551373346898e-06,
      "loss": 1.5522,
      "step": 95680
    },
    {
      "epoch": 48.67243133265514,
      "grad_norm": 40.261505126953125,
      "learning_rate": 1.3275686673448626e-06,
      "loss": 1.5424,
      "step": 95690
    },
    {
      "epoch": 48.677517802644964,
      "grad_norm": 41.148460388183594,
      "learning_rate": 1.3224821973550357e-06,
      "loss": 1.5291,
      "step": 95700
    },
    {
      "epoch": 48.68260427263479,
      "grad_norm": 39.134483337402344,
      "learning_rate": 1.3173957273652086e-06,
      "loss": 1.6429,
      "step": 95710
    },
    {
      "epoch": 48.68769074262462,
      "grad_norm": 39.11899185180664,
      "learning_rate": 1.3123092573753816e-06,
      "loss": 1.4238,
      "step": 95720
    },
    {
      "epoch": 48.692777212614445,
      "grad_norm": 47.10459518432617,
      "learning_rate": 1.3072227873855545e-06,
      "loss": 1.5642,
      "step": 95730
    },
    {
      "epoch": 48.69786368260427,
      "grad_norm": 33.371925354003906,
      "learning_rate": 1.3021363173957274e-06,
      "loss": 1.4045,
      "step": 95740
    },
    {
      "epoch": 48.7029501525941,
      "grad_norm": 38.06630325317383,
      "learning_rate": 1.2970498474059004e-06,
      "loss": 1.472,
      "step": 95750
    },
    {
      "epoch": 48.708036622583926,
      "grad_norm": 43.11213684082031,
      "learning_rate": 1.2919633774160733e-06,
      "loss": 1.4161,
      "step": 95760
    },
    {
      "epoch": 48.71312309257375,
      "grad_norm": 38.37006759643555,
      "learning_rate": 1.2868769074262461e-06,
      "loss": 1.5242,
      "step": 95770
    },
    {
      "epoch": 48.71820956256358,
      "grad_norm": 40.16234588623047,
      "learning_rate": 1.2817904374364192e-06,
      "loss": 1.6264,
      "step": 95780
    },
    {
      "epoch": 48.72329603255341,
      "grad_norm": 48.85588836669922,
      "learning_rate": 1.2767039674465923e-06,
      "loss": 1.4643,
      "step": 95790
    },
    {
      "epoch": 48.728382502543234,
      "grad_norm": 37.61814498901367,
      "learning_rate": 1.2716174974567651e-06,
      "loss": 1.5064,
      "step": 95800
    },
    {
      "epoch": 48.73346897253306,
      "grad_norm": 40.52140808105469,
      "learning_rate": 1.266531027466938e-06,
      "loss": 1.5456,
      "step": 95810
    },
    {
      "epoch": 48.73855544252289,
      "grad_norm": 50.667724609375,
      "learning_rate": 1.261444557477111e-06,
      "loss": 1.4973,
      "step": 95820
    },
    {
      "epoch": 48.743641912512714,
      "grad_norm": 39.11651611328125,
      "learning_rate": 1.256358087487284e-06,
      "loss": 1.5155,
      "step": 95830
    },
    {
      "epoch": 48.74872838250254,
      "grad_norm": 45.87966537475586,
      "learning_rate": 1.2512716174974568e-06,
      "loss": 1.5585,
      "step": 95840
    },
    {
      "epoch": 48.75381485249237,
      "grad_norm": 51.49262619018555,
      "learning_rate": 1.2461851475076299e-06,
      "loss": 1.5085,
      "step": 95850
    },
    {
      "epoch": 48.758901322482195,
      "grad_norm": 40.05573272705078,
      "learning_rate": 1.2410986775178027e-06,
      "loss": 1.4929,
      "step": 95860
    },
    {
      "epoch": 48.76398779247202,
      "grad_norm": 37.183616638183594,
      "learning_rate": 1.2360122075279756e-06,
      "loss": 1.5268,
      "step": 95870
    },
    {
      "epoch": 48.76907426246185,
      "grad_norm": 30.06068992614746,
      "learning_rate": 1.2309257375381486e-06,
      "loss": 1.5238,
      "step": 95880
    },
    {
      "epoch": 48.774160732451676,
      "grad_norm": 40.4029655456543,
      "learning_rate": 1.2258392675483215e-06,
      "loss": 1.5378,
      "step": 95890
    },
    {
      "epoch": 48.7792472024415,
      "grad_norm": 40.3610725402832,
      "learning_rate": 1.2207527975584944e-06,
      "loss": 1.5197,
      "step": 95900
    },
    {
      "epoch": 48.78433367243133,
      "grad_norm": 49.37360382080078,
      "learning_rate": 1.2156663275686674e-06,
      "loss": 1.4938,
      "step": 95910
    },
    {
      "epoch": 48.78942014242116,
      "grad_norm": 42.400962829589844,
      "learning_rate": 1.2105798575788403e-06,
      "loss": 1.4614,
      "step": 95920
    },
    {
      "epoch": 48.794506612410984,
      "grad_norm": 31.70199966430664,
      "learning_rate": 1.2054933875890131e-06,
      "loss": 1.4369,
      "step": 95930
    },
    {
      "epoch": 48.79959308240081,
      "grad_norm": 50.656246185302734,
      "learning_rate": 1.2004069175991862e-06,
      "loss": 1.5017,
      "step": 95940
    },
    {
      "epoch": 48.80467955239064,
      "grad_norm": 55.2496452331543,
      "learning_rate": 1.1953204476093593e-06,
      "loss": 1.5037,
      "step": 95950
    },
    {
      "epoch": 48.809766022380465,
      "grad_norm": 38.10850524902344,
      "learning_rate": 1.1902339776195321e-06,
      "loss": 1.4672,
      "step": 95960
    },
    {
      "epoch": 48.81485249237029,
      "grad_norm": 51.82173538208008,
      "learning_rate": 1.1851475076297052e-06,
      "loss": 1.5361,
      "step": 95970
    },
    {
      "epoch": 48.81993896236012,
      "grad_norm": 37.27944564819336,
      "learning_rate": 1.180061037639878e-06,
      "loss": 1.4406,
      "step": 95980
    },
    {
      "epoch": 48.825025432349946,
      "grad_norm": 60.195308685302734,
      "learning_rate": 1.174974567650051e-06,
      "loss": 1.4978,
      "step": 95990
    },
    {
      "epoch": 48.83011190233977,
      "grad_norm": 41.74873352050781,
      "learning_rate": 1.169888097660224e-06,
      "loss": 1.4987,
      "step": 96000
    },
    {
      "epoch": 48.8351983723296,
      "grad_norm": 43.898475646972656,
      "learning_rate": 1.1648016276703969e-06,
      "loss": 1.5619,
      "step": 96010
    },
    {
      "epoch": 48.84028484231943,
      "grad_norm": 38.930633544921875,
      "learning_rate": 1.1597151576805697e-06,
      "loss": 1.4483,
      "step": 96020
    },
    {
      "epoch": 48.845371312309254,
      "grad_norm": 36.3786735534668,
      "learning_rate": 1.1546286876907428e-06,
      "loss": 1.4989,
      "step": 96030
    },
    {
      "epoch": 48.85045778229908,
      "grad_norm": 43.98191833496094,
      "learning_rate": 1.1495422177009156e-06,
      "loss": 1.4597,
      "step": 96040
    },
    {
      "epoch": 48.85554425228891,
      "grad_norm": 38.16032028198242,
      "learning_rate": 1.1444557477110885e-06,
      "loss": 1.5045,
      "step": 96050
    },
    {
      "epoch": 48.86063072227874,
      "grad_norm": 44.910980224609375,
      "learning_rate": 1.1393692777212616e-06,
      "loss": 1.5257,
      "step": 96060
    },
    {
      "epoch": 48.86571719226856,
      "grad_norm": 61.19301986694336,
      "learning_rate": 1.1342828077314344e-06,
      "loss": 1.4419,
      "step": 96070
    },
    {
      "epoch": 48.870803662258396,
      "grad_norm": 34.7769660949707,
      "learning_rate": 1.1291963377416073e-06,
      "loss": 1.5511,
      "step": 96080
    },
    {
      "epoch": 48.87589013224822,
      "grad_norm": 38.14912796020508,
      "learning_rate": 1.1241098677517804e-06,
      "loss": 1.5123,
      "step": 96090
    },
    {
      "epoch": 48.88097660223805,
      "grad_norm": 38.2177619934082,
      "learning_rate": 1.1190233977619532e-06,
      "loss": 1.5231,
      "step": 96100
    },
    {
      "epoch": 48.88606307222788,
      "grad_norm": 46.24162292480469,
      "learning_rate": 1.113936927772126e-06,
      "loss": 1.3926,
      "step": 96110
    },
    {
      "epoch": 48.891149542217704,
      "grad_norm": 40.08623504638672,
      "learning_rate": 1.1088504577822991e-06,
      "loss": 1.5044,
      "step": 96120
    },
    {
      "epoch": 48.89623601220753,
      "grad_norm": 43.11024475097656,
      "learning_rate": 1.1037639877924722e-06,
      "loss": 1.4592,
      "step": 96130
    },
    {
      "epoch": 48.90132248219736,
      "grad_norm": 39.52260208129883,
      "learning_rate": 1.098677517802645e-06,
      "loss": 1.4839,
      "step": 96140
    },
    {
      "epoch": 48.906408952187185,
      "grad_norm": 41.014278411865234,
      "learning_rate": 1.093591047812818e-06,
      "loss": 1.5422,
      "step": 96150
    },
    {
      "epoch": 48.91149542217701,
      "grad_norm": 49.89872741699219,
      "learning_rate": 1.088504577822991e-06,
      "loss": 1.4783,
      "step": 96160
    },
    {
      "epoch": 48.91658189216684,
      "grad_norm": 46.017066955566406,
      "learning_rate": 1.0834181078331639e-06,
      "loss": 1.5178,
      "step": 96170
    },
    {
      "epoch": 48.921668362156666,
      "grad_norm": 37.83931350708008,
      "learning_rate": 1.0783316378433367e-06,
      "loss": 1.5085,
      "step": 96180
    },
    {
      "epoch": 48.92675483214649,
      "grad_norm": 42.89260482788086,
      "learning_rate": 1.0732451678535098e-06,
      "loss": 1.5367,
      "step": 96190
    },
    {
      "epoch": 48.93184130213632,
      "grad_norm": 49.51004409790039,
      "learning_rate": 1.0681586978636826e-06,
      "loss": 1.4695,
      "step": 96200
    },
    {
      "epoch": 48.93692777212615,
      "grad_norm": 38.88998031616211,
      "learning_rate": 1.0630722278738555e-06,
      "loss": 1.5081,
      "step": 96210
    },
    {
      "epoch": 48.942014242115974,
      "grad_norm": 40.603187561035156,
      "learning_rate": 1.0579857578840286e-06,
      "loss": 1.4551,
      "step": 96220
    },
    {
      "epoch": 48.9471007121058,
      "grad_norm": 48.392765045166016,
      "learning_rate": 1.0528992878942014e-06,
      "loss": 1.5385,
      "step": 96230
    },
    {
      "epoch": 48.95218718209563,
      "grad_norm": 35.558528900146484,
      "learning_rate": 1.0478128179043743e-06,
      "loss": 1.5249,
      "step": 96240
    },
    {
      "epoch": 48.957273652085455,
      "grad_norm": 38.79788589477539,
      "learning_rate": 1.0427263479145474e-06,
      "loss": 1.4945,
      "step": 96250
    },
    {
      "epoch": 48.96236012207528,
      "grad_norm": 36.13473129272461,
      "learning_rate": 1.0376398779247202e-06,
      "loss": 1.5052,
      "step": 96260
    },
    {
      "epoch": 48.96744659206511,
      "grad_norm": 40.09693908691406,
      "learning_rate": 1.032553407934893e-06,
      "loss": 1.3906,
      "step": 96270
    },
    {
      "epoch": 48.972533062054936,
      "grad_norm": 36.008514404296875,
      "learning_rate": 1.0274669379450664e-06,
      "loss": 1.4814,
      "step": 96280
    },
    {
      "epoch": 48.97761953204476,
      "grad_norm": 56.33244323730469,
      "learning_rate": 1.0223804679552392e-06,
      "loss": 1.5435,
      "step": 96290
    },
    {
      "epoch": 48.98270600203459,
      "grad_norm": 48.214107513427734,
      "learning_rate": 1.017293997965412e-06,
      "loss": 1.539,
      "step": 96300
    },
    {
      "epoch": 48.98779247202442,
      "grad_norm": 43.88268280029297,
      "learning_rate": 1.0122075279755851e-06,
      "loss": 1.5009,
      "step": 96310
    },
    {
      "epoch": 48.992878942014244,
      "grad_norm": 45.36857223510742,
      "learning_rate": 1.007121057985758e-06,
      "loss": 1.5045,
      "step": 96320
    },
    {
      "epoch": 48.99796541200407,
      "grad_norm": 49.165016174316406,
      "learning_rate": 1.0020345879959309e-06,
      "loss": 1.5382,
      "step": 96330
    },
    {
      "epoch": 49.0,
      "eval_loss": 5.136543273925781,
      "eval_runtime": 2.6564,
      "eval_samples_per_second": 1044.65,
      "eval_steps_per_second": 130.628,
      "step": 96334
    },
    {
      "epoch": 49.0030518819939,
      "grad_norm": 40.17380905151367,
      "learning_rate": 9.96948118006104e-07,
      "loss": 1.4838,
      "step": 96340
    },
    {
      "epoch": 49.008138351983725,
      "grad_norm": 37.60456848144531,
      "learning_rate": 9.918616480162768e-07,
      "loss": 1.4937,
      "step": 96350
    },
    {
      "epoch": 49.01322482197355,
      "grad_norm": 55.30430221557617,
      "learning_rate": 9.867751780264496e-07,
      "loss": 1.4708,
      "step": 96360
    },
    {
      "epoch": 49.01831129196338,
      "grad_norm": 37.661155700683594,
      "learning_rate": 9.816887080366227e-07,
      "loss": 1.4687,
      "step": 96370
    },
    {
      "epoch": 49.023397761953206,
      "grad_norm": 35.554664611816406,
      "learning_rate": 9.766022380467956e-07,
      "loss": 1.4851,
      "step": 96380
    },
    {
      "epoch": 49.02848423194303,
      "grad_norm": 45.31328201293945,
      "learning_rate": 9.715157680569684e-07,
      "loss": 1.532,
      "step": 96390
    },
    {
      "epoch": 49.03357070193286,
      "grad_norm": 46.59274673461914,
      "learning_rate": 9.664292980671415e-07,
      "loss": 1.5475,
      "step": 96400
    },
    {
      "epoch": 49.03865717192269,
      "grad_norm": 44.0249137878418,
      "learning_rate": 9.613428280773144e-07,
      "loss": 1.4304,
      "step": 96410
    },
    {
      "epoch": 49.04374364191251,
      "grad_norm": 38.375343322753906,
      "learning_rate": 9.562563580874872e-07,
      "loss": 1.4932,
      "step": 96420
    },
    {
      "epoch": 49.04883011190234,
      "grad_norm": 43.821983337402344,
      "learning_rate": 9.511698880976602e-07,
      "loss": 1.456,
      "step": 96430
    },
    {
      "epoch": 49.05391658189217,
      "grad_norm": 49.736839294433594,
      "learning_rate": 9.460834181078332e-07,
      "loss": 1.5284,
      "step": 96440
    },
    {
      "epoch": 49.059003051881994,
      "grad_norm": 35.971431732177734,
      "learning_rate": 9.409969481180062e-07,
      "loss": 1.4888,
      "step": 96450
    },
    {
      "epoch": 49.06408952187182,
      "grad_norm": 43.21403503417969,
      "learning_rate": 9.359104781281792e-07,
      "loss": 1.5105,
      "step": 96460
    },
    {
      "epoch": 49.06917599186165,
      "grad_norm": 44.8236198425293,
      "learning_rate": 9.30824008138352e-07,
      "loss": 1.4683,
      "step": 96470
    },
    {
      "epoch": 49.074262461851475,
      "grad_norm": 49.807151794433594,
      "learning_rate": 9.25737538148525e-07,
      "loss": 1.4909,
      "step": 96480
    },
    {
      "epoch": 49.0793489318413,
      "grad_norm": 37.74414825439453,
      "learning_rate": 9.20651068158698e-07,
      "loss": 1.6054,
      "step": 96490
    },
    {
      "epoch": 49.08443540183113,
      "grad_norm": 43.010154724121094,
      "learning_rate": 9.155645981688708e-07,
      "loss": 1.417,
      "step": 96500
    },
    {
      "epoch": 49.089521871820956,
      "grad_norm": 53.57047653198242,
      "learning_rate": 9.104781281790438e-07,
      "loss": 1.4668,
      "step": 96510
    },
    {
      "epoch": 49.09460834181078,
      "grad_norm": 45.696632385253906,
      "learning_rate": 9.053916581892167e-07,
      "loss": 1.5416,
      "step": 96520
    },
    {
      "epoch": 49.09969481180061,
      "grad_norm": 38.87919235229492,
      "learning_rate": 9.003051881993896e-07,
      "loss": 1.3952,
      "step": 96530
    },
    {
      "epoch": 49.10478128179044,
      "grad_norm": 35.27306365966797,
      "learning_rate": 8.952187182095626e-07,
      "loss": 1.4875,
      "step": 96540
    },
    {
      "epoch": 49.109867751780264,
      "grad_norm": 40.31787109375,
      "learning_rate": 8.901322482197355e-07,
      "loss": 1.473,
      "step": 96550
    },
    {
      "epoch": 49.11495422177009,
      "grad_norm": 42.17613983154297,
      "learning_rate": 8.850457782299084e-07,
      "loss": 1.5302,
      "step": 96560
    },
    {
      "epoch": 49.12004069175992,
      "grad_norm": 49.72455596923828,
      "learning_rate": 8.799593082400814e-07,
      "loss": 1.4374,
      "step": 96570
    },
    {
      "epoch": 49.125127161749745,
      "grad_norm": 49.25923538208008,
      "learning_rate": 8.748728382502543e-07,
      "loss": 1.4319,
      "step": 96580
    },
    {
      "epoch": 49.13021363173957,
      "grad_norm": 38.92036819458008,
      "learning_rate": 8.697863682604272e-07,
      "loss": 1.4816,
      "step": 96590
    },
    {
      "epoch": 49.1353001017294,
      "grad_norm": 43.321197509765625,
      "learning_rate": 8.646998982706004e-07,
      "loss": 1.4426,
      "step": 96600
    },
    {
      "epoch": 49.140386571719226,
      "grad_norm": 41.1418342590332,
      "learning_rate": 8.596134282807732e-07,
      "loss": 1.5126,
      "step": 96610
    },
    {
      "epoch": 49.14547304170905,
      "grad_norm": 57.57844543457031,
      "learning_rate": 8.545269582909462e-07,
      "loss": 1.5351,
      "step": 96620
    },
    {
      "epoch": 49.15055951169888,
      "grad_norm": 29.967605590820312,
      "learning_rate": 8.494404883011191e-07,
      "loss": 1.4429,
      "step": 96630
    },
    {
      "epoch": 49.15564598168871,
      "grad_norm": 36.24742126464844,
      "learning_rate": 8.44354018311292e-07,
      "loss": 1.4264,
      "step": 96640
    },
    {
      "epoch": 49.160732451678534,
      "grad_norm": 33.902488708496094,
      "learning_rate": 8.39267548321465e-07,
      "loss": 1.4231,
      "step": 96650
    },
    {
      "epoch": 49.16581892166836,
      "grad_norm": 54.10592269897461,
      "learning_rate": 8.341810783316379e-07,
      "loss": 1.4843,
      "step": 96660
    },
    {
      "epoch": 49.17090539165819,
      "grad_norm": 45.24458312988281,
      "learning_rate": 8.290946083418108e-07,
      "loss": 1.4626,
      "step": 96670
    },
    {
      "epoch": 49.175991861648015,
      "grad_norm": 35.06344223022461,
      "learning_rate": 8.240081383519837e-07,
      "loss": 1.5356,
      "step": 96680
    },
    {
      "epoch": 49.18107833163784,
      "grad_norm": 40.58748245239258,
      "learning_rate": 8.189216683621567e-07,
      "loss": 1.412,
      "step": 96690
    },
    {
      "epoch": 49.18616480162767,
      "grad_norm": 33.93024826049805,
      "learning_rate": 8.138351983723296e-07,
      "loss": 1.5442,
      "step": 96700
    },
    {
      "epoch": 49.191251271617496,
      "grad_norm": 34.45045471191406,
      "learning_rate": 8.087487283825025e-07,
      "loss": 1.4965,
      "step": 96710
    },
    {
      "epoch": 49.19633774160732,
      "grad_norm": 32.87578582763672,
      "learning_rate": 8.036622583926755e-07,
      "loss": 1.5755,
      "step": 96720
    },
    {
      "epoch": 49.20142421159715,
      "grad_norm": 42.14507293701172,
      "learning_rate": 7.985757884028484e-07,
      "loss": 1.4622,
      "step": 96730
    },
    {
      "epoch": 49.20651068158698,
      "grad_norm": 35.32314682006836,
      "learning_rate": 7.934893184130213e-07,
      "loss": 1.522,
      "step": 96740
    },
    {
      "epoch": 49.211597151576804,
      "grad_norm": 46.85531997680664,
      "learning_rate": 7.884028484231943e-07,
      "loss": 1.6335,
      "step": 96750
    },
    {
      "epoch": 49.21668362156663,
      "grad_norm": 31.71630859375,
      "learning_rate": 7.833163784333674e-07,
      "loss": 1.4694,
      "step": 96760
    },
    {
      "epoch": 49.22177009155646,
      "grad_norm": 32.57505416870117,
      "learning_rate": 7.782299084435402e-07,
      "loss": 1.5119,
      "step": 96770
    },
    {
      "epoch": 49.226856561546285,
      "grad_norm": 31.635332107543945,
      "learning_rate": 7.731434384537132e-07,
      "loss": 1.4782,
      "step": 96780
    },
    {
      "epoch": 49.23194303153611,
      "grad_norm": 50.688804626464844,
      "learning_rate": 7.68056968463886e-07,
      "loss": 1.5517,
      "step": 96790
    },
    {
      "epoch": 49.23702950152594,
      "grad_norm": 45.05756378173828,
      "learning_rate": 7.629704984740591e-07,
      "loss": 1.4686,
      "step": 96800
    },
    {
      "epoch": 49.242115971515766,
      "grad_norm": 41.93128967285156,
      "learning_rate": 7.57884028484232e-07,
      "loss": 1.4161,
      "step": 96810
    },
    {
      "epoch": 49.24720244150559,
      "grad_norm": 36.065956115722656,
      "learning_rate": 7.527975584944049e-07,
      "loss": 1.4402,
      "step": 96820
    },
    {
      "epoch": 49.25228891149542,
      "grad_norm": 42.76725769042969,
      "learning_rate": 7.477110885045779e-07,
      "loss": 1.5218,
      "step": 96830
    },
    {
      "epoch": 49.25737538148525,
      "grad_norm": 35.80218505859375,
      "learning_rate": 7.426246185147507e-07,
      "loss": 1.51,
      "step": 96840
    },
    {
      "epoch": 49.262461851475074,
      "grad_norm": 55.16725540161133,
      "learning_rate": 7.375381485249237e-07,
      "loss": 1.4679,
      "step": 96850
    },
    {
      "epoch": 49.2675483214649,
      "grad_norm": 49.37821578979492,
      "learning_rate": 7.324516785350967e-07,
      "loss": 1.5162,
      "step": 96860
    },
    {
      "epoch": 49.27263479145473,
      "grad_norm": 49.658660888671875,
      "learning_rate": 7.273652085452695e-07,
      "loss": 1.5077,
      "step": 96870
    },
    {
      "epoch": 49.277721261444555,
      "grad_norm": 48.945335388183594,
      "learning_rate": 7.222787385554425e-07,
      "loss": 1.4385,
      "step": 96880
    },
    {
      "epoch": 49.28280773143438,
      "grad_norm": 39.09305953979492,
      "learning_rate": 7.171922685656156e-07,
      "loss": 1.4344,
      "step": 96890
    },
    {
      "epoch": 49.28789420142421,
      "grad_norm": 39.56743240356445,
      "learning_rate": 7.121057985757884e-07,
      "loss": 1.4703,
      "step": 96900
    },
    {
      "epoch": 49.292980671414035,
      "grad_norm": 42.213592529296875,
      "learning_rate": 7.070193285859614e-07,
      "loss": 1.4738,
      "step": 96910
    },
    {
      "epoch": 49.29806714140386,
      "grad_norm": 51.97817611694336,
      "learning_rate": 7.019328585961344e-07,
      "loss": 1.4559,
      "step": 96920
    },
    {
      "epoch": 49.30315361139369,
      "grad_norm": 37.97030258178711,
      "learning_rate": 6.968463886063072e-07,
      "loss": 1.4656,
      "step": 96930
    },
    {
      "epoch": 49.308240081383516,
      "grad_norm": 44.35218048095703,
      "learning_rate": 6.917599186164802e-07,
      "loss": 1.5227,
      "step": 96940
    },
    {
      "epoch": 49.31332655137334,
      "grad_norm": 41.9231071472168,
      "learning_rate": 6.866734486266531e-07,
      "loss": 1.5417,
      "step": 96950
    },
    {
      "epoch": 49.31841302136317,
      "grad_norm": 36.55617141723633,
      "learning_rate": 6.81586978636826e-07,
      "loss": 1.4787,
      "step": 96960
    },
    {
      "epoch": 49.323499491353004,
      "grad_norm": 50.694454193115234,
      "learning_rate": 6.765005086469991e-07,
      "loss": 1.542,
      "step": 96970
    },
    {
      "epoch": 49.32858596134283,
      "grad_norm": 52.25084686279297,
      "learning_rate": 6.714140386571719e-07,
      "loss": 1.5262,
      "step": 96980
    },
    {
      "epoch": 49.33367243133266,
      "grad_norm": 39.48771286010742,
      "learning_rate": 6.663275686673449e-07,
      "loss": 1.5633,
      "step": 96990
    },
    {
      "epoch": 49.338758901322485,
      "grad_norm": 34.65407180786133,
      "learning_rate": 6.612410986775179e-07,
      "loss": 1.4758,
      "step": 97000
    },
    {
      "epoch": 49.34384537131231,
      "grad_norm": 42.91999816894531,
      "learning_rate": 6.561546286876908e-07,
      "loss": 1.5441,
      "step": 97010
    },
    {
      "epoch": 49.34893184130214,
      "grad_norm": 40.54000473022461,
      "learning_rate": 6.510681586978637e-07,
      "loss": 1.4371,
      "step": 97020
    },
    {
      "epoch": 49.354018311291966,
      "grad_norm": 41.32315444946289,
      "learning_rate": 6.459816887080366e-07,
      "loss": 1.4778,
      "step": 97030
    },
    {
      "epoch": 49.35910478128179,
      "grad_norm": 38.683837890625,
      "learning_rate": 6.408952187182096e-07,
      "loss": 1.5059,
      "step": 97040
    },
    {
      "epoch": 49.36419125127162,
      "grad_norm": 46.76529312133789,
      "learning_rate": 6.358087487283826e-07,
      "loss": 1.537,
      "step": 97050
    },
    {
      "epoch": 49.36927772126145,
      "grad_norm": 58.00795364379883,
      "learning_rate": 6.307222787385555e-07,
      "loss": 1.5676,
      "step": 97060
    },
    {
      "epoch": 49.374364191251274,
      "grad_norm": 42.767333984375,
      "learning_rate": 6.256358087487284e-07,
      "loss": 1.4824,
      "step": 97070
    },
    {
      "epoch": 49.3794506612411,
      "grad_norm": 50.95439147949219,
      "learning_rate": 6.205493387589014e-07,
      "loss": 1.4883,
      "step": 97080
    },
    {
      "epoch": 49.38453713123093,
      "grad_norm": 43.99070358276367,
      "learning_rate": 6.154628687690743e-07,
      "loss": 1.5163,
      "step": 97090
    },
    {
      "epoch": 49.389623601220755,
      "grad_norm": 44.68385314941406,
      "learning_rate": 6.103763987792472e-07,
      "loss": 1.569,
      "step": 97100
    },
    {
      "epoch": 49.39471007121058,
      "grad_norm": 36.17319107055664,
      "learning_rate": 6.052899287894201e-07,
      "loss": 1.4985,
      "step": 97110
    },
    {
      "epoch": 49.39979654120041,
      "grad_norm": 39.710350036621094,
      "learning_rate": 6.002034587995931e-07,
      "loss": 1.5207,
      "step": 97120
    },
    {
      "epoch": 49.404883011190236,
      "grad_norm": 50.38981246948242,
      "learning_rate": 5.951169888097661e-07,
      "loss": 1.5285,
      "step": 97130
    },
    {
      "epoch": 49.40996948118006,
      "grad_norm": 31.6038875579834,
      "learning_rate": 5.90030518819939e-07,
      "loss": 1.5365,
      "step": 97140
    },
    {
      "epoch": 49.41505595116989,
      "grad_norm": 32.559356689453125,
      "learning_rate": 5.84944048830112e-07,
      "loss": 1.5533,
      "step": 97150
    },
    {
      "epoch": 49.42014242115972,
      "grad_norm": 35.02333068847656,
      "learning_rate": 5.798575788402849e-07,
      "loss": 1.4855,
      "step": 97160
    },
    {
      "epoch": 49.425228891149544,
      "grad_norm": 49.85948944091797,
      "learning_rate": 5.747711088504578e-07,
      "loss": 1.5663,
      "step": 97170
    },
    {
      "epoch": 49.43031536113937,
      "grad_norm": 42.89327621459961,
      "learning_rate": 5.696846388606308e-07,
      "loss": 1.498,
      "step": 97180
    },
    {
      "epoch": 49.4354018311292,
      "grad_norm": 48.81507873535156,
      "learning_rate": 5.645981688708036e-07,
      "loss": 1.4707,
      "step": 97190
    },
    {
      "epoch": 49.440488301119025,
      "grad_norm": 42.16551971435547,
      "learning_rate": 5.595116988809766e-07,
      "loss": 1.5247,
      "step": 97200
    },
    {
      "epoch": 49.44557477110885,
      "grad_norm": 43.44207000732422,
      "learning_rate": 5.544252288911496e-07,
      "loss": 1.4947,
      "step": 97210
    },
    {
      "epoch": 49.45066124109868,
      "grad_norm": 46.46499252319336,
      "learning_rate": 5.493387589013225e-07,
      "loss": 1.4486,
      "step": 97220
    },
    {
      "epoch": 49.455747711088506,
      "grad_norm": 44.81382751464844,
      "learning_rate": 5.442522889114955e-07,
      "loss": 1.4675,
      "step": 97230
    },
    {
      "epoch": 49.46083418107833,
      "grad_norm": 40.911617279052734,
      "learning_rate": 5.391658189216684e-07,
      "loss": 1.5379,
      "step": 97240
    },
    {
      "epoch": 49.46592065106816,
      "grad_norm": 45.17825698852539,
      "learning_rate": 5.340793489318413e-07,
      "loss": 1.4938,
      "step": 97250
    },
    {
      "epoch": 49.47100712105799,
      "grad_norm": 37.04964065551758,
      "learning_rate": 5.289928789420143e-07,
      "loss": 1.5667,
      "step": 97260
    },
    {
      "epoch": 49.476093591047814,
      "grad_norm": 41.014198303222656,
      "learning_rate": 5.239064089521871e-07,
      "loss": 1.4359,
      "step": 97270
    },
    {
      "epoch": 49.48118006103764,
      "grad_norm": 46.697017669677734,
      "learning_rate": 5.188199389623601e-07,
      "loss": 1.4501,
      "step": 97280
    },
    {
      "epoch": 49.48626653102747,
      "grad_norm": 42.871219635009766,
      "learning_rate": 5.137334689725332e-07,
      "loss": 1.4493,
      "step": 97290
    },
    {
      "epoch": 49.491353001017295,
      "grad_norm": 46.721492767333984,
      "learning_rate": 5.08646998982706e-07,
      "loss": 1.5213,
      "step": 97300
    },
    {
      "epoch": 49.49643947100712,
      "grad_norm": 39.60346603393555,
      "learning_rate": 5.03560528992879e-07,
      "loss": 1.5031,
      "step": 97310
    },
    {
      "epoch": 49.50152594099695,
      "grad_norm": 44.75516891479492,
      "learning_rate": 4.98474059003052e-07,
      "loss": 1.4938,
      "step": 97320
    },
    {
      "epoch": 49.506612410986776,
      "grad_norm": 50.697017669677734,
      "learning_rate": 4.933875890132248e-07,
      "loss": 1.4738,
      "step": 97330
    },
    {
      "epoch": 49.5116988809766,
      "grad_norm": 33.66190719604492,
      "learning_rate": 4.883011190233978e-07,
      "loss": 1.5105,
      "step": 97340
    },
    {
      "epoch": 49.51678535096643,
      "grad_norm": 42.30119705200195,
      "learning_rate": 4.832146490335707e-07,
      "loss": 1.43,
      "step": 97350
    },
    {
      "epoch": 49.52187182095626,
      "grad_norm": 43.035186767578125,
      "learning_rate": 4.781281790437436e-07,
      "loss": 1.519,
      "step": 97360
    },
    {
      "epoch": 49.526958290946084,
      "grad_norm": 38.37263488769531,
      "learning_rate": 4.730417090539166e-07,
      "loss": 1.5157,
      "step": 97370
    },
    {
      "epoch": 49.53204476093591,
      "grad_norm": 52.48051452636719,
      "learning_rate": 4.679552390640896e-07,
      "loss": 1.5067,
      "step": 97380
    },
    {
      "epoch": 49.53713123092574,
      "grad_norm": 65.20552825927734,
      "learning_rate": 4.628687690742625e-07,
      "loss": 1.4349,
      "step": 97390
    },
    {
      "epoch": 49.542217700915565,
      "grad_norm": 48.32000732421875,
      "learning_rate": 4.577822990844354e-07,
      "loss": 1.4981,
      "step": 97400
    },
    {
      "epoch": 49.54730417090539,
      "grad_norm": 44.23946762084961,
      "learning_rate": 4.5269582909460837e-07,
      "loss": 1.4499,
      "step": 97410
    },
    {
      "epoch": 49.55239064089522,
      "grad_norm": 41.78070068359375,
      "learning_rate": 4.476093591047813e-07,
      "loss": 1.4688,
      "step": 97420
    },
    {
      "epoch": 49.557477110885046,
      "grad_norm": 49.50007247924805,
      "learning_rate": 4.425228891149542e-07,
      "loss": 1.4882,
      "step": 97430
    },
    {
      "epoch": 49.56256358087487,
      "grad_norm": 47.66153335571289,
      "learning_rate": 4.3743641912512716e-07,
      "loss": 1.5413,
      "step": 97440
    },
    {
      "epoch": 49.5676500508647,
      "grad_norm": 50.92040252685547,
      "learning_rate": 4.323499491353002e-07,
      "loss": 1.4875,
      "step": 97450
    },
    {
      "epoch": 49.57273652085453,
      "grad_norm": 37.28147888183594,
      "learning_rate": 4.272634791454731e-07,
      "loss": 1.4474,
      "step": 97460
    },
    {
      "epoch": 49.57782299084435,
      "grad_norm": 42.118125915527344,
      "learning_rate": 4.22177009155646e-07,
      "loss": 1.4753,
      "step": 97470
    },
    {
      "epoch": 49.58290946083418,
      "grad_norm": 39.906951904296875,
      "learning_rate": 4.1709053916581896e-07,
      "loss": 1.4451,
      "step": 97480
    },
    {
      "epoch": 49.58799593082401,
      "grad_norm": 36.01789474487305,
      "learning_rate": 4.120040691759919e-07,
      "loss": 1.4848,
      "step": 97490
    },
    {
      "epoch": 49.593082400813834,
      "grad_norm": 44.99053955078125,
      "learning_rate": 4.069175991861648e-07,
      "loss": 1.5221,
      "step": 97500
    },
    {
      "epoch": 49.59816887080366,
      "grad_norm": 37.30335235595703,
      "learning_rate": 4.0183112919633775e-07,
      "loss": 1.5545,
      "step": 97510
    },
    {
      "epoch": 49.60325534079349,
      "grad_norm": 45.92835235595703,
      "learning_rate": 3.9674465920651066e-07,
      "loss": 1.4828,
      "step": 97520
    },
    {
      "epoch": 49.608341810783315,
      "grad_norm": 39.86399841308594,
      "learning_rate": 3.916581892166837e-07,
      "loss": 1.5475,
      "step": 97530
    },
    {
      "epoch": 49.61342828077314,
      "grad_norm": 43.36680221557617,
      "learning_rate": 3.865717192268566e-07,
      "loss": 1.4564,
      "step": 97540
    },
    {
      "epoch": 49.61851475076297,
      "grad_norm": 40.082191467285156,
      "learning_rate": 3.8148524923702955e-07,
      "loss": 1.5409,
      "step": 97550
    },
    {
      "epoch": 49.623601220752796,
      "grad_norm": 45.4273567199707,
      "learning_rate": 3.7639877924720246e-07,
      "loss": 1.5062,
      "step": 97560
    },
    {
      "epoch": 49.62868769074262,
      "grad_norm": 44.412078857421875,
      "learning_rate": 3.713123092573754e-07,
      "loss": 1.4932,
      "step": 97570
    },
    {
      "epoch": 49.63377416073245,
      "grad_norm": 49.0833740234375,
      "learning_rate": 3.6622583926754834e-07,
      "loss": 1.5088,
      "step": 97580
    },
    {
      "epoch": 49.63886063072228,
      "grad_norm": 42.52371597290039,
      "learning_rate": 3.6113936927772125e-07,
      "loss": 1.5368,
      "step": 97590
    },
    {
      "epoch": 49.643947100712104,
      "grad_norm": 38.06002426147461,
      "learning_rate": 3.560528992878942e-07,
      "loss": 1.5289,
      "step": 97600
    },
    {
      "epoch": 49.64903357070193,
      "grad_norm": 42.832679748535156,
      "learning_rate": 3.509664292980672e-07,
      "loss": 1.4826,
      "step": 97610
    },
    {
      "epoch": 49.65412004069176,
      "grad_norm": 42.85578536987305,
      "learning_rate": 3.458799593082401e-07,
      "loss": 1.4422,
      "step": 97620
    },
    {
      "epoch": 49.659206510681585,
      "grad_norm": 53.76020812988281,
      "learning_rate": 3.40793489318413e-07,
      "loss": 1.4725,
      "step": 97630
    },
    {
      "epoch": 49.66429298067141,
      "grad_norm": 49.76866912841797,
      "learning_rate": 3.3570701932858596e-07,
      "loss": 1.5018,
      "step": 97640
    },
    {
      "epoch": 49.66937945066124,
      "grad_norm": 32.05210876464844,
      "learning_rate": 3.3062054933875893e-07,
      "loss": 1.5157,
      "step": 97650
    },
    {
      "epoch": 49.674465920651066,
      "grad_norm": 49.90946578979492,
      "learning_rate": 3.2553407934893184e-07,
      "loss": 1.501,
      "step": 97660
    },
    {
      "epoch": 49.67955239064089,
      "grad_norm": 49.80393981933594,
      "learning_rate": 3.204476093591048e-07,
      "loss": 1.5818,
      "step": 97670
    },
    {
      "epoch": 49.68463886063072,
      "grad_norm": 38.55398941040039,
      "learning_rate": 3.1536113936927777e-07,
      "loss": 1.4965,
      "step": 97680
    },
    {
      "epoch": 49.68972533062055,
      "grad_norm": 40.02992248535156,
      "learning_rate": 3.102746693794507e-07,
      "loss": 1.4837,
      "step": 97690
    },
    {
      "epoch": 49.694811800610374,
      "grad_norm": 37.006500244140625,
      "learning_rate": 3.051881993896236e-07,
      "loss": 1.454,
      "step": 97700
    },
    {
      "epoch": 49.6998982706002,
      "grad_norm": 41.676815032958984,
      "learning_rate": 3.0010172939979655e-07,
      "loss": 1.4988,
      "step": 97710
    },
    {
      "epoch": 49.70498474059003,
      "grad_norm": 40.39674377441406,
      "learning_rate": 2.950152594099695e-07,
      "loss": 1.5257,
      "step": 97720
    },
    {
      "epoch": 49.710071210579855,
      "grad_norm": 40.3306999206543,
      "learning_rate": 2.8992878942014243e-07,
      "loss": 1.4457,
      "step": 97730
    },
    {
      "epoch": 49.71515768056968,
      "grad_norm": 49.49463653564453,
      "learning_rate": 2.848423194303154e-07,
      "loss": 1.5027,
      "step": 97740
    },
    {
      "epoch": 49.72024415055951,
      "grad_norm": 43.51902770996094,
      "learning_rate": 2.797558494404883e-07,
      "loss": 1.5048,
      "step": 97750
    },
    {
      "epoch": 49.725330620549336,
      "grad_norm": 37.07566452026367,
      "learning_rate": 2.7466937945066127e-07,
      "loss": 1.5156,
      "step": 97760
    },
    {
      "epoch": 49.73041709053916,
      "grad_norm": 43.74675369262695,
      "learning_rate": 2.695829094608342e-07,
      "loss": 1.5147,
      "step": 97770
    },
    {
      "epoch": 49.73550356052899,
      "grad_norm": 43.74154281616211,
      "learning_rate": 2.6449643947100714e-07,
      "loss": 1.4499,
      "step": 97780
    },
    {
      "epoch": 49.74059003051882,
      "grad_norm": 44.99265670776367,
      "learning_rate": 2.5940996948118005e-07,
      "loss": 1.5773,
      "step": 97790
    },
    {
      "epoch": 49.745676500508644,
      "grad_norm": 47.13508605957031,
      "learning_rate": 2.54323499491353e-07,
      "loss": 1.4827,
      "step": 97800
    },
    {
      "epoch": 49.75076297049847,
      "grad_norm": 43.51888656616211,
      "learning_rate": 2.49237029501526e-07,
      "loss": 1.5255,
      "step": 97810
    },
    {
      "epoch": 49.7558494404883,
      "grad_norm": 43.88688278198242,
      "learning_rate": 2.441505595116989e-07,
      "loss": 1.4611,
      "step": 97820
    },
    {
      "epoch": 49.760935910478125,
      "grad_norm": 36.1530647277832,
      "learning_rate": 2.390640895218718e-07,
      "loss": 1.5285,
      "step": 97830
    },
    {
      "epoch": 49.76602238046795,
      "grad_norm": 36.90829086303711,
      "learning_rate": 2.339776195320448e-07,
      "loss": 1.503,
      "step": 97840
    },
    {
      "epoch": 49.77110885045778,
      "grad_norm": 43.782859802246094,
      "learning_rate": 2.288911495422177e-07,
      "loss": 1.5715,
      "step": 97850
    },
    {
      "epoch": 49.77619532044761,
      "grad_norm": 33.74543762207031,
      "learning_rate": 2.2380467955239064e-07,
      "loss": 1.493,
      "step": 97860
    },
    {
      "epoch": 49.78128179043744,
      "grad_norm": 32.85836410522461,
      "learning_rate": 2.1871820956256358e-07,
      "loss": 1.4435,
      "step": 97870
    },
    {
      "epoch": 49.78636826042727,
      "grad_norm": 48.92945098876953,
      "learning_rate": 2.1363173957273654e-07,
      "loss": 1.5192,
      "step": 97880
    },
    {
      "epoch": 49.791454730417094,
      "grad_norm": 41.51608657836914,
      "learning_rate": 2.0854526958290948e-07,
      "loss": 1.463,
      "step": 97890
    },
    {
      "epoch": 49.79654120040692,
      "grad_norm": 44.15818786621094,
      "learning_rate": 2.034587995930824e-07,
      "loss": 1.4449,
      "step": 97900
    },
    {
      "epoch": 49.80162767039675,
      "grad_norm": 44.044925689697266,
      "learning_rate": 1.9837232960325533e-07,
      "loss": 1.4377,
      "step": 97910
    },
    {
      "epoch": 49.806714140386575,
      "grad_norm": 53.557228088378906,
      "learning_rate": 1.932858596134283e-07,
      "loss": 1.4648,
      "step": 97920
    },
    {
      "epoch": 49.8118006103764,
      "grad_norm": 35.8502082824707,
      "learning_rate": 1.8819938962360123e-07,
      "loss": 1.4616,
      "step": 97930
    },
    {
      "epoch": 49.81688708036623,
      "grad_norm": 35.8035888671875,
      "learning_rate": 1.8311291963377417e-07,
      "loss": 1.5502,
      "step": 97940
    },
    {
      "epoch": 49.821973550356056,
      "grad_norm": 41.986473083496094,
      "learning_rate": 1.780264496439471e-07,
      "loss": 1.4395,
      "step": 97950
    },
    {
      "epoch": 49.82706002034588,
      "grad_norm": 41.04944610595703,
      "learning_rate": 1.7293997965412004e-07,
      "loss": 1.5048,
      "step": 97960
    },
    {
      "epoch": 49.83214649033571,
      "grad_norm": 48.508323669433594,
      "learning_rate": 1.6785350966429298e-07,
      "loss": 1.5911,
      "step": 97970
    },
    {
      "epoch": 49.83723296032554,
      "grad_norm": 43.04743957519531,
      "learning_rate": 1.6276703967446592e-07,
      "loss": 1.5715,
      "step": 97980
    },
    {
      "epoch": 49.842319430315364,
      "grad_norm": 34.828857421875,
      "learning_rate": 1.5768056968463888e-07,
      "loss": 1.5649,
      "step": 97990
    },
    {
      "epoch": 49.84740590030519,
      "grad_norm": 46.390254974365234,
      "learning_rate": 1.525940996948118e-07,
      "loss": 1.5881,
      "step": 98000
    },
    {
      "epoch": 49.85249237029502,
      "grad_norm": 41.83989334106445,
      "learning_rate": 1.4750762970498476e-07,
      "loss": 1.5853,
      "step": 98010
    },
    {
      "epoch": 49.857578840284845,
      "grad_norm": 40.78702926635742,
      "learning_rate": 1.424211597151577e-07,
      "loss": 1.5392,
      "step": 98020
    },
    {
      "epoch": 49.86266531027467,
      "grad_norm": 41.67129898071289,
      "learning_rate": 1.3733468972533063e-07,
      "loss": 1.5175,
      "step": 98030
    },
    {
      "epoch": 49.8677517802645,
      "grad_norm": 40.735557556152344,
      "learning_rate": 1.3224821973550357e-07,
      "loss": 1.5497,
      "step": 98040
    },
    {
      "epoch": 49.872838250254325,
      "grad_norm": 46.20461654663086,
      "learning_rate": 1.271617497456765e-07,
      "loss": 1.5094,
      "step": 98050
    },
    {
      "epoch": 49.87792472024415,
      "grad_norm": 51.34369659423828,
      "learning_rate": 1.2207527975584945e-07,
      "loss": 1.4299,
      "step": 98060
    },
    {
      "epoch": 49.88301119023398,
      "grad_norm": 57.680057525634766,
      "learning_rate": 1.169888097660224e-07,
      "loss": 1.4304,
      "step": 98070
    },
    {
      "epoch": 49.888097660223806,
      "grad_norm": 45.06082534790039,
      "learning_rate": 1.1190233977619532e-07,
      "loss": 1.5467,
      "step": 98080
    },
    {
      "epoch": 49.89318413021363,
      "grad_norm": 36.585121154785156,
      "learning_rate": 1.0681586978636827e-07,
      "loss": 1.4613,
      "step": 98090
    },
    {
      "epoch": 49.89827060020346,
      "grad_norm": 35.958152770996094,
      "learning_rate": 1.017293997965412e-07,
      "loss": 1.6031,
      "step": 98100
    },
    {
      "epoch": 49.90335707019329,
      "grad_norm": 43.143226623535156,
      "learning_rate": 9.664292980671415e-08,
      "loss": 1.4524,
      "step": 98110
    },
    {
      "epoch": 49.908443540183114,
      "grad_norm": 39.64223098754883,
      "learning_rate": 9.155645981688708e-08,
      "loss": 1.4793,
      "step": 98120
    },
    {
      "epoch": 49.91353001017294,
      "grad_norm": 42.9872932434082,
      "learning_rate": 8.646998982706002e-08,
      "loss": 1.437,
      "step": 98130
    },
    {
      "epoch": 49.91861648016277,
      "grad_norm": 44.89663314819336,
      "learning_rate": 8.138351983723296e-08,
      "loss": 1.5073,
      "step": 98140
    },
    {
      "epoch": 49.923702950152595,
      "grad_norm": 34.308292388916016,
      "learning_rate": 7.62970498474059e-08,
      "loss": 1.5026,
      "step": 98150
    },
    {
      "epoch": 49.92878942014242,
      "grad_norm": 45.276180267333984,
      "learning_rate": 7.121057985757885e-08,
      "loss": 1.4511,
      "step": 98160
    },
    {
      "epoch": 49.93387589013225,
      "grad_norm": 43.88961410522461,
      "learning_rate": 6.612410986775179e-08,
      "loss": 1.477,
      "step": 98170
    },
    {
      "epoch": 49.938962360122076,
      "grad_norm": 41.688636779785156,
      "learning_rate": 6.103763987792472e-08,
      "loss": 1.4929,
      "step": 98180
    },
    {
      "epoch": 49.9440488301119,
      "grad_norm": 53.441287994384766,
      "learning_rate": 5.595116988809766e-08,
      "loss": 1.4586,
      "step": 98190
    },
    {
      "epoch": 49.94913530010173,
      "grad_norm": 45.216922760009766,
      "learning_rate": 5.08646998982706e-08,
      "loss": 1.5078,
      "step": 98200
    },
    {
      "epoch": 49.95422177009156,
      "grad_norm": 43.829471588134766,
      "learning_rate": 4.577822990844354e-08,
      "loss": 1.4585,
      "step": 98210
    },
    {
      "epoch": 49.959308240081384,
      "grad_norm": 30.248342514038086,
      "learning_rate": 4.069175991861648e-08,
      "loss": 1.4547,
      "step": 98220
    },
    {
      "epoch": 49.96439471007121,
      "grad_norm": 45.813194274902344,
      "learning_rate": 3.5605289928789424e-08,
      "loss": 1.4826,
      "step": 98230
    },
    {
      "epoch": 49.96948118006104,
      "grad_norm": 36.49383544921875,
      "learning_rate": 3.051881993896236e-08,
      "loss": 1.4176,
      "step": 98240
    },
    {
      "epoch": 49.974567650050865,
      "grad_norm": 44.77225112915039,
      "learning_rate": 2.54323499491353e-08,
      "loss": 1.4612,
      "step": 98250
    },
    {
      "epoch": 49.97965412004069,
      "grad_norm": 40.48656463623047,
      "learning_rate": 2.034587995930824e-08,
      "loss": 1.5415,
      "step": 98260
    },
    {
      "epoch": 49.98474059003052,
      "grad_norm": 46.76436233520508,
      "learning_rate": 1.525940996948118e-08,
      "loss": 1.4833,
      "step": 98270
    },
    {
      "epoch": 49.989827060020346,
      "grad_norm": 47.19850158691406,
      "learning_rate": 1.017293997965412e-08,
      "loss": 1.4327,
      "step": 98280
    },
    {
      "epoch": 49.99491353001017,
      "grad_norm": 48.61103439331055,
      "learning_rate": 5.08646998982706e-09,
      "loss": 1.3622,
      "step": 98290
    },
    {
      "epoch": 50.0,
      "grad_norm": 47.348228454589844,
      "learning_rate": 0.0,
      "loss": 1.4885,
      "step": 98300
    },
    {
      "epoch": 50.0,
      "eval_loss": 5.135570049285889,
      "eval_runtime": 2.7411,
      "eval_samples_per_second": 1012.385,
      "eval_steps_per_second": 126.594,
      "step": 98300
    }
  ],
  "logging_steps": 10,
  "max_steps": 98300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 32674990080000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
