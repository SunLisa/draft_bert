{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4999.0,
  "eval_steps": 500,
  "global_step": 84983,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 5.664618968963623,
      "learning_rate": 4.9994117647058825e-05,
      "loss": 5.3608,
      "step": 10
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 2.9443488121032715,
      "learning_rate": 4.998823529411765e-05,
      "loss": 5.3018,
      "step": 20
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 3.5871706008911133,
      "learning_rate": 4.998235294117647e-05,
      "loss": 5.1696,
      "step": 30
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 3.553030252456665,
      "learning_rate": 4.9976470588235294e-05,
      "loss": 5.1372,
      "step": 40
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 4.123727798461914,
      "learning_rate": 4.997058823529412e-05,
      "loss": 4.9723,
      "step": 50
    },
    {
      "epoch": 3.5294117647058822,
      "grad_norm": 3.573457956314087,
      "learning_rate": 4.996470588235294e-05,
      "loss": 4.9872,
      "step": 60
    },
    {
      "epoch": 4.117647058823529,
      "grad_norm": 3.3653953075408936,
      "learning_rate": 4.995882352941177e-05,
      "loss": 4.9412,
      "step": 70
    },
    {
      "epoch": 4.705882352941177,
      "grad_norm": 3.900298833847046,
      "learning_rate": 4.995294117647059e-05,
      "loss": 4.9414,
      "step": 80
    },
    {
      "epoch": 5.294117647058823,
      "grad_norm": 2.858811855316162,
      "learning_rate": 4.9947058823529416e-05,
      "loss": 4.8795,
      "step": 90
    },
    {
      "epoch": 5.882352941176471,
      "grad_norm": 3.2695412635803223,
      "learning_rate": 4.994117647058824e-05,
      "loss": 4.8281,
      "step": 100
    },
    {
      "epoch": 6.470588235294118,
      "grad_norm": 3.7287967205047607,
      "learning_rate": 4.993529411764706e-05,
      "loss": 4.7946,
      "step": 110
    },
    {
      "epoch": 7.0588235294117645,
      "grad_norm": 3.4263978004455566,
      "learning_rate": 4.9929411764705885e-05,
      "loss": 4.7557,
      "step": 120
    },
    {
      "epoch": 7.647058823529412,
      "grad_norm": 3.2215306758880615,
      "learning_rate": 4.992352941176471e-05,
      "loss": 4.6884,
      "step": 130
    },
    {
      "epoch": 8.235294117647058,
      "grad_norm": 3.0884697437286377,
      "learning_rate": 4.991764705882353e-05,
      "loss": 4.6736,
      "step": 140
    },
    {
      "epoch": 8.823529411764707,
      "grad_norm": 3.259225606918335,
      "learning_rate": 4.991176470588236e-05,
      "loss": 4.7469,
      "step": 150
    },
    {
      "epoch": 9.411764705882353,
      "grad_norm": 2.8805370330810547,
      "learning_rate": 4.990588235294118e-05,
      "loss": 4.5898,
      "step": 160
    },
    {
      "epoch": 10.0,
      "grad_norm": 3.9449803829193115,
      "learning_rate": 4.99e-05,
      "loss": 4.5621,
      "step": 170
    },
    {
      "epoch": 10.588235294117647,
      "grad_norm": 2.8597733974456787,
      "learning_rate": 4.989411764705882e-05,
      "loss": 4.5723,
      "step": 180
    },
    {
      "epoch": 11.176470588235293,
      "grad_norm": 3.0999162197113037,
      "learning_rate": 4.9888235294117646e-05,
      "loss": 4.553,
      "step": 190
    },
    {
      "epoch": 11.764705882352942,
      "grad_norm": 3.894310712814331,
      "learning_rate": 4.9882352941176476e-05,
      "loss": 4.5545,
      "step": 200
    },
    {
      "epoch": 12.352941176470589,
      "grad_norm": 3.7853498458862305,
      "learning_rate": 4.98764705882353e-05,
      "loss": 4.4648,
      "step": 210
    },
    {
      "epoch": 12.941176470588236,
      "grad_norm": 3.4224839210510254,
      "learning_rate": 4.987058823529412e-05,
      "loss": 4.4106,
      "step": 220
    },
    {
      "epoch": 13.529411764705882,
      "grad_norm": 3.091512441635132,
      "learning_rate": 4.986470588235294e-05,
      "loss": 4.2728,
      "step": 230
    },
    {
      "epoch": 14.117647058823529,
      "grad_norm": 3.109865427017212,
      "learning_rate": 4.985882352941177e-05,
      "loss": 4.3795,
      "step": 240
    },
    {
      "epoch": 14.705882352941176,
      "grad_norm": 2.698369264602661,
      "learning_rate": 4.985294117647059e-05,
      "loss": 4.3518,
      "step": 250
    },
    {
      "epoch": 15.294117647058824,
      "grad_norm": 3.0702059268951416,
      "learning_rate": 4.9847058823529414e-05,
      "loss": 4.2533,
      "step": 260
    },
    {
      "epoch": 15.882352941176471,
      "grad_norm": 2.8006482124328613,
      "learning_rate": 4.984117647058824e-05,
      "loss": 4.2996,
      "step": 270
    },
    {
      "epoch": 16.470588235294116,
      "grad_norm": 2.7760980129241943,
      "learning_rate": 4.9835294117647066e-05,
      "loss": 4.2487,
      "step": 280
    },
    {
      "epoch": 17.058823529411764,
      "grad_norm": 3.740229606628418,
      "learning_rate": 4.982941176470588e-05,
      "loss": 4.1533,
      "step": 290
    },
    {
      "epoch": 17.647058823529413,
      "grad_norm": 2.850011110305786,
      "learning_rate": 4.9823529411764706e-05,
      "loss": 4.1631,
      "step": 300
    },
    {
      "epoch": 18.235294117647058,
      "grad_norm": 2.9452927112579346,
      "learning_rate": 4.981764705882353e-05,
      "loss": 4.1669,
      "step": 310
    },
    {
      "epoch": 18.823529411764707,
      "grad_norm": 3.9046435356140137,
      "learning_rate": 4.981176470588236e-05,
      "loss": 4.0658,
      "step": 320
    },
    {
      "epoch": 19.41176470588235,
      "grad_norm": 2.8508012294769287,
      "learning_rate": 4.980588235294118e-05,
      "loss": 4.1108,
      "step": 330
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.655233860015869,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 4.0625,
      "step": 340
    },
    {
      "epoch": 20.58823529411765,
      "grad_norm": 2.7430691719055176,
      "learning_rate": 4.979411764705883e-05,
      "loss": 3.9981,
      "step": 350
    },
    {
      "epoch": 21.176470588235293,
      "grad_norm": 2.525480031967163,
      "learning_rate": 4.978823529411765e-05,
      "loss": 4.139,
      "step": 360
    },
    {
      "epoch": 21.764705882352942,
      "grad_norm": 2.9588711261749268,
      "learning_rate": 4.978235294117647e-05,
      "loss": 3.9326,
      "step": 370
    },
    {
      "epoch": 22.352941176470587,
      "grad_norm": 3.4744906425476074,
      "learning_rate": 4.9776470588235296e-05,
      "loss": 3.8748,
      "step": 380
    },
    {
      "epoch": 22.941176470588236,
      "grad_norm": 2.5351710319519043,
      "learning_rate": 4.977058823529412e-05,
      "loss": 4.0734,
      "step": 390
    },
    {
      "epoch": 23.529411764705884,
      "grad_norm": 2.865008592605591,
      "learning_rate": 4.976470588235294e-05,
      "loss": 3.8791,
      "step": 400
    },
    {
      "epoch": 24.11764705882353,
      "grad_norm": 2.5265588760375977,
      "learning_rate": 4.975882352941177e-05,
      "loss": 3.8164,
      "step": 410
    },
    {
      "epoch": 24.705882352941178,
      "grad_norm": 2.494133234024048,
      "learning_rate": 4.975294117647059e-05,
      "loss": 3.8881,
      "step": 420
    },
    {
      "epoch": 25.294117647058822,
      "grad_norm": 2.7954254150390625,
      "learning_rate": 4.974705882352941e-05,
      "loss": 3.8853,
      "step": 430
    },
    {
      "epoch": 25.88235294117647,
      "grad_norm": 2.8275246620178223,
      "learning_rate": 4.9741176470588234e-05,
      "loss": 4.0904,
      "step": 440
    },
    {
      "epoch": 26.470588235294116,
      "grad_norm": 2.5428144931793213,
      "learning_rate": 4.9735294117647064e-05,
      "loss": 4.001,
      "step": 450
    },
    {
      "epoch": 27.058823529411764,
      "grad_norm": 2.6862733364105225,
      "learning_rate": 4.972941176470589e-05,
      "loss": 3.8483,
      "step": 460
    },
    {
      "epoch": 27.647058823529413,
      "grad_norm": 2.5890262126922607,
      "learning_rate": 4.972352941176471e-05,
      "loss": 3.8298,
      "step": 470
    },
    {
      "epoch": 28.235294117647058,
      "grad_norm": 2.3996336460113525,
      "learning_rate": 4.971764705882353e-05,
      "loss": 3.8851,
      "step": 480
    },
    {
      "epoch": 28.823529411764707,
      "grad_norm": 2.7708425521850586,
      "learning_rate": 4.9711764705882356e-05,
      "loss": 3.804,
      "step": 490
    },
    {
      "epoch": 29.41176470588235,
      "grad_norm": 2.295736074447632,
      "learning_rate": 4.970588235294118e-05,
      "loss": 3.8043,
      "step": 500
    },
    {
      "epoch": 30.0,
      "grad_norm": 2.3982880115509033,
      "learning_rate": 4.97e-05,
      "loss": 3.7348,
      "step": 510
    },
    {
      "epoch": 30.58823529411765,
      "grad_norm": 2.5879979133605957,
      "learning_rate": 4.9694117647058825e-05,
      "loss": 3.7347,
      "step": 520
    },
    {
      "epoch": 31.176470588235293,
      "grad_norm": 2.9537389278411865,
      "learning_rate": 4.9688235294117655e-05,
      "loss": 3.7739,
      "step": 530
    },
    {
      "epoch": 31.764705882352942,
      "grad_norm": 2.6167960166931152,
      "learning_rate": 4.968235294117648e-05,
      "loss": 3.6869,
      "step": 540
    },
    {
      "epoch": 32.35294117647059,
      "grad_norm": 2.4988605976104736,
      "learning_rate": 4.9676470588235294e-05,
      "loss": 3.7342,
      "step": 550
    },
    {
      "epoch": 32.94117647058823,
      "grad_norm": 2.687847375869751,
      "learning_rate": 4.967058823529412e-05,
      "loss": 3.7341,
      "step": 560
    },
    {
      "epoch": 33.529411764705884,
      "grad_norm": 2.518822193145752,
      "learning_rate": 4.966470588235294e-05,
      "loss": 3.7726,
      "step": 570
    },
    {
      "epoch": 34.11764705882353,
      "grad_norm": 2.4969546794891357,
      "learning_rate": 4.965882352941177e-05,
      "loss": 3.6609,
      "step": 580
    },
    {
      "epoch": 34.705882352941174,
      "grad_norm": 2.4807212352752686,
      "learning_rate": 4.965294117647059e-05,
      "loss": 3.7244,
      "step": 590
    },
    {
      "epoch": 35.294117647058826,
      "grad_norm": 2.5527071952819824,
      "learning_rate": 4.9647058823529416e-05,
      "loss": 3.5919,
      "step": 600
    },
    {
      "epoch": 35.88235294117647,
      "grad_norm": 3.2782785892486572,
      "learning_rate": 4.964117647058824e-05,
      "loss": 3.7291,
      "step": 610
    },
    {
      "epoch": 36.470588235294116,
      "grad_norm": 2.491262197494507,
      "learning_rate": 4.963529411764706e-05,
      "loss": 3.6353,
      "step": 620
    },
    {
      "epoch": 37.05882352941177,
      "grad_norm": 2.65370512008667,
      "learning_rate": 4.9629411764705885e-05,
      "loss": 3.6053,
      "step": 630
    },
    {
      "epoch": 37.64705882352941,
      "grad_norm": 2.4270479679107666,
      "learning_rate": 4.962352941176471e-05,
      "loss": 3.6228,
      "step": 640
    },
    {
      "epoch": 38.23529411764706,
      "grad_norm": 1.8881313800811768,
      "learning_rate": 4.961764705882353e-05,
      "loss": 3.5585,
      "step": 650
    },
    {
      "epoch": 38.8235294117647,
      "grad_norm": 2.863348960876465,
      "learning_rate": 4.961176470588236e-05,
      "loss": 3.5902,
      "step": 660
    },
    {
      "epoch": 39.411764705882355,
      "grad_norm": 2.402616262435913,
      "learning_rate": 4.9605882352941176e-05,
      "loss": 3.5656,
      "step": 670
    },
    {
      "epoch": 40.0,
      "grad_norm": 2.9799177646636963,
      "learning_rate": 4.96e-05,
      "loss": 3.5763,
      "step": 680
    },
    {
      "epoch": 40.588235294117645,
      "grad_norm": 2.309840202331543,
      "learning_rate": 4.959411764705882e-05,
      "loss": 3.6287,
      "step": 690
    },
    {
      "epoch": 41.1764705882353,
      "grad_norm": 2.5206403732299805,
      "learning_rate": 4.958823529411765e-05,
      "loss": 3.4515,
      "step": 700
    },
    {
      "epoch": 41.76470588235294,
      "grad_norm": 2.458329439163208,
      "learning_rate": 4.9582352941176475e-05,
      "loss": 3.5587,
      "step": 710
    },
    {
      "epoch": 42.35294117647059,
      "grad_norm": 2.845790147781372,
      "learning_rate": 4.95764705882353e-05,
      "loss": 3.5024,
      "step": 720
    },
    {
      "epoch": 42.94117647058823,
      "grad_norm": 2.3144757747650146,
      "learning_rate": 4.957058823529412e-05,
      "loss": 3.4556,
      "step": 730
    },
    {
      "epoch": 43.529411764705884,
      "grad_norm": 2.396700143814087,
      "learning_rate": 4.9564705882352944e-05,
      "loss": 3.5605,
      "step": 740
    },
    {
      "epoch": 44.11764705882353,
      "grad_norm": 2.142714262008667,
      "learning_rate": 4.955882352941177e-05,
      "loss": 3.6828,
      "step": 750
    },
    {
      "epoch": 44.705882352941174,
      "grad_norm": 2.7323460578918457,
      "learning_rate": 4.955294117647059e-05,
      "loss": 3.5487,
      "step": 760
    },
    {
      "epoch": 45.294117647058826,
      "grad_norm": 2.7791833877563477,
      "learning_rate": 4.954705882352941e-05,
      "loss": 3.4539,
      "step": 770
    },
    {
      "epoch": 45.88235294117647,
      "grad_norm": 2.8343143463134766,
      "learning_rate": 4.9541176470588236e-05,
      "loss": 3.4691,
      "step": 780
    },
    {
      "epoch": 46.470588235294116,
      "grad_norm": 2.62740421295166,
      "learning_rate": 4.9535294117647066e-05,
      "loss": 3.3904,
      "step": 790
    },
    {
      "epoch": 47.05882352941177,
      "grad_norm": 2.619959592819214,
      "learning_rate": 4.952941176470588e-05,
      "loss": 3.5354,
      "step": 800
    },
    {
      "epoch": 47.64705882352941,
      "grad_norm": 2.447833299636841,
      "learning_rate": 4.9523529411764705e-05,
      "loss": 3.5647,
      "step": 810
    },
    {
      "epoch": 48.23529411764706,
      "grad_norm": 2.5932233333587646,
      "learning_rate": 4.951764705882353e-05,
      "loss": 3.4206,
      "step": 820
    },
    {
      "epoch": 48.8235294117647,
      "grad_norm": 2.203495979309082,
      "learning_rate": 4.951176470588236e-05,
      "loss": 3.4381,
      "step": 830
    },
    {
      "epoch": 49.411764705882355,
      "grad_norm": 2.344658374786377,
      "learning_rate": 4.950588235294118e-05,
      "loss": 3.5387,
      "step": 840
    },
    {
      "epoch": 50.0,
      "grad_norm": 2.845707416534424,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 3.3973,
      "step": 850
    },
    {
      "epoch": 50.588235294117645,
      "grad_norm": 2.403463840484619,
      "learning_rate": 4.949411764705883e-05,
      "loss": 3.3746,
      "step": 860
    },
    {
      "epoch": 51.1764705882353,
      "grad_norm": 2.6911206245422363,
      "learning_rate": 4.948823529411765e-05,
      "loss": 3.5087,
      "step": 870
    },
    {
      "epoch": 51.76470588235294,
      "grad_norm": 2.561959981918335,
      "learning_rate": 4.948235294117647e-05,
      "loss": 3.3882,
      "step": 880
    },
    {
      "epoch": 52.35294117647059,
      "grad_norm": 2.7970988750457764,
      "learning_rate": 4.9476470588235296e-05,
      "loss": 3.4451,
      "step": 890
    },
    {
      "epoch": 52.94117647058823,
      "grad_norm": 2.3184590339660645,
      "learning_rate": 4.947058823529412e-05,
      "loss": 3.334,
      "step": 900
    },
    {
      "epoch": 53.529411764705884,
      "grad_norm": 2.3507964611053467,
      "learning_rate": 4.946470588235294e-05,
      "loss": 3.2543,
      "step": 910
    },
    {
      "epoch": 54.11764705882353,
      "grad_norm": 2.535876512527466,
      "learning_rate": 4.945882352941177e-05,
      "loss": 3.42,
      "step": 920
    },
    {
      "epoch": 54.705882352941174,
      "grad_norm": 2.6333978176116943,
      "learning_rate": 4.945294117647059e-05,
      "loss": 3.5339,
      "step": 930
    },
    {
      "epoch": 55.294117647058826,
      "grad_norm": 3.057274103164673,
      "learning_rate": 4.944705882352941e-05,
      "loss": 3.2634,
      "step": 940
    },
    {
      "epoch": 55.88235294117647,
      "grad_norm": 2.608336925506592,
      "learning_rate": 4.9441176470588234e-05,
      "loss": 3.2565,
      "step": 950
    },
    {
      "epoch": 56.470588235294116,
      "grad_norm": 2.982039213180542,
      "learning_rate": 4.9435294117647063e-05,
      "loss": 3.2588,
      "step": 960
    },
    {
      "epoch": 57.05882352941177,
      "grad_norm": 2.2857327461242676,
      "learning_rate": 4.9429411764705886e-05,
      "loss": 3.2737,
      "step": 970
    },
    {
      "epoch": 57.64705882352941,
      "grad_norm": 2.9484033584594727,
      "learning_rate": 4.942352941176471e-05,
      "loss": 3.2971,
      "step": 980
    },
    {
      "epoch": 58.23529411764706,
      "grad_norm": 2.4662885665893555,
      "learning_rate": 4.941764705882353e-05,
      "loss": 3.5303,
      "step": 990
    },
    {
      "epoch": 58.8235294117647,
      "grad_norm": 2.3508708477020264,
      "learning_rate": 4.9411764705882355e-05,
      "loss": 3.3403,
      "step": 1000
    },
    {
      "epoch": 59.411764705882355,
      "grad_norm": 2.9673056602478027,
      "learning_rate": 4.940588235294118e-05,
      "loss": 3.4185,
      "step": 1010
    },
    {
      "epoch": 60.0,
      "grad_norm": 2.8709750175476074,
      "learning_rate": 4.94e-05,
      "loss": 3.4689,
      "step": 1020
    },
    {
      "epoch": 60.588235294117645,
      "grad_norm": 2.8587446212768555,
      "learning_rate": 4.9394117647058824e-05,
      "loss": 3.2989,
      "step": 1030
    },
    {
      "epoch": 61.1764705882353,
      "grad_norm": 2.8504722118377686,
      "learning_rate": 4.9388235294117654e-05,
      "loss": 3.2242,
      "step": 1040
    },
    {
      "epoch": 61.76470588235294,
      "grad_norm": 2.8364791870117188,
      "learning_rate": 4.938235294117648e-05,
      "loss": 3.3451,
      "step": 1050
    },
    {
      "epoch": 62.35294117647059,
      "grad_norm": 2.6662099361419678,
      "learning_rate": 4.937647058823529e-05,
      "loss": 3.436,
      "step": 1060
    },
    {
      "epoch": 62.94117647058823,
      "grad_norm": 2.532487154006958,
      "learning_rate": 4.9370588235294116e-05,
      "loss": 3.3298,
      "step": 1070
    },
    {
      "epoch": 63.529411764705884,
      "grad_norm": 2.7093944549560547,
      "learning_rate": 4.9364705882352946e-05,
      "loss": 3.3196,
      "step": 1080
    },
    {
      "epoch": 64.11764705882354,
      "grad_norm": 2.492677927017212,
      "learning_rate": 4.935882352941177e-05,
      "loss": 3.4357,
      "step": 1090
    },
    {
      "epoch": 64.70588235294117,
      "grad_norm": 2.2169723510742188,
      "learning_rate": 4.935294117647059e-05,
      "loss": 3.4042,
      "step": 1100
    },
    {
      "epoch": 65.29411764705883,
      "grad_norm": 2.7207725048065186,
      "learning_rate": 4.9347058823529415e-05,
      "loss": 3.2114,
      "step": 1110
    },
    {
      "epoch": 65.88235294117646,
      "grad_norm": 2.7674803733825684,
      "learning_rate": 4.934117647058824e-05,
      "loss": 3.2468,
      "step": 1120
    },
    {
      "epoch": 66.47058823529412,
      "grad_norm": 2.696765661239624,
      "learning_rate": 4.933529411764706e-05,
      "loss": 3.2605,
      "step": 1130
    },
    {
      "epoch": 67.05882352941177,
      "grad_norm": 3.2727153301239014,
      "learning_rate": 4.9329411764705884e-05,
      "loss": 3.26,
      "step": 1140
    },
    {
      "epoch": 67.6470588235294,
      "grad_norm": 2.332435131072998,
      "learning_rate": 4.932352941176471e-05,
      "loss": 3.3167,
      "step": 1150
    },
    {
      "epoch": 68.23529411764706,
      "grad_norm": 2.7956697940826416,
      "learning_rate": 4.931764705882353e-05,
      "loss": 3.2942,
      "step": 1160
    },
    {
      "epoch": 68.82352941176471,
      "grad_norm": 2.916428327560425,
      "learning_rate": 4.931176470588236e-05,
      "loss": 3.1535,
      "step": 1170
    },
    {
      "epoch": 69.41176470588235,
      "grad_norm": 3.0416879653930664,
      "learning_rate": 4.930588235294118e-05,
      "loss": 3.3094,
      "step": 1180
    },
    {
      "epoch": 70.0,
      "grad_norm": 2.966545581817627,
      "learning_rate": 4.93e-05,
      "loss": 3.2789,
      "step": 1190
    },
    {
      "epoch": 70.58823529411765,
      "grad_norm": 3.1067397594451904,
      "learning_rate": 4.929411764705882e-05,
      "loss": 3.3882,
      "step": 1200
    },
    {
      "epoch": 71.17647058823529,
      "grad_norm": 2.7188618183135986,
      "learning_rate": 4.928823529411765e-05,
      "loss": 3.2623,
      "step": 1210
    },
    {
      "epoch": 71.76470588235294,
      "grad_norm": 4.254537105560303,
      "learning_rate": 4.9282352941176475e-05,
      "loss": 3.3281,
      "step": 1220
    },
    {
      "epoch": 72.3529411764706,
      "grad_norm": 2.7298367023468018,
      "learning_rate": 4.92764705882353e-05,
      "loss": 3.3979,
      "step": 1230
    },
    {
      "epoch": 72.94117647058823,
      "grad_norm": 2.33518123626709,
      "learning_rate": 4.927058823529412e-05,
      "loss": 3.2755,
      "step": 1240
    },
    {
      "epoch": 73.52941176470588,
      "grad_norm": 2.474726438522339,
      "learning_rate": 4.9264705882352944e-05,
      "loss": 3.23,
      "step": 1250
    },
    {
      "epoch": 74.11764705882354,
      "grad_norm": 2.7911863327026367,
      "learning_rate": 4.925882352941177e-05,
      "loss": 3.2061,
      "step": 1260
    },
    {
      "epoch": 74.70588235294117,
      "grad_norm": 2.564390182495117,
      "learning_rate": 4.925294117647059e-05,
      "loss": 3.2584,
      "step": 1270
    },
    {
      "epoch": 75.29411764705883,
      "grad_norm": 2.7819669246673584,
      "learning_rate": 4.924705882352941e-05,
      "loss": 3.1569,
      "step": 1280
    },
    {
      "epoch": 75.88235294117646,
      "grad_norm": 2.485311985015869,
      "learning_rate": 4.9241176470588236e-05,
      "loss": 3.2239,
      "step": 1290
    },
    {
      "epoch": 76.47058823529412,
      "grad_norm": 2.919236421585083,
      "learning_rate": 4.9235294117647065e-05,
      "loss": 3.1385,
      "step": 1300
    },
    {
      "epoch": 77.05882352941177,
      "grad_norm": 2.658320665359497,
      "learning_rate": 4.922941176470589e-05,
      "loss": 3.1519,
      "step": 1310
    },
    {
      "epoch": 77.6470588235294,
      "grad_norm": 3.0406336784362793,
      "learning_rate": 4.9223529411764705e-05,
      "loss": 3.2162,
      "step": 1320
    },
    {
      "epoch": 78.23529411764706,
      "grad_norm": 2.985821008682251,
      "learning_rate": 4.921764705882353e-05,
      "loss": 3.2092,
      "step": 1330
    },
    {
      "epoch": 78.82352941176471,
      "grad_norm": 3.516055107116699,
      "learning_rate": 4.921176470588236e-05,
      "loss": 3.287,
      "step": 1340
    },
    {
      "epoch": 79.41176470588235,
      "grad_norm": 2.638828754425049,
      "learning_rate": 4.920588235294118e-05,
      "loss": 3.1275,
      "step": 1350
    },
    {
      "epoch": 80.0,
      "grad_norm": 2.915910005569458,
      "learning_rate": 4.92e-05,
      "loss": 3.0961,
      "step": 1360
    },
    {
      "epoch": 80.58823529411765,
      "grad_norm": 2.484713077545166,
      "learning_rate": 4.9194117647058826e-05,
      "loss": 3.3344,
      "step": 1370
    },
    {
      "epoch": 81.17647058823529,
      "grad_norm": 2.4577219486236572,
      "learning_rate": 4.918823529411765e-05,
      "loss": 3.2865,
      "step": 1380
    },
    {
      "epoch": 81.76470588235294,
      "grad_norm": 3.8040430545806885,
      "learning_rate": 4.918235294117647e-05,
      "loss": 3.2667,
      "step": 1390
    },
    {
      "epoch": 82.3529411764706,
      "grad_norm": 2.541317939758301,
      "learning_rate": 4.9176470588235295e-05,
      "loss": 3.2489,
      "step": 1400
    },
    {
      "epoch": 82.94117647058823,
      "grad_norm": 3.034925937652588,
      "learning_rate": 4.917058823529412e-05,
      "loss": 3.0729,
      "step": 1410
    },
    {
      "epoch": 83.52941176470588,
      "grad_norm": 3.237955331802368,
      "learning_rate": 4.916470588235295e-05,
      "loss": 3.1365,
      "step": 1420
    },
    {
      "epoch": 84.11764705882354,
      "grad_norm": 3.1603639125823975,
      "learning_rate": 4.915882352941177e-05,
      "loss": 3.2229,
      "step": 1430
    },
    {
      "epoch": 84.70588235294117,
      "grad_norm": 2.2575395107269287,
      "learning_rate": 4.915294117647059e-05,
      "loss": 3.0351,
      "step": 1440
    },
    {
      "epoch": 85.29411764705883,
      "grad_norm": 2.925724744796753,
      "learning_rate": 4.914705882352941e-05,
      "loss": 3.2297,
      "step": 1450
    },
    {
      "epoch": 85.88235294117646,
      "grad_norm": 2.6846532821655273,
      "learning_rate": 4.914117647058823e-05,
      "loss": 3.3972,
      "step": 1460
    },
    {
      "epoch": 86.47058823529412,
      "grad_norm": 2.6611273288726807,
      "learning_rate": 4.913529411764706e-05,
      "loss": 3.1542,
      "step": 1470
    },
    {
      "epoch": 87.05882352941177,
      "grad_norm": 2.566290855407715,
      "learning_rate": 4.9129411764705886e-05,
      "loss": 3.2479,
      "step": 1480
    },
    {
      "epoch": 87.6470588235294,
      "grad_norm": 2.499427556991577,
      "learning_rate": 4.912352941176471e-05,
      "loss": 3.1016,
      "step": 1490
    },
    {
      "epoch": 88.23529411764706,
      "grad_norm": 2.6482789516448975,
      "learning_rate": 4.911764705882353e-05,
      "loss": 3.1897,
      "step": 1500
    },
    {
      "epoch": 88.82352941176471,
      "grad_norm": 3.2845678329467773,
      "learning_rate": 4.9111764705882355e-05,
      "loss": 3.2158,
      "step": 1510
    },
    {
      "epoch": 89.41176470588235,
      "grad_norm": 3.250657558441162,
      "learning_rate": 4.910588235294118e-05,
      "loss": 3.2452,
      "step": 1520
    },
    {
      "epoch": 90.0,
      "grad_norm": 3.527362108230591,
      "learning_rate": 4.91e-05,
      "loss": 3.3772,
      "step": 1530
    },
    {
      "epoch": 90.58823529411765,
      "grad_norm": 3.274778366088867,
      "learning_rate": 4.9094117647058824e-05,
      "loss": 3.0852,
      "step": 1540
    },
    {
      "epoch": 91.17647058823529,
      "grad_norm": 3.062007427215576,
      "learning_rate": 4.9088235294117654e-05,
      "loss": 3.0425,
      "step": 1550
    },
    {
      "epoch": 91.76470588235294,
      "grad_norm": 2.5805282592773438,
      "learning_rate": 4.9082352941176477e-05,
      "loss": 3.1465,
      "step": 1560
    },
    {
      "epoch": 92.3529411764706,
      "grad_norm": 3.1914165019989014,
      "learning_rate": 4.907647058823529e-05,
      "loss": 3.0377,
      "step": 1570
    },
    {
      "epoch": 92.94117647058823,
      "grad_norm": 2.990614175796509,
      "learning_rate": 4.9070588235294116e-05,
      "loss": 3.1602,
      "step": 1580
    },
    {
      "epoch": 93.52941176470588,
      "grad_norm": 2.758058786392212,
      "learning_rate": 4.9064705882352946e-05,
      "loss": 3.1577,
      "step": 1590
    },
    {
      "epoch": 94.11764705882354,
      "grad_norm": 3.169994831085205,
      "learning_rate": 4.905882352941177e-05,
      "loss": 3.1277,
      "step": 1600
    },
    {
      "epoch": 94.70588235294117,
      "grad_norm": 2.7944393157958984,
      "learning_rate": 4.905294117647059e-05,
      "loss": 3.2421,
      "step": 1610
    },
    {
      "epoch": 95.29411764705883,
      "grad_norm": 2.7212445735931396,
      "learning_rate": 4.9047058823529415e-05,
      "loss": 3.158,
      "step": 1620
    },
    {
      "epoch": 95.88235294117646,
      "grad_norm": 2.682387113571167,
      "learning_rate": 4.904117647058824e-05,
      "loss": 3.1883,
      "step": 1630
    },
    {
      "epoch": 96.47058823529412,
      "grad_norm": 3.4207191467285156,
      "learning_rate": 4.903529411764706e-05,
      "loss": 3.1005,
      "step": 1640
    },
    {
      "epoch": 97.05882352941177,
      "grad_norm": 2.659213066101074,
      "learning_rate": 4.9029411764705883e-05,
      "loss": 3.123,
      "step": 1650
    },
    {
      "epoch": 97.6470588235294,
      "grad_norm": 2.732632637023926,
      "learning_rate": 4.9023529411764706e-05,
      "loss": 3.2703,
      "step": 1660
    },
    {
      "epoch": 98.23529411764706,
      "grad_norm": 2.8228988647460938,
      "learning_rate": 4.901764705882353e-05,
      "loss": 2.9092,
      "step": 1670
    },
    {
      "epoch": 98.82352941176471,
      "grad_norm": 2.8090803623199463,
      "learning_rate": 4.901176470588236e-05,
      "loss": 3.3426,
      "step": 1680
    },
    {
      "epoch": 99.41176470588235,
      "grad_norm": 2.780756950378418,
      "learning_rate": 4.900588235294118e-05,
      "loss": 3.1135,
      "step": 1690
    },
    {
      "epoch": 100.0,
      "grad_norm": 3.3822314739227295,
      "learning_rate": 4.9e-05,
      "loss": 3.123,
      "step": 1700
    },
    {
      "epoch": 100.58823529411765,
      "grad_norm": 3.334406614303589,
      "learning_rate": 4.899411764705882e-05,
      "loss": 3.0232,
      "step": 1710
    },
    {
      "epoch": 101.17647058823529,
      "grad_norm": 2.5205721855163574,
      "learning_rate": 4.898823529411765e-05,
      "loss": 3.0762,
      "step": 1720
    },
    {
      "epoch": 101.76470588235294,
      "grad_norm": 3.3293910026550293,
      "learning_rate": 4.8982352941176474e-05,
      "loss": 3.0037,
      "step": 1730
    },
    {
      "epoch": 102.3529411764706,
      "grad_norm": 3.606463670730591,
      "learning_rate": 4.89764705882353e-05,
      "loss": 3.0769,
      "step": 1740
    },
    {
      "epoch": 102.94117647058823,
      "grad_norm": 3.248462677001953,
      "learning_rate": 4.897058823529412e-05,
      "loss": 3.1429,
      "step": 1750
    },
    {
      "epoch": 103.52941176470588,
      "grad_norm": 2.8746843338012695,
      "learning_rate": 4.896470588235294e-05,
      "loss": 3.1161,
      "step": 1760
    },
    {
      "epoch": 104.11764705882354,
      "grad_norm": 3.324528932571411,
      "learning_rate": 4.8958823529411766e-05,
      "loss": 3.1617,
      "step": 1770
    },
    {
      "epoch": 104.70588235294117,
      "grad_norm": 3.2294323444366455,
      "learning_rate": 4.895294117647059e-05,
      "loss": 3.0916,
      "step": 1780
    },
    {
      "epoch": 105.29411764705883,
      "grad_norm": 3.0634899139404297,
      "learning_rate": 4.894705882352941e-05,
      "loss": 3.0269,
      "step": 1790
    },
    {
      "epoch": 105.88235294117646,
      "grad_norm": 2.6934759616851807,
      "learning_rate": 4.894117647058824e-05,
      "loss": 3.1412,
      "step": 1800
    },
    {
      "epoch": 106.47058823529412,
      "grad_norm": 3.6700804233551025,
      "learning_rate": 4.8935294117647065e-05,
      "loss": 3.2062,
      "step": 1810
    },
    {
      "epoch": 107.05882352941177,
      "grad_norm": 2.762955904006958,
      "learning_rate": 4.892941176470589e-05,
      "loss": 3.0975,
      "step": 1820
    },
    {
      "epoch": 107.6470588235294,
      "grad_norm": 2.5627360343933105,
      "learning_rate": 4.8923529411764704e-05,
      "loss": 2.9297,
      "step": 1830
    },
    {
      "epoch": 108.23529411764706,
      "grad_norm": 3.2666683197021484,
      "learning_rate": 4.891764705882353e-05,
      "loss": 3.0978,
      "step": 1840
    },
    {
      "epoch": 108.82352941176471,
      "grad_norm": 2.977022409439087,
      "learning_rate": 4.891176470588236e-05,
      "loss": 3.0191,
      "step": 1850
    },
    {
      "epoch": 109.41176470588235,
      "grad_norm": 3.0391476154327393,
      "learning_rate": 4.890588235294118e-05,
      "loss": 3.1866,
      "step": 1860
    },
    {
      "epoch": 110.0,
      "grad_norm": 3.176281452178955,
      "learning_rate": 4.89e-05,
      "loss": 3.0537,
      "step": 1870
    },
    {
      "epoch": 110.58823529411765,
      "grad_norm": 2.8483099937438965,
      "learning_rate": 4.8894117647058826e-05,
      "loss": 3.1018,
      "step": 1880
    },
    {
      "epoch": 111.17647058823529,
      "grad_norm": 2.9466443061828613,
      "learning_rate": 4.888823529411765e-05,
      "loss": 3.1327,
      "step": 1890
    },
    {
      "epoch": 111.76470588235294,
      "grad_norm": 3.2105181217193604,
      "learning_rate": 4.888235294117647e-05,
      "loss": 3.0849,
      "step": 1900
    },
    {
      "epoch": 112.3529411764706,
      "grad_norm": 2.897418260574341,
      "learning_rate": 4.8876470588235295e-05,
      "loss": 3.1064,
      "step": 1910
    },
    {
      "epoch": 112.94117647058823,
      "grad_norm": 3.592863082885742,
      "learning_rate": 4.887058823529412e-05,
      "loss": 3.2211,
      "step": 1920
    },
    {
      "epoch": 113.52941176470588,
      "grad_norm": 2.6678085327148438,
      "learning_rate": 4.886470588235295e-05,
      "loss": 3.1005,
      "step": 1930
    },
    {
      "epoch": 114.11764705882354,
      "grad_norm": 3.553380012512207,
      "learning_rate": 4.885882352941177e-05,
      "loss": 3.1036,
      "step": 1940
    },
    {
      "epoch": 114.70588235294117,
      "grad_norm": 3.021162271499634,
      "learning_rate": 4.8852941176470593e-05,
      "loss": 3.2085,
      "step": 1950
    },
    {
      "epoch": 115.29411764705883,
      "grad_norm": 2.5943145751953125,
      "learning_rate": 4.884705882352941e-05,
      "loss": 3.1047,
      "step": 1960
    },
    {
      "epoch": 115.88235294117646,
      "grad_norm": 2.7958946228027344,
      "learning_rate": 4.884117647058824e-05,
      "loss": 3.1877,
      "step": 1970
    },
    {
      "epoch": 116.47058823529412,
      "grad_norm": 3.0888824462890625,
      "learning_rate": 4.883529411764706e-05,
      "loss": 3.1914,
      "step": 1980
    },
    {
      "epoch": 117.05882352941177,
      "grad_norm": 3.4439120292663574,
      "learning_rate": 4.8829411764705885e-05,
      "loss": 3.0935,
      "step": 1990
    },
    {
      "epoch": 117.6470588235294,
      "grad_norm": 3.2509076595306396,
      "learning_rate": 4.882352941176471e-05,
      "loss": 3.2264,
      "step": 2000
    },
    {
      "epoch": 118.23529411764706,
      "grad_norm": 3.4436681270599365,
      "learning_rate": 4.881764705882353e-05,
      "loss": 3.0125,
      "step": 2010
    },
    {
      "epoch": 118.82352941176471,
      "grad_norm": 3.104825019836426,
      "learning_rate": 4.8811764705882354e-05,
      "loss": 3.1917,
      "step": 2020
    },
    {
      "epoch": 119.41176470588235,
      "grad_norm": 2.8028512001037598,
      "learning_rate": 4.880588235294118e-05,
      "loss": 3.1546,
      "step": 2030
    },
    {
      "epoch": 120.0,
      "grad_norm": 3.163855791091919,
      "learning_rate": 4.88e-05,
      "loss": 3.071,
      "step": 2040
    },
    {
      "epoch": 120.58823529411765,
      "grad_norm": 3.051111936569214,
      "learning_rate": 4.879411764705882e-05,
      "loss": 3.0174,
      "step": 2050
    },
    {
      "epoch": 121.17647058823529,
      "grad_norm": 3.6283864974975586,
      "learning_rate": 4.878823529411765e-05,
      "loss": 2.9671,
      "step": 2060
    },
    {
      "epoch": 121.76470588235294,
      "grad_norm": 3.301435708999634,
      "learning_rate": 4.8782352941176476e-05,
      "loss": 3.0721,
      "step": 2070
    },
    {
      "epoch": 122.3529411764706,
      "grad_norm": 3.4575884342193604,
      "learning_rate": 4.87764705882353e-05,
      "loss": 2.9524,
      "step": 2080
    },
    {
      "epoch": 122.94117647058823,
      "grad_norm": 2.831554651260376,
      "learning_rate": 4.8770588235294115e-05,
      "loss": 3.2736,
      "step": 2090
    },
    {
      "epoch": 123.52941176470588,
      "grad_norm": 2.590301275253296,
      "learning_rate": 4.8764705882352945e-05,
      "loss": 3.0175,
      "step": 2100
    },
    {
      "epoch": 124.11764705882354,
      "grad_norm": 2.99053692817688,
      "learning_rate": 4.875882352941177e-05,
      "loss": 3.1775,
      "step": 2110
    },
    {
      "epoch": 124.70588235294117,
      "grad_norm": 3.2906055450439453,
      "learning_rate": 4.875294117647059e-05,
      "loss": 2.9974,
      "step": 2120
    },
    {
      "epoch": 125.29411764705883,
      "grad_norm": 2.1813292503356934,
      "learning_rate": 4.8747058823529414e-05,
      "loss": 2.9485,
      "step": 2130
    },
    {
      "epoch": 125.88235294117646,
      "grad_norm": 3.2913362979888916,
      "learning_rate": 4.874117647058824e-05,
      "loss": 3.1076,
      "step": 2140
    },
    {
      "epoch": 126.47058823529412,
      "grad_norm": 3.4082107543945312,
      "learning_rate": 4.873529411764706e-05,
      "loss": 3.0725,
      "step": 2150
    },
    {
      "epoch": 127.05882352941177,
      "grad_norm": 4.112016201019287,
      "learning_rate": 4.872941176470588e-05,
      "loss": 3.0149,
      "step": 2160
    },
    {
      "epoch": 127.6470588235294,
      "grad_norm": 3.229750633239746,
      "learning_rate": 4.8723529411764706e-05,
      "loss": 2.948,
      "step": 2170
    },
    {
      "epoch": 128.23529411764707,
      "grad_norm": 2.930748462677002,
      "learning_rate": 4.871764705882353e-05,
      "loss": 3.2139,
      "step": 2180
    },
    {
      "epoch": 128.8235294117647,
      "grad_norm": 2.91316556930542,
      "learning_rate": 4.871176470588236e-05,
      "loss": 3.1403,
      "step": 2190
    },
    {
      "epoch": 129.41176470588235,
      "grad_norm": 2.8961498737335205,
      "learning_rate": 4.870588235294118e-05,
      "loss": 2.8841,
      "step": 2200
    },
    {
      "epoch": 130.0,
      "grad_norm": 4.187008380889893,
      "learning_rate": 4.87e-05,
      "loss": 2.9597,
      "step": 2210
    },
    {
      "epoch": 130.58823529411765,
      "grad_norm": 3.9254677295684814,
      "learning_rate": 4.869411764705882e-05,
      "loss": 3.1839,
      "step": 2220
    },
    {
      "epoch": 131.1764705882353,
      "grad_norm": 3.6448006629943848,
      "learning_rate": 4.868823529411765e-05,
      "loss": 3.0981,
      "step": 2230
    },
    {
      "epoch": 131.76470588235293,
      "grad_norm": 2.9934628009796143,
      "learning_rate": 4.8682352941176474e-05,
      "loss": 3.0923,
      "step": 2240
    },
    {
      "epoch": 132.35294117647058,
      "grad_norm": 3.6719107627868652,
      "learning_rate": 4.86764705882353e-05,
      "loss": 3.0445,
      "step": 2250
    },
    {
      "epoch": 132.94117647058823,
      "grad_norm": 3.756920576095581,
      "learning_rate": 4.867058823529412e-05,
      "loss": 3.1902,
      "step": 2260
    },
    {
      "epoch": 133.52941176470588,
      "grad_norm": 3.426804780960083,
      "learning_rate": 4.866470588235294e-05,
      "loss": 3.1362,
      "step": 2270
    },
    {
      "epoch": 134.11764705882354,
      "grad_norm": 2.8260786533355713,
      "learning_rate": 4.8658823529411766e-05,
      "loss": 3.0968,
      "step": 2280
    },
    {
      "epoch": 134.7058823529412,
      "grad_norm": 3.6525661945343018,
      "learning_rate": 4.865294117647059e-05,
      "loss": 2.9784,
      "step": 2290
    },
    {
      "epoch": 135.2941176470588,
      "grad_norm": 3.6099438667297363,
      "learning_rate": 4.864705882352941e-05,
      "loss": 3.0995,
      "step": 2300
    },
    {
      "epoch": 135.88235294117646,
      "grad_norm": 3.271939992904663,
      "learning_rate": 4.864117647058824e-05,
      "loss": 3.0947,
      "step": 2310
    },
    {
      "epoch": 136.47058823529412,
      "grad_norm": 2.714066743850708,
      "learning_rate": 4.8635294117647064e-05,
      "loss": 3.1905,
      "step": 2320
    },
    {
      "epoch": 137.05882352941177,
      "grad_norm": 3.6124675273895264,
      "learning_rate": 4.862941176470589e-05,
      "loss": 3.2156,
      "step": 2330
    },
    {
      "epoch": 137.64705882352942,
      "grad_norm": 3.69584321975708,
      "learning_rate": 4.8623529411764704e-05,
      "loss": 3.0716,
      "step": 2340
    },
    {
      "epoch": 138.23529411764707,
      "grad_norm": 3.540010690689087,
      "learning_rate": 4.861764705882353e-05,
      "loss": 3.1647,
      "step": 2350
    },
    {
      "epoch": 138.8235294117647,
      "grad_norm": 2.6739606857299805,
      "learning_rate": 4.8611764705882356e-05,
      "loss": 3.0973,
      "step": 2360
    },
    {
      "epoch": 139.41176470588235,
      "grad_norm": 3.3222241401672363,
      "learning_rate": 4.860588235294118e-05,
      "loss": 3.0393,
      "step": 2370
    },
    {
      "epoch": 140.0,
      "grad_norm": 3.6540656089782715,
      "learning_rate": 4.86e-05,
      "loss": 3.1193,
      "step": 2380
    },
    {
      "epoch": 140.58823529411765,
      "grad_norm": 2.976029634475708,
      "learning_rate": 4.8594117647058825e-05,
      "loss": 3.01,
      "step": 2390
    },
    {
      "epoch": 141.1764705882353,
      "grad_norm": 2.8954553604125977,
      "learning_rate": 4.858823529411765e-05,
      "loss": 3.0374,
      "step": 2400
    },
    {
      "epoch": 141.76470588235293,
      "grad_norm": 3.0465219020843506,
      "learning_rate": 4.858235294117647e-05,
      "loss": 3.0671,
      "step": 2410
    },
    {
      "epoch": 142.35294117647058,
      "grad_norm": 3.1098926067352295,
      "learning_rate": 4.8576470588235294e-05,
      "loss": 2.858,
      "step": 2420
    },
    {
      "epoch": 142.94117647058823,
      "grad_norm": 2.7822437286376953,
      "learning_rate": 4.857058823529412e-05,
      "loss": 3.1116,
      "step": 2430
    },
    {
      "epoch": 143.52941176470588,
      "grad_norm": 3.1653330326080322,
      "learning_rate": 4.856470588235295e-05,
      "loss": 2.9148,
      "step": 2440
    },
    {
      "epoch": 144.11764705882354,
      "grad_norm": 3.2513186931610107,
      "learning_rate": 4.855882352941177e-05,
      "loss": 3.0611,
      "step": 2450
    },
    {
      "epoch": 144.7058823529412,
      "grad_norm": 2.8771848678588867,
      "learning_rate": 4.855294117647059e-05,
      "loss": 3.0061,
      "step": 2460
    },
    {
      "epoch": 145.2941176470588,
      "grad_norm": 3.1035115718841553,
      "learning_rate": 4.854705882352941e-05,
      "loss": 3.0027,
      "step": 2470
    },
    {
      "epoch": 145.88235294117646,
      "grad_norm": 3.764676332473755,
      "learning_rate": 4.854117647058824e-05,
      "loss": 2.9875,
      "step": 2480
    },
    {
      "epoch": 146.47058823529412,
      "grad_norm": 3.406752109527588,
      "learning_rate": 4.853529411764706e-05,
      "loss": 3.0109,
      "step": 2490
    },
    {
      "epoch": 147.05882352941177,
      "grad_norm": 3.5917251110076904,
      "learning_rate": 4.8529411764705885e-05,
      "loss": 2.9603,
      "step": 2500
    },
    {
      "epoch": 147.64705882352942,
      "grad_norm": 4.207528114318848,
      "learning_rate": 4.852352941176471e-05,
      "loss": 2.914,
      "step": 2510
    },
    {
      "epoch": 148.23529411764707,
      "grad_norm": 3.049720048904419,
      "learning_rate": 4.851764705882354e-05,
      "loss": 3.0604,
      "step": 2520
    },
    {
      "epoch": 148.8235294117647,
      "grad_norm": 3.1011998653411865,
      "learning_rate": 4.8511764705882354e-05,
      "loss": 3.0557,
      "step": 2530
    },
    {
      "epoch": 149.41176470588235,
      "grad_norm": 2.986093044281006,
      "learning_rate": 4.850588235294118e-05,
      "loss": 2.8939,
      "step": 2540
    },
    {
      "epoch": 150.0,
      "grad_norm": 4.601662635803223,
      "learning_rate": 4.85e-05,
      "loss": 2.9528,
      "step": 2550
    },
    {
      "epoch": 150.58823529411765,
      "grad_norm": 3.0506832599639893,
      "learning_rate": 4.849411764705882e-05,
      "loss": 3.0253,
      "step": 2560
    },
    {
      "epoch": 151.1764705882353,
      "grad_norm": 3.191168785095215,
      "learning_rate": 4.848823529411765e-05,
      "loss": 3.2311,
      "step": 2570
    },
    {
      "epoch": 151.76470588235293,
      "grad_norm": 2.8057668209075928,
      "learning_rate": 4.8482352941176476e-05,
      "loss": 2.9655,
      "step": 2580
    },
    {
      "epoch": 152.35294117647058,
      "grad_norm": 3.0275020599365234,
      "learning_rate": 4.84764705882353e-05,
      "loss": 2.9121,
      "step": 2590
    },
    {
      "epoch": 152.94117647058823,
      "grad_norm": 3.3310351371765137,
      "learning_rate": 4.8470588235294115e-05,
      "loss": 2.9442,
      "step": 2600
    },
    {
      "epoch": 153.52941176470588,
      "grad_norm": 2.855036497116089,
      "learning_rate": 4.8464705882352945e-05,
      "loss": 2.9716,
      "step": 2610
    },
    {
      "epoch": 154.11764705882354,
      "grad_norm": 3.0060291290283203,
      "learning_rate": 4.845882352941177e-05,
      "loss": 3.0141,
      "step": 2620
    },
    {
      "epoch": 154.7058823529412,
      "grad_norm": 4.530875205993652,
      "learning_rate": 4.845294117647059e-05,
      "loss": 3.1076,
      "step": 2630
    },
    {
      "epoch": 155.2941176470588,
      "grad_norm": 3.424205780029297,
      "learning_rate": 4.8447058823529413e-05,
      "loss": 2.9426,
      "step": 2640
    },
    {
      "epoch": 155.88235294117646,
      "grad_norm": 3.3968288898468018,
      "learning_rate": 4.8441176470588236e-05,
      "loss": 3.0285,
      "step": 2650
    },
    {
      "epoch": 156.47058823529412,
      "grad_norm": 5.38083553314209,
      "learning_rate": 4.843529411764706e-05,
      "loss": 2.952,
      "step": 2660
    },
    {
      "epoch": 157.05882352941177,
      "grad_norm": 3.90187406539917,
      "learning_rate": 4.842941176470588e-05,
      "loss": 3.0159,
      "step": 2670
    },
    {
      "epoch": 157.64705882352942,
      "grad_norm": 3.5614206790924072,
      "learning_rate": 4.8423529411764705e-05,
      "loss": 2.8904,
      "step": 2680
    },
    {
      "epoch": 158.23529411764707,
      "grad_norm": 3.213500738143921,
      "learning_rate": 4.8417647058823535e-05,
      "loss": 3.0757,
      "step": 2690
    },
    {
      "epoch": 158.8235294117647,
      "grad_norm": 2.5062074661254883,
      "learning_rate": 4.841176470588236e-05,
      "loss": 3.0505,
      "step": 2700
    },
    {
      "epoch": 159.41176470588235,
      "grad_norm": 2.8633410930633545,
      "learning_rate": 4.840588235294118e-05,
      "loss": 2.9721,
      "step": 2710
    },
    {
      "epoch": 160.0,
      "grad_norm": 3.7849607467651367,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 2.9143,
      "step": 2720
    },
    {
      "epoch": 160.58823529411765,
      "grad_norm": 2.772167682647705,
      "learning_rate": 4.839411764705882e-05,
      "loss": 3.0057,
      "step": 2730
    },
    {
      "epoch": 161.1764705882353,
      "grad_norm": 3.7353127002716064,
      "learning_rate": 4.838823529411765e-05,
      "loss": 2.9752,
      "step": 2740
    },
    {
      "epoch": 161.76470588235293,
      "grad_norm": 3.767813205718994,
      "learning_rate": 4.838235294117647e-05,
      "loss": 2.9338,
      "step": 2750
    },
    {
      "epoch": 162.35294117647058,
      "grad_norm": 3.063279151916504,
      "learning_rate": 4.8376470588235296e-05,
      "loss": 2.9945,
      "step": 2760
    },
    {
      "epoch": 162.94117647058823,
      "grad_norm": 2.9907708168029785,
      "learning_rate": 4.837058823529412e-05,
      "loss": 3.0618,
      "step": 2770
    },
    {
      "epoch": 163.52941176470588,
      "grad_norm": 2.8468592166900635,
      "learning_rate": 4.836470588235294e-05,
      "loss": 2.927,
      "step": 2780
    },
    {
      "epoch": 164.11764705882354,
      "grad_norm": 4.056343078613281,
      "learning_rate": 4.8358823529411765e-05,
      "loss": 3.0198,
      "step": 2790
    },
    {
      "epoch": 164.7058823529412,
      "grad_norm": 2.9880025386810303,
      "learning_rate": 4.835294117647059e-05,
      "loss": 3.0774,
      "step": 2800
    },
    {
      "epoch": 165.2941176470588,
      "grad_norm": 3.395373582839966,
      "learning_rate": 4.834705882352941e-05,
      "loss": 3.0695,
      "step": 2810
    },
    {
      "epoch": 165.88235294117646,
      "grad_norm": 3.3616650104522705,
      "learning_rate": 4.834117647058824e-05,
      "loss": 2.9689,
      "step": 2820
    },
    {
      "epoch": 166.47058823529412,
      "grad_norm": 4.297884941101074,
      "learning_rate": 4.8335294117647064e-05,
      "loss": 3.0834,
      "step": 2830
    },
    {
      "epoch": 167.05882352941177,
      "grad_norm": 3.3330538272857666,
      "learning_rate": 4.832941176470589e-05,
      "loss": 3.05,
      "step": 2840
    },
    {
      "epoch": 167.64705882352942,
      "grad_norm": 3.619223117828369,
      "learning_rate": 4.83235294117647e-05,
      "loss": 2.933,
      "step": 2850
    },
    {
      "epoch": 168.23529411764707,
      "grad_norm": 3.7003862857818604,
      "learning_rate": 4.831764705882353e-05,
      "loss": 3.0468,
      "step": 2860
    },
    {
      "epoch": 168.8235294117647,
      "grad_norm": 3.4298791885375977,
      "learning_rate": 4.8311764705882356e-05,
      "loss": 3.2237,
      "step": 2870
    },
    {
      "epoch": 169.41176470588235,
      "grad_norm": 5.004811763763428,
      "learning_rate": 4.830588235294118e-05,
      "loss": 2.9395,
      "step": 2880
    },
    {
      "epoch": 170.0,
      "grad_norm": 3.9604268074035645,
      "learning_rate": 4.83e-05,
      "loss": 3.076,
      "step": 2890
    },
    {
      "epoch": 170.58823529411765,
      "grad_norm": 3.464660167694092,
      "learning_rate": 4.829411764705883e-05,
      "loss": 2.9085,
      "step": 2900
    },
    {
      "epoch": 171.1764705882353,
      "grad_norm": 3.2978599071502686,
      "learning_rate": 4.828823529411765e-05,
      "loss": 3.1254,
      "step": 2910
    },
    {
      "epoch": 171.76470588235293,
      "grad_norm": 3.498880624771118,
      "learning_rate": 4.828235294117647e-05,
      "loss": 2.8965,
      "step": 2920
    },
    {
      "epoch": 172.35294117647058,
      "grad_norm": 5.108408451080322,
      "learning_rate": 4.8276470588235294e-05,
      "loss": 3.0719,
      "step": 2930
    },
    {
      "epoch": 172.94117647058823,
      "grad_norm": 3.7920453548431396,
      "learning_rate": 4.827058823529412e-05,
      "loss": 3.0761,
      "step": 2940
    },
    {
      "epoch": 173.52941176470588,
      "grad_norm": 3.5303304195404053,
      "learning_rate": 4.8264705882352946e-05,
      "loss": 3.0164,
      "step": 2950
    },
    {
      "epoch": 174.11764705882354,
      "grad_norm": 3.1321070194244385,
      "learning_rate": 4.825882352941177e-05,
      "loss": 2.987,
      "step": 2960
    },
    {
      "epoch": 174.7058823529412,
      "grad_norm": 3.242957830429077,
      "learning_rate": 4.825294117647059e-05,
      "loss": 3.0272,
      "step": 2970
    },
    {
      "epoch": 175.2941176470588,
      "grad_norm": 3.260582208633423,
      "learning_rate": 4.824705882352941e-05,
      "loss": 3.0916,
      "step": 2980
    },
    {
      "epoch": 175.88235294117646,
      "grad_norm": 3.9867630004882812,
      "learning_rate": 4.824117647058824e-05,
      "loss": 3.1026,
      "step": 2990
    },
    {
      "epoch": 176.47058823529412,
      "grad_norm": 3.2209455966949463,
      "learning_rate": 4.823529411764706e-05,
      "loss": 2.9608,
      "step": 3000
    },
    {
      "epoch": 177.05882352941177,
      "grad_norm": 4.837620735168457,
      "learning_rate": 4.8229411764705884e-05,
      "loss": 3.0289,
      "step": 3010
    },
    {
      "epoch": 177.64705882352942,
      "grad_norm": 4.41813850402832,
      "learning_rate": 4.822352941176471e-05,
      "loss": 3.0131,
      "step": 3020
    },
    {
      "epoch": 178.23529411764707,
      "grad_norm": 3.690789222717285,
      "learning_rate": 4.821764705882354e-05,
      "loss": 3.0071,
      "step": 3030
    },
    {
      "epoch": 178.8235294117647,
      "grad_norm": 5.023203372955322,
      "learning_rate": 4.821176470588235e-05,
      "loss": 2.9683,
      "step": 3040
    },
    {
      "epoch": 179.41176470588235,
      "grad_norm": 3.6746597290039062,
      "learning_rate": 4.8205882352941176e-05,
      "loss": 2.936,
      "step": 3050
    },
    {
      "epoch": 180.0,
      "grad_norm": 3.3441736698150635,
      "learning_rate": 4.82e-05,
      "loss": 2.8881,
      "step": 3060
    },
    {
      "epoch": 180.58823529411765,
      "grad_norm": 3.0922372341156006,
      "learning_rate": 4.819411764705883e-05,
      "loss": 2.8755,
      "step": 3070
    },
    {
      "epoch": 181.1764705882353,
      "grad_norm": 4.714075565338135,
      "learning_rate": 4.818823529411765e-05,
      "loss": 3.0088,
      "step": 3080
    },
    {
      "epoch": 181.76470588235293,
      "grad_norm": 3.669490337371826,
      "learning_rate": 4.8182352941176475e-05,
      "loss": 3.0301,
      "step": 3090
    },
    {
      "epoch": 182.35294117647058,
      "grad_norm": 3.1949145793914795,
      "learning_rate": 4.81764705882353e-05,
      "loss": 2.906,
      "step": 3100
    },
    {
      "epoch": 182.94117647058823,
      "grad_norm": 4.224120616912842,
      "learning_rate": 4.8170588235294114e-05,
      "loss": 2.9305,
      "step": 3110
    },
    {
      "epoch": 183.52941176470588,
      "grad_norm": 3.4239397048950195,
      "learning_rate": 4.8164705882352944e-05,
      "loss": 3.0093,
      "step": 3120
    },
    {
      "epoch": 184.11764705882354,
      "grad_norm": 3.894881010055542,
      "learning_rate": 4.815882352941177e-05,
      "loss": 3.0406,
      "step": 3130
    },
    {
      "epoch": 184.7058823529412,
      "grad_norm": 3.413724184036255,
      "learning_rate": 4.815294117647059e-05,
      "loss": 2.8786,
      "step": 3140
    },
    {
      "epoch": 185.2941176470588,
      "grad_norm": 3.187991142272949,
      "learning_rate": 4.814705882352941e-05,
      "loss": 3.0461,
      "step": 3150
    },
    {
      "epoch": 185.88235294117646,
      "grad_norm": 4.068168640136719,
      "learning_rate": 4.814117647058824e-05,
      "loss": 2.9818,
      "step": 3160
    },
    {
      "epoch": 186.47058823529412,
      "grad_norm": 3.3262529373168945,
      "learning_rate": 4.813529411764706e-05,
      "loss": 2.9318,
      "step": 3170
    },
    {
      "epoch": 187.05882352941177,
      "grad_norm": 3.6315298080444336,
      "learning_rate": 4.812941176470588e-05,
      "loss": 2.993,
      "step": 3180
    },
    {
      "epoch": 187.64705882352942,
      "grad_norm": 3.1630237102508545,
      "learning_rate": 4.8123529411764705e-05,
      "loss": 2.9874,
      "step": 3190
    },
    {
      "epoch": 188.23529411764707,
      "grad_norm": 2.785853624343872,
      "learning_rate": 4.8117647058823535e-05,
      "loss": 2.9212,
      "step": 3200
    },
    {
      "epoch": 188.8235294117647,
      "grad_norm": 3.3863425254821777,
      "learning_rate": 4.811176470588236e-05,
      "loss": 3.1209,
      "step": 3210
    },
    {
      "epoch": 189.41176470588235,
      "grad_norm": 3.59043550491333,
      "learning_rate": 4.810588235294118e-05,
      "loss": 2.9435,
      "step": 3220
    },
    {
      "epoch": 190.0,
      "grad_norm": 3.9283556938171387,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 2.9418,
      "step": 3230
    },
    {
      "epoch": 190.58823529411765,
      "grad_norm": 3.1138267517089844,
      "learning_rate": 4.809411764705883e-05,
      "loss": 2.8888,
      "step": 3240
    },
    {
      "epoch": 191.1764705882353,
      "grad_norm": 3.812023162841797,
      "learning_rate": 4.808823529411765e-05,
      "loss": 2.9547,
      "step": 3250
    },
    {
      "epoch": 191.76470588235293,
      "grad_norm": 3.201526641845703,
      "learning_rate": 4.808235294117647e-05,
      "loss": 3.0606,
      "step": 3260
    },
    {
      "epoch": 192.35294117647058,
      "grad_norm": 3.667320966720581,
      "learning_rate": 4.8076470588235296e-05,
      "loss": 2.9827,
      "step": 3270
    },
    {
      "epoch": 192.94117647058823,
      "grad_norm": 3.506173849105835,
      "learning_rate": 4.807058823529412e-05,
      "loss": 2.9989,
      "step": 3280
    },
    {
      "epoch": 193.52941176470588,
      "grad_norm": 3.0723228454589844,
      "learning_rate": 4.806470588235295e-05,
      "loss": 3.094,
      "step": 3290
    },
    {
      "epoch": 194.11764705882354,
      "grad_norm": 5.171865940093994,
      "learning_rate": 4.8058823529411765e-05,
      "loss": 3.0707,
      "step": 3300
    },
    {
      "epoch": 194.7058823529412,
      "grad_norm": 3.923550844192505,
      "learning_rate": 4.805294117647059e-05,
      "loss": 2.9154,
      "step": 3310
    },
    {
      "epoch": 195.2941176470588,
      "grad_norm": 3.4371159076690674,
      "learning_rate": 4.804705882352941e-05,
      "loss": 3.0338,
      "step": 3320
    },
    {
      "epoch": 195.88235294117646,
      "grad_norm": 3.465468168258667,
      "learning_rate": 4.804117647058824e-05,
      "loss": 2.9464,
      "step": 3330
    },
    {
      "epoch": 196.47058823529412,
      "grad_norm": 3.456350564956665,
      "learning_rate": 4.803529411764706e-05,
      "loss": 2.9497,
      "step": 3340
    },
    {
      "epoch": 197.05882352941177,
      "grad_norm": 2.9678499698638916,
      "learning_rate": 4.8029411764705886e-05,
      "loss": 3.1213,
      "step": 3350
    },
    {
      "epoch": 197.64705882352942,
      "grad_norm": 3.976201295852661,
      "learning_rate": 4.802352941176471e-05,
      "loss": 2.931,
      "step": 3360
    },
    {
      "epoch": 198.23529411764707,
      "grad_norm": 3.7065224647521973,
      "learning_rate": 4.801764705882353e-05,
      "loss": 2.9316,
      "step": 3370
    },
    {
      "epoch": 198.8235294117647,
      "grad_norm": 3.388458728790283,
      "learning_rate": 4.8011764705882355e-05,
      "loss": 3.0522,
      "step": 3380
    },
    {
      "epoch": 199.41176470588235,
      "grad_norm": 3.5575153827667236,
      "learning_rate": 4.800588235294118e-05,
      "loss": 2.8955,
      "step": 3390
    },
    {
      "epoch": 200.0,
      "grad_norm": 4.319662570953369,
      "learning_rate": 4.8e-05,
      "loss": 3.0891,
      "step": 3400
    },
    {
      "epoch": 200.58823529411765,
      "grad_norm": 4.028493881225586,
      "learning_rate": 4.799411764705883e-05,
      "loss": 2.8844,
      "step": 3410
    },
    {
      "epoch": 201.1764705882353,
      "grad_norm": 4.132038116455078,
      "learning_rate": 4.798823529411765e-05,
      "loss": 2.9003,
      "step": 3420
    },
    {
      "epoch": 201.76470588235293,
      "grad_norm": 3.452871084213257,
      "learning_rate": 4.798235294117647e-05,
      "loss": 2.9614,
      "step": 3430
    },
    {
      "epoch": 202.35294117647058,
      "grad_norm": 4.360397815704346,
      "learning_rate": 4.797647058823529e-05,
      "loss": 3.0247,
      "step": 3440
    },
    {
      "epoch": 202.94117647058823,
      "grad_norm": 3.399099826812744,
      "learning_rate": 4.797058823529412e-05,
      "loss": 3.0223,
      "step": 3450
    },
    {
      "epoch": 203.52941176470588,
      "grad_norm": 4.024383068084717,
      "learning_rate": 4.7964705882352946e-05,
      "loss": 2.8943,
      "step": 3460
    },
    {
      "epoch": 204.11764705882354,
      "grad_norm": 3.259287118911743,
      "learning_rate": 4.795882352941177e-05,
      "loss": 2.9886,
      "step": 3470
    },
    {
      "epoch": 204.7058823529412,
      "grad_norm": 3.901900053024292,
      "learning_rate": 4.795294117647059e-05,
      "loss": 3.0189,
      "step": 3480
    },
    {
      "epoch": 205.2941176470588,
      "grad_norm": 3.0797958374023438,
      "learning_rate": 4.7947058823529415e-05,
      "loss": 2.9998,
      "step": 3490
    },
    {
      "epoch": 205.88235294117646,
      "grad_norm": 4.186898231506348,
      "learning_rate": 4.794117647058824e-05,
      "loss": 2.9083,
      "step": 3500
    },
    {
      "epoch": 206.47058823529412,
      "grad_norm": 4.275518894195557,
      "learning_rate": 4.793529411764706e-05,
      "loss": 2.978,
      "step": 3510
    },
    {
      "epoch": 207.05882352941177,
      "grad_norm": 4.014444828033447,
      "learning_rate": 4.7929411764705884e-05,
      "loss": 3.0155,
      "step": 3520
    },
    {
      "epoch": 207.64705882352942,
      "grad_norm": 4.6688947677612305,
      "learning_rate": 4.792352941176471e-05,
      "loss": 2.8727,
      "step": 3530
    },
    {
      "epoch": 208.23529411764707,
      "grad_norm": 3.5561511516571045,
      "learning_rate": 4.7917647058823537e-05,
      "loss": 2.9751,
      "step": 3540
    },
    {
      "epoch": 208.8235294117647,
      "grad_norm": 4.459389686584473,
      "learning_rate": 4.791176470588235e-05,
      "loss": 2.9693,
      "step": 3550
    },
    {
      "epoch": 209.41176470588235,
      "grad_norm": 3.4378576278686523,
      "learning_rate": 4.7905882352941176e-05,
      "loss": 3.1088,
      "step": 3560
    },
    {
      "epoch": 210.0,
      "grad_norm": 4.522156715393066,
      "learning_rate": 4.79e-05,
      "loss": 3.0546,
      "step": 3570
    },
    {
      "epoch": 210.58823529411765,
      "grad_norm": 4.222122669219971,
      "learning_rate": 4.789411764705883e-05,
      "loss": 2.9373,
      "step": 3580
    },
    {
      "epoch": 211.1764705882353,
      "grad_norm": 4.0556511878967285,
      "learning_rate": 4.788823529411765e-05,
      "loss": 2.7396,
      "step": 3590
    },
    {
      "epoch": 211.76470588235293,
      "grad_norm": 3.9158267974853516,
      "learning_rate": 4.7882352941176475e-05,
      "loss": 2.9178,
      "step": 3600
    },
    {
      "epoch": 212.35294117647058,
      "grad_norm": 2.9676382541656494,
      "learning_rate": 4.78764705882353e-05,
      "loss": 2.8952,
      "step": 3610
    },
    {
      "epoch": 212.94117647058823,
      "grad_norm": 2.8783724308013916,
      "learning_rate": 4.787058823529412e-05,
      "loss": 2.7745,
      "step": 3620
    },
    {
      "epoch": 213.52941176470588,
      "grad_norm": 3.568453073501587,
      "learning_rate": 4.7864705882352943e-05,
      "loss": 2.9052,
      "step": 3630
    },
    {
      "epoch": 214.11764705882354,
      "grad_norm": 3.898195266723633,
      "learning_rate": 4.7858823529411766e-05,
      "loss": 2.9008,
      "step": 3640
    },
    {
      "epoch": 214.7058823529412,
      "grad_norm": 4.179990291595459,
      "learning_rate": 4.785294117647059e-05,
      "loss": 2.9421,
      "step": 3650
    },
    {
      "epoch": 215.2941176470588,
      "grad_norm": 3.3448214530944824,
      "learning_rate": 4.784705882352941e-05,
      "loss": 2.9557,
      "step": 3660
    },
    {
      "epoch": 215.88235294117646,
      "grad_norm": 3.8563289642333984,
      "learning_rate": 4.784117647058824e-05,
      "loss": 2.9081,
      "step": 3670
    },
    {
      "epoch": 216.47058823529412,
      "grad_norm": 4.041563034057617,
      "learning_rate": 4.783529411764706e-05,
      "loss": 2.9895,
      "step": 3680
    },
    {
      "epoch": 217.05882352941177,
      "grad_norm": 4.923456192016602,
      "learning_rate": 4.782941176470588e-05,
      "loss": 2.9211,
      "step": 3690
    },
    {
      "epoch": 217.64705882352942,
      "grad_norm": 3.332519054412842,
      "learning_rate": 4.7823529411764704e-05,
      "loss": 2.9729,
      "step": 3700
    },
    {
      "epoch": 218.23529411764707,
      "grad_norm": 3.0983052253723145,
      "learning_rate": 4.7817647058823534e-05,
      "loss": 3.014,
      "step": 3710
    },
    {
      "epoch": 218.8235294117647,
      "grad_norm": 2.961998224258423,
      "learning_rate": 4.781176470588236e-05,
      "loss": 2.8301,
      "step": 3720
    },
    {
      "epoch": 219.41176470588235,
      "grad_norm": 3.8331186771392822,
      "learning_rate": 4.780588235294118e-05,
      "loss": 2.8979,
      "step": 3730
    },
    {
      "epoch": 220.0,
      "grad_norm": 4.330545902252197,
      "learning_rate": 4.78e-05,
      "loss": 2.9745,
      "step": 3740
    },
    {
      "epoch": 220.58823529411765,
      "grad_norm": 3.8381378650665283,
      "learning_rate": 4.7794117647058826e-05,
      "loss": 2.762,
      "step": 3750
    },
    {
      "epoch": 221.1764705882353,
      "grad_norm": 4.877939224243164,
      "learning_rate": 4.778823529411765e-05,
      "loss": 2.9719,
      "step": 3760
    },
    {
      "epoch": 221.76470588235293,
      "grad_norm": 4.5338616371154785,
      "learning_rate": 4.778235294117647e-05,
      "loss": 3.0175,
      "step": 3770
    },
    {
      "epoch": 222.35294117647058,
      "grad_norm": 3.1548848152160645,
      "learning_rate": 4.7776470588235295e-05,
      "loss": 2.9187,
      "step": 3780
    },
    {
      "epoch": 222.94117647058823,
      "grad_norm": 3.715599298477173,
      "learning_rate": 4.7770588235294125e-05,
      "loss": 2.9484,
      "step": 3790
    },
    {
      "epoch": 223.52941176470588,
      "grad_norm": 2.912149429321289,
      "learning_rate": 4.776470588235295e-05,
      "loss": 2.8678,
      "step": 3800
    },
    {
      "epoch": 224.11764705882354,
      "grad_norm": 4.271626949310303,
      "learning_rate": 4.7758823529411764e-05,
      "loss": 2.929,
      "step": 3810
    },
    {
      "epoch": 224.7058823529412,
      "grad_norm": 4.477458953857422,
      "learning_rate": 4.775294117647059e-05,
      "loss": 2.9555,
      "step": 3820
    },
    {
      "epoch": 225.2941176470588,
      "grad_norm": 3.6284372806549072,
      "learning_rate": 4.774705882352941e-05,
      "loss": 3.0811,
      "step": 3830
    },
    {
      "epoch": 225.88235294117646,
      "grad_norm": 3.7160234451293945,
      "learning_rate": 4.774117647058824e-05,
      "loss": 2.9335,
      "step": 3840
    },
    {
      "epoch": 226.47058823529412,
      "grad_norm": 3.5504775047302246,
      "learning_rate": 4.773529411764706e-05,
      "loss": 2.8994,
      "step": 3850
    },
    {
      "epoch": 227.05882352941177,
      "grad_norm": 3.444197416305542,
      "learning_rate": 4.7729411764705886e-05,
      "loss": 2.8764,
      "step": 3860
    },
    {
      "epoch": 227.64705882352942,
      "grad_norm": 3.8050687313079834,
      "learning_rate": 4.772352941176471e-05,
      "loss": 2.8883,
      "step": 3870
    },
    {
      "epoch": 228.23529411764707,
      "grad_norm": 4.353732585906982,
      "learning_rate": 4.771764705882353e-05,
      "loss": 3.0011,
      "step": 3880
    },
    {
      "epoch": 228.8235294117647,
      "grad_norm": 3.31596040725708,
      "learning_rate": 4.7711764705882355e-05,
      "loss": 2.8396,
      "step": 3890
    },
    {
      "epoch": 229.41176470588235,
      "grad_norm": 3.8105831146240234,
      "learning_rate": 4.770588235294118e-05,
      "loss": 3.0951,
      "step": 3900
    },
    {
      "epoch": 230.0,
      "grad_norm": 4.51469087600708,
      "learning_rate": 4.77e-05,
      "loss": 2.9331,
      "step": 3910
    },
    {
      "epoch": 230.58823529411765,
      "grad_norm": 3.323157548904419,
      "learning_rate": 4.769411764705883e-05,
      "loss": 2.9513,
      "step": 3920
    },
    {
      "epoch": 231.1764705882353,
      "grad_norm": 4.206935882568359,
      "learning_rate": 4.7688235294117653e-05,
      "loss": 2.8175,
      "step": 3930
    },
    {
      "epoch": 231.76470588235293,
      "grad_norm": 4.060103893280029,
      "learning_rate": 4.768235294117647e-05,
      "loss": 2.9288,
      "step": 3940
    },
    {
      "epoch": 232.35294117647058,
      "grad_norm": 3.826586961746216,
      "learning_rate": 4.767647058823529e-05,
      "loss": 2.9974,
      "step": 3950
    },
    {
      "epoch": 232.94117647058823,
      "grad_norm": 4.092954158782959,
      "learning_rate": 4.767058823529412e-05,
      "loss": 2.8477,
      "step": 3960
    },
    {
      "epoch": 233.52941176470588,
      "grad_norm": 3.462071657180786,
      "learning_rate": 4.7664705882352945e-05,
      "loss": 2.9272,
      "step": 3970
    },
    {
      "epoch": 234.11764705882354,
      "grad_norm": 4.6704511642456055,
      "learning_rate": 4.765882352941177e-05,
      "loss": 2.9077,
      "step": 3980
    },
    {
      "epoch": 234.7058823529412,
      "grad_norm": 4.656249046325684,
      "learning_rate": 4.765294117647059e-05,
      "loss": 2.8164,
      "step": 3990
    },
    {
      "epoch": 235.2941176470588,
      "grad_norm": 3.0746607780456543,
      "learning_rate": 4.7647058823529414e-05,
      "loss": 2.9556,
      "step": 4000
    },
    {
      "epoch": 235.88235294117646,
      "grad_norm": 3.8579511642456055,
      "learning_rate": 4.764117647058824e-05,
      "loss": 2.9944,
      "step": 4010
    },
    {
      "epoch": 236.47058823529412,
      "grad_norm": 4.026883125305176,
      "learning_rate": 4.763529411764706e-05,
      "loss": 2.8244,
      "step": 4020
    },
    {
      "epoch": 237.05882352941177,
      "grad_norm": 3.8406479358673096,
      "learning_rate": 4.762941176470588e-05,
      "loss": 2.9056,
      "step": 4030
    },
    {
      "epoch": 237.64705882352942,
      "grad_norm": 4.275807857513428,
      "learning_rate": 4.7623529411764706e-05,
      "loss": 2.9012,
      "step": 4040
    },
    {
      "epoch": 238.23529411764707,
      "grad_norm": 4.104135990142822,
      "learning_rate": 4.7617647058823536e-05,
      "loss": 2.8949,
      "step": 4050
    },
    {
      "epoch": 238.8235294117647,
      "grad_norm": 5.491177558898926,
      "learning_rate": 4.761176470588236e-05,
      "loss": 3.1908,
      "step": 4060
    },
    {
      "epoch": 239.41176470588235,
      "grad_norm": 4.011606693267822,
      "learning_rate": 4.7605882352941175e-05,
      "loss": 2.9483,
      "step": 4070
    },
    {
      "epoch": 240.0,
      "grad_norm": 4.742879390716553,
      "learning_rate": 4.76e-05,
      "loss": 2.9523,
      "step": 4080
    },
    {
      "epoch": 240.58823529411765,
      "grad_norm": 3.306370735168457,
      "learning_rate": 4.759411764705883e-05,
      "loss": 2.9298,
      "step": 4090
    },
    {
      "epoch": 241.1764705882353,
      "grad_norm": 3.6371734142303467,
      "learning_rate": 4.758823529411765e-05,
      "loss": 2.9394,
      "step": 4100
    },
    {
      "epoch": 241.76470588235293,
      "grad_norm": 3.6960484981536865,
      "learning_rate": 4.7582352941176474e-05,
      "loss": 2.9008,
      "step": 4110
    },
    {
      "epoch": 242.35294117647058,
      "grad_norm": 4.3804707527160645,
      "learning_rate": 4.75764705882353e-05,
      "loss": 2.8737,
      "step": 4120
    },
    {
      "epoch": 242.94117647058823,
      "grad_norm": 3.7483572959899902,
      "learning_rate": 4.757058823529412e-05,
      "loss": 2.9918,
      "step": 4130
    },
    {
      "epoch": 243.52941176470588,
      "grad_norm": 4.365268707275391,
      "learning_rate": 4.756470588235294e-05,
      "loss": 2.9418,
      "step": 4140
    },
    {
      "epoch": 244.11764705882354,
      "grad_norm": 3.678576946258545,
      "learning_rate": 4.7558823529411766e-05,
      "loss": 2.965,
      "step": 4150
    },
    {
      "epoch": 244.7058823529412,
      "grad_norm": 4.630260944366455,
      "learning_rate": 4.755294117647059e-05,
      "loss": 2.8455,
      "step": 4160
    },
    {
      "epoch": 245.2941176470588,
      "grad_norm": 3.848544120788574,
      "learning_rate": 4.754705882352942e-05,
      "loss": 2.8765,
      "step": 4170
    },
    {
      "epoch": 245.88235294117646,
      "grad_norm": 3.795851945877075,
      "learning_rate": 4.754117647058824e-05,
      "loss": 2.8454,
      "step": 4180
    },
    {
      "epoch": 246.47058823529412,
      "grad_norm": 4.867648124694824,
      "learning_rate": 4.753529411764706e-05,
      "loss": 2.9077,
      "step": 4190
    },
    {
      "epoch": 247.05882352941177,
      "grad_norm": 3.8117685317993164,
      "learning_rate": 4.752941176470588e-05,
      "loss": 2.9007,
      "step": 4200
    },
    {
      "epoch": 247.64705882352942,
      "grad_norm": 3.427556037902832,
      "learning_rate": 4.7523529411764704e-05,
      "loss": 2.9789,
      "step": 4210
    },
    {
      "epoch": 248.23529411764707,
      "grad_norm": 3.4319934844970703,
      "learning_rate": 4.7517647058823534e-05,
      "loss": 3.0251,
      "step": 4220
    },
    {
      "epoch": 248.8235294117647,
      "grad_norm": 4.970730781555176,
      "learning_rate": 4.751176470588236e-05,
      "loss": 2.9227,
      "step": 4230
    },
    {
      "epoch": 249.41176470588235,
      "grad_norm": 3.7114908695220947,
      "learning_rate": 4.750588235294118e-05,
      "loss": 2.9432,
      "step": 4240
    },
    {
      "epoch": 250.0,
      "grad_norm": 3.4784281253814697,
      "learning_rate": 4.75e-05,
      "loss": 2.7594,
      "step": 4250
    },
    {
      "epoch": 250.58823529411765,
      "grad_norm": 3.4776432514190674,
      "learning_rate": 4.7494117647058826e-05,
      "loss": 2.8352,
      "step": 4260
    },
    {
      "epoch": 251.1764705882353,
      "grad_norm": 4.755953788757324,
      "learning_rate": 4.748823529411765e-05,
      "loss": 2.8664,
      "step": 4270
    },
    {
      "epoch": 251.76470588235293,
      "grad_norm": 4.038975715637207,
      "learning_rate": 4.748235294117647e-05,
      "loss": 2.8401,
      "step": 4280
    },
    {
      "epoch": 252.35294117647058,
      "grad_norm": 4.297626495361328,
      "learning_rate": 4.7476470588235295e-05,
      "loss": 2.9328,
      "step": 4290
    },
    {
      "epoch": 252.94117647058823,
      "grad_norm": 3.0400428771972656,
      "learning_rate": 4.7470588235294124e-05,
      "loss": 3.0241,
      "step": 4300
    },
    {
      "epoch": 253.52941176470588,
      "grad_norm": 3.6295907497406006,
      "learning_rate": 4.746470588235295e-05,
      "loss": 2.7852,
      "step": 4310
    },
    {
      "epoch": 254.11764705882354,
      "grad_norm": 3.051633358001709,
      "learning_rate": 4.7458823529411764e-05,
      "loss": 2.8308,
      "step": 4320
    },
    {
      "epoch": 254.7058823529412,
      "grad_norm": 4.192812442779541,
      "learning_rate": 4.7452941176470587e-05,
      "loss": 2.9239,
      "step": 4330
    },
    {
      "epoch": 255.2941176470588,
      "grad_norm": 3.6146926879882812,
      "learning_rate": 4.7447058823529416e-05,
      "loss": 2.82,
      "step": 4340
    },
    {
      "epoch": 255.88235294117646,
      "grad_norm": 4.757193088531494,
      "learning_rate": 4.744117647058824e-05,
      "loss": 2.9755,
      "step": 4350
    },
    {
      "epoch": 256.47058823529414,
      "grad_norm": 3.6965878009796143,
      "learning_rate": 4.743529411764706e-05,
      "loss": 3.029,
      "step": 4360
    },
    {
      "epoch": 257.05882352941177,
      "grad_norm": 4.109294414520264,
      "learning_rate": 4.7429411764705885e-05,
      "loss": 2.8767,
      "step": 4370
    },
    {
      "epoch": 257.6470588235294,
      "grad_norm": 3.226916551589966,
      "learning_rate": 4.742352941176471e-05,
      "loss": 3.0029,
      "step": 4380
    },
    {
      "epoch": 258.2352941176471,
      "grad_norm": 3.9723501205444336,
      "learning_rate": 4.741764705882353e-05,
      "loss": 2.8688,
      "step": 4390
    },
    {
      "epoch": 258.8235294117647,
      "grad_norm": 3.2782037258148193,
      "learning_rate": 4.7411764705882354e-05,
      "loss": 2.908,
      "step": 4400
    },
    {
      "epoch": 259.4117647058824,
      "grad_norm": 3.6765737533569336,
      "learning_rate": 4.740588235294118e-05,
      "loss": 2.8579,
      "step": 4410
    },
    {
      "epoch": 260.0,
      "grad_norm": 4.4406208992004395,
      "learning_rate": 4.74e-05,
      "loss": 2.9471,
      "step": 4420
    },
    {
      "epoch": 260.5882352941176,
      "grad_norm": 3.830777645111084,
      "learning_rate": 4.739411764705883e-05,
      "loss": 3.0358,
      "step": 4430
    },
    {
      "epoch": 261.1764705882353,
      "grad_norm": 3.1592116355895996,
      "learning_rate": 4.738823529411765e-05,
      "loss": 2.8699,
      "step": 4440
    },
    {
      "epoch": 261.7647058823529,
      "grad_norm": 3.3238070011138916,
      "learning_rate": 4.738235294117647e-05,
      "loss": 2.9496,
      "step": 4450
    },
    {
      "epoch": 262.3529411764706,
      "grad_norm": 4.180827617645264,
      "learning_rate": 4.737647058823529e-05,
      "loss": 2.9091,
      "step": 4460
    },
    {
      "epoch": 262.94117647058823,
      "grad_norm": 3.789933443069458,
      "learning_rate": 4.737058823529412e-05,
      "loss": 3.0011,
      "step": 4470
    },
    {
      "epoch": 263.52941176470586,
      "grad_norm": 4.867660045623779,
      "learning_rate": 4.7364705882352945e-05,
      "loss": 2.936,
      "step": 4480
    },
    {
      "epoch": 264.11764705882354,
      "grad_norm": 3.8932993412017822,
      "learning_rate": 4.735882352941177e-05,
      "loss": 3.0137,
      "step": 4490
    },
    {
      "epoch": 264.70588235294116,
      "grad_norm": 4.909325122833252,
      "learning_rate": 4.735294117647059e-05,
      "loss": 2.9665,
      "step": 4500
    },
    {
      "epoch": 265.29411764705884,
      "grad_norm": 3.69692063331604,
      "learning_rate": 4.7347058823529414e-05,
      "loss": 2.9089,
      "step": 4510
    },
    {
      "epoch": 265.88235294117646,
      "grad_norm": 3.6137173175811768,
      "learning_rate": 4.734117647058824e-05,
      "loss": 2.9725,
      "step": 4520
    },
    {
      "epoch": 266.47058823529414,
      "grad_norm": 4.416383266448975,
      "learning_rate": 4.733529411764706e-05,
      "loss": 2.9398,
      "step": 4530
    },
    {
      "epoch": 267.05882352941177,
      "grad_norm": 3.7427027225494385,
      "learning_rate": 4.732941176470588e-05,
      "loss": 2.9106,
      "step": 4540
    },
    {
      "epoch": 267.6470588235294,
      "grad_norm": 3.352476119995117,
      "learning_rate": 4.7323529411764706e-05,
      "loss": 2.8948,
      "step": 4550
    },
    {
      "epoch": 268.2352941176471,
      "grad_norm": 4.186389446258545,
      "learning_rate": 4.7317647058823536e-05,
      "loss": 2.9002,
      "step": 4560
    },
    {
      "epoch": 268.8235294117647,
      "grad_norm": 3.545689344406128,
      "learning_rate": 4.731176470588236e-05,
      "loss": 3.0179,
      "step": 4570
    },
    {
      "epoch": 269.4117647058824,
      "grad_norm": 3.6051015853881836,
      "learning_rate": 4.7305882352941175e-05,
      "loss": 2.8016,
      "step": 4580
    },
    {
      "epoch": 270.0,
      "grad_norm": 4.287453651428223,
      "learning_rate": 4.73e-05,
      "loss": 3.0175,
      "step": 4590
    },
    {
      "epoch": 270.5882352941176,
      "grad_norm": 3.9528095722198486,
      "learning_rate": 4.729411764705883e-05,
      "loss": 2.9179,
      "step": 4600
    },
    {
      "epoch": 271.1764705882353,
      "grad_norm": 3.7531511783599854,
      "learning_rate": 4.728823529411765e-05,
      "loss": 2.8767,
      "step": 4610
    },
    {
      "epoch": 271.7647058823529,
      "grad_norm": 4.140907287597656,
      "learning_rate": 4.7282352941176473e-05,
      "loss": 2.9751,
      "step": 4620
    },
    {
      "epoch": 272.3529411764706,
      "grad_norm": 3.8587911128997803,
      "learning_rate": 4.7276470588235296e-05,
      "loss": 2.8569,
      "step": 4630
    },
    {
      "epoch": 272.94117647058823,
      "grad_norm": 6.040763854980469,
      "learning_rate": 4.727058823529412e-05,
      "loss": 2.8783,
      "step": 4640
    },
    {
      "epoch": 273.52941176470586,
      "grad_norm": 4.736051082611084,
      "learning_rate": 4.726470588235294e-05,
      "loss": 2.8675,
      "step": 4650
    },
    {
      "epoch": 274.11764705882354,
      "grad_norm": 5.158653736114502,
      "learning_rate": 4.7258823529411765e-05,
      "loss": 2.9329,
      "step": 4660
    },
    {
      "epoch": 274.70588235294116,
      "grad_norm": 4.1041579246521,
      "learning_rate": 4.725294117647059e-05,
      "loss": 2.8472,
      "step": 4670
    },
    {
      "epoch": 275.29411764705884,
      "grad_norm": 3.9603898525238037,
      "learning_rate": 4.724705882352942e-05,
      "loss": 2.9825,
      "step": 4680
    },
    {
      "epoch": 275.88235294117646,
      "grad_norm": 3.7051706314086914,
      "learning_rate": 4.724117647058824e-05,
      "loss": 2.6619,
      "step": 4690
    },
    {
      "epoch": 276.47058823529414,
      "grad_norm": 4.065828800201416,
      "learning_rate": 4.7235294117647064e-05,
      "loss": 2.9166,
      "step": 4700
    },
    {
      "epoch": 277.05882352941177,
      "grad_norm": 4.059770107269287,
      "learning_rate": 4.722941176470588e-05,
      "loss": 2.9707,
      "step": 4710
    },
    {
      "epoch": 277.6470588235294,
      "grad_norm": 4.858542442321777,
      "learning_rate": 4.722352941176471e-05,
      "loss": 2.9377,
      "step": 4720
    },
    {
      "epoch": 278.2352941176471,
      "grad_norm": 5.242689609527588,
      "learning_rate": 4.721764705882353e-05,
      "loss": 2.9523,
      "step": 4730
    },
    {
      "epoch": 278.8235294117647,
      "grad_norm": 4.000163555145264,
      "learning_rate": 4.7211764705882356e-05,
      "loss": 2.8491,
      "step": 4740
    },
    {
      "epoch": 279.4117647058824,
      "grad_norm": 4.590585231781006,
      "learning_rate": 4.720588235294118e-05,
      "loss": 2.8835,
      "step": 4750
    },
    {
      "epoch": 280.0,
      "grad_norm": 5.006108283996582,
      "learning_rate": 4.72e-05,
      "loss": 2.7647,
      "step": 4760
    },
    {
      "epoch": 280.5882352941176,
      "grad_norm": 4.637463569641113,
      "learning_rate": 4.7194117647058825e-05,
      "loss": 2.9589,
      "step": 4770
    },
    {
      "epoch": 281.1764705882353,
      "grad_norm": 3.9546892642974854,
      "learning_rate": 4.718823529411765e-05,
      "loss": 2.8887,
      "step": 4780
    },
    {
      "epoch": 281.7647058823529,
      "grad_norm": 3.783047676086426,
      "learning_rate": 4.718235294117647e-05,
      "loss": 2.8341,
      "step": 4790
    },
    {
      "epoch": 282.3529411764706,
      "grad_norm": 3.8639936447143555,
      "learning_rate": 4.7176470588235294e-05,
      "loss": 2.9258,
      "step": 4800
    },
    {
      "epoch": 282.94117647058823,
      "grad_norm": 4.692182540893555,
      "learning_rate": 4.7170588235294124e-05,
      "loss": 2.889,
      "step": 4810
    },
    {
      "epoch": 283.52941176470586,
      "grad_norm": 3.8087244033813477,
      "learning_rate": 4.716470588235295e-05,
      "loss": 2.7626,
      "step": 4820
    },
    {
      "epoch": 284.11764705882354,
      "grad_norm": 3.7948243618011475,
      "learning_rate": 4.715882352941176e-05,
      "loss": 2.9174,
      "step": 4830
    },
    {
      "epoch": 284.70588235294116,
      "grad_norm": 3.5189294815063477,
      "learning_rate": 4.7152941176470586e-05,
      "loss": 2.8735,
      "step": 4840
    },
    {
      "epoch": 285.29411764705884,
      "grad_norm": 4.219771862030029,
      "learning_rate": 4.7147058823529416e-05,
      "loss": 2.7633,
      "step": 4850
    },
    {
      "epoch": 285.88235294117646,
      "grad_norm": 3.69757080078125,
      "learning_rate": 4.714117647058824e-05,
      "loss": 2.8584,
      "step": 4860
    },
    {
      "epoch": 286.47058823529414,
      "grad_norm": 6.331540107727051,
      "learning_rate": 4.713529411764706e-05,
      "loss": 2.8239,
      "step": 4870
    },
    {
      "epoch": 287.05882352941177,
      "grad_norm": 4.585336685180664,
      "learning_rate": 4.7129411764705885e-05,
      "loss": 2.9448,
      "step": 4880
    },
    {
      "epoch": 287.6470588235294,
      "grad_norm": 3.7502236366271973,
      "learning_rate": 4.712352941176471e-05,
      "loss": 2.742,
      "step": 4890
    },
    {
      "epoch": 288.2352941176471,
      "grad_norm": 3.9313883781433105,
      "learning_rate": 4.711764705882353e-05,
      "loss": 3.0465,
      "step": 4900
    },
    {
      "epoch": 288.8235294117647,
      "grad_norm": 4.2915520668029785,
      "learning_rate": 4.7111764705882354e-05,
      "loss": 2.7868,
      "step": 4910
    },
    {
      "epoch": 289.4117647058824,
      "grad_norm": 4.438564300537109,
      "learning_rate": 4.710588235294118e-05,
      "loss": 2.8321,
      "step": 4920
    },
    {
      "epoch": 290.0,
      "grad_norm": 6.370485305786133,
      "learning_rate": 4.71e-05,
      "loss": 2.7066,
      "step": 4930
    },
    {
      "epoch": 290.5882352941176,
      "grad_norm": 5.113689422607422,
      "learning_rate": 4.709411764705883e-05,
      "loss": 2.9416,
      "step": 4940
    },
    {
      "epoch": 291.1764705882353,
      "grad_norm": 3.6469788551330566,
      "learning_rate": 4.708823529411765e-05,
      "loss": 2.7526,
      "step": 4950
    },
    {
      "epoch": 291.7647058823529,
      "grad_norm": 5.2319135665893555,
      "learning_rate": 4.708235294117647e-05,
      "loss": 2.8326,
      "step": 4960
    },
    {
      "epoch": 292.3529411764706,
      "grad_norm": 3.558323621749878,
      "learning_rate": 4.707647058823529e-05,
      "loss": 2.8884,
      "step": 4970
    },
    {
      "epoch": 292.94117647058823,
      "grad_norm": 3.984520435333252,
      "learning_rate": 4.707058823529412e-05,
      "loss": 2.9323,
      "step": 4980
    },
    {
      "epoch": 293.52941176470586,
      "grad_norm": 4.771511554718018,
      "learning_rate": 4.7064705882352944e-05,
      "loss": 2.7698,
      "step": 4990
    },
    {
      "epoch": 294.11764705882354,
      "grad_norm": 4.80217981338501,
      "learning_rate": 4.705882352941177e-05,
      "loss": 2.9995,
      "step": 5000
    },
    {
      "epoch": 294.70588235294116,
      "grad_norm": 5.729180812835693,
      "learning_rate": 4.705294117647059e-05,
      "loss": 2.9624,
      "step": 5010
    },
    {
      "epoch": 295.29411764705884,
      "grad_norm": 6.216054439544678,
      "learning_rate": 4.704705882352941e-05,
      "loss": 2.9467,
      "step": 5020
    },
    {
      "epoch": 295.88235294117646,
      "grad_norm": 3.466930866241455,
      "learning_rate": 4.7041176470588236e-05,
      "loss": 2.8642,
      "step": 5030
    },
    {
      "epoch": 296.47058823529414,
      "grad_norm": 4.938321113586426,
      "learning_rate": 4.703529411764706e-05,
      "loss": 2.7344,
      "step": 5040
    },
    {
      "epoch": 297.05882352941177,
      "grad_norm": 4.305842876434326,
      "learning_rate": 4.702941176470588e-05,
      "loss": 3.0003,
      "step": 5050
    },
    {
      "epoch": 297.6470588235294,
      "grad_norm": 4.031480312347412,
      "learning_rate": 4.702352941176471e-05,
      "loss": 2.7319,
      "step": 5060
    },
    {
      "epoch": 298.2352941176471,
      "grad_norm": 4.950690269470215,
      "learning_rate": 4.7017647058823535e-05,
      "loss": 2.8165,
      "step": 5070
    },
    {
      "epoch": 298.8235294117647,
      "grad_norm": 4.298917770385742,
      "learning_rate": 4.701176470588236e-05,
      "loss": 2.7264,
      "step": 5080
    },
    {
      "epoch": 299.4117647058824,
      "grad_norm": 4.014349937438965,
      "learning_rate": 4.7005882352941174e-05,
      "loss": 2.9852,
      "step": 5090
    },
    {
      "epoch": 300.0,
      "grad_norm": 5.240386009216309,
      "learning_rate": 4.7e-05,
      "loss": 2.9782,
      "step": 5100
    },
    {
      "epoch": 300.5882352941176,
      "grad_norm": 4.603098392486572,
      "learning_rate": 4.699411764705883e-05,
      "loss": 2.9407,
      "step": 5110
    },
    {
      "epoch": 301.1764705882353,
      "grad_norm": 4.006007194519043,
      "learning_rate": 4.698823529411765e-05,
      "loss": 2.8453,
      "step": 5120
    },
    {
      "epoch": 301.7647058823529,
      "grad_norm": 5.459670543670654,
      "learning_rate": 4.698235294117647e-05,
      "loss": 2.7556,
      "step": 5130
    },
    {
      "epoch": 302.3529411764706,
      "grad_norm": 4.340421199798584,
      "learning_rate": 4.6976470588235296e-05,
      "loss": 2.7916,
      "step": 5140
    },
    {
      "epoch": 302.94117647058823,
      "grad_norm": 4.106766223907471,
      "learning_rate": 4.697058823529412e-05,
      "loss": 2.8618,
      "step": 5150
    },
    {
      "epoch": 303.52941176470586,
      "grad_norm": 5.855921268463135,
      "learning_rate": 4.696470588235294e-05,
      "loss": 2.8891,
      "step": 5160
    },
    {
      "epoch": 304.11764705882354,
      "grad_norm": 3.95896577835083,
      "learning_rate": 4.6958823529411765e-05,
      "loss": 2.8831,
      "step": 5170
    },
    {
      "epoch": 304.70588235294116,
      "grad_norm": 6.093185901641846,
      "learning_rate": 4.695294117647059e-05,
      "loss": 2.8456,
      "step": 5180
    },
    {
      "epoch": 305.29411764705884,
      "grad_norm": 4.411391258239746,
      "learning_rate": 4.694705882352942e-05,
      "loss": 2.8264,
      "step": 5190
    },
    {
      "epoch": 305.88235294117646,
      "grad_norm": 3.4602503776550293,
      "learning_rate": 4.694117647058824e-05,
      "loss": 2.9221,
      "step": 5200
    },
    {
      "epoch": 306.47058823529414,
      "grad_norm": 4.809797763824463,
      "learning_rate": 4.6935294117647064e-05,
      "loss": 2.8673,
      "step": 5210
    },
    {
      "epoch": 307.05882352941177,
      "grad_norm": 5.646324634552002,
      "learning_rate": 4.692941176470588e-05,
      "loss": 2.863,
      "step": 5220
    },
    {
      "epoch": 307.6470588235294,
      "grad_norm": 3.7450881004333496,
      "learning_rate": 4.692352941176471e-05,
      "loss": 2.8433,
      "step": 5230
    },
    {
      "epoch": 308.2352941176471,
      "grad_norm": 4.587032318115234,
      "learning_rate": 4.691764705882353e-05,
      "loss": 2.8698,
      "step": 5240
    },
    {
      "epoch": 308.8235294117647,
      "grad_norm": 5.203527927398682,
      "learning_rate": 4.6911764705882356e-05,
      "loss": 2.8485,
      "step": 5250
    },
    {
      "epoch": 309.4117647058824,
      "grad_norm": 4.528745174407959,
      "learning_rate": 4.690588235294118e-05,
      "loss": 2.8556,
      "step": 5260
    },
    {
      "epoch": 310.0,
      "grad_norm": 5.732422351837158,
      "learning_rate": 4.69e-05,
      "loss": 2.9042,
      "step": 5270
    },
    {
      "epoch": 310.5882352941176,
      "grad_norm": 3.7113707065582275,
      "learning_rate": 4.6894117647058825e-05,
      "loss": 2.8449,
      "step": 5280
    },
    {
      "epoch": 311.1764705882353,
      "grad_norm": 4.3244829177856445,
      "learning_rate": 4.688823529411765e-05,
      "loss": 2.7814,
      "step": 5290
    },
    {
      "epoch": 311.7647058823529,
      "grad_norm": 5.326668739318848,
      "learning_rate": 4.688235294117647e-05,
      "loss": 2.7257,
      "step": 5300
    },
    {
      "epoch": 312.3529411764706,
      "grad_norm": 4.155405521392822,
      "learning_rate": 4.6876470588235294e-05,
      "loss": 2.8,
      "step": 5310
    },
    {
      "epoch": 312.94117647058823,
      "grad_norm": 6.2894463539123535,
      "learning_rate": 4.687058823529412e-05,
      "loss": 2.9253,
      "step": 5320
    },
    {
      "epoch": 313.52941176470586,
      "grad_norm": 4.922241687774658,
      "learning_rate": 4.6864705882352946e-05,
      "loss": 2.7831,
      "step": 5330
    },
    {
      "epoch": 314.11764705882354,
      "grad_norm": 5.05827522277832,
      "learning_rate": 4.685882352941177e-05,
      "loss": 2.9243,
      "step": 5340
    },
    {
      "epoch": 314.70588235294116,
      "grad_norm": 4.750011444091797,
      "learning_rate": 4.6852941176470585e-05,
      "loss": 2.7447,
      "step": 5350
    },
    {
      "epoch": 315.29411764705884,
      "grad_norm": 4.217322826385498,
      "learning_rate": 4.6847058823529415e-05,
      "loss": 2.882,
      "step": 5360
    },
    {
      "epoch": 315.88235294117646,
      "grad_norm": 4.280003070831299,
      "learning_rate": 4.684117647058824e-05,
      "loss": 2.7421,
      "step": 5370
    },
    {
      "epoch": 316.47058823529414,
      "grad_norm": 4.383760452270508,
      "learning_rate": 4.683529411764706e-05,
      "loss": 2.7602,
      "step": 5380
    },
    {
      "epoch": 317.05882352941177,
      "grad_norm": 3.9124293327331543,
      "learning_rate": 4.6829411764705884e-05,
      "loss": 2.874,
      "step": 5390
    },
    {
      "epoch": 317.6470588235294,
      "grad_norm": 4.210997581481934,
      "learning_rate": 4.682352941176471e-05,
      "loss": 2.7663,
      "step": 5400
    },
    {
      "epoch": 318.2352941176471,
      "grad_norm": 5.194917678833008,
      "learning_rate": 4.681764705882353e-05,
      "loss": 2.91,
      "step": 5410
    },
    {
      "epoch": 318.8235294117647,
      "grad_norm": 5.712137222290039,
      "learning_rate": 4.681176470588235e-05,
      "loss": 2.9585,
      "step": 5420
    },
    {
      "epoch": 319.4117647058824,
      "grad_norm": 3.8810694217681885,
      "learning_rate": 4.6805882352941176e-05,
      "loss": 2.8528,
      "step": 5430
    },
    {
      "epoch": 320.0,
      "grad_norm": 5.241652488708496,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 2.7221,
      "step": 5440
    },
    {
      "epoch": 320.5882352941176,
      "grad_norm": 4.798607349395752,
      "learning_rate": 4.679411764705883e-05,
      "loss": 2.8117,
      "step": 5450
    },
    {
      "epoch": 321.1764705882353,
      "grad_norm": 4.281361103057861,
      "learning_rate": 4.678823529411765e-05,
      "loss": 2.9285,
      "step": 5460
    },
    {
      "epoch": 321.7647058823529,
      "grad_norm": 4.990331649780273,
      "learning_rate": 4.6782352941176475e-05,
      "loss": 2.8853,
      "step": 5470
    },
    {
      "epoch": 322.3529411764706,
      "grad_norm": 5.474656581878662,
      "learning_rate": 4.677647058823529e-05,
      "loss": 2.9339,
      "step": 5480
    },
    {
      "epoch": 322.94117647058823,
      "grad_norm": 5.11178731918335,
      "learning_rate": 4.677058823529412e-05,
      "loss": 2.8424,
      "step": 5490
    },
    {
      "epoch": 323.52941176470586,
      "grad_norm": 6.115445137023926,
      "learning_rate": 4.6764705882352944e-05,
      "loss": 2.7991,
      "step": 5500
    },
    {
      "epoch": 324.11764705882354,
      "grad_norm": 4.209878921508789,
      "learning_rate": 4.675882352941177e-05,
      "loss": 2.6557,
      "step": 5510
    },
    {
      "epoch": 324.70588235294116,
      "grad_norm": 5.159102916717529,
      "learning_rate": 4.675294117647059e-05,
      "loss": 2.8468,
      "step": 5520
    },
    {
      "epoch": 325.29411764705884,
      "grad_norm": 4.1075825691223145,
      "learning_rate": 4.674705882352941e-05,
      "loss": 2.8522,
      "step": 5530
    },
    {
      "epoch": 325.88235294117646,
      "grad_norm": 5.293395519256592,
      "learning_rate": 4.6741176470588236e-05,
      "loss": 2.8413,
      "step": 5540
    },
    {
      "epoch": 326.47058823529414,
      "grad_norm": 4.747574329376221,
      "learning_rate": 4.673529411764706e-05,
      "loss": 2.8809,
      "step": 5550
    },
    {
      "epoch": 327.05882352941177,
      "grad_norm": 3.995063066482544,
      "learning_rate": 4.672941176470588e-05,
      "loss": 2.7854,
      "step": 5560
    },
    {
      "epoch": 327.6470588235294,
      "grad_norm": 4.366208553314209,
      "learning_rate": 4.672352941176471e-05,
      "loss": 2.886,
      "step": 5570
    },
    {
      "epoch": 328.2352941176471,
      "grad_norm": 5.427539348602295,
      "learning_rate": 4.6717647058823535e-05,
      "loss": 2.7593,
      "step": 5580
    },
    {
      "epoch": 328.8235294117647,
      "grad_norm": 3.832659959793091,
      "learning_rate": 4.671176470588236e-05,
      "loss": 2.781,
      "step": 5590
    },
    {
      "epoch": 329.4117647058824,
      "grad_norm": 3.910635232925415,
      "learning_rate": 4.6705882352941174e-05,
      "loss": 2.8613,
      "step": 5600
    },
    {
      "epoch": 330.0,
      "grad_norm": 8.041980743408203,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 2.9325,
      "step": 5610
    },
    {
      "epoch": 330.5882352941176,
      "grad_norm": 5.0060954093933105,
      "learning_rate": 4.6694117647058826e-05,
      "loss": 2.7879,
      "step": 5620
    },
    {
      "epoch": 331.1764705882353,
      "grad_norm": 3.9800655841827393,
      "learning_rate": 4.668823529411765e-05,
      "loss": 2.9411,
      "step": 5630
    },
    {
      "epoch": 331.7647058823529,
      "grad_norm": 3.835155725479126,
      "learning_rate": 4.668235294117647e-05,
      "loss": 2.858,
      "step": 5640
    },
    {
      "epoch": 332.3529411764706,
      "grad_norm": 4.767171859741211,
      "learning_rate": 4.6676470588235295e-05,
      "loss": 2.7717,
      "step": 5650
    },
    {
      "epoch": 332.94117647058823,
      "grad_norm": 5.802820682525635,
      "learning_rate": 4.667058823529412e-05,
      "loss": 2.7758,
      "step": 5660
    },
    {
      "epoch": 333.52941176470586,
      "grad_norm": 3.7418465614318848,
      "learning_rate": 4.666470588235294e-05,
      "loss": 2.8622,
      "step": 5670
    },
    {
      "epoch": 334.11764705882354,
      "grad_norm": 5.495838165283203,
      "learning_rate": 4.6658823529411764e-05,
      "loss": 2.7891,
      "step": 5680
    },
    {
      "epoch": 334.70588235294116,
      "grad_norm": 4.389771461486816,
      "learning_rate": 4.665294117647059e-05,
      "loss": 2.7437,
      "step": 5690
    },
    {
      "epoch": 335.29411764705884,
      "grad_norm": 4.74864387512207,
      "learning_rate": 4.664705882352942e-05,
      "loss": 2.7951,
      "step": 5700
    },
    {
      "epoch": 335.88235294117646,
      "grad_norm": 3.8270950317382812,
      "learning_rate": 4.664117647058824e-05,
      "loss": 2.7868,
      "step": 5710
    },
    {
      "epoch": 336.47058823529414,
      "grad_norm": 4.398576736450195,
      "learning_rate": 4.663529411764706e-05,
      "loss": 2.8388,
      "step": 5720
    },
    {
      "epoch": 337.05882352941177,
      "grad_norm": 5.155301094055176,
      "learning_rate": 4.662941176470588e-05,
      "loss": 2.6934,
      "step": 5730
    },
    {
      "epoch": 337.6470588235294,
      "grad_norm": 4.620731353759766,
      "learning_rate": 4.662352941176471e-05,
      "loss": 2.7094,
      "step": 5740
    },
    {
      "epoch": 338.2352941176471,
      "grad_norm": 4.573372840881348,
      "learning_rate": 4.661764705882353e-05,
      "loss": 2.8674,
      "step": 5750
    },
    {
      "epoch": 338.8235294117647,
      "grad_norm": 5.50230073928833,
      "learning_rate": 4.6611764705882355e-05,
      "loss": 2.9655,
      "step": 5760
    },
    {
      "epoch": 339.4117647058824,
      "grad_norm": 3.812516689300537,
      "learning_rate": 4.660588235294118e-05,
      "loss": 2.8325,
      "step": 5770
    },
    {
      "epoch": 340.0,
      "grad_norm": 5.0665764808654785,
      "learning_rate": 4.660000000000001e-05,
      "loss": 2.7389,
      "step": 5780
    },
    {
      "epoch": 340.5882352941176,
      "grad_norm": 4.583507061004639,
      "learning_rate": 4.6594117647058824e-05,
      "loss": 2.982,
      "step": 5790
    },
    {
      "epoch": 341.1764705882353,
      "grad_norm": 4.715490818023682,
      "learning_rate": 4.658823529411765e-05,
      "loss": 2.8791,
      "step": 5800
    },
    {
      "epoch": 341.7647058823529,
      "grad_norm": 5.195098876953125,
      "learning_rate": 4.658235294117647e-05,
      "loss": 2.7395,
      "step": 5810
    },
    {
      "epoch": 342.3529411764706,
      "grad_norm": 4.378546237945557,
      "learning_rate": 4.657647058823529e-05,
      "loss": 2.8917,
      "step": 5820
    },
    {
      "epoch": 342.94117647058823,
      "grad_norm": 3.7297534942626953,
      "learning_rate": 4.657058823529412e-05,
      "loss": 2.8068,
      "step": 5830
    },
    {
      "epoch": 343.52941176470586,
      "grad_norm": 4.090474605560303,
      "learning_rate": 4.6564705882352946e-05,
      "loss": 2.8254,
      "step": 5840
    },
    {
      "epoch": 344.11764705882354,
      "grad_norm": 4.678157806396484,
      "learning_rate": 4.655882352941177e-05,
      "loss": 2.8396,
      "step": 5850
    },
    {
      "epoch": 344.70588235294116,
      "grad_norm": 4.954021453857422,
      "learning_rate": 4.6552941176470585e-05,
      "loss": 2.8245,
      "step": 5860
    },
    {
      "epoch": 345.29411764705884,
      "grad_norm": 4.601449966430664,
      "learning_rate": 4.6547058823529415e-05,
      "loss": 2.7566,
      "step": 5870
    },
    {
      "epoch": 345.88235294117646,
      "grad_norm": 4.228757381439209,
      "learning_rate": 4.654117647058824e-05,
      "loss": 2.827,
      "step": 5880
    },
    {
      "epoch": 346.47058823529414,
      "grad_norm": 4.853051662445068,
      "learning_rate": 4.653529411764706e-05,
      "loss": 2.8409,
      "step": 5890
    },
    {
      "epoch": 347.05882352941177,
      "grad_norm": 5.272507190704346,
      "learning_rate": 4.6529411764705884e-05,
      "loss": 2.8162,
      "step": 5900
    },
    {
      "epoch": 347.6470588235294,
      "grad_norm": 4.955953598022461,
      "learning_rate": 4.6523529411764713e-05,
      "loss": 2.8506,
      "step": 5910
    },
    {
      "epoch": 348.2352941176471,
      "grad_norm": 4.565065860748291,
      "learning_rate": 4.651764705882353e-05,
      "loss": 2.8626,
      "step": 5920
    },
    {
      "epoch": 348.8235294117647,
      "grad_norm": 5.791994571685791,
      "learning_rate": 4.651176470588235e-05,
      "loss": 2.7615,
      "step": 5930
    },
    {
      "epoch": 349.4117647058824,
      "grad_norm": 4.855273723602295,
      "learning_rate": 4.6505882352941176e-05,
      "loss": 2.885,
      "step": 5940
    },
    {
      "epoch": 350.0,
      "grad_norm": 4.636885643005371,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 2.8256,
      "step": 5950
    },
    {
      "epoch": 350.5882352941176,
      "grad_norm": 4.235751152038574,
      "learning_rate": 4.649411764705883e-05,
      "loss": 2.7876,
      "step": 5960
    },
    {
      "epoch": 351.1764705882353,
      "grad_norm": 5.428617000579834,
      "learning_rate": 4.648823529411765e-05,
      "loss": 2.6848,
      "step": 5970
    },
    {
      "epoch": 351.7647058823529,
      "grad_norm": 3.6670827865600586,
      "learning_rate": 4.6482352941176474e-05,
      "loss": 2.9104,
      "step": 5980
    },
    {
      "epoch": 352.3529411764706,
      "grad_norm": 5.378091335296631,
      "learning_rate": 4.64764705882353e-05,
      "loss": 2.7694,
      "step": 5990
    },
    {
      "epoch": 352.94117647058823,
      "grad_norm": 4.496878623962402,
      "learning_rate": 4.647058823529412e-05,
      "loss": 2.8014,
      "step": 6000
    },
    {
      "epoch": 353.52941176470586,
      "grad_norm": 5.880807876586914,
      "learning_rate": 4.646470588235294e-05,
      "loss": 2.7753,
      "step": 6010
    },
    {
      "epoch": 354.11764705882354,
      "grad_norm": 6.263209819793701,
      "learning_rate": 4.6458823529411766e-05,
      "loss": 2.913,
      "step": 6020
    },
    {
      "epoch": 354.70588235294116,
      "grad_norm": 3.6843667030334473,
      "learning_rate": 4.645294117647059e-05,
      "loss": 2.7092,
      "step": 6030
    },
    {
      "epoch": 355.29411764705884,
      "grad_norm": 5.489412784576416,
      "learning_rate": 4.644705882352942e-05,
      "loss": 2.725,
      "step": 6040
    },
    {
      "epoch": 355.88235294117646,
      "grad_norm": 3.8253915309906006,
      "learning_rate": 4.6441176470588235e-05,
      "loss": 2.7157,
      "step": 6050
    },
    {
      "epoch": 356.47058823529414,
      "grad_norm": 4.519117832183838,
      "learning_rate": 4.643529411764706e-05,
      "loss": 2.8254,
      "step": 6060
    },
    {
      "epoch": 357.05882352941177,
      "grad_norm": 7.9596638679504395,
      "learning_rate": 4.642941176470588e-05,
      "loss": 2.7567,
      "step": 6070
    },
    {
      "epoch": 357.6470588235294,
      "grad_norm": 5.206328392028809,
      "learning_rate": 4.642352941176471e-05,
      "loss": 2.8054,
      "step": 6080
    },
    {
      "epoch": 358.2352941176471,
      "grad_norm": 4.532703876495361,
      "learning_rate": 4.6417647058823534e-05,
      "loss": 2.8333,
      "step": 6090
    },
    {
      "epoch": 358.8235294117647,
      "grad_norm": 4.582950592041016,
      "learning_rate": 4.641176470588236e-05,
      "loss": 2.7494,
      "step": 6100
    },
    {
      "epoch": 359.4117647058824,
      "grad_norm": 3.9864816665649414,
      "learning_rate": 4.640588235294118e-05,
      "loss": 2.7573,
      "step": 6110
    },
    {
      "epoch": 360.0,
      "grad_norm": 6.524282455444336,
      "learning_rate": 4.64e-05,
      "loss": 2.7131,
      "step": 6120
    },
    {
      "epoch": 360.5882352941176,
      "grad_norm": 4.978061199188232,
      "learning_rate": 4.6394117647058826e-05,
      "loss": 2.8669,
      "step": 6130
    },
    {
      "epoch": 361.1764705882353,
      "grad_norm": 4.612645149230957,
      "learning_rate": 4.638823529411765e-05,
      "loss": 2.7132,
      "step": 6140
    },
    {
      "epoch": 361.7647058823529,
      "grad_norm": 4.722289562225342,
      "learning_rate": 4.638235294117647e-05,
      "loss": 2.7046,
      "step": 6150
    },
    {
      "epoch": 362.3529411764706,
      "grad_norm": 5.122498989105225,
      "learning_rate": 4.63764705882353e-05,
      "loss": 2.8628,
      "step": 6160
    },
    {
      "epoch": 362.94117647058823,
      "grad_norm": 4.246691703796387,
      "learning_rate": 4.637058823529412e-05,
      "loss": 2.6882,
      "step": 6170
    },
    {
      "epoch": 363.52941176470586,
      "grad_norm": 4.782416820526123,
      "learning_rate": 4.636470588235294e-05,
      "loss": 2.7172,
      "step": 6180
    },
    {
      "epoch": 364.11764705882354,
      "grad_norm": 5.605868816375732,
      "learning_rate": 4.6358823529411764e-05,
      "loss": 2.5778,
      "step": 6190
    },
    {
      "epoch": 364.70588235294116,
      "grad_norm": 5.86005973815918,
      "learning_rate": 4.635294117647059e-05,
      "loss": 2.8129,
      "step": 6200
    },
    {
      "epoch": 365.29411764705884,
      "grad_norm": 5.561763763427734,
      "learning_rate": 4.634705882352942e-05,
      "loss": 2.7516,
      "step": 6210
    },
    {
      "epoch": 365.88235294117646,
      "grad_norm": 3.964890241622925,
      "learning_rate": 4.634117647058824e-05,
      "loss": 2.7612,
      "step": 6220
    },
    {
      "epoch": 366.47058823529414,
      "grad_norm": 4.671624660491943,
      "learning_rate": 4.633529411764706e-05,
      "loss": 2.772,
      "step": 6230
    },
    {
      "epoch": 367.05882352941177,
      "grad_norm": 4.304250240325928,
      "learning_rate": 4.6329411764705886e-05,
      "loss": 2.8198,
      "step": 6240
    },
    {
      "epoch": 367.6470588235294,
      "grad_norm": 6.401433944702148,
      "learning_rate": 4.632352941176471e-05,
      "loss": 2.7885,
      "step": 6250
    },
    {
      "epoch": 368.2352941176471,
      "grad_norm": 4.52760648727417,
      "learning_rate": 4.631764705882353e-05,
      "loss": 2.8267,
      "step": 6260
    },
    {
      "epoch": 368.8235294117647,
      "grad_norm": 4.573633193969727,
      "learning_rate": 4.6311764705882355e-05,
      "loss": 2.7715,
      "step": 6270
    },
    {
      "epoch": 369.4117647058824,
      "grad_norm": 7.692042350769043,
      "learning_rate": 4.630588235294118e-05,
      "loss": 2.691,
      "step": 6280
    },
    {
      "epoch": 370.0,
      "grad_norm": 4.482017993927002,
      "learning_rate": 4.630000000000001e-05,
      "loss": 2.7761,
      "step": 6290
    },
    {
      "epoch": 370.5882352941176,
      "grad_norm": 4.2505717277526855,
      "learning_rate": 4.6294117647058824e-05,
      "loss": 2.8083,
      "step": 6300
    },
    {
      "epoch": 371.1764705882353,
      "grad_norm": 5.7786970138549805,
      "learning_rate": 4.6288235294117647e-05,
      "loss": 2.8258,
      "step": 6310
    },
    {
      "epoch": 371.7647058823529,
      "grad_norm": 6.522492408752441,
      "learning_rate": 4.628235294117647e-05,
      "loss": 2.8319,
      "step": 6320
    },
    {
      "epoch": 372.3529411764706,
      "grad_norm": 6.375690937042236,
      "learning_rate": 4.62764705882353e-05,
      "loss": 2.76,
      "step": 6330
    },
    {
      "epoch": 372.94117647058823,
      "grad_norm": 4.877622604370117,
      "learning_rate": 4.627058823529412e-05,
      "loss": 2.7824,
      "step": 6340
    },
    {
      "epoch": 373.52941176470586,
      "grad_norm": 4.892551898956299,
      "learning_rate": 4.6264705882352945e-05,
      "loss": 2.6549,
      "step": 6350
    },
    {
      "epoch": 374.11764705882354,
      "grad_norm": 4.550563812255859,
      "learning_rate": 4.625882352941177e-05,
      "loss": 2.7396,
      "step": 6360
    },
    {
      "epoch": 374.70588235294116,
      "grad_norm": 5.074413299560547,
      "learning_rate": 4.6252941176470584e-05,
      "loss": 2.794,
      "step": 6370
    },
    {
      "epoch": 375.29411764705884,
      "grad_norm": 6.342470645904541,
      "learning_rate": 4.6247058823529414e-05,
      "loss": 2.8111,
      "step": 6380
    },
    {
      "epoch": 375.88235294117646,
      "grad_norm": 5.9674506187438965,
      "learning_rate": 4.624117647058824e-05,
      "loss": 2.7957,
      "step": 6390
    },
    {
      "epoch": 376.47058823529414,
      "grad_norm": 4.185474872589111,
      "learning_rate": 4.623529411764706e-05,
      "loss": 2.807,
      "step": 6400
    },
    {
      "epoch": 377.05882352941177,
      "grad_norm": 5.9351677894592285,
      "learning_rate": 4.622941176470588e-05,
      "loss": 2.8605,
      "step": 6410
    },
    {
      "epoch": 377.6470588235294,
      "grad_norm": 4.214183330535889,
      "learning_rate": 4.622352941176471e-05,
      "loss": 2.6537,
      "step": 6420
    },
    {
      "epoch": 378.2352941176471,
      "grad_norm": 3.765720844268799,
      "learning_rate": 4.621764705882353e-05,
      "loss": 2.6468,
      "step": 6430
    },
    {
      "epoch": 378.8235294117647,
      "grad_norm": 5.012840747833252,
      "learning_rate": 4.621176470588235e-05,
      "loss": 2.6821,
      "step": 6440
    },
    {
      "epoch": 379.4117647058824,
      "grad_norm": 4.845261096954346,
      "learning_rate": 4.6205882352941175e-05,
      "loss": 2.7845,
      "step": 6450
    },
    {
      "epoch": 380.0,
      "grad_norm": 5.320188999176025,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 2.7743,
      "step": 6460
    },
    {
      "epoch": 380.5882352941176,
      "grad_norm": 5.305717945098877,
      "learning_rate": 4.619411764705883e-05,
      "loss": 2.8323,
      "step": 6470
    },
    {
      "epoch": 381.1764705882353,
      "grad_norm": 5.184345245361328,
      "learning_rate": 4.618823529411765e-05,
      "loss": 2.648,
      "step": 6480
    },
    {
      "epoch": 381.7647058823529,
      "grad_norm": 5.618959426879883,
      "learning_rate": 4.6182352941176474e-05,
      "loss": 2.6868,
      "step": 6490
    },
    {
      "epoch": 382.3529411764706,
      "grad_norm": 5.1965012550354,
      "learning_rate": 4.61764705882353e-05,
      "loss": 2.6382,
      "step": 6500
    },
    {
      "epoch": 382.94117647058823,
      "grad_norm": 6.092191696166992,
      "learning_rate": 4.617058823529412e-05,
      "loss": 2.6716,
      "step": 6510
    },
    {
      "epoch": 383.52941176470586,
      "grad_norm": 5.971559524536133,
      "learning_rate": 4.616470588235294e-05,
      "loss": 2.7888,
      "step": 6520
    },
    {
      "epoch": 384.11764705882354,
      "grad_norm": 6.6918745040893555,
      "learning_rate": 4.6158823529411766e-05,
      "loss": 2.7711,
      "step": 6530
    },
    {
      "epoch": 384.70588235294116,
      "grad_norm": 5.537605285644531,
      "learning_rate": 4.615294117647059e-05,
      "loss": 2.7221,
      "step": 6540
    },
    {
      "epoch": 385.29411764705884,
      "grad_norm": 4.910004138946533,
      "learning_rate": 4.614705882352942e-05,
      "loss": 2.8156,
      "step": 6550
    },
    {
      "epoch": 385.88235294117646,
      "grad_norm": 6.908366680145264,
      "learning_rate": 4.6141176470588235e-05,
      "loss": 2.6996,
      "step": 6560
    },
    {
      "epoch": 386.47058823529414,
      "grad_norm": 3.8603312969207764,
      "learning_rate": 4.613529411764706e-05,
      "loss": 2.7358,
      "step": 6570
    },
    {
      "epoch": 387.05882352941177,
      "grad_norm": 5.125916957855225,
      "learning_rate": 4.612941176470588e-05,
      "loss": 2.7492,
      "step": 6580
    },
    {
      "epoch": 387.6470588235294,
      "grad_norm": 5.400735855102539,
      "learning_rate": 4.612352941176471e-05,
      "loss": 2.7501,
      "step": 6590
    },
    {
      "epoch": 388.2352941176471,
      "grad_norm": 5.359391212463379,
      "learning_rate": 4.6117647058823534e-05,
      "loss": 2.7575,
      "step": 6600
    },
    {
      "epoch": 388.8235294117647,
      "grad_norm": 5.117685317993164,
      "learning_rate": 4.6111764705882356e-05,
      "loss": 2.7074,
      "step": 6610
    },
    {
      "epoch": 389.4117647058824,
      "grad_norm": 5.57338809967041,
      "learning_rate": 4.610588235294118e-05,
      "loss": 2.7437,
      "step": 6620
    },
    {
      "epoch": 390.0,
      "grad_norm": 5.66701602935791,
      "learning_rate": 4.61e-05,
      "loss": 2.8642,
      "step": 6630
    },
    {
      "epoch": 390.5882352941176,
      "grad_norm": 5.652499198913574,
      "learning_rate": 4.6094117647058825e-05,
      "loss": 2.7844,
      "step": 6640
    },
    {
      "epoch": 391.1764705882353,
      "grad_norm": 5.832165718078613,
      "learning_rate": 4.608823529411765e-05,
      "loss": 2.7875,
      "step": 6650
    },
    {
      "epoch": 391.7647058823529,
      "grad_norm": 4.654178142547607,
      "learning_rate": 4.608235294117647e-05,
      "loss": 2.7186,
      "step": 6660
    },
    {
      "epoch": 392.3529411764706,
      "grad_norm": 5.397233486175537,
      "learning_rate": 4.60764705882353e-05,
      "loss": 2.64,
      "step": 6670
    },
    {
      "epoch": 392.94117647058823,
      "grad_norm": 4.900176048278809,
      "learning_rate": 4.6070588235294124e-05,
      "loss": 2.7707,
      "step": 6680
    },
    {
      "epoch": 393.52941176470586,
      "grad_norm": 4.79541015625,
      "learning_rate": 4.606470588235294e-05,
      "loss": 2.7081,
      "step": 6690
    },
    {
      "epoch": 394.11764705882354,
      "grad_norm": 5.3823981285095215,
      "learning_rate": 4.605882352941176e-05,
      "loss": 2.6709,
      "step": 6700
    },
    {
      "epoch": 394.70588235294116,
      "grad_norm": 4.825778007507324,
      "learning_rate": 4.605294117647059e-05,
      "loss": 2.8083,
      "step": 6710
    },
    {
      "epoch": 395.29411764705884,
      "grad_norm": 5.580279350280762,
      "learning_rate": 4.6047058823529416e-05,
      "loss": 2.7988,
      "step": 6720
    },
    {
      "epoch": 395.88235294117646,
      "grad_norm": 4.957757472991943,
      "learning_rate": 4.604117647058824e-05,
      "loss": 2.6157,
      "step": 6730
    },
    {
      "epoch": 396.47058823529414,
      "grad_norm": 6.085195541381836,
      "learning_rate": 4.603529411764706e-05,
      "loss": 2.689,
      "step": 6740
    },
    {
      "epoch": 397.05882352941177,
      "grad_norm": 7.3715972900390625,
      "learning_rate": 4.6029411764705885e-05,
      "loss": 2.6605,
      "step": 6750
    },
    {
      "epoch": 397.6470588235294,
      "grad_norm": 5.357855319976807,
      "learning_rate": 4.602352941176471e-05,
      "loss": 2.6376,
      "step": 6760
    },
    {
      "epoch": 398.2352941176471,
      "grad_norm": 4.639465808868408,
      "learning_rate": 4.601764705882353e-05,
      "loss": 2.8396,
      "step": 6770
    },
    {
      "epoch": 398.8235294117647,
      "grad_norm": 6.930776119232178,
      "learning_rate": 4.6011764705882354e-05,
      "loss": 2.6952,
      "step": 6780
    },
    {
      "epoch": 399.4117647058824,
      "grad_norm": 5.704322338104248,
      "learning_rate": 4.600588235294118e-05,
      "loss": 2.7679,
      "step": 6790
    },
    {
      "epoch": 400.0,
      "grad_norm": 7.426507472991943,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.7366,
      "step": 6800
    },
    {
      "epoch": 400.5882352941176,
      "grad_norm": 7.968347549438477,
      "learning_rate": 4.599411764705883e-05,
      "loss": 2.9061,
      "step": 6810
    },
    {
      "epoch": 401.1764705882353,
      "grad_norm": 4.7678046226501465,
      "learning_rate": 4.5988235294117646e-05,
      "loss": 2.8695,
      "step": 6820
    },
    {
      "epoch": 401.7647058823529,
      "grad_norm": 4.663789749145508,
      "learning_rate": 4.598235294117647e-05,
      "loss": 2.742,
      "step": 6830
    },
    {
      "epoch": 402.3529411764706,
      "grad_norm": 6.339473724365234,
      "learning_rate": 4.59764705882353e-05,
      "loss": 2.7938,
      "step": 6840
    },
    {
      "epoch": 402.94117647058823,
      "grad_norm": 5.085391044616699,
      "learning_rate": 4.597058823529412e-05,
      "loss": 2.7361,
      "step": 6850
    },
    {
      "epoch": 403.52941176470586,
      "grad_norm": 4.964785099029541,
      "learning_rate": 4.5964705882352945e-05,
      "loss": 2.8015,
      "step": 6860
    },
    {
      "epoch": 404.11764705882354,
      "grad_norm": 5.151578903198242,
      "learning_rate": 4.595882352941177e-05,
      "loss": 2.8279,
      "step": 6870
    },
    {
      "epoch": 404.70588235294116,
      "grad_norm": 6.837118625640869,
      "learning_rate": 4.595294117647059e-05,
      "loss": 2.8302,
      "step": 6880
    },
    {
      "epoch": 405.29411764705884,
      "grad_norm": 7.302601337432861,
      "learning_rate": 4.5947058823529414e-05,
      "loss": 2.7717,
      "step": 6890
    },
    {
      "epoch": 405.88235294117646,
      "grad_norm": 5.0328369140625,
      "learning_rate": 4.594117647058824e-05,
      "loss": 2.6608,
      "step": 6900
    },
    {
      "epoch": 406.47058823529414,
      "grad_norm": 5.358635425567627,
      "learning_rate": 4.593529411764706e-05,
      "loss": 2.7008,
      "step": 6910
    },
    {
      "epoch": 407.05882352941177,
      "grad_norm": 5.589084148406982,
      "learning_rate": 4.592941176470588e-05,
      "loss": 2.7528,
      "step": 6920
    },
    {
      "epoch": 407.6470588235294,
      "grad_norm": 6.570413112640381,
      "learning_rate": 4.592352941176471e-05,
      "loss": 2.681,
      "step": 6930
    },
    {
      "epoch": 408.2352941176471,
      "grad_norm": 4.757656574249268,
      "learning_rate": 4.591764705882353e-05,
      "loss": 2.7051,
      "step": 6940
    },
    {
      "epoch": 408.8235294117647,
      "grad_norm": 4.9781813621521,
      "learning_rate": 4.591176470588235e-05,
      "loss": 2.7336,
      "step": 6950
    },
    {
      "epoch": 409.4117647058824,
      "grad_norm": 5.391984462738037,
      "learning_rate": 4.5905882352941175e-05,
      "loss": 2.7567,
      "step": 6960
    },
    {
      "epoch": 410.0,
      "grad_norm": 5.273234844207764,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 2.7498,
      "step": 6970
    },
    {
      "epoch": 410.5882352941176,
      "grad_norm": 6.1876091957092285,
      "learning_rate": 4.589411764705883e-05,
      "loss": 2.7057,
      "step": 6980
    },
    {
      "epoch": 411.1764705882353,
      "grad_norm": 5.6880927085876465,
      "learning_rate": 4.588823529411765e-05,
      "loss": 2.7657,
      "step": 6990
    },
    {
      "epoch": 411.7647058823529,
      "grad_norm": 4.962862014770508,
      "learning_rate": 4.588235294117647e-05,
      "loss": 2.6317,
      "step": 7000
    },
    {
      "epoch": 412.3529411764706,
      "grad_norm": 7.325470924377441,
      "learning_rate": 4.5876470588235296e-05,
      "loss": 2.6552,
      "step": 7010
    },
    {
      "epoch": 412.94117647058823,
      "grad_norm": 5.922883033752441,
      "learning_rate": 4.587058823529412e-05,
      "loss": 2.7077,
      "step": 7020
    },
    {
      "epoch": 413.52941176470586,
      "grad_norm": 6.175452709197998,
      "learning_rate": 4.586470588235294e-05,
      "loss": 2.7165,
      "step": 7030
    },
    {
      "epoch": 414.11764705882354,
      "grad_norm": 5.0006561279296875,
      "learning_rate": 4.5858823529411765e-05,
      "loss": 2.7236,
      "step": 7040
    },
    {
      "epoch": 414.70588235294116,
      "grad_norm": 4.160520553588867,
      "learning_rate": 4.5852941176470595e-05,
      "loss": 2.6017,
      "step": 7050
    },
    {
      "epoch": 415.29411764705884,
      "grad_norm": 4.565916538238525,
      "learning_rate": 4.584705882352942e-05,
      "loss": 2.783,
      "step": 7060
    },
    {
      "epoch": 415.88235294117646,
      "grad_norm": 4.627112865447998,
      "learning_rate": 4.5841176470588234e-05,
      "loss": 2.7743,
      "step": 7070
    },
    {
      "epoch": 416.47058823529414,
      "grad_norm": 5.353892803192139,
      "learning_rate": 4.583529411764706e-05,
      "loss": 2.7661,
      "step": 7080
    },
    {
      "epoch": 417.05882352941177,
      "grad_norm": 5.283071994781494,
      "learning_rate": 4.582941176470588e-05,
      "loss": 2.7728,
      "step": 7090
    },
    {
      "epoch": 417.6470588235294,
      "grad_norm": 4.641373157501221,
      "learning_rate": 4.582352941176471e-05,
      "loss": 2.5998,
      "step": 7100
    },
    {
      "epoch": 418.2352941176471,
      "grad_norm": 6.131810665130615,
      "learning_rate": 4.581764705882353e-05,
      "loss": 2.7225,
      "step": 7110
    },
    {
      "epoch": 418.8235294117647,
      "grad_norm": 6.269175052642822,
      "learning_rate": 4.5811764705882356e-05,
      "loss": 2.6131,
      "step": 7120
    },
    {
      "epoch": 419.4117647058824,
      "grad_norm": 6.146518230438232,
      "learning_rate": 4.580588235294118e-05,
      "loss": 2.8454,
      "step": 7130
    },
    {
      "epoch": 420.0,
      "grad_norm": 5.66581392288208,
      "learning_rate": 4.58e-05,
      "loss": 2.5157,
      "step": 7140
    },
    {
      "epoch": 420.5882352941176,
      "grad_norm": 6.826657772064209,
      "learning_rate": 4.5794117647058825e-05,
      "loss": 2.7394,
      "step": 7150
    },
    {
      "epoch": 421.1764705882353,
      "grad_norm": 4.828327178955078,
      "learning_rate": 4.578823529411765e-05,
      "loss": 2.7263,
      "step": 7160
    },
    {
      "epoch": 421.7647058823529,
      "grad_norm": 5.280299663543701,
      "learning_rate": 4.578235294117647e-05,
      "loss": 2.7266,
      "step": 7170
    },
    {
      "epoch": 422.3529411764706,
      "grad_norm": 4.726701736450195,
      "learning_rate": 4.57764705882353e-05,
      "loss": 2.708,
      "step": 7180
    },
    {
      "epoch": 422.94117647058823,
      "grad_norm": 6.244973182678223,
      "learning_rate": 4.5770588235294124e-05,
      "loss": 2.7338,
      "step": 7190
    },
    {
      "epoch": 423.52941176470586,
      "grad_norm": 4.378095626831055,
      "learning_rate": 4.576470588235294e-05,
      "loss": 2.6233,
      "step": 7200
    },
    {
      "epoch": 424.11764705882354,
      "grad_norm": 5.7533674240112305,
      "learning_rate": 4.575882352941176e-05,
      "loss": 2.617,
      "step": 7210
    },
    {
      "epoch": 424.70588235294116,
      "grad_norm": 3.928546905517578,
      "learning_rate": 4.575294117647059e-05,
      "loss": 2.6617,
      "step": 7220
    },
    {
      "epoch": 425.29411764705884,
      "grad_norm": 4.382499694824219,
      "learning_rate": 4.5747058823529416e-05,
      "loss": 2.8241,
      "step": 7230
    },
    {
      "epoch": 425.88235294117646,
      "grad_norm": 4.556126594543457,
      "learning_rate": 4.574117647058824e-05,
      "loss": 2.7835,
      "step": 7240
    },
    {
      "epoch": 426.47058823529414,
      "grad_norm": 4.463908672332764,
      "learning_rate": 4.573529411764706e-05,
      "loss": 2.7436,
      "step": 7250
    },
    {
      "epoch": 427.05882352941177,
      "grad_norm": 5.3072967529296875,
      "learning_rate": 4.5729411764705885e-05,
      "loss": 2.7117,
      "step": 7260
    },
    {
      "epoch": 427.6470588235294,
      "grad_norm": 4.9572930335998535,
      "learning_rate": 4.572352941176471e-05,
      "loss": 2.692,
      "step": 7270
    },
    {
      "epoch": 428.2352941176471,
      "grad_norm": 5.843211650848389,
      "learning_rate": 4.571764705882353e-05,
      "loss": 2.7014,
      "step": 7280
    },
    {
      "epoch": 428.8235294117647,
      "grad_norm": 4.889221668243408,
      "learning_rate": 4.5711764705882354e-05,
      "loss": 2.582,
      "step": 7290
    },
    {
      "epoch": 429.4117647058824,
      "grad_norm": 4.0761799812316895,
      "learning_rate": 4.5705882352941177e-05,
      "loss": 2.6062,
      "step": 7300
    },
    {
      "epoch": 430.0,
      "grad_norm": 5.118371486663818,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 2.6766,
      "step": 7310
    },
    {
      "epoch": 430.5882352941176,
      "grad_norm": 5.1473164558410645,
      "learning_rate": 4.569411764705883e-05,
      "loss": 2.8007,
      "step": 7320
    },
    {
      "epoch": 431.1764705882353,
      "grad_norm": 5.572137832641602,
      "learning_rate": 4.5688235294117645e-05,
      "loss": 2.6601,
      "step": 7330
    },
    {
      "epoch": 431.7647058823529,
      "grad_norm": 5.276971340179443,
      "learning_rate": 4.568235294117647e-05,
      "loss": 2.7845,
      "step": 7340
    },
    {
      "epoch": 432.3529411764706,
      "grad_norm": 5.227826118469238,
      "learning_rate": 4.56764705882353e-05,
      "loss": 2.7748,
      "step": 7350
    },
    {
      "epoch": 432.94117647058823,
      "grad_norm": 6.306338787078857,
      "learning_rate": 4.567058823529412e-05,
      "loss": 2.8108,
      "step": 7360
    },
    {
      "epoch": 433.52941176470586,
      "grad_norm": 7.046097755432129,
      "learning_rate": 4.5664705882352944e-05,
      "loss": 2.7336,
      "step": 7370
    },
    {
      "epoch": 434.11764705882354,
      "grad_norm": 5.379312038421631,
      "learning_rate": 4.565882352941177e-05,
      "loss": 2.7654,
      "step": 7380
    },
    {
      "epoch": 434.70588235294116,
      "grad_norm": 6.052024841308594,
      "learning_rate": 4.565294117647059e-05,
      "loss": 2.6567,
      "step": 7390
    },
    {
      "epoch": 435.29411764705884,
      "grad_norm": 5.718947410583496,
      "learning_rate": 4.564705882352941e-05,
      "loss": 2.7176,
      "step": 7400
    },
    {
      "epoch": 435.88235294117646,
      "grad_norm": 4.215487003326416,
      "learning_rate": 4.5641176470588236e-05,
      "loss": 2.7154,
      "step": 7410
    },
    {
      "epoch": 436.47058823529414,
      "grad_norm": 5.520727157592773,
      "learning_rate": 4.563529411764706e-05,
      "loss": 2.7309,
      "step": 7420
    },
    {
      "epoch": 437.05882352941177,
      "grad_norm": 5.201280117034912,
      "learning_rate": 4.562941176470589e-05,
      "loss": 2.8908,
      "step": 7430
    },
    {
      "epoch": 437.6470588235294,
      "grad_norm": 5.3185200691223145,
      "learning_rate": 4.562352941176471e-05,
      "loss": 2.7148,
      "step": 7440
    },
    {
      "epoch": 438.2352941176471,
      "grad_norm": 5.3539228439331055,
      "learning_rate": 4.5617647058823535e-05,
      "loss": 2.7162,
      "step": 7450
    },
    {
      "epoch": 438.8235294117647,
      "grad_norm": 4.73029899597168,
      "learning_rate": 4.561176470588235e-05,
      "loss": 2.6388,
      "step": 7460
    },
    {
      "epoch": 439.4117647058824,
      "grad_norm": 5.156703948974609,
      "learning_rate": 4.5605882352941174e-05,
      "loss": 2.7862,
      "step": 7470
    },
    {
      "epoch": 440.0,
      "grad_norm": 5.948369979858398,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 2.8188,
      "step": 7480
    },
    {
      "epoch": 440.5882352941176,
      "grad_norm": 5.5692548751831055,
      "learning_rate": 4.559411764705883e-05,
      "loss": 2.6282,
      "step": 7490
    },
    {
      "epoch": 441.1764705882353,
      "grad_norm": 5.616092681884766,
      "learning_rate": 4.558823529411765e-05,
      "loss": 2.7368,
      "step": 7500
    },
    {
      "epoch": 441.7647058823529,
      "grad_norm": 4.938198566436768,
      "learning_rate": 4.558235294117647e-05,
      "loss": 2.6711,
      "step": 7510
    },
    {
      "epoch": 442.3529411764706,
      "grad_norm": 5.040167331695557,
      "learning_rate": 4.5576470588235296e-05,
      "loss": 2.7206,
      "step": 7520
    },
    {
      "epoch": 442.94117647058823,
      "grad_norm": 4.493061065673828,
      "learning_rate": 4.557058823529412e-05,
      "loss": 2.79,
      "step": 7530
    },
    {
      "epoch": 443.52941176470586,
      "grad_norm": 4.6466851234436035,
      "learning_rate": 4.556470588235294e-05,
      "loss": 2.6968,
      "step": 7540
    },
    {
      "epoch": 444.11764705882354,
      "grad_norm": 6.048868179321289,
      "learning_rate": 4.5558823529411765e-05,
      "loss": 2.6859,
      "step": 7550
    },
    {
      "epoch": 444.70588235294116,
      "grad_norm": 6.2247419357299805,
      "learning_rate": 4.5552941176470595e-05,
      "loss": 2.7135,
      "step": 7560
    },
    {
      "epoch": 445.29411764705884,
      "grad_norm": 5.661073684692383,
      "learning_rate": 4.554705882352942e-05,
      "loss": 2.7462,
      "step": 7570
    },
    {
      "epoch": 445.88235294117646,
      "grad_norm": 4.77769136428833,
      "learning_rate": 4.5541176470588234e-05,
      "loss": 2.6298,
      "step": 7580
    },
    {
      "epoch": 446.47058823529414,
      "grad_norm": 5.4974894523620605,
      "learning_rate": 4.553529411764706e-05,
      "loss": 2.6382,
      "step": 7590
    },
    {
      "epoch": 447.05882352941177,
      "grad_norm": 5.548829555511475,
      "learning_rate": 4.5529411764705886e-05,
      "loss": 2.6662,
      "step": 7600
    },
    {
      "epoch": 447.6470588235294,
      "grad_norm": 6.512589454650879,
      "learning_rate": 4.552352941176471e-05,
      "loss": 2.5194,
      "step": 7610
    },
    {
      "epoch": 448.2352941176471,
      "grad_norm": 6.001546859741211,
      "learning_rate": 4.551764705882353e-05,
      "loss": 2.6912,
      "step": 7620
    },
    {
      "epoch": 448.8235294117647,
      "grad_norm": 7.738578796386719,
      "learning_rate": 4.5511764705882355e-05,
      "loss": 2.8304,
      "step": 7630
    },
    {
      "epoch": 449.4117647058824,
      "grad_norm": 6.709195137023926,
      "learning_rate": 4.550588235294118e-05,
      "loss": 2.6092,
      "step": 7640
    },
    {
      "epoch": 450.0,
      "grad_norm": 6.986541271209717,
      "learning_rate": 4.55e-05,
      "loss": 2.6783,
      "step": 7650
    },
    {
      "epoch": 450.5882352941176,
      "grad_norm": 4.478932857513428,
      "learning_rate": 4.5494117647058824e-05,
      "loss": 2.741,
      "step": 7660
    },
    {
      "epoch": 451.1764705882353,
      "grad_norm": 6.5368499755859375,
      "learning_rate": 4.548823529411765e-05,
      "loss": 2.6333,
      "step": 7670
    },
    {
      "epoch": 451.7647058823529,
      "grad_norm": 4.95388650894165,
      "learning_rate": 4.548235294117647e-05,
      "loss": 2.6574,
      "step": 7680
    },
    {
      "epoch": 452.3529411764706,
      "grad_norm": 7.063873767852783,
      "learning_rate": 4.54764705882353e-05,
      "loss": 2.814,
      "step": 7690
    },
    {
      "epoch": 452.94117647058823,
      "grad_norm": 5.794785499572754,
      "learning_rate": 4.547058823529412e-05,
      "loss": 2.7646,
      "step": 7700
    },
    {
      "epoch": 453.52941176470586,
      "grad_norm": 7.408601760864258,
      "learning_rate": 4.546470588235294e-05,
      "loss": 2.6426,
      "step": 7710
    },
    {
      "epoch": 454.11764705882354,
      "grad_norm": 6.2715325355529785,
      "learning_rate": 4.545882352941176e-05,
      "loss": 2.9333,
      "step": 7720
    },
    {
      "epoch": 454.70588235294116,
      "grad_norm": 7.087886810302734,
      "learning_rate": 4.545294117647059e-05,
      "loss": 2.7305,
      "step": 7730
    },
    {
      "epoch": 455.29411764705884,
      "grad_norm": 5.807037830352783,
      "learning_rate": 4.5447058823529415e-05,
      "loss": 2.7687,
      "step": 7740
    },
    {
      "epoch": 455.88235294117646,
      "grad_norm": 4.997759819030762,
      "learning_rate": 4.544117647058824e-05,
      "loss": 2.6021,
      "step": 7750
    },
    {
      "epoch": 456.47058823529414,
      "grad_norm": 5.491547584533691,
      "learning_rate": 4.543529411764706e-05,
      "loss": 2.8578,
      "step": 7760
    },
    {
      "epoch": 457.05882352941177,
      "grad_norm": 6.974318504333496,
      "learning_rate": 4.5429411764705884e-05,
      "loss": 2.7133,
      "step": 7770
    },
    {
      "epoch": 457.6470588235294,
      "grad_norm": 5.855898857116699,
      "learning_rate": 4.542352941176471e-05,
      "loss": 2.5747,
      "step": 7780
    },
    {
      "epoch": 458.2352941176471,
      "grad_norm": 4.146535396575928,
      "learning_rate": 4.541764705882353e-05,
      "loss": 2.6207,
      "step": 7790
    },
    {
      "epoch": 458.8235294117647,
      "grad_norm": 5.459590435028076,
      "learning_rate": 4.541176470588235e-05,
      "loss": 2.6401,
      "step": 7800
    },
    {
      "epoch": 459.4117647058824,
      "grad_norm": 6.258319854736328,
      "learning_rate": 4.5405882352941176e-05,
      "loss": 2.8996,
      "step": 7810
    },
    {
      "epoch": 460.0,
      "grad_norm": 7.2442522048950195,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 2.7587,
      "step": 7820
    },
    {
      "epoch": 460.5882352941176,
      "grad_norm": 6.3507561683654785,
      "learning_rate": 4.539411764705883e-05,
      "loss": 2.8253,
      "step": 7830
    },
    {
      "epoch": 461.1764705882353,
      "grad_norm": 5.180647850036621,
      "learning_rate": 4.5388235294117645e-05,
      "loss": 2.6049,
      "step": 7840
    },
    {
      "epoch": 461.7647058823529,
      "grad_norm": 5.949892520904541,
      "learning_rate": 4.538235294117647e-05,
      "loss": 2.6688,
      "step": 7850
    },
    {
      "epoch": 462.3529411764706,
      "grad_norm": 5.528293132781982,
      "learning_rate": 4.53764705882353e-05,
      "loss": 2.5245,
      "step": 7860
    },
    {
      "epoch": 462.94117647058823,
      "grad_norm": 6.169504642486572,
      "learning_rate": 4.537058823529412e-05,
      "loss": 2.6128,
      "step": 7870
    },
    {
      "epoch": 463.52941176470586,
      "grad_norm": 7.087143898010254,
      "learning_rate": 4.5364705882352944e-05,
      "loss": 2.6484,
      "step": 7880
    },
    {
      "epoch": 464.11764705882354,
      "grad_norm": 6.601476192474365,
      "learning_rate": 4.535882352941177e-05,
      "loss": 2.7396,
      "step": 7890
    },
    {
      "epoch": 464.70588235294116,
      "grad_norm": 5.945483684539795,
      "learning_rate": 4.535294117647059e-05,
      "loss": 2.6188,
      "step": 7900
    },
    {
      "epoch": 465.29411764705884,
      "grad_norm": 7.681111812591553,
      "learning_rate": 4.534705882352941e-05,
      "loss": 2.5899,
      "step": 7910
    },
    {
      "epoch": 465.88235294117646,
      "grad_norm": 4.9963059425354,
      "learning_rate": 4.5341176470588236e-05,
      "loss": 2.6938,
      "step": 7920
    },
    {
      "epoch": 466.47058823529414,
      "grad_norm": 7.2850165367126465,
      "learning_rate": 4.533529411764706e-05,
      "loss": 2.6035,
      "step": 7930
    },
    {
      "epoch": 467.05882352941177,
      "grad_norm": 5.037938117980957,
      "learning_rate": 4.532941176470589e-05,
      "loss": 2.6705,
      "step": 7940
    },
    {
      "epoch": 467.6470588235294,
      "grad_norm": 5.283469200134277,
      "learning_rate": 4.532352941176471e-05,
      "loss": 2.647,
      "step": 7950
    },
    {
      "epoch": 468.2352941176471,
      "grad_norm": 4.163634777069092,
      "learning_rate": 4.5317647058823534e-05,
      "loss": 2.7118,
      "step": 7960
    },
    {
      "epoch": 468.8235294117647,
      "grad_norm": 5.291820526123047,
      "learning_rate": 4.531176470588235e-05,
      "loss": 2.5795,
      "step": 7970
    },
    {
      "epoch": 469.4117647058824,
      "grad_norm": 6.024492263793945,
      "learning_rate": 4.530588235294118e-05,
      "loss": 2.6937,
      "step": 7980
    },
    {
      "epoch": 470.0,
      "grad_norm": 5.4524455070495605,
      "learning_rate": 4.53e-05,
      "loss": 2.6213,
      "step": 7990
    },
    {
      "epoch": 470.5882352941176,
      "grad_norm": 6.226545810699463,
      "learning_rate": 4.5294117647058826e-05,
      "loss": 2.6673,
      "step": 8000
    },
    {
      "epoch": 471.1764705882353,
      "grad_norm": 5.441841125488281,
      "learning_rate": 4.528823529411765e-05,
      "loss": 2.6525,
      "step": 8010
    },
    {
      "epoch": 471.7647058823529,
      "grad_norm": 4.616420745849609,
      "learning_rate": 4.528235294117647e-05,
      "loss": 2.7598,
      "step": 8020
    },
    {
      "epoch": 472.3529411764706,
      "grad_norm": 5.9546661376953125,
      "learning_rate": 4.5276470588235295e-05,
      "loss": 2.7777,
      "step": 8030
    },
    {
      "epoch": 472.94117647058823,
      "grad_norm": 5.308837890625,
      "learning_rate": 4.527058823529412e-05,
      "loss": 2.5787,
      "step": 8040
    },
    {
      "epoch": 473.52941176470586,
      "grad_norm": 6.592496395111084,
      "learning_rate": 4.526470588235294e-05,
      "loss": 2.7261,
      "step": 8050
    },
    {
      "epoch": 474.11764705882354,
      "grad_norm": 8.970817565917969,
      "learning_rate": 4.5258823529411764e-05,
      "loss": 2.7473,
      "step": 8060
    },
    {
      "epoch": 474.70588235294116,
      "grad_norm": 4.836677551269531,
      "learning_rate": 4.5252941176470594e-05,
      "loss": 2.7324,
      "step": 8070
    },
    {
      "epoch": 475.29411764705884,
      "grad_norm": 7.439720153808594,
      "learning_rate": 4.524705882352942e-05,
      "loss": 2.592,
      "step": 8080
    },
    {
      "epoch": 475.88235294117646,
      "grad_norm": 6.673875331878662,
      "learning_rate": 4.524117647058824e-05,
      "loss": 2.7604,
      "step": 8090
    },
    {
      "epoch": 476.47058823529414,
      "grad_norm": 5.400139331817627,
      "learning_rate": 4.5235294117647056e-05,
      "loss": 2.5404,
      "step": 8100
    },
    {
      "epoch": 477.05882352941177,
      "grad_norm": 8.512996673583984,
      "learning_rate": 4.5229411764705886e-05,
      "loss": 2.7638,
      "step": 8110
    },
    {
      "epoch": 477.6470588235294,
      "grad_norm": 5.274264812469482,
      "learning_rate": 4.522352941176471e-05,
      "loss": 2.7077,
      "step": 8120
    },
    {
      "epoch": 478.2352941176471,
      "grad_norm": 4.570178031921387,
      "learning_rate": 4.521764705882353e-05,
      "loss": 2.6643,
      "step": 8130
    },
    {
      "epoch": 478.8235294117647,
      "grad_norm": 6.034195423126221,
      "learning_rate": 4.5211764705882355e-05,
      "loss": 2.5771,
      "step": 8140
    },
    {
      "epoch": 479.4117647058824,
      "grad_norm": 5.601174354553223,
      "learning_rate": 4.520588235294118e-05,
      "loss": 2.5671,
      "step": 8150
    },
    {
      "epoch": 480.0,
      "grad_norm": 8.732476234436035,
      "learning_rate": 4.52e-05,
      "loss": 2.5468,
      "step": 8160
    },
    {
      "epoch": 480.5882352941176,
      "grad_norm": 5.503907680511475,
      "learning_rate": 4.5194117647058824e-05,
      "loss": 2.654,
      "step": 8170
    },
    {
      "epoch": 481.1764705882353,
      "grad_norm": 6.762074947357178,
      "learning_rate": 4.518823529411765e-05,
      "loss": 2.6711,
      "step": 8180
    },
    {
      "epoch": 481.7647058823529,
      "grad_norm": 6.327527046203613,
      "learning_rate": 4.518235294117647e-05,
      "loss": 2.6204,
      "step": 8190
    },
    {
      "epoch": 482.3529411764706,
      "grad_norm": 5.901119232177734,
      "learning_rate": 4.51764705882353e-05,
      "loss": 2.6391,
      "step": 8200
    },
    {
      "epoch": 482.94117647058823,
      "grad_norm": 4.836418151855469,
      "learning_rate": 4.517058823529412e-05,
      "loss": 2.647,
      "step": 8210
    },
    {
      "epoch": 483.52941176470586,
      "grad_norm": 6.739509582519531,
      "learning_rate": 4.5164705882352946e-05,
      "loss": 2.6736,
      "step": 8220
    },
    {
      "epoch": 484.11764705882354,
      "grad_norm": 4.808406352996826,
      "learning_rate": 4.515882352941176e-05,
      "loss": 2.6431,
      "step": 8230
    },
    {
      "epoch": 484.70588235294116,
      "grad_norm": 6.298581123352051,
      "learning_rate": 4.515294117647059e-05,
      "loss": 2.6361,
      "step": 8240
    },
    {
      "epoch": 485.29411764705884,
      "grad_norm": 5.634302616119385,
      "learning_rate": 4.5147058823529415e-05,
      "loss": 2.5523,
      "step": 8250
    },
    {
      "epoch": 485.88235294117646,
      "grad_norm": 5.03770637512207,
      "learning_rate": 4.514117647058824e-05,
      "loss": 2.6443,
      "step": 8260
    },
    {
      "epoch": 486.47058823529414,
      "grad_norm": 6.211006164550781,
      "learning_rate": 4.513529411764706e-05,
      "loss": 2.806,
      "step": 8270
    },
    {
      "epoch": 487.05882352941177,
      "grad_norm": 7.934748649597168,
      "learning_rate": 4.5129411764705884e-05,
      "loss": 2.6646,
      "step": 8280
    },
    {
      "epoch": 487.6470588235294,
      "grad_norm": 6.098084926605225,
      "learning_rate": 4.5123529411764707e-05,
      "loss": 2.6828,
      "step": 8290
    },
    {
      "epoch": 488.2352941176471,
      "grad_norm": 4.57448673248291,
      "learning_rate": 4.511764705882353e-05,
      "loss": 2.5417,
      "step": 8300
    },
    {
      "epoch": 488.8235294117647,
      "grad_norm": 5.119872570037842,
      "learning_rate": 4.511176470588235e-05,
      "loss": 2.6239,
      "step": 8310
    },
    {
      "epoch": 489.4117647058824,
      "grad_norm": 6.781409740447998,
      "learning_rate": 4.510588235294118e-05,
      "loss": 2.6492,
      "step": 8320
    },
    {
      "epoch": 490.0,
      "grad_norm": 8.174928665161133,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 2.5291,
      "step": 8330
    },
    {
      "epoch": 490.5882352941176,
      "grad_norm": 5.663301944732666,
      "learning_rate": 4.509411764705883e-05,
      "loss": 2.7304,
      "step": 8340
    },
    {
      "epoch": 491.1764705882353,
      "grad_norm": 8.729888916015625,
      "learning_rate": 4.5088235294117644e-05,
      "loss": 2.7168,
      "step": 8350
    },
    {
      "epoch": 491.7647058823529,
      "grad_norm": 7.103817462921143,
      "learning_rate": 4.508235294117647e-05,
      "loss": 2.5684,
      "step": 8360
    },
    {
      "epoch": 492.3529411764706,
      "grad_norm": 7.997485160827637,
      "learning_rate": 4.50764705882353e-05,
      "loss": 2.5594,
      "step": 8370
    },
    {
      "epoch": 492.94117647058823,
      "grad_norm": 6.850675106048584,
      "learning_rate": 4.507058823529412e-05,
      "loss": 2.742,
      "step": 8380
    },
    {
      "epoch": 493.52941176470586,
      "grad_norm": 5.732715129852295,
      "learning_rate": 4.506470588235294e-05,
      "loss": 2.511,
      "step": 8390
    },
    {
      "epoch": 494.11764705882354,
      "grad_norm": 7.649827003479004,
      "learning_rate": 4.5058823529411766e-05,
      "loss": 2.6301,
      "step": 8400
    },
    {
      "epoch": 494.70588235294116,
      "grad_norm": 5.41689395904541,
      "learning_rate": 4.505294117647059e-05,
      "loss": 2.6186,
      "step": 8410
    },
    {
      "epoch": 495.29411764705884,
      "grad_norm": 7.008578777313232,
      "learning_rate": 4.504705882352941e-05,
      "loss": 2.6788,
      "step": 8420
    },
    {
      "epoch": 495.88235294117646,
      "grad_norm": 5.741420269012451,
      "learning_rate": 4.5041176470588235e-05,
      "loss": 2.678,
      "step": 8430
    },
    {
      "epoch": 496.47058823529414,
      "grad_norm": 5.79753303527832,
      "learning_rate": 4.503529411764706e-05,
      "loss": 2.7881,
      "step": 8440
    },
    {
      "epoch": 497.05882352941177,
      "grad_norm": 6.5941033363342285,
      "learning_rate": 4.502941176470589e-05,
      "loss": 2.658,
      "step": 8450
    },
    {
      "epoch": 497.6470588235294,
      "grad_norm": 6.539538383483887,
      "learning_rate": 4.502352941176471e-05,
      "loss": 2.6344,
      "step": 8460
    },
    {
      "epoch": 498.2352941176471,
      "grad_norm": 6.366264343261719,
      "learning_rate": 4.5017647058823534e-05,
      "loss": 2.5425,
      "step": 8470
    },
    {
      "epoch": 498.8235294117647,
      "grad_norm": 7.851151466369629,
      "learning_rate": 4.501176470588235e-05,
      "loss": 2.6742,
      "step": 8480
    },
    {
      "epoch": 499.4117647058824,
      "grad_norm": 5.753756999969482,
      "learning_rate": 4.500588235294118e-05,
      "loss": 2.6474,
      "step": 8490
    },
    {
      "epoch": 500.0,
      "grad_norm": 5.8155388832092285,
      "learning_rate": 4.5e-05,
      "loss": 2.663,
      "step": 8500
    },
    {
      "epoch": 500.5882352941176,
      "grad_norm": 7.363581657409668,
      "learning_rate": 4.4994117647058826e-05,
      "loss": 2.5897,
      "step": 8510
    },
    {
      "epoch": 501.1764705882353,
      "grad_norm": 5.949336051940918,
      "learning_rate": 4.498823529411765e-05,
      "loss": 2.5901,
      "step": 8520
    },
    {
      "epoch": 501.7647058823529,
      "grad_norm": 6.517917633056641,
      "learning_rate": 4.498235294117648e-05,
      "loss": 2.8388,
      "step": 8530
    },
    {
      "epoch": 502.3529411764706,
      "grad_norm": 6.614443302154541,
      "learning_rate": 4.4976470588235295e-05,
      "loss": 2.6061,
      "step": 8540
    },
    {
      "epoch": 502.94117647058823,
      "grad_norm": 5.306547164916992,
      "learning_rate": 4.497058823529412e-05,
      "loss": 2.5749,
      "step": 8550
    },
    {
      "epoch": 503.52941176470586,
      "grad_norm": 5.807610988616943,
      "learning_rate": 4.496470588235294e-05,
      "loss": 2.7691,
      "step": 8560
    },
    {
      "epoch": 504.11764705882354,
      "grad_norm": 6.471726894378662,
      "learning_rate": 4.4958823529411764e-05,
      "loss": 2.6429,
      "step": 8570
    },
    {
      "epoch": 504.70588235294116,
      "grad_norm": 5.640124797821045,
      "learning_rate": 4.4952941176470594e-05,
      "loss": 2.5805,
      "step": 8580
    },
    {
      "epoch": 505.29411764705884,
      "grad_norm": 4.840231895446777,
      "learning_rate": 4.4947058823529417e-05,
      "loss": 2.6205,
      "step": 8590
    },
    {
      "epoch": 505.88235294117646,
      "grad_norm": 5.875251770019531,
      "learning_rate": 4.494117647058824e-05,
      "loss": 2.566,
      "step": 8600
    },
    {
      "epoch": 506.47058823529414,
      "grad_norm": 5.8345465660095215,
      "learning_rate": 4.4935294117647056e-05,
      "loss": 2.48,
      "step": 8610
    },
    {
      "epoch": 507.05882352941177,
      "grad_norm": 5.416911602020264,
      "learning_rate": 4.4929411764705885e-05,
      "loss": 2.6254,
      "step": 8620
    },
    {
      "epoch": 507.6470588235294,
      "grad_norm": 5.9475321769714355,
      "learning_rate": 4.492352941176471e-05,
      "loss": 2.6458,
      "step": 8630
    },
    {
      "epoch": 508.2352941176471,
      "grad_norm": 6.6880202293396,
      "learning_rate": 4.491764705882353e-05,
      "loss": 2.6822,
      "step": 8640
    },
    {
      "epoch": 508.8235294117647,
      "grad_norm": 5.599514961242676,
      "learning_rate": 4.4911764705882354e-05,
      "loss": 2.5688,
      "step": 8650
    },
    {
      "epoch": 509.4117647058824,
      "grad_norm": 5.998929023742676,
      "learning_rate": 4.4905882352941184e-05,
      "loss": 2.6812,
      "step": 8660
    },
    {
      "epoch": 510.0,
      "grad_norm": 7.499534606933594,
      "learning_rate": 4.49e-05,
      "loss": 2.5363,
      "step": 8670
    },
    {
      "epoch": 510.5882352941176,
      "grad_norm": 5.561249732971191,
      "learning_rate": 4.4894117647058823e-05,
      "loss": 2.6605,
      "step": 8680
    },
    {
      "epoch": 511.1764705882353,
      "grad_norm": 6.422592639923096,
      "learning_rate": 4.4888235294117646e-05,
      "loss": 2.7123,
      "step": 8690
    },
    {
      "epoch": 511.7647058823529,
      "grad_norm": 5.721835136413574,
      "learning_rate": 4.4882352941176476e-05,
      "loss": 2.6591,
      "step": 8700
    },
    {
      "epoch": 512.3529411764706,
      "grad_norm": 5.0921101570129395,
      "learning_rate": 4.48764705882353e-05,
      "loss": 2.5308,
      "step": 8710
    },
    {
      "epoch": 512.9411764705883,
      "grad_norm": 5.384548664093018,
      "learning_rate": 4.487058823529412e-05,
      "loss": 2.7839,
      "step": 8720
    },
    {
      "epoch": 513.5294117647059,
      "grad_norm": 4.717669486999512,
      "learning_rate": 4.4864705882352945e-05,
      "loss": 2.6087,
      "step": 8730
    },
    {
      "epoch": 514.1176470588235,
      "grad_norm": 7.097065448760986,
      "learning_rate": 4.485882352941176e-05,
      "loss": 2.5889,
      "step": 8740
    },
    {
      "epoch": 514.7058823529412,
      "grad_norm": 6.520040988922119,
      "learning_rate": 4.485294117647059e-05,
      "loss": 2.4998,
      "step": 8750
    },
    {
      "epoch": 515.2941176470588,
      "grad_norm": 5.41622257232666,
      "learning_rate": 4.4847058823529414e-05,
      "loss": 2.6026,
      "step": 8760
    },
    {
      "epoch": 515.8823529411765,
      "grad_norm": 6.532198429107666,
      "learning_rate": 4.484117647058824e-05,
      "loss": 2.7607,
      "step": 8770
    },
    {
      "epoch": 516.4705882352941,
      "grad_norm": 5.891966819763184,
      "learning_rate": 4.483529411764706e-05,
      "loss": 2.5855,
      "step": 8780
    },
    {
      "epoch": 517.0588235294117,
      "grad_norm": 6.652442932128906,
      "learning_rate": 4.482941176470589e-05,
      "loss": 2.5553,
      "step": 8790
    },
    {
      "epoch": 517.6470588235294,
      "grad_norm": 5.952583312988281,
      "learning_rate": 4.4823529411764706e-05,
      "loss": 2.5962,
      "step": 8800
    },
    {
      "epoch": 518.2352941176471,
      "grad_norm": 5.971527099609375,
      "learning_rate": 4.481764705882353e-05,
      "loss": 2.6771,
      "step": 8810
    },
    {
      "epoch": 518.8235294117648,
      "grad_norm": 6.12833833694458,
      "learning_rate": 4.481176470588235e-05,
      "loss": 2.4282,
      "step": 8820
    },
    {
      "epoch": 519.4117647058823,
      "grad_norm": 7.672847747802734,
      "learning_rate": 4.480588235294118e-05,
      "loss": 2.5699,
      "step": 8830
    },
    {
      "epoch": 520.0,
      "grad_norm": 7.014542102813721,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 2.6196,
      "step": 8840
    },
    {
      "epoch": 520.5882352941177,
      "grad_norm": 4.5471296310424805,
      "learning_rate": 4.479411764705883e-05,
      "loss": 2.6825,
      "step": 8850
    },
    {
      "epoch": 521.1764705882352,
      "grad_norm": 6.299719333648682,
      "learning_rate": 4.478823529411765e-05,
      "loss": 2.6232,
      "step": 8860
    },
    {
      "epoch": 521.7647058823529,
      "grad_norm": 5.358712196350098,
      "learning_rate": 4.4782352941176474e-05,
      "loss": 2.7872,
      "step": 8870
    },
    {
      "epoch": 522.3529411764706,
      "grad_norm": 4.927674293518066,
      "learning_rate": 4.47764705882353e-05,
      "loss": 2.6256,
      "step": 8880
    },
    {
      "epoch": 522.9411764705883,
      "grad_norm": 5.611540794372559,
      "learning_rate": 4.477058823529412e-05,
      "loss": 2.5166,
      "step": 8890
    },
    {
      "epoch": 523.5294117647059,
      "grad_norm": 9.753655433654785,
      "learning_rate": 4.476470588235294e-05,
      "loss": 2.6284,
      "step": 8900
    },
    {
      "epoch": 524.1176470588235,
      "grad_norm": 6.398709774017334,
      "learning_rate": 4.4758823529411766e-05,
      "loss": 2.6029,
      "step": 8910
    },
    {
      "epoch": 524.7058823529412,
      "grad_norm": 8.235616683959961,
      "learning_rate": 4.475294117647059e-05,
      "loss": 2.6381,
      "step": 8920
    },
    {
      "epoch": 525.2941176470588,
      "grad_norm": 7.867367744445801,
      "learning_rate": 4.474705882352941e-05,
      "loss": 2.6192,
      "step": 8930
    },
    {
      "epoch": 525.8823529411765,
      "grad_norm": 5.800793647766113,
      "learning_rate": 4.4741176470588235e-05,
      "loss": 2.5999,
      "step": 8940
    },
    {
      "epoch": 526.4705882352941,
      "grad_norm": 7.639871597290039,
      "learning_rate": 4.473529411764706e-05,
      "loss": 2.4684,
      "step": 8950
    },
    {
      "epoch": 527.0588235294117,
      "grad_norm": 6.464935302734375,
      "learning_rate": 4.472941176470589e-05,
      "loss": 2.6759,
      "step": 8960
    },
    {
      "epoch": 527.6470588235294,
      "grad_norm": 5.911649703979492,
      "learning_rate": 4.472352941176471e-05,
      "loss": 2.534,
      "step": 8970
    },
    {
      "epoch": 528.2352941176471,
      "grad_norm": 6.536330223083496,
      "learning_rate": 4.471764705882353e-05,
      "loss": 2.6617,
      "step": 8980
    },
    {
      "epoch": 528.8235294117648,
      "grad_norm": 5.839545726776123,
      "learning_rate": 4.4711764705882356e-05,
      "loss": 2.5018,
      "step": 8990
    },
    {
      "epoch": 529.4117647058823,
      "grad_norm": 7.193487644195557,
      "learning_rate": 4.470588235294118e-05,
      "loss": 2.5966,
      "step": 9000
    },
    {
      "epoch": 530.0,
      "grad_norm": 6.506171703338623,
      "learning_rate": 4.47e-05,
      "loss": 2.4892,
      "step": 9010
    },
    {
      "epoch": 530.5882352941177,
      "grad_norm": 7.176978588104248,
      "learning_rate": 4.4694117647058825e-05,
      "loss": 2.7164,
      "step": 9020
    },
    {
      "epoch": 531.1764705882352,
      "grad_norm": 5.853668212890625,
      "learning_rate": 4.468823529411765e-05,
      "loss": 2.638,
      "step": 9030
    },
    {
      "epoch": 531.7647058823529,
      "grad_norm": 7.911562442779541,
      "learning_rate": 4.468235294117648e-05,
      "loss": 2.542,
      "step": 9040
    },
    {
      "epoch": 532.3529411764706,
      "grad_norm": 5.7171149253845215,
      "learning_rate": 4.4676470588235294e-05,
      "loss": 2.5997,
      "step": 9050
    },
    {
      "epoch": 532.9411764705883,
      "grad_norm": 9.082596778869629,
      "learning_rate": 4.467058823529412e-05,
      "loss": 2.5769,
      "step": 9060
    },
    {
      "epoch": 533.5294117647059,
      "grad_norm": 6.643250465393066,
      "learning_rate": 4.466470588235294e-05,
      "loss": 2.4623,
      "step": 9070
    },
    {
      "epoch": 534.1176470588235,
      "grad_norm": 5.914425373077393,
      "learning_rate": 4.465882352941177e-05,
      "loss": 2.663,
      "step": 9080
    },
    {
      "epoch": 534.7058823529412,
      "grad_norm": 5.256445407867432,
      "learning_rate": 4.465294117647059e-05,
      "loss": 2.4986,
      "step": 9090
    },
    {
      "epoch": 535.2941176470588,
      "grad_norm": 6.855877876281738,
      "learning_rate": 4.4647058823529416e-05,
      "loss": 2.5341,
      "step": 9100
    },
    {
      "epoch": 535.8823529411765,
      "grad_norm": 6.5876145362854,
      "learning_rate": 4.464117647058824e-05,
      "loss": 2.4644,
      "step": 9110
    },
    {
      "epoch": 536.4705882352941,
      "grad_norm": 8.319486618041992,
      "learning_rate": 4.4635294117647055e-05,
      "loss": 2.7621,
      "step": 9120
    },
    {
      "epoch": 537.0588235294117,
      "grad_norm": 7.8535356521606445,
      "learning_rate": 4.4629411764705885e-05,
      "loss": 2.6594,
      "step": 9130
    },
    {
      "epoch": 537.6470588235294,
      "grad_norm": 5.965566158294678,
      "learning_rate": 4.462352941176471e-05,
      "loss": 2.6138,
      "step": 9140
    },
    {
      "epoch": 538.2352941176471,
      "grad_norm": 6.658907413482666,
      "learning_rate": 4.461764705882353e-05,
      "loss": 2.4679,
      "step": 9150
    },
    {
      "epoch": 538.8235294117648,
      "grad_norm": 5.388052940368652,
      "learning_rate": 4.4611764705882354e-05,
      "loss": 2.6506,
      "step": 9160
    },
    {
      "epoch": 539.4117647058823,
      "grad_norm": 6.356945037841797,
      "learning_rate": 4.4605882352941184e-05,
      "loss": 2.5511,
      "step": 9170
    },
    {
      "epoch": 540.0,
      "grad_norm": 9.061415672302246,
      "learning_rate": 4.46e-05,
      "loss": 2.557,
      "step": 9180
    },
    {
      "epoch": 540.5882352941177,
      "grad_norm": 5.921602725982666,
      "learning_rate": 4.459411764705882e-05,
      "loss": 2.5661,
      "step": 9190
    },
    {
      "epoch": 541.1764705882352,
      "grad_norm": 6.83628511428833,
      "learning_rate": 4.4588235294117646e-05,
      "loss": 2.6151,
      "step": 9200
    },
    {
      "epoch": 541.7647058823529,
      "grad_norm": 6.944824695587158,
      "learning_rate": 4.4582352941176476e-05,
      "loss": 2.6484,
      "step": 9210
    },
    {
      "epoch": 542.3529411764706,
      "grad_norm": 5.832526206970215,
      "learning_rate": 4.45764705882353e-05,
      "loss": 2.5786,
      "step": 9220
    },
    {
      "epoch": 542.9411764705883,
      "grad_norm": 6.765356063842773,
      "learning_rate": 4.457058823529412e-05,
      "loss": 2.6856,
      "step": 9230
    },
    {
      "epoch": 543.5294117647059,
      "grad_norm": 5.230446815490723,
      "learning_rate": 4.4564705882352945e-05,
      "loss": 2.6765,
      "step": 9240
    },
    {
      "epoch": 544.1176470588235,
      "grad_norm": 6.092923641204834,
      "learning_rate": 4.455882352941177e-05,
      "loss": 2.4797,
      "step": 9250
    },
    {
      "epoch": 544.7058823529412,
      "grad_norm": 6.0735931396484375,
      "learning_rate": 4.455294117647059e-05,
      "loss": 2.649,
      "step": 9260
    },
    {
      "epoch": 545.2941176470588,
      "grad_norm": 5.449681758880615,
      "learning_rate": 4.4547058823529414e-05,
      "loss": 2.5897,
      "step": 9270
    },
    {
      "epoch": 545.8823529411765,
      "grad_norm": 6.516177177429199,
      "learning_rate": 4.4541176470588237e-05,
      "loss": 2.5366,
      "step": 9280
    },
    {
      "epoch": 546.4705882352941,
      "grad_norm": 6.25967264175415,
      "learning_rate": 4.453529411764706e-05,
      "loss": 2.455,
      "step": 9290
    },
    {
      "epoch": 547.0588235294117,
      "grad_norm": 8.238560676574707,
      "learning_rate": 4.452941176470589e-05,
      "loss": 2.7221,
      "step": 9300
    },
    {
      "epoch": 547.6470588235294,
      "grad_norm": 6.648443698883057,
      "learning_rate": 4.4523529411764706e-05,
      "loss": 2.5672,
      "step": 9310
    },
    {
      "epoch": 548.2352941176471,
      "grad_norm": 6.453373908996582,
      "learning_rate": 4.451764705882353e-05,
      "loss": 2.6317,
      "step": 9320
    },
    {
      "epoch": 548.8235294117648,
      "grad_norm": 8.580318450927734,
      "learning_rate": 4.451176470588235e-05,
      "loss": 2.6382,
      "step": 9330
    },
    {
      "epoch": 549.4117647058823,
      "grad_norm": 7.163834571838379,
      "learning_rate": 4.450588235294118e-05,
      "loss": 2.4592,
      "step": 9340
    },
    {
      "epoch": 550.0,
      "grad_norm": 6.451145172119141,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 2.5085,
      "step": 9350
    },
    {
      "epoch": 550.5882352941177,
      "grad_norm": 7.1214447021484375,
      "learning_rate": 4.449411764705883e-05,
      "loss": 2.6832,
      "step": 9360
    },
    {
      "epoch": 551.1764705882352,
      "grad_norm": 5.1639533042907715,
      "learning_rate": 4.448823529411765e-05,
      "loss": 2.4992,
      "step": 9370
    },
    {
      "epoch": 551.7647058823529,
      "grad_norm": 6.494882106781006,
      "learning_rate": 4.448235294117647e-05,
      "loss": 2.6573,
      "step": 9380
    },
    {
      "epoch": 552.3529411764706,
      "grad_norm": 5.208673477172852,
      "learning_rate": 4.4476470588235296e-05,
      "loss": 2.585,
      "step": 9390
    },
    {
      "epoch": 552.9411764705883,
      "grad_norm": 5.991398811340332,
      "learning_rate": 4.447058823529412e-05,
      "loss": 2.7339,
      "step": 9400
    },
    {
      "epoch": 553.5294117647059,
      "grad_norm": 6.217681884765625,
      "learning_rate": 4.446470588235294e-05,
      "loss": 2.62,
      "step": 9410
    },
    {
      "epoch": 554.1176470588235,
      "grad_norm": 5.701542854309082,
      "learning_rate": 4.445882352941177e-05,
      "loss": 2.5868,
      "step": 9420
    },
    {
      "epoch": 554.7058823529412,
      "grad_norm": 8.012191772460938,
      "learning_rate": 4.4452941176470595e-05,
      "loss": 2.5731,
      "step": 9430
    },
    {
      "epoch": 555.2941176470588,
      "grad_norm": 6.779556751251221,
      "learning_rate": 4.444705882352941e-05,
      "loss": 2.5749,
      "step": 9440
    },
    {
      "epoch": 555.8823529411765,
      "grad_norm": 5.799886703491211,
      "learning_rate": 4.4441176470588234e-05,
      "loss": 2.4883,
      "step": 9450
    },
    {
      "epoch": 556.4705882352941,
      "grad_norm": 8.116607666015625,
      "learning_rate": 4.443529411764706e-05,
      "loss": 2.6138,
      "step": 9460
    },
    {
      "epoch": 557.0588235294117,
      "grad_norm": 6.684986591339111,
      "learning_rate": 4.442941176470589e-05,
      "loss": 2.5594,
      "step": 9470
    },
    {
      "epoch": 557.6470588235294,
      "grad_norm": 6.3466057777404785,
      "learning_rate": 4.442352941176471e-05,
      "loss": 2.6255,
      "step": 9480
    },
    {
      "epoch": 558.2352941176471,
      "grad_norm": 5.508194923400879,
      "learning_rate": 4.441764705882353e-05,
      "loss": 2.6307,
      "step": 9490
    },
    {
      "epoch": 558.8235294117648,
      "grad_norm": 4.4463582038879395,
      "learning_rate": 4.4411764705882356e-05,
      "loss": 2.4838,
      "step": 9500
    },
    {
      "epoch": 559.4117647058823,
      "grad_norm": 8.028951644897461,
      "learning_rate": 4.440588235294118e-05,
      "loss": 2.7741,
      "step": 9510
    },
    {
      "epoch": 560.0,
      "grad_norm": 6.251744270324707,
      "learning_rate": 4.44e-05,
      "loss": 2.4604,
      "step": 9520
    },
    {
      "epoch": 560.5882352941177,
      "grad_norm": 5.909728050231934,
      "learning_rate": 4.4394117647058825e-05,
      "loss": 2.4808,
      "step": 9530
    },
    {
      "epoch": 561.1764705882352,
      "grad_norm": 5.6935577392578125,
      "learning_rate": 4.438823529411765e-05,
      "loss": 2.634,
      "step": 9540
    },
    {
      "epoch": 561.7647058823529,
      "grad_norm": 9.85890007019043,
      "learning_rate": 4.438235294117648e-05,
      "loss": 2.4988,
      "step": 9550
    },
    {
      "epoch": 562.3529411764706,
      "grad_norm": 5.519827365875244,
      "learning_rate": 4.4376470588235294e-05,
      "loss": 2.5643,
      "step": 9560
    },
    {
      "epoch": 562.9411764705883,
      "grad_norm": 6.509539604187012,
      "learning_rate": 4.437058823529412e-05,
      "loss": 2.669,
      "step": 9570
    },
    {
      "epoch": 563.5294117647059,
      "grad_norm": 6.283972263336182,
      "learning_rate": 4.436470588235294e-05,
      "loss": 2.5397,
      "step": 9580
    },
    {
      "epoch": 564.1176470588235,
      "grad_norm": 6.04908561706543,
      "learning_rate": 4.435882352941177e-05,
      "loss": 2.5872,
      "step": 9590
    },
    {
      "epoch": 564.7058823529412,
      "grad_norm": 5.501291275024414,
      "learning_rate": 4.435294117647059e-05,
      "loss": 2.6798,
      "step": 9600
    },
    {
      "epoch": 565.2941176470588,
      "grad_norm": 6.545689582824707,
      "learning_rate": 4.4347058823529415e-05,
      "loss": 2.5269,
      "step": 9610
    },
    {
      "epoch": 565.8823529411765,
      "grad_norm": 6.800654411315918,
      "learning_rate": 4.434117647058824e-05,
      "loss": 2.7173,
      "step": 9620
    },
    {
      "epoch": 566.4705882352941,
      "grad_norm": 4.688452243804932,
      "learning_rate": 4.433529411764706e-05,
      "loss": 2.5719,
      "step": 9630
    },
    {
      "epoch": 567.0588235294117,
      "grad_norm": 6.424612045288086,
      "learning_rate": 4.4329411764705884e-05,
      "loss": 2.5051,
      "step": 9640
    },
    {
      "epoch": 567.6470588235294,
      "grad_norm": 7.236243724822998,
      "learning_rate": 4.432352941176471e-05,
      "loss": 2.6023,
      "step": 9650
    },
    {
      "epoch": 568.2352941176471,
      "grad_norm": 7.253350257873535,
      "learning_rate": 4.431764705882353e-05,
      "loss": 2.4558,
      "step": 9660
    },
    {
      "epoch": 568.8235294117648,
      "grad_norm": 8.2889986038208,
      "learning_rate": 4.4311764705882353e-05,
      "loss": 2.426,
      "step": 9670
    },
    {
      "epoch": 569.4117647058823,
      "grad_norm": 6.503994464874268,
      "learning_rate": 4.430588235294118e-05,
      "loss": 2.6173,
      "step": 9680
    },
    {
      "epoch": 570.0,
      "grad_norm": 7.044290065765381,
      "learning_rate": 4.43e-05,
      "loss": 2.7446,
      "step": 9690
    },
    {
      "epoch": 570.5882352941177,
      "grad_norm": 7.075687885284424,
      "learning_rate": 4.429411764705882e-05,
      "loss": 2.4596,
      "step": 9700
    },
    {
      "epoch": 571.1764705882352,
      "grad_norm": 7.167264938354492,
      "learning_rate": 4.4288235294117645e-05,
      "loss": 2.5104,
      "step": 9710
    },
    {
      "epoch": 571.7647058823529,
      "grad_norm": 6.183384895324707,
      "learning_rate": 4.4282352941176475e-05,
      "loss": 2.739,
      "step": 9720
    },
    {
      "epoch": 572.3529411764706,
      "grad_norm": 9.480733871459961,
      "learning_rate": 4.42764705882353e-05,
      "loss": 2.4713,
      "step": 9730
    },
    {
      "epoch": 572.9411764705883,
      "grad_norm": 5.9375152587890625,
      "learning_rate": 4.427058823529412e-05,
      "loss": 2.5379,
      "step": 9740
    },
    {
      "epoch": 573.5294117647059,
      "grad_norm": 6.308737754821777,
      "learning_rate": 4.4264705882352944e-05,
      "loss": 2.5579,
      "step": 9750
    },
    {
      "epoch": 574.1176470588235,
      "grad_norm": 6.079983711242676,
      "learning_rate": 4.425882352941177e-05,
      "loss": 2.6832,
      "step": 9760
    },
    {
      "epoch": 574.7058823529412,
      "grad_norm": 8.459444046020508,
      "learning_rate": 4.425294117647059e-05,
      "loss": 2.6007,
      "step": 9770
    },
    {
      "epoch": 575.2941176470588,
      "grad_norm": 6.551179885864258,
      "learning_rate": 4.424705882352941e-05,
      "loss": 2.5361,
      "step": 9780
    },
    {
      "epoch": 575.8823529411765,
      "grad_norm": 8.318785667419434,
      "learning_rate": 4.4241176470588236e-05,
      "loss": 2.5875,
      "step": 9790
    },
    {
      "epoch": 576.4705882352941,
      "grad_norm": 6.293193817138672,
      "learning_rate": 4.4235294117647066e-05,
      "loss": 2.4388,
      "step": 9800
    },
    {
      "epoch": 577.0588235294117,
      "grad_norm": 7.216352462768555,
      "learning_rate": 4.422941176470589e-05,
      "loss": 2.6795,
      "step": 9810
    },
    {
      "epoch": 577.6470588235294,
      "grad_norm": 8.274787902832031,
      "learning_rate": 4.4223529411764705e-05,
      "loss": 2.6531,
      "step": 9820
    },
    {
      "epoch": 578.2352941176471,
      "grad_norm": 6.808097839355469,
      "learning_rate": 4.421764705882353e-05,
      "loss": 2.5599,
      "step": 9830
    },
    {
      "epoch": 578.8235294117648,
      "grad_norm": 5.8368821144104,
      "learning_rate": 4.421176470588235e-05,
      "loss": 2.4573,
      "step": 9840
    },
    {
      "epoch": 579.4117647058823,
      "grad_norm": 5.493198394775391,
      "learning_rate": 4.420588235294118e-05,
      "loss": 2.556,
      "step": 9850
    },
    {
      "epoch": 580.0,
      "grad_norm": 7.315199375152588,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 2.7625,
      "step": 9860
    },
    {
      "epoch": 580.5882352941177,
      "grad_norm": 6.359651565551758,
      "learning_rate": 4.419411764705883e-05,
      "loss": 2.6547,
      "step": 9870
    },
    {
      "epoch": 581.1764705882352,
      "grad_norm": 5.847802639007568,
      "learning_rate": 4.418823529411765e-05,
      "loss": 2.5388,
      "step": 9880
    },
    {
      "epoch": 581.7647058823529,
      "grad_norm": 5.747915267944336,
      "learning_rate": 4.418235294117647e-05,
      "loss": 2.6383,
      "step": 9890
    },
    {
      "epoch": 582.3529411764706,
      "grad_norm": 6.617254257202148,
      "learning_rate": 4.4176470588235296e-05,
      "loss": 2.4327,
      "step": 9900
    },
    {
      "epoch": 582.9411764705883,
      "grad_norm": 7.394537448883057,
      "learning_rate": 4.417058823529412e-05,
      "loss": 2.6227,
      "step": 9910
    },
    {
      "epoch": 583.5294117647059,
      "grad_norm": 5.841801643371582,
      "learning_rate": 4.416470588235294e-05,
      "loss": 2.6193,
      "step": 9920
    },
    {
      "epoch": 584.1176470588235,
      "grad_norm": 7.780980110168457,
      "learning_rate": 4.415882352941177e-05,
      "loss": 2.5656,
      "step": 9930
    },
    {
      "epoch": 584.7058823529412,
      "grad_norm": 7.563164710998535,
      "learning_rate": 4.4152941176470594e-05,
      "loss": 2.4988,
      "step": 9940
    },
    {
      "epoch": 585.2941176470588,
      "grad_norm": 7.02275276184082,
      "learning_rate": 4.414705882352941e-05,
      "loss": 2.6265,
      "step": 9950
    },
    {
      "epoch": 585.8823529411765,
      "grad_norm": 7.200617790222168,
      "learning_rate": 4.4141176470588234e-05,
      "loss": 2.5847,
      "step": 9960
    },
    {
      "epoch": 586.4705882352941,
      "grad_norm": 5.825108528137207,
      "learning_rate": 4.413529411764706e-05,
      "loss": 2.5383,
      "step": 9970
    },
    {
      "epoch": 587.0588235294117,
      "grad_norm": 7.206190586090088,
      "learning_rate": 4.4129411764705886e-05,
      "loss": 2.5963,
      "step": 9980
    },
    {
      "epoch": 587.6470588235294,
      "grad_norm": 7.0064287185668945,
      "learning_rate": 4.412352941176471e-05,
      "loss": 2.6555,
      "step": 9990
    },
    {
      "epoch": 588.2352941176471,
      "grad_norm": 6.376890182495117,
      "learning_rate": 4.411764705882353e-05,
      "loss": 2.5605,
      "step": 10000
    },
    {
      "epoch": 588.8235294117648,
      "grad_norm": 6.5583176612854,
      "learning_rate": 4.4111764705882355e-05,
      "loss": 2.5055,
      "step": 10010
    },
    {
      "epoch": 589.4117647058823,
      "grad_norm": 6.618348598480225,
      "learning_rate": 4.410588235294118e-05,
      "loss": 2.5522,
      "step": 10020
    },
    {
      "epoch": 590.0,
      "grad_norm": 8.062836647033691,
      "learning_rate": 4.41e-05,
      "loss": 2.5572,
      "step": 10030
    },
    {
      "epoch": 590.5882352941177,
      "grad_norm": 7.382353782653809,
      "learning_rate": 4.4094117647058824e-05,
      "loss": 2.5445,
      "step": 10040
    },
    {
      "epoch": 591.1764705882352,
      "grad_norm": 5.883112907409668,
      "learning_rate": 4.408823529411765e-05,
      "loss": 2.5938,
      "step": 10050
    },
    {
      "epoch": 591.7647058823529,
      "grad_norm": 7.303788661956787,
      "learning_rate": 4.408235294117648e-05,
      "loss": 2.5823,
      "step": 10060
    },
    {
      "epoch": 592.3529411764706,
      "grad_norm": 5.350208759307861,
      "learning_rate": 4.40764705882353e-05,
      "loss": 2.5527,
      "step": 10070
    },
    {
      "epoch": 592.9411764705883,
      "grad_norm": 7.721700668334961,
      "learning_rate": 4.4070588235294116e-05,
      "loss": 2.4701,
      "step": 10080
    },
    {
      "epoch": 593.5294117647059,
      "grad_norm": 6.252139091491699,
      "learning_rate": 4.406470588235294e-05,
      "loss": 2.5895,
      "step": 10090
    },
    {
      "epoch": 594.1176470588235,
      "grad_norm": 8.891708374023438,
      "learning_rate": 4.405882352941177e-05,
      "loss": 2.5182,
      "step": 10100
    },
    {
      "epoch": 594.7058823529412,
      "grad_norm": 6.937434196472168,
      "learning_rate": 4.405294117647059e-05,
      "loss": 2.563,
      "step": 10110
    },
    {
      "epoch": 595.2941176470588,
      "grad_norm": 8.689627647399902,
      "learning_rate": 4.4047058823529415e-05,
      "loss": 2.6334,
      "step": 10120
    },
    {
      "epoch": 595.8823529411765,
      "grad_norm": 10.181591033935547,
      "learning_rate": 4.404117647058824e-05,
      "loss": 2.5541,
      "step": 10130
    },
    {
      "epoch": 596.4705882352941,
      "grad_norm": 6.340726852416992,
      "learning_rate": 4.403529411764706e-05,
      "loss": 2.6228,
      "step": 10140
    },
    {
      "epoch": 597.0588235294117,
      "grad_norm": 7.113727569580078,
      "learning_rate": 4.4029411764705884e-05,
      "loss": 2.6236,
      "step": 10150
    },
    {
      "epoch": 597.6470588235294,
      "grad_norm": 6.524574279785156,
      "learning_rate": 4.402352941176471e-05,
      "loss": 2.5346,
      "step": 10160
    },
    {
      "epoch": 598.2352941176471,
      "grad_norm": 8.383344650268555,
      "learning_rate": 4.401764705882353e-05,
      "loss": 2.583,
      "step": 10170
    },
    {
      "epoch": 598.8235294117648,
      "grad_norm": 4.833410263061523,
      "learning_rate": 4.401176470588235e-05,
      "loss": 2.4937,
      "step": 10180
    },
    {
      "epoch": 599.4117647058823,
      "grad_norm": 5.570763111114502,
      "learning_rate": 4.400588235294118e-05,
      "loss": 2.532,
      "step": 10190
    },
    {
      "epoch": 600.0,
      "grad_norm": 7.882204532623291,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.5898,
      "step": 10200
    },
    {
      "epoch": 600.5882352941177,
      "grad_norm": 7.246582984924316,
      "learning_rate": 4.399411764705882e-05,
      "loss": 2.4408,
      "step": 10210
    },
    {
      "epoch": 601.1764705882352,
      "grad_norm": 7.088494777679443,
      "learning_rate": 4.3988235294117645e-05,
      "loss": 2.611,
      "step": 10220
    },
    {
      "epoch": 601.7647058823529,
      "grad_norm": 9.17350959777832,
      "learning_rate": 4.3982352941176475e-05,
      "loss": 2.58,
      "step": 10230
    },
    {
      "epoch": 602.3529411764706,
      "grad_norm": 7.271414756774902,
      "learning_rate": 4.39764705882353e-05,
      "loss": 2.3886,
      "step": 10240
    },
    {
      "epoch": 602.9411764705883,
      "grad_norm": 8.022132873535156,
      "learning_rate": 4.397058823529412e-05,
      "loss": 2.5614,
      "step": 10250
    },
    {
      "epoch": 603.5294117647059,
      "grad_norm": 5.274277687072754,
      "learning_rate": 4.3964705882352944e-05,
      "loss": 2.5049,
      "step": 10260
    },
    {
      "epoch": 604.1176470588235,
      "grad_norm": 11.593769073486328,
      "learning_rate": 4.3958823529411767e-05,
      "loss": 2.4385,
      "step": 10270
    },
    {
      "epoch": 604.7058823529412,
      "grad_norm": 7.930903434753418,
      "learning_rate": 4.395294117647059e-05,
      "loss": 2.5043,
      "step": 10280
    },
    {
      "epoch": 605.2941176470588,
      "grad_norm": 8.291908264160156,
      "learning_rate": 4.394705882352941e-05,
      "loss": 2.5718,
      "step": 10290
    },
    {
      "epoch": 605.8823529411765,
      "grad_norm": 7.130170822143555,
      "learning_rate": 4.3941176470588236e-05,
      "loss": 2.5511,
      "step": 10300
    },
    {
      "epoch": 606.4705882352941,
      "grad_norm": 6.773037910461426,
      "learning_rate": 4.3935294117647065e-05,
      "loss": 2.6285,
      "step": 10310
    },
    {
      "epoch": 607.0588235294117,
      "grad_norm": 7.713050842285156,
      "learning_rate": 4.392941176470589e-05,
      "loss": 2.453,
      "step": 10320
    },
    {
      "epoch": 607.6470588235294,
      "grad_norm": 7.340815544128418,
      "learning_rate": 4.3923529411764704e-05,
      "loss": 2.5241,
      "step": 10330
    },
    {
      "epoch": 608.2352941176471,
      "grad_norm": 7.146053791046143,
      "learning_rate": 4.391764705882353e-05,
      "loss": 2.5336,
      "step": 10340
    },
    {
      "epoch": 608.8235294117648,
      "grad_norm": 8.496742248535156,
      "learning_rate": 4.391176470588236e-05,
      "loss": 2.517,
      "step": 10350
    },
    {
      "epoch": 609.4117647058823,
      "grad_norm": 7.978594779968262,
      "learning_rate": 4.390588235294118e-05,
      "loss": 2.53,
      "step": 10360
    },
    {
      "epoch": 610.0,
      "grad_norm": 7.5765061378479,
      "learning_rate": 4.39e-05,
      "loss": 2.5902,
      "step": 10370
    },
    {
      "epoch": 610.5882352941177,
      "grad_norm": 7.264175891876221,
      "learning_rate": 4.3894117647058826e-05,
      "loss": 2.5206,
      "step": 10380
    },
    {
      "epoch": 611.1764705882352,
      "grad_norm": 6.797484397888184,
      "learning_rate": 4.388823529411765e-05,
      "loss": 2.5182,
      "step": 10390
    },
    {
      "epoch": 611.7647058823529,
      "grad_norm": 6.629887580871582,
      "learning_rate": 4.388235294117647e-05,
      "loss": 2.6245,
      "step": 10400
    },
    {
      "epoch": 612.3529411764706,
      "grad_norm": 6.085594654083252,
      "learning_rate": 4.3876470588235295e-05,
      "loss": 2.5388,
      "step": 10410
    },
    {
      "epoch": 612.9411764705883,
      "grad_norm": 7.408662796020508,
      "learning_rate": 4.387058823529412e-05,
      "loss": 2.4727,
      "step": 10420
    },
    {
      "epoch": 613.5294117647059,
      "grad_norm": 5.899563312530518,
      "learning_rate": 4.386470588235294e-05,
      "loss": 2.513,
      "step": 10430
    },
    {
      "epoch": 614.1176470588235,
      "grad_norm": 6.273059844970703,
      "learning_rate": 4.385882352941177e-05,
      "loss": 2.5707,
      "step": 10440
    },
    {
      "epoch": 614.7058823529412,
      "grad_norm": 6.6173272132873535,
      "learning_rate": 4.3852941176470594e-05,
      "loss": 2.4771,
      "step": 10450
    },
    {
      "epoch": 615.2941176470588,
      "grad_norm": 6.8681840896606445,
      "learning_rate": 4.384705882352941e-05,
      "loss": 2.3378,
      "step": 10460
    },
    {
      "epoch": 615.8823529411765,
      "grad_norm": 6.165900707244873,
      "learning_rate": 4.384117647058823e-05,
      "loss": 2.5889,
      "step": 10470
    },
    {
      "epoch": 616.4705882352941,
      "grad_norm": 5.8859782218933105,
      "learning_rate": 4.383529411764706e-05,
      "loss": 2.5342,
      "step": 10480
    },
    {
      "epoch": 617.0588235294117,
      "grad_norm": 8.570508003234863,
      "learning_rate": 4.3829411764705886e-05,
      "loss": 2.4604,
      "step": 10490
    },
    {
      "epoch": 617.6470588235294,
      "grad_norm": 11.124624252319336,
      "learning_rate": 4.382352941176471e-05,
      "loss": 2.5379,
      "step": 10500
    },
    {
      "epoch": 618.2352941176471,
      "grad_norm": 9.06743335723877,
      "learning_rate": 4.381764705882353e-05,
      "loss": 2.6146,
      "step": 10510
    },
    {
      "epoch": 618.8235294117648,
      "grad_norm": 6.323627471923828,
      "learning_rate": 4.3811764705882355e-05,
      "loss": 2.4017,
      "step": 10520
    },
    {
      "epoch": 619.4117647058823,
      "grad_norm": 7.89569616317749,
      "learning_rate": 4.380588235294118e-05,
      "loss": 2.5723,
      "step": 10530
    },
    {
      "epoch": 620.0,
      "grad_norm": 8.714694023132324,
      "learning_rate": 4.38e-05,
      "loss": 2.668,
      "step": 10540
    },
    {
      "epoch": 620.5882352941177,
      "grad_norm": 6.399235248565674,
      "learning_rate": 4.3794117647058824e-05,
      "loss": 2.3441,
      "step": 10550
    },
    {
      "epoch": 621.1764705882352,
      "grad_norm": 6.3192315101623535,
      "learning_rate": 4.378823529411765e-05,
      "loss": 2.5777,
      "step": 10560
    },
    {
      "epoch": 621.7647058823529,
      "grad_norm": 7.437580585479736,
      "learning_rate": 4.3782352941176477e-05,
      "loss": 2.4242,
      "step": 10570
    },
    {
      "epoch": 622.3529411764706,
      "grad_norm": 6.769479751586914,
      "learning_rate": 4.37764705882353e-05,
      "loss": 2.5367,
      "step": 10580
    },
    {
      "epoch": 622.9411764705883,
      "grad_norm": 6.063060283660889,
      "learning_rate": 4.3770588235294116e-05,
      "loss": 2.5438,
      "step": 10590
    },
    {
      "epoch": 623.5294117647059,
      "grad_norm": 6.409984111785889,
      "learning_rate": 4.376470588235294e-05,
      "loss": 2.5651,
      "step": 10600
    },
    {
      "epoch": 624.1176470588235,
      "grad_norm": 8.126779556274414,
      "learning_rate": 4.375882352941177e-05,
      "loss": 2.4704,
      "step": 10610
    },
    {
      "epoch": 624.7058823529412,
      "grad_norm": 6.211299896240234,
      "learning_rate": 4.375294117647059e-05,
      "loss": 2.5376,
      "step": 10620
    },
    {
      "epoch": 625.2941176470588,
      "grad_norm": 8.387174606323242,
      "learning_rate": 4.3747058823529414e-05,
      "loss": 2.474,
      "step": 10630
    },
    {
      "epoch": 625.8823529411765,
      "grad_norm": 6.437540531158447,
      "learning_rate": 4.374117647058824e-05,
      "loss": 2.6581,
      "step": 10640
    },
    {
      "epoch": 626.4705882352941,
      "grad_norm": 10.111109733581543,
      "learning_rate": 4.373529411764706e-05,
      "loss": 2.5283,
      "step": 10650
    },
    {
      "epoch": 627.0588235294117,
      "grad_norm": 9.270008087158203,
      "learning_rate": 4.3729411764705883e-05,
      "loss": 2.6247,
      "step": 10660
    },
    {
      "epoch": 627.6470588235294,
      "grad_norm": 6.666697978973389,
      "learning_rate": 4.3723529411764706e-05,
      "loss": 2.6401,
      "step": 10670
    },
    {
      "epoch": 628.2352941176471,
      "grad_norm": 8.752827644348145,
      "learning_rate": 4.371764705882353e-05,
      "loss": 2.4794,
      "step": 10680
    },
    {
      "epoch": 628.8235294117648,
      "grad_norm": 6.552857875823975,
      "learning_rate": 4.371176470588236e-05,
      "loss": 2.5838,
      "step": 10690
    },
    {
      "epoch": 629.4117647058823,
      "grad_norm": 8.877348899841309,
      "learning_rate": 4.370588235294118e-05,
      "loss": 2.4831,
      "step": 10700
    },
    {
      "epoch": 630.0,
      "grad_norm": 7.77608060836792,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 2.3877,
      "step": 10710
    },
    {
      "epoch": 630.5882352941177,
      "grad_norm": 6.313931941986084,
      "learning_rate": 4.369411764705882e-05,
      "loss": 2.5586,
      "step": 10720
    },
    {
      "epoch": 631.1764705882352,
      "grad_norm": 7.101028919219971,
      "learning_rate": 4.3688235294117644e-05,
      "loss": 2.5207,
      "step": 10730
    },
    {
      "epoch": 631.7647058823529,
      "grad_norm": 9.251930236816406,
      "learning_rate": 4.3682352941176474e-05,
      "loss": 2.5673,
      "step": 10740
    },
    {
      "epoch": 632.3529411764706,
      "grad_norm": 7.3613667488098145,
      "learning_rate": 4.36764705882353e-05,
      "loss": 2.6948,
      "step": 10750
    },
    {
      "epoch": 632.9411764705883,
      "grad_norm": 8.102230072021484,
      "learning_rate": 4.367058823529412e-05,
      "loss": 2.5172,
      "step": 10760
    },
    {
      "epoch": 633.5294117647059,
      "grad_norm": 8.0179443359375,
      "learning_rate": 4.366470588235294e-05,
      "loss": 2.3783,
      "step": 10770
    },
    {
      "epoch": 634.1176470588235,
      "grad_norm": 7.1956963539123535,
      "learning_rate": 4.3658823529411766e-05,
      "loss": 2.5198,
      "step": 10780
    },
    {
      "epoch": 634.7058823529412,
      "grad_norm": 6.6417317390441895,
      "learning_rate": 4.365294117647059e-05,
      "loss": 2.4613,
      "step": 10790
    },
    {
      "epoch": 635.2941176470588,
      "grad_norm": 7.107535362243652,
      "learning_rate": 4.364705882352941e-05,
      "loss": 2.4716,
      "step": 10800
    },
    {
      "epoch": 635.8823529411765,
      "grad_norm": 6.560012340545654,
      "learning_rate": 4.3641176470588235e-05,
      "loss": 2.6222,
      "step": 10810
    },
    {
      "epoch": 636.4705882352941,
      "grad_norm": 5.813868999481201,
      "learning_rate": 4.3635294117647065e-05,
      "loss": 2.4154,
      "step": 10820
    },
    {
      "epoch": 637.0588235294117,
      "grad_norm": 5.8884477615356445,
      "learning_rate": 4.362941176470589e-05,
      "loss": 2.4765,
      "step": 10830
    },
    {
      "epoch": 637.6470588235294,
      "grad_norm": 5.506068229675293,
      "learning_rate": 4.362352941176471e-05,
      "loss": 2.5958,
      "step": 10840
    },
    {
      "epoch": 638.2352941176471,
      "grad_norm": 6.809711456298828,
      "learning_rate": 4.361764705882353e-05,
      "loss": 2.4409,
      "step": 10850
    },
    {
      "epoch": 638.8235294117648,
      "grad_norm": 6.113213062286377,
      "learning_rate": 4.361176470588236e-05,
      "loss": 2.6584,
      "step": 10860
    },
    {
      "epoch": 639.4117647058823,
      "grad_norm": 6.1208648681640625,
      "learning_rate": 4.360588235294118e-05,
      "loss": 2.3904,
      "step": 10870
    },
    {
      "epoch": 640.0,
      "grad_norm": 9.834654808044434,
      "learning_rate": 4.36e-05,
      "loss": 2.3628,
      "step": 10880
    },
    {
      "epoch": 640.5882352941177,
      "grad_norm": 10.566854476928711,
      "learning_rate": 4.3594117647058826e-05,
      "loss": 2.5584,
      "step": 10890
    },
    {
      "epoch": 641.1764705882352,
      "grad_norm": 8.397210121154785,
      "learning_rate": 4.358823529411765e-05,
      "loss": 2.5783,
      "step": 10900
    },
    {
      "epoch": 641.7647058823529,
      "grad_norm": 5.879737377166748,
      "learning_rate": 4.358235294117647e-05,
      "loss": 2.4558,
      "step": 10910
    },
    {
      "epoch": 642.3529411764706,
      "grad_norm": 6.364665985107422,
      "learning_rate": 4.3576470588235295e-05,
      "loss": 2.38,
      "step": 10920
    },
    {
      "epoch": 642.9411764705883,
      "grad_norm": 8.70871639251709,
      "learning_rate": 4.357058823529412e-05,
      "loss": 2.5038,
      "step": 10930
    },
    {
      "epoch": 643.5294117647059,
      "grad_norm": 6.985787868499756,
      "learning_rate": 4.356470588235294e-05,
      "loss": 2.5218,
      "step": 10940
    },
    {
      "epoch": 644.1176470588235,
      "grad_norm": 8.637277603149414,
      "learning_rate": 4.355882352941177e-05,
      "loss": 2.5439,
      "step": 10950
    },
    {
      "epoch": 644.7058823529412,
      "grad_norm": 5.83228063583374,
      "learning_rate": 4.355294117647059e-05,
      "loss": 2.3976,
      "step": 10960
    },
    {
      "epoch": 645.2941176470588,
      "grad_norm": 6.057901859283447,
      "learning_rate": 4.3547058823529416e-05,
      "loss": 2.5079,
      "step": 10970
    },
    {
      "epoch": 645.8823529411765,
      "grad_norm": 9.455888748168945,
      "learning_rate": 4.354117647058823e-05,
      "loss": 2.5754,
      "step": 10980
    },
    {
      "epoch": 646.4705882352941,
      "grad_norm": 8.631431579589844,
      "learning_rate": 4.353529411764706e-05,
      "loss": 2.507,
      "step": 10990
    },
    {
      "epoch": 647.0588235294117,
      "grad_norm": 7.36447286605835,
      "learning_rate": 4.3529411764705885e-05,
      "loss": 2.4852,
      "step": 11000
    },
    {
      "epoch": 647.6470588235294,
      "grad_norm": 8.243206024169922,
      "learning_rate": 4.352352941176471e-05,
      "loss": 2.5543,
      "step": 11010
    },
    {
      "epoch": 648.2352941176471,
      "grad_norm": 7.221534252166748,
      "learning_rate": 4.351764705882353e-05,
      "loss": 2.485,
      "step": 11020
    },
    {
      "epoch": 648.8235294117648,
      "grad_norm": 6.953683853149414,
      "learning_rate": 4.3511764705882354e-05,
      "loss": 2.5651,
      "step": 11030
    },
    {
      "epoch": 649.4117647058823,
      "grad_norm": 8.550655364990234,
      "learning_rate": 4.350588235294118e-05,
      "loss": 2.4424,
      "step": 11040
    },
    {
      "epoch": 650.0,
      "grad_norm": 7.269076347351074,
      "learning_rate": 4.35e-05,
      "loss": 2.466,
      "step": 11050
    },
    {
      "epoch": 650.5882352941177,
      "grad_norm": 6.184641361236572,
      "learning_rate": 4.349411764705882e-05,
      "loss": 2.5447,
      "step": 11060
    },
    {
      "epoch": 651.1764705882352,
      "grad_norm": 7.706774711608887,
      "learning_rate": 4.348823529411765e-05,
      "loss": 2.4648,
      "step": 11070
    },
    {
      "epoch": 651.7647058823529,
      "grad_norm": 8.012035369873047,
      "learning_rate": 4.3482352941176476e-05,
      "loss": 2.5526,
      "step": 11080
    },
    {
      "epoch": 652.3529411764706,
      "grad_norm": 8.498109817504883,
      "learning_rate": 4.34764705882353e-05,
      "loss": 2.5711,
      "step": 11090
    },
    {
      "epoch": 652.9411764705883,
      "grad_norm": 7.261684417724609,
      "learning_rate": 4.3470588235294115e-05,
      "loss": 2.6157,
      "step": 11100
    },
    {
      "epoch": 653.5294117647059,
      "grad_norm": 6.575220108032227,
      "learning_rate": 4.346470588235294e-05,
      "loss": 2.326,
      "step": 11110
    },
    {
      "epoch": 654.1176470588235,
      "grad_norm": 5.541048526763916,
      "learning_rate": 4.345882352941177e-05,
      "loss": 2.4547,
      "step": 11120
    },
    {
      "epoch": 654.7058823529412,
      "grad_norm": 7.369632720947266,
      "learning_rate": 4.345294117647059e-05,
      "loss": 2.5246,
      "step": 11130
    },
    {
      "epoch": 655.2941176470588,
      "grad_norm": 6.533823013305664,
      "learning_rate": 4.3447058823529414e-05,
      "loss": 2.495,
      "step": 11140
    },
    {
      "epoch": 655.8823529411765,
      "grad_norm": 7.046050548553467,
      "learning_rate": 4.344117647058824e-05,
      "loss": 2.3952,
      "step": 11150
    },
    {
      "epoch": 656.4705882352941,
      "grad_norm": 8.165483474731445,
      "learning_rate": 4.343529411764706e-05,
      "loss": 2.4589,
      "step": 11160
    },
    {
      "epoch": 657.0588235294117,
      "grad_norm": 5.187401294708252,
      "learning_rate": 4.342941176470588e-05,
      "loss": 2.4806,
      "step": 11170
    },
    {
      "epoch": 657.6470588235294,
      "grad_norm": 7.249730110168457,
      "learning_rate": 4.3423529411764706e-05,
      "loss": 2.7108,
      "step": 11180
    },
    {
      "epoch": 658.2352941176471,
      "grad_norm": 8.469979286193848,
      "learning_rate": 4.341764705882353e-05,
      "loss": 2.6107,
      "step": 11190
    },
    {
      "epoch": 658.8235294117648,
      "grad_norm": 7.791102886199951,
      "learning_rate": 4.341176470588236e-05,
      "loss": 2.5281,
      "step": 11200
    },
    {
      "epoch": 659.4117647058823,
      "grad_norm": 9.150508880615234,
      "learning_rate": 4.340588235294118e-05,
      "loss": 2.5036,
      "step": 11210
    },
    {
      "epoch": 660.0,
      "grad_norm": 8.162134170532227,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 2.5045,
      "step": 11220
    },
    {
      "epoch": 660.5882352941177,
      "grad_norm": 8.69321346282959,
      "learning_rate": 4.339411764705882e-05,
      "loss": 2.4781,
      "step": 11230
    },
    {
      "epoch": 661.1764705882352,
      "grad_norm": 7.3182244300842285,
      "learning_rate": 4.338823529411765e-05,
      "loss": 2.4943,
      "step": 11240
    },
    {
      "epoch": 661.7647058823529,
      "grad_norm": 6.664271831512451,
      "learning_rate": 4.3382352941176474e-05,
      "loss": 2.4581,
      "step": 11250
    },
    {
      "epoch": 662.3529411764706,
      "grad_norm": 11.921892166137695,
      "learning_rate": 4.3376470588235297e-05,
      "loss": 2.5454,
      "step": 11260
    },
    {
      "epoch": 662.9411764705883,
      "grad_norm": 5.3071794509887695,
      "learning_rate": 4.337058823529412e-05,
      "loss": 2.4363,
      "step": 11270
    },
    {
      "epoch": 663.5294117647059,
      "grad_norm": 6.442382335662842,
      "learning_rate": 4.336470588235294e-05,
      "loss": 2.4733,
      "step": 11280
    },
    {
      "epoch": 664.1176470588235,
      "grad_norm": 7.307966232299805,
      "learning_rate": 4.3358823529411766e-05,
      "loss": 2.3636,
      "step": 11290
    },
    {
      "epoch": 664.7058823529412,
      "grad_norm": 7.973419666290283,
      "learning_rate": 4.335294117647059e-05,
      "loss": 2.4403,
      "step": 11300
    },
    {
      "epoch": 665.2941176470588,
      "grad_norm": 6.781071186065674,
      "learning_rate": 4.334705882352941e-05,
      "loss": 2.6936,
      "step": 11310
    },
    {
      "epoch": 665.8823529411765,
      "grad_norm": 6.985008716583252,
      "learning_rate": 4.3341176470588234e-05,
      "loss": 2.4325,
      "step": 11320
    },
    {
      "epoch": 666.4705882352941,
      "grad_norm": 9.259710311889648,
      "learning_rate": 4.3335294117647064e-05,
      "loss": 2.5494,
      "step": 11330
    },
    {
      "epoch": 667.0588235294117,
      "grad_norm": 6.882449150085449,
      "learning_rate": 4.332941176470589e-05,
      "loss": 2.598,
      "step": 11340
    },
    {
      "epoch": 667.6470588235294,
      "grad_norm": 6.595367908477783,
      "learning_rate": 4.332352941176471e-05,
      "loss": 2.5737,
      "step": 11350
    },
    {
      "epoch": 668.2352941176471,
      "grad_norm": 6.583248615264893,
      "learning_rate": 4.3317647058823526e-05,
      "loss": 2.5296,
      "step": 11360
    },
    {
      "epoch": 668.8235294117648,
      "grad_norm": 8.003667831420898,
      "learning_rate": 4.3311764705882356e-05,
      "loss": 2.4677,
      "step": 11370
    },
    {
      "epoch": 669.4117647058823,
      "grad_norm": 9.474508285522461,
      "learning_rate": 4.330588235294118e-05,
      "loss": 2.4527,
      "step": 11380
    },
    {
      "epoch": 670.0,
      "grad_norm": 8.288052558898926,
      "learning_rate": 4.33e-05,
      "loss": 2.3714,
      "step": 11390
    },
    {
      "epoch": 670.5882352941177,
      "grad_norm": 7.296923637390137,
      "learning_rate": 4.3294117647058825e-05,
      "loss": 2.517,
      "step": 11400
    },
    {
      "epoch": 671.1764705882352,
      "grad_norm": 9.031098365783691,
      "learning_rate": 4.3288235294117655e-05,
      "loss": 2.5236,
      "step": 11410
    },
    {
      "epoch": 671.7647058823529,
      "grad_norm": 6.218034267425537,
      "learning_rate": 4.328235294117647e-05,
      "loss": 2.5192,
      "step": 11420
    },
    {
      "epoch": 672.3529411764706,
      "grad_norm": 7.188657283782959,
      "learning_rate": 4.3276470588235294e-05,
      "loss": 2.5273,
      "step": 11430
    },
    {
      "epoch": 672.9411764705883,
      "grad_norm": 7.340742111206055,
      "learning_rate": 4.327058823529412e-05,
      "loss": 2.3917,
      "step": 11440
    },
    {
      "epoch": 673.5294117647059,
      "grad_norm": 7.325582981109619,
      "learning_rate": 4.326470588235294e-05,
      "loss": 2.5359,
      "step": 11450
    },
    {
      "epoch": 674.1176470588235,
      "grad_norm": 7.473074913024902,
      "learning_rate": 4.325882352941177e-05,
      "loss": 2.4077,
      "step": 11460
    },
    {
      "epoch": 674.7058823529412,
      "grad_norm": 8.968694686889648,
      "learning_rate": 4.325294117647059e-05,
      "loss": 2.592,
      "step": 11470
    },
    {
      "epoch": 675.2941176470588,
      "grad_norm": 7.005386829376221,
      "learning_rate": 4.3247058823529416e-05,
      "loss": 2.5895,
      "step": 11480
    },
    {
      "epoch": 675.8823529411765,
      "grad_norm": 7.794670581817627,
      "learning_rate": 4.324117647058823e-05,
      "loss": 2.4608,
      "step": 11490
    },
    {
      "epoch": 676.4705882352941,
      "grad_norm": 9.233641624450684,
      "learning_rate": 4.323529411764706e-05,
      "loss": 2.6037,
      "step": 11500
    },
    {
      "epoch": 677.0588235294117,
      "grad_norm": 8.158552169799805,
      "learning_rate": 4.3229411764705885e-05,
      "loss": 2.475,
      "step": 11510
    },
    {
      "epoch": 677.6470588235294,
      "grad_norm": 5.572381019592285,
      "learning_rate": 4.322352941176471e-05,
      "loss": 2.3806,
      "step": 11520
    },
    {
      "epoch": 678.2352941176471,
      "grad_norm": 8.721282958984375,
      "learning_rate": 4.321764705882353e-05,
      "loss": 2.5419,
      "step": 11530
    },
    {
      "epoch": 678.8235294117648,
      "grad_norm": 6.716038227081299,
      "learning_rate": 4.3211764705882354e-05,
      "loss": 2.4518,
      "step": 11540
    },
    {
      "epoch": 679.4117647058823,
      "grad_norm": 8.27112865447998,
      "learning_rate": 4.320588235294118e-05,
      "loss": 2.4436,
      "step": 11550
    },
    {
      "epoch": 680.0,
      "grad_norm": 8.82567310333252,
      "learning_rate": 4.32e-05,
      "loss": 2.5123,
      "step": 11560
    },
    {
      "epoch": 680.5882352941177,
      "grad_norm": 7.759181976318359,
      "learning_rate": 4.319411764705882e-05,
      "loss": 2.4457,
      "step": 11570
    },
    {
      "epoch": 681.1764705882352,
      "grad_norm": 9.80737590789795,
      "learning_rate": 4.318823529411765e-05,
      "loss": 2.6043,
      "step": 11580
    },
    {
      "epoch": 681.7647058823529,
      "grad_norm": 11.004327774047852,
      "learning_rate": 4.3182352941176475e-05,
      "loss": 2.395,
      "step": 11590
    },
    {
      "epoch": 682.3529411764706,
      "grad_norm": 7.367632865905762,
      "learning_rate": 4.31764705882353e-05,
      "loss": 2.5437,
      "step": 11600
    },
    {
      "epoch": 682.9411764705883,
      "grad_norm": 8.193273544311523,
      "learning_rate": 4.317058823529412e-05,
      "loss": 2.4957,
      "step": 11610
    },
    {
      "epoch": 683.5294117647059,
      "grad_norm": 7.512339115142822,
      "learning_rate": 4.3164705882352944e-05,
      "loss": 2.5046,
      "step": 11620
    },
    {
      "epoch": 684.1176470588235,
      "grad_norm": 7.812434196472168,
      "learning_rate": 4.315882352941177e-05,
      "loss": 2.4271,
      "step": 11630
    },
    {
      "epoch": 684.7058823529412,
      "grad_norm": 7.101038932800293,
      "learning_rate": 4.315294117647059e-05,
      "loss": 2.4675,
      "step": 11640
    },
    {
      "epoch": 685.2941176470588,
      "grad_norm": 8.313017845153809,
      "learning_rate": 4.3147058823529413e-05,
      "loss": 2.4919,
      "step": 11650
    },
    {
      "epoch": 685.8823529411765,
      "grad_norm": 7.544209957122803,
      "learning_rate": 4.3141176470588236e-05,
      "loss": 2.4707,
      "step": 11660
    },
    {
      "epoch": 686.4705882352941,
      "grad_norm": 7.220925807952881,
      "learning_rate": 4.313529411764706e-05,
      "loss": 2.405,
      "step": 11670
    },
    {
      "epoch": 687.0588235294117,
      "grad_norm": 6.372223377227783,
      "learning_rate": 4.312941176470588e-05,
      "loss": 2.3616,
      "step": 11680
    },
    {
      "epoch": 687.6470588235294,
      "grad_norm": 7.498076915740967,
      "learning_rate": 4.3123529411764705e-05,
      "loss": 2.5542,
      "step": 11690
    },
    {
      "epoch": 688.2352941176471,
      "grad_norm": 8.855295181274414,
      "learning_rate": 4.311764705882353e-05,
      "loss": 2.4126,
      "step": 11700
    },
    {
      "epoch": 688.8235294117648,
      "grad_norm": 5.767844200134277,
      "learning_rate": 4.311176470588236e-05,
      "loss": 2.4702,
      "step": 11710
    },
    {
      "epoch": 689.4117647058823,
      "grad_norm": 7.447583198547363,
      "learning_rate": 4.310588235294118e-05,
      "loss": 2.3409,
      "step": 11720
    },
    {
      "epoch": 690.0,
      "grad_norm": 7.089053630828857,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 2.4805,
      "step": 11730
    },
    {
      "epoch": 690.5882352941177,
      "grad_norm": 9.718208312988281,
      "learning_rate": 4.309411764705883e-05,
      "loss": 2.5251,
      "step": 11740
    },
    {
      "epoch": 691.1764705882352,
      "grad_norm": 7.991934299468994,
      "learning_rate": 4.308823529411765e-05,
      "loss": 2.5017,
      "step": 11750
    },
    {
      "epoch": 691.7647058823529,
      "grad_norm": 9.351628303527832,
      "learning_rate": 4.308235294117647e-05,
      "loss": 2.5758,
      "step": 11760
    },
    {
      "epoch": 692.3529411764706,
      "grad_norm": 8.77591609954834,
      "learning_rate": 4.3076470588235296e-05,
      "loss": 2.453,
      "step": 11770
    },
    {
      "epoch": 692.9411764705883,
      "grad_norm": 7.630061626434326,
      "learning_rate": 4.307058823529412e-05,
      "loss": 2.5635,
      "step": 11780
    },
    {
      "epoch": 693.5294117647059,
      "grad_norm": 8.392230987548828,
      "learning_rate": 4.306470588235295e-05,
      "loss": 2.5289,
      "step": 11790
    },
    {
      "epoch": 694.1176470588235,
      "grad_norm": 10.288302421569824,
      "learning_rate": 4.3058823529411765e-05,
      "loss": 2.3999,
      "step": 11800
    },
    {
      "epoch": 694.7058823529412,
      "grad_norm": 7.315017223358154,
      "learning_rate": 4.305294117647059e-05,
      "loss": 2.5542,
      "step": 11810
    },
    {
      "epoch": 695.2941176470588,
      "grad_norm": 8.093253135681152,
      "learning_rate": 4.304705882352941e-05,
      "loss": 2.5391,
      "step": 11820
    },
    {
      "epoch": 695.8823529411765,
      "grad_norm": 10.3141450881958,
      "learning_rate": 4.3041176470588234e-05,
      "loss": 2.5094,
      "step": 11830
    },
    {
      "epoch": 696.4705882352941,
      "grad_norm": 5.932569980621338,
      "learning_rate": 4.3035294117647064e-05,
      "loss": 2.5452,
      "step": 11840
    },
    {
      "epoch": 697.0588235294117,
      "grad_norm": 7.281031608581543,
      "learning_rate": 4.302941176470589e-05,
      "loss": 2.5474,
      "step": 11850
    },
    {
      "epoch": 697.6470588235294,
      "grad_norm": 6.4812140464782715,
      "learning_rate": 4.302352941176471e-05,
      "loss": 2.3609,
      "step": 11860
    },
    {
      "epoch": 698.2352941176471,
      "grad_norm": 7.761916160583496,
      "learning_rate": 4.3017647058823526e-05,
      "loss": 2.5247,
      "step": 11870
    },
    {
      "epoch": 698.8235294117648,
      "grad_norm": 10.509611129760742,
      "learning_rate": 4.3011764705882356e-05,
      "loss": 2.6652,
      "step": 11880
    },
    {
      "epoch": 699.4117647058823,
      "grad_norm": 7.35728645324707,
      "learning_rate": 4.300588235294118e-05,
      "loss": 2.4147,
      "step": 11890
    },
    {
      "epoch": 700.0,
      "grad_norm": 8.3755464553833,
      "learning_rate": 4.3e-05,
      "loss": 2.5735,
      "step": 11900
    },
    {
      "epoch": 700.5882352941177,
      "grad_norm": 7.152573108673096,
      "learning_rate": 4.2994117647058825e-05,
      "loss": 2.5189,
      "step": 11910
    },
    {
      "epoch": 701.1764705882352,
      "grad_norm": 7.780263423919678,
      "learning_rate": 4.2988235294117654e-05,
      "loss": 2.4987,
      "step": 11920
    },
    {
      "epoch": 701.7647058823529,
      "grad_norm": 6.582975387573242,
      "learning_rate": 4.298235294117647e-05,
      "loss": 2.5072,
      "step": 11930
    },
    {
      "epoch": 702.3529411764706,
      "grad_norm": 7.9775471687316895,
      "learning_rate": 4.2976470588235294e-05,
      "loss": 2.4763,
      "step": 11940
    },
    {
      "epoch": 702.9411764705883,
      "grad_norm": 7.95052433013916,
      "learning_rate": 4.2970588235294117e-05,
      "loss": 2.5346,
      "step": 11950
    },
    {
      "epoch": 703.5294117647059,
      "grad_norm": 7.159454822540283,
      "learning_rate": 4.2964705882352946e-05,
      "loss": 2.3946,
      "step": 11960
    },
    {
      "epoch": 704.1176470588235,
      "grad_norm": 6.072862148284912,
      "learning_rate": 4.295882352941177e-05,
      "loss": 2.3656,
      "step": 11970
    },
    {
      "epoch": 704.7058823529412,
      "grad_norm": 6.414888381958008,
      "learning_rate": 4.295294117647059e-05,
      "loss": 2.4932,
      "step": 11980
    },
    {
      "epoch": 705.2941176470588,
      "grad_norm": 7.755912780761719,
      "learning_rate": 4.2947058823529415e-05,
      "loss": 2.5042,
      "step": 11990
    },
    {
      "epoch": 705.8823529411765,
      "grad_norm": 9.39468002319336,
      "learning_rate": 4.294117647058823e-05,
      "loss": 2.547,
      "step": 12000
    },
    {
      "epoch": 706.4705882352941,
      "grad_norm": 9.307348251342773,
      "learning_rate": 4.293529411764706e-05,
      "loss": 2.5879,
      "step": 12010
    },
    {
      "epoch": 707.0588235294117,
      "grad_norm": 7.8638081550598145,
      "learning_rate": 4.2929411764705884e-05,
      "loss": 2.6107,
      "step": 12020
    },
    {
      "epoch": 707.6470588235294,
      "grad_norm": 7.67689847946167,
      "learning_rate": 4.292352941176471e-05,
      "loss": 2.3704,
      "step": 12030
    },
    {
      "epoch": 708.2352941176471,
      "grad_norm": 9.593748092651367,
      "learning_rate": 4.291764705882353e-05,
      "loss": 2.5089,
      "step": 12040
    },
    {
      "epoch": 708.8235294117648,
      "grad_norm": 8.234495162963867,
      "learning_rate": 4.291176470588236e-05,
      "loss": 2.5676,
      "step": 12050
    },
    {
      "epoch": 709.4117647058823,
      "grad_norm": 7.279428958892822,
      "learning_rate": 4.2905882352941176e-05,
      "loss": 2.4449,
      "step": 12060
    },
    {
      "epoch": 710.0,
      "grad_norm": 9.334785461425781,
      "learning_rate": 4.29e-05,
      "loss": 2.4767,
      "step": 12070
    },
    {
      "epoch": 710.5882352941177,
      "grad_norm": 8.047459602355957,
      "learning_rate": 4.289411764705882e-05,
      "loss": 2.4538,
      "step": 12080
    },
    {
      "epoch": 711.1764705882352,
      "grad_norm": 7.416830539703369,
      "learning_rate": 4.288823529411765e-05,
      "loss": 2.3431,
      "step": 12090
    },
    {
      "epoch": 711.7647058823529,
      "grad_norm": 9.362421035766602,
      "learning_rate": 4.2882352941176475e-05,
      "loss": 2.4034,
      "step": 12100
    },
    {
      "epoch": 712.3529411764706,
      "grad_norm": 9.871054649353027,
      "learning_rate": 4.28764705882353e-05,
      "loss": 2.5512,
      "step": 12110
    },
    {
      "epoch": 712.9411764705883,
      "grad_norm": 7.700473308563232,
      "learning_rate": 4.287058823529412e-05,
      "loss": 2.4248,
      "step": 12120
    },
    {
      "epoch": 713.5294117647059,
      "grad_norm": 7.622216701507568,
      "learning_rate": 4.2864705882352944e-05,
      "loss": 2.5349,
      "step": 12130
    },
    {
      "epoch": 714.1176470588235,
      "grad_norm": 8.139777183532715,
      "learning_rate": 4.285882352941177e-05,
      "loss": 2.6576,
      "step": 12140
    },
    {
      "epoch": 714.7058823529412,
      "grad_norm": 10.59482192993164,
      "learning_rate": 4.285294117647059e-05,
      "loss": 2.566,
      "step": 12150
    },
    {
      "epoch": 715.2941176470588,
      "grad_norm": 7.98383092880249,
      "learning_rate": 4.284705882352941e-05,
      "loss": 2.5232,
      "step": 12160
    },
    {
      "epoch": 715.8823529411765,
      "grad_norm": 7.718008995056152,
      "learning_rate": 4.2841176470588236e-05,
      "loss": 2.4698,
      "step": 12170
    },
    {
      "epoch": 716.4705882352941,
      "grad_norm": 8.151469230651855,
      "learning_rate": 4.2835294117647066e-05,
      "loss": 2.3903,
      "step": 12180
    },
    {
      "epoch": 717.0588235294117,
      "grad_norm": 6.068988800048828,
      "learning_rate": 4.282941176470588e-05,
      "loss": 2.3751,
      "step": 12190
    },
    {
      "epoch": 717.6470588235294,
      "grad_norm": 6.957728862762451,
      "learning_rate": 4.2823529411764705e-05,
      "loss": 2.4229,
      "step": 12200
    },
    {
      "epoch": 718.2352941176471,
      "grad_norm": 6.453092575073242,
      "learning_rate": 4.281764705882353e-05,
      "loss": 2.2873,
      "step": 12210
    },
    {
      "epoch": 718.8235294117648,
      "grad_norm": 7.059919834136963,
      "learning_rate": 4.281176470588236e-05,
      "loss": 2.4726,
      "step": 12220
    },
    {
      "epoch": 719.4117647058823,
      "grad_norm": 11.165593147277832,
      "learning_rate": 4.280588235294118e-05,
      "loss": 2.3789,
      "step": 12230
    },
    {
      "epoch": 720.0,
      "grad_norm": 8.554256439208984,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 2.4065,
      "step": 12240
    },
    {
      "epoch": 720.5882352941177,
      "grad_norm": 9.0234375,
      "learning_rate": 4.2794117647058827e-05,
      "loss": 2.5068,
      "step": 12250
    },
    {
      "epoch": 721.1764705882352,
      "grad_norm": 6.353789329528809,
      "learning_rate": 4.278823529411765e-05,
      "loss": 2.3876,
      "step": 12260
    },
    {
      "epoch": 721.7647058823529,
      "grad_norm": 10.905908584594727,
      "learning_rate": 4.278235294117647e-05,
      "loss": 2.5105,
      "step": 12270
    },
    {
      "epoch": 722.3529411764706,
      "grad_norm": 7.7849273681640625,
      "learning_rate": 4.2776470588235296e-05,
      "loss": 2.5367,
      "step": 12280
    },
    {
      "epoch": 722.9411764705883,
      "grad_norm": 7.329533100128174,
      "learning_rate": 4.277058823529412e-05,
      "loss": 2.4159,
      "step": 12290
    },
    {
      "epoch": 723.5294117647059,
      "grad_norm": 8.476338386535645,
      "learning_rate": 4.276470588235295e-05,
      "loss": 2.4716,
      "step": 12300
    },
    {
      "epoch": 724.1176470588235,
      "grad_norm": 11.182329177856445,
      "learning_rate": 4.2758823529411764e-05,
      "loss": 2.4296,
      "step": 12310
    },
    {
      "epoch": 724.7058823529412,
      "grad_norm": 8.267412185668945,
      "learning_rate": 4.275294117647059e-05,
      "loss": 2.4461,
      "step": 12320
    },
    {
      "epoch": 725.2941176470588,
      "grad_norm": 8.045207023620605,
      "learning_rate": 4.274705882352941e-05,
      "loss": 2.3687,
      "step": 12330
    },
    {
      "epoch": 725.8823529411765,
      "grad_norm": 9.380148887634277,
      "learning_rate": 4.274117647058824e-05,
      "loss": 2.3415,
      "step": 12340
    },
    {
      "epoch": 726.4705882352941,
      "grad_norm": 8.151023864746094,
      "learning_rate": 4.273529411764706e-05,
      "loss": 2.4578,
      "step": 12350
    },
    {
      "epoch": 727.0588235294117,
      "grad_norm": 7.804295063018799,
      "learning_rate": 4.2729411764705886e-05,
      "loss": 2.4167,
      "step": 12360
    },
    {
      "epoch": 727.6470588235294,
      "grad_norm": 9.46786117553711,
      "learning_rate": 4.272352941176471e-05,
      "loss": 2.5382,
      "step": 12370
    },
    {
      "epoch": 728.2352941176471,
      "grad_norm": 7.373458385467529,
      "learning_rate": 4.271764705882353e-05,
      "loss": 2.4451,
      "step": 12380
    },
    {
      "epoch": 728.8235294117648,
      "grad_norm": 8.897689819335938,
      "learning_rate": 4.2711764705882355e-05,
      "loss": 2.5575,
      "step": 12390
    },
    {
      "epoch": 729.4117647058823,
      "grad_norm": 6.766746520996094,
      "learning_rate": 4.270588235294118e-05,
      "loss": 2.5288,
      "step": 12400
    },
    {
      "epoch": 730.0,
      "grad_norm": 11.045370101928711,
      "learning_rate": 4.27e-05,
      "loss": 2.3662,
      "step": 12410
    },
    {
      "epoch": 730.5882352941177,
      "grad_norm": 7.818549633026123,
      "learning_rate": 4.2694117647058824e-05,
      "loss": 2.3723,
      "step": 12420
    },
    {
      "epoch": 731.1764705882352,
      "grad_norm": 6.231003761291504,
      "learning_rate": 4.2688235294117654e-05,
      "loss": 2.4136,
      "step": 12430
    },
    {
      "epoch": 731.7647058823529,
      "grad_norm": 8.248189926147461,
      "learning_rate": 4.268235294117647e-05,
      "loss": 2.4901,
      "step": 12440
    },
    {
      "epoch": 732.3529411764706,
      "grad_norm": 8.716909408569336,
      "learning_rate": 4.267647058823529e-05,
      "loss": 2.3444,
      "step": 12450
    },
    {
      "epoch": 732.9411764705883,
      "grad_norm": 6.320550441741943,
      "learning_rate": 4.2670588235294116e-05,
      "loss": 2.2859,
      "step": 12460
    },
    {
      "epoch": 733.5294117647059,
      "grad_norm": 8.659375190734863,
      "learning_rate": 4.2664705882352946e-05,
      "loss": 2.447,
      "step": 12470
    },
    {
      "epoch": 734.1176470588235,
      "grad_norm": 7.773621559143066,
      "learning_rate": 4.265882352941177e-05,
      "loss": 2.3985,
      "step": 12480
    },
    {
      "epoch": 734.7058823529412,
      "grad_norm": 9.403586387634277,
      "learning_rate": 4.265294117647059e-05,
      "loss": 2.3288,
      "step": 12490
    },
    {
      "epoch": 735.2941176470588,
      "grad_norm": 9.428403854370117,
      "learning_rate": 4.2647058823529415e-05,
      "loss": 2.5052,
      "step": 12500
    },
    {
      "epoch": 735.8823529411765,
      "grad_norm": 9.43735408782959,
      "learning_rate": 4.264117647058824e-05,
      "loss": 2.4183,
      "step": 12510
    },
    {
      "epoch": 736.4705882352941,
      "grad_norm": 8.462032318115234,
      "learning_rate": 4.263529411764706e-05,
      "loss": 2.5039,
      "step": 12520
    },
    {
      "epoch": 737.0588235294117,
      "grad_norm": 9.801628112792969,
      "learning_rate": 4.2629411764705884e-05,
      "loss": 2.3875,
      "step": 12530
    },
    {
      "epoch": 737.6470588235294,
      "grad_norm": 7.516066551208496,
      "learning_rate": 4.262352941176471e-05,
      "loss": 2.4362,
      "step": 12540
    },
    {
      "epoch": 738.2352941176471,
      "grad_norm": 8.7501220703125,
      "learning_rate": 4.261764705882353e-05,
      "loss": 2.4497,
      "step": 12550
    },
    {
      "epoch": 738.8235294117648,
      "grad_norm": 7.7939558029174805,
      "learning_rate": 4.261176470588236e-05,
      "loss": 2.4256,
      "step": 12560
    },
    {
      "epoch": 739.4117647058823,
      "grad_norm": 6.593450546264648,
      "learning_rate": 4.2605882352941176e-05,
      "loss": 2.2269,
      "step": 12570
    },
    {
      "epoch": 740.0,
      "grad_norm": 7.962016582489014,
      "learning_rate": 4.26e-05,
      "loss": 2.5145,
      "step": 12580
    },
    {
      "epoch": 740.5882352941177,
      "grad_norm": 8.06233024597168,
      "learning_rate": 4.259411764705882e-05,
      "loss": 2.3199,
      "step": 12590
    },
    {
      "epoch": 741.1764705882352,
      "grad_norm": 7.682052135467529,
      "learning_rate": 4.258823529411765e-05,
      "loss": 2.3342,
      "step": 12600
    },
    {
      "epoch": 741.7647058823529,
      "grad_norm": 6.066977024078369,
      "learning_rate": 4.2582352941176474e-05,
      "loss": 2.4369,
      "step": 12610
    },
    {
      "epoch": 742.3529411764706,
      "grad_norm": 7.785977363586426,
      "learning_rate": 4.25764705882353e-05,
      "loss": 2.2543,
      "step": 12620
    },
    {
      "epoch": 742.9411764705883,
      "grad_norm": 7.708880424499512,
      "learning_rate": 4.257058823529412e-05,
      "loss": 2.4568,
      "step": 12630
    },
    {
      "epoch": 743.5294117647059,
      "grad_norm": 11.386734008789062,
      "learning_rate": 4.2564705882352943e-05,
      "loss": 2.4632,
      "step": 12640
    },
    {
      "epoch": 744.1176470588235,
      "grad_norm": 8.778287887573242,
      "learning_rate": 4.2558823529411766e-05,
      "loss": 2.3039,
      "step": 12650
    },
    {
      "epoch": 744.7058823529412,
      "grad_norm": 6.702001094818115,
      "learning_rate": 4.255294117647059e-05,
      "loss": 2.3585,
      "step": 12660
    },
    {
      "epoch": 745.2941176470588,
      "grad_norm": 7.770513534545898,
      "learning_rate": 4.254705882352941e-05,
      "loss": 2.3132,
      "step": 12670
    },
    {
      "epoch": 745.8823529411765,
      "grad_norm": 5.932409286499023,
      "learning_rate": 4.254117647058824e-05,
      "loss": 2.3161,
      "step": 12680
    },
    {
      "epoch": 746.4705882352941,
      "grad_norm": 9.730159759521484,
      "learning_rate": 4.2535294117647065e-05,
      "loss": 2.5054,
      "step": 12690
    },
    {
      "epoch": 747.0588235294117,
      "grad_norm": 9.803568840026855,
      "learning_rate": 4.252941176470588e-05,
      "loss": 2.4225,
      "step": 12700
    },
    {
      "epoch": 747.6470588235294,
      "grad_norm": 7.830201148986816,
      "learning_rate": 4.2523529411764704e-05,
      "loss": 2.5031,
      "step": 12710
    },
    {
      "epoch": 748.2352941176471,
      "grad_norm": 6.867314338684082,
      "learning_rate": 4.251764705882353e-05,
      "loss": 2.422,
      "step": 12720
    },
    {
      "epoch": 748.8235294117648,
      "grad_norm": 7.483773231506348,
      "learning_rate": 4.251176470588236e-05,
      "loss": 2.5728,
      "step": 12730
    },
    {
      "epoch": 749.4117647058823,
      "grad_norm": 8.573813438415527,
      "learning_rate": 4.250588235294118e-05,
      "loss": 2.3116,
      "step": 12740
    },
    {
      "epoch": 750.0,
      "grad_norm": 11.077871322631836,
      "learning_rate": 4.25e-05,
      "loss": 2.5479,
      "step": 12750
    },
    {
      "epoch": 750.5882352941177,
      "grad_norm": 7.458105564117432,
      "learning_rate": 4.2494117647058826e-05,
      "loss": 2.492,
      "step": 12760
    },
    {
      "epoch": 751.1764705882352,
      "grad_norm": 8.32106876373291,
      "learning_rate": 4.248823529411765e-05,
      "loss": 2.3509,
      "step": 12770
    },
    {
      "epoch": 751.7647058823529,
      "grad_norm": 7.9789934158325195,
      "learning_rate": 4.248235294117647e-05,
      "loss": 2.4901,
      "step": 12780
    },
    {
      "epoch": 752.3529411764706,
      "grad_norm": 7.739683628082275,
      "learning_rate": 4.2476470588235295e-05,
      "loss": 2.4245,
      "step": 12790
    },
    {
      "epoch": 752.9411764705883,
      "grad_norm": 7.841762542724609,
      "learning_rate": 4.247058823529412e-05,
      "loss": 2.4076,
      "step": 12800
    },
    {
      "epoch": 753.5294117647059,
      "grad_norm": 9.57021713256836,
      "learning_rate": 4.246470588235295e-05,
      "loss": 2.3374,
      "step": 12810
    },
    {
      "epoch": 754.1176470588235,
      "grad_norm": 8.624343872070312,
      "learning_rate": 4.245882352941177e-05,
      "loss": 2.3634,
      "step": 12820
    },
    {
      "epoch": 754.7058823529412,
      "grad_norm": 7.570971488952637,
      "learning_rate": 4.245294117647059e-05,
      "loss": 2.4306,
      "step": 12830
    },
    {
      "epoch": 755.2941176470588,
      "grad_norm": 9.884578704833984,
      "learning_rate": 4.244705882352941e-05,
      "loss": 2.382,
      "step": 12840
    },
    {
      "epoch": 755.8823529411765,
      "grad_norm": 8.051460266113281,
      "learning_rate": 4.244117647058824e-05,
      "loss": 2.3483,
      "step": 12850
    },
    {
      "epoch": 756.4705882352941,
      "grad_norm": 9.180910110473633,
      "learning_rate": 4.243529411764706e-05,
      "loss": 2.4385,
      "step": 12860
    },
    {
      "epoch": 757.0588235294117,
      "grad_norm": 6.641872406005859,
      "learning_rate": 4.2429411764705886e-05,
      "loss": 2.372,
      "step": 12870
    },
    {
      "epoch": 757.6470588235294,
      "grad_norm": 10.541228294372559,
      "learning_rate": 4.242352941176471e-05,
      "loss": 2.5254,
      "step": 12880
    },
    {
      "epoch": 758.2352941176471,
      "grad_norm": 9.248269081115723,
      "learning_rate": 4.241764705882353e-05,
      "loss": 2.4492,
      "step": 12890
    },
    {
      "epoch": 758.8235294117648,
      "grad_norm": 9.783591270446777,
      "learning_rate": 4.2411764705882355e-05,
      "loss": 2.3886,
      "step": 12900
    },
    {
      "epoch": 759.4117647058823,
      "grad_norm": 10.45578384399414,
      "learning_rate": 4.240588235294118e-05,
      "loss": 2.3438,
      "step": 12910
    },
    {
      "epoch": 760.0,
      "grad_norm": 9.74516487121582,
      "learning_rate": 4.24e-05,
      "loss": 2.4643,
      "step": 12920
    },
    {
      "epoch": 760.5882352941177,
      "grad_norm": 7.159418106079102,
      "learning_rate": 4.2394117647058824e-05,
      "loss": 2.4042,
      "step": 12930
    },
    {
      "epoch": 761.1764705882352,
      "grad_norm": 7.078542232513428,
      "learning_rate": 4.238823529411765e-05,
      "loss": 2.4625,
      "step": 12940
    },
    {
      "epoch": 761.7647058823529,
      "grad_norm": 6.770571231842041,
      "learning_rate": 4.2382352941176476e-05,
      "loss": 2.4568,
      "step": 12950
    },
    {
      "epoch": 762.3529411764706,
      "grad_norm": 8.291488647460938,
      "learning_rate": 4.237647058823529e-05,
      "loss": 2.3223,
      "step": 12960
    },
    {
      "epoch": 762.9411764705883,
      "grad_norm": 7.274552822113037,
      "learning_rate": 4.2370588235294116e-05,
      "loss": 2.3604,
      "step": 12970
    },
    {
      "epoch": 763.5294117647059,
      "grad_norm": 10.30411148071289,
      "learning_rate": 4.2364705882352945e-05,
      "loss": 2.3799,
      "step": 12980
    },
    {
      "epoch": 764.1176470588235,
      "grad_norm": 12.249300956726074,
      "learning_rate": 4.235882352941177e-05,
      "loss": 2.3963,
      "step": 12990
    },
    {
      "epoch": 764.7058823529412,
      "grad_norm": 8.050979614257812,
      "learning_rate": 4.235294117647059e-05,
      "loss": 2.359,
      "step": 13000
    },
    {
      "epoch": 765.2941176470588,
      "grad_norm": 7.46402645111084,
      "learning_rate": 4.2347058823529414e-05,
      "loss": 2.4181,
      "step": 13010
    },
    {
      "epoch": 765.8823529411765,
      "grad_norm": 9.208273887634277,
      "learning_rate": 4.234117647058824e-05,
      "loss": 2.388,
      "step": 13020
    },
    {
      "epoch": 766.4705882352941,
      "grad_norm": 7.299652099609375,
      "learning_rate": 4.233529411764706e-05,
      "loss": 2.2562,
      "step": 13030
    },
    {
      "epoch": 767.0588235294117,
      "grad_norm": 8.255202293395996,
      "learning_rate": 4.232941176470588e-05,
      "loss": 2.3458,
      "step": 13040
    },
    {
      "epoch": 767.6470588235294,
      "grad_norm": 9.587874412536621,
      "learning_rate": 4.2323529411764706e-05,
      "loss": 2.4324,
      "step": 13050
    },
    {
      "epoch": 768.2352941176471,
      "grad_norm": 8.792816162109375,
      "learning_rate": 4.2317647058823536e-05,
      "loss": 2.5302,
      "step": 13060
    },
    {
      "epoch": 768.8235294117648,
      "grad_norm": 8.102399826049805,
      "learning_rate": 4.231176470588236e-05,
      "loss": 2.3795,
      "step": 13070
    },
    {
      "epoch": 769.4117647058823,
      "grad_norm": 7.6694440841674805,
      "learning_rate": 4.2305882352941175e-05,
      "loss": 2.4905,
      "step": 13080
    },
    {
      "epoch": 770.0,
      "grad_norm": 7.8714919090271,
      "learning_rate": 4.23e-05,
      "loss": 2.4121,
      "step": 13090
    },
    {
      "epoch": 770.5882352941177,
      "grad_norm": 8.066959381103516,
      "learning_rate": 4.229411764705882e-05,
      "loss": 2.3244,
      "step": 13100
    },
    {
      "epoch": 771.1764705882352,
      "grad_norm": 9.425167083740234,
      "learning_rate": 4.228823529411765e-05,
      "loss": 2.3827,
      "step": 13110
    },
    {
      "epoch": 771.7647058823529,
      "grad_norm": 8.648947715759277,
      "learning_rate": 4.2282352941176474e-05,
      "loss": 2.5364,
      "step": 13120
    },
    {
      "epoch": 772.3529411764706,
      "grad_norm": 9.237092018127441,
      "learning_rate": 4.22764705882353e-05,
      "loss": 2.4272,
      "step": 13130
    },
    {
      "epoch": 772.9411764705883,
      "grad_norm": 8.363777160644531,
      "learning_rate": 4.227058823529412e-05,
      "loss": 2.4106,
      "step": 13140
    },
    {
      "epoch": 773.5294117647059,
      "grad_norm": 10.068556785583496,
      "learning_rate": 4.226470588235294e-05,
      "loss": 2.3323,
      "step": 13150
    },
    {
      "epoch": 774.1176470588235,
      "grad_norm": 9.12453556060791,
      "learning_rate": 4.2258823529411766e-05,
      "loss": 2.3388,
      "step": 13160
    },
    {
      "epoch": 774.7058823529412,
      "grad_norm": 8.4796724319458,
      "learning_rate": 4.225294117647059e-05,
      "loss": 2.3895,
      "step": 13170
    },
    {
      "epoch": 775.2941176470588,
      "grad_norm": 8.009461402893066,
      "learning_rate": 4.224705882352941e-05,
      "loss": 2.3062,
      "step": 13180
    },
    {
      "epoch": 775.8823529411765,
      "grad_norm": 9.272442817687988,
      "learning_rate": 4.224117647058824e-05,
      "loss": 2.4672,
      "step": 13190
    },
    {
      "epoch": 776.4705882352941,
      "grad_norm": 9.609271049499512,
      "learning_rate": 4.2235294117647065e-05,
      "loss": 2.4778,
      "step": 13200
    },
    {
      "epoch": 777.0588235294117,
      "grad_norm": 8.73651123046875,
      "learning_rate": 4.222941176470588e-05,
      "loss": 2.2577,
      "step": 13210
    },
    {
      "epoch": 777.6470588235294,
      "grad_norm": 9.667984962463379,
      "learning_rate": 4.2223529411764704e-05,
      "loss": 2.3489,
      "step": 13220
    },
    {
      "epoch": 778.2352941176471,
      "grad_norm": 8.341902732849121,
      "learning_rate": 4.2217647058823534e-05,
      "loss": 2.4304,
      "step": 13230
    },
    {
      "epoch": 778.8235294117648,
      "grad_norm": 8.891916275024414,
      "learning_rate": 4.2211764705882357e-05,
      "loss": 2.392,
      "step": 13240
    },
    {
      "epoch": 779.4117647058823,
      "grad_norm": 7.340169429779053,
      "learning_rate": 4.220588235294118e-05,
      "loss": 2.4528,
      "step": 13250
    },
    {
      "epoch": 780.0,
      "grad_norm": 8.822256088256836,
      "learning_rate": 4.22e-05,
      "loss": 2.3307,
      "step": 13260
    },
    {
      "epoch": 780.5882352941177,
      "grad_norm": 12.86271858215332,
      "learning_rate": 4.2194117647058826e-05,
      "loss": 2.3085,
      "step": 13270
    },
    {
      "epoch": 781.1764705882352,
      "grad_norm": 8.755270957946777,
      "learning_rate": 4.218823529411765e-05,
      "loss": 2.3745,
      "step": 13280
    },
    {
      "epoch": 781.7647058823529,
      "grad_norm": 8.96668815612793,
      "learning_rate": 4.218235294117647e-05,
      "loss": 2.4518,
      "step": 13290
    },
    {
      "epoch": 782.3529411764706,
      "grad_norm": 7.270473957061768,
      "learning_rate": 4.2176470588235294e-05,
      "loss": 2.3787,
      "step": 13300
    },
    {
      "epoch": 782.9411764705883,
      "grad_norm": 7.092601776123047,
      "learning_rate": 4.217058823529412e-05,
      "loss": 2.3539,
      "step": 13310
    },
    {
      "epoch": 783.5294117647059,
      "grad_norm": 9.176977157592773,
      "learning_rate": 4.216470588235295e-05,
      "loss": 2.3614,
      "step": 13320
    },
    {
      "epoch": 784.1176470588235,
      "grad_norm": 6.611345291137695,
      "learning_rate": 4.215882352941177e-05,
      "loss": 2.3271,
      "step": 13330
    },
    {
      "epoch": 784.7058823529412,
      "grad_norm": 10.811034202575684,
      "learning_rate": 4.2152941176470586e-05,
      "loss": 2.4113,
      "step": 13340
    },
    {
      "epoch": 785.2941176470588,
      "grad_norm": 7.973050594329834,
      "learning_rate": 4.214705882352941e-05,
      "loss": 2.4517,
      "step": 13350
    },
    {
      "epoch": 785.8823529411765,
      "grad_norm": 8.966296195983887,
      "learning_rate": 4.214117647058824e-05,
      "loss": 2.3936,
      "step": 13360
    },
    {
      "epoch": 786.4705882352941,
      "grad_norm": 7.497239589691162,
      "learning_rate": 4.213529411764706e-05,
      "loss": 2.3413,
      "step": 13370
    },
    {
      "epoch": 787.0588235294117,
      "grad_norm": 8.780570983886719,
      "learning_rate": 4.2129411764705885e-05,
      "loss": 2.3465,
      "step": 13380
    },
    {
      "epoch": 787.6470588235294,
      "grad_norm": 8.93994140625,
      "learning_rate": 4.212352941176471e-05,
      "loss": 2.35,
      "step": 13390
    },
    {
      "epoch": 788.2352941176471,
      "grad_norm": 7.941917896270752,
      "learning_rate": 4.211764705882353e-05,
      "loss": 2.4118,
      "step": 13400
    },
    {
      "epoch": 788.8235294117648,
      "grad_norm": 8.221318244934082,
      "learning_rate": 4.2111764705882354e-05,
      "loss": 2.3881,
      "step": 13410
    },
    {
      "epoch": 789.4117647058823,
      "grad_norm": 7.166317939758301,
      "learning_rate": 4.210588235294118e-05,
      "loss": 2.4574,
      "step": 13420
    },
    {
      "epoch": 790.0,
      "grad_norm": 8.700493812561035,
      "learning_rate": 4.21e-05,
      "loss": 2.2907,
      "step": 13430
    },
    {
      "epoch": 790.5882352941177,
      "grad_norm": 10.22326946258545,
      "learning_rate": 4.209411764705883e-05,
      "loss": 2.4773,
      "step": 13440
    },
    {
      "epoch": 791.1764705882352,
      "grad_norm": 9.674663543701172,
      "learning_rate": 4.208823529411765e-05,
      "loss": 2.4089,
      "step": 13450
    },
    {
      "epoch": 791.7647058823529,
      "grad_norm": 7.713269233703613,
      "learning_rate": 4.2082352941176476e-05,
      "loss": 2.3383,
      "step": 13460
    },
    {
      "epoch": 792.3529411764706,
      "grad_norm": 8.421931266784668,
      "learning_rate": 4.207647058823529e-05,
      "loss": 2.4261,
      "step": 13470
    },
    {
      "epoch": 792.9411764705883,
      "grad_norm": 7.540201663970947,
      "learning_rate": 4.2070588235294115e-05,
      "loss": 2.2978,
      "step": 13480
    },
    {
      "epoch": 793.5294117647059,
      "grad_norm": 11.160274505615234,
      "learning_rate": 4.2064705882352945e-05,
      "loss": 2.3666,
      "step": 13490
    },
    {
      "epoch": 794.1176470588235,
      "grad_norm": 9.903383255004883,
      "learning_rate": 4.205882352941177e-05,
      "loss": 2.4506,
      "step": 13500
    },
    {
      "epoch": 794.7058823529412,
      "grad_norm": 7.839216232299805,
      "learning_rate": 4.205294117647059e-05,
      "loss": 2.4846,
      "step": 13510
    },
    {
      "epoch": 795.2941176470588,
      "grad_norm": 7.147043228149414,
      "learning_rate": 4.2047058823529414e-05,
      "loss": 2.3428,
      "step": 13520
    },
    {
      "epoch": 795.8823529411765,
      "grad_norm": 6.890655994415283,
      "learning_rate": 4.204117647058824e-05,
      "loss": 2.4126,
      "step": 13530
    },
    {
      "epoch": 796.4705882352941,
      "grad_norm": 10.075827598571777,
      "learning_rate": 4.203529411764706e-05,
      "loss": 2.3708,
      "step": 13540
    },
    {
      "epoch": 797.0588235294117,
      "grad_norm": 9.908870697021484,
      "learning_rate": 4.202941176470588e-05,
      "loss": 2.4682,
      "step": 13550
    },
    {
      "epoch": 797.6470588235294,
      "grad_norm": 7.521270275115967,
      "learning_rate": 4.2023529411764706e-05,
      "loss": 2.3566,
      "step": 13560
    },
    {
      "epoch": 798.2352941176471,
      "grad_norm": 8.16069507598877,
      "learning_rate": 4.2017647058823535e-05,
      "loss": 2.4206,
      "step": 13570
    },
    {
      "epoch": 798.8235294117648,
      "grad_norm": 12.446846008300781,
      "learning_rate": 4.201176470588236e-05,
      "loss": 2.4542,
      "step": 13580
    },
    {
      "epoch": 799.4117647058823,
      "grad_norm": 8.670208930969238,
      "learning_rate": 4.200588235294118e-05,
      "loss": 2.4538,
      "step": 13590
    },
    {
      "epoch": 800.0,
      "grad_norm": 9.869492530822754,
      "learning_rate": 4.2e-05,
      "loss": 2.408,
      "step": 13600
    },
    {
      "epoch": 800.5882352941177,
      "grad_norm": 8.116345405578613,
      "learning_rate": 4.199411764705883e-05,
      "loss": 2.4735,
      "step": 13610
    },
    {
      "epoch": 801.1764705882352,
      "grad_norm": 7.320099353790283,
      "learning_rate": 4.198823529411765e-05,
      "loss": 2.2177,
      "step": 13620
    },
    {
      "epoch": 801.7647058823529,
      "grad_norm": 7.316307544708252,
      "learning_rate": 4.1982352941176473e-05,
      "loss": 2.3285,
      "step": 13630
    },
    {
      "epoch": 802.3529411764706,
      "grad_norm": 7.910567283630371,
      "learning_rate": 4.1976470588235296e-05,
      "loss": 2.357,
      "step": 13640
    },
    {
      "epoch": 802.9411764705883,
      "grad_norm": 10.561155319213867,
      "learning_rate": 4.197058823529412e-05,
      "loss": 2.3715,
      "step": 13650
    },
    {
      "epoch": 803.5294117647059,
      "grad_norm": 8.628866195678711,
      "learning_rate": 4.196470588235294e-05,
      "loss": 2.3459,
      "step": 13660
    },
    {
      "epoch": 804.1176470588235,
      "grad_norm": 8.203107833862305,
      "learning_rate": 4.1958823529411765e-05,
      "loss": 2.4394,
      "step": 13670
    },
    {
      "epoch": 804.7058823529412,
      "grad_norm": 9.941311836242676,
      "learning_rate": 4.195294117647059e-05,
      "loss": 2.4294,
      "step": 13680
    },
    {
      "epoch": 805.2941176470588,
      "grad_norm": 8.024276733398438,
      "learning_rate": 4.194705882352941e-05,
      "loss": 2.4789,
      "step": 13690
    },
    {
      "epoch": 805.8823529411765,
      "grad_norm": 8.268712997436523,
      "learning_rate": 4.194117647058824e-05,
      "loss": 2.4124,
      "step": 13700
    },
    {
      "epoch": 806.4705882352941,
      "grad_norm": 9.36040210723877,
      "learning_rate": 4.1935294117647064e-05,
      "loss": 2.2366,
      "step": 13710
    },
    {
      "epoch": 807.0588235294117,
      "grad_norm": 11.74146842956543,
      "learning_rate": 4.192941176470589e-05,
      "loss": 2.292,
      "step": 13720
    },
    {
      "epoch": 807.6470588235294,
      "grad_norm": 8.683156967163086,
      "learning_rate": 4.19235294117647e-05,
      "loss": 2.3813,
      "step": 13730
    },
    {
      "epoch": 808.2352941176471,
      "grad_norm": 7.441049098968506,
      "learning_rate": 4.191764705882353e-05,
      "loss": 2.3629,
      "step": 13740
    },
    {
      "epoch": 808.8235294117648,
      "grad_norm": 7.653610706329346,
      "learning_rate": 4.1911764705882356e-05,
      "loss": 2.4164,
      "step": 13750
    },
    {
      "epoch": 809.4117647058823,
      "grad_norm": 9.76729679107666,
      "learning_rate": 4.190588235294118e-05,
      "loss": 2.2159,
      "step": 13760
    },
    {
      "epoch": 810.0,
      "grad_norm": 9.425139427185059,
      "learning_rate": 4.19e-05,
      "loss": 2.2932,
      "step": 13770
    },
    {
      "epoch": 810.5882352941177,
      "grad_norm": 7.773075580596924,
      "learning_rate": 4.1894117647058825e-05,
      "loss": 2.3245,
      "step": 13780
    },
    {
      "epoch": 811.1764705882352,
      "grad_norm": 8.565954208374023,
      "learning_rate": 4.188823529411765e-05,
      "loss": 2.2515,
      "step": 13790
    },
    {
      "epoch": 811.7647058823529,
      "grad_norm": 8.232810974121094,
      "learning_rate": 4.188235294117647e-05,
      "loss": 2.3142,
      "step": 13800
    },
    {
      "epoch": 812.3529411764706,
      "grad_norm": 8.463274955749512,
      "learning_rate": 4.1876470588235294e-05,
      "loss": 2.4577,
      "step": 13810
    },
    {
      "epoch": 812.9411764705883,
      "grad_norm": 7.900930881500244,
      "learning_rate": 4.187058823529412e-05,
      "loss": 2.4417,
      "step": 13820
    },
    {
      "epoch": 813.5294117647059,
      "grad_norm": 8.768573760986328,
      "learning_rate": 4.186470588235295e-05,
      "loss": 2.3753,
      "step": 13830
    },
    {
      "epoch": 814.1176470588235,
      "grad_norm": 8.346162796020508,
      "learning_rate": 4.185882352941177e-05,
      "loss": 2.3012,
      "step": 13840
    },
    {
      "epoch": 814.7058823529412,
      "grad_norm": 10.718045234680176,
      "learning_rate": 4.1852941176470586e-05,
      "loss": 2.4383,
      "step": 13850
    },
    {
      "epoch": 815.2941176470588,
      "grad_norm": 6.922980785369873,
      "learning_rate": 4.184705882352941e-05,
      "loss": 2.5058,
      "step": 13860
    },
    {
      "epoch": 815.8823529411765,
      "grad_norm": 9.31265640258789,
      "learning_rate": 4.184117647058824e-05,
      "loss": 2.3473,
      "step": 13870
    },
    {
      "epoch": 816.4705882352941,
      "grad_norm": 9.51674747467041,
      "learning_rate": 4.183529411764706e-05,
      "loss": 2.3769,
      "step": 13880
    },
    {
      "epoch": 817.0588235294117,
      "grad_norm": 8.583873748779297,
      "learning_rate": 4.1829411764705885e-05,
      "loss": 2.5081,
      "step": 13890
    },
    {
      "epoch": 817.6470588235294,
      "grad_norm": 7.269331932067871,
      "learning_rate": 4.182352941176471e-05,
      "loss": 2.277,
      "step": 13900
    },
    {
      "epoch": 818.2352941176471,
      "grad_norm": 9.049457550048828,
      "learning_rate": 4.181764705882353e-05,
      "loss": 2.3968,
      "step": 13910
    },
    {
      "epoch": 818.8235294117648,
      "grad_norm": 8.461586952209473,
      "learning_rate": 4.1811764705882354e-05,
      "loss": 2.3834,
      "step": 13920
    },
    {
      "epoch": 819.4117647058823,
      "grad_norm": 9.816585540771484,
      "learning_rate": 4.1805882352941177e-05,
      "loss": 2.3325,
      "step": 13930
    },
    {
      "epoch": 820.0,
      "grad_norm": 12.103680610656738,
      "learning_rate": 4.18e-05,
      "loss": 2.4558,
      "step": 13940
    },
    {
      "epoch": 820.5882352941177,
      "grad_norm": 7.89895486831665,
      "learning_rate": 4.179411764705883e-05,
      "loss": 2.3911,
      "step": 13950
    },
    {
      "epoch": 821.1764705882352,
      "grad_norm": 11.999406814575195,
      "learning_rate": 4.178823529411765e-05,
      "loss": 2.401,
      "step": 13960
    },
    {
      "epoch": 821.7647058823529,
      "grad_norm": 10.471415519714355,
      "learning_rate": 4.1782352941176475e-05,
      "loss": 2.3892,
      "step": 13970
    },
    {
      "epoch": 822.3529411764706,
      "grad_norm": 9.237914085388184,
      "learning_rate": 4.177647058823529e-05,
      "loss": 2.3999,
      "step": 13980
    },
    {
      "epoch": 822.9411764705883,
      "grad_norm": 8.353962898254395,
      "learning_rate": 4.1770588235294115e-05,
      "loss": 2.3445,
      "step": 13990
    },
    {
      "epoch": 823.5294117647059,
      "grad_norm": 7.734553337097168,
      "learning_rate": 4.1764705882352944e-05,
      "loss": 2.3187,
      "step": 14000
    },
    {
      "epoch": 824.1176470588235,
      "grad_norm": 8.48478889465332,
      "learning_rate": 4.175882352941177e-05,
      "loss": 2.3148,
      "step": 14010
    },
    {
      "epoch": 824.7058823529412,
      "grad_norm": 12.228581428527832,
      "learning_rate": 4.175294117647059e-05,
      "loss": 2.315,
      "step": 14020
    },
    {
      "epoch": 825.2941176470588,
      "grad_norm": 12.256587028503418,
      "learning_rate": 4.174705882352941e-05,
      "loss": 2.3769,
      "step": 14030
    },
    {
      "epoch": 825.8823529411765,
      "grad_norm": 11.404190063476562,
      "learning_rate": 4.1741176470588236e-05,
      "loss": 2.3183,
      "step": 14040
    },
    {
      "epoch": 826.4705882352941,
      "grad_norm": 8.861495971679688,
      "learning_rate": 4.173529411764706e-05,
      "loss": 2.2933,
      "step": 14050
    },
    {
      "epoch": 827.0588235294117,
      "grad_norm": 8.651110649108887,
      "learning_rate": 4.172941176470588e-05,
      "loss": 2.3072,
      "step": 14060
    },
    {
      "epoch": 827.6470588235294,
      "grad_norm": 8.027101516723633,
      "learning_rate": 4.1723529411764705e-05,
      "loss": 2.2466,
      "step": 14070
    },
    {
      "epoch": 828.2352941176471,
      "grad_norm": 9.95347785949707,
      "learning_rate": 4.1717647058823535e-05,
      "loss": 2.3999,
      "step": 14080
    },
    {
      "epoch": 828.8235294117648,
      "grad_norm": 9.486366271972656,
      "learning_rate": 4.171176470588236e-05,
      "loss": 2.3117,
      "step": 14090
    },
    {
      "epoch": 829.4117647058823,
      "grad_norm": 9.697687149047852,
      "learning_rate": 4.170588235294118e-05,
      "loss": 2.3343,
      "step": 14100
    },
    {
      "epoch": 830.0,
      "grad_norm": 14.198180198669434,
      "learning_rate": 4.17e-05,
      "loss": 2.4953,
      "step": 14110
    },
    {
      "epoch": 830.5882352941177,
      "grad_norm": 6.336080074310303,
      "learning_rate": 4.169411764705883e-05,
      "loss": 2.3408,
      "step": 14120
    },
    {
      "epoch": 831.1764705882352,
      "grad_norm": 11.06264591217041,
      "learning_rate": 4.168823529411765e-05,
      "loss": 2.365,
      "step": 14130
    },
    {
      "epoch": 831.7647058823529,
      "grad_norm": 10.70277214050293,
      "learning_rate": 4.168235294117647e-05,
      "loss": 2.3338,
      "step": 14140
    },
    {
      "epoch": 832.3529411764706,
      "grad_norm": 9.642220497131348,
      "learning_rate": 4.1676470588235296e-05,
      "loss": 2.2752,
      "step": 14150
    },
    {
      "epoch": 832.9411764705883,
      "grad_norm": 9.909794807434082,
      "learning_rate": 4.1670588235294126e-05,
      "loss": 2.4024,
      "step": 14160
    },
    {
      "epoch": 833.5294117647059,
      "grad_norm": 9.081198692321777,
      "learning_rate": 4.166470588235294e-05,
      "loss": 2.3412,
      "step": 14170
    },
    {
      "epoch": 834.1176470588235,
      "grad_norm": 8.843530654907227,
      "learning_rate": 4.1658823529411765e-05,
      "loss": 2.3995,
      "step": 14180
    },
    {
      "epoch": 834.7058823529412,
      "grad_norm": 8.882997512817383,
      "learning_rate": 4.165294117647059e-05,
      "loss": 2.4466,
      "step": 14190
    },
    {
      "epoch": 835.2941176470588,
      "grad_norm": 9.57774829864502,
      "learning_rate": 4.164705882352941e-05,
      "loss": 2.1774,
      "step": 14200
    },
    {
      "epoch": 835.8823529411765,
      "grad_norm": 9.092605590820312,
      "learning_rate": 4.164117647058824e-05,
      "loss": 2.3137,
      "step": 14210
    },
    {
      "epoch": 836.4705882352941,
      "grad_norm": 9.07287883758545,
      "learning_rate": 4.1635294117647064e-05,
      "loss": 2.3928,
      "step": 14220
    },
    {
      "epoch": 837.0588235294117,
      "grad_norm": 11.244365692138672,
      "learning_rate": 4.1629411764705887e-05,
      "loss": 2.2529,
      "step": 14230
    },
    {
      "epoch": 837.6470588235294,
      "grad_norm": 8.908853530883789,
      "learning_rate": 4.16235294117647e-05,
      "loss": 2.282,
      "step": 14240
    },
    {
      "epoch": 838.2352941176471,
      "grad_norm": 6.879950523376465,
      "learning_rate": 4.161764705882353e-05,
      "loss": 2.4977,
      "step": 14250
    },
    {
      "epoch": 838.8235294117648,
      "grad_norm": 7.599447727203369,
      "learning_rate": 4.1611764705882356e-05,
      "loss": 2.3454,
      "step": 14260
    },
    {
      "epoch": 839.4117647058823,
      "grad_norm": 8.253448486328125,
      "learning_rate": 4.160588235294118e-05,
      "loss": 2.4347,
      "step": 14270
    },
    {
      "epoch": 840.0,
      "grad_norm": 11.002601623535156,
      "learning_rate": 4.16e-05,
      "loss": 2.3167,
      "step": 14280
    },
    {
      "epoch": 840.5882352941177,
      "grad_norm": 9.37584114074707,
      "learning_rate": 4.1594117647058824e-05,
      "loss": 2.3444,
      "step": 14290
    },
    {
      "epoch": 841.1764705882352,
      "grad_norm": 9.587567329406738,
      "learning_rate": 4.158823529411765e-05,
      "loss": 2.3582,
      "step": 14300
    },
    {
      "epoch": 841.7647058823529,
      "grad_norm": 7.67360258102417,
      "learning_rate": 4.158235294117647e-05,
      "loss": 2.2024,
      "step": 14310
    },
    {
      "epoch": 842.3529411764706,
      "grad_norm": 9.055212020874023,
      "learning_rate": 4.1576470588235293e-05,
      "loss": 2.3612,
      "step": 14320
    },
    {
      "epoch": 842.9411764705883,
      "grad_norm": 9.2354736328125,
      "learning_rate": 4.157058823529412e-05,
      "loss": 2.355,
      "step": 14330
    },
    {
      "epoch": 843.5294117647059,
      "grad_norm": 9.280665397644043,
      "learning_rate": 4.1564705882352946e-05,
      "loss": 2.2175,
      "step": 14340
    },
    {
      "epoch": 844.1176470588235,
      "grad_norm": 6.820106029510498,
      "learning_rate": 4.155882352941177e-05,
      "loss": 2.3453,
      "step": 14350
    },
    {
      "epoch": 844.7058823529412,
      "grad_norm": 8.846373558044434,
      "learning_rate": 4.155294117647059e-05,
      "loss": 2.5256,
      "step": 14360
    },
    {
      "epoch": 845.2941176470588,
      "grad_norm": 7.1591973304748535,
      "learning_rate": 4.154705882352941e-05,
      "loss": 2.4598,
      "step": 14370
    },
    {
      "epoch": 845.8823529411765,
      "grad_norm": 10.419533729553223,
      "learning_rate": 4.154117647058824e-05,
      "loss": 2.2926,
      "step": 14380
    },
    {
      "epoch": 846.4705882352941,
      "grad_norm": 8.575712203979492,
      "learning_rate": 4.153529411764706e-05,
      "loss": 2.2802,
      "step": 14390
    },
    {
      "epoch": 847.0588235294117,
      "grad_norm": 9.08335018157959,
      "learning_rate": 4.1529411764705884e-05,
      "loss": 2.2542,
      "step": 14400
    },
    {
      "epoch": 847.6470588235294,
      "grad_norm": 10.913323402404785,
      "learning_rate": 4.152352941176471e-05,
      "loss": 2.3143,
      "step": 14410
    },
    {
      "epoch": 848.2352941176471,
      "grad_norm": 8.726556777954102,
      "learning_rate": 4.151764705882353e-05,
      "loss": 2.2612,
      "step": 14420
    },
    {
      "epoch": 848.8235294117648,
      "grad_norm": 7.270930290222168,
      "learning_rate": 4.151176470588235e-05,
      "loss": 2.3992,
      "step": 14430
    },
    {
      "epoch": 849.4117647058823,
      "grad_norm": 11.091127395629883,
      "learning_rate": 4.1505882352941176e-05,
      "loss": 2.2847,
      "step": 14440
    },
    {
      "epoch": 850.0,
      "grad_norm": 11.006160736083984,
      "learning_rate": 4.15e-05,
      "loss": 2.3644,
      "step": 14450
    },
    {
      "epoch": 850.5882352941177,
      "grad_norm": 8.307302474975586,
      "learning_rate": 4.149411764705883e-05,
      "loss": 2.3706,
      "step": 14460
    },
    {
      "epoch": 851.1764705882352,
      "grad_norm": 11.560636520385742,
      "learning_rate": 4.148823529411765e-05,
      "loss": 2.3766,
      "step": 14470
    },
    {
      "epoch": 851.7647058823529,
      "grad_norm": 9.690911293029785,
      "learning_rate": 4.1482352941176475e-05,
      "loss": 2.3784,
      "step": 14480
    },
    {
      "epoch": 852.3529411764706,
      "grad_norm": 10.758156776428223,
      "learning_rate": 4.147647058823529e-05,
      "loss": 2.3711,
      "step": 14490
    },
    {
      "epoch": 852.9411764705883,
      "grad_norm": 10.838531494140625,
      "learning_rate": 4.147058823529412e-05,
      "loss": 2.4827,
      "step": 14500
    },
    {
      "epoch": 853.5294117647059,
      "grad_norm": 11.29833984375,
      "learning_rate": 4.1464705882352944e-05,
      "loss": 2.2959,
      "step": 14510
    },
    {
      "epoch": 854.1176470588235,
      "grad_norm": 10.713726997375488,
      "learning_rate": 4.145882352941177e-05,
      "loss": 2.2891,
      "step": 14520
    },
    {
      "epoch": 854.7058823529412,
      "grad_norm": 9.02546501159668,
      "learning_rate": 4.145294117647059e-05,
      "loss": 2.3349,
      "step": 14530
    },
    {
      "epoch": 855.2941176470588,
      "grad_norm": 9.15945053100586,
      "learning_rate": 4.144705882352941e-05,
      "loss": 2.409,
      "step": 14540
    },
    {
      "epoch": 855.8823529411765,
      "grad_norm": 10.127447128295898,
      "learning_rate": 4.1441176470588236e-05,
      "loss": 2.2673,
      "step": 14550
    },
    {
      "epoch": 856.4705882352941,
      "grad_norm": 10.899810791015625,
      "learning_rate": 4.143529411764706e-05,
      "loss": 2.279,
      "step": 14560
    },
    {
      "epoch": 857.0588235294117,
      "grad_norm": 8.403017044067383,
      "learning_rate": 4.142941176470588e-05,
      "loss": 2.2658,
      "step": 14570
    },
    {
      "epoch": 857.6470588235294,
      "grad_norm": 8.516036987304688,
      "learning_rate": 4.1423529411764705e-05,
      "loss": 2.3841,
      "step": 14580
    },
    {
      "epoch": 858.2352941176471,
      "grad_norm": 7.704684734344482,
      "learning_rate": 4.1417647058823534e-05,
      "loss": 2.2908,
      "step": 14590
    },
    {
      "epoch": 858.8235294117648,
      "grad_norm": 7.866951942443848,
      "learning_rate": 4.141176470588236e-05,
      "loss": 2.3396,
      "step": 14600
    },
    {
      "epoch": 859.4117647058823,
      "grad_norm": 11.455497741699219,
      "learning_rate": 4.140588235294118e-05,
      "loss": 2.3977,
      "step": 14610
    },
    {
      "epoch": 860.0,
      "grad_norm": 9.955911636352539,
      "learning_rate": 4.14e-05,
      "loss": 2.2852,
      "step": 14620
    },
    {
      "epoch": 860.5882352941177,
      "grad_norm": 7.530265808105469,
      "learning_rate": 4.1394117647058826e-05,
      "loss": 2.2003,
      "step": 14630
    },
    {
      "epoch": 861.1764705882352,
      "grad_norm": 9.197066307067871,
      "learning_rate": 4.138823529411765e-05,
      "loss": 2.2912,
      "step": 14640
    },
    {
      "epoch": 861.7647058823529,
      "grad_norm": 7.300858974456787,
      "learning_rate": 4.138235294117647e-05,
      "loss": 2.1567,
      "step": 14650
    },
    {
      "epoch": 862.3529411764706,
      "grad_norm": 8.167996406555176,
      "learning_rate": 4.1376470588235295e-05,
      "loss": 2.3936,
      "step": 14660
    },
    {
      "epoch": 862.9411764705883,
      "grad_norm": 9.153800964355469,
      "learning_rate": 4.1370588235294125e-05,
      "loss": 2.3517,
      "step": 14670
    },
    {
      "epoch": 863.5294117647059,
      "grad_norm": 9.853170394897461,
      "learning_rate": 4.136470588235294e-05,
      "loss": 2.1736,
      "step": 14680
    },
    {
      "epoch": 864.1176470588235,
      "grad_norm": 7.303945541381836,
      "learning_rate": 4.1358823529411764e-05,
      "loss": 2.3555,
      "step": 14690
    },
    {
      "epoch": 864.7058823529412,
      "grad_norm": 7.977559566497803,
      "learning_rate": 4.135294117647059e-05,
      "loss": 2.4326,
      "step": 14700
    },
    {
      "epoch": 865.2941176470588,
      "grad_norm": 9.409945487976074,
      "learning_rate": 4.134705882352942e-05,
      "loss": 2.2903,
      "step": 14710
    },
    {
      "epoch": 865.8823529411765,
      "grad_norm": 10.055749893188477,
      "learning_rate": 4.134117647058824e-05,
      "loss": 2.2908,
      "step": 14720
    },
    {
      "epoch": 866.4705882352941,
      "grad_norm": 6.985443115234375,
      "learning_rate": 4.133529411764706e-05,
      "loss": 2.3286,
      "step": 14730
    },
    {
      "epoch": 867.0588235294117,
      "grad_norm": 9.136153221130371,
      "learning_rate": 4.1329411764705886e-05,
      "loss": 2.4279,
      "step": 14740
    },
    {
      "epoch": 867.6470588235294,
      "grad_norm": 10.918648719787598,
      "learning_rate": 4.13235294117647e-05,
      "loss": 2.2098,
      "step": 14750
    },
    {
      "epoch": 868.2352941176471,
      "grad_norm": 9.238113403320312,
      "learning_rate": 4.131764705882353e-05,
      "loss": 2.3289,
      "step": 14760
    },
    {
      "epoch": 868.8235294117648,
      "grad_norm": 8.542582511901855,
      "learning_rate": 4.1311764705882355e-05,
      "loss": 2.3774,
      "step": 14770
    },
    {
      "epoch": 869.4117647058823,
      "grad_norm": 10.924734115600586,
      "learning_rate": 4.130588235294118e-05,
      "loss": 2.3519,
      "step": 14780
    },
    {
      "epoch": 870.0,
      "grad_norm": 11.119685173034668,
      "learning_rate": 4.13e-05,
      "loss": 2.3505,
      "step": 14790
    },
    {
      "epoch": 870.5882352941177,
      "grad_norm": 10.198698997497559,
      "learning_rate": 4.129411764705883e-05,
      "loss": 2.2205,
      "step": 14800
    },
    {
      "epoch": 871.1764705882352,
      "grad_norm": 7.660735130310059,
      "learning_rate": 4.128823529411765e-05,
      "loss": 2.354,
      "step": 14810
    },
    {
      "epoch": 871.7647058823529,
      "grad_norm": 7.950500011444092,
      "learning_rate": 4.128235294117647e-05,
      "loss": 2.2908,
      "step": 14820
    },
    {
      "epoch": 872.3529411764706,
      "grad_norm": 9.457101821899414,
      "learning_rate": 4.127647058823529e-05,
      "loss": 2.2987,
      "step": 14830
    },
    {
      "epoch": 872.9411764705883,
      "grad_norm": 6.980762004852295,
      "learning_rate": 4.127058823529412e-05,
      "loss": 2.2202,
      "step": 14840
    },
    {
      "epoch": 873.5294117647059,
      "grad_norm": 9.056758880615234,
      "learning_rate": 4.1264705882352946e-05,
      "loss": 2.1114,
      "step": 14850
    },
    {
      "epoch": 874.1176470588235,
      "grad_norm": 9.974802017211914,
      "learning_rate": 4.125882352941177e-05,
      "loss": 2.3288,
      "step": 14860
    },
    {
      "epoch": 874.7058823529412,
      "grad_norm": 11.297835350036621,
      "learning_rate": 4.125294117647059e-05,
      "loss": 2.3617,
      "step": 14870
    },
    {
      "epoch": 875.2941176470588,
      "grad_norm": 7.992568492889404,
      "learning_rate": 4.1247058823529415e-05,
      "loss": 2.3288,
      "step": 14880
    },
    {
      "epoch": 875.8823529411765,
      "grad_norm": 7.502938747406006,
      "learning_rate": 4.124117647058824e-05,
      "loss": 2.3771,
      "step": 14890
    },
    {
      "epoch": 876.4705882352941,
      "grad_norm": 8.022025108337402,
      "learning_rate": 4.123529411764706e-05,
      "loss": 2.3104,
      "step": 14900
    },
    {
      "epoch": 877.0588235294117,
      "grad_norm": 10.427968978881836,
      "learning_rate": 4.1229411764705884e-05,
      "loss": 2.3593,
      "step": 14910
    },
    {
      "epoch": 877.6470588235294,
      "grad_norm": 10.550703048706055,
      "learning_rate": 4.1223529411764707e-05,
      "loss": 2.342,
      "step": 14920
    },
    {
      "epoch": 878.2352941176471,
      "grad_norm": 8.127603530883789,
      "learning_rate": 4.1217647058823536e-05,
      "loss": 2.2814,
      "step": 14930
    },
    {
      "epoch": 878.8235294117648,
      "grad_norm": 7.899695873260498,
      "learning_rate": 4.121176470588235e-05,
      "loss": 2.2456,
      "step": 14940
    },
    {
      "epoch": 879.4117647058823,
      "grad_norm": 9.413328170776367,
      "learning_rate": 4.1205882352941176e-05,
      "loss": 2.2274,
      "step": 14950
    },
    {
      "epoch": 880.0,
      "grad_norm": 10.867985725402832,
      "learning_rate": 4.12e-05,
      "loss": 2.3108,
      "step": 14960
    },
    {
      "epoch": 880.5882352941177,
      "grad_norm": 8.095331192016602,
      "learning_rate": 4.119411764705883e-05,
      "loss": 2.2858,
      "step": 14970
    },
    {
      "epoch": 881.1764705882352,
      "grad_norm": 8.579835891723633,
      "learning_rate": 4.118823529411765e-05,
      "loss": 2.355,
      "step": 14980
    },
    {
      "epoch": 881.7647058823529,
      "grad_norm": 10.129121780395508,
      "learning_rate": 4.1182352941176474e-05,
      "loss": 2.301,
      "step": 14990
    },
    {
      "epoch": 882.3529411764706,
      "grad_norm": 7.768776893615723,
      "learning_rate": 4.11764705882353e-05,
      "loss": 2.3093,
      "step": 15000
    },
    {
      "epoch": 882.9411764705883,
      "grad_norm": 12.030128479003906,
      "learning_rate": 4.117058823529412e-05,
      "loss": 2.3789,
      "step": 15010
    },
    {
      "epoch": 883.5294117647059,
      "grad_norm": 8.4779052734375,
      "learning_rate": 4.116470588235294e-05,
      "loss": 2.2441,
      "step": 15020
    },
    {
      "epoch": 884.1176470588235,
      "grad_norm": 10.1468505859375,
      "learning_rate": 4.1158823529411766e-05,
      "loss": 2.3146,
      "step": 15030
    },
    {
      "epoch": 884.7058823529412,
      "grad_norm": 8.75776195526123,
      "learning_rate": 4.115294117647059e-05,
      "loss": 2.2933,
      "step": 15040
    },
    {
      "epoch": 885.2941176470588,
      "grad_norm": 8.783154487609863,
      "learning_rate": 4.114705882352942e-05,
      "loss": 2.2921,
      "step": 15050
    },
    {
      "epoch": 885.8823529411765,
      "grad_norm": 10.281731605529785,
      "learning_rate": 4.1141176470588235e-05,
      "loss": 2.2254,
      "step": 15060
    },
    {
      "epoch": 886.4705882352941,
      "grad_norm": 9.586156845092773,
      "learning_rate": 4.113529411764706e-05,
      "loss": 2.1849,
      "step": 15070
    },
    {
      "epoch": 887.0588235294117,
      "grad_norm": 8.64764404296875,
      "learning_rate": 4.112941176470588e-05,
      "loss": 2.1552,
      "step": 15080
    },
    {
      "epoch": 887.6470588235294,
      "grad_norm": 9.537623405456543,
      "learning_rate": 4.1123529411764704e-05,
      "loss": 2.1772,
      "step": 15090
    },
    {
      "epoch": 888.2352941176471,
      "grad_norm": 8.400566101074219,
      "learning_rate": 4.1117647058823534e-05,
      "loss": 2.3512,
      "step": 15100
    },
    {
      "epoch": 888.8235294117648,
      "grad_norm": 9.26977252960205,
      "learning_rate": 4.111176470588236e-05,
      "loss": 2.308,
      "step": 15110
    },
    {
      "epoch": 889.4117647058823,
      "grad_norm": 11.504459381103516,
      "learning_rate": 4.110588235294118e-05,
      "loss": 2.3124,
      "step": 15120
    },
    {
      "epoch": 890.0,
      "grad_norm": 8.293302536010742,
      "learning_rate": 4.11e-05,
      "loss": 2.1516,
      "step": 15130
    },
    {
      "epoch": 890.5882352941177,
      "grad_norm": 9.479873657226562,
      "learning_rate": 4.1094117647058826e-05,
      "loss": 2.157,
      "step": 15140
    },
    {
      "epoch": 891.1764705882352,
      "grad_norm": 8.521963119506836,
      "learning_rate": 4.108823529411765e-05,
      "loss": 2.1552,
      "step": 15150
    },
    {
      "epoch": 891.7647058823529,
      "grad_norm": 9.192180633544922,
      "learning_rate": 4.108235294117647e-05,
      "loss": 2.2004,
      "step": 15160
    },
    {
      "epoch": 892.3529411764706,
      "grad_norm": 9.045723915100098,
      "learning_rate": 4.1076470588235295e-05,
      "loss": 2.2836,
      "step": 15170
    },
    {
      "epoch": 892.9411764705883,
      "grad_norm": 10.307459831237793,
      "learning_rate": 4.1070588235294125e-05,
      "loss": 2.2527,
      "step": 15180
    },
    {
      "epoch": 893.5294117647059,
      "grad_norm": 9.70256233215332,
      "learning_rate": 4.106470588235294e-05,
      "loss": 2.2622,
      "step": 15190
    },
    {
      "epoch": 894.1176470588235,
      "grad_norm": 11.82995319366455,
      "learning_rate": 4.1058823529411764e-05,
      "loss": 2.4713,
      "step": 15200
    },
    {
      "epoch": 894.7058823529412,
      "grad_norm": 8.997893333435059,
      "learning_rate": 4.105294117647059e-05,
      "loss": 2.2223,
      "step": 15210
    },
    {
      "epoch": 895.2941176470588,
      "grad_norm": 9.342328071594238,
      "learning_rate": 4.1047058823529417e-05,
      "loss": 2.2633,
      "step": 15220
    },
    {
      "epoch": 895.8823529411765,
      "grad_norm": 12.755958557128906,
      "learning_rate": 4.104117647058824e-05,
      "loss": 2.3031,
      "step": 15230
    },
    {
      "epoch": 896.4705882352941,
      "grad_norm": 11.856467247009277,
      "learning_rate": 4.103529411764706e-05,
      "loss": 2.3752,
      "step": 15240
    },
    {
      "epoch": 897.0588235294117,
      "grad_norm": 9.933012008666992,
      "learning_rate": 4.1029411764705886e-05,
      "loss": 2.1932,
      "step": 15250
    },
    {
      "epoch": 897.6470588235294,
      "grad_norm": 8.691141128540039,
      "learning_rate": 4.10235294117647e-05,
      "loss": 2.2073,
      "step": 15260
    },
    {
      "epoch": 898.2352941176471,
      "grad_norm": 8.498127937316895,
      "learning_rate": 4.101764705882353e-05,
      "loss": 2.2621,
      "step": 15270
    },
    {
      "epoch": 898.8235294117648,
      "grad_norm": 10.571714401245117,
      "learning_rate": 4.1011764705882354e-05,
      "loss": 2.37,
      "step": 15280
    },
    {
      "epoch": 899.4117647058823,
      "grad_norm": 9.542920112609863,
      "learning_rate": 4.100588235294118e-05,
      "loss": 2.4004,
      "step": 15290
    },
    {
      "epoch": 900.0,
      "grad_norm": 9.33984088897705,
      "learning_rate": 4.1e-05,
      "loss": 2.1771,
      "step": 15300
    },
    {
      "epoch": 900.5882352941177,
      "grad_norm": 12.821011543273926,
      "learning_rate": 4.099411764705883e-05,
      "loss": 2.2144,
      "step": 15310
    },
    {
      "epoch": 901.1764705882352,
      "grad_norm": 9.194266319274902,
      "learning_rate": 4.0988235294117646e-05,
      "loss": 2.2432,
      "step": 15320
    },
    {
      "epoch": 901.7647058823529,
      "grad_norm": 11.96777629852295,
      "learning_rate": 4.098235294117647e-05,
      "loss": 2.3206,
      "step": 15330
    },
    {
      "epoch": 902.3529411764706,
      "grad_norm": 10.662154197692871,
      "learning_rate": 4.097647058823529e-05,
      "loss": 2.2973,
      "step": 15340
    },
    {
      "epoch": 902.9411764705883,
      "grad_norm": 8.479971885681152,
      "learning_rate": 4.097058823529412e-05,
      "loss": 2.2899,
      "step": 15350
    },
    {
      "epoch": 903.5294117647059,
      "grad_norm": 9.77161693572998,
      "learning_rate": 4.0964705882352945e-05,
      "loss": 2.4747,
      "step": 15360
    },
    {
      "epoch": 904.1176470588235,
      "grad_norm": 7.532540798187256,
      "learning_rate": 4.095882352941177e-05,
      "loss": 2.1037,
      "step": 15370
    },
    {
      "epoch": 904.7058823529412,
      "grad_norm": 11.289388656616211,
      "learning_rate": 4.095294117647059e-05,
      "loss": 2.345,
      "step": 15380
    },
    {
      "epoch": 905.2941176470588,
      "grad_norm": 9.98150634765625,
      "learning_rate": 4.0947058823529414e-05,
      "loss": 2.2623,
      "step": 15390
    },
    {
      "epoch": 905.8823529411765,
      "grad_norm": 9.231409072875977,
      "learning_rate": 4.094117647058824e-05,
      "loss": 2.1667,
      "step": 15400
    },
    {
      "epoch": 906.4705882352941,
      "grad_norm": 9.840421676635742,
      "learning_rate": 4.093529411764706e-05,
      "loss": 2.2785,
      "step": 15410
    },
    {
      "epoch": 907.0588235294117,
      "grad_norm": 9.271565437316895,
      "learning_rate": 4.092941176470588e-05,
      "loss": 2.1164,
      "step": 15420
    },
    {
      "epoch": 907.6470588235294,
      "grad_norm": 11.08096694946289,
      "learning_rate": 4.092352941176471e-05,
      "loss": 2.1589,
      "step": 15430
    },
    {
      "epoch": 908.2352941176471,
      "grad_norm": 7.553733825683594,
      "learning_rate": 4.0917647058823536e-05,
      "loss": 2.2214,
      "step": 15440
    },
    {
      "epoch": 908.8235294117648,
      "grad_norm": 9.58646297454834,
      "learning_rate": 4.091176470588235e-05,
      "loss": 2.3191,
      "step": 15450
    },
    {
      "epoch": 909.4117647058823,
      "grad_norm": 9.839163780212402,
      "learning_rate": 4.0905882352941175e-05,
      "loss": 2.1822,
      "step": 15460
    },
    {
      "epoch": 910.0,
      "grad_norm": 12.140541076660156,
      "learning_rate": 4.09e-05,
      "loss": 2.2459,
      "step": 15470
    },
    {
      "epoch": 910.5882352941177,
      "grad_norm": 8.31442928314209,
      "learning_rate": 4.089411764705883e-05,
      "loss": 2.2716,
      "step": 15480
    },
    {
      "epoch": 911.1764705882352,
      "grad_norm": 9.579964637756348,
      "learning_rate": 4.088823529411765e-05,
      "loss": 2.1941,
      "step": 15490
    },
    {
      "epoch": 911.7647058823529,
      "grad_norm": 10.318364143371582,
      "learning_rate": 4.0882352941176474e-05,
      "loss": 2.1928,
      "step": 15500
    },
    {
      "epoch": 912.3529411764706,
      "grad_norm": 12.242923736572266,
      "learning_rate": 4.08764705882353e-05,
      "loss": 2.2844,
      "step": 15510
    },
    {
      "epoch": 912.9411764705883,
      "grad_norm": 8.773031234741211,
      "learning_rate": 4.087058823529412e-05,
      "loss": 2.1174,
      "step": 15520
    },
    {
      "epoch": 913.5294117647059,
      "grad_norm": 11.237749099731445,
      "learning_rate": 4.086470588235294e-05,
      "loss": 2.323,
      "step": 15530
    },
    {
      "epoch": 914.1176470588235,
      "grad_norm": 8.94963264465332,
      "learning_rate": 4.0858823529411766e-05,
      "loss": 2.2447,
      "step": 15540
    },
    {
      "epoch": 914.7058823529412,
      "grad_norm": 12.574645042419434,
      "learning_rate": 4.085294117647059e-05,
      "loss": 2.298,
      "step": 15550
    },
    {
      "epoch": 915.2941176470588,
      "grad_norm": 11.460762023925781,
      "learning_rate": 4.084705882352942e-05,
      "loss": 2.2204,
      "step": 15560
    },
    {
      "epoch": 915.8823529411765,
      "grad_norm": 9.05580997467041,
      "learning_rate": 4.084117647058824e-05,
      "loss": 2.2929,
      "step": 15570
    },
    {
      "epoch": 916.4705882352941,
      "grad_norm": 9.851593971252441,
      "learning_rate": 4.083529411764706e-05,
      "loss": 2.2201,
      "step": 15580
    },
    {
      "epoch": 917.0588235294117,
      "grad_norm": 7.849664211273193,
      "learning_rate": 4.082941176470588e-05,
      "loss": 2.3186,
      "step": 15590
    },
    {
      "epoch": 917.6470588235294,
      "grad_norm": 9.880399703979492,
      "learning_rate": 4.082352941176471e-05,
      "loss": 2.1944,
      "step": 15600
    },
    {
      "epoch": 918.2352941176471,
      "grad_norm": 8.706696510314941,
      "learning_rate": 4.0817647058823533e-05,
      "loss": 2.2329,
      "step": 15610
    },
    {
      "epoch": 918.8235294117648,
      "grad_norm": 9.138603210449219,
      "learning_rate": 4.0811764705882356e-05,
      "loss": 2.2812,
      "step": 15620
    },
    {
      "epoch": 919.4117647058823,
      "grad_norm": 8.081804275512695,
      "learning_rate": 4.080588235294118e-05,
      "loss": 2.2832,
      "step": 15630
    },
    {
      "epoch": 920.0,
      "grad_norm": 11.040623664855957,
      "learning_rate": 4.08e-05,
      "loss": 2.1859,
      "step": 15640
    },
    {
      "epoch": 920.5882352941177,
      "grad_norm": 9.704880714416504,
      "learning_rate": 4.0794117647058825e-05,
      "loss": 2.1866,
      "step": 15650
    },
    {
      "epoch": 921.1764705882352,
      "grad_norm": 8.441998481750488,
      "learning_rate": 4.078823529411765e-05,
      "loss": 2.2934,
      "step": 15660
    },
    {
      "epoch": 921.7647058823529,
      "grad_norm": 10.137004852294922,
      "learning_rate": 4.078235294117647e-05,
      "loss": 2.2588,
      "step": 15670
    },
    {
      "epoch": 922.3529411764706,
      "grad_norm": 8.917342185974121,
      "learning_rate": 4.0776470588235294e-05,
      "loss": 2.3687,
      "step": 15680
    },
    {
      "epoch": 922.9411764705883,
      "grad_norm": 7.785975933074951,
      "learning_rate": 4.0770588235294124e-05,
      "loss": 2.2865,
      "step": 15690
    },
    {
      "epoch": 923.5294117647059,
      "grad_norm": 10.480506896972656,
      "learning_rate": 4.076470588235295e-05,
      "loss": 2.1527,
      "step": 15700
    },
    {
      "epoch": 924.1176470588235,
      "grad_norm": 8.12036418914795,
      "learning_rate": 4.075882352941176e-05,
      "loss": 2.2402,
      "step": 15710
    },
    {
      "epoch": 924.7058823529412,
      "grad_norm": 12.617659568786621,
      "learning_rate": 4.0752941176470586e-05,
      "loss": 2.2481,
      "step": 15720
    },
    {
      "epoch": 925.2941176470588,
      "grad_norm": 10.931203842163086,
      "learning_rate": 4.0747058823529416e-05,
      "loss": 2.3479,
      "step": 15730
    },
    {
      "epoch": 925.8823529411765,
      "grad_norm": 10.850179672241211,
      "learning_rate": 4.074117647058824e-05,
      "loss": 2.2035,
      "step": 15740
    },
    {
      "epoch": 926.4705882352941,
      "grad_norm": 9.696600914001465,
      "learning_rate": 4.073529411764706e-05,
      "loss": 2.3307,
      "step": 15750
    },
    {
      "epoch": 927.0588235294117,
      "grad_norm": 12.204697608947754,
      "learning_rate": 4.0729411764705885e-05,
      "loss": 2.3797,
      "step": 15760
    },
    {
      "epoch": 927.6470588235294,
      "grad_norm": 8.614609718322754,
      "learning_rate": 4.072352941176471e-05,
      "loss": 2.279,
      "step": 15770
    },
    {
      "epoch": 928.2352941176471,
      "grad_norm": 10.291152954101562,
      "learning_rate": 4.071764705882353e-05,
      "loss": 2.0762,
      "step": 15780
    },
    {
      "epoch": 928.8235294117648,
      "grad_norm": 9.297516822814941,
      "learning_rate": 4.0711764705882354e-05,
      "loss": 2.2498,
      "step": 15790
    },
    {
      "epoch": 929.4117647058823,
      "grad_norm": 9.620559692382812,
      "learning_rate": 4.070588235294118e-05,
      "loss": 2.271,
      "step": 15800
    },
    {
      "epoch": 930.0,
      "grad_norm": 12.01029109954834,
      "learning_rate": 4.07e-05,
      "loss": 2.3331,
      "step": 15810
    },
    {
      "epoch": 930.5882352941177,
      "grad_norm": 9.722662925720215,
      "learning_rate": 4.069411764705883e-05,
      "loss": 2.2043,
      "step": 15820
    },
    {
      "epoch": 931.1764705882352,
      "grad_norm": 8.90379810333252,
      "learning_rate": 4.0688235294117646e-05,
      "loss": 2.2854,
      "step": 15830
    },
    {
      "epoch": 931.7647058823529,
      "grad_norm": 12.885661125183105,
      "learning_rate": 4.068235294117647e-05,
      "loss": 2.3564,
      "step": 15840
    },
    {
      "epoch": 932.3529411764706,
      "grad_norm": 9.178102493286133,
      "learning_rate": 4.067647058823529e-05,
      "loss": 2.3249,
      "step": 15850
    },
    {
      "epoch": 932.9411764705883,
      "grad_norm": 7.861687183380127,
      "learning_rate": 4.067058823529412e-05,
      "loss": 2.2261,
      "step": 15860
    },
    {
      "epoch": 933.5294117647059,
      "grad_norm": 8.545021057128906,
      "learning_rate": 4.0664705882352945e-05,
      "loss": 2.1879,
      "step": 15870
    },
    {
      "epoch": 934.1176470588235,
      "grad_norm": 7.560941219329834,
      "learning_rate": 4.065882352941177e-05,
      "loss": 2.3053,
      "step": 15880
    },
    {
      "epoch": 934.7058823529412,
      "grad_norm": 10.848748207092285,
      "learning_rate": 4.065294117647059e-05,
      "loss": 2.2928,
      "step": 15890
    },
    {
      "epoch": 935.2941176470588,
      "grad_norm": 9.398640632629395,
      "learning_rate": 4.0647058823529414e-05,
      "loss": 2.3534,
      "step": 15900
    },
    {
      "epoch": 935.8823529411765,
      "grad_norm": 9.816967010498047,
      "learning_rate": 4.064117647058824e-05,
      "loss": 2.1865,
      "step": 15910
    },
    {
      "epoch": 936.4705882352941,
      "grad_norm": 10.032017707824707,
      "learning_rate": 4.063529411764706e-05,
      "loss": 2.2142,
      "step": 15920
    },
    {
      "epoch": 937.0588235294117,
      "grad_norm": 10.614672660827637,
      "learning_rate": 4.062941176470588e-05,
      "loss": 2.3271,
      "step": 15930
    },
    {
      "epoch": 937.6470588235294,
      "grad_norm": 7.756228923797607,
      "learning_rate": 4.062352941176471e-05,
      "loss": 2.1139,
      "step": 15940
    },
    {
      "epoch": 938.2352941176471,
      "grad_norm": 10.94885540008545,
      "learning_rate": 4.0617647058823535e-05,
      "loss": 2.1968,
      "step": 15950
    },
    {
      "epoch": 938.8235294117648,
      "grad_norm": 9.34837818145752,
      "learning_rate": 4.061176470588235e-05,
      "loss": 2.2285,
      "step": 15960
    },
    {
      "epoch": 939.4117647058823,
      "grad_norm": 11.720346450805664,
      "learning_rate": 4.0605882352941175e-05,
      "loss": 2.1561,
      "step": 15970
    },
    {
      "epoch": 940.0,
      "grad_norm": 10.54277229309082,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 2.1316,
      "step": 15980
    },
    {
      "epoch": 940.5882352941177,
      "grad_norm": 11.362186431884766,
      "learning_rate": 4.059411764705883e-05,
      "loss": 2.2113,
      "step": 15990
    },
    {
      "epoch": 941.1764705882352,
      "grad_norm": 13.245101928710938,
      "learning_rate": 4.058823529411765e-05,
      "loss": 2.2198,
      "step": 16000
    },
    {
      "epoch": 941.7647058823529,
      "grad_norm": 8.838150024414062,
      "learning_rate": 4.058235294117647e-05,
      "loss": 2.1969,
      "step": 16010
    },
    {
      "epoch": 942.3529411764706,
      "grad_norm": 11.417945861816406,
      "learning_rate": 4.0576470588235296e-05,
      "loss": 2.356,
      "step": 16020
    },
    {
      "epoch": 942.9411764705883,
      "grad_norm": 10.068644523620605,
      "learning_rate": 4.057058823529412e-05,
      "loss": 2.1539,
      "step": 16030
    },
    {
      "epoch": 943.5294117647059,
      "grad_norm": 11.835071563720703,
      "learning_rate": 4.056470588235294e-05,
      "loss": 2.2764,
      "step": 16040
    },
    {
      "epoch": 944.1176470588235,
      "grad_norm": 8.959035873413086,
      "learning_rate": 4.0558823529411765e-05,
      "loss": 2.2929,
      "step": 16050
    },
    {
      "epoch": 944.7058823529412,
      "grad_norm": 11.093192100524902,
      "learning_rate": 4.055294117647059e-05,
      "loss": 2.2449,
      "step": 16060
    },
    {
      "epoch": 945.2941176470588,
      "grad_norm": 6.946351528167725,
      "learning_rate": 4.054705882352942e-05,
      "loss": 2.1821,
      "step": 16070
    },
    {
      "epoch": 945.8823529411765,
      "grad_norm": 10.956871032714844,
      "learning_rate": 4.054117647058824e-05,
      "loss": 2.2373,
      "step": 16080
    },
    {
      "epoch": 946.4705882352941,
      "grad_norm": 8.064620018005371,
      "learning_rate": 4.053529411764706e-05,
      "loss": 2.2098,
      "step": 16090
    },
    {
      "epoch": 947.0588235294117,
      "grad_norm": 8.904658317565918,
      "learning_rate": 4.052941176470588e-05,
      "loss": 2.2856,
      "step": 16100
    },
    {
      "epoch": 947.6470588235294,
      "grad_norm": 11.07581615447998,
      "learning_rate": 4.052352941176471e-05,
      "loss": 2.3368,
      "step": 16110
    },
    {
      "epoch": 948.2352941176471,
      "grad_norm": 7.874765872955322,
      "learning_rate": 4.051764705882353e-05,
      "loss": 2.1714,
      "step": 16120
    },
    {
      "epoch": 948.8235294117648,
      "grad_norm": 11.47193431854248,
      "learning_rate": 4.0511764705882356e-05,
      "loss": 2.2249,
      "step": 16130
    },
    {
      "epoch": 949.4117647058823,
      "grad_norm": 12.960203170776367,
      "learning_rate": 4.050588235294118e-05,
      "loss": 2.2034,
      "step": 16140
    },
    {
      "epoch": 950.0,
      "grad_norm": 13.557697296142578,
      "learning_rate": 4.05e-05,
      "loss": 2.217,
      "step": 16150
    },
    {
      "epoch": 950.5882352941177,
      "grad_norm": 10.241022109985352,
      "learning_rate": 4.0494117647058825e-05,
      "loss": 2.3642,
      "step": 16160
    },
    {
      "epoch": 951.1764705882352,
      "grad_norm": 13.352941513061523,
      "learning_rate": 4.048823529411765e-05,
      "loss": 2.2734,
      "step": 16170
    },
    {
      "epoch": 951.7647058823529,
      "grad_norm": 11.392696380615234,
      "learning_rate": 4.048235294117647e-05,
      "loss": 2.2349,
      "step": 16180
    },
    {
      "epoch": 952.3529411764706,
      "grad_norm": 13.063790321350098,
      "learning_rate": 4.0476470588235294e-05,
      "loss": 2.291,
      "step": 16190
    },
    {
      "epoch": 952.9411764705883,
      "grad_norm": 10.167203903198242,
      "learning_rate": 4.0470588235294124e-05,
      "loss": 2.0852,
      "step": 16200
    },
    {
      "epoch": 953.5294117647059,
      "grad_norm": 8.734597206115723,
      "learning_rate": 4.0464705882352947e-05,
      "loss": 2.298,
      "step": 16210
    },
    {
      "epoch": 954.1176470588235,
      "grad_norm": 11.274006843566895,
      "learning_rate": 4.045882352941176e-05,
      "loss": 2.3319,
      "step": 16220
    },
    {
      "epoch": 954.7058823529412,
      "grad_norm": 9.301284790039062,
      "learning_rate": 4.0452941176470586e-05,
      "loss": 2.2182,
      "step": 16230
    },
    {
      "epoch": 955.2941176470588,
      "grad_norm": 9.736536979675293,
      "learning_rate": 4.0447058823529416e-05,
      "loss": 2.1251,
      "step": 16240
    },
    {
      "epoch": 955.8823529411765,
      "grad_norm": 11.411835670471191,
      "learning_rate": 4.044117647058824e-05,
      "loss": 2.1843,
      "step": 16250
    },
    {
      "epoch": 956.4705882352941,
      "grad_norm": 9.499898910522461,
      "learning_rate": 4.043529411764706e-05,
      "loss": 2.1179,
      "step": 16260
    },
    {
      "epoch": 957.0588235294117,
      "grad_norm": 9.045147895812988,
      "learning_rate": 4.0429411764705885e-05,
      "loss": 2.1755,
      "step": 16270
    },
    {
      "epoch": 957.6470588235294,
      "grad_norm": 9.44805908203125,
      "learning_rate": 4.042352941176471e-05,
      "loss": 2.3089,
      "step": 16280
    },
    {
      "epoch": 958.2352941176471,
      "grad_norm": 9.34536361694336,
      "learning_rate": 4.041764705882353e-05,
      "loss": 2.1979,
      "step": 16290
    },
    {
      "epoch": 958.8235294117648,
      "grad_norm": 10.326476097106934,
      "learning_rate": 4.0411764705882353e-05,
      "loss": 1.9948,
      "step": 16300
    },
    {
      "epoch": 959.4117647058823,
      "grad_norm": 11.380847930908203,
      "learning_rate": 4.0405882352941176e-05,
      "loss": 2.2127,
      "step": 16310
    },
    {
      "epoch": 960.0,
      "grad_norm": 10.005021095275879,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 2.2775,
      "step": 16320
    },
    {
      "epoch": 960.5882352941177,
      "grad_norm": 8.987242698669434,
      "learning_rate": 4.039411764705883e-05,
      "loss": 2.1875,
      "step": 16330
    },
    {
      "epoch": 961.1764705882352,
      "grad_norm": 9.600890159606934,
      "learning_rate": 4.038823529411765e-05,
      "loss": 2.2807,
      "step": 16340
    },
    {
      "epoch": 961.7647058823529,
      "grad_norm": 8.54485034942627,
      "learning_rate": 4.038235294117647e-05,
      "loss": 2.3308,
      "step": 16350
    },
    {
      "epoch": 962.3529411764706,
      "grad_norm": 12.199250221252441,
      "learning_rate": 4.037647058823529e-05,
      "loss": 2.2363,
      "step": 16360
    },
    {
      "epoch": 962.9411764705883,
      "grad_norm": 10.246525764465332,
      "learning_rate": 4.037058823529412e-05,
      "loss": 2.1814,
      "step": 16370
    },
    {
      "epoch": 963.5294117647059,
      "grad_norm": 8.458858489990234,
      "learning_rate": 4.0364705882352944e-05,
      "loss": 2.2508,
      "step": 16380
    },
    {
      "epoch": 964.1176470588235,
      "grad_norm": 9.611799240112305,
      "learning_rate": 4.035882352941177e-05,
      "loss": 2.2563,
      "step": 16390
    },
    {
      "epoch": 964.7058823529412,
      "grad_norm": 9.852383613586426,
      "learning_rate": 4.035294117647059e-05,
      "loss": 2.1705,
      "step": 16400
    },
    {
      "epoch": 965.2941176470588,
      "grad_norm": 8.683302879333496,
      "learning_rate": 4.034705882352941e-05,
      "loss": 2.1965,
      "step": 16410
    },
    {
      "epoch": 965.8823529411765,
      "grad_norm": 9.290910720825195,
      "learning_rate": 4.0341176470588236e-05,
      "loss": 2.2202,
      "step": 16420
    },
    {
      "epoch": 966.4705882352941,
      "grad_norm": 11.541220664978027,
      "learning_rate": 4.033529411764706e-05,
      "loss": 2.3047,
      "step": 16430
    },
    {
      "epoch": 967.0588235294117,
      "grad_norm": 8.409510612487793,
      "learning_rate": 4.032941176470588e-05,
      "loss": 2.2546,
      "step": 16440
    },
    {
      "epoch": 967.6470588235294,
      "grad_norm": 10.390310287475586,
      "learning_rate": 4.032352941176471e-05,
      "loss": 2.2032,
      "step": 16450
    },
    {
      "epoch": 968.2352941176471,
      "grad_norm": 9.481415748596191,
      "learning_rate": 4.0317647058823535e-05,
      "loss": 2.2816,
      "step": 16460
    },
    {
      "epoch": 968.8235294117648,
      "grad_norm": 13.511281967163086,
      "learning_rate": 4.031176470588236e-05,
      "loss": 2.1399,
      "step": 16470
    },
    {
      "epoch": 969.4117647058823,
      "grad_norm": 11.499360084533691,
      "learning_rate": 4.0305882352941174e-05,
      "loss": 2.396,
      "step": 16480
    },
    {
      "epoch": 970.0,
      "grad_norm": 12.836404800415039,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 2.2394,
      "step": 16490
    },
    {
      "epoch": 970.5882352941177,
      "grad_norm": 13.308906555175781,
      "learning_rate": 4.029411764705883e-05,
      "loss": 2.1822,
      "step": 16500
    },
    {
      "epoch": 971.1764705882352,
      "grad_norm": 9.41395092010498,
      "learning_rate": 4.028823529411765e-05,
      "loss": 2.3309,
      "step": 16510
    },
    {
      "epoch": 971.7647058823529,
      "grad_norm": 13.009517669677734,
      "learning_rate": 4.028235294117647e-05,
      "loss": 2.1772,
      "step": 16520
    },
    {
      "epoch": 972.3529411764706,
      "grad_norm": 8.27701187133789,
      "learning_rate": 4.0276470588235296e-05,
      "loss": 2.1327,
      "step": 16530
    },
    {
      "epoch": 972.9411764705883,
      "grad_norm": 8.752676963806152,
      "learning_rate": 4.027058823529412e-05,
      "loss": 2.1011,
      "step": 16540
    },
    {
      "epoch": 973.5294117647059,
      "grad_norm": 9.75937557220459,
      "learning_rate": 4.026470588235294e-05,
      "loss": 2.2893,
      "step": 16550
    },
    {
      "epoch": 974.1176470588235,
      "grad_norm": 8.686891555786133,
      "learning_rate": 4.0258823529411765e-05,
      "loss": 2.1124,
      "step": 16560
    },
    {
      "epoch": 974.7058823529412,
      "grad_norm": 11.145119667053223,
      "learning_rate": 4.025294117647059e-05,
      "loss": 2.1925,
      "step": 16570
    },
    {
      "epoch": 975.2941176470588,
      "grad_norm": 9.762831687927246,
      "learning_rate": 4.024705882352942e-05,
      "loss": 2.1947,
      "step": 16580
    },
    {
      "epoch": 975.8823529411765,
      "grad_norm": 10.749747276306152,
      "learning_rate": 4.024117647058824e-05,
      "loss": 2.2193,
      "step": 16590
    },
    {
      "epoch": 976.4705882352941,
      "grad_norm": 8.924663543701172,
      "learning_rate": 4.023529411764706e-05,
      "loss": 2.1351,
      "step": 16600
    },
    {
      "epoch": 977.0588235294117,
      "grad_norm": 9.408175468444824,
      "learning_rate": 4.022941176470588e-05,
      "loss": 2.2734,
      "step": 16610
    },
    {
      "epoch": 977.6470588235294,
      "grad_norm": 10.897994995117188,
      "learning_rate": 4.022352941176471e-05,
      "loss": 2.2401,
      "step": 16620
    },
    {
      "epoch": 978.2352941176471,
      "grad_norm": 8.211268424987793,
      "learning_rate": 4.021764705882353e-05,
      "loss": 2.2184,
      "step": 16630
    },
    {
      "epoch": 978.8235294117648,
      "grad_norm": 8.76340103149414,
      "learning_rate": 4.0211764705882355e-05,
      "loss": 2.2036,
      "step": 16640
    },
    {
      "epoch": 979.4117647058823,
      "grad_norm": 9.475438117980957,
      "learning_rate": 4.020588235294118e-05,
      "loss": 2.217,
      "step": 16650
    },
    {
      "epoch": 980.0,
      "grad_norm": 13.002382278442383,
      "learning_rate": 4.02e-05,
      "loss": 2.2744,
      "step": 16660
    },
    {
      "epoch": 980.5882352941177,
      "grad_norm": 9.971107482910156,
      "learning_rate": 4.0194117647058824e-05,
      "loss": 2.2663,
      "step": 16670
    },
    {
      "epoch": 981.1764705882352,
      "grad_norm": 9.714838981628418,
      "learning_rate": 4.018823529411765e-05,
      "loss": 2.1329,
      "step": 16680
    },
    {
      "epoch": 981.7647058823529,
      "grad_norm": 11.564682006835938,
      "learning_rate": 4.018235294117647e-05,
      "loss": 2.1759,
      "step": 16690
    },
    {
      "epoch": 982.3529411764706,
      "grad_norm": 11.12285041809082,
      "learning_rate": 4.01764705882353e-05,
      "loss": 2.2453,
      "step": 16700
    },
    {
      "epoch": 982.9411764705883,
      "grad_norm": 11.171143531799316,
      "learning_rate": 4.017058823529412e-05,
      "loss": 2.2096,
      "step": 16710
    },
    {
      "epoch": 983.5294117647059,
      "grad_norm": 10.80918025970459,
      "learning_rate": 4.0164705882352946e-05,
      "loss": 2.2498,
      "step": 16720
    },
    {
      "epoch": 984.1176470588235,
      "grad_norm": 14.728852272033691,
      "learning_rate": 4.015882352941176e-05,
      "loss": 2.1862,
      "step": 16730
    },
    {
      "epoch": 984.7058823529412,
      "grad_norm": 11.18774700164795,
      "learning_rate": 4.0152941176470585e-05,
      "loss": 2.1806,
      "step": 16740
    },
    {
      "epoch": 985.2941176470588,
      "grad_norm": 13.551471710205078,
      "learning_rate": 4.0147058823529415e-05,
      "loss": 2.1869,
      "step": 16750
    },
    {
      "epoch": 985.8823529411765,
      "grad_norm": 9.852275848388672,
      "learning_rate": 4.014117647058824e-05,
      "loss": 2.2876,
      "step": 16760
    },
    {
      "epoch": 986.4705882352941,
      "grad_norm": 8.02947998046875,
      "learning_rate": 4.013529411764706e-05,
      "loss": 2.1626,
      "step": 16770
    },
    {
      "epoch": 987.0588235294117,
      "grad_norm": 11.739206314086914,
      "learning_rate": 4.0129411764705884e-05,
      "loss": 2.2453,
      "step": 16780
    },
    {
      "epoch": 987.6470588235294,
      "grad_norm": 10.12697696685791,
      "learning_rate": 4.012352941176471e-05,
      "loss": 2.1859,
      "step": 16790
    },
    {
      "epoch": 988.2352941176471,
      "grad_norm": 11.160623550415039,
      "learning_rate": 4.011764705882353e-05,
      "loss": 2.1621,
      "step": 16800
    },
    {
      "epoch": 988.8235294117648,
      "grad_norm": 9.70751667022705,
      "learning_rate": 4.011176470588235e-05,
      "loss": 2.1444,
      "step": 16810
    },
    {
      "epoch": 989.4117647058823,
      "grad_norm": 9.100128173828125,
      "learning_rate": 4.0105882352941176e-05,
      "loss": 2.124,
      "step": 16820
    },
    {
      "epoch": 990.0,
      "grad_norm": 15.35663890838623,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 2.2792,
      "step": 16830
    },
    {
      "epoch": 990.5882352941177,
      "grad_norm": 11.39997386932373,
      "learning_rate": 4.009411764705883e-05,
      "loss": 2.1976,
      "step": 16840
    },
    {
      "epoch": 991.1764705882352,
      "grad_norm": 9.475069999694824,
      "learning_rate": 4.008823529411765e-05,
      "loss": 2.3456,
      "step": 16850
    },
    {
      "epoch": 991.7647058823529,
      "grad_norm": 9.033671379089355,
      "learning_rate": 4.008235294117647e-05,
      "loss": 2.1835,
      "step": 16860
    },
    {
      "epoch": 992.3529411764706,
      "grad_norm": 10.3665771484375,
      "learning_rate": 4.00764705882353e-05,
      "loss": 2.3021,
      "step": 16870
    },
    {
      "epoch": 992.9411764705883,
      "grad_norm": 9.598991394042969,
      "learning_rate": 4.007058823529412e-05,
      "loss": 2.1736,
      "step": 16880
    },
    {
      "epoch": 993.5294117647059,
      "grad_norm": 9.492645263671875,
      "learning_rate": 4.0064705882352944e-05,
      "loss": 2.1688,
      "step": 16890
    },
    {
      "epoch": 994.1176470588235,
      "grad_norm": 9.108532905578613,
      "learning_rate": 4.005882352941177e-05,
      "loss": 2.2322,
      "step": 16900
    },
    {
      "epoch": 994.7058823529412,
      "grad_norm": 10.410486221313477,
      "learning_rate": 4.005294117647059e-05,
      "loss": 2.2504,
      "step": 16910
    },
    {
      "epoch": 995.2941176470588,
      "grad_norm": 10.666850090026855,
      "learning_rate": 4.004705882352941e-05,
      "loss": 2.1872,
      "step": 16920
    },
    {
      "epoch": 995.8823529411765,
      "grad_norm": 12.47800350189209,
      "learning_rate": 4.0041176470588236e-05,
      "loss": 2.2534,
      "step": 16930
    },
    {
      "epoch": 996.4705882352941,
      "grad_norm": 9.89855670928955,
      "learning_rate": 4.003529411764706e-05,
      "loss": 2.2224,
      "step": 16940
    },
    {
      "epoch": 997.0588235294117,
      "grad_norm": 8.252302169799805,
      "learning_rate": 4.002941176470588e-05,
      "loss": 2.1586,
      "step": 16950
    },
    {
      "epoch": 997.6470588235294,
      "grad_norm": 11.326061248779297,
      "learning_rate": 4.002352941176471e-05,
      "loss": 2.2469,
      "step": 16960
    },
    {
      "epoch": 998.2352941176471,
      "grad_norm": 11.081330299377441,
      "learning_rate": 4.0017647058823534e-05,
      "loss": 2.1652,
      "step": 16970
    },
    {
      "epoch": 998.8235294117648,
      "grad_norm": 11.416297912597656,
      "learning_rate": 4.001176470588236e-05,
      "loss": 2.1906,
      "step": 16980
    },
    {
      "epoch": 999.4117647058823,
      "grad_norm": 11.244470596313477,
      "learning_rate": 4.0005882352941174e-05,
      "loss": 2.1328,
      "step": 16990
    },
    {
      "epoch": 1000.0,
      "grad_norm": 16.830753326416016,
      "learning_rate": 4e-05,
      "loss": 2.3577,
      "step": 17000
    },
    {
      "epoch": 1000.5882352941177,
      "grad_norm": 10.464076042175293,
      "learning_rate": 3.9994117647058826e-05,
      "loss": 2.0622,
      "step": 17010
    },
    {
      "epoch": 1001.1764705882352,
      "grad_norm": 9.811016082763672,
      "learning_rate": 3.998823529411765e-05,
      "loss": 2.1848,
      "step": 17020
    },
    {
      "epoch": 1001.7647058823529,
      "grad_norm": 9.00339126586914,
      "learning_rate": 3.998235294117647e-05,
      "loss": 2.081,
      "step": 17030
    },
    {
      "epoch": 1002.3529411764706,
      "grad_norm": 11.997312545776367,
      "learning_rate": 3.9976470588235295e-05,
      "loss": 2.1622,
      "step": 17040
    },
    {
      "epoch": 1002.9411764705883,
      "grad_norm": 9.3151273727417,
      "learning_rate": 3.997058823529412e-05,
      "loss": 2.3132,
      "step": 17050
    },
    {
      "epoch": 1003.5294117647059,
      "grad_norm": 10.959065437316895,
      "learning_rate": 3.996470588235294e-05,
      "loss": 2.217,
      "step": 17060
    },
    {
      "epoch": 1004.1176470588235,
      "grad_norm": 11.570197105407715,
      "learning_rate": 3.9958823529411764e-05,
      "loss": 2.2756,
      "step": 17070
    },
    {
      "epoch": 1004.7058823529412,
      "grad_norm": 10.594267845153809,
      "learning_rate": 3.995294117647059e-05,
      "loss": 2.1486,
      "step": 17080
    },
    {
      "epoch": 1005.2941176470588,
      "grad_norm": 9.874429702758789,
      "learning_rate": 3.994705882352942e-05,
      "loss": 2.1201,
      "step": 17090
    },
    {
      "epoch": 1005.8823529411765,
      "grad_norm": 9.302041053771973,
      "learning_rate": 3.994117647058824e-05,
      "loss": 2.1323,
      "step": 17100
    },
    {
      "epoch": 1006.4705882352941,
      "grad_norm": 13.432379722595215,
      "learning_rate": 3.993529411764706e-05,
      "loss": 2.2978,
      "step": 17110
    },
    {
      "epoch": 1007.0588235294117,
      "grad_norm": 12.819732666015625,
      "learning_rate": 3.992941176470588e-05,
      "loss": 2.0884,
      "step": 17120
    },
    {
      "epoch": 1007.6470588235294,
      "grad_norm": 12.58803653717041,
      "learning_rate": 3.992352941176471e-05,
      "loss": 2.1669,
      "step": 17130
    },
    {
      "epoch": 1008.2352941176471,
      "grad_norm": 11.130874633789062,
      "learning_rate": 3.991764705882353e-05,
      "loss": 2.3052,
      "step": 17140
    },
    {
      "epoch": 1008.8235294117648,
      "grad_norm": 9.790423393249512,
      "learning_rate": 3.9911764705882355e-05,
      "loss": 2.0385,
      "step": 17150
    },
    {
      "epoch": 1009.4117647058823,
      "grad_norm": 14.574631690979004,
      "learning_rate": 3.990588235294118e-05,
      "loss": 2.2692,
      "step": 17160
    },
    {
      "epoch": 1010.0,
      "grad_norm": 9.61003589630127,
      "learning_rate": 3.99e-05,
      "loss": 2.2271,
      "step": 17170
    },
    {
      "epoch": 1010.5882352941177,
      "grad_norm": 9.946730613708496,
      "learning_rate": 3.9894117647058824e-05,
      "loss": 2.1005,
      "step": 17180
    },
    {
      "epoch": 1011.1764705882352,
      "grad_norm": 9.794121742248535,
      "learning_rate": 3.988823529411765e-05,
      "loss": 2.2139,
      "step": 17190
    },
    {
      "epoch": 1011.7647058823529,
      "grad_norm": 12.792597770690918,
      "learning_rate": 3.988235294117647e-05,
      "loss": 2.1412,
      "step": 17200
    },
    {
      "epoch": 1012.3529411764706,
      "grad_norm": 11.427395820617676,
      "learning_rate": 3.98764705882353e-05,
      "loss": 2.059,
      "step": 17210
    },
    {
      "epoch": 1012.9411764705883,
      "grad_norm": 11.054469108581543,
      "learning_rate": 3.987058823529412e-05,
      "loss": 2.2456,
      "step": 17220
    },
    {
      "epoch": 1013.5294117647059,
      "grad_norm": 9.409120559692383,
      "learning_rate": 3.9864705882352946e-05,
      "loss": 2.246,
      "step": 17230
    },
    {
      "epoch": 1014.1176470588235,
      "grad_norm": 10.109783172607422,
      "learning_rate": 3.985882352941176e-05,
      "loss": 2.0484,
      "step": 17240
    },
    {
      "epoch": 1014.7058823529412,
      "grad_norm": 10.016142845153809,
      "learning_rate": 3.985294117647059e-05,
      "loss": 2.2032,
      "step": 17250
    },
    {
      "epoch": 1015.2941176470588,
      "grad_norm": 9.156023979187012,
      "learning_rate": 3.9847058823529415e-05,
      "loss": 2.1129,
      "step": 17260
    },
    {
      "epoch": 1015.8823529411765,
      "grad_norm": 11.057316780090332,
      "learning_rate": 3.984117647058824e-05,
      "loss": 2.4088,
      "step": 17270
    },
    {
      "epoch": 1016.4705882352941,
      "grad_norm": 11.217646598815918,
      "learning_rate": 3.983529411764706e-05,
      "loss": 2.1951,
      "step": 17280
    },
    {
      "epoch": 1017.0588235294117,
      "grad_norm": 11.135693550109863,
      "learning_rate": 3.9829411764705883e-05,
      "loss": 2.216,
      "step": 17290
    },
    {
      "epoch": 1017.6470588235294,
      "grad_norm": 9.816744804382324,
      "learning_rate": 3.9823529411764706e-05,
      "loss": 2.142,
      "step": 17300
    },
    {
      "epoch": 1018.2352941176471,
      "grad_norm": 10.485724449157715,
      "learning_rate": 3.981764705882353e-05,
      "loss": 2.2601,
      "step": 17310
    },
    {
      "epoch": 1018.8235294117648,
      "grad_norm": 9.402945518493652,
      "learning_rate": 3.981176470588235e-05,
      "loss": 2.0969,
      "step": 17320
    },
    {
      "epoch": 1019.4117647058823,
      "grad_norm": 12.549117088317871,
      "learning_rate": 3.9805882352941175e-05,
      "loss": 2.116,
      "step": 17330
    },
    {
      "epoch": 1020.0,
      "grad_norm": 13.807108879089355,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 2.1938,
      "step": 17340
    },
    {
      "epoch": 1020.5882352941177,
      "grad_norm": 8.298866271972656,
      "learning_rate": 3.979411764705883e-05,
      "loss": 2.2003,
      "step": 17350
    },
    {
      "epoch": 1021.1764705882352,
      "grad_norm": 9.706637382507324,
      "learning_rate": 3.978823529411765e-05,
      "loss": 2.2631,
      "step": 17360
    },
    {
      "epoch": 1021.7647058823529,
      "grad_norm": 10.49267578125,
      "learning_rate": 3.978235294117647e-05,
      "loss": 2.1763,
      "step": 17370
    },
    {
      "epoch": 1022.3529411764706,
      "grad_norm": 11.438126564025879,
      "learning_rate": 3.97764705882353e-05,
      "loss": 2.1368,
      "step": 17380
    },
    {
      "epoch": 1022.9411764705883,
      "grad_norm": 10.70329761505127,
      "learning_rate": 3.977058823529412e-05,
      "loss": 2.1728,
      "step": 17390
    },
    {
      "epoch": 1023.5294117647059,
      "grad_norm": 11.532241821289062,
      "learning_rate": 3.976470588235294e-05,
      "loss": 2.2714,
      "step": 17400
    },
    {
      "epoch": 1024.1176470588234,
      "grad_norm": 10.559463500976562,
      "learning_rate": 3.9758823529411766e-05,
      "loss": 2.1711,
      "step": 17410
    },
    {
      "epoch": 1024.7058823529412,
      "grad_norm": 15.270745277404785,
      "learning_rate": 3.9752941176470596e-05,
      "loss": 2.1836,
      "step": 17420
    },
    {
      "epoch": 1025.2941176470588,
      "grad_norm": 10.708084106445312,
      "learning_rate": 3.974705882352941e-05,
      "loss": 2.0624,
      "step": 17430
    },
    {
      "epoch": 1025.8823529411766,
      "grad_norm": 11.060120582580566,
      "learning_rate": 3.9741176470588235e-05,
      "loss": 2.2072,
      "step": 17440
    },
    {
      "epoch": 1026.4705882352941,
      "grad_norm": 10.052484512329102,
      "learning_rate": 3.973529411764706e-05,
      "loss": 2.1954,
      "step": 17450
    },
    {
      "epoch": 1027.0588235294117,
      "grad_norm": 9.0772066116333,
      "learning_rate": 3.972941176470588e-05,
      "loss": 2.2747,
      "step": 17460
    },
    {
      "epoch": 1027.6470588235295,
      "grad_norm": 11.144219398498535,
      "learning_rate": 3.972352941176471e-05,
      "loss": 2.2729,
      "step": 17470
    },
    {
      "epoch": 1028.235294117647,
      "grad_norm": 8.980666160583496,
      "learning_rate": 3.9717647058823534e-05,
      "loss": 2.0863,
      "step": 17480
    },
    {
      "epoch": 1028.8235294117646,
      "grad_norm": 11.149394035339355,
      "learning_rate": 3.971176470588236e-05,
      "loss": 2.1165,
      "step": 17490
    },
    {
      "epoch": 1029.4117647058824,
      "grad_norm": 10.965478897094727,
      "learning_rate": 3.970588235294117e-05,
      "loss": 2.2375,
      "step": 17500
    },
    {
      "epoch": 1030.0,
      "grad_norm": 13.317082405090332,
      "learning_rate": 3.97e-05,
      "loss": 2.1069,
      "step": 17510
    },
    {
      "epoch": 1030.5882352941176,
      "grad_norm": 12.894904136657715,
      "learning_rate": 3.9694117647058826e-05,
      "loss": 2.0373,
      "step": 17520
    },
    {
      "epoch": 1031.1764705882354,
      "grad_norm": 10.055974960327148,
      "learning_rate": 3.968823529411765e-05,
      "loss": 2.2292,
      "step": 17530
    },
    {
      "epoch": 1031.764705882353,
      "grad_norm": 11.153812408447266,
      "learning_rate": 3.968235294117647e-05,
      "loss": 2.1994,
      "step": 17540
    },
    {
      "epoch": 1032.3529411764705,
      "grad_norm": 9.972662925720215,
      "learning_rate": 3.96764705882353e-05,
      "loss": 2.0259,
      "step": 17550
    },
    {
      "epoch": 1032.9411764705883,
      "grad_norm": 9.74210262298584,
      "learning_rate": 3.967058823529412e-05,
      "loss": 2.1272,
      "step": 17560
    },
    {
      "epoch": 1033.5294117647059,
      "grad_norm": 8.746384620666504,
      "learning_rate": 3.966470588235294e-05,
      "loss": 2.069,
      "step": 17570
    },
    {
      "epoch": 1034.1176470588234,
      "grad_norm": 11.277053833007812,
      "learning_rate": 3.9658823529411764e-05,
      "loss": 2.1531,
      "step": 17580
    },
    {
      "epoch": 1034.7058823529412,
      "grad_norm": 9.163512229919434,
      "learning_rate": 3.9652941176470593e-05,
      "loss": 2.2389,
      "step": 17590
    },
    {
      "epoch": 1035.2941176470588,
      "grad_norm": 10.36952018737793,
      "learning_rate": 3.9647058823529416e-05,
      "loss": 2.1971,
      "step": 17600
    },
    {
      "epoch": 1035.8823529411766,
      "grad_norm": 10.177316665649414,
      "learning_rate": 3.964117647058824e-05,
      "loss": 2.1581,
      "step": 17610
    },
    {
      "epoch": 1036.4705882352941,
      "grad_norm": 12.163348197937012,
      "learning_rate": 3.963529411764706e-05,
      "loss": 2.0804,
      "step": 17620
    },
    {
      "epoch": 1037.0588235294117,
      "grad_norm": 17.468061447143555,
      "learning_rate": 3.962941176470588e-05,
      "loss": 2.1058,
      "step": 17630
    },
    {
      "epoch": 1037.6470588235295,
      "grad_norm": 12.314107894897461,
      "learning_rate": 3.962352941176471e-05,
      "loss": 2.094,
      "step": 17640
    },
    {
      "epoch": 1038.235294117647,
      "grad_norm": 8.926970481872559,
      "learning_rate": 3.961764705882353e-05,
      "loss": 2.1072,
      "step": 17650
    },
    {
      "epoch": 1038.8235294117646,
      "grad_norm": 10.939376831054688,
      "learning_rate": 3.9611764705882354e-05,
      "loss": 2.0682,
      "step": 17660
    },
    {
      "epoch": 1039.4117647058824,
      "grad_norm": 11.793060302734375,
      "learning_rate": 3.960588235294118e-05,
      "loss": 2.2613,
      "step": 17670
    },
    {
      "epoch": 1040.0,
      "grad_norm": 17.97602081298828,
      "learning_rate": 3.960000000000001e-05,
      "loss": 2.1438,
      "step": 17680
    },
    {
      "epoch": 1040.5882352941176,
      "grad_norm": 11.571403503417969,
      "learning_rate": 3.959411764705882e-05,
      "loss": 1.9781,
      "step": 17690
    },
    {
      "epoch": 1041.1764705882354,
      "grad_norm": 12.028213500976562,
      "learning_rate": 3.9588235294117646e-05,
      "loss": 2.17,
      "step": 17700
    },
    {
      "epoch": 1041.764705882353,
      "grad_norm": 9.729110717773438,
      "learning_rate": 3.958235294117647e-05,
      "loss": 2.0704,
      "step": 17710
    },
    {
      "epoch": 1042.3529411764705,
      "grad_norm": 9.62533950805664,
      "learning_rate": 3.95764705882353e-05,
      "loss": 2.1164,
      "step": 17720
    },
    {
      "epoch": 1042.9411764705883,
      "grad_norm": 11.097369194030762,
      "learning_rate": 3.957058823529412e-05,
      "loss": 2.1076,
      "step": 17730
    },
    {
      "epoch": 1043.5294117647059,
      "grad_norm": 11.239827156066895,
      "learning_rate": 3.9564705882352945e-05,
      "loss": 2.2933,
      "step": 17740
    },
    {
      "epoch": 1044.1176470588234,
      "grad_norm": 10.396063804626465,
      "learning_rate": 3.955882352941177e-05,
      "loss": 2.1573,
      "step": 17750
    },
    {
      "epoch": 1044.7058823529412,
      "grad_norm": 8.448149681091309,
      "learning_rate": 3.955294117647059e-05,
      "loss": 2.0861,
      "step": 17760
    },
    {
      "epoch": 1045.2941176470588,
      "grad_norm": 9.24939250946045,
      "learning_rate": 3.9547058823529414e-05,
      "loss": 2.1208,
      "step": 17770
    },
    {
      "epoch": 1045.8823529411766,
      "grad_norm": 8.19542121887207,
      "learning_rate": 3.954117647058824e-05,
      "loss": 2.1492,
      "step": 17780
    },
    {
      "epoch": 1046.4705882352941,
      "grad_norm": 12.959171295166016,
      "learning_rate": 3.953529411764706e-05,
      "loss": 2.0617,
      "step": 17790
    },
    {
      "epoch": 1047.0588235294117,
      "grad_norm": 9.74923038482666,
      "learning_rate": 3.952941176470588e-05,
      "loss": 2.3059,
      "step": 17800
    },
    {
      "epoch": 1047.6470588235295,
      "grad_norm": 10.732665061950684,
      "learning_rate": 3.9523529411764706e-05,
      "loss": 2.1495,
      "step": 17810
    },
    {
      "epoch": 1048.235294117647,
      "grad_norm": 11.237049102783203,
      "learning_rate": 3.951764705882353e-05,
      "loss": 2.2207,
      "step": 17820
    },
    {
      "epoch": 1048.8235294117646,
      "grad_norm": 12.831642150878906,
      "learning_rate": 3.951176470588235e-05,
      "loss": 2.2081,
      "step": 17830
    },
    {
      "epoch": 1049.4117647058824,
      "grad_norm": 11.422910690307617,
      "learning_rate": 3.9505882352941175e-05,
      "loss": 2.0713,
      "step": 17840
    },
    {
      "epoch": 1050.0,
      "grad_norm": 12.047762870788574,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 2.2048,
      "step": 17850
    },
    {
      "epoch": 1050.5882352941176,
      "grad_norm": 9.636959075927734,
      "learning_rate": 3.949411764705883e-05,
      "loss": 2.0447,
      "step": 17860
    },
    {
      "epoch": 1051.1764705882354,
      "grad_norm": 9.48889446258545,
      "learning_rate": 3.948823529411765e-05,
      "loss": 2.0409,
      "step": 17870
    },
    {
      "epoch": 1051.764705882353,
      "grad_norm": 10.548211097717285,
      "learning_rate": 3.9482352941176474e-05,
      "loss": 2.0083,
      "step": 17880
    },
    {
      "epoch": 1052.3529411764705,
      "grad_norm": 10.421194076538086,
      "learning_rate": 3.94764705882353e-05,
      "loss": 2.3452,
      "step": 17890
    },
    {
      "epoch": 1052.9411764705883,
      "grad_norm": 11.52953052520752,
      "learning_rate": 3.947058823529412e-05,
      "loss": 2.1609,
      "step": 17900
    },
    {
      "epoch": 1053.5294117647059,
      "grad_norm": 10.836285591125488,
      "learning_rate": 3.946470588235294e-05,
      "loss": 2.1309,
      "step": 17910
    },
    {
      "epoch": 1054.1176470588234,
      "grad_norm": 10.72829532623291,
      "learning_rate": 3.9458823529411766e-05,
      "loss": 2.104,
      "step": 17920
    },
    {
      "epoch": 1054.7058823529412,
      "grad_norm": 9.92480754852295,
      "learning_rate": 3.9452941176470595e-05,
      "loss": 2.2004,
      "step": 17930
    },
    {
      "epoch": 1055.2941176470588,
      "grad_norm": 12.001250267028809,
      "learning_rate": 3.944705882352941e-05,
      "loss": 2.1579,
      "step": 17940
    },
    {
      "epoch": 1055.8823529411766,
      "grad_norm": 11.19460678100586,
      "learning_rate": 3.9441176470588235e-05,
      "loss": 2.165,
      "step": 17950
    },
    {
      "epoch": 1056.4705882352941,
      "grad_norm": 11.427847862243652,
      "learning_rate": 3.943529411764706e-05,
      "loss": 2.147,
      "step": 17960
    },
    {
      "epoch": 1057.0588235294117,
      "grad_norm": 10.563824653625488,
      "learning_rate": 3.942941176470589e-05,
      "loss": 2.1395,
      "step": 17970
    },
    {
      "epoch": 1057.6470588235295,
      "grad_norm": 11.509167671203613,
      "learning_rate": 3.942352941176471e-05,
      "loss": 2.1034,
      "step": 17980
    },
    {
      "epoch": 1058.235294117647,
      "grad_norm": 10.465616226196289,
      "learning_rate": 3.941764705882353e-05,
      "loss": 2.011,
      "step": 17990
    },
    {
      "epoch": 1058.8235294117646,
      "grad_norm": 8.029096603393555,
      "learning_rate": 3.9411764705882356e-05,
      "loss": 2.0233,
      "step": 18000
    },
    {
      "epoch": 1059.4117647058824,
      "grad_norm": 11.564478874206543,
      "learning_rate": 3.940588235294117e-05,
      "loss": 2.1209,
      "step": 18010
    },
    {
      "epoch": 1060.0,
      "grad_norm": 12.533398628234863,
      "learning_rate": 3.94e-05,
      "loss": 2.2314,
      "step": 18020
    },
    {
      "epoch": 1060.5882352941176,
      "grad_norm": 10.13084602355957,
      "learning_rate": 3.9394117647058825e-05,
      "loss": 2.1367,
      "step": 18030
    },
    {
      "epoch": 1061.1764705882354,
      "grad_norm": 10.86794662475586,
      "learning_rate": 3.938823529411765e-05,
      "loss": 2.173,
      "step": 18040
    },
    {
      "epoch": 1061.764705882353,
      "grad_norm": 13.3825101852417,
      "learning_rate": 3.938235294117647e-05,
      "loss": 2.2198,
      "step": 18050
    },
    {
      "epoch": 1062.3529411764705,
      "grad_norm": 12.28870964050293,
      "learning_rate": 3.93764705882353e-05,
      "loss": 2.1529,
      "step": 18060
    },
    {
      "epoch": 1062.9411764705883,
      "grad_norm": 12.557308197021484,
      "learning_rate": 3.937058823529412e-05,
      "loss": 2.1777,
      "step": 18070
    },
    {
      "epoch": 1063.5294117647059,
      "grad_norm": 10.498889923095703,
      "learning_rate": 3.936470588235294e-05,
      "loss": 2.274,
      "step": 18080
    },
    {
      "epoch": 1064.1176470588234,
      "grad_norm": 11.845873832702637,
      "learning_rate": 3.935882352941176e-05,
      "loss": 2.1462,
      "step": 18090
    },
    {
      "epoch": 1064.7058823529412,
      "grad_norm": 10.210806846618652,
      "learning_rate": 3.935294117647059e-05,
      "loss": 2.0993,
      "step": 18100
    },
    {
      "epoch": 1065.2941176470588,
      "grad_norm": 15.425264358520508,
      "learning_rate": 3.9347058823529416e-05,
      "loss": 2.1755,
      "step": 18110
    },
    {
      "epoch": 1065.8823529411766,
      "grad_norm": 10.485901832580566,
      "learning_rate": 3.934117647058824e-05,
      "loss": 2.2297,
      "step": 18120
    },
    {
      "epoch": 1066.4705882352941,
      "grad_norm": 12.250749588012695,
      "learning_rate": 3.933529411764706e-05,
      "loss": 2.2104,
      "step": 18130
    },
    {
      "epoch": 1067.0588235294117,
      "grad_norm": 11.263753890991211,
      "learning_rate": 3.9329411764705885e-05,
      "loss": 2.1412,
      "step": 18140
    },
    {
      "epoch": 1067.6470588235295,
      "grad_norm": 13.062752723693848,
      "learning_rate": 3.932352941176471e-05,
      "loss": 2.1755,
      "step": 18150
    },
    {
      "epoch": 1068.235294117647,
      "grad_norm": 8.896390914916992,
      "learning_rate": 3.931764705882353e-05,
      "loss": 2.1854,
      "step": 18160
    },
    {
      "epoch": 1068.8235294117646,
      "grad_norm": 13.200261116027832,
      "learning_rate": 3.9311764705882354e-05,
      "loss": 2.195,
      "step": 18170
    },
    {
      "epoch": 1069.4117647058824,
      "grad_norm": 10.178077697753906,
      "learning_rate": 3.930588235294118e-05,
      "loss": 2.0632,
      "step": 18180
    },
    {
      "epoch": 1070.0,
      "grad_norm": 14.809138298034668,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 2.2094,
      "step": 18190
    },
    {
      "epoch": 1070.5882352941176,
      "grad_norm": 11.964882850646973,
      "learning_rate": 3.929411764705882e-05,
      "loss": 2.0892,
      "step": 18200
    },
    {
      "epoch": 1071.1764705882354,
      "grad_norm": 12.0783109664917,
      "learning_rate": 3.9288235294117646e-05,
      "loss": 2.0996,
      "step": 18210
    },
    {
      "epoch": 1071.764705882353,
      "grad_norm": 11.661174774169922,
      "learning_rate": 3.928235294117647e-05,
      "loss": 2.0637,
      "step": 18220
    },
    {
      "epoch": 1072.3529411764705,
      "grad_norm": 12.079453468322754,
      "learning_rate": 3.92764705882353e-05,
      "loss": 2.0404,
      "step": 18230
    },
    {
      "epoch": 1072.9411764705883,
      "grad_norm": 13.91577434539795,
      "learning_rate": 3.927058823529412e-05,
      "loss": 2.2277,
      "step": 18240
    },
    {
      "epoch": 1073.5294117647059,
      "grad_norm": 14.536924362182617,
      "learning_rate": 3.9264705882352945e-05,
      "loss": 2.1258,
      "step": 18250
    },
    {
      "epoch": 1074.1176470588234,
      "grad_norm": 9.256760597229004,
      "learning_rate": 3.925882352941177e-05,
      "loss": 2.1822,
      "step": 18260
    },
    {
      "epoch": 1074.7058823529412,
      "grad_norm": 13.03426456451416,
      "learning_rate": 3.925294117647059e-05,
      "loss": 2.1419,
      "step": 18270
    },
    {
      "epoch": 1075.2941176470588,
      "grad_norm": 10.85112476348877,
      "learning_rate": 3.9247058823529413e-05,
      "loss": 2.0599,
      "step": 18280
    },
    {
      "epoch": 1075.8823529411766,
      "grad_norm": 11.945072174072266,
      "learning_rate": 3.9241176470588236e-05,
      "loss": 2.148,
      "step": 18290
    },
    {
      "epoch": 1076.4705882352941,
      "grad_norm": 12.40876579284668,
      "learning_rate": 3.923529411764706e-05,
      "loss": 2.0245,
      "step": 18300
    },
    {
      "epoch": 1077.0588235294117,
      "grad_norm": 10.8834228515625,
      "learning_rate": 3.922941176470589e-05,
      "loss": 2.1395,
      "step": 18310
    },
    {
      "epoch": 1077.6470588235295,
      "grad_norm": 12.565703392028809,
      "learning_rate": 3.922352941176471e-05,
      "loss": 2.1619,
      "step": 18320
    },
    {
      "epoch": 1078.235294117647,
      "grad_norm": 13.126307487487793,
      "learning_rate": 3.921764705882353e-05,
      "loss": 2.2421,
      "step": 18330
    },
    {
      "epoch": 1078.8235294117646,
      "grad_norm": 12.310079574584961,
      "learning_rate": 3.921176470588235e-05,
      "loss": 2.1613,
      "step": 18340
    },
    {
      "epoch": 1079.4117647058824,
      "grad_norm": 9.53220272064209,
      "learning_rate": 3.9205882352941174e-05,
      "loss": 2.0711,
      "step": 18350
    },
    {
      "epoch": 1080.0,
      "grad_norm": 12.803754806518555,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 2.0972,
      "step": 18360
    },
    {
      "epoch": 1080.5882352941176,
      "grad_norm": 12.954780578613281,
      "learning_rate": 3.919411764705883e-05,
      "loss": 2.0991,
      "step": 18370
    },
    {
      "epoch": 1081.1764705882354,
      "grad_norm": 14.399454116821289,
      "learning_rate": 3.918823529411765e-05,
      "loss": 2.1561,
      "step": 18380
    },
    {
      "epoch": 1081.764705882353,
      "grad_norm": 10.986102104187012,
      "learning_rate": 3.918235294117647e-05,
      "loss": 2.0888,
      "step": 18390
    },
    {
      "epoch": 1082.3529411764705,
      "grad_norm": 11.034863471984863,
      "learning_rate": 3.9176470588235296e-05,
      "loss": 2.1521,
      "step": 18400
    },
    {
      "epoch": 1082.9411764705883,
      "grad_norm": 12.105878829956055,
      "learning_rate": 3.917058823529412e-05,
      "loss": 2.171,
      "step": 18410
    },
    {
      "epoch": 1083.5294117647059,
      "grad_norm": 9.987798690795898,
      "learning_rate": 3.916470588235294e-05,
      "loss": 2.1929,
      "step": 18420
    },
    {
      "epoch": 1084.1176470588234,
      "grad_norm": 9.951519966125488,
      "learning_rate": 3.9158823529411765e-05,
      "loss": 1.9697,
      "step": 18430
    },
    {
      "epoch": 1084.7058823529412,
      "grad_norm": 11.740571022033691,
      "learning_rate": 3.9152941176470595e-05,
      "loss": 2.0597,
      "step": 18440
    },
    {
      "epoch": 1085.2941176470588,
      "grad_norm": 10.83354377746582,
      "learning_rate": 3.914705882352942e-05,
      "loss": 2.0881,
      "step": 18450
    },
    {
      "epoch": 1085.8823529411766,
      "grad_norm": 10.375672340393066,
      "learning_rate": 3.9141176470588234e-05,
      "loss": 2.1048,
      "step": 18460
    },
    {
      "epoch": 1086.4705882352941,
      "grad_norm": 12.506996154785156,
      "learning_rate": 3.913529411764706e-05,
      "loss": 2.0854,
      "step": 18470
    },
    {
      "epoch": 1087.0588235294117,
      "grad_norm": 11.480012893676758,
      "learning_rate": 3.912941176470589e-05,
      "loss": 2.1703,
      "step": 18480
    },
    {
      "epoch": 1087.6470588235295,
      "grad_norm": 9.089701652526855,
      "learning_rate": 3.912352941176471e-05,
      "loss": 2.0662,
      "step": 18490
    },
    {
      "epoch": 1088.235294117647,
      "grad_norm": 12.922221183776855,
      "learning_rate": 3.911764705882353e-05,
      "loss": 2.1614,
      "step": 18500
    },
    {
      "epoch": 1088.8235294117646,
      "grad_norm": 10.018811225891113,
      "learning_rate": 3.9111764705882356e-05,
      "loss": 2.0166,
      "step": 18510
    },
    {
      "epoch": 1089.4117647058824,
      "grad_norm": 14.623395919799805,
      "learning_rate": 3.910588235294118e-05,
      "loss": 2.2243,
      "step": 18520
    },
    {
      "epoch": 1090.0,
      "grad_norm": 11.847392082214355,
      "learning_rate": 3.91e-05,
      "loss": 2.1774,
      "step": 18530
    },
    {
      "epoch": 1090.5882352941176,
      "grad_norm": 8.416250228881836,
      "learning_rate": 3.9094117647058825e-05,
      "loss": 2.1324,
      "step": 18540
    },
    {
      "epoch": 1091.1764705882354,
      "grad_norm": 9.421360969543457,
      "learning_rate": 3.908823529411765e-05,
      "loss": 2.0346,
      "step": 18550
    },
    {
      "epoch": 1091.764705882353,
      "grad_norm": 12.349271774291992,
      "learning_rate": 3.908235294117647e-05,
      "loss": 2.1575,
      "step": 18560
    },
    {
      "epoch": 1092.3529411764705,
      "grad_norm": 11.245037078857422,
      "learning_rate": 3.90764705882353e-05,
      "loss": 2.0712,
      "step": 18570
    },
    {
      "epoch": 1092.9411764705883,
      "grad_norm": 11.400418281555176,
      "learning_rate": 3.907058823529412e-05,
      "loss": 2.0918,
      "step": 18580
    },
    {
      "epoch": 1093.5294117647059,
      "grad_norm": 9.491520881652832,
      "learning_rate": 3.906470588235294e-05,
      "loss": 2.0192,
      "step": 18590
    },
    {
      "epoch": 1094.1176470588234,
      "grad_norm": 11.953338623046875,
      "learning_rate": 3.905882352941176e-05,
      "loss": 1.9504,
      "step": 18600
    },
    {
      "epoch": 1094.7058823529412,
      "grad_norm": 13.609404563903809,
      "learning_rate": 3.905294117647059e-05,
      "loss": 2.2184,
      "step": 18610
    },
    {
      "epoch": 1095.2941176470588,
      "grad_norm": 12.377507209777832,
      "learning_rate": 3.9047058823529415e-05,
      "loss": 2.2442,
      "step": 18620
    },
    {
      "epoch": 1095.8823529411766,
      "grad_norm": 12.59093189239502,
      "learning_rate": 3.904117647058824e-05,
      "loss": 2.1151,
      "step": 18630
    },
    {
      "epoch": 1096.4705882352941,
      "grad_norm": 10.68412971496582,
      "learning_rate": 3.903529411764706e-05,
      "loss": 2.1355,
      "step": 18640
    },
    {
      "epoch": 1097.0588235294117,
      "grad_norm": 11.195656776428223,
      "learning_rate": 3.9029411764705884e-05,
      "loss": 2.1419,
      "step": 18650
    },
    {
      "epoch": 1097.6470588235295,
      "grad_norm": 12.446856498718262,
      "learning_rate": 3.902352941176471e-05,
      "loss": 2.0556,
      "step": 18660
    },
    {
      "epoch": 1098.235294117647,
      "grad_norm": 12.736409187316895,
      "learning_rate": 3.901764705882353e-05,
      "loss": 2.1748,
      "step": 18670
    },
    {
      "epoch": 1098.8235294117646,
      "grad_norm": 11.16213321685791,
      "learning_rate": 3.901176470588235e-05,
      "loss": 2.0887,
      "step": 18680
    },
    {
      "epoch": 1099.4117647058824,
      "grad_norm": 8.51433277130127,
      "learning_rate": 3.900588235294118e-05,
      "loss": 1.9987,
      "step": 18690
    },
    {
      "epoch": 1100.0,
      "grad_norm": 11.709518432617188,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 2.0225,
      "step": 18700
    },
    {
      "epoch": 1100.5882352941176,
      "grad_norm": 11.727051734924316,
      "learning_rate": 3.899411764705882e-05,
      "loss": 2.1154,
      "step": 18710
    },
    {
      "epoch": 1101.1764705882354,
      "grad_norm": 11.349689483642578,
      "learning_rate": 3.8988235294117645e-05,
      "loss": 2.1062,
      "step": 18720
    },
    {
      "epoch": 1101.764705882353,
      "grad_norm": 12.314374923706055,
      "learning_rate": 3.898235294117647e-05,
      "loss": 2.1395,
      "step": 18730
    },
    {
      "epoch": 1102.3529411764705,
      "grad_norm": 13.560599327087402,
      "learning_rate": 3.89764705882353e-05,
      "loss": 2.0506,
      "step": 18740
    },
    {
      "epoch": 1102.9411764705883,
      "grad_norm": 13.26806926727295,
      "learning_rate": 3.897058823529412e-05,
      "loss": 2.2254,
      "step": 18750
    },
    {
      "epoch": 1103.5294117647059,
      "grad_norm": 12.828312873840332,
      "learning_rate": 3.8964705882352944e-05,
      "loss": 2.3325,
      "step": 18760
    },
    {
      "epoch": 1104.1176470588234,
      "grad_norm": 10.567780494689941,
      "learning_rate": 3.895882352941177e-05,
      "loss": 2.1415,
      "step": 18770
    },
    {
      "epoch": 1104.7058823529412,
      "grad_norm": 15.216736793518066,
      "learning_rate": 3.895294117647059e-05,
      "loss": 2.0193,
      "step": 18780
    },
    {
      "epoch": 1105.2941176470588,
      "grad_norm": 12.52806282043457,
      "learning_rate": 3.894705882352941e-05,
      "loss": 2.1066,
      "step": 18790
    },
    {
      "epoch": 1105.8823529411766,
      "grad_norm": 14.215229034423828,
      "learning_rate": 3.8941176470588236e-05,
      "loss": 1.9833,
      "step": 18800
    },
    {
      "epoch": 1106.4705882352941,
      "grad_norm": 9.431519508361816,
      "learning_rate": 3.893529411764706e-05,
      "loss": 2.0244,
      "step": 18810
    },
    {
      "epoch": 1107.0588235294117,
      "grad_norm": 10.379531860351562,
      "learning_rate": 3.892941176470589e-05,
      "loss": 2.1158,
      "step": 18820
    },
    {
      "epoch": 1107.6470588235295,
      "grad_norm": 11.178291320800781,
      "learning_rate": 3.892352941176471e-05,
      "loss": 2.1635,
      "step": 18830
    },
    {
      "epoch": 1108.235294117647,
      "grad_norm": 13.878548622131348,
      "learning_rate": 3.891764705882353e-05,
      "loss": 2.0693,
      "step": 18840
    },
    {
      "epoch": 1108.8235294117646,
      "grad_norm": 10.58812141418457,
      "learning_rate": 3.891176470588235e-05,
      "loss": 2.1072,
      "step": 18850
    },
    {
      "epoch": 1109.4117647058824,
      "grad_norm": 10.529618263244629,
      "learning_rate": 3.890588235294118e-05,
      "loss": 2.1427,
      "step": 18860
    },
    {
      "epoch": 1110.0,
      "grad_norm": 12.505454063415527,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 2.1237,
      "step": 18870
    },
    {
      "epoch": 1110.5882352941176,
      "grad_norm": 11.461698532104492,
      "learning_rate": 3.889411764705883e-05,
      "loss": 2.0932,
      "step": 18880
    },
    {
      "epoch": 1111.1764705882354,
      "grad_norm": 12.269026756286621,
      "learning_rate": 3.888823529411765e-05,
      "loss": 2.0961,
      "step": 18890
    },
    {
      "epoch": 1111.764705882353,
      "grad_norm": 11.360793113708496,
      "learning_rate": 3.888235294117647e-05,
      "loss": 2.153,
      "step": 18900
    },
    {
      "epoch": 1112.3529411764705,
      "grad_norm": 13.393471717834473,
      "learning_rate": 3.8876470588235296e-05,
      "loss": 2.1122,
      "step": 18910
    },
    {
      "epoch": 1112.9411764705883,
      "grad_norm": 11.880382537841797,
      "learning_rate": 3.887058823529412e-05,
      "loss": 2.0161,
      "step": 18920
    },
    {
      "epoch": 1113.5294117647059,
      "grad_norm": 10.254257202148438,
      "learning_rate": 3.886470588235294e-05,
      "loss": 1.9026,
      "step": 18930
    },
    {
      "epoch": 1114.1176470588234,
      "grad_norm": 9.481183052062988,
      "learning_rate": 3.8858823529411765e-05,
      "loss": 2.0636,
      "step": 18940
    },
    {
      "epoch": 1114.7058823529412,
      "grad_norm": 9.97640323638916,
      "learning_rate": 3.8852941176470594e-05,
      "loss": 2.1596,
      "step": 18950
    },
    {
      "epoch": 1115.2941176470588,
      "grad_norm": 11.292840957641602,
      "learning_rate": 3.884705882352942e-05,
      "loss": 2.09,
      "step": 18960
    },
    {
      "epoch": 1115.8823529411766,
      "grad_norm": 11.00842571258545,
      "learning_rate": 3.8841176470588234e-05,
      "loss": 2.1127,
      "step": 18970
    },
    {
      "epoch": 1116.4705882352941,
      "grad_norm": 9.152032852172852,
      "learning_rate": 3.8835294117647057e-05,
      "loss": 1.9092,
      "step": 18980
    },
    {
      "epoch": 1117.0588235294117,
      "grad_norm": 8.389714241027832,
      "learning_rate": 3.8829411764705886e-05,
      "loss": 2.0123,
      "step": 18990
    },
    {
      "epoch": 1117.6470588235295,
      "grad_norm": 10.133063316345215,
      "learning_rate": 3.882352941176471e-05,
      "loss": 1.994,
      "step": 19000
    },
    {
      "epoch": 1118.235294117647,
      "grad_norm": 10.12423038482666,
      "learning_rate": 3.881764705882353e-05,
      "loss": 2.1685,
      "step": 19010
    },
    {
      "epoch": 1118.8235294117646,
      "grad_norm": 10.867494583129883,
      "learning_rate": 3.8811764705882355e-05,
      "loss": 2.0899,
      "step": 19020
    },
    {
      "epoch": 1119.4117647058824,
      "grad_norm": 9.555085182189941,
      "learning_rate": 3.880588235294118e-05,
      "loss": 2.0931,
      "step": 19030
    },
    {
      "epoch": 1120.0,
      "grad_norm": 12.811092376708984,
      "learning_rate": 3.88e-05,
      "loss": 2.1224,
      "step": 19040
    },
    {
      "epoch": 1120.5882352941176,
      "grad_norm": 10.086151123046875,
      "learning_rate": 3.8794117647058824e-05,
      "loss": 2.1336,
      "step": 19050
    },
    {
      "epoch": 1121.1764705882354,
      "grad_norm": 10.523910522460938,
      "learning_rate": 3.878823529411765e-05,
      "loss": 2.0563,
      "step": 19060
    },
    {
      "epoch": 1121.764705882353,
      "grad_norm": 11.314129829406738,
      "learning_rate": 3.878235294117648e-05,
      "loss": 2.0221,
      "step": 19070
    },
    {
      "epoch": 1122.3529411764705,
      "grad_norm": 12.36819839477539,
      "learning_rate": 3.87764705882353e-05,
      "loss": 2.1022,
      "step": 19080
    },
    {
      "epoch": 1122.9411764705883,
      "grad_norm": 11.648728370666504,
      "learning_rate": 3.877058823529412e-05,
      "loss": 2.0939,
      "step": 19090
    },
    {
      "epoch": 1123.5294117647059,
      "grad_norm": 8.790047645568848,
      "learning_rate": 3.876470588235294e-05,
      "loss": 1.9851,
      "step": 19100
    },
    {
      "epoch": 1124.1176470588234,
      "grad_norm": 13.007240295410156,
      "learning_rate": 3.875882352941176e-05,
      "loss": 2.043,
      "step": 19110
    },
    {
      "epoch": 1124.7058823529412,
      "grad_norm": 11.150315284729004,
      "learning_rate": 3.875294117647059e-05,
      "loss": 2.1368,
      "step": 19120
    },
    {
      "epoch": 1125.2941176470588,
      "grad_norm": 14.8306303024292,
      "learning_rate": 3.8747058823529415e-05,
      "loss": 2.1196,
      "step": 19130
    },
    {
      "epoch": 1125.8823529411766,
      "grad_norm": 9.591663360595703,
      "learning_rate": 3.874117647058824e-05,
      "loss": 2.1501,
      "step": 19140
    },
    {
      "epoch": 1126.4705882352941,
      "grad_norm": 11.145343780517578,
      "learning_rate": 3.873529411764706e-05,
      "loss": 2.1171,
      "step": 19150
    },
    {
      "epoch": 1127.0588235294117,
      "grad_norm": 11.63860034942627,
      "learning_rate": 3.8729411764705884e-05,
      "loss": 2.1896,
      "step": 19160
    },
    {
      "epoch": 1127.6470588235295,
      "grad_norm": 11.416142463684082,
      "learning_rate": 3.872352941176471e-05,
      "loss": 2.0752,
      "step": 19170
    },
    {
      "epoch": 1128.235294117647,
      "grad_norm": 13.223581314086914,
      "learning_rate": 3.871764705882353e-05,
      "loss": 2.0728,
      "step": 19180
    },
    {
      "epoch": 1128.8235294117646,
      "grad_norm": 10.803489685058594,
      "learning_rate": 3.871176470588235e-05,
      "loss": 2.0199,
      "step": 19190
    },
    {
      "epoch": 1129.4117647058824,
      "grad_norm": 14.136168479919434,
      "learning_rate": 3.870588235294118e-05,
      "loss": 2.0219,
      "step": 19200
    },
    {
      "epoch": 1130.0,
      "grad_norm": 15.9584379196167,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 2.0822,
      "step": 19210
    },
    {
      "epoch": 1130.5882352941176,
      "grad_norm": 11.378002166748047,
      "learning_rate": 3.869411764705882e-05,
      "loss": 2.0868,
      "step": 19220
    },
    {
      "epoch": 1131.1764705882354,
      "grad_norm": 11.331208229064941,
      "learning_rate": 3.8688235294117645e-05,
      "loss": 1.9932,
      "step": 19230
    },
    {
      "epoch": 1131.764705882353,
      "grad_norm": 11.48776626586914,
      "learning_rate": 3.8682352941176475e-05,
      "loss": 2.012,
      "step": 19240
    },
    {
      "epoch": 1132.3529411764705,
      "grad_norm": 18.23564910888672,
      "learning_rate": 3.86764705882353e-05,
      "loss": 2.1106,
      "step": 19250
    },
    {
      "epoch": 1132.9411764705883,
      "grad_norm": 11.581395149230957,
      "learning_rate": 3.867058823529412e-05,
      "loss": 2.0947,
      "step": 19260
    },
    {
      "epoch": 1133.5294117647059,
      "grad_norm": 14.612661361694336,
      "learning_rate": 3.8664705882352943e-05,
      "loss": 2.107,
      "step": 19270
    },
    {
      "epoch": 1134.1176470588234,
      "grad_norm": 11.078609466552734,
      "learning_rate": 3.8658823529411766e-05,
      "loss": 2.0141,
      "step": 19280
    },
    {
      "epoch": 1134.7058823529412,
      "grad_norm": 19.237791061401367,
      "learning_rate": 3.865294117647059e-05,
      "loss": 2.2928,
      "step": 19290
    },
    {
      "epoch": 1135.2941176470588,
      "grad_norm": 10.442700386047363,
      "learning_rate": 3.864705882352941e-05,
      "loss": 2.1695,
      "step": 19300
    },
    {
      "epoch": 1135.8823529411766,
      "grad_norm": 13.633621215820312,
      "learning_rate": 3.8641176470588235e-05,
      "loss": 2.0906,
      "step": 19310
    },
    {
      "epoch": 1136.4705882352941,
      "grad_norm": 14.389969825744629,
      "learning_rate": 3.863529411764706e-05,
      "loss": 2.1067,
      "step": 19320
    },
    {
      "epoch": 1137.0588235294117,
      "grad_norm": 9.85632610321045,
      "learning_rate": 3.862941176470589e-05,
      "loss": 2.0107,
      "step": 19330
    },
    {
      "epoch": 1137.6470588235295,
      "grad_norm": 9.763245582580566,
      "learning_rate": 3.862352941176471e-05,
      "loss": 2.1301,
      "step": 19340
    },
    {
      "epoch": 1138.235294117647,
      "grad_norm": 12.497770309448242,
      "learning_rate": 3.861764705882353e-05,
      "loss": 2.1585,
      "step": 19350
    },
    {
      "epoch": 1138.8235294117646,
      "grad_norm": 10.640183448791504,
      "learning_rate": 3.861176470588235e-05,
      "loss": 2.0713,
      "step": 19360
    },
    {
      "epoch": 1139.4117647058824,
      "grad_norm": 10.139582633972168,
      "learning_rate": 3.860588235294118e-05,
      "loss": 2.1153,
      "step": 19370
    },
    {
      "epoch": 1140.0,
      "grad_norm": 13.024953842163086,
      "learning_rate": 3.86e-05,
      "loss": 2.0505,
      "step": 19380
    },
    {
      "epoch": 1140.5882352941176,
      "grad_norm": 11.0262451171875,
      "learning_rate": 3.8594117647058826e-05,
      "loss": 1.9726,
      "step": 19390
    },
    {
      "epoch": 1141.1764705882354,
      "grad_norm": 13.559713363647461,
      "learning_rate": 3.858823529411765e-05,
      "loss": 2.0406,
      "step": 19400
    },
    {
      "epoch": 1141.764705882353,
      "grad_norm": 12.402663230895996,
      "learning_rate": 3.858235294117647e-05,
      "loss": 2.0369,
      "step": 19410
    },
    {
      "epoch": 1142.3529411764705,
      "grad_norm": 13.801654815673828,
      "learning_rate": 3.8576470588235295e-05,
      "loss": 1.9796,
      "step": 19420
    },
    {
      "epoch": 1142.9411764705883,
      "grad_norm": 12.093019485473633,
      "learning_rate": 3.857058823529412e-05,
      "loss": 1.9791,
      "step": 19430
    },
    {
      "epoch": 1143.5294117647059,
      "grad_norm": 10.017271995544434,
      "learning_rate": 3.856470588235294e-05,
      "loss": 2.1375,
      "step": 19440
    },
    {
      "epoch": 1144.1176470588234,
      "grad_norm": 10.888717651367188,
      "learning_rate": 3.8558823529411764e-05,
      "loss": 2.0404,
      "step": 19450
    },
    {
      "epoch": 1144.7058823529412,
      "grad_norm": 12.378255844116211,
      "learning_rate": 3.8552941176470594e-05,
      "loss": 2.0083,
      "step": 19460
    },
    {
      "epoch": 1145.2941176470588,
      "grad_norm": 10.644136428833008,
      "learning_rate": 3.854705882352942e-05,
      "loss": 2.0471,
      "step": 19470
    },
    {
      "epoch": 1145.8823529411766,
      "grad_norm": 10.76071834564209,
      "learning_rate": 3.854117647058823e-05,
      "loss": 2.1183,
      "step": 19480
    },
    {
      "epoch": 1146.4705882352941,
      "grad_norm": 11.524072647094727,
      "learning_rate": 3.8535294117647056e-05,
      "loss": 2.1549,
      "step": 19490
    },
    {
      "epoch": 1147.0588235294117,
      "grad_norm": 11.117770195007324,
      "learning_rate": 3.8529411764705886e-05,
      "loss": 2.0647,
      "step": 19500
    },
    {
      "epoch": 1147.6470588235295,
      "grad_norm": 12.238669395446777,
      "learning_rate": 3.852352941176471e-05,
      "loss": 2.0039,
      "step": 19510
    },
    {
      "epoch": 1148.235294117647,
      "grad_norm": 12.984454154968262,
      "learning_rate": 3.851764705882353e-05,
      "loss": 2.0921,
      "step": 19520
    },
    {
      "epoch": 1148.8235294117646,
      "grad_norm": 11.413782119750977,
      "learning_rate": 3.8511764705882355e-05,
      "loss": 2.05,
      "step": 19530
    },
    {
      "epoch": 1149.4117647058824,
      "grad_norm": 11.801657676696777,
      "learning_rate": 3.850588235294118e-05,
      "loss": 2.0996,
      "step": 19540
    },
    {
      "epoch": 1150.0,
      "grad_norm": 17.985445022583008,
      "learning_rate": 3.85e-05,
      "loss": 2.0866,
      "step": 19550
    },
    {
      "epoch": 1150.5882352941176,
      "grad_norm": 13.025552749633789,
      "learning_rate": 3.8494117647058824e-05,
      "loss": 2.1332,
      "step": 19560
    },
    {
      "epoch": 1151.1764705882354,
      "grad_norm": 11.800162315368652,
      "learning_rate": 3.848823529411765e-05,
      "loss": 2.1325,
      "step": 19570
    },
    {
      "epoch": 1151.764705882353,
      "grad_norm": 12.582123756408691,
      "learning_rate": 3.8482352941176476e-05,
      "loss": 2.077,
      "step": 19580
    },
    {
      "epoch": 1152.3529411764705,
      "grad_norm": 13.665536880493164,
      "learning_rate": 3.84764705882353e-05,
      "loss": 2.0154,
      "step": 19590
    },
    {
      "epoch": 1152.9411764705883,
      "grad_norm": 11.708953857421875,
      "learning_rate": 3.847058823529412e-05,
      "loss": 2.0776,
      "step": 19600
    },
    {
      "epoch": 1153.5294117647059,
      "grad_norm": 9.601398468017578,
      "learning_rate": 3.846470588235294e-05,
      "loss": 2.086,
      "step": 19610
    },
    {
      "epoch": 1154.1176470588234,
      "grad_norm": 10.954194068908691,
      "learning_rate": 3.845882352941176e-05,
      "loss": 2.0292,
      "step": 19620
    },
    {
      "epoch": 1154.7058823529412,
      "grad_norm": 12.280049324035645,
      "learning_rate": 3.845294117647059e-05,
      "loss": 1.9899,
      "step": 19630
    },
    {
      "epoch": 1155.2941176470588,
      "grad_norm": 12.342705726623535,
      "learning_rate": 3.8447058823529414e-05,
      "loss": 2.0343,
      "step": 19640
    },
    {
      "epoch": 1155.8823529411766,
      "grad_norm": 12.857637405395508,
      "learning_rate": 3.844117647058824e-05,
      "loss": 2.111,
      "step": 19650
    },
    {
      "epoch": 1156.4705882352941,
      "grad_norm": 13.39129638671875,
      "learning_rate": 3.843529411764706e-05,
      "loss": 1.8989,
      "step": 19660
    },
    {
      "epoch": 1157.0588235294117,
      "grad_norm": 12.56831169128418,
      "learning_rate": 3.842941176470588e-05,
      "loss": 2.103,
      "step": 19670
    },
    {
      "epoch": 1157.6470588235295,
      "grad_norm": 11.897034645080566,
      "learning_rate": 3.8423529411764706e-05,
      "loss": 2.1145,
      "step": 19680
    },
    {
      "epoch": 1158.235294117647,
      "grad_norm": 11.036184310913086,
      "learning_rate": 3.841764705882353e-05,
      "loss": 2.2141,
      "step": 19690
    },
    {
      "epoch": 1158.8235294117646,
      "grad_norm": 11.763725280761719,
      "learning_rate": 3.841176470588235e-05,
      "loss": 2.0591,
      "step": 19700
    },
    {
      "epoch": 1159.4117647058824,
      "grad_norm": 12.16253662109375,
      "learning_rate": 3.840588235294118e-05,
      "loss": 2.104,
      "step": 19710
    },
    {
      "epoch": 1160.0,
      "grad_norm": 12.825389862060547,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 1.9651,
      "step": 19720
    },
    {
      "epoch": 1160.5882352941176,
      "grad_norm": 13.326976776123047,
      "learning_rate": 3.839411764705883e-05,
      "loss": 2.0172,
      "step": 19730
    },
    {
      "epoch": 1161.1764705882354,
      "grad_norm": 12.431203842163086,
      "learning_rate": 3.8388235294117644e-05,
      "loss": 2.1103,
      "step": 19740
    },
    {
      "epoch": 1161.764705882353,
      "grad_norm": 10.699277877807617,
      "learning_rate": 3.8382352941176474e-05,
      "loss": 2.0119,
      "step": 19750
    },
    {
      "epoch": 1162.3529411764705,
      "grad_norm": 10.836564064025879,
      "learning_rate": 3.83764705882353e-05,
      "loss": 2.0242,
      "step": 19760
    },
    {
      "epoch": 1162.9411764705883,
      "grad_norm": 10.190244674682617,
      "learning_rate": 3.837058823529412e-05,
      "loss": 2.0138,
      "step": 19770
    },
    {
      "epoch": 1163.5294117647059,
      "grad_norm": 18.07331657409668,
      "learning_rate": 3.836470588235294e-05,
      "loss": 2.0324,
      "step": 19780
    },
    {
      "epoch": 1164.1176470588234,
      "grad_norm": 16.50125503540039,
      "learning_rate": 3.8358823529411766e-05,
      "loss": 1.8738,
      "step": 19790
    },
    {
      "epoch": 1164.7058823529412,
      "grad_norm": 15.128410339355469,
      "learning_rate": 3.835294117647059e-05,
      "loss": 2.1337,
      "step": 19800
    },
    {
      "epoch": 1165.2941176470588,
      "grad_norm": 9.683548927307129,
      "learning_rate": 3.834705882352941e-05,
      "loss": 2.0412,
      "step": 19810
    },
    {
      "epoch": 1165.8823529411766,
      "grad_norm": 11.72012710571289,
      "learning_rate": 3.8341176470588235e-05,
      "loss": 2.0791,
      "step": 19820
    },
    {
      "epoch": 1166.4705882352941,
      "grad_norm": 12.464881896972656,
      "learning_rate": 3.833529411764706e-05,
      "loss": 2.061,
      "step": 19830
    },
    {
      "epoch": 1167.0588235294117,
      "grad_norm": 10.753523826599121,
      "learning_rate": 3.832941176470589e-05,
      "loss": 2.155,
      "step": 19840
    },
    {
      "epoch": 1167.6470588235295,
      "grad_norm": 14.067197799682617,
      "learning_rate": 3.832352941176471e-05,
      "loss": 2.1104,
      "step": 19850
    },
    {
      "epoch": 1168.235294117647,
      "grad_norm": 10.679353713989258,
      "learning_rate": 3.8317647058823534e-05,
      "loss": 2.0429,
      "step": 19860
    },
    {
      "epoch": 1168.8235294117646,
      "grad_norm": 11.455422401428223,
      "learning_rate": 3.831176470588235e-05,
      "loss": 2.0417,
      "step": 19870
    },
    {
      "epoch": 1169.4117647058824,
      "grad_norm": 14.648298263549805,
      "learning_rate": 3.830588235294118e-05,
      "loss": 2.0056,
      "step": 19880
    },
    {
      "epoch": 1170.0,
      "grad_norm": 12.537230491638184,
      "learning_rate": 3.83e-05,
      "loss": 2.1327,
      "step": 19890
    },
    {
      "epoch": 1170.5882352941176,
      "grad_norm": 13.872367858886719,
      "learning_rate": 3.8294117647058826e-05,
      "loss": 2.1835,
      "step": 19900
    },
    {
      "epoch": 1171.1764705882354,
      "grad_norm": 10.256505966186523,
      "learning_rate": 3.828823529411765e-05,
      "loss": 2.0206,
      "step": 19910
    },
    {
      "epoch": 1171.764705882353,
      "grad_norm": 14.099343299865723,
      "learning_rate": 3.828235294117647e-05,
      "loss": 2.0613,
      "step": 19920
    },
    {
      "epoch": 1172.3529411764705,
      "grad_norm": 12.982564926147461,
      "learning_rate": 3.8276470588235295e-05,
      "loss": 2.0093,
      "step": 19930
    },
    {
      "epoch": 1172.9411764705883,
      "grad_norm": 10.242298126220703,
      "learning_rate": 3.827058823529412e-05,
      "loss": 1.9479,
      "step": 19940
    },
    {
      "epoch": 1173.5294117647059,
      "grad_norm": 12.73963451385498,
      "learning_rate": 3.826470588235294e-05,
      "loss": 2.0181,
      "step": 19950
    },
    {
      "epoch": 1174.1176470588234,
      "grad_norm": 10.95582389831543,
      "learning_rate": 3.825882352941177e-05,
      "loss": 2.0772,
      "step": 19960
    },
    {
      "epoch": 1174.7058823529412,
      "grad_norm": 12.10942268371582,
      "learning_rate": 3.825294117647059e-05,
      "loss": 2.0962,
      "step": 19970
    },
    {
      "epoch": 1175.2941176470588,
      "grad_norm": 9.887836456298828,
      "learning_rate": 3.8247058823529416e-05,
      "loss": 1.873,
      "step": 19980
    },
    {
      "epoch": 1175.8823529411766,
      "grad_norm": 13.02220630645752,
      "learning_rate": 3.824117647058823e-05,
      "loss": 2.078,
      "step": 19990
    },
    {
      "epoch": 1176.4705882352941,
      "grad_norm": 11.804945945739746,
      "learning_rate": 3.8235294117647055e-05,
      "loss": 2.0218,
      "step": 20000
    },
    {
      "epoch": 1177.0588235294117,
      "grad_norm": 13.436394691467285,
      "learning_rate": 3.8229411764705885e-05,
      "loss": 2.1175,
      "step": 20010
    },
    {
      "epoch": 1177.6470588235295,
      "grad_norm": 11.609514236450195,
      "learning_rate": 3.822352941176471e-05,
      "loss": 2.1387,
      "step": 20020
    },
    {
      "epoch": 1178.235294117647,
      "grad_norm": 9.741278648376465,
      "learning_rate": 3.821764705882353e-05,
      "loss": 2.0133,
      "step": 20030
    },
    {
      "epoch": 1178.8235294117646,
      "grad_norm": 16.416105270385742,
      "learning_rate": 3.8211764705882354e-05,
      "loss": 2.059,
      "step": 20040
    },
    {
      "epoch": 1179.4117647058824,
      "grad_norm": 15.7221097946167,
      "learning_rate": 3.820588235294118e-05,
      "loss": 1.9884,
      "step": 20050
    },
    {
      "epoch": 1180.0,
      "grad_norm": 13.44285774230957,
      "learning_rate": 3.82e-05,
      "loss": 2.0994,
      "step": 20060
    },
    {
      "epoch": 1180.5882352941176,
      "grad_norm": 15.89470100402832,
      "learning_rate": 3.819411764705882e-05,
      "loss": 2.1616,
      "step": 20070
    },
    {
      "epoch": 1181.1764705882354,
      "grad_norm": 9.592641830444336,
      "learning_rate": 3.8188235294117646e-05,
      "loss": 2.0627,
      "step": 20080
    },
    {
      "epoch": 1181.764705882353,
      "grad_norm": 11.438831329345703,
      "learning_rate": 3.8182352941176476e-05,
      "loss": 1.8917,
      "step": 20090
    },
    {
      "epoch": 1182.3529411764705,
      "grad_norm": 11.157989501953125,
      "learning_rate": 3.81764705882353e-05,
      "loss": 2.0946,
      "step": 20100
    },
    {
      "epoch": 1182.9411764705883,
      "grad_norm": 12.63386058807373,
      "learning_rate": 3.817058823529412e-05,
      "loss": 2.0596,
      "step": 20110
    },
    {
      "epoch": 1183.5294117647059,
      "grad_norm": 15.223869323730469,
      "learning_rate": 3.816470588235294e-05,
      "loss": 2.1438,
      "step": 20120
    },
    {
      "epoch": 1184.1176470588234,
      "grad_norm": 13.105884552001953,
      "learning_rate": 3.815882352941177e-05,
      "loss": 2.1014,
      "step": 20130
    },
    {
      "epoch": 1184.7058823529412,
      "grad_norm": 10.72351360321045,
      "learning_rate": 3.815294117647059e-05,
      "loss": 1.9254,
      "step": 20140
    },
    {
      "epoch": 1185.2941176470588,
      "grad_norm": 15.275362014770508,
      "learning_rate": 3.8147058823529414e-05,
      "loss": 1.9895,
      "step": 20150
    },
    {
      "epoch": 1185.8823529411766,
      "grad_norm": 10.04564094543457,
      "learning_rate": 3.814117647058824e-05,
      "loss": 2.0805,
      "step": 20160
    },
    {
      "epoch": 1186.4705882352941,
      "grad_norm": 14.272266387939453,
      "learning_rate": 3.813529411764706e-05,
      "loss": 2.0636,
      "step": 20170
    },
    {
      "epoch": 1187.0588235294117,
      "grad_norm": 10.660472869873047,
      "learning_rate": 3.812941176470588e-05,
      "loss": 2.1383,
      "step": 20180
    },
    {
      "epoch": 1187.6470588235295,
      "grad_norm": 10.696858406066895,
      "learning_rate": 3.8123529411764706e-05,
      "loss": 2.0214,
      "step": 20190
    },
    {
      "epoch": 1188.235294117647,
      "grad_norm": 12.48668384552002,
      "learning_rate": 3.811764705882353e-05,
      "loss": 2.1071,
      "step": 20200
    },
    {
      "epoch": 1188.8235294117646,
      "grad_norm": 12.303376197814941,
      "learning_rate": 3.811176470588235e-05,
      "loss": 2.0656,
      "step": 20210
    },
    {
      "epoch": 1189.4117647058824,
      "grad_norm": 11.049729347229004,
      "learning_rate": 3.810588235294118e-05,
      "loss": 2.0393,
      "step": 20220
    },
    {
      "epoch": 1190.0,
      "grad_norm": 13.852720260620117,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 1.8825,
      "step": 20230
    },
    {
      "epoch": 1190.5882352941176,
      "grad_norm": 10.691366195678711,
      "learning_rate": 3.809411764705883e-05,
      "loss": 1.9439,
      "step": 20240
    },
    {
      "epoch": 1191.1764705882354,
      "grad_norm": 9.340723991394043,
      "learning_rate": 3.8088235294117644e-05,
      "loss": 2.0289,
      "step": 20250
    },
    {
      "epoch": 1191.764705882353,
      "grad_norm": 16.306217193603516,
      "learning_rate": 3.8082352941176473e-05,
      "loss": 2.0953,
      "step": 20260
    },
    {
      "epoch": 1192.3529411764705,
      "grad_norm": 10.234135627746582,
      "learning_rate": 3.8076470588235296e-05,
      "loss": 2.0039,
      "step": 20270
    },
    {
      "epoch": 1192.9411764705883,
      "grad_norm": 12.902140617370605,
      "learning_rate": 3.807058823529412e-05,
      "loss": 2.1228,
      "step": 20280
    },
    {
      "epoch": 1193.5294117647059,
      "grad_norm": 10.896245956420898,
      "learning_rate": 3.806470588235294e-05,
      "loss": 1.9776,
      "step": 20290
    },
    {
      "epoch": 1194.1176470588234,
      "grad_norm": 13.084528923034668,
      "learning_rate": 3.805882352941177e-05,
      "loss": 1.9671,
      "step": 20300
    },
    {
      "epoch": 1194.7058823529412,
      "grad_norm": 10.961814880371094,
      "learning_rate": 3.805294117647059e-05,
      "loss": 2.0546,
      "step": 20310
    },
    {
      "epoch": 1195.2941176470588,
      "grad_norm": 14.559464454650879,
      "learning_rate": 3.804705882352941e-05,
      "loss": 2.0205,
      "step": 20320
    },
    {
      "epoch": 1195.8823529411766,
      "grad_norm": 11.139994621276855,
      "learning_rate": 3.8041176470588234e-05,
      "loss": 2.0111,
      "step": 20330
    },
    {
      "epoch": 1196.4705882352941,
      "grad_norm": 13.335579872131348,
      "learning_rate": 3.8035294117647064e-05,
      "loss": 2.069,
      "step": 20340
    },
    {
      "epoch": 1197.0588235294117,
      "grad_norm": 16.133420944213867,
      "learning_rate": 3.802941176470589e-05,
      "loss": 2.0235,
      "step": 20350
    },
    {
      "epoch": 1197.6470588235295,
      "grad_norm": 13.6235990524292,
      "learning_rate": 3.802352941176471e-05,
      "loss": 1.8927,
      "step": 20360
    },
    {
      "epoch": 1198.235294117647,
      "grad_norm": 12.926706314086914,
      "learning_rate": 3.801764705882353e-05,
      "loss": 1.9083,
      "step": 20370
    },
    {
      "epoch": 1198.8235294117646,
      "grad_norm": 9.751651763916016,
      "learning_rate": 3.801176470588235e-05,
      "loss": 1.8645,
      "step": 20380
    },
    {
      "epoch": 1199.4117647058824,
      "grad_norm": 16.228618621826172,
      "learning_rate": 3.800588235294118e-05,
      "loss": 2.0531,
      "step": 20390
    },
    {
      "epoch": 1200.0,
      "grad_norm": 11.762250900268555,
      "learning_rate": 3.8e-05,
      "loss": 1.9926,
      "step": 20400
    },
    {
      "epoch": 1200.5882352941176,
      "grad_norm": 13.855447769165039,
      "learning_rate": 3.7994117647058825e-05,
      "loss": 2.0161,
      "step": 20410
    },
    {
      "epoch": 1201.1764705882354,
      "grad_norm": 10.328700065612793,
      "learning_rate": 3.798823529411765e-05,
      "loss": 2.0652,
      "step": 20420
    },
    {
      "epoch": 1201.764705882353,
      "grad_norm": 12.897801399230957,
      "learning_rate": 3.798235294117648e-05,
      "loss": 1.9172,
      "step": 20430
    },
    {
      "epoch": 1202.3529411764705,
      "grad_norm": 11.11495304107666,
      "learning_rate": 3.7976470588235294e-05,
      "loss": 2.04,
      "step": 20440
    },
    {
      "epoch": 1202.9411764705883,
      "grad_norm": 10.378695487976074,
      "learning_rate": 3.797058823529412e-05,
      "loss": 2.0075,
      "step": 20450
    },
    {
      "epoch": 1203.5294117647059,
      "grad_norm": 13.25484848022461,
      "learning_rate": 3.796470588235294e-05,
      "loss": 1.9985,
      "step": 20460
    },
    {
      "epoch": 1204.1176470588234,
      "grad_norm": 12.007392883300781,
      "learning_rate": 3.795882352941177e-05,
      "loss": 1.9947,
      "step": 20470
    },
    {
      "epoch": 1204.7058823529412,
      "grad_norm": 10.904163360595703,
      "learning_rate": 3.795294117647059e-05,
      "loss": 1.9918,
      "step": 20480
    },
    {
      "epoch": 1205.2941176470588,
      "grad_norm": 9.820558547973633,
      "learning_rate": 3.7947058823529416e-05,
      "loss": 2.2095,
      "step": 20490
    },
    {
      "epoch": 1205.8823529411766,
      "grad_norm": 10.69511604309082,
      "learning_rate": 3.794117647058824e-05,
      "loss": 2.1231,
      "step": 20500
    },
    {
      "epoch": 1206.4705882352941,
      "grad_norm": 12.059810638427734,
      "learning_rate": 3.793529411764706e-05,
      "loss": 2.092,
      "step": 20510
    },
    {
      "epoch": 1207.0588235294117,
      "grad_norm": 10.01011848449707,
      "learning_rate": 3.7929411764705885e-05,
      "loss": 2.0788,
      "step": 20520
    },
    {
      "epoch": 1207.6470588235295,
      "grad_norm": 18.280412673950195,
      "learning_rate": 3.792352941176471e-05,
      "loss": 2.0123,
      "step": 20530
    },
    {
      "epoch": 1208.235294117647,
      "grad_norm": 13.282716751098633,
      "learning_rate": 3.791764705882353e-05,
      "loss": 1.8846,
      "step": 20540
    },
    {
      "epoch": 1208.8235294117646,
      "grad_norm": 11.636641502380371,
      "learning_rate": 3.7911764705882354e-05,
      "loss": 1.9463,
      "step": 20550
    },
    {
      "epoch": 1209.4117647058824,
      "grad_norm": 11.969502449035645,
      "learning_rate": 3.790588235294118e-05,
      "loss": 1.9894,
      "step": 20560
    },
    {
      "epoch": 1210.0,
      "grad_norm": 14.040963172912598,
      "learning_rate": 3.79e-05,
      "loss": 2.0663,
      "step": 20570
    },
    {
      "epoch": 1210.5882352941176,
      "grad_norm": 9.661882400512695,
      "learning_rate": 3.789411764705882e-05,
      "loss": 1.945,
      "step": 20580
    },
    {
      "epoch": 1211.1764705882354,
      "grad_norm": 12.266439437866211,
      "learning_rate": 3.7888235294117646e-05,
      "loss": 2.0154,
      "step": 20590
    },
    {
      "epoch": 1211.764705882353,
      "grad_norm": 9.906458854675293,
      "learning_rate": 3.7882352941176475e-05,
      "loss": 2.0143,
      "step": 20600
    },
    {
      "epoch": 1212.3529411764705,
      "grad_norm": 13.29624080657959,
      "learning_rate": 3.78764705882353e-05,
      "loss": 2.0572,
      "step": 20610
    },
    {
      "epoch": 1212.9411764705883,
      "grad_norm": 14.029153823852539,
      "learning_rate": 3.787058823529412e-05,
      "loss": 2.0323,
      "step": 20620
    },
    {
      "epoch": 1213.5294117647059,
      "grad_norm": 15.09172248840332,
      "learning_rate": 3.7864705882352944e-05,
      "loss": 2.0117,
      "step": 20630
    },
    {
      "epoch": 1214.1176470588234,
      "grad_norm": 11.420415878295898,
      "learning_rate": 3.785882352941177e-05,
      "loss": 1.9029,
      "step": 20640
    },
    {
      "epoch": 1214.7058823529412,
      "grad_norm": 13.17779541015625,
      "learning_rate": 3.785294117647059e-05,
      "loss": 2.0571,
      "step": 20650
    },
    {
      "epoch": 1215.2941176470588,
      "grad_norm": 12.520426750183105,
      "learning_rate": 3.784705882352941e-05,
      "loss": 2.1201,
      "step": 20660
    },
    {
      "epoch": 1215.8823529411766,
      "grad_norm": 13.09422779083252,
      "learning_rate": 3.7841176470588236e-05,
      "loss": 2.007,
      "step": 20670
    },
    {
      "epoch": 1216.4705882352941,
      "grad_norm": 12.690826416015625,
      "learning_rate": 3.7835294117647066e-05,
      "loss": 1.9352,
      "step": 20680
    },
    {
      "epoch": 1217.0588235294117,
      "grad_norm": 17.249406814575195,
      "learning_rate": 3.782941176470588e-05,
      "loss": 2.1101,
      "step": 20690
    },
    {
      "epoch": 1217.6470588235295,
      "grad_norm": 11.577337265014648,
      "learning_rate": 3.7823529411764705e-05,
      "loss": 1.906,
      "step": 20700
    },
    {
      "epoch": 1218.235294117647,
      "grad_norm": 14.50784683227539,
      "learning_rate": 3.781764705882353e-05,
      "loss": 1.953,
      "step": 20710
    },
    {
      "epoch": 1218.8235294117646,
      "grad_norm": 11.371694564819336,
      "learning_rate": 3.781176470588235e-05,
      "loss": 1.9613,
      "step": 20720
    },
    {
      "epoch": 1219.4117647058824,
      "grad_norm": 11.356917381286621,
      "learning_rate": 3.780588235294118e-05,
      "loss": 1.927,
      "step": 20730
    },
    {
      "epoch": 1220.0,
      "grad_norm": 15.461541175842285,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 1.9863,
      "step": 20740
    },
    {
      "epoch": 1220.5882352941176,
      "grad_norm": 16.289588928222656,
      "learning_rate": 3.779411764705883e-05,
      "loss": 2.0365,
      "step": 20750
    },
    {
      "epoch": 1221.1764705882354,
      "grad_norm": 10.842876434326172,
      "learning_rate": 3.778823529411764e-05,
      "loss": 1.9633,
      "step": 20760
    },
    {
      "epoch": 1221.764705882353,
      "grad_norm": 14.026691436767578,
      "learning_rate": 3.778235294117647e-05,
      "loss": 2.0368,
      "step": 20770
    },
    {
      "epoch": 1222.3529411764705,
      "grad_norm": 11.006020545959473,
      "learning_rate": 3.7776470588235296e-05,
      "loss": 1.9904,
      "step": 20780
    },
    {
      "epoch": 1222.9411764705883,
      "grad_norm": 17.129383087158203,
      "learning_rate": 3.777058823529412e-05,
      "loss": 1.9592,
      "step": 20790
    },
    {
      "epoch": 1223.5294117647059,
      "grad_norm": 11.110986709594727,
      "learning_rate": 3.776470588235294e-05,
      "loss": 1.8993,
      "step": 20800
    },
    {
      "epoch": 1224.1176470588234,
      "grad_norm": 12.494154930114746,
      "learning_rate": 3.775882352941177e-05,
      "loss": 1.9326,
      "step": 20810
    },
    {
      "epoch": 1224.7058823529412,
      "grad_norm": 13.309327125549316,
      "learning_rate": 3.775294117647059e-05,
      "loss": 1.8991,
      "step": 20820
    },
    {
      "epoch": 1225.2941176470588,
      "grad_norm": 12.089374542236328,
      "learning_rate": 3.774705882352941e-05,
      "loss": 2.1604,
      "step": 20830
    },
    {
      "epoch": 1225.8823529411766,
      "grad_norm": 16.445419311523438,
      "learning_rate": 3.7741176470588234e-05,
      "loss": 2.0486,
      "step": 20840
    },
    {
      "epoch": 1226.4705882352941,
      "grad_norm": 12.0797758102417,
      "learning_rate": 3.7735294117647064e-05,
      "loss": 1.9518,
      "step": 20850
    },
    {
      "epoch": 1227.0588235294117,
      "grad_norm": 11.971589088439941,
      "learning_rate": 3.772941176470589e-05,
      "loss": 2.0829,
      "step": 20860
    },
    {
      "epoch": 1227.6470588235295,
      "grad_norm": 12.541440963745117,
      "learning_rate": 3.772352941176471e-05,
      "loss": 2.1017,
      "step": 20870
    },
    {
      "epoch": 1228.235294117647,
      "grad_norm": 11.509324073791504,
      "learning_rate": 3.771764705882353e-05,
      "loss": 2.0704,
      "step": 20880
    },
    {
      "epoch": 1228.8235294117646,
      "grad_norm": 12.2058687210083,
      "learning_rate": 3.771176470588235e-05,
      "loss": 1.9693,
      "step": 20890
    },
    {
      "epoch": 1229.4117647058824,
      "grad_norm": 14.0297269821167,
      "learning_rate": 3.770588235294118e-05,
      "loss": 1.9948,
      "step": 20900
    },
    {
      "epoch": 1230.0,
      "grad_norm": 10.411468505859375,
      "learning_rate": 3.77e-05,
      "loss": 2.0133,
      "step": 20910
    },
    {
      "epoch": 1230.5882352941176,
      "grad_norm": 11.10869312286377,
      "learning_rate": 3.7694117647058825e-05,
      "loss": 1.9781,
      "step": 20920
    },
    {
      "epoch": 1231.1764705882354,
      "grad_norm": 13.114480972290039,
      "learning_rate": 3.768823529411765e-05,
      "loss": 2.0156,
      "step": 20930
    },
    {
      "epoch": 1231.764705882353,
      "grad_norm": 15.269270896911621,
      "learning_rate": 3.768235294117648e-05,
      "loss": 1.9734,
      "step": 20940
    },
    {
      "epoch": 1232.3529411764705,
      "grad_norm": 12.236560821533203,
      "learning_rate": 3.7676470588235294e-05,
      "loss": 1.9806,
      "step": 20950
    },
    {
      "epoch": 1232.9411764705883,
      "grad_norm": 11.879426002502441,
      "learning_rate": 3.7670588235294117e-05,
      "loss": 1.9415,
      "step": 20960
    },
    {
      "epoch": 1233.5294117647059,
      "grad_norm": 12.861737251281738,
      "learning_rate": 3.766470588235294e-05,
      "loss": 2.1214,
      "step": 20970
    },
    {
      "epoch": 1234.1176470588234,
      "grad_norm": 10.965615272521973,
      "learning_rate": 3.765882352941177e-05,
      "loss": 2.0458,
      "step": 20980
    },
    {
      "epoch": 1234.7058823529412,
      "grad_norm": 11.872862815856934,
      "learning_rate": 3.765294117647059e-05,
      "loss": 1.9914,
      "step": 20990
    },
    {
      "epoch": 1235.2941176470588,
      "grad_norm": 10.604524612426758,
      "learning_rate": 3.7647058823529415e-05,
      "loss": 1.8267,
      "step": 21000
    },
    {
      "epoch": 1235.8823529411766,
      "grad_norm": 14.506662368774414,
      "learning_rate": 3.764117647058824e-05,
      "loss": 1.9939,
      "step": 21010
    },
    {
      "epoch": 1236.4705882352941,
      "grad_norm": 12.155707359313965,
      "learning_rate": 3.763529411764706e-05,
      "loss": 2.0021,
      "step": 21020
    },
    {
      "epoch": 1237.0588235294117,
      "grad_norm": 11.723618507385254,
      "learning_rate": 3.7629411764705884e-05,
      "loss": 1.9865,
      "step": 21030
    },
    {
      "epoch": 1237.6470588235295,
      "grad_norm": 11.737823486328125,
      "learning_rate": 3.762352941176471e-05,
      "loss": 1.8568,
      "step": 21040
    },
    {
      "epoch": 1238.235294117647,
      "grad_norm": 12.187386512756348,
      "learning_rate": 3.761764705882353e-05,
      "loss": 2.0799,
      "step": 21050
    },
    {
      "epoch": 1238.8235294117646,
      "grad_norm": 12.091934204101562,
      "learning_rate": 3.761176470588236e-05,
      "loss": 2.0797,
      "step": 21060
    },
    {
      "epoch": 1239.4117647058824,
      "grad_norm": 11.220479011535645,
      "learning_rate": 3.760588235294118e-05,
      "loss": 2.0464,
      "step": 21070
    },
    {
      "epoch": 1240.0,
      "grad_norm": 14.239174842834473,
      "learning_rate": 3.76e-05,
      "loss": 1.8632,
      "step": 21080
    },
    {
      "epoch": 1240.5882352941176,
      "grad_norm": 12.350018501281738,
      "learning_rate": 3.759411764705882e-05,
      "loss": 2.0659,
      "step": 21090
    },
    {
      "epoch": 1241.1764705882354,
      "grad_norm": 12.156198501586914,
      "learning_rate": 3.7588235294117645e-05,
      "loss": 2.0947,
      "step": 21100
    },
    {
      "epoch": 1241.764705882353,
      "grad_norm": 13.849339485168457,
      "learning_rate": 3.7582352941176475e-05,
      "loss": 2.0723,
      "step": 21110
    },
    {
      "epoch": 1242.3529411764705,
      "grad_norm": 11.148804664611816,
      "learning_rate": 3.75764705882353e-05,
      "loss": 1.9593,
      "step": 21120
    },
    {
      "epoch": 1242.9411764705883,
      "grad_norm": 11.828155517578125,
      "learning_rate": 3.757058823529412e-05,
      "loss": 2.1315,
      "step": 21130
    },
    {
      "epoch": 1243.5294117647059,
      "grad_norm": 19.361026763916016,
      "learning_rate": 3.7564705882352944e-05,
      "loss": 1.9482,
      "step": 21140
    },
    {
      "epoch": 1244.1176470588234,
      "grad_norm": 15.645003318786621,
      "learning_rate": 3.755882352941177e-05,
      "loss": 1.8993,
      "step": 21150
    },
    {
      "epoch": 1244.7058823529412,
      "grad_norm": 15.425376892089844,
      "learning_rate": 3.755294117647059e-05,
      "loss": 1.9686,
      "step": 21160
    },
    {
      "epoch": 1245.2941176470588,
      "grad_norm": 11.962958335876465,
      "learning_rate": 3.754705882352941e-05,
      "loss": 1.9918,
      "step": 21170
    },
    {
      "epoch": 1245.8823529411766,
      "grad_norm": 13.710702896118164,
      "learning_rate": 3.7541176470588236e-05,
      "loss": 1.83,
      "step": 21180
    },
    {
      "epoch": 1246.4705882352941,
      "grad_norm": 11.883143424987793,
      "learning_rate": 3.7535294117647066e-05,
      "loss": 2.0259,
      "step": 21190
    },
    {
      "epoch": 1247.0588235294117,
      "grad_norm": 15.743953704833984,
      "learning_rate": 3.752941176470588e-05,
      "loss": 2.0269,
      "step": 21200
    },
    {
      "epoch": 1247.6470588235295,
      "grad_norm": 10.256950378417969,
      "learning_rate": 3.7523529411764705e-05,
      "loss": 1.9727,
      "step": 21210
    },
    {
      "epoch": 1248.235294117647,
      "grad_norm": 14.171487808227539,
      "learning_rate": 3.751764705882353e-05,
      "loss": 1.8378,
      "step": 21220
    },
    {
      "epoch": 1248.8235294117646,
      "grad_norm": 11.534546852111816,
      "learning_rate": 3.751176470588236e-05,
      "loss": 1.9988,
      "step": 21230
    },
    {
      "epoch": 1249.4117647058824,
      "grad_norm": 12.187092781066895,
      "learning_rate": 3.750588235294118e-05,
      "loss": 1.8692,
      "step": 21240
    },
    {
      "epoch": 1250.0,
      "grad_norm": 15.975415229797363,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.1292,
      "step": 21250
    },
    {
      "epoch": 1250.5882352941176,
      "grad_norm": 11.599163055419922,
      "learning_rate": 3.7494117647058826e-05,
      "loss": 2.0775,
      "step": 21260
    },
    {
      "epoch": 1251.1764705882354,
      "grad_norm": 14.022554397583008,
      "learning_rate": 3.748823529411765e-05,
      "loss": 1.9332,
      "step": 21270
    },
    {
      "epoch": 1251.764705882353,
      "grad_norm": 11.254043579101562,
      "learning_rate": 3.748235294117647e-05,
      "loss": 1.9748,
      "step": 21280
    },
    {
      "epoch": 1252.3529411764705,
      "grad_norm": 14.523905754089355,
      "learning_rate": 3.7476470588235295e-05,
      "loss": 1.9761,
      "step": 21290
    },
    {
      "epoch": 1252.9411764705883,
      "grad_norm": 15.539510726928711,
      "learning_rate": 3.747058823529412e-05,
      "loss": 1.8872,
      "step": 21300
    },
    {
      "epoch": 1253.5294117647059,
      "grad_norm": 13.156778335571289,
      "learning_rate": 3.746470588235294e-05,
      "loss": 1.8963,
      "step": 21310
    },
    {
      "epoch": 1254.1176470588234,
      "grad_norm": 14.027810096740723,
      "learning_rate": 3.745882352941177e-05,
      "loss": 2.0218,
      "step": 21320
    },
    {
      "epoch": 1254.7058823529412,
      "grad_norm": 12.6126708984375,
      "learning_rate": 3.745294117647059e-05,
      "loss": 2.1192,
      "step": 21330
    },
    {
      "epoch": 1255.2941176470588,
      "grad_norm": 11.242515563964844,
      "learning_rate": 3.744705882352941e-05,
      "loss": 1.8707,
      "step": 21340
    },
    {
      "epoch": 1255.8823529411766,
      "grad_norm": 11.506847381591797,
      "learning_rate": 3.744117647058823e-05,
      "loss": 2.0659,
      "step": 21350
    },
    {
      "epoch": 1256.4705882352941,
      "grad_norm": 13.219560623168945,
      "learning_rate": 3.743529411764706e-05,
      "loss": 2.1696,
      "step": 21360
    },
    {
      "epoch": 1257.0588235294117,
      "grad_norm": 12.809113502502441,
      "learning_rate": 3.7429411764705886e-05,
      "loss": 1.9432,
      "step": 21370
    },
    {
      "epoch": 1257.6470588235295,
      "grad_norm": 13.429837226867676,
      "learning_rate": 3.742352941176471e-05,
      "loss": 2.0442,
      "step": 21380
    },
    {
      "epoch": 1258.235294117647,
      "grad_norm": 13.51071548461914,
      "learning_rate": 3.741764705882353e-05,
      "loss": 1.9111,
      "step": 21390
    },
    {
      "epoch": 1258.8235294117646,
      "grad_norm": 11.085253715515137,
      "learning_rate": 3.7411764705882355e-05,
      "loss": 1.9964,
      "step": 21400
    },
    {
      "epoch": 1259.4117647058824,
      "grad_norm": 14.364269256591797,
      "learning_rate": 3.740588235294118e-05,
      "loss": 1.8161,
      "step": 21410
    },
    {
      "epoch": 1260.0,
      "grad_norm": 11.390108108520508,
      "learning_rate": 3.74e-05,
      "loss": 1.9924,
      "step": 21420
    },
    {
      "epoch": 1260.5882352941176,
      "grad_norm": 12.640719413757324,
      "learning_rate": 3.7394117647058824e-05,
      "loss": 1.9425,
      "step": 21430
    },
    {
      "epoch": 1261.1764705882354,
      "grad_norm": 13.367015838623047,
      "learning_rate": 3.738823529411765e-05,
      "loss": 1.8915,
      "step": 21440
    },
    {
      "epoch": 1261.764705882353,
      "grad_norm": 12.747657775878906,
      "learning_rate": 3.738235294117648e-05,
      "loss": 1.9915,
      "step": 21450
    },
    {
      "epoch": 1262.3529411764705,
      "grad_norm": 14.13775634765625,
      "learning_rate": 3.737647058823529e-05,
      "loss": 2.0281,
      "step": 21460
    },
    {
      "epoch": 1262.9411764705883,
      "grad_norm": 12.118759155273438,
      "learning_rate": 3.7370588235294116e-05,
      "loss": 1.9551,
      "step": 21470
    },
    {
      "epoch": 1263.5294117647059,
      "grad_norm": 13.904050827026367,
      "learning_rate": 3.736470588235294e-05,
      "loss": 2.0085,
      "step": 21480
    },
    {
      "epoch": 1264.1176470588234,
      "grad_norm": 17.102659225463867,
      "learning_rate": 3.735882352941177e-05,
      "loss": 2.0518,
      "step": 21490
    },
    {
      "epoch": 1264.7058823529412,
      "grad_norm": 12.817965507507324,
      "learning_rate": 3.735294117647059e-05,
      "loss": 2.0938,
      "step": 21500
    },
    {
      "epoch": 1265.2941176470588,
      "grad_norm": 12.666047096252441,
      "learning_rate": 3.7347058823529415e-05,
      "loss": 1.9899,
      "step": 21510
    },
    {
      "epoch": 1265.8823529411766,
      "grad_norm": 12.306702613830566,
      "learning_rate": 3.734117647058824e-05,
      "loss": 1.9092,
      "step": 21520
    },
    {
      "epoch": 1266.4705882352941,
      "grad_norm": 12.262748718261719,
      "learning_rate": 3.733529411764706e-05,
      "loss": 1.9922,
      "step": 21530
    },
    {
      "epoch": 1267.0588235294117,
      "grad_norm": 12.056458473205566,
      "learning_rate": 3.7329411764705884e-05,
      "loss": 1.9409,
      "step": 21540
    },
    {
      "epoch": 1267.6470588235295,
      "grad_norm": 9.689667701721191,
      "learning_rate": 3.732352941176471e-05,
      "loss": 1.9639,
      "step": 21550
    },
    {
      "epoch": 1268.235294117647,
      "grad_norm": 18.924104690551758,
      "learning_rate": 3.731764705882353e-05,
      "loss": 2.0063,
      "step": 21560
    },
    {
      "epoch": 1268.8235294117646,
      "grad_norm": 12.508395195007324,
      "learning_rate": 3.731176470588236e-05,
      "loss": 1.9544,
      "step": 21570
    },
    {
      "epoch": 1269.4117647058824,
      "grad_norm": 14.162755012512207,
      "learning_rate": 3.730588235294118e-05,
      "loss": 2.0125,
      "step": 21580
    },
    {
      "epoch": 1270.0,
      "grad_norm": 20.486215591430664,
      "learning_rate": 3.73e-05,
      "loss": 2.1627,
      "step": 21590
    },
    {
      "epoch": 1270.5882352941176,
      "grad_norm": 10.658561706542969,
      "learning_rate": 3.729411764705882e-05,
      "loss": 1.8136,
      "step": 21600
    },
    {
      "epoch": 1271.1764705882354,
      "grad_norm": 11.120485305786133,
      "learning_rate": 3.728823529411765e-05,
      "loss": 2.0255,
      "step": 21610
    },
    {
      "epoch": 1271.764705882353,
      "grad_norm": 15.593971252441406,
      "learning_rate": 3.7282352941176474e-05,
      "loss": 1.9289,
      "step": 21620
    },
    {
      "epoch": 1272.3529411764705,
      "grad_norm": 12.222176551818848,
      "learning_rate": 3.72764705882353e-05,
      "loss": 1.9007,
      "step": 21630
    },
    {
      "epoch": 1272.9411764705883,
      "grad_norm": 12.719053268432617,
      "learning_rate": 3.727058823529412e-05,
      "loss": 1.9495,
      "step": 21640
    },
    {
      "epoch": 1273.5294117647059,
      "grad_norm": 11.224931716918945,
      "learning_rate": 3.726470588235294e-05,
      "loss": 2.012,
      "step": 21650
    },
    {
      "epoch": 1274.1176470588234,
      "grad_norm": 15.272863388061523,
      "learning_rate": 3.7258823529411766e-05,
      "loss": 1.9557,
      "step": 21660
    },
    {
      "epoch": 1274.7058823529412,
      "grad_norm": 11.68804931640625,
      "learning_rate": 3.725294117647059e-05,
      "loss": 1.9188,
      "step": 21670
    },
    {
      "epoch": 1275.2941176470588,
      "grad_norm": 11.94296932220459,
      "learning_rate": 3.724705882352941e-05,
      "loss": 1.9689,
      "step": 21680
    },
    {
      "epoch": 1275.8823529411766,
      "grad_norm": 14.048630714416504,
      "learning_rate": 3.7241176470588235e-05,
      "loss": 1.9828,
      "step": 21690
    },
    {
      "epoch": 1276.4705882352941,
      "grad_norm": 11.648157119750977,
      "learning_rate": 3.7235294117647065e-05,
      "loss": 1.9063,
      "step": 21700
    },
    {
      "epoch": 1277.0588235294117,
      "grad_norm": 12.1068115234375,
      "learning_rate": 3.722941176470589e-05,
      "loss": 2.0211,
      "step": 21710
    },
    {
      "epoch": 1277.6470588235295,
      "grad_norm": 13.649590492248535,
      "learning_rate": 3.7223529411764704e-05,
      "loss": 1.9407,
      "step": 21720
    },
    {
      "epoch": 1278.235294117647,
      "grad_norm": 12.8589448928833,
      "learning_rate": 3.721764705882353e-05,
      "loss": 1.9793,
      "step": 21730
    },
    {
      "epoch": 1278.8235294117646,
      "grad_norm": 12.951889038085938,
      "learning_rate": 3.721176470588236e-05,
      "loss": 1.8818,
      "step": 21740
    },
    {
      "epoch": 1279.4117647058824,
      "grad_norm": 11.842442512512207,
      "learning_rate": 3.720588235294118e-05,
      "loss": 1.9278,
      "step": 21750
    },
    {
      "epoch": 1280.0,
      "grad_norm": 13.726991653442383,
      "learning_rate": 3.72e-05,
      "loss": 2.0112,
      "step": 21760
    },
    {
      "epoch": 1280.5882352941176,
      "grad_norm": 11.763693809509277,
      "learning_rate": 3.7194117647058826e-05,
      "loss": 1.868,
      "step": 21770
    },
    {
      "epoch": 1281.1764705882354,
      "grad_norm": 15.850907325744629,
      "learning_rate": 3.718823529411765e-05,
      "loss": 1.8432,
      "step": 21780
    },
    {
      "epoch": 1281.764705882353,
      "grad_norm": 10.777117729187012,
      "learning_rate": 3.718235294117647e-05,
      "loss": 2.0315,
      "step": 21790
    },
    {
      "epoch": 1282.3529411764705,
      "grad_norm": 12.304088592529297,
      "learning_rate": 3.7176470588235295e-05,
      "loss": 2.0135,
      "step": 21800
    },
    {
      "epoch": 1282.9411764705883,
      "grad_norm": 13.833399772644043,
      "learning_rate": 3.717058823529412e-05,
      "loss": 1.9048,
      "step": 21810
    },
    {
      "epoch": 1283.5294117647059,
      "grad_norm": 11.085948944091797,
      "learning_rate": 3.716470588235294e-05,
      "loss": 1.9789,
      "step": 21820
    },
    {
      "epoch": 1284.1176470588234,
      "grad_norm": 14.108857154846191,
      "learning_rate": 3.715882352941177e-05,
      "loss": 1.9172,
      "step": 21830
    },
    {
      "epoch": 1284.7058823529412,
      "grad_norm": 12.228686332702637,
      "learning_rate": 3.7152941176470594e-05,
      "loss": 1.9687,
      "step": 21840
    },
    {
      "epoch": 1285.2941176470588,
      "grad_norm": 12.861466407775879,
      "learning_rate": 3.714705882352941e-05,
      "loss": 1.9401,
      "step": 21850
    },
    {
      "epoch": 1285.8823529411766,
      "grad_norm": 12.011706352233887,
      "learning_rate": 3.714117647058823e-05,
      "loss": 2.03,
      "step": 21860
    },
    {
      "epoch": 1286.4705882352941,
      "grad_norm": 15.731895446777344,
      "learning_rate": 3.713529411764706e-05,
      "loss": 1.9844,
      "step": 21870
    },
    {
      "epoch": 1287.0588235294117,
      "grad_norm": 11.116586685180664,
      "learning_rate": 3.7129411764705886e-05,
      "loss": 1.9752,
      "step": 21880
    },
    {
      "epoch": 1287.6470588235295,
      "grad_norm": 13.7760648727417,
      "learning_rate": 3.712352941176471e-05,
      "loss": 1.96,
      "step": 21890
    },
    {
      "epoch": 1288.235294117647,
      "grad_norm": 16.24973487854004,
      "learning_rate": 3.711764705882353e-05,
      "loss": 1.9658,
      "step": 21900
    },
    {
      "epoch": 1288.8235294117646,
      "grad_norm": 12.755441665649414,
      "learning_rate": 3.7111764705882355e-05,
      "loss": 1.9019,
      "step": 21910
    },
    {
      "epoch": 1289.4117647058824,
      "grad_norm": 12.908038139343262,
      "learning_rate": 3.710588235294118e-05,
      "loss": 1.9192,
      "step": 21920
    },
    {
      "epoch": 1290.0,
      "grad_norm": 13.997720718383789,
      "learning_rate": 3.71e-05,
      "loss": 1.9844,
      "step": 21930
    },
    {
      "epoch": 1290.5882352941176,
      "grad_norm": 12.255452156066895,
      "learning_rate": 3.7094117647058824e-05,
      "loss": 1.8201,
      "step": 21940
    },
    {
      "epoch": 1291.1764705882354,
      "grad_norm": 14.507265090942383,
      "learning_rate": 3.708823529411765e-05,
      "loss": 1.861,
      "step": 21950
    },
    {
      "epoch": 1291.764705882353,
      "grad_norm": 14.984881401062012,
      "learning_rate": 3.7082352941176476e-05,
      "loss": 2.0094,
      "step": 21960
    },
    {
      "epoch": 1292.3529411764705,
      "grad_norm": 14.973692893981934,
      "learning_rate": 3.707647058823529e-05,
      "loss": 1.884,
      "step": 21970
    },
    {
      "epoch": 1292.9411764705883,
      "grad_norm": 16.2597599029541,
      "learning_rate": 3.7070588235294115e-05,
      "loss": 1.8469,
      "step": 21980
    },
    {
      "epoch": 1293.5294117647059,
      "grad_norm": 11.845943450927734,
      "learning_rate": 3.706470588235294e-05,
      "loss": 1.9746,
      "step": 21990
    },
    {
      "epoch": 1294.1176470588234,
      "grad_norm": 11.861799240112305,
      "learning_rate": 3.705882352941177e-05,
      "loss": 1.8667,
      "step": 22000
    },
    {
      "epoch": 1294.7058823529412,
      "grad_norm": 13.025107383728027,
      "learning_rate": 3.705294117647059e-05,
      "loss": 1.8708,
      "step": 22010
    },
    {
      "epoch": 1295.2941176470588,
      "grad_norm": 13.322088241577148,
      "learning_rate": 3.7047058823529414e-05,
      "loss": 1.9042,
      "step": 22020
    },
    {
      "epoch": 1295.8823529411766,
      "grad_norm": 13.667562484741211,
      "learning_rate": 3.704117647058824e-05,
      "loss": 1.9961,
      "step": 22030
    },
    {
      "epoch": 1296.4705882352941,
      "grad_norm": 17.590482711791992,
      "learning_rate": 3.703529411764706e-05,
      "loss": 1.9102,
      "step": 22040
    },
    {
      "epoch": 1297.0588235294117,
      "grad_norm": 12.20468807220459,
      "learning_rate": 3.702941176470588e-05,
      "loss": 1.8469,
      "step": 22050
    },
    {
      "epoch": 1297.6470588235295,
      "grad_norm": 13.840906143188477,
      "learning_rate": 3.7023529411764706e-05,
      "loss": 2.1822,
      "step": 22060
    },
    {
      "epoch": 1298.235294117647,
      "grad_norm": 13.350807189941406,
      "learning_rate": 3.701764705882353e-05,
      "loss": 1.8886,
      "step": 22070
    },
    {
      "epoch": 1298.8235294117646,
      "grad_norm": 13.123834609985352,
      "learning_rate": 3.701176470588236e-05,
      "loss": 2.0473,
      "step": 22080
    },
    {
      "epoch": 1299.4117647058824,
      "grad_norm": 15.030195236206055,
      "learning_rate": 3.700588235294118e-05,
      "loss": 1.9587,
      "step": 22090
    },
    {
      "epoch": 1300.0,
      "grad_norm": 14.255193710327148,
      "learning_rate": 3.7e-05,
      "loss": 1.8517,
      "step": 22100
    },
    {
      "epoch": 1300.5882352941176,
      "grad_norm": 11.657597541809082,
      "learning_rate": 3.699411764705882e-05,
      "loss": 1.9405,
      "step": 22110
    },
    {
      "epoch": 1301.1764705882354,
      "grad_norm": 13.970770835876465,
      "learning_rate": 3.698823529411765e-05,
      "loss": 1.9844,
      "step": 22120
    },
    {
      "epoch": 1301.764705882353,
      "grad_norm": 12.172163963317871,
      "learning_rate": 3.6982352941176474e-05,
      "loss": 1.9401,
      "step": 22130
    },
    {
      "epoch": 1302.3529411764705,
      "grad_norm": 13.753214836120605,
      "learning_rate": 3.69764705882353e-05,
      "loss": 1.8855,
      "step": 22140
    },
    {
      "epoch": 1302.9411764705883,
      "grad_norm": 14.145511627197266,
      "learning_rate": 3.697058823529412e-05,
      "loss": 1.9808,
      "step": 22150
    },
    {
      "epoch": 1303.5294117647059,
      "grad_norm": 11.032093048095703,
      "learning_rate": 3.696470588235294e-05,
      "loss": 1.9276,
      "step": 22160
    },
    {
      "epoch": 1304.1176470588234,
      "grad_norm": 16.721511840820312,
      "learning_rate": 3.6958823529411766e-05,
      "loss": 1.9962,
      "step": 22170
    },
    {
      "epoch": 1304.7058823529412,
      "grad_norm": 12.509610176086426,
      "learning_rate": 3.695294117647059e-05,
      "loss": 2.0705,
      "step": 22180
    },
    {
      "epoch": 1305.2941176470588,
      "grad_norm": 13.477540969848633,
      "learning_rate": 3.694705882352941e-05,
      "loss": 1.7771,
      "step": 22190
    },
    {
      "epoch": 1305.8823529411766,
      "grad_norm": 14.706533432006836,
      "learning_rate": 3.6941176470588235e-05,
      "loss": 1.9452,
      "step": 22200
    },
    {
      "epoch": 1306.4705882352941,
      "grad_norm": 10.785846710205078,
      "learning_rate": 3.6935294117647065e-05,
      "loss": 1.9697,
      "step": 22210
    },
    {
      "epoch": 1307.0588235294117,
      "grad_norm": 14.019386291503906,
      "learning_rate": 3.692941176470589e-05,
      "loss": 1.8539,
      "step": 22220
    },
    {
      "epoch": 1307.6470588235295,
      "grad_norm": 11.779374122619629,
      "learning_rate": 3.6923529411764704e-05,
      "loss": 2.059,
      "step": 22230
    },
    {
      "epoch": 1308.235294117647,
      "grad_norm": 13.908282279968262,
      "learning_rate": 3.691764705882353e-05,
      "loss": 1.8772,
      "step": 22240
    },
    {
      "epoch": 1308.8235294117646,
      "grad_norm": 11.926874160766602,
      "learning_rate": 3.6911764705882356e-05,
      "loss": 1.9659,
      "step": 22250
    },
    {
      "epoch": 1309.4117647058824,
      "grad_norm": 12.793990135192871,
      "learning_rate": 3.690588235294118e-05,
      "loss": 1.956,
      "step": 22260
    },
    {
      "epoch": 1310.0,
      "grad_norm": 11.390286445617676,
      "learning_rate": 3.69e-05,
      "loss": 1.9324,
      "step": 22270
    },
    {
      "epoch": 1310.5882352941176,
      "grad_norm": 13.570899963378906,
      "learning_rate": 3.6894117647058825e-05,
      "loss": 1.8063,
      "step": 22280
    },
    {
      "epoch": 1311.1764705882354,
      "grad_norm": 12.386542320251465,
      "learning_rate": 3.688823529411765e-05,
      "loss": 1.9378,
      "step": 22290
    },
    {
      "epoch": 1311.764705882353,
      "grad_norm": 11.651972770690918,
      "learning_rate": 3.688235294117647e-05,
      "loss": 1.8706,
      "step": 22300
    },
    {
      "epoch": 1312.3529411764705,
      "grad_norm": 14.963922500610352,
      "learning_rate": 3.6876470588235294e-05,
      "loss": 1.9219,
      "step": 22310
    },
    {
      "epoch": 1312.9411764705883,
      "grad_norm": 15.735331535339355,
      "learning_rate": 3.687058823529412e-05,
      "loss": 1.969,
      "step": 22320
    },
    {
      "epoch": 1313.5294117647059,
      "grad_norm": 14.406940460205078,
      "learning_rate": 3.686470588235295e-05,
      "loss": 1.9036,
      "step": 22330
    },
    {
      "epoch": 1314.1176470588234,
      "grad_norm": 15.119393348693848,
      "learning_rate": 3.685882352941177e-05,
      "loss": 2.0335,
      "step": 22340
    },
    {
      "epoch": 1314.7058823529412,
      "grad_norm": 11.392020225524902,
      "learning_rate": 3.685294117647059e-05,
      "loss": 1.9602,
      "step": 22350
    },
    {
      "epoch": 1315.2941176470588,
      "grad_norm": 13.697000503540039,
      "learning_rate": 3.684705882352941e-05,
      "loss": 1.9395,
      "step": 22360
    },
    {
      "epoch": 1315.8823529411766,
      "grad_norm": 13.828025817871094,
      "learning_rate": 3.684117647058823e-05,
      "loss": 1.9046,
      "step": 22370
    },
    {
      "epoch": 1316.4705882352941,
      "grad_norm": 13.01418685913086,
      "learning_rate": 3.683529411764706e-05,
      "loss": 1.9136,
      "step": 22380
    },
    {
      "epoch": 1317.0588235294117,
      "grad_norm": 16.137100219726562,
      "learning_rate": 3.6829411764705885e-05,
      "loss": 1.8399,
      "step": 22390
    },
    {
      "epoch": 1317.6470588235295,
      "grad_norm": 17.429174423217773,
      "learning_rate": 3.682352941176471e-05,
      "loss": 1.9119,
      "step": 22400
    },
    {
      "epoch": 1318.235294117647,
      "grad_norm": 10.737403869628906,
      "learning_rate": 3.681764705882353e-05,
      "loss": 1.9566,
      "step": 22410
    },
    {
      "epoch": 1318.8235294117646,
      "grad_norm": 13.19611930847168,
      "learning_rate": 3.6811764705882354e-05,
      "loss": 1.907,
      "step": 22420
    },
    {
      "epoch": 1319.4117647058824,
      "grad_norm": 14.357513427734375,
      "learning_rate": 3.680588235294118e-05,
      "loss": 1.9061,
      "step": 22430
    },
    {
      "epoch": 1320.0,
      "grad_norm": 17.381101608276367,
      "learning_rate": 3.68e-05,
      "loss": 1.8109,
      "step": 22440
    },
    {
      "epoch": 1320.5882352941176,
      "grad_norm": 10.33917236328125,
      "learning_rate": 3.679411764705882e-05,
      "loss": 1.8507,
      "step": 22450
    },
    {
      "epoch": 1321.1764705882354,
      "grad_norm": 11.312424659729004,
      "learning_rate": 3.678823529411765e-05,
      "loss": 1.8173,
      "step": 22460
    },
    {
      "epoch": 1321.764705882353,
      "grad_norm": 12.754473686218262,
      "learning_rate": 3.6782352941176476e-05,
      "loss": 1.8737,
      "step": 22470
    },
    {
      "epoch": 1322.3529411764705,
      "grad_norm": 13.327383041381836,
      "learning_rate": 3.67764705882353e-05,
      "loss": 1.9104,
      "step": 22480
    },
    {
      "epoch": 1322.9411764705883,
      "grad_norm": 14.628286361694336,
      "learning_rate": 3.6770588235294115e-05,
      "loss": 1.9499,
      "step": 22490
    },
    {
      "epoch": 1323.5294117647059,
      "grad_norm": 13.417313575744629,
      "learning_rate": 3.6764705882352945e-05,
      "loss": 2.0007,
      "step": 22500
    },
    {
      "epoch": 1324.1176470588234,
      "grad_norm": 12.92173957824707,
      "learning_rate": 3.675882352941177e-05,
      "loss": 1.9585,
      "step": 22510
    },
    {
      "epoch": 1324.7058823529412,
      "grad_norm": 14.819913864135742,
      "learning_rate": 3.675294117647059e-05,
      "loss": 1.9376,
      "step": 22520
    },
    {
      "epoch": 1325.2941176470588,
      "grad_norm": 12.180807113647461,
      "learning_rate": 3.6747058823529414e-05,
      "loss": 1.9183,
      "step": 22530
    },
    {
      "epoch": 1325.8823529411766,
      "grad_norm": 10.970020294189453,
      "learning_rate": 3.674117647058824e-05,
      "loss": 1.9876,
      "step": 22540
    },
    {
      "epoch": 1326.4705882352941,
      "grad_norm": 12.586061477661133,
      "learning_rate": 3.673529411764706e-05,
      "loss": 1.9585,
      "step": 22550
    },
    {
      "epoch": 1327.0588235294117,
      "grad_norm": 15.565783500671387,
      "learning_rate": 3.672941176470588e-05,
      "loss": 1.8556,
      "step": 22560
    },
    {
      "epoch": 1327.6470588235295,
      "grad_norm": 11.036567687988281,
      "learning_rate": 3.6723529411764706e-05,
      "loss": 1.7949,
      "step": 22570
    },
    {
      "epoch": 1328.235294117647,
      "grad_norm": 13.929518699645996,
      "learning_rate": 3.671764705882353e-05,
      "loss": 1.8547,
      "step": 22580
    },
    {
      "epoch": 1328.8235294117646,
      "grad_norm": 17.347457885742188,
      "learning_rate": 3.671176470588236e-05,
      "loss": 1.8103,
      "step": 22590
    },
    {
      "epoch": 1329.4117647058824,
      "grad_norm": 13.425527572631836,
      "learning_rate": 3.670588235294118e-05,
      "loss": 1.9986,
      "step": 22600
    },
    {
      "epoch": 1330.0,
      "grad_norm": 16.096450805664062,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 1.8501,
      "step": 22610
    },
    {
      "epoch": 1330.5882352941176,
      "grad_norm": 13.744213104248047,
      "learning_rate": 3.669411764705882e-05,
      "loss": 1.7586,
      "step": 22620
    },
    {
      "epoch": 1331.1764705882354,
      "grad_norm": 15.678457260131836,
      "learning_rate": 3.668823529411765e-05,
      "loss": 1.8426,
      "step": 22630
    },
    {
      "epoch": 1331.764705882353,
      "grad_norm": 15.027702331542969,
      "learning_rate": 3.668235294117647e-05,
      "loss": 1.7409,
      "step": 22640
    },
    {
      "epoch": 1332.3529411764705,
      "grad_norm": 10.52092456817627,
      "learning_rate": 3.6676470588235296e-05,
      "loss": 1.922,
      "step": 22650
    },
    {
      "epoch": 1332.9411764705883,
      "grad_norm": 14.702021598815918,
      "learning_rate": 3.667058823529412e-05,
      "loss": 1.8696,
      "step": 22660
    },
    {
      "epoch": 1333.5294117647059,
      "grad_norm": 14.268960952758789,
      "learning_rate": 3.666470588235294e-05,
      "loss": 1.8276,
      "step": 22670
    },
    {
      "epoch": 1334.1176470588234,
      "grad_norm": 14.561713218688965,
      "learning_rate": 3.6658823529411765e-05,
      "loss": 1.882,
      "step": 22680
    },
    {
      "epoch": 1334.7058823529412,
      "grad_norm": 10.942076683044434,
      "learning_rate": 3.665294117647059e-05,
      "loss": 1.8631,
      "step": 22690
    },
    {
      "epoch": 1335.2941176470588,
      "grad_norm": 14.029401779174805,
      "learning_rate": 3.664705882352941e-05,
      "loss": 1.9398,
      "step": 22700
    },
    {
      "epoch": 1335.8823529411766,
      "grad_norm": 9.66747760772705,
      "learning_rate": 3.6641176470588234e-05,
      "loss": 2.0228,
      "step": 22710
    },
    {
      "epoch": 1336.4705882352941,
      "grad_norm": 13.04299545288086,
      "learning_rate": 3.6635294117647064e-05,
      "loss": 1.7956,
      "step": 22720
    },
    {
      "epoch": 1337.0588235294117,
      "grad_norm": 13.333685874938965,
      "learning_rate": 3.662941176470589e-05,
      "loss": 1.8579,
      "step": 22730
    },
    {
      "epoch": 1337.6470588235295,
      "grad_norm": 14.291633605957031,
      "learning_rate": 3.66235294117647e-05,
      "loss": 1.9452,
      "step": 22740
    },
    {
      "epoch": 1338.235294117647,
      "grad_norm": 15.854719161987305,
      "learning_rate": 3.6617647058823526e-05,
      "loss": 1.9197,
      "step": 22750
    },
    {
      "epoch": 1338.8235294117646,
      "grad_norm": 13.913763046264648,
      "learning_rate": 3.6611764705882356e-05,
      "loss": 1.8582,
      "step": 22760
    },
    {
      "epoch": 1339.4117647058824,
      "grad_norm": 13.153414726257324,
      "learning_rate": 3.660588235294118e-05,
      "loss": 1.9057,
      "step": 22770
    },
    {
      "epoch": 1340.0,
      "grad_norm": 18.056894302368164,
      "learning_rate": 3.66e-05,
      "loss": 1.8793,
      "step": 22780
    },
    {
      "epoch": 1340.5882352941176,
      "grad_norm": 17.478609085083008,
      "learning_rate": 3.6594117647058825e-05,
      "loss": 2.1425,
      "step": 22790
    },
    {
      "epoch": 1341.1764705882354,
      "grad_norm": 12.40874195098877,
      "learning_rate": 3.658823529411765e-05,
      "loss": 1.9395,
      "step": 22800
    },
    {
      "epoch": 1341.764705882353,
      "grad_norm": 15.470632553100586,
      "learning_rate": 3.658235294117647e-05,
      "loss": 1.9107,
      "step": 22810
    },
    {
      "epoch": 1342.3529411764705,
      "grad_norm": 16.56340217590332,
      "learning_rate": 3.6576470588235294e-05,
      "loss": 1.6731,
      "step": 22820
    },
    {
      "epoch": 1342.9411764705883,
      "grad_norm": 11.213215827941895,
      "learning_rate": 3.657058823529412e-05,
      "loss": 1.8628,
      "step": 22830
    },
    {
      "epoch": 1343.5294117647059,
      "grad_norm": 15.963366508483887,
      "learning_rate": 3.656470588235295e-05,
      "loss": 1.9154,
      "step": 22840
    },
    {
      "epoch": 1344.1176470588234,
      "grad_norm": 14.58694076538086,
      "learning_rate": 3.655882352941177e-05,
      "loss": 1.9024,
      "step": 22850
    },
    {
      "epoch": 1344.7058823529412,
      "grad_norm": 10.134659767150879,
      "learning_rate": 3.655294117647059e-05,
      "loss": 1.9133,
      "step": 22860
    },
    {
      "epoch": 1345.2941176470588,
      "grad_norm": 17.845449447631836,
      "learning_rate": 3.654705882352941e-05,
      "loss": 1.9516,
      "step": 22870
    },
    {
      "epoch": 1345.8823529411766,
      "grad_norm": 15.507174491882324,
      "learning_rate": 3.654117647058824e-05,
      "loss": 1.9269,
      "step": 22880
    },
    {
      "epoch": 1346.4705882352941,
      "grad_norm": 13.240230560302734,
      "learning_rate": 3.653529411764706e-05,
      "loss": 1.9086,
      "step": 22890
    },
    {
      "epoch": 1347.0588235294117,
      "grad_norm": 14.201759338378906,
      "learning_rate": 3.6529411764705885e-05,
      "loss": 1.828,
      "step": 22900
    },
    {
      "epoch": 1347.6470588235295,
      "grad_norm": 10.888190269470215,
      "learning_rate": 3.652352941176471e-05,
      "loss": 1.9415,
      "step": 22910
    },
    {
      "epoch": 1348.235294117647,
      "grad_norm": 10.711539268493652,
      "learning_rate": 3.651764705882353e-05,
      "loss": 1.929,
      "step": 22920
    },
    {
      "epoch": 1348.8235294117646,
      "grad_norm": 9.62005615234375,
      "learning_rate": 3.6511764705882354e-05,
      "loss": 1.9063,
      "step": 22930
    },
    {
      "epoch": 1349.4117647058824,
      "grad_norm": 10.361289978027344,
      "learning_rate": 3.6505882352941177e-05,
      "loss": 1.7946,
      "step": 22940
    },
    {
      "epoch": 1350.0,
      "grad_norm": 17.801509857177734,
      "learning_rate": 3.65e-05,
      "loss": 1.9334,
      "step": 22950
    },
    {
      "epoch": 1350.5882352941176,
      "grad_norm": 13.71734619140625,
      "learning_rate": 3.649411764705882e-05,
      "loss": 1.8693,
      "step": 22960
    },
    {
      "epoch": 1351.1764705882354,
      "grad_norm": 13.82253360748291,
      "learning_rate": 3.648823529411765e-05,
      "loss": 2.0499,
      "step": 22970
    },
    {
      "epoch": 1351.764705882353,
      "grad_norm": 11.843464851379395,
      "learning_rate": 3.6482352941176475e-05,
      "loss": 1.7765,
      "step": 22980
    },
    {
      "epoch": 1352.3529411764705,
      "grad_norm": 14.847420692443848,
      "learning_rate": 3.64764705882353e-05,
      "loss": 2.0256,
      "step": 22990
    },
    {
      "epoch": 1352.9411764705883,
      "grad_norm": 14.104802131652832,
      "learning_rate": 3.6470588235294114e-05,
      "loss": 1.8477,
      "step": 23000
    },
    {
      "epoch": 1353.5294117647059,
      "grad_norm": 16.74824333190918,
      "learning_rate": 3.6464705882352944e-05,
      "loss": 1.7783,
      "step": 23010
    },
    {
      "epoch": 1354.1176470588234,
      "grad_norm": 14.119367599487305,
      "learning_rate": 3.645882352941177e-05,
      "loss": 1.9842,
      "step": 23020
    },
    {
      "epoch": 1354.7058823529412,
      "grad_norm": 12.60112190246582,
      "learning_rate": 3.645294117647059e-05,
      "loss": 1.7354,
      "step": 23030
    },
    {
      "epoch": 1355.2941176470588,
      "grad_norm": 19.264236450195312,
      "learning_rate": 3.644705882352941e-05,
      "loss": 1.9479,
      "step": 23040
    },
    {
      "epoch": 1355.8823529411766,
      "grad_norm": 15.256561279296875,
      "learning_rate": 3.644117647058824e-05,
      "loss": 1.9447,
      "step": 23050
    },
    {
      "epoch": 1356.4705882352941,
      "grad_norm": 14.103941917419434,
      "learning_rate": 3.643529411764706e-05,
      "loss": 1.8605,
      "step": 23060
    },
    {
      "epoch": 1357.0588235294117,
      "grad_norm": 14.917346000671387,
      "learning_rate": 3.642941176470588e-05,
      "loss": 1.8437,
      "step": 23070
    },
    {
      "epoch": 1357.6470588235295,
      "grad_norm": 12.262855529785156,
      "learning_rate": 3.6423529411764705e-05,
      "loss": 1.8756,
      "step": 23080
    },
    {
      "epoch": 1358.235294117647,
      "grad_norm": 9.21945858001709,
      "learning_rate": 3.641764705882353e-05,
      "loss": 1.7762,
      "step": 23090
    },
    {
      "epoch": 1358.8235294117646,
      "grad_norm": 11.91504192352295,
      "learning_rate": 3.641176470588236e-05,
      "loss": 1.9326,
      "step": 23100
    },
    {
      "epoch": 1359.4117647058824,
      "grad_norm": 11.25436019897461,
      "learning_rate": 3.640588235294118e-05,
      "loss": 1.949,
      "step": 23110
    },
    {
      "epoch": 1360.0,
      "grad_norm": 12.695619583129883,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 1.8492,
      "step": 23120
    },
    {
      "epoch": 1360.5882352941176,
      "grad_norm": 13.104275703430176,
      "learning_rate": 3.639411764705882e-05,
      "loss": 1.8078,
      "step": 23130
    },
    {
      "epoch": 1361.1764705882354,
      "grad_norm": 14.746880531311035,
      "learning_rate": 3.638823529411765e-05,
      "loss": 1.876,
      "step": 23140
    },
    {
      "epoch": 1361.764705882353,
      "grad_norm": 12.259244918823242,
      "learning_rate": 3.638235294117647e-05,
      "loss": 1.7801,
      "step": 23150
    },
    {
      "epoch": 1362.3529411764705,
      "grad_norm": 10.086821556091309,
      "learning_rate": 3.6376470588235296e-05,
      "loss": 1.8799,
      "step": 23160
    },
    {
      "epoch": 1362.9411764705883,
      "grad_norm": 16.05948829650879,
      "learning_rate": 3.637058823529412e-05,
      "loss": 1.8592,
      "step": 23170
    },
    {
      "epoch": 1363.5294117647059,
      "grad_norm": 10.771324157714844,
      "learning_rate": 3.636470588235295e-05,
      "loss": 1.9617,
      "step": 23180
    },
    {
      "epoch": 1364.1176470588234,
      "grad_norm": 17.52728843688965,
      "learning_rate": 3.6358823529411765e-05,
      "loss": 1.9563,
      "step": 23190
    },
    {
      "epoch": 1364.7058823529412,
      "grad_norm": 11.779769897460938,
      "learning_rate": 3.635294117647059e-05,
      "loss": 1.9548,
      "step": 23200
    },
    {
      "epoch": 1365.2941176470588,
      "grad_norm": 11.779977798461914,
      "learning_rate": 3.634705882352941e-05,
      "loss": 1.8314,
      "step": 23210
    },
    {
      "epoch": 1365.8823529411766,
      "grad_norm": 11.696149826049805,
      "learning_rate": 3.634117647058824e-05,
      "loss": 1.7659,
      "step": 23220
    },
    {
      "epoch": 1366.4705882352941,
      "grad_norm": 12.029607772827148,
      "learning_rate": 3.6335294117647064e-05,
      "loss": 2.027,
      "step": 23230
    },
    {
      "epoch": 1367.0588235294117,
      "grad_norm": 11.045605659484863,
      "learning_rate": 3.6329411764705886e-05,
      "loss": 1.8351,
      "step": 23240
    },
    {
      "epoch": 1367.6470588235295,
      "grad_norm": 11.557561874389648,
      "learning_rate": 3.632352941176471e-05,
      "loss": 1.8107,
      "step": 23250
    },
    {
      "epoch": 1368.235294117647,
      "grad_norm": 14.167051315307617,
      "learning_rate": 3.6317647058823526e-05,
      "loss": 1.8309,
      "step": 23260
    },
    {
      "epoch": 1368.8235294117646,
      "grad_norm": 11.914138793945312,
      "learning_rate": 3.6311764705882355e-05,
      "loss": 1.8202,
      "step": 23270
    },
    {
      "epoch": 1369.4117647058824,
      "grad_norm": 12.921414375305176,
      "learning_rate": 3.630588235294118e-05,
      "loss": 1.821,
      "step": 23280
    },
    {
      "epoch": 1370.0,
      "grad_norm": 15.020438194274902,
      "learning_rate": 3.63e-05,
      "loss": 1.9266,
      "step": 23290
    },
    {
      "epoch": 1370.5882352941176,
      "grad_norm": 17.14405059814453,
      "learning_rate": 3.6294117647058824e-05,
      "loss": 1.8885,
      "step": 23300
    },
    {
      "epoch": 1371.1764705882354,
      "grad_norm": 14.704730987548828,
      "learning_rate": 3.628823529411765e-05,
      "loss": 1.9745,
      "step": 23310
    },
    {
      "epoch": 1371.764705882353,
      "grad_norm": 14.180624008178711,
      "learning_rate": 3.628235294117647e-05,
      "loss": 1.8838,
      "step": 23320
    },
    {
      "epoch": 1372.3529411764705,
      "grad_norm": 12.868789672851562,
      "learning_rate": 3.627647058823529e-05,
      "loss": 1.8388,
      "step": 23330
    },
    {
      "epoch": 1372.9411764705883,
      "grad_norm": 14.53938102722168,
      "learning_rate": 3.6270588235294116e-05,
      "loss": 1.9103,
      "step": 23340
    },
    {
      "epoch": 1373.5294117647059,
      "grad_norm": 13.249490737915039,
      "learning_rate": 3.6264705882352946e-05,
      "loss": 1.7994,
      "step": 23350
    },
    {
      "epoch": 1374.1176470588234,
      "grad_norm": 12.060986518859863,
      "learning_rate": 3.625882352941177e-05,
      "loss": 1.8535,
      "step": 23360
    },
    {
      "epoch": 1374.7058823529412,
      "grad_norm": 17.349336624145508,
      "learning_rate": 3.625294117647059e-05,
      "loss": 1.8926,
      "step": 23370
    },
    {
      "epoch": 1375.2941176470588,
      "grad_norm": 13.76191520690918,
      "learning_rate": 3.6247058823529415e-05,
      "loss": 1.7705,
      "step": 23380
    },
    {
      "epoch": 1375.8823529411766,
      "grad_norm": 12.552698135375977,
      "learning_rate": 3.624117647058824e-05,
      "loss": 1.9529,
      "step": 23390
    },
    {
      "epoch": 1376.4705882352941,
      "grad_norm": 12.21097469329834,
      "learning_rate": 3.623529411764706e-05,
      "loss": 1.99,
      "step": 23400
    },
    {
      "epoch": 1377.0588235294117,
      "grad_norm": 13.802898406982422,
      "learning_rate": 3.6229411764705884e-05,
      "loss": 1.8623,
      "step": 23410
    },
    {
      "epoch": 1377.6470588235295,
      "grad_norm": 12.718835830688477,
      "learning_rate": 3.622352941176471e-05,
      "loss": 1.8844,
      "step": 23420
    },
    {
      "epoch": 1378.235294117647,
      "grad_norm": 12.039000511169434,
      "learning_rate": 3.621764705882353e-05,
      "loss": 1.7594,
      "step": 23430
    },
    {
      "epoch": 1378.8235294117646,
      "grad_norm": 13.085856437683105,
      "learning_rate": 3.621176470588235e-05,
      "loss": 1.9718,
      "step": 23440
    },
    {
      "epoch": 1379.4117647058824,
      "grad_norm": 13.729118347167969,
      "learning_rate": 3.6205882352941176e-05,
      "loss": 1.9659,
      "step": 23450
    },
    {
      "epoch": 1380.0,
      "grad_norm": 16.199644088745117,
      "learning_rate": 3.62e-05,
      "loss": 1.8559,
      "step": 23460
    },
    {
      "epoch": 1380.5882352941176,
      "grad_norm": 17.72271156311035,
      "learning_rate": 3.619411764705882e-05,
      "loss": 1.8249,
      "step": 23470
    },
    {
      "epoch": 1381.1764705882354,
      "grad_norm": 15.051597595214844,
      "learning_rate": 3.618823529411765e-05,
      "loss": 1.9394,
      "step": 23480
    },
    {
      "epoch": 1381.764705882353,
      "grad_norm": 12.480610847473145,
      "learning_rate": 3.6182352941176475e-05,
      "loss": 1.8688,
      "step": 23490
    },
    {
      "epoch": 1382.3529411764705,
      "grad_norm": 14.392988204956055,
      "learning_rate": 3.61764705882353e-05,
      "loss": 1.864,
      "step": 23500
    },
    {
      "epoch": 1382.9411764705883,
      "grad_norm": 12.442888259887695,
      "learning_rate": 3.6170588235294114e-05,
      "loss": 1.8516,
      "step": 23510
    },
    {
      "epoch": 1383.5294117647059,
      "grad_norm": 15.906510353088379,
      "learning_rate": 3.6164705882352944e-05,
      "loss": 2.0317,
      "step": 23520
    },
    {
      "epoch": 1384.1176470588234,
      "grad_norm": 15.871562957763672,
      "learning_rate": 3.615882352941177e-05,
      "loss": 1.8827,
      "step": 23530
    },
    {
      "epoch": 1384.7058823529412,
      "grad_norm": 14.629555702209473,
      "learning_rate": 3.615294117647059e-05,
      "loss": 1.9349,
      "step": 23540
    },
    {
      "epoch": 1385.2941176470588,
      "grad_norm": 13.565082550048828,
      "learning_rate": 3.614705882352941e-05,
      "loss": 1.8667,
      "step": 23550
    },
    {
      "epoch": 1385.8823529411766,
      "grad_norm": 12.458029747009277,
      "learning_rate": 3.614117647058824e-05,
      "loss": 1.8049,
      "step": 23560
    },
    {
      "epoch": 1386.4705882352941,
      "grad_norm": 12.783731460571289,
      "learning_rate": 3.613529411764706e-05,
      "loss": 1.8383,
      "step": 23570
    },
    {
      "epoch": 1387.0588235294117,
      "grad_norm": 16.09444808959961,
      "learning_rate": 3.612941176470588e-05,
      "loss": 1.8194,
      "step": 23580
    },
    {
      "epoch": 1387.6470588235295,
      "grad_norm": 13.555473327636719,
      "learning_rate": 3.6123529411764705e-05,
      "loss": 1.8037,
      "step": 23590
    },
    {
      "epoch": 1388.235294117647,
      "grad_norm": 13.675329208374023,
      "learning_rate": 3.6117647058823534e-05,
      "loss": 1.8623,
      "step": 23600
    },
    {
      "epoch": 1388.8235294117646,
      "grad_norm": 16.45490837097168,
      "learning_rate": 3.611176470588236e-05,
      "loss": 1.9024,
      "step": 23610
    },
    {
      "epoch": 1389.4117647058824,
      "grad_norm": 15.483681678771973,
      "learning_rate": 3.610588235294118e-05,
      "loss": 1.6925,
      "step": 23620
    },
    {
      "epoch": 1390.0,
      "grad_norm": 15.267387390136719,
      "learning_rate": 3.61e-05,
      "loss": 1.7938,
      "step": 23630
    },
    {
      "epoch": 1390.5882352941176,
      "grad_norm": 13.441234588623047,
      "learning_rate": 3.609411764705882e-05,
      "loss": 1.8386,
      "step": 23640
    },
    {
      "epoch": 1391.1764705882354,
      "grad_norm": 13.382433891296387,
      "learning_rate": 3.608823529411765e-05,
      "loss": 1.868,
      "step": 23650
    },
    {
      "epoch": 1391.764705882353,
      "grad_norm": 17.311189651489258,
      "learning_rate": 3.608235294117647e-05,
      "loss": 1.9575,
      "step": 23660
    },
    {
      "epoch": 1392.3529411764705,
      "grad_norm": 14.632424354553223,
      "learning_rate": 3.6076470588235295e-05,
      "loss": 1.8447,
      "step": 23670
    },
    {
      "epoch": 1392.9411764705883,
      "grad_norm": 14.037824630737305,
      "learning_rate": 3.607058823529412e-05,
      "loss": 1.7927,
      "step": 23680
    },
    {
      "epoch": 1393.5294117647059,
      "grad_norm": 14.70932674407959,
      "learning_rate": 3.606470588235295e-05,
      "loss": 1.78,
      "step": 23690
    },
    {
      "epoch": 1394.1176470588234,
      "grad_norm": 13.298376083374023,
      "learning_rate": 3.6058823529411764e-05,
      "loss": 1.8818,
      "step": 23700
    },
    {
      "epoch": 1394.7058823529412,
      "grad_norm": 11.52798080444336,
      "learning_rate": 3.605294117647059e-05,
      "loss": 1.7774,
      "step": 23710
    },
    {
      "epoch": 1395.2941176470588,
      "grad_norm": 13.708819389343262,
      "learning_rate": 3.604705882352941e-05,
      "loss": 1.8139,
      "step": 23720
    },
    {
      "epoch": 1395.8823529411766,
      "grad_norm": 11.60035514831543,
      "learning_rate": 3.604117647058824e-05,
      "loss": 1.8971,
      "step": 23730
    },
    {
      "epoch": 1396.4705882352941,
      "grad_norm": 12.576396942138672,
      "learning_rate": 3.603529411764706e-05,
      "loss": 1.8496,
      "step": 23740
    },
    {
      "epoch": 1397.0588235294117,
      "grad_norm": 12.771588325500488,
      "learning_rate": 3.6029411764705886e-05,
      "loss": 1.8881,
      "step": 23750
    },
    {
      "epoch": 1397.6470588235295,
      "grad_norm": 11.4514799118042,
      "learning_rate": 3.602352941176471e-05,
      "loss": 1.9013,
      "step": 23760
    },
    {
      "epoch": 1398.235294117647,
      "grad_norm": 10.601831436157227,
      "learning_rate": 3.601764705882353e-05,
      "loss": 1.8438,
      "step": 23770
    },
    {
      "epoch": 1398.8235294117646,
      "grad_norm": 13.912939071655273,
      "learning_rate": 3.6011764705882355e-05,
      "loss": 1.7584,
      "step": 23780
    },
    {
      "epoch": 1399.4117647058824,
      "grad_norm": 17.382034301757812,
      "learning_rate": 3.600588235294118e-05,
      "loss": 1.762,
      "step": 23790
    },
    {
      "epoch": 1400.0,
      "grad_norm": 17.032384872436523,
      "learning_rate": 3.6e-05,
      "loss": 1.8707,
      "step": 23800
    },
    {
      "epoch": 1400.5882352941176,
      "grad_norm": 13.324875831604004,
      "learning_rate": 3.5994117647058824e-05,
      "loss": 1.7939,
      "step": 23810
    },
    {
      "epoch": 1401.1764705882354,
      "grad_norm": 14.300386428833008,
      "learning_rate": 3.5988235294117654e-05,
      "loss": 1.8162,
      "step": 23820
    },
    {
      "epoch": 1401.764705882353,
      "grad_norm": 11.118367195129395,
      "learning_rate": 3.598235294117647e-05,
      "loss": 1.8303,
      "step": 23830
    },
    {
      "epoch": 1402.3529411764705,
      "grad_norm": 12.406688690185547,
      "learning_rate": 3.597647058823529e-05,
      "loss": 1.7606,
      "step": 23840
    },
    {
      "epoch": 1402.9411764705883,
      "grad_norm": 16.361949920654297,
      "learning_rate": 3.5970588235294116e-05,
      "loss": 1.8026,
      "step": 23850
    },
    {
      "epoch": 1403.5294117647059,
      "grad_norm": 13.876462936401367,
      "learning_rate": 3.5964705882352946e-05,
      "loss": 1.8683,
      "step": 23860
    },
    {
      "epoch": 1404.1176470588234,
      "grad_norm": 13.63269329071045,
      "learning_rate": 3.595882352941177e-05,
      "loss": 1.8551,
      "step": 23870
    },
    {
      "epoch": 1404.7058823529412,
      "grad_norm": 14.542613983154297,
      "learning_rate": 3.595294117647059e-05,
      "loss": 1.8775,
      "step": 23880
    },
    {
      "epoch": 1405.2941176470588,
      "grad_norm": 10.613370895385742,
      "learning_rate": 3.5947058823529415e-05,
      "loss": 1.7462,
      "step": 23890
    },
    {
      "epoch": 1405.8823529411766,
      "grad_norm": 17.153188705444336,
      "learning_rate": 3.594117647058824e-05,
      "loss": 1.862,
      "step": 23900
    },
    {
      "epoch": 1406.4705882352941,
      "grad_norm": 15.569724082946777,
      "learning_rate": 3.593529411764706e-05,
      "loss": 1.9281,
      "step": 23910
    },
    {
      "epoch": 1407.0588235294117,
      "grad_norm": 12.941712379455566,
      "learning_rate": 3.5929411764705884e-05,
      "loss": 1.6924,
      "step": 23920
    },
    {
      "epoch": 1407.6470588235295,
      "grad_norm": 14.833459854125977,
      "learning_rate": 3.5923529411764707e-05,
      "loss": 1.9148,
      "step": 23930
    },
    {
      "epoch": 1408.235294117647,
      "grad_norm": 15.604670524597168,
      "learning_rate": 3.5917647058823536e-05,
      "loss": 1.787,
      "step": 23940
    },
    {
      "epoch": 1408.8235294117646,
      "grad_norm": 13.920770645141602,
      "learning_rate": 3.591176470588235e-05,
      "loss": 1.8594,
      "step": 23950
    },
    {
      "epoch": 1409.4117647058824,
      "grad_norm": 16.202844619750977,
      "learning_rate": 3.5905882352941175e-05,
      "loss": 1.8094,
      "step": 23960
    },
    {
      "epoch": 1410.0,
      "grad_norm": 18.856002807617188,
      "learning_rate": 3.59e-05,
      "loss": 1.9856,
      "step": 23970
    },
    {
      "epoch": 1410.5882352941176,
      "grad_norm": 10.26340389251709,
      "learning_rate": 3.589411764705882e-05,
      "loss": 1.7832,
      "step": 23980
    },
    {
      "epoch": 1411.1764705882354,
      "grad_norm": 14.73755931854248,
      "learning_rate": 3.588823529411765e-05,
      "loss": 2.0196,
      "step": 23990
    },
    {
      "epoch": 1411.764705882353,
      "grad_norm": 19.840927124023438,
      "learning_rate": 3.5882352941176474e-05,
      "loss": 1.8126,
      "step": 24000
    },
    {
      "epoch": 1412.3529411764705,
      "grad_norm": 16.508712768554688,
      "learning_rate": 3.58764705882353e-05,
      "loss": 1.8163,
      "step": 24010
    },
    {
      "epoch": 1412.9411764705883,
      "grad_norm": 13.803569793701172,
      "learning_rate": 3.587058823529412e-05,
      "loss": 1.858,
      "step": 24020
    },
    {
      "epoch": 1413.5294117647059,
      "grad_norm": 11.27096176147461,
      "learning_rate": 3.586470588235294e-05,
      "loss": 1.8395,
      "step": 24030
    },
    {
      "epoch": 1414.1176470588234,
      "grad_norm": 13.840675354003906,
      "learning_rate": 3.5858823529411766e-05,
      "loss": 2.0044,
      "step": 24040
    },
    {
      "epoch": 1414.7058823529412,
      "grad_norm": 17.77585220336914,
      "learning_rate": 3.585294117647059e-05,
      "loss": 1.7195,
      "step": 24050
    },
    {
      "epoch": 1415.2941176470588,
      "grad_norm": 11.845598220825195,
      "learning_rate": 3.584705882352941e-05,
      "loss": 1.8389,
      "step": 24060
    },
    {
      "epoch": 1415.8823529411766,
      "grad_norm": 14.940958023071289,
      "learning_rate": 3.584117647058824e-05,
      "loss": 1.7567,
      "step": 24070
    },
    {
      "epoch": 1416.4705882352941,
      "grad_norm": 18.27631950378418,
      "learning_rate": 3.583529411764706e-05,
      "loss": 1.7963,
      "step": 24080
    },
    {
      "epoch": 1417.0588235294117,
      "grad_norm": 13.403844833374023,
      "learning_rate": 3.582941176470588e-05,
      "loss": 1.7799,
      "step": 24090
    },
    {
      "epoch": 1417.6470588235295,
      "grad_norm": 14.784637451171875,
      "learning_rate": 3.5823529411764704e-05,
      "loss": 1.8201,
      "step": 24100
    },
    {
      "epoch": 1418.235294117647,
      "grad_norm": 13.0119047164917,
      "learning_rate": 3.5817647058823534e-05,
      "loss": 1.7801,
      "step": 24110
    },
    {
      "epoch": 1418.8235294117646,
      "grad_norm": 12.779402732849121,
      "learning_rate": 3.581176470588236e-05,
      "loss": 1.9607,
      "step": 24120
    },
    {
      "epoch": 1419.4117647058824,
      "grad_norm": 14.370819091796875,
      "learning_rate": 3.580588235294118e-05,
      "loss": 1.7859,
      "step": 24130
    },
    {
      "epoch": 1420.0,
      "grad_norm": 23.60363006591797,
      "learning_rate": 3.58e-05,
      "loss": 1.8886,
      "step": 24140
    },
    {
      "epoch": 1420.5882352941176,
      "grad_norm": 15.215414047241211,
      "learning_rate": 3.5794117647058826e-05,
      "loss": 1.7937,
      "step": 24150
    },
    {
      "epoch": 1421.1764705882354,
      "grad_norm": 14.529176712036133,
      "learning_rate": 3.578823529411765e-05,
      "loss": 1.7869,
      "step": 24160
    },
    {
      "epoch": 1421.764705882353,
      "grad_norm": 12.25711441040039,
      "learning_rate": 3.578235294117647e-05,
      "loss": 1.8935,
      "step": 24170
    },
    {
      "epoch": 1422.3529411764705,
      "grad_norm": 16.16508674621582,
      "learning_rate": 3.5776470588235295e-05,
      "loss": 1.947,
      "step": 24180
    },
    {
      "epoch": 1422.9411764705883,
      "grad_norm": 13.216095924377441,
      "learning_rate": 3.577058823529412e-05,
      "loss": 2.0705,
      "step": 24190
    },
    {
      "epoch": 1423.5294117647059,
      "grad_norm": 13.367461204528809,
      "learning_rate": 3.576470588235295e-05,
      "loss": 1.7937,
      "step": 24200
    },
    {
      "epoch": 1424.1176470588234,
      "grad_norm": 13.940308570861816,
      "learning_rate": 3.5758823529411764e-05,
      "loss": 1.8254,
      "step": 24210
    },
    {
      "epoch": 1424.7058823529412,
      "grad_norm": 15.285505294799805,
      "learning_rate": 3.575294117647059e-05,
      "loss": 1.7683,
      "step": 24220
    },
    {
      "epoch": 1425.2941176470588,
      "grad_norm": 16.273149490356445,
      "learning_rate": 3.574705882352941e-05,
      "loss": 1.7656,
      "step": 24230
    },
    {
      "epoch": 1425.8823529411766,
      "grad_norm": 14.151044845581055,
      "learning_rate": 3.574117647058824e-05,
      "loss": 1.9114,
      "step": 24240
    },
    {
      "epoch": 1426.4705882352941,
      "grad_norm": 15.139336585998535,
      "learning_rate": 3.573529411764706e-05,
      "loss": 1.8094,
      "step": 24250
    },
    {
      "epoch": 1427.0588235294117,
      "grad_norm": 17.24203872680664,
      "learning_rate": 3.5729411764705885e-05,
      "loss": 1.8365,
      "step": 24260
    },
    {
      "epoch": 1427.6470588235295,
      "grad_norm": 13.272078514099121,
      "learning_rate": 3.572352941176471e-05,
      "loss": 1.8704,
      "step": 24270
    },
    {
      "epoch": 1428.235294117647,
      "grad_norm": 16.551692962646484,
      "learning_rate": 3.571764705882353e-05,
      "loss": 1.6933,
      "step": 24280
    },
    {
      "epoch": 1428.8235294117646,
      "grad_norm": 15.204256057739258,
      "learning_rate": 3.5711764705882354e-05,
      "loss": 1.7522,
      "step": 24290
    },
    {
      "epoch": 1429.4117647058824,
      "grad_norm": 11.163338661193848,
      "learning_rate": 3.570588235294118e-05,
      "loss": 1.7161,
      "step": 24300
    },
    {
      "epoch": 1430.0,
      "grad_norm": 18.494670867919922,
      "learning_rate": 3.57e-05,
      "loss": 1.8161,
      "step": 24310
    },
    {
      "epoch": 1430.5882352941176,
      "grad_norm": 12.435065269470215,
      "learning_rate": 3.569411764705883e-05,
      "loss": 1.8632,
      "step": 24320
    },
    {
      "epoch": 1431.1764705882354,
      "grad_norm": 13.80042839050293,
      "learning_rate": 3.568823529411765e-05,
      "loss": 1.8755,
      "step": 24330
    },
    {
      "epoch": 1431.764705882353,
      "grad_norm": 12.996187210083008,
      "learning_rate": 3.568235294117647e-05,
      "loss": 1.8355,
      "step": 24340
    },
    {
      "epoch": 1432.3529411764705,
      "grad_norm": 12.538721084594727,
      "learning_rate": 3.567647058823529e-05,
      "loss": 1.8411,
      "step": 24350
    },
    {
      "epoch": 1432.9411764705883,
      "grad_norm": 13.462035179138184,
      "learning_rate": 3.5670588235294115e-05,
      "loss": 1.8511,
      "step": 24360
    },
    {
      "epoch": 1433.5294117647059,
      "grad_norm": 12.218538284301758,
      "learning_rate": 3.5664705882352945e-05,
      "loss": 1.8037,
      "step": 24370
    },
    {
      "epoch": 1434.1176470588234,
      "grad_norm": 12.56678581237793,
      "learning_rate": 3.565882352941177e-05,
      "loss": 1.8255,
      "step": 24380
    },
    {
      "epoch": 1434.7058823529412,
      "grad_norm": 12.767909049987793,
      "learning_rate": 3.565294117647059e-05,
      "loss": 1.7313,
      "step": 24390
    },
    {
      "epoch": 1435.2941176470588,
      "grad_norm": 18.06397247314453,
      "learning_rate": 3.5647058823529414e-05,
      "loss": 1.7665,
      "step": 24400
    },
    {
      "epoch": 1435.8823529411766,
      "grad_norm": 14.189526557922363,
      "learning_rate": 3.564117647058824e-05,
      "loss": 1.9112,
      "step": 24410
    },
    {
      "epoch": 1436.4705882352941,
      "grad_norm": 10.212507247924805,
      "learning_rate": 3.563529411764706e-05,
      "loss": 1.7023,
      "step": 24420
    },
    {
      "epoch": 1437.0588235294117,
      "grad_norm": 11.943575859069824,
      "learning_rate": 3.562941176470588e-05,
      "loss": 1.8124,
      "step": 24430
    },
    {
      "epoch": 1437.6470588235295,
      "grad_norm": 15.448814392089844,
      "learning_rate": 3.5623529411764706e-05,
      "loss": 1.8875,
      "step": 24440
    },
    {
      "epoch": 1438.235294117647,
      "grad_norm": 13.454504013061523,
      "learning_rate": 3.5617647058823536e-05,
      "loss": 1.8146,
      "step": 24450
    },
    {
      "epoch": 1438.8235294117646,
      "grad_norm": 14.29754638671875,
      "learning_rate": 3.561176470588236e-05,
      "loss": 1.8061,
      "step": 24460
    },
    {
      "epoch": 1439.4117647058824,
      "grad_norm": 14.55666732788086,
      "learning_rate": 3.5605882352941175e-05,
      "loss": 1.8492,
      "step": 24470
    },
    {
      "epoch": 1440.0,
      "grad_norm": 19.53079605102539,
      "learning_rate": 3.56e-05,
      "loss": 1.9692,
      "step": 24480
    },
    {
      "epoch": 1440.5882352941176,
      "grad_norm": 13.317192077636719,
      "learning_rate": 3.559411764705883e-05,
      "loss": 1.8225,
      "step": 24490
    },
    {
      "epoch": 1441.1764705882354,
      "grad_norm": 16.166168212890625,
      "learning_rate": 3.558823529411765e-05,
      "loss": 1.9561,
      "step": 24500
    },
    {
      "epoch": 1441.764705882353,
      "grad_norm": 13.181015968322754,
      "learning_rate": 3.5582352941176474e-05,
      "loss": 1.8545,
      "step": 24510
    },
    {
      "epoch": 1442.3529411764705,
      "grad_norm": 18.10891342163086,
      "learning_rate": 3.55764705882353e-05,
      "loss": 1.876,
      "step": 24520
    },
    {
      "epoch": 1442.9411764705883,
      "grad_norm": 16.58759880065918,
      "learning_rate": 3.557058823529412e-05,
      "loss": 1.8467,
      "step": 24530
    },
    {
      "epoch": 1443.5294117647059,
      "grad_norm": 11.942256927490234,
      "learning_rate": 3.556470588235294e-05,
      "loss": 1.8628,
      "step": 24540
    },
    {
      "epoch": 1444.1176470588234,
      "grad_norm": 13.21800708770752,
      "learning_rate": 3.5558823529411766e-05,
      "loss": 1.8576,
      "step": 24550
    },
    {
      "epoch": 1444.7058823529412,
      "grad_norm": 14.384501457214355,
      "learning_rate": 3.555294117647059e-05,
      "loss": 1.8895,
      "step": 24560
    },
    {
      "epoch": 1445.2941176470588,
      "grad_norm": 13.064810752868652,
      "learning_rate": 3.554705882352941e-05,
      "loss": 1.7544,
      "step": 24570
    },
    {
      "epoch": 1445.8823529411766,
      "grad_norm": 12.338403701782227,
      "learning_rate": 3.554117647058824e-05,
      "loss": 1.83,
      "step": 24580
    },
    {
      "epoch": 1446.4705882352941,
      "grad_norm": 17.132848739624023,
      "learning_rate": 3.5535294117647064e-05,
      "loss": 1.8697,
      "step": 24590
    },
    {
      "epoch": 1447.0588235294117,
      "grad_norm": 12.81957721710205,
      "learning_rate": 3.552941176470588e-05,
      "loss": 1.9957,
      "step": 24600
    },
    {
      "epoch": 1447.6470588235295,
      "grad_norm": 13.141423225402832,
      "learning_rate": 3.5523529411764704e-05,
      "loss": 1.7416,
      "step": 24610
    },
    {
      "epoch": 1448.235294117647,
      "grad_norm": 13.747355461120605,
      "learning_rate": 3.551764705882353e-05,
      "loss": 1.8721,
      "step": 24620
    },
    {
      "epoch": 1448.8235294117646,
      "grad_norm": 11.979423522949219,
      "learning_rate": 3.5511764705882356e-05,
      "loss": 1.8155,
      "step": 24630
    },
    {
      "epoch": 1449.4117647058824,
      "grad_norm": 13.313885688781738,
      "learning_rate": 3.550588235294118e-05,
      "loss": 1.925,
      "step": 24640
    },
    {
      "epoch": 1450.0,
      "grad_norm": 14.601566314697266,
      "learning_rate": 3.55e-05,
      "loss": 1.6901,
      "step": 24650
    },
    {
      "epoch": 1450.5882352941176,
      "grad_norm": 11.70254898071289,
      "learning_rate": 3.5494117647058825e-05,
      "loss": 1.7889,
      "step": 24660
    },
    {
      "epoch": 1451.1764705882354,
      "grad_norm": 15.670191764831543,
      "learning_rate": 3.548823529411765e-05,
      "loss": 1.8608,
      "step": 24670
    },
    {
      "epoch": 1451.764705882353,
      "grad_norm": 15.391332626342773,
      "learning_rate": 3.548235294117647e-05,
      "loss": 1.8317,
      "step": 24680
    },
    {
      "epoch": 1452.3529411764705,
      "grad_norm": 15.442082405090332,
      "learning_rate": 3.5476470588235294e-05,
      "loss": 1.8279,
      "step": 24690
    },
    {
      "epoch": 1452.9411764705883,
      "grad_norm": 16.9542179107666,
      "learning_rate": 3.5470588235294124e-05,
      "loss": 1.8829,
      "step": 24700
    },
    {
      "epoch": 1453.5294117647059,
      "grad_norm": 13.456086158752441,
      "learning_rate": 3.546470588235295e-05,
      "loss": 1.765,
      "step": 24710
    },
    {
      "epoch": 1454.1176470588234,
      "grad_norm": 15.585417747497559,
      "learning_rate": 3.545882352941176e-05,
      "loss": 1.8293,
      "step": 24720
    },
    {
      "epoch": 1454.7058823529412,
      "grad_norm": 14.242266654968262,
      "learning_rate": 3.5452941176470586e-05,
      "loss": 1.8311,
      "step": 24730
    },
    {
      "epoch": 1455.2941176470588,
      "grad_norm": 17.059398651123047,
      "learning_rate": 3.544705882352941e-05,
      "loss": 2.0516,
      "step": 24740
    },
    {
      "epoch": 1455.8823529411766,
      "grad_norm": 17.178251266479492,
      "learning_rate": 3.544117647058824e-05,
      "loss": 1.8656,
      "step": 24750
    },
    {
      "epoch": 1456.4705882352941,
      "grad_norm": 17.23699378967285,
      "learning_rate": 3.543529411764706e-05,
      "loss": 1.7014,
      "step": 24760
    },
    {
      "epoch": 1457.0588235294117,
      "grad_norm": 15.120710372924805,
      "learning_rate": 3.5429411764705885e-05,
      "loss": 1.7727,
      "step": 24770
    },
    {
      "epoch": 1457.6470588235295,
      "grad_norm": 12.472417831420898,
      "learning_rate": 3.542352941176471e-05,
      "loss": 1.8724,
      "step": 24780
    },
    {
      "epoch": 1458.235294117647,
      "grad_norm": 15.82059097290039,
      "learning_rate": 3.541764705882353e-05,
      "loss": 1.8079,
      "step": 24790
    },
    {
      "epoch": 1458.8235294117646,
      "grad_norm": 13.698654174804688,
      "learning_rate": 3.5411764705882354e-05,
      "loss": 1.5977,
      "step": 24800
    },
    {
      "epoch": 1459.4117647058824,
      "grad_norm": 16.267534255981445,
      "learning_rate": 3.540588235294118e-05,
      "loss": 1.8472,
      "step": 24810
    },
    {
      "epoch": 1460.0,
      "grad_norm": 12.291669845581055,
      "learning_rate": 3.54e-05,
      "loss": 1.8021,
      "step": 24820
    },
    {
      "epoch": 1460.5882352941176,
      "grad_norm": 16.983692169189453,
      "learning_rate": 3.539411764705883e-05,
      "loss": 1.7765,
      "step": 24830
    },
    {
      "epoch": 1461.1764705882354,
      "grad_norm": 11.981891632080078,
      "learning_rate": 3.538823529411765e-05,
      "loss": 1.7599,
      "step": 24840
    },
    {
      "epoch": 1461.764705882353,
      "grad_norm": 13.955869674682617,
      "learning_rate": 3.538235294117647e-05,
      "loss": 1.6444,
      "step": 24850
    },
    {
      "epoch": 1462.3529411764705,
      "grad_norm": 12.929253578186035,
      "learning_rate": 3.537647058823529e-05,
      "loss": 1.7861,
      "step": 24860
    },
    {
      "epoch": 1462.9411764705883,
      "grad_norm": 16.298221588134766,
      "learning_rate": 3.537058823529412e-05,
      "loss": 1.8392,
      "step": 24870
    },
    {
      "epoch": 1463.5294117647059,
      "grad_norm": 15.775242805480957,
      "learning_rate": 3.5364705882352945e-05,
      "loss": 1.836,
      "step": 24880
    },
    {
      "epoch": 1464.1176470588234,
      "grad_norm": 15.12729263305664,
      "learning_rate": 3.535882352941177e-05,
      "loss": 1.8514,
      "step": 24890
    },
    {
      "epoch": 1464.7058823529412,
      "grad_norm": 12.821349143981934,
      "learning_rate": 3.535294117647059e-05,
      "loss": 1.8537,
      "step": 24900
    },
    {
      "epoch": 1465.2941176470588,
      "grad_norm": 16.590721130371094,
      "learning_rate": 3.5347058823529414e-05,
      "loss": 1.8935,
      "step": 24910
    },
    {
      "epoch": 1465.8823529411766,
      "grad_norm": 13.231184959411621,
      "learning_rate": 3.5341176470588237e-05,
      "loss": 1.8457,
      "step": 24920
    },
    {
      "epoch": 1466.4705882352941,
      "grad_norm": 16.913562774658203,
      "learning_rate": 3.533529411764706e-05,
      "loss": 1.7964,
      "step": 24930
    },
    {
      "epoch": 1467.0588235294117,
      "grad_norm": 11.312345504760742,
      "learning_rate": 3.532941176470588e-05,
      "loss": 1.8085,
      "step": 24940
    },
    {
      "epoch": 1467.6470588235295,
      "grad_norm": 18.116079330444336,
      "learning_rate": 3.5323529411764705e-05,
      "loss": 1.6788,
      "step": 24950
    },
    {
      "epoch": 1468.235294117647,
      "grad_norm": 11.739140510559082,
      "learning_rate": 3.5317647058823535e-05,
      "loss": 1.7961,
      "step": 24960
    },
    {
      "epoch": 1468.8235294117646,
      "grad_norm": 15.56081771850586,
      "learning_rate": 3.531176470588236e-05,
      "loss": 1.7956,
      "step": 24970
    },
    {
      "epoch": 1469.4117647058824,
      "grad_norm": 14.62789249420166,
      "learning_rate": 3.5305882352941174e-05,
      "loss": 1.8743,
      "step": 24980
    },
    {
      "epoch": 1470.0,
      "grad_norm": 15.726627349853516,
      "learning_rate": 3.53e-05,
      "loss": 1.834,
      "step": 24990
    },
    {
      "epoch": 1470.5882352941176,
      "grad_norm": 15.201982498168945,
      "learning_rate": 3.529411764705883e-05,
      "loss": 1.8512,
      "step": 25000
    },
    {
      "epoch": 1471.1764705882354,
      "grad_norm": 15.804484367370605,
      "learning_rate": 3.528823529411765e-05,
      "loss": 1.7869,
      "step": 25010
    },
    {
      "epoch": 1471.764705882353,
      "grad_norm": 15.266661643981934,
      "learning_rate": 3.528235294117647e-05,
      "loss": 1.6832,
      "step": 25020
    },
    {
      "epoch": 1472.3529411764705,
      "grad_norm": 12.177105903625488,
      "learning_rate": 3.5276470588235296e-05,
      "loss": 1.9032,
      "step": 25030
    },
    {
      "epoch": 1472.9411764705883,
      "grad_norm": 13.327686309814453,
      "learning_rate": 3.527058823529412e-05,
      "loss": 1.8219,
      "step": 25040
    },
    {
      "epoch": 1473.5294117647059,
      "grad_norm": 20.38330841064453,
      "learning_rate": 3.526470588235294e-05,
      "loss": 1.783,
      "step": 25050
    },
    {
      "epoch": 1474.1176470588234,
      "grad_norm": 13.58383846282959,
      "learning_rate": 3.5258823529411765e-05,
      "loss": 1.7963,
      "step": 25060
    },
    {
      "epoch": 1474.7058823529412,
      "grad_norm": 15.019810676574707,
      "learning_rate": 3.525294117647059e-05,
      "loss": 1.7251,
      "step": 25070
    },
    {
      "epoch": 1475.2941176470588,
      "grad_norm": 11.343415260314941,
      "learning_rate": 3.524705882352941e-05,
      "loss": 1.6392,
      "step": 25080
    },
    {
      "epoch": 1475.8823529411766,
      "grad_norm": 10.719179153442383,
      "learning_rate": 3.524117647058824e-05,
      "loss": 1.8594,
      "step": 25090
    },
    {
      "epoch": 1476.4705882352941,
      "grad_norm": 16.04034423828125,
      "learning_rate": 3.5235294117647064e-05,
      "loss": 1.7753,
      "step": 25100
    },
    {
      "epoch": 1477.0588235294117,
      "grad_norm": 16.561113357543945,
      "learning_rate": 3.522941176470588e-05,
      "loss": 1.7697,
      "step": 25110
    },
    {
      "epoch": 1477.6470588235295,
      "grad_norm": 13.90344524383545,
      "learning_rate": 3.52235294117647e-05,
      "loss": 1.7888,
      "step": 25120
    },
    {
      "epoch": 1478.235294117647,
      "grad_norm": 12.0066499710083,
      "learning_rate": 3.521764705882353e-05,
      "loss": 1.8418,
      "step": 25130
    },
    {
      "epoch": 1478.8235294117646,
      "grad_norm": 12.179886817932129,
      "learning_rate": 3.5211764705882356e-05,
      "loss": 1.8839,
      "step": 25140
    },
    {
      "epoch": 1479.4117647058824,
      "grad_norm": 13.444892883300781,
      "learning_rate": 3.520588235294118e-05,
      "loss": 1.8088,
      "step": 25150
    },
    {
      "epoch": 1480.0,
      "grad_norm": 19.63933563232422,
      "learning_rate": 3.52e-05,
      "loss": 1.7709,
      "step": 25160
    },
    {
      "epoch": 1480.5882352941176,
      "grad_norm": 13.283266067504883,
      "learning_rate": 3.5194117647058825e-05,
      "loss": 1.7402,
      "step": 25170
    },
    {
      "epoch": 1481.1764705882354,
      "grad_norm": 17.16364288330078,
      "learning_rate": 3.518823529411765e-05,
      "loss": 1.7814,
      "step": 25180
    },
    {
      "epoch": 1481.764705882353,
      "grad_norm": 13.792681694030762,
      "learning_rate": 3.518235294117647e-05,
      "loss": 1.6412,
      "step": 25190
    },
    {
      "epoch": 1482.3529411764705,
      "grad_norm": 14.014781951904297,
      "learning_rate": 3.5176470588235294e-05,
      "loss": 1.7198,
      "step": 25200
    },
    {
      "epoch": 1482.9411764705883,
      "grad_norm": 17.280677795410156,
      "learning_rate": 3.5170588235294124e-05,
      "loss": 1.8206,
      "step": 25210
    },
    {
      "epoch": 1483.5294117647059,
      "grad_norm": 15.399802207946777,
      "learning_rate": 3.5164705882352946e-05,
      "loss": 1.8115,
      "step": 25220
    },
    {
      "epoch": 1484.1176470588234,
      "grad_norm": 14.468405723571777,
      "learning_rate": 3.515882352941177e-05,
      "loss": 1.8177,
      "step": 25230
    },
    {
      "epoch": 1484.7058823529412,
      "grad_norm": 15.035308837890625,
      "learning_rate": 3.5152941176470586e-05,
      "loss": 1.7846,
      "step": 25240
    },
    {
      "epoch": 1485.2941176470588,
      "grad_norm": 17.73392105102539,
      "learning_rate": 3.514705882352941e-05,
      "loss": 1.8343,
      "step": 25250
    },
    {
      "epoch": 1485.8823529411766,
      "grad_norm": 15.065890312194824,
      "learning_rate": 3.514117647058824e-05,
      "loss": 1.8683,
      "step": 25260
    },
    {
      "epoch": 1486.4705882352941,
      "grad_norm": 19.114822387695312,
      "learning_rate": 3.513529411764706e-05,
      "loss": 1.8139,
      "step": 25270
    },
    {
      "epoch": 1487.0588235294117,
      "grad_norm": 13.632885932922363,
      "learning_rate": 3.5129411764705884e-05,
      "loss": 1.612,
      "step": 25280
    },
    {
      "epoch": 1487.6470588235295,
      "grad_norm": 13.465003967285156,
      "learning_rate": 3.512352941176471e-05,
      "loss": 1.8914,
      "step": 25290
    },
    {
      "epoch": 1488.235294117647,
      "grad_norm": 12.015048027038574,
      "learning_rate": 3.511764705882353e-05,
      "loss": 1.7534,
      "step": 25300
    },
    {
      "epoch": 1488.8235294117646,
      "grad_norm": 19.180356979370117,
      "learning_rate": 3.5111764705882353e-05,
      "loss": 1.7504,
      "step": 25310
    },
    {
      "epoch": 1489.4117647058824,
      "grad_norm": 20.208826065063477,
      "learning_rate": 3.5105882352941176e-05,
      "loss": 1.6935,
      "step": 25320
    },
    {
      "epoch": 1490.0,
      "grad_norm": 15.619515419006348,
      "learning_rate": 3.51e-05,
      "loss": 1.8946,
      "step": 25330
    },
    {
      "epoch": 1490.5882352941176,
      "grad_norm": 14.785903930664062,
      "learning_rate": 3.509411764705883e-05,
      "loss": 1.7187,
      "step": 25340
    },
    {
      "epoch": 1491.1764705882354,
      "grad_norm": 14.577994346618652,
      "learning_rate": 3.508823529411765e-05,
      "loss": 1.8141,
      "step": 25350
    },
    {
      "epoch": 1491.764705882353,
      "grad_norm": 15.063780784606934,
      "learning_rate": 3.5082352941176475e-05,
      "loss": 1.8339,
      "step": 25360
    },
    {
      "epoch": 1492.3529411764705,
      "grad_norm": 16.076021194458008,
      "learning_rate": 3.507647058823529e-05,
      "loss": 1.7714,
      "step": 25370
    },
    {
      "epoch": 1492.9411764705883,
      "grad_norm": 14.056157112121582,
      "learning_rate": 3.507058823529412e-05,
      "loss": 1.6875,
      "step": 25380
    },
    {
      "epoch": 1493.5294117647059,
      "grad_norm": 13.112013816833496,
      "learning_rate": 3.5064705882352944e-05,
      "loss": 1.7289,
      "step": 25390
    },
    {
      "epoch": 1494.1176470588234,
      "grad_norm": 14.337563514709473,
      "learning_rate": 3.505882352941177e-05,
      "loss": 1.8204,
      "step": 25400
    },
    {
      "epoch": 1494.7058823529412,
      "grad_norm": 11.86214542388916,
      "learning_rate": 3.505294117647059e-05,
      "loss": 1.6846,
      "step": 25410
    },
    {
      "epoch": 1495.2941176470588,
      "grad_norm": 11.421478271484375,
      "learning_rate": 3.504705882352941e-05,
      "loss": 1.6211,
      "step": 25420
    },
    {
      "epoch": 1495.8823529411766,
      "grad_norm": 14.790026664733887,
      "learning_rate": 3.5041176470588236e-05,
      "loss": 1.7273,
      "step": 25430
    },
    {
      "epoch": 1496.4705882352941,
      "grad_norm": 13.511914253234863,
      "learning_rate": 3.503529411764706e-05,
      "loss": 1.8388,
      "step": 25440
    },
    {
      "epoch": 1497.0588235294117,
      "grad_norm": 14.105345726013184,
      "learning_rate": 3.502941176470588e-05,
      "loss": 1.944,
      "step": 25450
    },
    {
      "epoch": 1497.6470588235295,
      "grad_norm": 13.024380683898926,
      "learning_rate": 3.5023529411764705e-05,
      "loss": 1.7609,
      "step": 25460
    },
    {
      "epoch": 1498.235294117647,
      "grad_norm": 16.62344741821289,
      "learning_rate": 3.5017647058823535e-05,
      "loss": 1.8627,
      "step": 25470
    },
    {
      "epoch": 1498.8235294117646,
      "grad_norm": 13.2499418258667,
      "learning_rate": 3.501176470588236e-05,
      "loss": 1.7199,
      "step": 25480
    },
    {
      "epoch": 1499.4117647058824,
      "grad_norm": 12.106566429138184,
      "learning_rate": 3.5005882352941174e-05,
      "loss": 1.6761,
      "step": 25490
    },
    {
      "epoch": 1500.0,
      "grad_norm": 18.556276321411133,
      "learning_rate": 3.5e-05,
      "loss": 1.6915,
      "step": 25500
    },
    {
      "epoch": 1500.5882352941176,
      "grad_norm": 15.687694549560547,
      "learning_rate": 3.499411764705883e-05,
      "loss": 1.7242,
      "step": 25510
    },
    {
      "epoch": 1501.1764705882354,
      "grad_norm": 14.796854972839355,
      "learning_rate": 3.498823529411765e-05,
      "loss": 1.7494,
      "step": 25520
    },
    {
      "epoch": 1501.764705882353,
      "grad_norm": 17.960817337036133,
      "learning_rate": 3.498235294117647e-05,
      "loss": 1.8597,
      "step": 25530
    },
    {
      "epoch": 1502.3529411764705,
      "grad_norm": 14.822564125061035,
      "learning_rate": 3.4976470588235296e-05,
      "loss": 1.736,
      "step": 25540
    },
    {
      "epoch": 1502.9411764705883,
      "grad_norm": 14.884907722473145,
      "learning_rate": 3.497058823529412e-05,
      "loss": 1.7715,
      "step": 25550
    },
    {
      "epoch": 1503.5294117647059,
      "grad_norm": 15.636905670166016,
      "learning_rate": 3.496470588235294e-05,
      "loss": 1.8372,
      "step": 25560
    },
    {
      "epoch": 1504.1176470588234,
      "grad_norm": 14.266682624816895,
      "learning_rate": 3.4958823529411765e-05,
      "loss": 1.7675,
      "step": 25570
    },
    {
      "epoch": 1504.7058823529412,
      "grad_norm": 19.245174407958984,
      "learning_rate": 3.495294117647059e-05,
      "loss": 1.7399,
      "step": 25580
    },
    {
      "epoch": 1505.2941176470588,
      "grad_norm": 15.23443603515625,
      "learning_rate": 3.494705882352942e-05,
      "loss": 1.7054,
      "step": 25590
    },
    {
      "epoch": 1505.8823529411766,
      "grad_norm": 15.361745834350586,
      "learning_rate": 3.494117647058824e-05,
      "loss": 1.736,
      "step": 25600
    },
    {
      "epoch": 1506.4705882352941,
      "grad_norm": 15.196101188659668,
      "learning_rate": 3.493529411764706e-05,
      "loss": 1.7572,
      "step": 25610
    },
    {
      "epoch": 1507.0588235294117,
      "grad_norm": 12.434167861938477,
      "learning_rate": 3.492941176470588e-05,
      "loss": 1.7907,
      "step": 25620
    },
    {
      "epoch": 1507.6470588235295,
      "grad_norm": 14.613511085510254,
      "learning_rate": 3.49235294117647e-05,
      "loss": 1.8963,
      "step": 25630
    },
    {
      "epoch": 1508.235294117647,
      "grad_norm": 12.52574634552002,
      "learning_rate": 3.491764705882353e-05,
      "loss": 1.7282,
      "step": 25640
    },
    {
      "epoch": 1508.8235294117646,
      "grad_norm": 12.649870872497559,
      "learning_rate": 3.4911764705882355e-05,
      "loss": 1.7867,
      "step": 25650
    },
    {
      "epoch": 1509.4117647058824,
      "grad_norm": 11.732515335083008,
      "learning_rate": 3.490588235294118e-05,
      "loss": 1.8106,
      "step": 25660
    },
    {
      "epoch": 1510.0,
      "grad_norm": 16.17209815979004,
      "learning_rate": 3.49e-05,
      "loss": 1.7521,
      "step": 25670
    },
    {
      "epoch": 1510.5882352941176,
      "grad_norm": 13.612937927246094,
      "learning_rate": 3.4894117647058824e-05,
      "loss": 1.6973,
      "step": 25680
    },
    {
      "epoch": 1511.1764705882354,
      "grad_norm": 11.349863052368164,
      "learning_rate": 3.488823529411765e-05,
      "loss": 1.7559,
      "step": 25690
    },
    {
      "epoch": 1511.764705882353,
      "grad_norm": 17.512876510620117,
      "learning_rate": 3.488235294117647e-05,
      "loss": 1.745,
      "step": 25700
    },
    {
      "epoch": 1512.3529411764705,
      "grad_norm": 16.031352996826172,
      "learning_rate": 3.487647058823529e-05,
      "loss": 1.6889,
      "step": 25710
    },
    {
      "epoch": 1512.9411764705883,
      "grad_norm": 15.42462158203125,
      "learning_rate": 3.487058823529412e-05,
      "loss": 1.7239,
      "step": 25720
    },
    {
      "epoch": 1513.5294117647059,
      "grad_norm": 15.099016189575195,
      "learning_rate": 3.4864705882352946e-05,
      "loss": 1.8591,
      "step": 25730
    },
    {
      "epoch": 1514.1176470588234,
      "grad_norm": 15.506037712097168,
      "learning_rate": 3.485882352941177e-05,
      "loss": 1.7955,
      "step": 25740
    },
    {
      "epoch": 1514.7058823529412,
      "grad_norm": 12.311927795410156,
      "learning_rate": 3.4852941176470585e-05,
      "loss": 1.704,
      "step": 25750
    },
    {
      "epoch": 1515.2941176470588,
      "grad_norm": 13.14070987701416,
      "learning_rate": 3.4847058823529415e-05,
      "loss": 1.7096,
      "step": 25760
    },
    {
      "epoch": 1515.8823529411766,
      "grad_norm": 14.087037086486816,
      "learning_rate": 3.484117647058824e-05,
      "loss": 1.6609,
      "step": 25770
    },
    {
      "epoch": 1516.4705882352941,
      "grad_norm": 16.972248077392578,
      "learning_rate": 3.483529411764706e-05,
      "loss": 1.7575,
      "step": 25780
    },
    {
      "epoch": 1517.0588235294117,
      "grad_norm": 15.26832389831543,
      "learning_rate": 3.4829411764705884e-05,
      "loss": 1.8095,
      "step": 25790
    },
    {
      "epoch": 1517.6470588235295,
      "grad_norm": 14.284852027893066,
      "learning_rate": 3.482352941176471e-05,
      "loss": 1.873,
      "step": 25800
    },
    {
      "epoch": 1518.235294117647,
      "grad_norm": 15.122254371643066,
      "learning_rate": 3.481764705882353e-05,
      "loss": 1.8412,
      "step": 25810
    },
    {
      "epoch": 1518.8235294117646,
      "grad_norm": 13.525522232055664,
      "learning_rate": 3.481176470588235e-05,
      "loss": 1.5925,
      "step": 25820
    },
    {
      "epoch": 1519.4117647058824,
      "grad_norm": 13.33499526977539,
      "learning_rate": 3.4805882352941176e-05,
      "loss": 1.7544,
      "step": 25830
    },
    {
      "epoch": 1520.0,
      "grad_norm": 12.444738388061523,
      "learning_rate": 3.48e-05,
      "loss": 1.74,
      "step": 25840
    },
    {
      "epoch": 1520.5882352941176,
      "grad_norm": 12.924883842468262,
      "learning_rate": 3.479411764705883e-05,
      "loss": 1.574,
      "step": 25850
    },
    {
      "epoch": 1521.1764705882354,
      "grad_norm": 12.429524421691895,
      "learning_rate": 3.478823529411765e-05,
      "loss": 1.7363,
      "step": 25860
    },
    {
      "epoch": 1521.764705882353,
      "grad_norm": 16.433238983154297,
      "learning_rate": 3.4782352941176475e-05,
      "loss": 1.6571,
      "step": 25870
    },
    {
      "epoch": 1522.3529411764705,
      "grad_norm": 16.982627868652344,
      "learning_rate": 3.477647058823529e-05,
      "loss": 1.6808,
      "step": 25880
    },
    {
      "epoch": 1522.9411764705883,
      "grad_norm": 14.216693878173828,
      "learning_rate": 3.477058823529412e-05,
      "loss": 1.779,
      "step": 25890
    },
    {
      "epoch": 1523.5294117647059,
      "grad_norm": 15.915305137634277,
      "learning_rate": 3.4764705882352944e-05,
      "loss": 1.6315,
      "step": 25900
    },
    {
      "epoch": 1524.1176470588234,
      "grad_norm": 15.307366371154785,
      "learning_rate": 3.4758823529411767e-05,
      "loss": 1.8496,
      "step": 25910
    },
    {
      "epoch": 1524.7058823529412,
      "grad_norm": 11.483681678771973,
      "learning_rate": 3.475294117647059e-05,
      "loss": 1.8034,
      "step": 25920
    },
    {
      "epoch": 1525.2941176470588,
      "grad_norm": 21.116811752319336,
      "learning_rate": 3.474705882352941e-05,
      "loss": 1.761,
      "step": 25930
    },
    {
      "epoch": 1525.8823529411766,
      "grad_norm": 13.505842208862305,
      "learning_rate": 3.4741176470588236e-05,
      "loss": 1.7985,
      "step": 25940
    },
    {
      "epoch": 1526.4705882352941,
      "grad_norm": 13.156484603881836,
      "learning_rate": 3.473529411764706e-05,
      "loss": 1.7889,
      "step": 25950
    },
    {
      "epoch": 1527.0588235294117,
      "grad_norm": 15.234335899353027,
      "learning_rate": 3.472941176470588e-05,
      "loss": 1.7098,
      "step": 25960
    },
    {
      "epoch": 1527.6470588235295,
      "grad_norm": 18.89858627319336,
      "learning_rate": 3.472352941176471e-05,
      "loss": 1.6732,
      "step": 25970
    },
    {
      "epoch": 1528.235294117647,
      "grad_norm": 16.438302993774414,
      "learning_rate": 3.4717647058823534e-05,
      "loss": 1.701,
      "step": 25980
    },
    {
      "epoch": 1528.8235294117646,
      "grad_norm": 16.95621681213379,
      "learning_rate": 3.471176470588236e-05,
      "loss": 1.7618,
      "step": 25990
    },
    {
      "epoch": 1529.4117647058824,
      "grad_norm": 15.34073257446289,
      "learning_rate": 3.470588235294118e-05,
      "loss": 1.8103,
      "step": 26000
    },
    {
      "epoch": 1530.0,
      "grad_norm": 19.947362899780273,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 1.7725,
      "step": 26010
    },
    {
      "epoch": 1530.5882352941176,
      "grad_norm": 13.316301345825195,
      "learning_rate": 3.4694117647058826e-05,
      "loss": 1.7209,
      "step": 26020
    },
    {
      "epoch": 1531.1764705882354,
      "grad_norm": 15.984450340270996,
      "learning_rate": 3.468823529411765e-05,
      "loss": 1.8912,
      "step": 26030
    },
    {
      "epoch": 1531.764705882353,
      "grad_norm": 17.962770462036133,
      "learning_rate": 3.468235294117647e-05,
      "loss": 1.7119,
      "step": 26040
    },
    {
      "epoch": 1532.3529411764705,
      "grad_norm": 14.678467750549316,
      "learning_rate": 3.4676470588235295e-05,
      "loss": 1.6557,
      "step": 26050
    },
    {
      "epoch": 1532.9411764705883,
      "grad_norm": 18.177400588989258,
      "learning_rate": 3.467058823529412e-05,
      "loss": 1.6954,
      "step": 26060
    },
    {
      "epoch": 1533.5294117647059,
      "grad_norm": 14.56309700012207,
      "learning_rate": 3.466470588235294e-05,
      "loss": 1.7228,
      "step": 26070
    },
    {
      "epoch": 1534.1176470588234,
      "grad_norm": 15.91767692565918,
      "learning_rate": 3.4658823529411764e-05,
      "loss": 1.8932,
      "step": 26080
    },
    {
      "epoch": 1534.7058823529412,
      "grad_norm": 14.024698257446289,
      "learning_rate": 3.465294117647059e-05,
      "loss": 1.6998,
      "step": 26090
    },
    {
      "epoch": 1535.2941176470588,
      "grad_norm": 20.666324615478516,
      "learning_rate": 3.464705882352942e-05,
      "loss": 1.7242,
      "step": 26100
    },
    {
      "epoch": 1535.8823529411766,
      "grad_norm": 16.069162368774414,
      "learning_rate": 3.464117647058824e-05,
      "loss": 1.7147,
      "step": 26110
    },
    {
      "epoch": 1536.4705882352941,
      "grad_norm": 12.922245025634766,
      "learning_rate": 3.463529411764706e-05,
      "loss": 1.6934,
      "step": 26120
    },
    {
      "epoch": 1537.0588235294117,
      "grad_norm": 14.61648941040039,
      "learning_rate": 3.4629411764705886e-05,
      "loss": 1.6103,
      "step": 26130
    },
    {
      "epoch": 1537.6470588235295,
      "grad_norm": 15.25657844543457,
      "learning_rate": 3.462352941176471e-05,
      "loss": 1.5545,
      "step": 26140
    },
    {
      "epoch": 1538.235294117647,
      "grad_norm": 19.59864044189453,
      "learning_rate": 3.461764705882353e-05,
      "loss": 1.7715,
      "step": 26150
    },
    {
      "epoch": 1538.8235294117646,
      "grad_norm": 14.726306915283203,
      "learning_rate": 3.4611764705882355e-05,
      "loss": 1.7409,
      "step": 26160
    },
    {
      "epoch": 1539.4117647058824,
      "grad_norm": 18.480859756469727,
      "learning_rate": 3.460588235294118e-05,
      "loss": 1.8196,
      "step": 26170
    },
    {
      "epoch": 1540.0,
      "grad_norm": 17.679542541503906,
      "learning_rate": 3.46e-05,
      "loss": 1.838,
      "step": 26180
    },
    {
      "epoch": 1540.5882352941176,
      "grad_norm": 13.475624084472656,
      "learning_rate": 3.4594117647058824e-05,
      "loss": 1.685,
      "step": 26190
    },
    {
      "epoch": 1541.1764705882354,
      "grad_norm": 12.115907669067383,
      "learning_rate": 3.458823529411765e-05,
      "loss": 1.5944,
      "step": 26200
    },
    {
      "epoch": 1541.764705882353,
      "grad_norm": 14.496781349182129,
      "learning_rate": 3.458235294117647e-05,
      "loss": 1.6008,
      "step": 26210
    },
    {
      "epoch": 1542.3529411764705,
      "grad_norm": 16.427825927734375,
      "learning_rate": 3.457647058823529e-05,
      "loss": 1.6752,
      "step": 26220
    },
    {
      "epoch": 1542.9411764705883,
      "grad_norm": 14.727025985717773,
      "learning_rate": 3.457058823529412e-05,
      "loss": 1.8757,
      "step": 26230
    },
    {
      "epoch": 1543.5294117647059,
      "grad_norm": 13.893214225769043,
      "learning_rate": 3.4564705882352945e-05,
      "loss": 1.7625,
      "step": 26240
    },
    {
      "epoch": 1544.1176470588234,
      "grad_norm": 16.703033447265625,
      "learning_rate": 3.455882352941177e-05,
      "loss": 1.7303,
      "step": 26250
    },
    {
      "epoch": 1544.7058823529412,
      "grad_norm": 14.2412748336792,
      "learning_rate": 3.4552941176470585e-05,
      "loss": 1.6759,
      "step": 26260
    },
    {
      "epoch": 1545.2941176470588,
      "grad_norm": 17.02399444580078,
      "learning_rate": 3.4547058823529414e-05,
      "loss": 1.8127,
      "step": 26270
    },
    {
      "epoch": 1545.8823529411766,
      "grad_norm": 15.739773750305176,
      "learning_rate": 3.454117647058824e-05,
      "loss": 1.6355,
      "step": 26280
    },
    {
      "epoch": 1546.4705882352941,
      "grad_norm": 15.59939956665039,
      "learning_rate": 3.453529411764706e-05,
      "loss": 1.7618,
      "step": 26290
    },
    {
      "epoch": 1547.0588235294117,
      "grad_norm": 11.644620895385742,
      "learning_rate": 3.4529411764705883e-05,
      "loss": 1.7731,
      "step": 26300
    },
    {
      "epoch": 1547.6470588235295,
      "grad_norm": 14.467264175415039,
      "learning_rate": 3.452352941176471e-05,
      "loss": 1.8612,
      "step": 26310
    },
    {
      "epoch": 1548.235294117647,
      "grad_norm": 14.871696472167969,
      "learning_rate": 3.451764705882353e-05,
      "loss": 1.7458,
      "step": 26320
    },
    {
      "epoch": 1548.8235294117646,
      "grad_norm": 11.865276336669922,
      "learning_rate": 3.451176470588235e-05,
      "loss": 1.6174,
      "step": 26330
    },
    {
      "epoch": 1549.4117647058824,
      "grad_norm": 14.787266731262207,
      "learning_rate": 3.4505882352941175e-05,
      "loss": 1.7401,
      "step": 26340
    },
    {
      "epoch": 1550.0,
      "grad_norm": 14.418251037597656,
      "learning_rate": 3.45e-05,
      "loss": 1.7413,
      "step": 26350
    },
    {
      "epoch": 1550.5882352941176,
      "grad_norm": 15.810961723327637,
      "learning_rate": 3.449411764705883e-05,
      "loss": 1.8203,
      "step": 26360
    },
    {
      "epoch": 1551.1764705882354,
      "grad_norm": 15.177878379821777,
      "learning_rate": 3.448823529411765e-05,
      "loss": 1.5514,
      "step": 26370
    },
    {
      "epoch": 1551.764705882353,
      "grad_norm": 12.167708396911621,
      "learning_rate": 3.4482352941176474e-05,
      "loss": 1.572,
      "step": 26380
    },
    {
      "epoch": 1552.3529411764705,
      "grad_norm": 14.451297760009766,
      "learning_rate": 3.447647058823529e-05,
      "loss": 1.8654,
      "step": 26390
    },
    {
      "epoch": 1552.9411764705883,
      "grad_norm": 14.540870666503906,
      "learning_rate": 3.447058823529412e-05,
      "loss": 1.813,
      "step": 26400
    },
    {
      "epoch": 1553.5294117647059,
      "grad_norm": 15.228190422058105,
      "learning_rate": 3.446470588235294e-05,
      "loss": 1.877,
      "step": 26410
    },
    {
      "epoch": 1554.1176470588234,
      "grad_norm": 17.110475540161133,
      "learning_rate": 3.4458823529411766e-05,
      "loss": 1.6655,
      "step": 26420
    },
    {
      "epoch": 1554.7058823529412,
      "grad_norm": 14.458511352539062,
      "learning_rate": 3.445294117647059e-05,
      "loss": 1.7933,
      "step": 26430
    },
    {
      "epoch": 1555.2941176470588,
      "grad_norm": 19.681690216064453,
      "learning_rate": 3.444705882352942e-05,
      "loss": 1.6817,
      "step": 26440
    },
    {
      "epoch": 1555.8823529411766,
      "grad_norm": 12.644691467285156,
      "learning_rate": 3.4441176470588235e-05,
      "loss": 1.5953,
      "step": 26450
    },
    {
      "epoch": 1556.4705882352941,
      "grad_norm": 16.60399627685547,
      "learning_rate": 3.443529411764706e-05,
      "loss": 1.7401,
      "step": 26460
    },
    {
      "epoch": 1557.0588235294117,
      "grad_norm": 14.353065490722656,
      "learning_rate": 3.442941176470588e-05,
      "loss": 1.5269,
      "step": 26470
    },
    {
      "epoch": 1557.6470588235295,
      "grad_norm": 16.351293563842773,
      "learning_rate": 3.442352941176471e-05,
      "loss": 1.6732,
      "step": 26480
    },
    {
      "epoch": 1558.235294117647,
      "grad_norm": 14.460123062133789,
      "learning_rate": 3.4417647058823534e-05,
      "loss": 1.692,
      "step": 26490
    },
    {
      "epoch": 1558.8235294117646,
      "grad_norm": 15.774667739868164,
      "learning_rate": 3.441176470588236e-05,
      "loss": 1.6626,
      "step": 26500
    },
    {
      "epoch": 1559.4117647058824,
      "grad_norm": 14.445869445800781,
      "learning_rate": 3.440588235294118e-05,
      "loss": 1.7901,
      "step": 26510
    },
    {
      "epoch": 1560.0,
      "grad_norm": 14.530320167541504,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 1.7258,
      "step": 26520
    },
    {
      "epoch": 1560.5882352941176,
      "grad_norm": 14.893078804016113,
      "learning_rate": 3.4394117647058826e-05,
      "loss": 1.7684,
      "step": 26530
    },
    {
      "epoch": 1561.1764705882354,
      "grad_norm": 16.263315200805664,
      "learning_rate": 3.438823529411765e-05,
      "loss": 1.8338,
      "step": 26540
    },
    {
      "epoch": 1561.764705882353,
      "grad_norm": 13.654608726501465,
      "learning_rate": 3.438235294117647e-05,
      "loss": 1.6143,
      "step": 26550
    },
    {
      "epoch": 1562.3529411764705,
      "grad_norm": 16.94268226623535,
      "learning_rate": 3.4376470588235295e-05,
      "loss": 1.8289,
      "step": 26560
    },
    {
      "epoch": 1562.9411764705883,
      "grad_norm": 11.980910301208496,
      "learning_rate": 3.4370588235294124e-05,
      "loss": 1.8242,
      "step": 26570
    },
    {
      "epoch": 1563.5294117647059,
      "grad_norm": 20.09008026123047,
      "learning_rate": 3.436470588235294e-05,
      "loss": 1.8344,
      "step": 26580
    },
    {
      "epoch": 1564.1176470588234,
      "grad_norm": 14.983344078063965,
      "learning_rate": 3.4358823529411764e-05,
      "loss": 1.768,
      "step": 26590
    },
    {
      "epoch": 1564.7058823529412,
      "grad_norm": 15.49234676361084,
      "learning_rate": 3.4352941176470587e-05,
      "loss": 1.7997,
      "step": 26600
    },
    {
      "epoch": 1565.2941176470588,
      "grad_norm": 16.28949737548828,
      "learning_rate": 3.4347058823529416e-05,
      "loss": 1.6169,
      "step": 26610
    },
    {
      "epoch": 1565.8823529411766,
      "grad_norm": 15.546060562133789,
      "learning_rate": 3.434117647058824e-05,
      "loss": 1.725,
      "step": 26620
    },
    {
      "epoch": 1566.4705882352941,
      "grad_norm": 15.624185562133789,
      "learning_rate": 3.433529411764706e-05,
      "loss": 1.6383,
      "step": 26630
    },
    {
      "epoch": 1567.0588235294117,
      "grad_norm": 15.12536334991455,
      "learning_rate": 3.4329411764705885e-05,
      "loss": 1.8395,
      "step": 26640
    },
    {
      "epoch": 1567.6470588235295,
      "grad_norm": 14.85607624053955,
      "learning_rate": 3.432352941176471e-05,
      "loss": 1.6964,
      "step": 26650
    },
    {
      "epoch": 1568.235294117647,
      "grad_norm": 16.48522186279297,
      "learning_rate": 3.431764705882353e-05,
      "loss": 1.7577,
      "step": 26660
    },
    {
      "epoch": 1568.8235294117646,
      "grad_norm": 15.210750579833984,
      "learning_rate": 3.4311764705882354e-05,
      "loss": 1.6459,
      "step": 26670
    },
    {
      "epoch": 1569.4117647058824,
      "grad_norm": 13.730278968811035,
      "learning_rate": 3.430588235294118e-05,
      "loss": 1.6766,
      "step": 26680
    },
    {
      "epoch": 1570.0,
      "grad_norm": 20.62847137451172,
      "learning_rate": 3.430000000000001e-05,
      "loss": 1.719,
      "step": 26690
    },
    {
      "epoch": 1570.5882352941176,
      "grad_norm": 15.974223136901855,
      "learning_rate": 3.429411764705882e-05,
      "loss": 1.6715,
      "step": 26700
    },
    {
      "epoch": 1571.1764705882354,
      "grad_norm": 17.17926597595215,
      "learning_rate": 3.4288235294117646e-05,
      "loss": 1.6978,
      "step": 26710
    },
    {
      "epoch": 1571.764705882353,
      "grad_norm": 16.917999267578125,
      "learning_rate": 3.428235294117647e-05,
      "loss": 1.7837,
      "step": 26720
    },
    {
      "epoch": 1572.3529411764705,
      "grad_norm": 13.359847068786621,
      "learning_rate": 3.427647058823529e-05,
      "loss": 1.7148,
      "step": 26730
    },
    {
      "epoch": 1572.9411764705883,
      "grad_norm": 14.098640441894531,
      "learning_rate": 3.427058823529412e-05,
      "loss": 1.6551,
      "step": 26740
    },
    {
      "epoch": 1573.5294117647059,
      "grad_norm": 12.93938159942627,
      "learning_rate": 3.4264705882352945e-05,
      "loss": 1.6877,
      "step": 26750
    },
    {
      "epoch": 1574.1176470588234,
      "grad_norm": 14.295278549194336,
      "learning_rate": 3.425882352941177e-05,
      "loss": 1.7425,
      "step": 26760
    },
    {
      "epoch": 1574.7058823529412,
      "grad_norm": 15.60519027709961,
      "learning_rate": 3.425294117647059e-05,
      "loss": 1.5833,
      "step": 26770
    },
    {
      "epoch": 1575.2941176470588,
      "grad_norm": 10.036102294921875,
      "learning_rate": 3.4247058823529414e-05,
      "loss": 1.6584,
      "step": 26780
    },
    {
      "epoch": 1575.8823529411766,
      "grad_norm": 16.917652130126953,
      "learning_rate": 3.424117647058824e-05,
      "loss": 1.8197,
      "step": 26790
    },
    {
      "epoch": 1576.4705882352941,
      "grad_norm": 14.337342262268066,
      "learning_rate": 3.423529411764706e-05,
      "loss": 1.7151,
      "step": 26800
    },
    {
      "epoch": 1577.0588235294117,
      "grad_norm": 17.597436904907227,
      "learning_rate": 3.422941176470588e-05,
      "loss": 1.6828,
      "step": 26810
    },
    {
      "epoch": 1577.6470588235295,
      "grad_norm": 16.62872314453125,
      "learning_rate": 3.422352941176471e-05,
      "loss": 1.6574,
      "step": 26820
    },
    {
      "epoch": 1578.235294117647,
      "grad_norm": 15.47134780883789,
      "learning_rate": 3.421764705882353e-05,
      "loss": 1.7271,
      "step": 26830
    },
    {
      "epoch": 1578.8235294117646,
      "grad_norm": 16.589292526245117,
      "learning_rate": 3.421176470588235e-05,
      "loss": 1.797,
      "step": 26840
    },
    {
      "epoch": 1579.4117647058824,
      "grad_norm": 14.845207214355469,
      "learning_rate": 3.4205882352941175e-05,
      "loss": 1.6039,
      "step": 26850
    },
    {
      "epoch": 1580.0,
      "grad_norm": 17.939456939697266,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 1.6946,
      "step": 26860
    },
    {
      "epoch": 1580.5882352941176,
      "grad_norm": 18.068435668945312,
      "learning_rate": 3.419411764705883e-05,
      "loss": 1.6419,
      "step": 26870
    },
    {
      "epoch": 1581.1764705882354,
      "grad_norm": 14.954469680786133,
      "learning_rate": 3.418823529411765e-05,
      "loss": 1.6046,
      "step": 26880
    },
    {
      "epoch": 1581.764705882353,
      "grad_norm": 12.785722732543945,
      "learning_rate": 3.4182352941176474e-05,
      "loss": 1.6755,
      "step": 26890
    },
    {
      "epoch": 1582.3529411764705,
      "grad_norm": 13.519923210144043,
      "learning_rate": 3.417647058823529e-05,
      "loss": 1.781,
      "step": 26900
    },
    {
      "epoch": 1582.9411764705883,
      "grad_norm": 16.16637420654297,
      "learning_rate": 3.417058823529412e-05,
      "loss": 1.7255,
      "step": 26910
    },
    {
      "epoch": 1583.5294117647059,
      "grad_norm": 18.02170181274414,
      "learning_rate": 3.416470588235294e-05,
      "loss": 1.7693,
      "step": 26920
    },
    {
      "epoch": 1584.1176470588234,
      "grad_norm": 16.45232582092285,
      "learning_rate": 3.4158823529411766e-05,
      "loss": 1.7737,
      "step": 26930
    },
    {
      "epoch": 1584.7058823529412,
      "grad_norm": 17.377239227294922,
      "learning_rate": 3.415294117647059e-05,
      "loss": 1.6054,
      "step": 26940
    },
    {
      "epoch": 1585.2941176470588,
      "grad_norm": 17.829456329345703,
      "learning_rate": 3.414705882352942e-05,
      "loss": 1.7717,
      "step": 26950
    },
    {
      "epoch": 1585.8823529411766,
      "grad_norm": 14.990917205810547,
      "learning_rate": 3.4141176470588234e-05,
      "loss": 1.7638,
      "step": 26960
    },
    {
      "epoch": 1586.4705882352941,
      "grad_norm": 18.163965225219727,
      "learning_rate": 3.413529411764706e-05,
      "loss": 1.7765,
      "step": 26970
    },
    {
      "epoch": 1587.0588235294117,
      "grad_norm": 13.00649356842041,
      "learning_rate": 3.412941176470588e-05,
      "loss": 1.6411,
      "step": 26980
    },
    {
      "epoch": 1587.6470588235295,
      "grad_norm": 13.485749244689941,
      "learning_rate": 3.412352941176471e-05,
      "loss": 1.5302,
      "step": 26990
    },
    {
      "epoch": 1588.235294117647,
      "grad_norm": 15.829303741455078,
      "learning_rate": 3.411764705882353e-05,
      "loss": 1.6724,
      "step": 27000
    },
    {
      "epoch": 1588.8235294117646,
      "grad_norm": 14.937854766845703,
      "learning_rate": 3.4111764705882356e-05,
      "loss": 1.6512,
      "step": 27010
    },
    {
      "epoch": 1589.4117647058824,
      "grad_norm": 17.574317932128906,
      "learning_rate": 3.410588235294118e-05,
      "loss": 1.8269,
      "step": 27020
    },
    {
      "epoch": 1590.0,
      "grad_norm": 16.77802848815918,
      "learning_rate": 3.41e-05,
      "loss": 1.7009,
      "step": 27030
    },
    {
      "epoch": 1590.5882352941176,
      "grad_norm": 14.723126411437988,
      "learning_rate": 3.4094117647058825e-05,
      "loss": 1.6544,
      "step": 27040
    },
    {
      "epoch": 1591.1764705882354,
      "grad_norm": 12.487137794494629,
      "learning_rate": 3.408823529411765e-05,
      "loss": 1.6917,
      "step": 27050
    },
    {
      "epoch": 1591.764705882353,
      "grad_norm": 15.400541305541992,
      "learning_rate": 3.408235294117647e-05,
      "loss": 1.8222,
      "step": 27060
    },
    {
      "epoch": 1592.3529411764705,
      "grad_norm": 17.401287078857422,
      "learning_rate": 3.4076470588235294e-05,
      "loss": 1.7781,
      "step": 27070
    },
    {
      "epoch": 1592.9411764705883,
      "grad_norm": 14.543831825256348,
      "learning_rate": 3.4070588235294124e-05,
      "loss": 1.711,
      "step": 27080
    },
    {
      "epoch": 1593.5294117647059,
      "grad_norm": 15.338516235351562,
      "learning_rate": 3.406470588235294e-05,
      "loss": 1.7775,
      "step": 27090
    },
    {
      "epoch": 1594.1176470588234,
      "grad_norm": 14.88813304901123,
      "learning_rate": 3.405882352941176e-05,
      "loss": 1.6332,
      "step": 27100
    },
    {
      "epoch": 1594.7058823529412,
      "grad_norm": 11.923986434936523,
      "learning_rate": 3.4052941176470586e-05,
      "loss": 1.7673,
      "step": 27110
    },
    {
      "epoch": 1595.2941176470588,
      "grad_norm": 16.081689834594727,
      "learning_rate": 3.4047058823529416e-05,
      "loss": 1.7211,
      "step": 27120
    },
    {
      "epoch": 1595.8823529411766,
      "grad_norm": 17.36713981628418,
      "learning_rate": 3.404117647058824e-05,
      "loss": 1.8043,
      "step": 27130
    },
    {
      "epoch": 1596.4705882352941,
      "grad_norm": 15.621728897094727,
      "learning_rate": 3.403529411764706e-05,
      "loss": 1.697,
      "step": 27140
    },
    {
      "epoch": 1597.0588235294117,
      "grad_norm": 18.838857650756836,
      "learning_rate": 3.4029411764705885e-05,
      "loss": 1.7638,
      "step": 27150
    },
    {
      "epoch": 1597.6470588235295,
      "grad_norm": 18.31058692932129,
      "learning_rate": 3.402352941176471e-05,
      "loss": 1.8015,
      "step": 27160
    },
    {
      "epoch": 1598.235294117647,
      "grad_norm": 16.397308349609375,
      "learning_rate": 3.401764705882353e-05,
      "loss": 1.7177,
      "step": 27170
    },
    {
      "epoch": 1598.8235294117646,
      "grad_norm": 14.454851150512695,
      "learning_rate": 3.4011764705882354e-05,
      "loss": 1.6364,
      "step": 27180
    },
    {
      "epoch": 1599.4117647058824,
      "grad_norm": 18.083332061767578,
      "learning_rate": 3.400588235294118e-05,
      "loss": 1.5992,
      "step": 27190
    },
    {
      "epoch": 1600.0,
      "grad_norm": 19.32710075378418,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.8008,
      "step": 27200
    },
    {
      "epoch": 1600.5882352941176,
      "grad_norm": 14.924423217773438,
      "learning_rate": 3.399411764705883e-05,
      "loss": 1.7425,
      "step": 27210
    },
    {
      "epoch": 1601.1764705882354,
      "grad_norm": 15.464178085327148,
      "learning_rate": 3.3988235294117646e-05,
      "loss": 1.6688,
      "step": 27220
    },
    {
      "epoch": 1601.764705882353,
      "grad_norm": 14.146198272705078,
      "learning_rate": 3.398235294117647e-05,
      "loss": 1.7137,
      "step": 27230
    },
    {
      "epoch": 1602.3529411764705,
      "grad_norm": 13.43579387664795,
      "learning_rate": 3.39764705882353e-05,
      "loss": 1.711,
      "step": 27240
    },
    {
      "epoch": 1602.9411764705883,
      "grad_norm": 16.493457794189453,
      "learning_rate": 3.397058823529412e-05,
      "loss": 1.7066,
      "step": 27250
    },
    {
      "epoch": 1603.5294117647059,
      "grad_norm": 14.732352256774902,
      "learning_rate": 3.3964705882352944e-05,
      "loss": 1.7708,
      "step": 27260
    },
    {
      "epoch": 1604.1176470588234,
      "grad_norm": 15.528922080993652,
      "learning_rate": 3.395882352941177e-05,
      "loss": 1.632,
      "step": 27270
    },
    {
      "epoch": 1604.7058823529412,
      "grad_norm": 18.28770637512207,
      "learning_rate": 3.395294117647059e-05,
      "loss": 1.6519,
      "step": 27280
    },
    {
      "epoch": 1605.2941176470588,
      "grad_norm": 12.934954643249512,
      "learning_rate": 3.3947058823529413e-05,
      "loss": 1.6028,
      "step": 27290
    },
    {
      "epoch": 1605.8823529411766,
      "grad_norm": 16.27779769897461,
      "learning_rate": 3.3941176470588236e-05,
      "loss": 1.6245,
      "step": 27300
    },
    {
      "epoch": 1606.4705882352941,
      "grad_norm": 18.40718650817871,
      "learning_rate": 3.393529411764706e-05,
      "loss": 1.6487,
      "step": 27310
    },
    {
      "epoch": 1607.0588235294117,
      "grad_norm": 14.71866226196289,
      "learning_rate": 3.392941176470588e-05,
      "loss": 1.7431,
      "step": 27320
    },
    {
      "epoch": 1607.6470588235295,
      "grad_norm": 14.521716117858887,
      "learning_rate": 3.392352941176471e-05,
      "loss": 1.6343,
      "step": 27330
    },
    {
      "epoch": 1608.235294117647,
      "grad_norm": 14.878134727478027,
      "learning_rate": 3.3917647058823535e-05,
      "loss": 1.7223,
      "step": 27340
    },
    {
      "epoch": 1608.8235294117646,
      "grad_norm": 15.49095344543457,
      "learning_rate": 3.391176470588235e-05,
      "loss": 1.8034,
      "step": 27350
    },
    {
      "epoch": 1609.4117647058824,
      "grad_norm": 13.202213287353516,
      "learning_rate": 3.3905882352941174e-05,
      "loss": 1.5993,
      "step": 27360
    },
    {
      "epoch": 1610.0,
      "grad_norm": 15.579830169677734,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 1.7125,
      "step": 27370
    },
    {
      "epoch": 1610.5882352941176,
      "grad_norm": 13.256224632263184,
      "learning_rate": 3.389411764705883e-05,
      "loss": 1.6206,
      "step": 27380
    },
    {
      "epoch": 1611.1764705882354,
      "grad_norm": 15.657977104187012,
      "learning_rate": 3.388823529411765e-05,
      "loss": 1.6497,
      "step": 27390
    },
    {
      "epoch": 1611.764705882353,
      "grad_norm": 17.68763542175293,
      "learning_rate": 3.388235294117647e-05,
      "loss": 1.6618,
      "step": 27400
    },
    {
      "epoch": 1612.3529411764705,
      "grad_norm": 15.138903617858887,
      "learning_rate": 3.3876470588235296e-05,
      "loss": 1.8485,
      "step": 27410
    },
    {
      "epoch": 1612.9411764705883,
      "grad_norm": 18.3171443939209,
      "learning_rate": 3.387058823529412e-05,
      "loss": 1.6708,
      "step": 27420
    },
    {
      "epoch": 1613.5294117647059,
      "grad_norm": 15.584707260131836,
      "learning_rate": 3.386470588235294e-05,
      "loss": 1.7219,
      "step": 27430
    },
    {
      "epoch": 1614.1176470588234,
      "grad_norm": 16.23143196105957,
      "learning_rate": 3.3858823529411765e-05,
      "loss": 1.7465,
      "step": 27440
    },
    {
      "epoch": 1614.7058823529412,
      "grad_norm": 13.391483306884766,
      "learning_rate": 3.385294117647059e-05,
      "loss": 1.7054,
      "step": 27450
    },
    {
      "epoch": 1615.2941176470588,
      "grad_norm": 12.552010536193848,
      "learning_rate": 3.384705882352942e-05,
      "loss": 1.6204,
      "step": 27460
    },
    {
      "epoch": 1615.8823529411766,
      "grad_norm": 21.454757690429688,
      "learning_rate": 3.3841176470588234e-05,
      "loss": 1.7391,
      "step": 27470
    },
    {
      "epoch": 1616.4705882352941,
      "grad_norm": 18.427574157714844,
      "learning_rate": 3.383529411764706e-05,
      "loss": 1.7061,
      "step": 27480
    },
    {
      "epoch": 1617.0588235294117,
      "grad_norm": 12.158073425292969,
      "learning_rate": 3.382941176470588e-05,
      "loss": 1.7666,
      "step": 27490
    },
    {
      "epoch": 1617.6470588235295,
      "grad_norm": 15.587129592895508,
      "learning_rate": 3.382352941176471e-05,
      "loss": 1.695,
      "step": 27500
    },
    {
      "epoch": 1618.235294117647,
      "grad_norm": 11.946342468261719,
      "learning_rate": 3.381764705882353e-05,
      "loss": 1.4995,
      "step": 27510
    },
    {
      "epoch": 1618.8235294117646,
      "grad_norm": 15.28387451171875,
      "learning_rate": 3.3811764705882356e-05,
      "loss": 1.7172,
      "step": 27520
    },
    {
      "epoch": 1619.4117647058824,
      "grad_norm": 13.721275329589844,
      "learning_rate": 3.380588235294118e-05,
      "loss": 1.6582,
      "step": 27530
    },
    {
      "epoch": 1620.0,
      "grad_norm": 20.317527770996094,
      "learning_rate": 3.38e-05,
      "loss": 1.5155,
      "step": 27540
    },
    {
      "epoch": 1620.5882352941176,
      "grad_norm": 16.586397171020508,
      "learning_rate": 3.3794117647058825e-05,
      "loss": 1.7035,
      "step": 27550
    },
    {
      "epoch": 1621.1764705882354,
      "grad_norm": 19.748493194580078,
      "learning_rate": 3.378823529411765e-05,
      "loss": 1.6044,
      "step": 27560
    },
    {
      "epoch": 1621.764705882353,
      "grad_norm": 15.565264701843262,
      "learning_rate": 3.378235294117647e-05,
      "loss": 1.6073,
      "step": 27570
    },
    {
      "epoch": 1622.3529411764705,
      "grad_norm": 13.883384704589844,
      "learning_rate": 3.37764705882353e-05,
      "loss": 1.6159,
      "step": 27580
    },
    {
      "epoch": 1622.9411764705883,
      "grad_norm": 17.678367614746094,
      "learning_rate": 3.377058823529412e-05,
      "loss": 1.6706,
      "step": 27590
    },
    {
      "epoch": 1623.5294117647059,
      "grad_norm": 12.617807388305664,
      "learning_rate": 3.376470588235294e-05,
      "loss": 1.6394,
      "step": 27600
    },
    {
      "epoch": 1624.1176470588234,
      "grad_norm": 17.387935638427734,
      "learning_rate": 3.375882352941176e-05,
      "loss": 1.7462,
      "step": 27610
    },
    {
      "epoch": 1624.7058823529412,
      "grad_norm": 18.67819595336914,
      "learning_rate": 3.3752941176470586e-05,
      "loss": 1.7192,
      "step": 27620
    },
    {
      "epoch": 1625.2941176470588,
      "grad_norm": 15.023439407348633,
      "learning_rate": 3.3747058823529415e-05,
      "loss": 1.6219,
      "step": 27630
    },
    {
      "epoch": 1625.8823529411766,
      "grad_norm": 16.33765411376953,
      "learning_rate": 3.374117647058824e-05,
      "loss": 1.7152,
      "step": 27640
    },
    {
      "epoch": 1626.4705882352941,
      "grad_norm": 16.662708282470703,
      "learning_rate": 3.373529411764706e-05,
      "loss": 1.6819,
      "step": 27650
    },
    {
      "epoch": 1627.0588235294117,
      "grad_norm": 15.501580238342285,
      "learning_rate": 3.3729411764705884e-05,
      "loss": 1.6579,
      "step": 27660
    },
    {
      "epoch": 1627.6470588235295,
      "grad_norm": 13.643531799316406,
      "learning_rate": 3.372352941176471e-05,
      "loss": 1.7052,
      "step": 27670
    },
    {
      "epoch": 1628.235294117647,
      "grad_norm": 16.473543167114258,
      "learning_rate": 3.371764705882353e-05,
      "loss": 1.7138,
      "step": 27680
    },
    {
      "epoch": 1628.8235294117646,
      "grad_norm": 14.369729995727539,
      "learning_rate": 3.371176470588235e-05,
      "loss": 1.6191,
      "step": 27690
    },
    {
      "epoch": 1629.4117647058824,
      "grad_norm": 18.150175094604492,
      "learning_rate": 3.3705882352941176e-05,
      "loss": 1.6697,
      "step": 27700
    },
    {
      "epoch": 1630.0,
      "grad_norm": 17.148860931396484,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 1.5932,
      "step": 27710
    },
    {
      "epoch": 1630.5882352941176,
      "grad_norm": 14.037652969360352,
      "learning_rate": 3.369411764705883e-05,
      "loss": 1.5303,
      "step": 27720
    },
    {
      "epoch": 1631.1764705882354,
      "grad_norm": 12.143607139587402,
      "learning_rate": 3.3688235294117645e-05,
      "loss": 1.5409,
      "step": 27730
    },
    {
      "epoch": 1631.764705882353,
      "grad_norm": 14.140030860900879,
      "learning_rate": 3.368235294117647e-05,
      "loss": 1.5971,
      "step": 27740
    },
    {
      "epoch": 1632.3529411764705,
      "grad_norm": 13.592121124267578,
      "learning_rate": 3.36764705882353e-05,
      "loss": 1.6798,
      "step": 27750
    },
    {
      "epoch": 1632.9411764705883,
      "grad_norm": 20.483110427856445,
      "learning_rate": 3.367058823529412e-05,
      "loss": 1.7502,
      "step": 27760
    },
    {
      "epoch": 1633.5294117647059,
      "grad_norm": 15.490023612976074,
      "learning_rate": 3.3664705882352944e-05,
      "loss": 1.8126,
      "step": 27770
    },
    {
      "epoch": 1634.1176470588234,
      "grad_norm": 13.897242546081543,
      "learning_rate": 3.365882352941177e-05,
      "loss": 1.7712,
      "step": 27780
    },
    {
      "epoch": 1634.7058823529412,
      "grad_norm": 13.590271949768066,
      "learning_rate": 3.365294117647059e-05,
      "loss": 1.6605,
      "step": 27790
    },
    {
      "epoch": 1635.2941176470588,
      "grad_norm": 12.094223022460938,
      "learning_rate": 3.364705882352941e-05,
      "loss": 1.5754,
      "step": 27800
    },
    {
      "epoch": 1635.8823529411766,
      "grad_norm": 16.52745246887207,
      "learning_rate": 3.3641176470588236e-05,
      "loss": 1.6266,
      "step": 27810
    },
    {
      "epoch": 1636.4705882352941,
      "grad_norm": 17.515544891357422,
      "learning_rate": 3.363529411764706e-05,
      "loss": 1.6942,
      "step": 27820
    },
    {
      "epoch": 1637.0588235294117,
      "grad_norm": 17.09905242919922,
      "learning_rate": 3.362941176470588e-05,
      "loss": 1.5977,
      "step": 27830
    },
    {
      "epoch": 1637.6470588235295,
      "grad_norm": 16.62407112121582,
      "learning_rate": 3.362352941176471e-05,
      "loss": 1.7051,
      "step": 27840
    },
    {
      "epoch": 1638.235294117647,
      "grad_norm": 14.023420333862305,
      "learning_rate": 3.3617647058823535e-05,
      "loss": 1.6995,
      "step": 27850
    },
    {
      "epoch": 1638.8235294117646,
      "grad_norm": 16.784854888916016,
      "learning_rate": 3.361176470588235e-05,
      "loss": 1.6382,
      "step": 27860
    },
    {
      "epoch": 1639.4117647058824,
      "grad_norm": 15.7844877243042,
      "learning_rate": 3.3605882352941174e-05,
      "loss": 1.5831,
      "step": 27870
    },
    {
      "epoch": 1640.0,
      "grad_norm": 17.80034065246582,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 1.7402,
      "step": 27880
    },
    {
      "epoch": 1640.5882352941176,
      "grad_norm": 16.882322311401367,
      "learning_rate": 3.3594117647058827e-05,
      "loss": 1.5856,
      "step": 27890
    },
    {
      "epoch": 1641.1764705882354,
      "grad_norm": 14.96129322052002,
      "learning_rate": 3.358823529411765e-05,
      "loss": 1.5739,
      "step": 27900
    },
    {
      "epoch": 1641.764705882353,
      "grad_norm": 14.729231834411621,
      "learning_rate": 3.358235294117647e-05,
      "loss": 1.536,
      "step": 27910
    },
    {
      "epoch": 1642.3529411764705,
      "grad_norm": 16.290483474731445,
      "learning_rate": 3.3576470588235296e-05,
      "loss": 1.6611,
      "step": 27920
    },
    {
      "epoch": 1642.9411764705883,
      "grad_norm": 14.943546295166016,
      "learning_rate": 3.357058823529412e-05,
      "loss": 1.7431,
      "step": 27930
    },
    {
      "epoch": 1643.5294117647059,
      "grad_norm": 14.215536117553711,
      "learning_rate": 3.356470588235294e-05,
      "loss": 1.6425,
      "step": 27940
    },
    {
      "epoch": 1644.1176470588234,
      "grad_norm": 14.673896789550781,
      "learning_rate": 3.3558823529411764e-05,
      "loss": 1.5683,
      "step": 27950
    },
    {
      "epoch": 1644.7058823529412,
      "grad_norm": 17.70771598815918,
      "learning_rate": 3.3552941176470594e-05,
      "loss": 1.6049,
      "step": 27960
    },
    {
      "epoch": 1645.2941176470588,
      "grad_norm": 14.693060874938965,
      "learning_rate": 3.354705882352942e-05,
      "loss": 1.6114,
      "step": 27970
    },
    {
      "epoch": 1645.8823529411766,
      "grad_norm": 13.359416007995605,
      "learning_rate": 3.354117647058824e-05,
      "loss": 1.6053,
      "step": 27980
    },
    {
      "epoch": 1646.4705882352941,
      "grad_norm": 17.041297912597656,
      "learning_rate": 3.3535294117647056e-05,
      "loss": 1.6673,
      "step": 27990
    },
    {
      "epoch": 1647.0588235294117,
      "grad_norm": 13.070966720581055,
      "learning_rate": 3.352941176470588e-05,
      "loss": 1.6572,
      "step": 28000
    },
    {
      "epoch": 1647.6470588235295,
      "grad_norm": 13.485944747924805,
      "learning_rate": 3.352352941176471e-05,
      "loss": 1.6013,
      "step": 28010
    },
    {
      "epoch": 1648.235294117647,
      "grad_norm": 12.367331504821777,
      "learning_rate": 3.351764705882353e-05,
      "loss": 1.5923,
      "step": 28020
    },
    {
      "epoch": 1648.8235294117646,
      "grad_norm": 14.819018363952637,
      "learning_rate": 3.3511764705882355e-05,
      "loss": 1.68,
      "step": 28030
    },
    {
      "epoch": 1649.4117647058824,
      "grad_norm": 15.462610244750977,
      "learning_rate": 3.350588235294118e-05,
      "loss": 1.6128,
      "step": 28040
    },
    {
      "epoch": 1650.0,
      "grad_norm": 17.54456329345703,
      "learning_rate": 3.35e-05,
      "loss": 1.759,
      "step": 28050
    },
    {
      "epoch": 1650.5882352941176,
      "grad_norm": 19.331932067871094,
      "learning_rate": 3.3494117647058824e-05,
      "loss": 1.6508,
      "step": 28060
    },
    {
      "epoch": 1651.1764705882354,
      "grad_norm": 13.80168342590332,
      "learning_rate": 3.348823529411765e-05,
      "loss": 1.5168,
      "step": 28070
    },
    {
      "epoch": 1651.764705882353,
      "grad_norm": 15.641789436340332,
      "learning_rate": 3.348235294117647e-05,
      "loss": 1.703,
      "step": 28080
    },
    {
      "epoch": 1652.3529411764705,
      "grad_norm": 15.288338661193848,
      "learning_rate": 3.34764705882353e-05,
      "loss": 1.6885,
      "step": 28090
    },
    {
      "epoch": 1652.9411764705883,
      "grad_norm": 15.33580207824707,
      "learning_rate": 3.347058823529412e-05,
      "loss": 1.6929,
      "step": 28100
    },
    {
      "epoch": 1653.5294117647059,
      "grad_norm": 16.062074661254883,
      "learning_rate": 3.3464705882352946e-05,
      "loss": 1.6244,
      "step": 28110
    },
    {
      "epoch": 1654.1176470588234,
      "grad_norm": 16.905555725097656,
      "learning_rate": 3.345882352941176e-05,
      "loss": 1.7158,
      "step": 28120
    },
    {
      "epoch": 1654.7058823529412,
      "grad_norm": 17.139968872070312,
      "learning_rate": 3.345294117647059e-05,
      "loss": 1.5403,
      "step": 28130
    },
    {
      "epoch": 1655.2941176470588,
      "grad_norm": 13.06469440460205,
      "learning_rate": 3.3447058823529415e-05,
      "loss": 1.5177,
      "step": 28140
    },
    {
      "epoch": 1655.8823529411766,
      "grad_norm": 17.738889694213867,
      "learning_rate": 3.344117647058824e-05,
      "loss": 1.6334,
      "step": 28150
    },
    {
      "epoch": 1656.4705882352941,
      "grad_norm": 11.980423927307129,
      "learning_rate": 3.343529411764706e-05,
      "loss": 1.6973,
      "step": 28160
    },
    {
      "epoch": 1657.0588235294117,
      "grad_norm": 14.880091667175293,
      "learning_rate": 3.3429411764705884e-05,
      "loss": 1.6396,
      "step": 28170
    },
    {
      "epoch": 1657.6470588235295,
      "grad_norm": 15.679051399230957,
      "learning_rate": 3.342352941176471e-05,
      "loss": 1.5849,
      "step": 28180
    },
    {
      "epoch": 1658.235294117647,
      "grad_norm": 18.24298858642578,
      "learning_rate": 3.341764705882353e-05,
      "loss": 1.6849,
      "step": 28190
    },
    {
      "epoch": 1658.8235294117646,
      "grad_norm": 16.92922592163086,
      "learning_rate": 3.341176470588235e-05,
      "loss": 1.6757,
      "step": 28200
    },
    {
      "epoch": 1659.4117647058824,
      "grad_norm": 17.681570053100586,
      "learning_rate": 3.3405882352941176e-05,
      "loss": 1.6504,
      "step": 28210
    },
    {
      "epoch": 1660.0,
      "grad_norm": 18.619060516357422,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 1.6503,
      "step": 28220
    },
    {
      "epoch": 1660.5882352941176,
      "grad_norm": 14.538798332214355,
      "learning_rate": 3.339411764705883e-05,
      "loss": 1.6761,
      "step": 28230
    },
    {
      "epoch": 1661.1764705882354,
      "grad_norm": 14.067826271057129,
      "learning_rate": 3.3388235294117645e-05,
      "loss": 1.5229,
      "step": 28240
    },
    {
      "epoch": 1661.764705882353,
      "grad_norm": 12.218356132507324,
      "learning_rate": 3.338235294117647e-05,
      "loss": 1.5378,
      "step": 28250
    },
    {
      "epoch": 1662.3529411764705,
      "grad_norm": 19.820295333862305,
      "learning_rate": 3.33764705882353e-05,
      "loss": 1.57,
      "step": 28260
    },
    {
      "epoch": 1662.9411764705883,
      "grad_norm": 15.81490421295166,
      "learning_rate": 3.337058823529412e-05,
      "loss": 1.6469,
      "step": 28270
    },
    {
      "epoch": 1663.5294117647059,
      "grad_norm": 21.397600173950195,
      "learning_rate": 3.3364705882352943e-05,
      "loss": 1.491,
      "step": 28280
    },
    {
      "epoch": 1664.1176470588234,
      "grad_norm": 15.071565628051758,
      "learning_rate": 3.3358823529411766e-05,
      "loss": 1.7149,
      "step": 28290
    },
    {
      "epoch": 1664.7058823529412,
      "grad_norm": 16.022981643676758,
      "learning_rate": 3.335294117647059e-05,
      "loss": 1.664,
      "step": 28300
    },
    {
      "epoch": 1665.2941176470588,
      "grad_norm": 15.45783805847168,
      "learning_rate": 3.334705882352941e-05,
      "loss": 1.5974,
      "step": 28310
    },
    {
      "epoch": 1665.8823529411766,
      "grad_norm": 18.564865112304688,
      "learning_rate": 3.3341176470588235e-05,
      "loss": 1.7269,
      "step": 28320
    },
    {
      "epoch": 1666.4705882352941,
      "grad_norm": 17.58980941772461,
      "learning_rate": 3.333529411764706e-05,
      "loss": 1.7502,
      "step": 28330
    },
    {
      "epoch": 1667.0588235294117,
      "grad_norm": 15.61136531829834,
      "learning_rate": 3.332941176470588e-05,
      "loss": 1.565,
      "step": 28340
    },
    {
      "epoch": 1667.6470588235295,
      "grad_norm": 12.334218978881836,
      "learning_rate": 3.332352941176471e-05,
      "loss": 1.5908,
      "step": 28350
    },
    {
      "epoch": 1668.235294117647,
      "grad_norm": 15.6768798828125,
      "learning_rate": 3.3317647058823534e-05,
      "loss": 1.6552,
      "step": 28360
    },
    {
      "epoch": 1668.8235294117646,
      "grad_norm": 16.10577392578125,
      "learning_rate": 3.331176470588235e-05,
      "loss": 1.6885,
      "step": 28370
    },
    {
      "epoch": 1669.4117647058824,
      "grad_norm": 16.76653289794922,
      "learning_rate": 3.330588235294117e-05,
      "loss": 1.5267,
      "step": 28380
    },
    {
      "epoch": 1670.0,
      "grad_norm": 19.15654945373535,
      "learning_rate": 3.33e-05,
      "loss": 1.7523,
      "step": 28390
    },
    {
      "epoch": 1670.5882352941176,
      "grad_norm": 13.12744140625,
      "learning_rate": 3.3294117647058826e-05,
      "loss": 1.7436,
      "step": 28400
    },
    {
      "epoch": 1671.1764705882354,
      "grad_norm": 14.731158256530762,
      "learning_rate": 3.328823529411765e-05,
      "loss": 1.5989,
      "step": 28410
    },
    {
      "epoch": 1671.764705882353,
      "grad_norm": 12.508108139038086,
      "learning_rate": 3.328235294117647e-05,
      "loss": 1.5639,
      "step": 28420
    },
    {
      "epoch": 1672.3529411764705,
      "grad_norm": 13.673603057861328,
      "learning_rate": 3.3276470588235295e-05,
      "loss": 1.7233,
      "step": 28430
    },
    {
      "epoch": 1672.9411764705883,
      "grad_norm": 14.454371452331543,
      "learning_rate": 3.327058823529412e-05,
      "loss": 1.5499,
      "step": 28440
    },
    {
      "epoch": 1673.5294117647059,
      "grad_norm": 16.131826400756836,
      "learning_rate": 3.326470588235294e-05,
      "loss": 1.5311,
      "step": 28450
    },
    {
      "epoch": 1674.1176470588234,
      "grad_norm": 14.559745788574219,
      "learning_rate": 3.3258823529411764e-05,
      "loss": 1.6868,
      "step": 28460
    },
    {
      "epoch": 1674.7058823529412,
      "grad_norm": 14.242931365966797,
      "learning_rate": 3.3252941176470594e-05,
      "loss": 1.6652,
      "step": 28470
    },
    {
      "epoch": 1675.2941176470588,
      "grad_norm": 14.779168128967285,
      "learning_rate": 3.324705882352942e-05,
      "loss": 1.5968,
      "step": 28480
    },
    {
      "epoch": 1675.8823529411766,
      "grad_norm": 16.043611526489258,
      "learning_rate": 3.324117647058824e-05,
      "loss": 1.6474,
      "step": 28490
    },
    {
      "epoch": 1676.4705882352941,
      "grad_norm": 14.735197067260742,
      "learning_rate": 3.3235294117647056e-05,
      "loss": 1.7035,
      "step": 28500
    },
    {
      "epoch": 1677.0588235294117,
      "grad_norm": 16.48112678527832,
      "learning_rate": 3.3229411764705886e-05,
      "loss": 1.5786,
      "step": 28510
    },
    {
      "epoch": 1677.6470588235295,
      "grad_norm": 15.74503231048584,
      "learning_rate": 3.322352941176471e-05,
      "loss": 1.6881,
      "step": 28520
    },
    {
      "epoch": 1678.235294117647,
      "grad_norm": 17.418174743652344,
      "learning_rate": 3.321764705882353e-05,
      "loss": 1.6415,
      "step": 28530
    },
    {
      "epoch": 1678.8235294117646,
      "grad_norm": 17.458532333374023,
      "learning_rate": 3.3211764705882355e-05,
      "loss": 1.6729,
      "step": 28540
    },
    {
      "epoch": 1679.4117647058824,
      "grad_norm": 20.494890213012695,
      "learning_rate": 3.320588235294118e-05,
      "loss": 1.5956,
      "step": 28550
    },
    {
      "epoch": 1680.0,
      "grad_norm": 17.40886688232422,
      "learning_rate": 3.32e-05,
      "loss": 1.5743,
      "step": 28560
    },
    {
      "epoch": 1680.5882352941176,
      "grad_norm": 13.422760009765625,
      "learning_rate": 3.3194117647058824e-05,
      "loss": 1.69,
      "step": 28570
    },
    {
      "epoch": 1681.1764705882354,
      "grad_norm": 16.07931900024414,
      "learning_rate": 3.3188235294117647e-05,
      "loss": 1.6342,
      "step": 28580
    },
    {
      "epoch": 1681.764705882353,
      "grad_norm": 18.400846481323242,
      "learning_rate": 3.318235294117647e-05,
      "loss": 1.6705,
      "step": 28590
    },
    {
      "epoch": 1682.3529411764705,
      "grad_norm": 15.218917846679688,
      "learning_rate": 3.31764705882353e-05,
      "loss": 1.6213,
      "step": 28600
    },
    {
      "epoch": 1682.9411764705883,
      "grad_norm": 21.785131454467773,
      "learning_rate": 3.317058823529412e-05,
      "loss": 1.7286,
      "step": 28610
    },
    {
      "epoch": 1683.5294117647059,
      "grad_norm": 13.64930534362793,
      "learning_rate": 3.3164705882352945e-05,
      "loss": 1.6543,
      "step": 28620
    },
    {
      "epoch": 1684.1176470588234,
      "grad_norm": 16.897741317749023,
      "learning_rate": 3.315882352941176e-05,
      "loss": 1.6263,
      "step": 28630
    },
    {
      "epoch": 1684.7058823529412,
      "grad_norm": 19.26803970336914,
      "learning_rate": 3.315294117647059e-05,
      "loss": 1.6099,
      "step": 28640
    },
    {
      "epoch": 1685.2941176470588,
      "grad_norm": 13.877135276794434,
      "learning_rate": 3.3147058823529414e-05,
      "loss": 1.6699,
      "step": 28650
    },
    {
      "epoch": 1685.8823529411766,
      "grad_norm": 13.89793872833252,
      "learning_rate": 3.314117647058824e-05,
      "loss": 1.6813,
      "step": 28660
    },
    {
      "epoch": 1686.4705882352941,
      "grad_norm": 14.800390243530273,
      "learning_rate": 3.313529411764706e-05,
      "loss": 1.6486,
      "step": 28670
    },
    {
      "epoch": 1687.0588235294117,
      "grad_norm": 15.60633659362793,
      "learning_rate": 3.312941176470588e-05,
      "loss": 1.6716,
      "step": 28680
    },
    {
      "epoch": 1687.6470588235295,
      "grad_norm": 15.026617050170898,
      "learning_rate": 3.3123529411764706e-05,
      "loss": 1.6339,
      "step": 28690
    },
    {
      "epoch": 1688.235294117647,
      "grad_norm": 16.288286209106445,
      "learning_rate": 3.311764705882353e-05,
      "loss": 1.6144,
      "step": 28700
    },
    {
      "epoch": 1688.8235294117646,
      "grad_norm": 18.65994644165039,
      "learning_rate": 3.311176470588235e-05,
      "loss": 1.5966,
      "step": 28710
    },
    {
      "epoch": 1689.4117647058824,
      "grad_norm": 13.505452156066895,
      "learning_rate": 3.3105882352941175e-05,
      "loss": 1.6074,
      "step": 28720
    },
    {
      "epoch": 1690.0,
      "grad_norm": 20.294872283935547,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 1.5883,
      "step": 28730
    },
    {
      "epoch": 1690.5882352941176,
      "grad_norm": 10.692461967468262,
      "learning_rate": 3.309411764705883e-05,
      "loss": 1.4719,
      "step": 28740
    },
    {
      "epoch": 1691.1764705882354,
      "grad_norm": 12.11012077331543,
      "learning_rate": 3.308823529411765e-05,
      "loss": 1.6224,
      "step": 28750
    },
    {
      "epoch": 1691.764705882353,
      "grad_norm": 14.053610801696777,
      "learning_rate": 3.308235294117647e-05,
      "loss": 1.5989,
      "step": 28760
    },
    {
      "epoch": 1692.3529411764705,
      "grad_norm": 16.01801109313965,
      "learning_rate": 3.30764705882353e-05,
      "loss": 1.5665,
      "step": 28770
    },
    {
      "epoch": 1692.9411764705883,
      "grad_norm": 17.773977279663086,
      "learning_rate": 3.307058823529412e-05,
      "loss": 1.6832,
      "step": 28780
    },
    {
      "epoch": 1693.5294117647059,
      "grad_norm": 15.891200065612793,
      "learning_rate": 3.306470588235294e-05,
      "loss": 1.5237,
      "step": 28790
    },
    {
      "epoch": 1694.1176470588234,
      "grad_norm": 14.213123321533203,
      "learning_rate": 3.3058823529411766e-05,
      "loss": 1.5466,
      "step": 28800
    },
    {
      "epoch": 1694.7058823529412,
      "grad_norm": 14.40264892578125,
      "learning_rate": 3.305294117647059e-05,
      "loss": 1.5837,
      "step": 28810
    },
    {
      "epoch": 1695.2941176470588,
      "grad_norm": 15.485198020935059,
      "learning_rate": 3.304705882352941e-05,
      "loss": 1.5098,
      "step": 28820
    },
    {
      "epoch": 1695.8823529411766,
      "grad_norm": 14.097332000732422,
      "learning_rate": 3.3041176470588235e-05,
      "loss": 1.6348,
      "step": 28830
    },
    {
      "epoch": 1696.4705882352941,
      "grad_norm": 13.106980323791504,
      "learning_rate": 3.303529411764706e-05,
      "loss": 1.5577,
      "step": 28840
    },
    {
      "epoch": 1697.0588235294117,
      "grad_norm": 13.034233093261719,
      "learning_rate": 3.302941176470589e-05,
      "loss": 1.5121,
      "step": 28850
    },
    {
      "epoch": 1697.6470588235295,
      "grad_norm": 20.107648849487305,
      "learning_rate": 3.302352941176471e-05,
      "loss": 1.7291,
      "step": 28860
    },
    {
      "epoch": 1698.235294117647,
      "grad_norm": 14.926873207092285,
      "learning_rate": 3.3017647058823534e-05,
      "loss": 1.6035,
      "step": 28870
    },
    {
      "epoch": 1698.8235294117646,
      "grad_norm": 18.23377227783203,
      "learning_rate": 3.301176470588235e-05,
      "loss": 1.6489,
      "step": 28880
    },
    {
      "epoch": 1699.4117647058824,
      "grad_norm": 16.218120574951172,
      "learning_rate": 3.300588235294117e-05,
      "loss": 1.6074,
      "step": 28890
    },
    {
      "epoch": 1700.0,
      "grad_norm": 21.88128089904785,
      "learning_rate": 3.3e-05,
      "loss": 1.6473,
      "step": 28900
    },
    {
      "epoch": 1700.5882352941176,
      "grad_norm": 15.17747688293457,
      "learning_rate": 3.2994117647058826e-05,
      "loss": 1.4953,
      "step": 28910
    },
    {
      "epoch": 1701.1764705882354,
      "grad_norm": 16.55447769165039,
      "learning_rate": 3.298823529411765e-05,
      "loss": 1.5861,
      "step": 28920
    },
    {
      "epoch": 1701.764705882353,
      "grad_norm": 18.53488540649414,
      "learning_rate": 3.298235294117647e-05,
      "loss": 1.5244,
      "step": 28930
    },
    {
      "epoch": 1702.3529411764705,
      "grad_norm": 17.036664962768555,
      "learning_rate": 3.2976470588235294e-05,
      "loss": 1.5817,
      "step": 28940
    },
    {
      "epoch": 1702.9411764705883,
      "grad_norm": 13.583056449890137,
      "learning_rate": 3.297058823529412e-05,
      "loss": 1.5809,
      "step": 28950
    },
    {
      "epoch": 1703.5294117647059,
      "grad_norm": 11.906163215637207,
      "learning_rate": 3.296470588235294e-05,
      "loss": 1.6389,
      "step": 28960
    },
    {
      "epoch": 1704.1176470588234,
      "grad_norm": 13.608661651611328,
      "learning_rate": 3.2958823529411763e-05,
      "loss": 1.6356,
      "step": 28970
    },
    {
      "epoch": 1704.7058823529412,
      "grad_norm": 14.946844100952148,
      "learning_rate": 3.295294117647059e-05,
      "loss": 1.56,
      "step": 28980
    },
    {
      "epoch": 1705.2941176470588,
      "grad_norm": 12.817403793334961,
      "learning_rate": 3.2947058823529416e-05,
      "loss": 1.6014,
      "step": 28990
    },
    {
      "epoch": 1705.8823529411766,
      "grad_norm": 17.052560806274414,
      "learning_rate": 3.294117647058824e-05,
      "loss": 1.6076,
      "step": 29000
    },
    {
      "epoch": 1706.4705882352941,
      "grad_norm": 19.741531372070312,
      "learning_rate": 3.2935294117647055e-05,
      "loss": 1.5216,
      "step": 29010
    },
    {
      "epoch": 1707.0588235294117,
      "grad_norm": 18.10409164428711,
      "learning_rate": 3.2929411764705885e-05,
      "loss": 1.5551,
      "step": 29020
    },
    {
      "epoch": 1707.6470588235295,
      "grad_norm": 18.352153778076172,
      "learning_rate": 3.292352941176471e-05,
      "loss": 1.5699,
      "step": 29030
    },
    {
      "epoch": 1708.235294117647,
      "grad_norm": 13.658308982849121,
      "learning_rate": 3.291764705882353e-05,
      "loss": 1.5898,
      "step": 29040
    },
    {
      "epoch": 1708.8235294117646,
      "grad_norm": 16.844867706298828,
      "learning_rate": 3.2911764705882354e-05,
      "loss": 1.5343,
      "step": 29050
    },
    {
      "epoch": 1709.4117647058824,
      "grad_norm": 11.406625747680664,
      "learning_rate": 3.290588235294118e-05,
      "loss": 1.7201,
      "step": 29060
    },
    {
      "epoch": 1710.0,
      "grad_norm": 12.91666316986084,
      "learning_rate": 3.29e-05,
      "loss": 1.5703,
      "step": 29070
    },
    {
      "epoch": 1710.5882352941176,
      "grad_norm": 16.922529220581055,
      "learning_rate": 3.289411764705882e-05,
      "loss": 1.595,
      "step": 29080
    },
    {
      "epoch": 1711.1764705882354,
      "grad_norm": 15.120097160339355,
      "learning_rate": 3.2888235294117646e-05,
      "loss": 1.5788,
      "step": 29090
    },
    {
      "epoch": 1711.764705882353,
      "grad_norm": 12.726319313049316,
      "learning_rate": 3.288235294117647e-05,
      "loss": 1.5558,
      "step": 29100
    },
    {
      "epoch": 1712.3529411764705,
      "grad_norm": 16.11162757873535,
      "learning_rate": 3.28764705882353e-05,
      "loss": 1.7153,
      "step": 29110
    },
    {
      "epoch": 1712.9411764705883,
      "grad_norm": 19.386802673339844,
      "learning_rate": 3.287058823529412e-05,
      "loss": 1.6136,
      "step": 29120
    },
    {
      "epoch": 1713.5294117647059,
      "grad_norm": 14.259451866149902,
      "learning_rate": 3.2864705882352945e-05,
      "loss": 1.5597,
      "step": 29130
    },
    {
      "epoch": 1714.1176470588234,
      "grad_norm": 16.098085403442383,
      "learning_rate": 3.285882352941176e-05,
      "loss": 1.5411,
      "step": 29140
    },
    {
      "epoch": 1714.7058823529412,
      "grad_norm": 13.583677291870117,
      "learning_rate": 3.285294117647059e-05,
      "loss": 1.5315,
      "step": 29150
    },
    {
      "epoch": 1715.2941176470588,
      "grad_norm": 18.840341567993164,
      "learning_rate": 3.2847058823529414e-05,
      "loss": 1.6956,
      "step": 29160
    },
    {
      "epoch": 1715.8823529411766,
      "grad_norm": 13.417688369750977,
      "learning_rate": 3.284117647058824e-05,
      "loss": 1.5643,
      "step": 29170
    },
    {
      "epoch": 1716.4705882352941,
      "grad_norm": 17.309412002563477,
      "learning_rate": 3.283529411764706e-05,
      "loss": 1.6683,
      "step": 29180
    },
    {
      "epoch": 1717.0588235294117,
      "grad_norm": 18.284191131591797,
      "learning_rate": 3.282941176470589e-05,
      "loss": 1.7394,
      "step": 29190
    },
    {
      "epoch": 1717.6470588235295,
      "grad_norm": 15.86214542388916,
      "learning_rate": 3.2823529411764706e-05,
      "loss": 1.7243,
      "step": 29200
    },
    {
      "epoch": 1718.235294117647,
      "grad_norm": 13.634845733642578,
      "learning_rate": 3.281764705882353e-05,
      "loss": 1.4729,
      "step": 29210
    },
    {
      "epoch": 1718.8235294117646,
      "grad_norm": 18.42963409423828,
      "learning_rate": 3.281176470588235e-05,
      "loss": 1.6457,
      "step": 29220
    },
    {
      "epoch": 1719.4117647058824,
      "grad_norm": 16.11472511291504,
      "learning_rate": 3.280588235294118e-05,
      "loss": 1.6203,
      "step": 29230
    },
    {
      "epoch": 1720.0,
      "grad_norm": 21.44336700439453,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 1.6326,
      "step": 29240
    },
    {
      "epoch": 1720.5882352941176,
      "grad_norm": 16.74961280822754,
      "learning_rate": 3.279411764705883e-05,
      "loss": 1.632,
      "step": 29250
    },
    {
      "epoch": 1721.1764705882354,
      "grad_norm": 19.35008430480957,
      "learning_rate": 3.278823529411765e-05,
      "loss": 1.6927,
      "step": 29260
    },
    {
      "epoch": 1721.764705882353,
      "grad_norm": 18.965023040771484,
      "learning_rate": 3.278235294117647e-05,
      "loss": 1.6391,
      "step": 29270
    },
    {
      "epoch": 1722.3529411764705,
      "grad_norm": 17.81443214416504,
      "learning_rate": 3.2776470588235296e-05,
      "loss": 1.556,
      "step": 29280
    },
    {
      "epoch": 1722.9411764705883,
      "grad_norm": 14.304478645324707,
      "learning_rate": 3.277058823529412e-05,
      "loss": 1.5427,
      "step": 29290
    },
    {
      "epoch": 1723.5294117647059,
      "grad_norm": 14.49512004852295,
      "learning_rate": 3.276470588235294e-05,
      "loss": 1.6166,
      "step": 29300
    },
    {
      "epoch": 1724.1176470588234,
      "grad_norm": 17.235708236694336,
      "learning_rate": 3.2758823529411765e-05,
      "loss": 1.6049,
      "step": 29310
    },
    {
      "epoch": 1724.7058823529412,
      "grad_norm": 13.461576461791992,
      "learning_rate": 3.2752941176470595e-05,
      "loss": 1.7094,
      "step": 29320
    },
    {
      "epoch": 1725.2941176470588,
      "grad_norm": 16.22252655029297,
      "learning_rate": 3.274705882352941e-05,
      "loss": 1.6251,
      "step": 29330
    },
    {
      "epoch": 1725.8823529411766,
      "grad_norm": 16.856292724609375,
      "learning_rate": 3.2741176470588234e-05,
      "loss": 1.5886,
      "step": 29340
    },
    {
      "epoch": 1726.4705882352941,
      "grad_norm": 12.992222785949707,
      "learning_rate": 3.273529411764706e-05,
      "loss": 1.6783,
      "step": 29350
    },
    {
      "epoch": 1727.0588235294117,
      "grad_norm": 19.363372802734375,
      "learning_rate": 3.272941176470589e-05,
      "loss": 1.6258,
      "step": 29360
    },
    {
      "epoch": 1727.6470588235295,
      "grad_norm": 17.244136810302734,
      "learning_rate": 3.272352941176471e-05,
      "loss": 1.6526,
      "step": 29370
    },
    {
      "epoch": 1728.235294117647,
      "grad_norm": 19.789052963256836,
      "learning_rate": 3.271764705882353e-05,
      "loss": 1.8198,
      "step": 29380
    },
    {
      "epoch": 1728.8235294117646,
      "grad_norm": 17.28280258178711,
      "learning_rate": 3.2711764705882356e-05,
      "loss": 1.556,
      "step": 29390
    },
    {
      "epoch": 1729.4117647058824,
      "grad_norm": 19.582256317138672,
      "learning_rate": 3.270588235294118e-05,
      "loss": 1.5913,
      "step": 29400
    },
    {
      "epoch": 1730.0,
      "grad_norm": 16.158658981323242,
      "learning_rate": 3.27e-05,
      "loss": 1.5492,
      "step": 29410
    },
    {
      "epoch": 1730.5882352941176,
      "grad_norm": 12.668757438659668,
      "learning_rate": 3.2694117647058825e-05,
      "loss": 1.4391,
      "step": 29420
    },
    {
      "epoch": 1731.1764705882354,
      "grad_norm": 16.098369598388672,
      "learning_rate": 3.268823529411765e-05,
      "loss": 1.701,
      "step": 29430
    },
    {
      "epoch": 1731.764705882353,
      "grad_norm": 16.098352432250977,
      "learning_rate": 3.268235294117647e-05,
      "loss": 1.6027,
      "step": 29440
    },
    {
      "epoch": 1732.3529411764705,
      "grad_norm": 16.30190658569336,
      "learning_rate": 3.2676470588235294e-05,
      "loss": 1.6073,
      "step": 29450
    },
    {
      "epoch": 1732.9411764705883,
      "grad_norm": 14.181408882141113,
      "learning_rate": 3.267058823529412e-05,
      "loss": 1.5028,
      "step": 29460
    },
    {
      "epoch": 1733.5294117647059,
      "grad_norm": 26.39238929748535,
      "learning_rate": 3.266470588235294e-05,
      "loss": 1.6287,
      "step": 29470
    },
    {
      "epoch": 1734.1176470588234,
      "grad_norm": 13.586219787597656,
      "learning_rate": 3.265882352941176e-05,
      "loss": 1.5756,
      "step": 29480
    },
    {
      "epoch": 1734.7058823529412,
      "grad_norm": 17.68779945373535,
      "learning_rate": 3.265294117647059e-05,
      "loss": 1.5043,
      "step": 29490
    },
    {
      "epoch": 1735.2941176470588,
      "grad_norm": 17.004749298095703,
      "learning_rate": 3.2647058823529416e-05,
      "loss": 1.5866,
      "step": 29500
    },
    {
      "epoch": 1735.8823529411766,
      "grad_norm": 20.00072479248047,
      "learning_rate": 3.264117647058824e-05,
      "loss": 1.6095,
      "step": 29510
    },
    {
      "epoch": 1736.4705882352941,
      "grad_norm": 14.654834747314453,
      "learning_rate": 3.263529411764706e-05,
      "loss": 1.543,
      "step": 29520
    },
    {
      "epoch": 1737.0588235294117,
      "grad_norm": 15.048053741455078,
      "learning_rate": 3.2629411764705885e-05,
      "loss": 1.5824,
      "step": 29530
    },
    {
      "epoch": 1737.6470588235295,
      "grad_norm": 14.924727439880371,
      "learning_rate": 3.262352941176471e-05,
      "loss": 1.587,
      "step": 29540
    },
    {
      "epoch": 1738.235294117647,
      "grad_norm": 16.201736450195312,
      "learning_rate": 3.261764705882353e-05,
      "loss": 1.6467,
      "step": 29550
    },
    {
      "epoch": 1738.8235294117646,
      "grad_norm": 11.188100814819336,
      "learning_rate": 3.2611764705882354e-05,
      "loss": 1.5969,
      "step": 29560
    },
    {
      "epoch": 1739.4117647058824,
      "grad_norm": 17.22394371032715,
      "learning_rate": 3.260588235294118e-05,
      "loss": 1.5809,
      "step": 29570
    },
    {
      "epoch": 1740.0,
      "grad_norm": 19.073591232299805,
      "learning_rate": 3.26e-05,
      "loss": 1.56,
      "step": 29580
    },
    {
      "epoch": 1740.5882352941176,
      "grad_norm": 13.99620532989502,
      "learning_rate": 3.259411764705882e-05,
      "loss": 1.6272,
      "step": 29590
    },
    {
      "epoch": 1741.1764705882354,
      "grad_norm": 19.690109252929688,
      "learning_rate": 3.2588235294117646e-05,
      "loss": 1.7258,
      "step": 29600
    },
    {
      "epoch": 1741.764705882353,
      "grad_norm": 17.979337692260742,
      "learning_rate": 3.258235294117647e-05,
      "loss": 1.5774,
      "step": 29610
    },
    {
      "epoch": 1742.3529411764705,
      "grad_norm": 15.81134033203125,
      "learning_rate": 3.25764705882353e-05,
      "loss": 1.5271,
      "step": 29620
    },
    {
      "epoch": 1742.9411764705883,
      "grad_norm": 25.240854263305664,
      "learning_rate": 3.257058823529412e-05,
      "loss": 1.6705,
      "step": 29630
    },
    {
      "epoch": 1743.5294117647059,
      "grad_norm": 15.18446159362793,
      "learning_rate": 3.2564705882352944e-05,
      "loss": 1.564,
      "step": 29640
    },
    {
      "epoch": 1744.1176470588234,
      "grad_norm": 15.372367858886719,
      "learning_rate": 3.255882352941176e-05,
      "loss": 1.6391,
      "step": 29650
    },
    {
      "epoch": 1744.7058823529412,
      "grad_norm": 16.043058395385742,
      "learning_rate": 3.255294117647059e-05,
      "loss": 1.5027,
      "step": 29660
    },
    {
      "epoch": 1745.2941176470588,
      "grad_norm": 17.85704231262207,
      "learning_rate": 3.254705882352941e-05,
      "loss": 1.4438,
      "step": 29670
    },
    {
      "epoch": 1745.8823529411766,
      "grad_norm": 18.655473709106445,
      "learning_rate": 3.2541176470588236e-05,
      "loss": 1.5134,
      "step": 29680
    },
    {
      "epoch": 1746.4705882352941,
      "grad_norm": 13.317028999328613,
      "learning_rate": 3.253529411764706e-05,
      "loss": 1.7697,
      "step": 29690
    },
    {
      "epoch": 1747.0588235294117,
      "grad_norm": 19.372661590576172,
      "learning_rate": 3.252941176470589e-05,
      "loss": 1.6524,
      "step": 29700
    },
    {
      "epoch": 1747.6470588235295,
      "grad_norm": 14.357887268066406,
      "learning_rate": 3.2523529411764705e-05,
      "loss": 1.5745,
      "step": 29710
    },
    {
      "epoch": 1748.235294117647,
      "grad_norm": 13.64876651763916,
      "learning_rate": 3.251764705882353e-05,
      "loss": 1.6277,
      "step": 29720
    },
    {
      "epoch": 1748.8235294117646,
      "grad_norm": 15.013139724731445,
      "learning_rate": 3.251176470588235e-05,
      "loss": 1.5655,
      "step": 29730
    },
    {
      "epoch": 1749.4117647058824,
      "grad_norm": 17.921369552612305,
      "learning_rate": 3.250588235294118e-05,
      "loss": 1.6738,
      "step": 29740
    },
    {
      "epoch": 1750.0,
      "grad_norm": 21.932077407836914,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.5338,
      "step": 29750
    },
    {
      "epoch": 1750.5882352941176,
      "grad_norm": 15.158740997314453,
      "learning_rate": 3.249411764705883e-05,
      "loss": 1.6288,
      "step": 29760
    },
    {
      "epoch": 1751.1764705882354,
      "grad_norm": 16.30708122253418,
      "learning_rate": 3.248823529411765e-05,
      "loss": 1.598,
      "step": 29770
    },
    {
      "epoch": 1751.764705882353,
      "grad_norm": 15.811577796936035,
      "learning_rate": 3.248235294117647e-05,
      "loss": 1.6879,
      "step": 29780
    },
    {
      "epoch": 1752.3529411764705,
      "grad_norm": 15.131630897521973,
      "learning_rate": 3.2476470588235296e-05,
      "loss": 1.5761,
      "step": 29790
    },
    {
      "epoch": 1752.9411764705883,
      "grad_norm": 18.386268615722656,
      "learning_rate": 3.247058823529412e-05,
      "loss": 1.719,
      "step": 29800
    },
    {
      "epoch": 1753.5294117647059,
      "grad_norm": 17.946149826049805,
      "learning_rate": 3.246470588235294e-05,
      "loss": 1.5207,
      "step": 29810
    },
    {
      "epoch": 1754.1176470588234,
      "grad_norm": 15.040108680725098,
      "learning_rate": 3.2458823529411765e-05,
      "loss": 1.6036,
      "step": 29820
    },
    {
      "epoch": 1754.7058823529412,
      "grad_norm": 12.309539794921875,
      "learning_rate": 3.2452941176470595e-05,
      "loss": 1.5538,
      "step": 29830
    },
    {
      "epoch": 1755.2941176470588,
      "grad_norm": 15.583721160888672,
      "learning_rate": 3.244705882352941e-05,
      "loss": 1.4997,
      "step": 29840
    },
    {
      "epoch": 1755.8823529411766,
      "grad_norm": 11.525579452514648,
      "learning_rate": 3.2441176470588234e-05,
      "loss": 1.5416,
      "step": 29850
    },
    {
      "epoch": 1756.4705882352941,
      "grad_norm": 16.930261611938477,
      "learning_rate": 3.243529411764706e-05,
      "loss": 1.6454,
      "step": 29860
    },
    {
      "epoch": 1757.0588235294117,
      "grad_norm": 13.093640327453613,
      "learning_rate": 3.2429411764705887e-05,
      "loss": 1.4597,
      "step": 29870
    },
    {
      "epoch": 1757.6470588235295,
      "grad_norm": 17.444990158081055,
      "learning_rate": 3.242352941176471e-05,
      "loss": 1.5564,
      "step": 29880
    },
    {
      "epoch": 1758.235294117647,
      "grad_norm": 19.112285614013672,
      "learning_rate": 3.241764705882353e-05,
      "loss": 1.6149,
      "step": 29890
    },
    {
      "epoch": 1758.8235294117646,
      "grad_norm": 24.82308578491211,
      "learning_rate": 3.2411764705882356e-05,
      "loss": 1.6215,
      "step": 29900
    },
    {
      "epoch": 1759.4117647058824,
      "grad_norm": 16.240659713745117,
      "learning_rate": 3.240588235294118e-05,
      "loss": 1.5406,
      "step": 29910
    },
    {
      "epoch": 1760.0,
      "grad_norm": 20.164630889892578,
      "learning_rate": 3.24e-05,
      "loss": 1.6175,
      "step": 29920
    },
    {
      "epoch": 1760.5882352941176,
      "grad_norm": 15.122407913208008,
      "learning_rate": 3.2394117647058824e-05,
      "loss": 1.6027,
      "step": 29930
    },
    {
      "epoch": 1761.1764705882354,
      "grad_norm": 17.11459732055664,
      "learning_rate": 3.238823529411765e-05,
      "loss": 1.5532,
      "step": 29940
    },
    {
      "epoch": 1761.764705882353,
      "grad_norm": 16.414201736450195,
      "learning_rate": 3.238235294117648e-05,
      "loss": 1.6518,
      "step": 29950
    },
    {
      "epoch": 1762.3529411764705,
      "grad_norm": 15.454399108886719,
      "learning_rate": 3.23764705882353e-05,
      "loss": 1.4865,
      "step": 29960
    },
    {
      "epoch": 1762.9411764705883,
      "grad_norm": 14.86186408996582,
      "learning_rate": 3.2370588235294116e-05,
      "loss": 1.5455,
      "step": 29970
    },
    {
      "epoch": 1763.5294117647059,
      "grad_norm": 18.562105178833008,
      "learning_rate": 3.236470588235294e-05,
      "loss": 1.5836,
      "step": 29980
    },
    {
      "epoch": 1764.1176470588234,
      "grad_norm": 14.01279067993164,
      "learning_rate": 3.235882352941176e-05,
      "loss": 1.3023,
      "step": 29990
    },
    {
      "epoch": 1764.7058823529412,
      "grad_norm": 15.444244384765625,
      "learning_rate": 3.235294117647059e-05,
      "loss": 1.6742,
      "step": 30000
    },
    {
      "epoch": 1765.2941176470588,
      "grad_norm": 15.239813804626465,
      "learning_rate": 3.2347058823529415e-05,
      "loss": 1.5466,
      "step": 30010
    },
    {
      "epoch": 1765.8823529411766,
      "grad_norm": 15.671578407287598,
      "learning_rate": 3.234117647058824e-05,
      "loss": 1.6076,
      "step": 30020
    },
    {
      "epoch": 1766.4705882352941,
      "grad_norm": 15.250523567199707,
      "learning_rate": 3.233529411764706e-05,
      "loss": 1.5817,
      "step": 30030
    },
    {
      "epoch": 1767.0588235294117,
      "grad_norm": 13.435477256774902,
      "learning_rate": 3.2329411764705884e-05,
      "loss": 1.4991,
      "step": 30040
    },
    {
      "epoch": 1767.6470588235295,
      "grad_norm": 21.599769592285156,
      "learning_rate": 3.232352941176471e-05,
      "loss": 1.6023,
      "step": 30050
    },
    {
      "epoch": 1768.235294117647,
      "grad_norm": 17.240983963012695,
      "learning_rate": 3.231764705882353e-05,
      "loss": 1.6031,
      "step": 30060
    },
    {
      "epoch": 1768.8235294117646,
      "grad_norm": 16.81552505493164,
      "learning_rate": 3.231176470588235e-05,
      "loss": 1.5812,
      "step": 30070
    },
    {
      "epoch": 1769.4117647058824,
      "grad_norm": 15.442451477050781,
      "learning_rate": 3.230588235294118e-05,
      "loss": 1.5572,
      "step": 30080
    },
    {
      "epoch": 1770.0,
      "grad_norm": 15.640841484069824,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 1.5158,
      "step": 30090
    },
    {
      "epoch": 1770.5882352941176,
      "grad_norm": 15.807077407836914,
      "learning_rate": 3.229411764705882e-05,
      "loss": 1.5308,
      "step": 30100
    },
    {
      "epoch": 1771.1764705882354,
      "grad_norm": 15.102141380310059,
      "learning_rate": 3.2288235294117645e-05,
      "loss": 1.5097,
      "step": 30110
    },
    {
      "epoch": 1771.764705882353,
      "grad_norm": 12.676377296447754,
      "learning_rate": 3.2282352941176475e-05,
      "loss": 1.6136,
      "step": 30120
    },
    {
      "epoch": 1772.3529411764705,
      "grad_norm": 19.220233917236328,
      "learning_rate": 3.22764705882353e-05,
      "loss": 1.5288,
      "step": 30130
    },
    {
      "epoch": 1772.9411764705883,
      "grad_norm": 15.094029426574707,
      "learning_rate": 3.227058823529412e-05,
      "loss": 1.5855,
      "step": 30140
    },
    {
      "epoch": 1773.5294117647059,
      "grad_norm": 21.579071044921875,
      "learning_rate": 3.2264705882352944e-05,
      "loss": 1.5397,
      "step": 30150
    },
    {
      "epoch": 1774.1176470588234,
      "grad_norm": 15.839942932128906,
      "learning_rate": 3.225882352941177e-05,
      "loss": 1.6299,
      "step": 30160
    },
    {
      "epoch": 1774.7058823529412,
      "grad_norm": 13.562103271484375,
      "learning_rate": 3.225294117647059e-05,
      "loss": 1.5002,
      "step": 30170
    },
    {
      "epoch": 1775.2941176470588,
      "grad_norm": 20.512239456176758,
      "learning_rate": 3.224705882352941e-05,
      "loss": 1.4959,
      "step": 30180
    },
    {
      "epoch": 1775.8823529411766,
      "grad_norm": 12.80566120147705,
      "learning_rate": 3.2241176470588236e-05,
      "loss": 1.5002,
      "step": 30190
    },
    {
      "epoch": 1776.4705882352941,
      "grad_norm": 17.729034423828125,
      "learning_rate": 3.223529411764706e-05,
      "loss": 1.5592,
      "step": 30200
    },
    {
      "epoch": 1777.0588235294117,
      "grad_norm": 14.80971622467041,
      "learning_rate": 3.222941176470589e-05,
      "loss": 1.5361,
      "step": 30210
    },
    {
      "epoch": 1777.6470588235295,
      "grad_norm": 18.360164642333984,
      "learning_rate": 3.2223529411764705e-05,
      "loss": 1.543,
      "step": 30220
    },
    {
      "epoch": 1778.235294117647,
      "grad_norm": 16.65789794921875,
      "learning_rate": 3.221764705882353e-05,
      "loss": 1.5121,
      "step": 30230
    },
    {
      "epoch": 1778.8235294117646,
      "grad_norm": 15.590206146240234,
      "learning_rate": 3.221176470588235e-05,
      "loss": 1.4743,
      "step": 30240
    },
    {
      "epoch": 1779.4117647058824,
      "grad_norm": 23.7172794342041,
      "learning_rate": 3.220588235294118e-05,
      "loss": 1.559,
      "step": 30250
    },
    {
      "epoch": 1780.0,
      "grad_norm": 19.38072395324707,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 1.5251,
      "step": 30260
    },
    {
      "epoch": 1780.5882352941176,
      "grad_norm": 15.890539169311523,
      "learning_rate": 3.2194117647058826e-05,
      "loss": 1.5954,
      "step": 30270
    },
    {
      "epoch": 1781.1764705882354,
      "grad_norm": 20.253263473510742,
      "learning_rate": 3.218823529411765e-05,
      "loss": 1.5959,
      "step": 30280
    },
    {
      "epoch": 1781.764705882353,
      "grad_norm": 15.799142837524414,
      "learning_rate": 3.218235294117647e-05,
      "loss": 1.639,
      "step": 30290
    },
    {
      "epoch": 1782.3529411764705,
      "grad_norm": 18.241676330566406,
      "learning_rate": 3.2176470588235295e-05,
      "loss": 1.4781,
      "step": 30300
    },
    {
      "epoch": 1782.9411764705883,
      "grad_norm": 15.498875617980957,
      "learning_rate": 3.217058823529412e-05,
      "loss": 1.5952,
      "step": 30310
    },
    {
      "epoch": 1783.5294117647059,
      "grad_norm": 17.284406661987305,
      "learning_rate": 3.216470588235294e-05,
      "loss": 1.7301,
      "step": 30320
    },
    {
      "epoch": 1784.1176470588234,
      "grad_norm": 16.392969131469727,
      "learning_rate": 3.215882352941177e-05,
      "loss": 1.5535,
      "step": 30330
    },
    {
      "epoch": 1784.7058823529412,
      "grad_norm": 15.62321949005127,
      "learning_rate": 3.2152941176470594e-05,
      "loss": 1.4992,
      "step": 30340
    },
    {
      "epoch": 1785.2941176470588,
      "grad_norm": 17.843095779418945,
      "learning_rate": 3.214705882352941e-05,
      "loss": 1.5635,
      "step": 30350
    },
    {
      "epoch": 1785.8823529411766,
      "grad_norm": 13.4697904586792,
      "learning_rate": 3.214117647058823e-05,
      "loss": 1.526,
      "step": 30360
    },
    {
      "epoch": 1786.4705882352941,
      "grad_norm": 16.487272262573242,
      "learning_rate": 3.2135294117647056e-05,
      "loss": 1.569,
      "step": 30370
    },
    {
      "epoch": 1787.0588235294117,
      "grad_norm": 16.455284118652344,
      "learning_rate": 3.2129411764705886e-05,
      "loss": 1.6099,
      "step": 30380
    },
    {
      "epoch": 1787.6470588235295,
      "grad_norm": 12.866127014160156,
      "learning_rate": 3.212352941176471e-05,
      "loss": 1.4358,
      "step": 30390
    },
    {
      "epoch": 1788.235294117647,
      "grad_norm": 15.471657752990723,
      "learning_rate": 3.211764705882353e-05,
      "loss": 1.4882,
      "step": 30400
    },
    {
      "epoch": 1788.8235294117646,
      "grad_norm": 17.802106857299805,
      "learning_rate": 3.2111764705882355e-05,
      "loss": 1.5945,
      "step": 30410
    },
    {
      "epoch": 1789.4117647058824,
      "grad_norm": 14.79995059967041,
      "learning_rate": 3.210588235294118e-05,
      "loss": 1.6026,
      "step": 30420
    },
    {
      "epoch": 1790.0,
      "grad_norm": 23.332490921020508,
      "learning_rate": 3.21e-05,
      "loss": 1.4989,
      "step": 30430
    },
    {
      "epoch": 1790.5882352941176,
      "grad_norm": 19.171825408935547,
      "learning_rate": 3.2094117647058824e-05,
      "loss": 1.6148,
      "step": 30440
    },
    {
      "epoch": 1791.1764705882354,
      "grad_norm": 16.30184555053711,
      "learning_rate": 3.208823529411765e-05,
      "loss": 1.5551,
      "step": 30450
    },
    {
      "epoch": 1791.764705882353,
      "grad_norm": 16.376007080078125,
      "learning_rate": 3.208235294117648e-05,
      "loss": 1.5592,
      "step": 30460
    },
    {
      "epoch": 1792.3529411764705,
      "grad_norm": 14.67302131652832,
      "learning_rate": 3.20764705882353e-05,
      "loss": 1.4188,
      "step": 30470
    },
    {
      "epoch": 1792.9411764705883,
      "grad_norm": 14.816583633422852,
      "learning_rate": 3.2070588235294116e-05,
      "loss": 1.6206,
      "step": 30480
    },
    {
      "epoch": 1793.5294117647059,
      "grad_norm": 14.515792846679688,
      "learning_rate": 3.206470588235294e-05,
      "loss": 1.5121,
      "step": 30490
    },
    {
      "epoch": 1794.1176470588234,
      "grad_norm": 15.842851638793945,
      "learning_rate": 3.205882352941177e-05,
      "loss": 1.4967,
      "step": 30500
    },
    {
      "epoch": 1794.7058823529412,
      "grad_norm": 15.803489685058594,
      "learning_rate": 3.205294117647059e-05,
      "loss": 1.5712,
      "step": 30510
    },
    {
      "epoch": 1795.2941176470588,
      "grad_norm": 15.169997215270996,
      "learning_rate": 3.2047058823529415e-05,
      "loss": 1.4914,
      "step": 30520
    },
    {
      "epoch": 1795.8823529411766,
      "grad_norm": 16.60847282409668,
      "learning_rate": 3.204117647058824e-05,
      "loss": 1.5743,
      "step": 30530
    },
    {
      "epoch": 1796.4705882352941,
      "grad_norm": 16.88208770751953,
      "learning_rate": 3.203529411764706e-05,
      "loss": 1.5226,
      "step": 30540
    },
    {
      "epoch": 1797.0588235294117,
      "grad_norm": 14.20096206665039,
      "learning_rate": 3.2029411764705884e-05,
      "loss": 1.5667,
      "step": 30550
    },
    {
      "epoch": 1797.6470588235295,
      "grad_norm": 14.877398490905762,
      "learning_rate": 3.2023529411764707e-05,
      "loss": 1.4802,
      "step": 30560
    },
    {
      "epoch": 1798.235294117647,
      "grad_norm": 17.94636344909668,
      "learning_rate": 3.201764705882353e-05,
      "loss": 1.5148,
      "step": 30570
    },
    {
      "epoch": 1798.8235294117646,
      "grad_norm": 16.157970428466797,
      "learning_rate": 3.201176470588235e-05,
      "loss": 1.6028,
      "step": 30580
    },
    {
      "epoch": 1799.4117647058824,
      "grad_norm": 14.119902610778809,
      "learning_rate": 3.200588235294118e-05,
      "loss": 1.4794,
      "step": 30590
    },
    {
      "epoch": 1800.0,
      "grad_norm": 20.01949691772461,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.5589,
      "step": 30600
    },
    {
      "epoch": 1800.5882352941176,
      "grad_norm": 16.368310928344727,
      "learning_rate": 3.199411764705882e-05,
      "loss": 1.4872,
      "step": 30610
    },
    {
      "epoch": 1801.1764705882354,
      "grad_norm": 14.424698829650879,
      "learning_rate": 3.1988235294117645e-05,
      "loss": 1.5076,
      "step": 30620
    },
    {
      "epoch": 1801.764705882353,
      "grad_norm": 18.957727432250977,
      "learning_rate": 3.1982352941176474e-05,
      "loss": 1.495,
      "step": 30630
    },
    {
      "epoch": 1802.3529411764705,
      "grad_norm": 14.442717552185059,
      "learning_rate": 3.19764705882353e-05,
      "loss": 1.5173,
      "step": 30640
    },
    {
      "epoch": 1802.9411764705883,
      "grad_norm": 13.730793952941895,
      "learning_rate": 3.197058823529412e-05,
      "loss": 1.4725,
      "step": 30650
    },
    {
      "epoch": 1803.5294117647059,
      "grad_norm": 15.516770362854004,
      "learning_rate": 3.196470588235294e-05,
      "loss": 1.4539,
      "step": 30660
    },
    {
      "epoch": 1804.1176470588234,
      "grad_norm": 24.143802642822266,
      "learning_rate": 3.1958823529411766e-05,
      "loss": 1.6439,
      "step": 30670
    },
    {
      "epoch": 1804.7058823529412,
      "grad_norm": 15.898406982421875,
      "learning_rate": 3.195294117647059e-05,
      "loss": 1.5623,
      "step": 30680
    },
    {
      "epoch": 1805.2941176470588,
      "grad_norm": 16.586849212646484,
      "learning_rate": 3.194705882352941e-05,
      "loss": 1.5583,
      "step": 30690
    },
    {
      "epoch": 1805.8823529411766,
      "grad_norm": 18.000028610229492,
      "learning_rate": 3.1941176470588235e-05,
      "loss": 1.5492,
      "step": 30700
    },
    {
      "epoch": 1806.4705882352941,
      "grad_norm": 15.117292404174805,
      "learning_rate": 3.193529411764706e-05,
      "loss": 1.5095,
      "step": 30710
    },
    {
      "epoch": 1807.0588235294117,
      "grad_norm": 15.065349578857422,
      "learning_rate": 3.192941176470589e-05,
      "loss": 1.4683,
      "step": 30720
    },
    {
      "epoch": 1807.6470588235295,
      "grad_norm": 14.896013259887695,
      "learning_rate": 3.192352941176471e-05,
      "loss": 1.4624,
      "step": 30730
    },
    {
      "epoch": 1808.235294117647,
      "grad_norm": 19.063467025756836,
      "learning_rate": 3.191764705882353e-05,
      "loss": 1.6325,
      "step": 30740
    },
    {
      "epoch": 1808.8235294117646,
      "grad_norm": 18.98703384399414,
      "learning_rate": 3.191176470588235e-05,
      "loss": 1.4503,
      "step": 30750
    },
    {
      "epoch": 1809.4117647058824,
      "grad_norm": 16.251646041870117,
      "learning_rate": 3.190588235294118e-05,
      "loss": 1.5061,
      "step": 30760
    },
    {
      "epoch": 1810.0,
      "grad_norm": 18.305767059326172,
      "learning_rate": 3.19e-05,
      "loss": 1.6489,
      "step": 30770
    },
    {
      "epoch": 1810.5882352941176,
      "grad_norm": 15.425382614135742,
      "learning_rate": 3.1894117647058826e-05,
      "loss": 1.5781,
      "step": 30780
    },
    {
      "epoch": 1811.1764705882354,
      "grad_norm": 17.40567398071289,
      "learning_rate": 3.188823529411765e-05,
      "loss": 1.5821,
      "step": 30790
    },
    {
      "epoch": 1811.764705882353,
      "grad_norm": 15.010656356811523,
      "learning_rate": 3.188235294117647e-05,
      "loss": 1.4802,
      "step": 30800
    },
    {
      "epoch": 1812.3529411764705,
      "grad_norm": 15.677118301391602,
      "learning_rate": 3.1876470588235295e-05,
      "loss": 1.479,
      "step": 30810
    },
    {
      "epoch": 1812.9411764705883,
      "grad_norm": 16.439159393310547,
      "learning_rate": 3.187058823529412e-05,
      "loss": 1.41,
      "step": 30820
    },
    {
      "epoch": 1813.5294117647059,
      "grad_norm": 14.456993103027344,
      "learning_rate": 3.186470588235294e-05,
      "loss": 1.4622,
      "step": 30830
    },
    {
      "epoch": 1814.1176470588234,
      "grad_norm": 17.80435562133789,
      "learning_rate": 3.185882352941177e-05,
      "loss": 1.5837,
      "step": 30840
    },
    {
      "epoch": 1814.7058823529412,
      "grad_norm": 16.371244430541992,
      "learning_rate": 3.1852941176470594e-05,
      "loss": 1.5125,
      "step": 30850
    },
    {
      "epoch": 1815.2941176470588,
      "grad_norm": 13.752588272094727,
      "learning_rate": 3.184705882352941e-05,
      "loss": 1.5792,
      "step": 30860
    },
    {
      "epoch": 1815.8823529411766,
      "grad_norm": 17.17806053161621,
      "learning_rate": 3.184117647058823e-05,
      "loss": 1.6693,
      "step": 30870
    },
    {
      "epoch": 1816.4705882352941,
      "grad_norm": 15.813694953918457,
      "learning_rate": 3.1835294117647056e-05,
      "loss": 1.533,
      "step": 30880
    },
    {
      "epoch": 1817.0588235294117,
      "grad_norm": 15.35364818572998,
      "learning_rate": 3.1829411764705886e-05,
      "loss": 1.3867,
      "step": 30890
    },
    {
      "epoch": 1817.6470588235295,
      "grad_norm": 20.17778205871582,
      "learning_rate": 3.182352941176471e-05,
      "loss": 1.5225,
      "step": 30900
    },
    {
      "epoch": 1818.235294117647,
      "grad_norm": 16.437341690063477,
      "learning_rate": 3.181764705882353e-05,
      "loss": 1.4662,
      "step": 30910
    },
    {
      "epoch": 1818.8235294117646,
      "grad_norm": 16.709938049316406,
      "learning_rate": 3.1811764705882354e-05,
      "loss": 1.629,
      "step": 30920
    },
    {
      "epoch": 1819.4117647058824,
      "grad_norm": 16.219934463500977,
      "learning_rate": 3.180588235294118e-05,
      "loss": 1.5113,
      "step": 30930
    },
    {
      "epoch": 1820.0,
      "grad_norm": 19.157955169677734,
      "learning_rate": 3.18e-05,
      "loss": 1.6014,
      "step": 30940
    },
    {
      "epoch": 1820.5882352941176,
      "grad_norm": 19.981163024902344,
      "learning_rate": 3.1794117647058823e-05,
      "loss": 1.5935,
      "step": 30950
    },
    {
      "epoch": 1821.1764705882354,
      "grad_norm": 14.301920890808105,
      "learning_rate": 3.1788235294117646e-05,
      "loss": 1.5169,
      "step": 30960
    },
    {
      "epoch": 1821.764705882353,
      "grad_norm": 18.747934341430664,
      "learning_rate": 3.1782352941176476e-05,
      "loss": 1.4318,
      "step": 30970
    },
    {
      "epoch": 1822.3529411764705,
      "grad_norm": 18.528709411621094,
      "learning_rate": 3.17764705882353e-05,
      "loss": 1.5758,
      "step": 30980
    },
    {
      "epoch": 1822.9411764705883,
      "grad_norm": 16.46670150756836,
      "learning_rate": 3.1770588235294115e-05,
      "loss": 1.4812,
      "step": 30990
    },
    {
      "epoch": 1823.5294117647059,
      "grad_norm": 15.448352813720703,
      "learning_rate": 3.176470588235294e-05,
      "loss": 1.5295,
      "step": 31000
    },
    {
      "epoch": 1824.1176470588234,
      "grad_norm": 18.295854568481445,
      "learning_rate": 3.175882352941177e-05,
      "loss": 1.4658,
      "step": 31010
    },
    {
      "epoch": 1824.7058823529412,
      "grad_norm": 16.584285736083984,
      "learning_rate": 3.175294117647059e-05,
      "loss": 1.5362,
      "step": 31020
    },
    {
      "epoch": 1825.2941176470588,
      "grad_norm": 13.938260078430176,
      "learning_rate": 3.1747058823529414e-05,
      "loss": 1.483,
      "step": 31030
    },
    {
      "epoch": 1825.8823529411766,
      "grad_norm": 13.235635757446289,
      "learning_rate": 3.174117647058824e-05,
      "loss": 1.5411,
      "step": 31040
    },
    {
      "epoch": 1826.4705882352941,
      "grad_norm": 14.785968780517578,
      "learning_rate": 3.173529411764706e-05,
      "loss": 1.4488,
      "step": 31050
    },
    {
      "epoch": 1827.0588235294117,
      "grad_norm": 14.947707176208496,
      "learning_rate": 3.172941176470588e-05,
      "loss": 1.4159,
      "step": 31060
    },
    {
      "epoch": 1827.6470588235295,
      "grad_norm": 19.98772621154785,
      "learning_rate": 3.1723529411764706e-05,
      "loss": 1.576,
      "step": 31070
    },
    {
      "epoch": 1828.235294117647,
      "grad_norm": 18.421598434448242,
      "learning_rate": 3.171764705882353e-05,
      "loss": 1.6086,
      "step": 31080
    },
    {
      "epoch": 1828.8235294117646,
      "grad_norm": 19.8580379486084,
      "learning_rate": 3.171176470588235e-05,
      "loss": 1.6722,
      "step": 31090
    },
    {
      "epoch": 1829.4117647058824,
      "grad_norm": 17.501169204711914,
      "learning_rate": 3.170588235294118e-05,
      "loss": 1.6476,
      "step": 31100
    },
    {
      "epoch": 1830.0,
      "grad_norm": 17.600143432617188,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 1.6297,
      "step": 31110
    },
    {
      "epoch": 1830.5882352941176,
      "grad_norm": 17.55073356628418,
      "learning_rate": 3.169411764705882e-05,
      "loss": 1.6063,
      "step": 31120
    },
    {
      "epoch": 1831.1764705882354,
      "grad_norm": 16.428943634033203,
      "learning_rate": 3.1688235294117644e-05,
      "loss": 1.5666,
      "step": 31130
    },
    {
      "epoch": 1831.764705882353,
      "grad_norm": 13.158114433288574,
      "learning_rate": 3.1682352941176474e-05,
      "loss": 1.5874,
      "step": 31140
    },
    {
      "epoch": 1832.3529411764705,
      "grad_norm": 19.59943389892578,
      "learning_rate": 3.16764705882353e-05,
      "loss": 1.6646,
      "step": 31150
    },
    {
      "epoch": 1832.9411764705883,
      "grad_norm": 14.838712692260742,
      "learning_rate": 3.167058823529412e-05,
      "loss": 1.4624,
      "step": 31160
    },
    {
      "epoch": 1833.5294117647059,
      "grad_norm": 14.516780853271484,
      "learning_rate": 3.166470588235294e-05,
      "loss": 1.441,
      "step": 31170
    },
    {
      "epoch": 1834.1176470588234,
      "grad_norm": 14.548568725585938,
      "learning_rate": 3.1658823529411766e-05,
      "loss": 1.5428,
      "step": 31180
    },
    {
      "epoch": 1834.7058823529412,
      "grad_norm": 19.817392349243164,
      "learning_rate": 3.165294117647059e-05,
      "loss": 1.4753,
      "step": 31190
    },
    {
      "epoch": 1835.2941176470588,
      "grad_norm": 14.4425687789917,
      "learning_rate": 3.164705882352941e-05,
      "loss": 1.6071,
      "step": 31200
    },
    {
      "epoch": 1835.8823529411766,
      "grad_norm": 15.31916332244873,
      "learning_rate": 3.1641176470588235e-05,
      "loss": 1.5005,
      "step": 31210
    },
    {
      "epoch": 1836.4705882352941,
      "grad_norm": 15.242443084716797,
      "learning_rate": 3.1635294117647064e-05,
      "loss": 1.542,
      "step": 31220
    },
    {
      "epoch": 1837.0588235294117,
      "grad_norm": 16.02510643005371,
      "learning_rate": 3.162941176470589e-05,
      "loss": 1.4717,
      "step": 31230
    },
    {
      "epoch": 1837.6470588235295,
      "grad_norm": 17.05820083618164,
      "learning_rate": 3.162352941176471e-05,
      "loss": 1.5301,
      "step": 31240
    },
    {
      "epoch": 1838.235294117647,
      "grad_norm": 12.42089557647705,
      "learning_rate": 3.161764705882353e-05,
      "loss": 1.4657,
      "step": 31250
    },
    {
      "epoch": 1838.8235294117646,
      "grad_norm": 12.921708106994629,
      "learning_rate": 3.161176470588235e-05,
      "loss": 1.5909,
      "step": 31260
    },
    {
      "epoch": 1839.4117647058824,
      "grad_norm": 18.51886749267578,
      "learning_rate": 3.160588235294118e-05,
      "loss": 1.4689,
      "step": 31270
    },
    {
      "epoch": 1840.0,
      "grad_norm": 19.17095375061035,
      "learning_rate": 3.16e-05,
      "loss": 1.5624,
      "step": 31280
    },
    {
      "epoch": 1840.5882352941176,
      "grad_norm": 21.55699920654297,
      "learning_rate": 3.1594117647058825e-05,
      "loss": 1.4217,
      "step": 31290
    },
    {
      "epoch": 1841.1764705882354,
      "grad_norm": 21.559858322143555,
      "learning_rate": 3.158823529411765e-05,
      "loss": 1.6593,
      "step": 31300
    },
    {
      "epoch": 1841.764705882353,
      "grad_norm": 19.151872634887695,
      "learning_rate": 3.158235294117647e-05,
      "loss": 1.4374,
      "step": 31310
    },
    {
      "epoch": 1842.3529411764705,
      "grad_norm": 14.934032440185547,
      "learning_rate": 3.1576470588235294e-05,
      "loss": 1.4589,
      "step": 31320
    },
    {
      "epoch": 1842.9411764705883,
      "grad_norm": 17.804277420043945,
      "learning_rate": 3.157058823529412e-05,
      "loss": 1.374,
      "step": 31330
    },
    {
      "epoch": 1843.5294117647059,
      "grad_norm": 17.089784622192383,
      "learning_rate": 3.156470588235294e-05,
      "loss": 1.4615,
      "step": 31340
    },
    {
      "epoch": 1844.1176470588234,
      "grad_norm": 18.449357986450195,
      "learning_rate": 3.155882352941177e-05,
      "loss": 1.5281,
      "step": 31350
    },
    {
      "epoch": 1844.7058823529412,
      "grad_norm": 20.332412719726562,
      "learning_rate": 3.155294117647059e-05,
      "loss": 1.644,
      "step": 31360
    },
    {
      "epoch": 1845.2941176470588,
      "grad_norm": 14.173255920410156,
      "learning_rate": 3.1547058823529416e-05,
      "loss": 1.388,
      "step": 31370
    },
    {
      "epoch": 1845.8823529411766,
      "grad_norm": 19.05540657043457,
      "learning_rate": 3.154117647058823e-05,
      "loss": 1.5348,
      "step": 31380
    },
    {
      "epoch": 1846.4705882352941,
      "grad_norm": 18.374704360961914,
      "learning_rate": 3.153529411764706e-05,
      "loss": 1.5522,
      "step": 31390
    },
    {
      "epoch": 1847.0588235294117,
      "grad_norm": 18.386302947998047,
      "learning_rate": 3.1529411764705885e-05,
      "loss": 1.5603,
      "step": 31400
    },
    {
      "epoch": 1847.6470588235295,
      "grad_norm": 18.12917137145996,
      "learning_rate": 3.152352941176471e-05,
      "loss": 1.5633,
      "step": 31410
    },
    {
      "epoch": 1848.235294117647,
      "grad_norm": 18.56067657470703,
      "learning_rate": 3.151764705882353e-05,
      "loss": 1.5129,
      "step": 31420
    },
    {
      "epoch": 1848.8235294117646,
      "grad_norm": 20.614830017089844,
      "learning_rate": 3.1511764705882354e-05,
      "loss": 1.4794,
      "step": 31430
    },
    {
      "epoch": 1849.4117647058824,
      "grad_norm": 12.68554973602295,
      "learning_rate": 3.150588235294118e-05,
      "loss": 1.439,
      "step": 31440
    },
    {
      "epoch": 1850.0,
      "grad_norm": 16.308645248413086,
      "learning_rate": 3.15e-05,
      "loss": 1.3286,
      "step": 31450
    },
    {
      "epoch": 1850.5882352941176,
      "grad_norm": 14.854606628417969,
      "learning_rate": 3.149411764705882e-05,
      "loss": 1.5743,
      "step": 31460
    },
    {
      "epoch": 1851.1764705882354,
      "grad_norm": 14.39435863494873,
      "learning_rate": 3.1488235294117646e-05,
      "loss": 1.5134,
      "step": 31470
    },
    {
      "epoch": 1851.764705882353,
      "grad_norm": 16.918272018432617,
      "learning_rate": 3.1482352941176476e-05,
      "loss": 1.4873,
      "step": 31480
    },
    {
      "epoch": 1852.3529411764705,
      "grad_norm": 21.60880470275879,
      "learning_rate": 3.14764705882353e-05,
      "loss": 1.5802,
      "step": 31490
    },
    {
      "epoch": 1852.9411764705883,
      "grad_norm": 16.000078201293945,
      "learning_rate": 3.147058823529412e-05,
      "loss": 1.4496,
      "step": 31500
    },
    {
      "epoch": 1853.5294117647059,
      "grad_norm": 14.44872760772705,
      "learning_rate": 3.146470588235294e-05,
      "loss": 1.4938,
      "step": 31510
    },
    {
      "epoch": 1854.1176470588234,
      "grad_norm": 18.96018409729004,
      "learning_rate": 3.145882352941177e-05,
      "loss": 1.6132,
      "step": 31520
    },
    {
      "epoch": 1854.7058823529412,
      "grad_norm": 15.582141876220703,
      "learning_rate": 3.145294117647059e-05,
      "loss": 1.4623,
      "step": 31530
    },
    {
      "epoch": 1855.2941176470588,
      "grad_norm": 18.08297348022461,
      "learning_rate": 3.1447058823529414e-05,
      "loss": 1.589,
      "step": 31540
    },
    {
      "epoch": 1855.8823529411766,
      "grad_norm": 16.83698844909668,
      "learning_rate": 3.1441176470588237e-05,
      "loss": 1.5679,
      "step": 31550
    },
    {
      "epoch": 1856.4705882352941,
      "grad_norm": 14.00875186920166,
      "learning_rate": 3.143529411764706e-05,
      "loss": 1.4224,
      "step": 31560
    },
    {
      "epoch": 1857.0588235294117,
      "grad_norm": 16.542627334594727,
      "learning_rate": 3.142941176470588e-05,
      "loss": 1.5189,
      "step": 31570
    },
    {
      "epoch": 1857.6470588235295,
      "grad_norm": 14.101863861083984,
      "learning_rate": 3.1423529411764706e-05,
      "loss": 1.4458,
      "step": 31580
    },
    {
      "epoch": 1858.235294117647,
      "grad_norm": 19.445472717285156,
      "learning_rate": 3.141764705882353e-05,
      "loss": 1.5337,
      "step": 31590
    },
    {
      "epoch": 1858.8235294117646,
      "grad_norm": 14.158671379089355,
      "learning_rate": 3.141176470588236e-05,
      "loss": 1.4191,
      "step": 31600
    },
    {
      "epoch": 1859.4117647058824,
      "grad_norm": 18.35856819152832,
      "learning_rate": 3.140588235294118e-05,
      "loss": 1.4457,
      "step": 31610
    },
    {
      "epoch": 1860.0,
      "grad_norm": 17.956636428833008,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 1.4764,
      "step": 31620
    },
    {
      "epoch": 1860.5882352941176,
      "grad_norm": 14.679737091064453,
      "learning_rate": 3.139411764705882e-05,
      "loss": 1.4581,
      "step": 31630
    },
    {
      "epoch": 1861.1764705882354,
      "grad_norm": 19.799528121948242,
      "learning_rate": 3.1388235294117643e-05,
      "loss": 1.5032,
      "step": 31640
    },
    {
      "epoch": 1861.764705882353,
      "grad_norm": 21.64106559753418,
      "learning_rate": 3.138235294117647e-05,
      "loss": 1.4981,
      "step": 31650
    },
    {
      "epoch": 1862.3529411764705,
      "grad_norm": 22.157052993774414,
      "learning_rate": 3.1376470588235296e-05,
      "loss": 1.5369,
      "step": 31660
    },
    {
      "epoch": 1862.9411764705883,
      "grad_norm": 21.766010284423828,
      "learning_rate": 3.137058823529412e-05,
      "loss": 1.4403,
      "step": 31670
    },
    {
      "epoch": 1863.5294117647059,
      "grad_norm": 17.054794311523438,
      "learning_rate": 3.136470588235294e-05,
      "loss": 1.4062,
      "step": 31680
    },
    {
      "epoch": 1864.1176470588234,
      "grad_norm": 15.371048927307129,
      "learning_rate": 3.1358823529411765e-05,
      "loss": 1.5432,
      "step": 31690
    },
    {
      "epoch": 1864.7058823529412,
      "grad_norm": 17.523759841918945,
      "learning_rate": 3.135294117647059e-05,
      "loss": 1.5886,
      "step": 31700
    },
    {
      "epoch": 1865.2941176470588,
      "grad_norm": 19.331417083740234,
      "learning_rate": 3.134705882352941e-05,
      "loss": 1.5187,
      "step": 31710
    },
    {
      "epoch": 1865.8823529411766,
      "grad_norm": 17.91674041748047,
      "learning_rate": 3.1341176470588234e-05,
      "loss": 1.5871,
      "step": 31720
    },
    {
      "epoch": 1866.4705882352941,
      "grad_norm": 17.261171340942383,
      "learning_rate": 3.1335294117647064e-05,
      "loss": 1.5666,
      "step": 31730
    },
    {
      "epoch": 1867.0588235294117,
      "grad_norm": 13.873396873474121,
      "learning_rate": 3.132941176470589e-05,
      "loss": 1.4434,
      "step": 31740
    },
    {
      "epoch": 1867.6470588235295,
      "grad_norm": 19.121606826782227,
      "learning_rate": 3.132352941176471e-05,
      "loss": 1.5197,
      "step": 31750
    },
    {
      "epoch": 1868.235294117647,
      "grad_norm": 17.413175582885742,
      "learning_rate": 3.1317647058823526e-05,
      "loss": 1.4823,
      "step": 31760
    },
    {
      "epoch": 1868.8235294117646,
      "grad_norm": 14.567145347595215,
      "learning_rate": 3.1311764705882356e-05,
      "loss": 1.356,
      "step": 31770
    },
    {
      "epoch": 1869.4117647058824,
      "grad_norm": 17.03534507751465,
      "learning_rate": 3.130588235294118e-05,
      "loss": 1.553,
      "step": 31780
    },
    {
      "epoch": 1870.0,
      "grad_norm": 21.21500587463379,
      "learning_rate": 3.13e-05,
      "loss": 1.6185,
      "step": 31790
    },
    {
      "epoch": 1870.5882352941176,
      "grad_norm": 16.887771606445312,
      "learning_rate": 3.1294117647058825e-05,
      "loss": 1.6445,
      "step": 31800
    },
    {
      "epoch": 1871.1764705882354,
      "grad_norm": 15.304319381713867,
      "learning_rate": 3.128823529411765e-05,
      "loss": 1.4701,
      "step": 31810
    },
    {
      "epoch": 1871.764705882353,
      "grad_norm": 17.58928680419922,
      "learning_rate": 3.128235294117647e-05,
      "loss": 1.3916,
      "step": 31820
    },
    {
      "epoch": 1872.3529411764705,
      "grad_norm": 21.56966781616211,
      "learning_rate": 3.1276470588235294e-05,
      "loss": 1.5276,
      "step": 31830
    },
    {
      "epoch": 1872.9411764705883,
      "grad_norm": 17.037750244140625,
      "learning_rate": 3.127058823529412e-05,
      "loss": 1.464,
      "step": 31840
    },
    {
      "epoch": 1873.5294117647059,
      "grad_norm": 17.75379753112793,
      "learning_rate": 3.126470588235294e-05,
      "loss": 1.581,
      "step": 31850
    },
    {
      "epoch": 1874.1176470588234,
      "grad_norm": 14.94149112701416,
      "learning_rate": 3.125882352941177e-05,
      "loss": 1.4869,
      "step": 31860
    },
    {
      "epoch": 1874.7058823529412,
      "grad_norm": 14.876623153686523,
      "learning_rate": 3.125294117647059e-05,
      "loss": 1.437,
      "step": 31870
    },
    {
      "epoch": 1875.2941176470588,
      "grad_norm": 12.9834623336792,
      "learning_rate": 3.1247058823529416e-05,
      "loss": 1.5038,
      "step": 31880
    },
    {
      "epoch": 1875.8823529411766,
      "grad_norm": 15.422881126403809,
      "learning_rate": 3.124117647058823e-05,
      "loss": 1.3987,
      "step": 31890
    },
    {
      "epoch": 1876.4705882352941,
      "grad_norm": 17.308290481567383,
      "learning_rate": 3.123529411764706e-05,
      "loss": 1.4982,
      "step": 31900
    },
    {
      "epoch": 1877.0588235294117,
      "grad_norm": 18.925537109375,
      "learning_rate": 3.1229411764705884e-05,
      "loss": 1.497,
      "step": 31910
    },
    {
      "epoch": 1877.6470588235295,
      "grad_norm": 16.06136131286621,
      "learning_rate": 3.122352941176471e-05,
      "loss": 1.4438,
      "step": 31920
    },
    {
      "epoch": 1878.235294117647,
      "grad_norm": 16.974925994873047,
      "learning_rate": 3.121764705882353e-05,
      "loss": 1.4172,
      "step": 31930
    },
    {
      "epoch": 1878.8235294117646,
      "grad_norm": 22.025209426879883,
      "learning_rate": 3.121176470588236e-05,
      "loss": 1.5608,
      "step": 31940
    },
    {
      "epoch": 1879.4117647058824,
      "grad_norm": 21.132057189941406,
      "learning_rate": 3.1205882352941176e-05,
      "loss": 1.5139,
      "step": 31950
    },
    {
      "epoch": 1880.0,
      "grad_norm": 19.74379539489746,
      "learning_rate": 3.12e-05,
      "loss": 1.5045,
      "step": 31960
    },
    {
      "epoch": 1880.5882352941176,
      "grad_norm": 17.529830932617188,
      "learning_rate": 3.119411764705882e-05,
      "loss": 1.5398,
      "step": 31970
    },
    {
      "epoch": 1881.1764705882354,
      "grad_norm": 16.009984970092773,
      "learning_rate": 3.1188235294117645e-05,
      "loss": 1.4391,
      "step": 31980
    },
    {
      "epoch": 1881.764705882353,
      "grad_norm": 19.0700740814209,
      "learning_rate": 3.1182352941176475e-05,
      "loss": 1.4606,
      "step": 31990
    },
    {
      "epoch": 1882.3529411764705,
      "grad_norm": 15.055337905883789,
      "learning_rate": 3.11764705882353e-05,
      "loss": 1.4501,
      "step": 32000
    },
    {
      "epoch": 1882.9411764705883,
      "grad_norm": 19.884479522705078,
      "learning_rate": 3.117058823529412e-05,
      "loss": 1.5535,
      "step": 32010
    },
    {
      "epoch": 1883.5294117647059,
      "grad_norm": 25.073944091796875,
      "learning_rate": 3.116470588235294e-05,
      "loss": 1.4223,
      "step": 32020
    },
    {
      "epoch": 1884.1176470588234,
      "grad_norm": 19.584165573120117,
      "learning_rate": 3.115882352941177e-05,
      "loss": 1.4491,
      "step": 32030
    },
    {
      "epoch": 1884.7058823529412,
      "grad_norm": 15.819419860839844,
      "learning_rate": 3.115294117647059e-05,
      "loss": 1.5396,
      "step": 32040
    },
    {
      "epoch": 1885.2941176470588,
      "grad_norm": 16.469741821289062,
      "learning_rate": 3.114705882352941e-05,
      "loss": 1.4709,
      "step": 32050
    },
    {
      "epoch": 1885.8823529411766,
      "grad_norm": 22.53199005126953,
      "learning_rate": 3.1141176470588236e-05,
      "loss": 1.3598,
      "step": 32060
    },
    {
      "epoch": 1886.4705882352941,
      "grad_norm": 18.398651123046875,
      "learning_rate": 3.1135294117647066e-05,
      "loss": 1.4935,
      "step": 32070
    },
    {
      "epoch": 1887.0588235294117,
      "grad_norm": 15.001670837402344,
      "learning_rate": 3.112941176470588e-05,
      "loss": 1.4638,
      "step": 32080
    },
    {
      "epoch": 1887.6470588235295,
      "grad_norm": 20.055450439453125,
      "learning_rate": 3.1123529411764705e-05,
      "loss": 1.4217,
      "step": 32090
    },
    {
      "epoch": 1888.235294117647,
      "grad_norm": 17.24721336364746,
      "learning_rate": 3.111764705882353e-05,
      "loss": 1.4374,
      "step": 32100
    },
    {
      "epoch": 1888.8235294117646,
      "grad_norm": 16.778385162353516,
      "learning_rate": 3.111176470588236e-05,
      "loss": 1.5216,
      "step": 32110
    },
    {
      "epoch": 1889.4117647058824,
      "grad_norm": 16.061582565307617,
      "learning_rate": 3.110588235294118e-05,
      "loss": 1.4253,
      "step": 32120
    },
    {
      "epoch": 1890.0,
      "grad_norm": 20.124544143676758,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 1.6009,
      "step": 32130
    },
    {
      "epoch": 1890.5882352941176,
      "grad_norm": 12.726531982421875,
      "learning_rate": 3.109411764705883e-05,
      "loss": 1.4639,
      "step": 32140
    },
    {
      "epoch": 1891.1764705882354,
      "grad_norm": 20.622880935668945,
      "learning_rate": 3.108823529411764e-05,
      "loss": 1.5483,
      "step": 32150
    },
    {
      "epoch": 1891.764705882353,
      "grad_norm": 12.695695877075195,
      "learning_rate": 3.108235294117647e-05,
      "loss": 1.4444,
      "step": 32160
    },
    {
      "epoch": 1892.3529411764705,
      "grad_norm": 12.843294143676758,
      "learning_rate": 3.1076470588235296e-05,
      "loss": 1.4584,
      "step": 32170
    },
    {
      "epoch": 1892.9411764705883,
      "grad_norm": 14.003561973571777,
      "learning_rate": 3.107058823529412e-05,
      "loss": 1.3424,
      "step": 32180
    },
    {
      "epoch": 1893.5294117647059,
      "grad_norm": 17.11441993713379,
      "learning_rate": 3.106470588235294e-05,
      "loss": 1.4126,
      "step": 32190
    },
    {
      "epoch": 1894.1176470588234,
      "grad_norm": 16.20136070251465,
      "learning_rate": 3.1058823529411765e-05,
      "loss": 1.5101,
      "step": 32200
    },
    {
      "epoch": 1894.7058823529412,
      "grad_norm": 19.267135620117188,
      "learning_rate": 3.105294117647059e-05,
      "loss": 1.5229,
      "step": 32210
    },
    {
      "epoch": 1895.2941176470588,
      "grad_norm": 18.163898468017578,
      "learning_rate": 3.104705882352941e-05,
      "loss": 1.3785,
      "step": 32220
    },
    {
      "epoch": 1895.8823529411766,
      "grad_norm": 20.575366973876953,
      "learning_rate": 3.1041176470588234e-05,
      "loss": 1.5029,
      "step": 32230
    },
    {
      "epoch": 1896.4705882352941,
      "grad_norm": 15.790343284606934,
      "learning_rate": 3.1035294117647063e-05,
      "loss": 1.4434,
      "step": 32240
    },
    {
      "epoch": 1897.0588235294117,
      "grad_norm": 15.521720886230469,
      "learning_rate": 3.1029411764705886e-05,
      "loss": 1.3781,
      "step": 32250
    },
    {
      "epoch": 1897.6470588235295,
      "grad_norm": 14.806966781616211,
      "learning_rate": 3.102352941176471e-05,
      "loss": 1.4776,
      "step": 32260
    },
    {
      "epoch": 1898.235294117647,
      "grad_norm": 20.807701110839844,
      "learning_rate": 3.101764705882353e-05,
      "loss": 1.5,
      "step": 32270
    },
    {
      "epoch": 1898.8235294117646,
      "grad_norm": 19.317075729370117,
      "learning_rate": 3.1011764705882355e-05,
      "loss": 1.48,
      "step": 32280
    },
    {
      "epoch": 1899.4117647058824,
      "grad_norm": 16.638986587524414,
      "learning_rate": 3.100588235294118e-05,
      "loss": 1.5295,
      "step": 32290
    },
    {
      "epoch": 1900.0,
      "grad_norm": 19.45154571533203,
      "learning_rate": 3.1e-05,
      "loss": 1.4956,
      "step": 32300
    },
    {
      "epoch": 1900.5882352941176,
      "grad_norm": 18.884862899780273,
      "learning_rate": 3.0994117647058824e-05,
      "loss": 1.4955,
      "step": 32310
    },
    {
      "epoch": 1901.1764705882354,
      "grad_norm": 16.435400009155273,
      "learning_rate": 3.0988235294117654e-05,
      "loss": 1.3623,
      "step": 32320
    },
    {
      "epoch": 1901.764705882353,
      "grad_norm": 21.950340270996094,
      "learning_rate": 3.098235294117647e-05,
      "loss": 1.4751,
      "step": 32330
    },
    {
      "epoch": 1902.3529411764705,
      "grad_norm": 17.8618106842041,
      "learning_rate": 3.097647058823529e-05,
      "loss": 1.4518,
      "step": 32340
    },
    {
      "epoch": 1902.9411764705883,
      "grad_norm": 18.115938186645508,
      "learning_rate": 3.0970588235294116e-05,
      "loss": 1.4675,
      "step": 32350
    },
    {
      "epoch": 1903.5294117647059,
      "grad_norm": 15.081244468688965,
      "learning_rate": 3.096470588235294e-05,
      "loss": 1.5281,
      "step": 32360
    },
    {
      "epoch": 1904.1176470588234,
      "grad_norm": 17.770906448364258,
      "learning_rate": 3.095882352941177e-05,
      "loss": 1.5174,
      "step": 32370
    },
    {
      "epoch": 1904.7058823529412,
      "grad_norm": 15.990351676940918,
      "learning_rate": 3.095294117647059e-05,
      "loss": 1.4238,
      "step": 32380
    },
    {
      "epoch": 1905.2941176470588,
      "grad_norm": 16.86367416381836,
      "learning_rate": 3.0947058823529415e-05,
      "loss": 1.4756,
      "step": 32390
    },
    {
      "epoch": 1905.8823529411766,
      "grad_norm": 14.663148880004883,
      "learning_rate": 3.094117647058823e-05,
      "loss": 1.4508,
      "step": 32400
    },
    {
      "epoch": 1906.4705882352941,
      "grad_norm": 15.763511657714844,
      "learning_rate": 3.093529411764706e-05,
      "loss": 1.401,
      "step": 32410
    },
    {
      "epoch": 1907.0588235294117,
      "grad_norm": 17.986045837402344,
      "learning_rate": 3.0929411764705884e-05,
      "loss": 1.4094,
      "step": 32420
    },
    {
      "epoch": 1907.6470588235295,
      "grad_norm": 19.99689292907715,
      "learning_rate": 3.092352941176471e-05,
      "loss": 1.4314,
      "step": 32430
    },
    {
      "epoch": 1908.235294117647,
      "grad_norm": 15.536979675292969,
      "learning_rate": 3.091764705882353e-05,
      "loss": 1.6399,
      "step": 32440
    },
    {
      "epoch": 1908.8235294117646,
      "grad_norm": 20.311254501342773,
      "learning_rate": 3.091176470588236e-05,
      "loss": 1.4706,
      "step": 32450
    },
    {
      "epoch": 1909.4117647058824,
      "grad_norm": 18.194631576538086,
      "learning_rate": 3.0905882352941176e-05,
      "loss": 1.4356,
      "step": 32460
    },
    {
      "epoch": 1910.0,
      "grad_norm": 24.141530990600586,
      "learning_rate": 3.09e-05,
      "loss": 1.4362,
      "step": 32470
    },
    {
      "epoch": 1910.5882352941176,
      "grad_norm": 17.299453735351562,
      "learning_rate": 3.089411764705882e-05,
      "loss": 1.3903,
      "step": 32480
    },
    {
      "epoch": 1911.1764705882354,
      "grad_norm": 15.718855857849121,
      "learning_rate": 3.088823529411765e-05,
      "loss": 1.4284,
      "step": 32490
    },
    {
      "epoch": 1911.764705882353,
      "grad_norm": 19.634429931640625,
      "learning_rate": 3.0882352941176475e-05,
      "loss": 1.3776,
      "step": 32500
    },
    {
      "epoch": 1912.3529411764705,
      "grad_norm": 14.827998161315918,
      "learning_rate": 3.08764705882353e-05,
      "loss": 1.5307,
      "step": 32510
    },
    {
      "epoch": 1912.9411764705883,
      "grad_norm": 16.343334197998047,
      "learning_rate": 3.087058823529412e-05,
      "loss": 1.4182,
      "step": 32520
    },
    {
      "epoch": 1913.5294117647059,
      "grad_norm": 14.726727485656738,
      "learning_rate": 3.086470588235294e-05,
      "loss": 1.3406,
      "step": 32530
    },
    {
      "epoch": 1914.1176470588234,
      "grad_norm": 15.910353660583496,
      "learning_rate": 3.0858823529411767e-05,
      "loss": 1.5068,
      "step": 32540
    },
    {
      "epoch": 1914.7058823529412,
      "grad_norm": 15.072662353515625,
      "learning_rate": 3.085294117647059e-05,
      "loss": 1.4526,
      "step": 32550
    },
    {
      "epoch": 1915.2941176470588,
      "grad_norm": 18.9957218170166,
      "learning_rate": 3.084705882352941e-05,
      "loss": 1.4883,
      "step": 32560
    },
    {
      "epoch": 1915.8823529411766,
      "grad_norm": 15.909564971923828,
      "learning_rate": 3.0841176470588236e-05,
      "loss": 1.4986,
      "step": 32570
    },
    {
      "epoch": 1916.4705882352941,
      "grad_norm": 16.34189796447754,
      "learning_rate": 3.0835294117647065e-05,
      "loss": 1.4817,
      "step": 32580
    },
    {
      "epoch": 1917.0588235294117,
      "grad_norm": 16.979427337646484,
      "learning_rate": 3.082941176470588e-05,
      "loss": 1.4956,
      "step": 32590
    },
    {
      "epoch": 1917.6470588235295,
      "grad_norm": 15.325616836547852,
      "learning_rate": 3.0823529411764705e-05,
      "loss": 1.3705,
      "step": 32600
    },
    {
      "epoch": 1918.235294117647,
      "grad_norm": 18.94398307800293,
      "learning_rate": 3.081764705882353e-05,
      "loss": 1.54,
      "step": 32610
    },
    {
      "epoch": 1918.8235294117646,
      "grad_norm": 22.195087432861328,
      "learning_rate": 3.081176470588236e-05,
      "loss": 1.5341,
      "step": 32620
    },
    {
      "epoch": 1919.4117647058824,
      "grad_norm": 17.130800247192383,
      "learning_rate": 3.080588235294118e-05,
      "loss": 1.3308,
      "step": 32630
    },
    {
      "epoch": 1920.0,
      "grad_norm": 16.295854568481445,
      "learning_rate": 3.08e-05,
      "loss": 1.349,
      "step": 32640
    },
    {
      "epoch": 1920.5882352941176,
      "grad_norm": 22.477378845214844,
      "learning_rate": 3.0794117647058826e-05,
      "loss": 1.5688,
      "step": 32650
    },
    {
      "epoch": 1921.1764705882354,
      "grad_norm": 18.509960174560547,
      "learning_rate": 3.078823529411765e-05,
      "loss": 1.3883,
      "step": 32660
    },
    {
      "epoch": 1921.764705882353,
      "grad_norm": 17.876739501953125,
      "learning_rate": 3.078235294117647e-05,
      "loss": 1.4263,
      "step": 32670
    },
    {
      "epoch": 1922.3529411764705,
      "grad_norm": 17.050683975219727,
      "learning_rate": 3.0776470588235295e-05,
      "loss": 1.3621,
      "step": 32680
    },
    {
      "epoch": 1922.9411764705883,
      "grad_norm": 15.198906898498535,
      "learning_rate": 3.077058823529412e-05,
      "loss": 1.3517,
      "step": 32690
    },
    {
      "epoch": 1923.5294117647059,
      "grad_norm": 18.425945281982422,
      "learning_rate": 3.076470588235294e-05,
      "loss": 1.5773,
      "step": 32700
    },
    {
      "epoch": 1924.1176470588234,
      "grad_norm": 15.238297462463379,
      "learning_rate": 3.075882352941177e-05,
      "loss": 1.4529,
      "step": 32710
    },
    {
      "epoch": 1924.7058823529412,
      "grad_norm": 17.345190048217773,
      "learning_rate": 3.075294117647059e-05,
      "loss": 1.2727,
      "step": 32720
    },
    {
      "epoch": 1925.2941176470588,
      "grad_norm": 15.412188529968262,
      "learning_rate": 3.074705882352941e-05,
      "loss": 1.4106,
      "step": 32730
    },
    {
      "epoch": 1925.8823529411766,
      "grad_norm": 15.878535270690918,
      "learning_rate": 3.074117647058823e-05,
      "loss": 1.4436,
      "step": 32740
    },
    {
      "epoch": 1926.4705882352941,
      "grad_norm": 14.603523254394531,
      "learning_rate": 3.073529411764706e-05,
      "loss": 1.4093,
      "step": 32750
    },
    {
      "epoch": 1927.0588235294117,
      "grad_norm": 27.298059463500977,
      "learning_rate": 3.0729411764705886e-05,
      "loss": 1.4829,
      "step": 32760
    },
    {
      "epoch": 1927.6470588235295,
      "grad_norm": 16.414020538330078,
      "learning_rate": 3.072352941176471e-05,
      "loss": 1.4388,
      "step": 32770
    },
    {
      "epoch": 1928.235294117647,
      "grad_norm": 14.845548629760742,
      "learning_rate": 3.071764705882353e-05,
      "loss": 1.438,
      "step": 32780
    },
    {
      "epoch": 1928.8235294117646,
      "grad_norm": 15.068572998046875,
      "learning_rate": 3.0711764705882355e-05,
      "loss": 1.4227,
      "step": 32790
    },
    {
      "epoch": 1929.4117647058824,
      "grad_norm": 13.512601852416992,
      "learning_rate": 3.070588235294118e-05,
      "loss": 1.4636,
      "step": 32800
    },
    {
      "epoch": 1930.0,
      "grad_norm": 17.629478454589844,
      "learning_rate": 3.07e-05,
      "loss": 1.4606,
      "step": 32810
    },
    {
      "epoch": 1930.5882352941176,
      "grad_norm": 16.13740348815918,
      "learning_rate": 3.0694117647058824e-05,
      "loss": 1.4781,
      "step": 32820
    },
    {
      "epoch": 1931.1764705882354,
      "grad_norm": 19.453819274902344,
      "learning_rate": 3.0688235294117654e-05,
      "loss": 1.4947,
      "step": 32830
    },
    {
      "epoch": 1931.764705882353,
      "grad_norm": 13.855709075927734,
      "learning_rate": 3.0682352941176477e-05,
      "loss": 1.3711,
      "step": 32840
    },
    {
      "epoch": 1932.3529411764705,
      "grad_norm": 20.890396118164062,
      "learning_rate": 3.067647058823529e-05,
      "loss": 1.4358,
      "step": 32850
    },
    {
      "epoch": 1932.9411764705883,
      "grad_norm": 13.388445854187012,
      "learning_rate": 3.0670588235294116e-05,
      "loss": 1.2916,
      "step": 32860
    },
    {
      "epoch": 1933.5294117647059,
      "grad_norm": 17.130844116210938,
      "learning_rate": 3.0664705882352946e-05,
      "loss": 1.5675,
      "step": 32870
    },
    {
      "epoch": 1934.1176470588234,
      "grad_norm": 17.581689834594727,
      "learning_rate": 3.065882352941177e-05,
      "loss": 1.3468,
      "step": 32880
    },
    {
      "epoch": 1934.7058823529412,
      "grad_norm": 15.673380851745605,
      "learning_rate": 3.065294117647059e-05,
      "loss": 1.4457,
      "step": 32890
    },
    {
      "epoch": 1935.2941176470588,
      "grad_norm": 14.94808292388916,
      "learning_rate": 3.0647058823529415e-05,
      "loss": 1.4248,
      "step": 32900
    },
    {
      "epoch": 1935.8823529411766,
      "grad_norm": 17.09842300415039,
      "learning_rate": 3.064117647058824e-05,
      "loss": 1.3501,
      "step": 32910
    },
    {
      "epoch": 1936.4705882352941,
      "grad_norm": 17.763227462768555,
      "learning_rate": 3.063529411764706e-05,
      "loss": 1.4514,
      "step": 32920
    },
    {
      "epoch": 1937.0588235294117,
      "grad_norm": 16.057435989379883,
      "learning_rate": 3.0629411764705883e-05,
      "loss": 1.4458,
      "step": 32930
    },
    {
      "epoch": 1937.6470588235295,
      "grad_norm": 19.51148796081543,
      "learning_rate": 3.0623529411764706e-05,
      "loss": 1.3774,
      "step": 32940
    },
    {
      "epoch": 1938.235294117647,
      "grad_norm": 14.203883171081543,
      "learning_rate": 3.061764705882353e-05,
      "loss": 1.4193,
      "step": 32950
    },
    {
      "epoch": 1938.8235294117646,
      "grad_norm": 14.41971492767334,
      "learning_rate": 3.061176470588236e-05,
      "loss": 1.4734,
      "step": 32960
    },
    {
      "epoch": 1939.4117647058824,
      "grad_norm": 16.732009887695312,
      "learning_rate": 3.0605882352941175e-05,
      "loss": 1.4581,
      "step": 32970
    },
    {
      "epoch": 1940.0,
      "grad_norm": 20.73750114440918,
      "learning_rate": 3.06e-05,
      "loss": 1.5477,
      "step": 32980
    },
    {
      "epoch": 1940.5882352941176,
      "grad_norm": 16.555356979370117,
      "learning_rate": 3.059411764705882e-05,
      "loss": 1.4706,
      "step": 32990
    },
    {
      "epoch": 1941.1764705882354,
      "grad_norm": 17.90119171142578,
      "learning_rate": 3.058823529411765e-05,
      "loss": 1.3562,
      "step": 33000
    },
    {
      "epoch": 1941.764705882353,
      "grad_norm": 18.367929458618164,
      "learning_rate": 3.0582352941176474e-05,
      "loss": 1.3847,
      "step": 33010
    },
    {
      "epoch": 1942.3529411764705,
      "grad_norm": 16.126955032348633,
      "learning_rate": 3.05764705882353e-05,
      "loss": 1.5108,
      "step": 33020
    },
    {
      "epoch": 1942.9411764705883,
      "grad_norm": 14.365242004394531,
      "learning_rate": 3.057058823529412e-05,
      "loss": 1.3666,
      "step": 33030
    },
    {
      "epoch": 1943.5294117647059,
      "grad_norm": 17.9783992767334,
      "learning_rate": 3.056470588235294e-05,
      "loss": 1.4659,
      "step": 33040
    },
    {
      "epoch": 1944.1176470588234,
      "grad_norm": 13.528844833374023,
      "learning_rate": 3.0558823529411766e-05,
      "loss": 1.3591,
      "step": 33050
    },
    {
      "epoch": 1944.7058823529412,
      "grad_norm": 19.466096878051758,
      "learning_rate": 3.055294117647059e-05,
      "loss": 1.4229,
      "step": 33060
    },
    {
      "epoch": 1945.2941176470588,
      "grad_norm": 15.269213676452637,
      "learning_rate": 3.054705882352941e-05,
      "loss": 1.4925,
      "step": 33070
    },
    {
      "epoch": 1945.8823529411766,
      "grad_norm": 20.299781799316406,
      "learning_rate": 3.0541176470588235e-05,
      "loss": 1.3965,
      "step": 33080
    },
    {
      "epoch": 1946.4705882352941,
      "grad_norm": 23.08163070678711,
      "learning_rate": 3.0535294117647065e-05,
      "loss": 1.4585,
      "step": 33090
    },
    {
      "epoch": 1947.0588235294117,
      "grad_norm": 15.456463813781738,
      "learning_rate": 3.052941176470588e-05,
      "loss": 1.3813,
      "step": 33100
    },
    {
      "epoch": 1947.6470588235295,
      "grad_norm": 18.96067237854004,
      "learning_rate": 3.0523529411764704e-05,
      "loss": 1.431,
      "step": 33110
    },
    {
      "epoch": 1948.235294117647,
      "grad_norm": 18.023624420166016,
      "learning_rate": 3.051764705882353e-05,
      "loss": 1.4377,
      "step": 33120
    },
    {
      "epoch": 1948.8235294117646,
      "grad_norm": 19.436237335205078,
      "learning_rate": 3.0511764705882357e-05,
      "loss": 1.544,
      "step": 33130
    },
    {
      "epoch": 1949.4117647058824,
      "grad_norm": 15.984268188476562,
      "learning_rate": 3.050588235294118e-05,
      "loss": 1.4299,
      "step": 33140
    },
    {
      "epoch": 1950.0,
      "grad_norm": 14.985488891601562,
      "learning_rate": 3.05e-05,
      "loss": 1.3927,
      "step": 33150
    },
    {
      "epoch": 1950.5882352941176,
      "grad_norm": 15.905189514160156,
      "learning_rate": 3.0494117647058822e-05,
      "loss": 1.4859,
      "step": 33160
    },
    {
      "epoch": 1951.1764705882354,
      "grad_norm": 25.195470809936523,
      "learning_rate": 3.0488235294117652e-05,
      "loss": 1.4412,
      "step": 33170
    },
    {
      "epoch": 1951.764705882353,
      "grad_norm": 20.54694366455078,
      "learning_rate": 3.0482352941176472e-05,
      "loss": 1.4173,
      "step": 33180
    },
    {
      "epoch": 1952.3529411764705,
      "grad_norm": 15.915360450744629,
      "learning_rate": 3.0476470588235295e-05,
      "loss": 1.497,
      "step": 33190
    },
    {
      "epoch": 1952.9411764705883,
      "grad_norm": 16.886449813842773,
      "learning_rate": 3.0470588235294118e-05,
      "loss": 1.5474,
      "step": 33200
    },
    {
      "epoch": 1953.5294117647059,
      "grad_norm": 13.089735984802246,
      "learning_rate": 3.0464705882352944e-05,
      "loss": 1.4452,
      "step": 33210
    },
    {
      "epoch": 1954.1176470588234,
      "grad_norm": 14.631329536437988,
      "learning_rate": 3.0458823529411767e-05,
      "loss": 1.4292,
      "step": 33220
    },
    {
      "epoch": 1954.7058823529412,
      "grad_norm": 19.90480613708496,
      "learning_rate": 3.045294117647059e-05,
      "loss": 1.507,
      "step": 33230
    },
    {
      "epoch": 1955.2941176470588,
      "grad_norm": 21.653310775756836,
      "learning_rate": 3.0447058823529413e-05,
      "loss": 1.4471,
      "step": 33240
    },
    {
      "epoch": 1955.8823529411766,
      "grad_norm": 18.183032989501953,
      "learning_rate": 3.0441176470588233e-05,
      "loss": 1.5393,
      "step": 33250
    },
    {
      "epoch": 1956.4705882352941,
      "grad_norm": 17.812856674194336,
      "learning_rate": 3.0435294117647062e-05,
      "loss": 1.3842,
      "step": 33260
    },
    {
      "epoch": 1957.0588235294117,
      "grad_norm": 18.63383674621582,
      "learning_rate": 3.0429411764705885e-05,
      "loss": 1.3762,
      "step": 33270
    },
    {
      "epoch": 1957.6470588235295,
      "grad_norm": 16.913267135620117,
      "learning_rate": 3.0423529411764705e-05,
      "loss": 1.5718,
      "step": 33280
    },
    {
      "epoch": 1958.235294117647,
      "grad_norm": 18.064794540405273,
      "learning_rate": 3.0417647058823528e-05,
      "loss": 1.4755,
      "step": 33290
    },
    {
      "epoch": 1958.8235294117646,
      "grad_norm": 17.621562957763672,
      "learning_rate": 3.0411764705882358e-05,
      "loss": 1.3978,
      "step": 33300
    },
    {
      "epoch": 1959.4117647058824,
      "grad_norm": 13.512092590332031,
      "learning_rate": 3.0405882352941177e-05,
      "loss": 1.3804,
      "step": 33310
    },
    {
      "epoch": 1960.0,
      "grad_norm": 21.451194763183594,
      "learning_rate": 3.04e-05,
      "loss": 1.5099,
      "step": 33320
    },
    {
      "epoch": 1960.5882352941176,
      "grad_norm": 20.151626586914062,
      "learning_rate": 3.0394117647058823e-05,
      "loss": 1.441,
      "step": 33330
    },
    {
      "epoch": 1961.1764705882354,
      "grad_norm": 15.0086088180542,
      "learning_rate": 3.038823529411765e-05,
      "loss": 1.4309,
      "step": 33340
    },
    {
      "epoch": 1961.764705882353,
      "grad_norm": 18.52457046508789,
      "learning_rate": 3.0382352941176473e-05,
      "loss": 1.3809,
      "step": 33350
    },
    {
      "epoch": 1962.3529411764705,
      "grad_norm": 17.121139526367188,
      "learning_rate": 3.0376470588235296e-05,
      "loss": 1.4709,
      "step": 33360
    },
    {
      "epoch": 1962.9411764705883,
      "grad_norm": 17.199180603027344,
      "learning_rate": 3.037058823529412e-05,
      "loss": 1.4847,
      "step": 33370
    },
    {
      "epoch": 1963.5294117647059,
      "grad_norm": 16.482141494750977,
      "learning_rate": 3.0364705882352945e-05,
      "loss": 1.3677,
      "step": 33380
    },
    {
      "epoch": 1964.1176470588234,
      "grad_norm": 17.960891723632812,
      "learning_rate": 3.0358823529411768e-05,
      "loss": 1.5162,
      "step": 33390
    },
    {
      "epoch": 1964.7058823529412,
      "grad_norm": 17.11719512939453,
      "learning_rate": 3.035294117647059e-05,
      "loss": 1.4039,
      "step": 33400
    },
    {
      "epoch": 1965.2941176470588,
      "grad_norm": 20.479694366455078,
      "learning_rate": 3.034705882352941e-05,
      "loss": 1.4859,
      "step": 33410
    },
    {
      "epoch": 1965.8823529411766,
      "grad_norm": 18.385643005371094,
      "learning_rate": 3.0341176470588234e-05,
      "loss": 1.4197,
      "step": 33420
    },
    {
      "epoch": 1966.4705882352941,
      "grad_norm": 16.66425323486328,
      "learning_rate": 3.0335294117647063e-05,
      "loss": 1.4146,
      "step": 33430
    },
    {
      "epoch": 1967.0588235294117,
      "grad_norm": 18.32236099243164,
      "learning_rate": 3.0329411764705883e-05,
      "loss": 1.4151,
      "step": 33440
    },
    {
      "epoch": 1967.6470588235295,
      "grad_norm": 17.125577926635742,
      "learning_rate": 3.0323529411764706e-05,
      "loss": 1.3526,
      "step": 33450
    },
    {
      "epoch": 1968.235294117647,
      "grad_norm": 23.134765625,
      "learning_rate": 3.031764705882353e-05,
      "loss": 1.6092,
      "step": 33460
    },
    {
      "epoch": 1968.8235294117646,
      "grad_norm": 20.021608352661133,
      "learning_rate": 3.0311764705882355e-05,
      "loss": 1.3705,
      "step": 33470
    },
    {
      "epoch": 1969.4117647058824,
      "grad_norm": 19.19270133972168,
      "learning_rate": 3.0305882352941178e-05,
      "loss": 1.4451,
      "step": 33480
    },
    {
      "epoch": 1970.0,
      "grad_norm": 18.967721939086914,
      "learning_rate": 3.03e-05,
      "loss": 1.5094,
      "step": 33490
    },
    {
      "epoch": 1970.5882352941176,
      "grad_norm": 22.30864143371582,
      "learning_rate": 3.0294117647058824e-05,
      "loss": 1.4447,
      "step": 33500
    },
    {
      "epoch": 1971.1764705882354,
      "grad_norm": 20.70465850830078,
      "learning_rate": 3.028823529411765e-05,
      "loss": 1.3577,
      "step": 33510
    },
    {
      "epoch": 1971.764705882353,
      "grad_norm": 15.336941719055176,
      "learning_rate": 3.0282352941176474e-05,
      "loss": 1.313,
      "step": 33520
    },
    {
      "epoch": 1972.3529411764705,
      "grad_norm": 18.62221336364746,
      "learning_rate": 3.0276470588235297e-05,
      "loss": 1.4092,
      "step": 33530
    },
    {
      "epoch": 1972.9411764705883,
      "grad_norm": 22.167186737060547,
      "learning_rate": 3.0270588235294116e-05,
      "loss": 1.4481,
      "step": 33540
    },
    {
      "epoch": 1973.5294117647059,
      "grad_norm": 23.35572052001953,
      "learning_rate": 3.0264705882352946e-05,
      "loss": 1.3981,
      "step": 33550
    },
    {
      "epoch": 1974.1176470588234,
      "grad_norm": 19.84236717224121,
      "learning_rate": 3.0258823529411766e-05,
      "loss": 1.5025,
      "step": 33560
    },
    {
      "epoch": 1974.7058823529412,
      "grad_norm": 18.795795440673828,
      "learning_rate": 3.025294117647059e-05,
      "loss": 1.4008,
      "step": 33570
    },
    {
      "epoch": 1975.2941176470588,
      "grad_norm": 20.114273071289062,
      "learning_rate": 3.024705882352941e-05,
      "loss": 1.4358,
      "step": 33580
    },
    {
      "epoch": 1975.8823529411766,
      "grad_norm": 21.047021865844727,
      "learning_rate": 3.0241176470588238e-05,
      "loss": 1.5018,
      "step": 33590
    },
    {
      "epoch": 1976.4705882352941,
      "grad_norm": 18.520620346069336,
      "learning_rate": 3.023529411764706e-05,
      "loss": 1.3948,
      "step": 33600
    },
    {
      "epoch": 1977.0588235294117,
      "grad_norm": 19.035642623901367,
      "learning_rate": 3.0229411764705884e-05,
      "loss": 1.5965,
      "step": 33610
    },
    {
      "epoch": 1977.6470588235295,
      "grad_norm": 14.396780967712402,
      "learning_rate": 3.0223529411764707e-05,
      "loss": 1.3339,
      "step": 33620
    },
    {
      "epoch": 1978.235294117647,
      "grad_norm": 17.85283851623535,
      "learning_rate": 3.021764705882353e-05,
      "loss": 1.4531,
      "step": 33630
    },
    {
      "epoch": 1978.8235294117646,
      "grad_norm": 20.37982177734375,
      "learning_rate": 3.0211764705882356e-05,
      "loss": 1.3621,
      "step": 33640
    },
    {
      "epoch": 1979.4117647058824,
      "grad_norm": 16.34931755065918,
      "learning_rate": 3.020588235294118e-05,
      "loss": 1.3307,
      "step": 33650
    },
    {
      "epoch": 1980.0,
      "grad_norm": 22.93321990966797,
      "learning_rate": 3.02e-05,
      "loss": 1.4098,
      "step": 33660
    },
    {
      "epoch": 1980.5882352941176,
      "grad_norm": 17.343273162841797,
      "learning_rate": 3.0194117647058822e-05,
      "loss": 1.3808,
      "step": 33670
    },
    {
      "epoch": 1981.1764705882354,
      "grad_norm": 16.286720275878906,
      "learning_rate": 3.018823529411765e-05,
      "loss": 1.5154,
      "step": 33680
    },
    {
      "epoch": 1981.764705882353,
      "grad_norm": 20.792516708374023,
      "learning_rate": 3.018235294117647e-05,
      "loss": 1.4851,
      "step": 33690
    },
    {
      "epoch": 1982.3529411764705,
      "grad_norm": 24.20887565612793,
      "learning_rate": 3.0176470588235294e-05,
      "loss": 1.5984,
      "step": 33700
    },
    {
      "epoch": 1982.9411764705883,
      "grad_norm": 18.478132247924805,
      "learning_rate": 3.0170588235294117e-05,
      "loss": 1.4302,
      "step": 33710
    },
    {
      "epoch": 1983.5294117647059,
      "grad_norm": 19.850440979003906,
      "learning_rate": 3.0164705882352944e-05,
      "loss": 1.403,
      "step": 33720
    },
    {
      "epoch": 1984.1176470588234,
      "grad_norm": 17.967763900756836,
      "learning_rate": 3.0158823529411767e-05,
      "loss": 1.5426,
      "step": 33730
    },
    {
      "epoch": 1984.7058823529412,
      "grad_norm": 19.005346298217773,
      "learning_rate": 3.015294117647059e-05,
      "loss": 1.4379,
      "step": 33740
    },
    {
      "epoch": 1985.2941176470588,
      "grad_norm": 16.994218826293945,
      "learning_rate": 3.0147058823529413e-05,
      "loss": 1.4364,
      "step": 33750
    },
    {
      "epoch": 1985.8823529411766,
      "grad_norm": 17.968856811523438,
      "learning_rate": 3.014117647058824e-05,
      "loss": 1.3597,
      "step": 33760
    },
    {
      "epoch": 1986.4705882352941,
      "grad_norm": 17.651126861572266,
      "learning_rate": 3.0135294117647062e-05,
      "loss": 1.4904,
      "step": 33770
    },
    {
      "epoch": 1987.0588235294117,
      "grad_norm": 17.401681900024414,
      "learning_rate": 3.0129411764705885e-05,
      "loss": 1.5709,
      "step": 33780
    },
    {
      "epoch": 1987.6470588235295,
      "grad_norm": 16.211820602416992,
      "learning_rate": 3.0123529411764704e-05,
      "loss": 1.4691,
      "step": 33790
    },
    {
      "epoch": 1988.235294117647,
      "grad_norm": 19.546607971191406,
      "learning_rate": 3.0117647058823527e-05,
      "loss": 1.4946,
      "step": 33800
    },
    {
      "epoch": 1988.8235294117646,
      "grad_norm": 17.140104293823242,
      "learning_rate": 3.0111764705882357e-05,
      "loss": 1.4943,
      "step": 33810
    },
    {
      "epoch": 1989.4117647058824,
      "grad_norm": 15.903889656066895,
      "learning_rate": 3.0105882352941177e-05,
      "loss": 1.4116,
      "step": 33820
    },
    {
      "epoch": 1990.0,
      "grad_norm": 25.330469131469727,
      "learning_rate": 3.01e-05,
      "loss": 1.4313,
      "step": 33830
    },
    {
      "epoch": 1990.5882352941176,
      "grad_norm": 17.016176223754883,
      "learning_rate": 3.0094117647058823e-05,
      "loss": 1.3822,
      "step": 33840
    },
    {
      "epoch": 1991.1764705882354,
      "grad_norm": 17.40071678161621,
      "learning_rate": 3.008823529411765e-05,
      "loss": 1.412,
      "step": 33850
    },
    {
      "epoch": 1991.764705882353,
      "grad_norm": 22.531280517578125,
      "learning_rate": 3.0082352941176472e-05,
      "loss": 1.4423,
      "step": 33860
    },
    {
      "epoch": 1992.3529411764705,
      "grad_norm": 19.511756896972656,
      "learning_rate": 3.0076470588235295e-05,
      "loss": 1.4857,
      "step": 33870
    },
    {
      "epoch": 1992.9411764705883,
      "grad_norm": 17.600418090820312,
      "learning_rate": 3.0070588235294118e-05,
      "loss": 1.5134,
      "step": 33880
    },
    {
      "epoch": 1993.5294117647059,
      "grad_norm": 15.824353218078613,
      "learning_rate": 3.0064705882352945e-05,
      "loss": 1.4627,
      "step": 33890
    },
    {
      "epoch": 1994.1176470588234,
      "grad_norm": 16.05689811706543,
      "learning_rate": 3.0058823529411767e-05,
      "loss": 1.3397,
      "step": 33900
    },
    {
      "epoch": 1994.7058823529412,
      "grad_norm": 24.600507736206055,
      "learning_rate": 3.005294117647059e-05,
      "loss": 1.3922,
      "step": 33910
    },
    {
      "epoch": 1995.2941176470588,
      "grad_norm": 18.17123031616211,
      "learning_rate": 3.004705882352941e-05,
      "loss": 1.4191,
      "step": 33920
    },
    {
      "epoch": 1995.8823529411766,
      "grad_norm": 20.46461296081543,
      "learning_rate": 3.004117647058824e-05,
      "loss": 1.4621,
      "step": 33930
    },
    {
      "epoch": 1996.4705882352941,
      "grad_norm": 18.80616569519043,
      "learning_rate": 3.0035294117647063e-05,
      "loss": 1.5144,
      "step": 33940
    },
    {
      "epoch": 1997.0588235294117,
      "grad_norm": 21.293291091918945,
      "learning_rate": 3.0029411764705882e-05,
      "loss": 1.4257,
      "step": 33950
    },
    {
      "epoch": 1997.6470588235295,
      "grad_norm": 18.65142822265625,
      "learning_rate": 3.0023529411764705e-05,
      "loss": 1.3526,
      "step": 33960
    },
    {
      "epoch": 1998.235294117647,
      "grad_norm": 15.476202011108398,
      "learning_rate": 3.001764705882353e-05,
      "loss": 1.4474,
      "step": 33970
    },
    {
      "epoch": 1998.8235294117646,
      "grad_norm": 22.733701705932617,
      "learning_rate": 3.0011764705882355e-05,
      "loss": 1.4237,
      "step": 33980
    },
    {
      "epoch": 1999.4117647058824,
      "grad_norm": 18.375600814819336,
      "learning_rate": 3.0005882352941178e-05,
      "loss": 1.39,
      "step": 33990
    },
    {
      "epoch": 2000.0,
      "grad_norm": 19.8796443939209,
      "learning_rate": 3e-05,
      "loss": 1.3387,
      "step": 34000
    },
    {
      "epoch": 2000.5882352941176,
      "grad_norm": 19.56934356689453,
      "learning_rate": 2.9994117647058824e-05,
      "loss": 1.4329,
      "step": 34010
    },
    {
      "epoch": 2001.1764705882354,
      "grad_norm": 16.05986785888672,
      "learning_rate": 2.998823529411765e-05,
      "loss": 1.5202,
      "step": 34020
    },
    {
      "epoch": 2001.764705882353,
      "grad_norm": 16.456573486328125,
      "learning_rate": 2.9982352941176473e-05,
      "loss": 1.3304,
      "step": 34030
    },
    {
      "epoch": 2002.3529411764705,
      "grad_norm": 15.983977317810059,
      "learning_rate": 2.9976470588235296e-05,
      "loss": 1.3129,
      "step": 34040
    },
    {
      "epoch": 2002.9411764705883,
      "grad_norm": 28.85066795349121,
      "learning_rate": 2.9970588235294116e-05,
      "loss": 1.4847,
      "step": 34050
    },
    {
      "epoch": 2003.5294117647059,
      "grad_norm": 24.52608871459961,
      "learning_rate": 2.9964705882352945e-05,
      "loss": 1.3799,
      "step": 34060
    },
    {
      "epoch": 2004.1176470588234,
      "grad_norm": 17.33782196044922,
      "learning_rate": 2.995882352941177e-05,
      "loss": 1.4147,
      "step": 34070
    },
    {
      "epoch": 2004.7058823529412,
      "grad_norm": 18.772777557373047,
      "learning_rate": 2.9952941176470588e-05,
      "loss": 1.3686,
      "step": 34080
    },
    {
      "epoch": 2005.2941176470588,
      "grad_norm": 17.20423698425293,
      "learning_rate": 2.994705882352941e-05,
      "loss": 1.3584,
      "step": 34090
    },
    {
      "epoch": 2005.8823529411766,
      "grad_norm": 15.224227905273438,
      "learning_rate": 2.994117647058824e-05,
      "loss": 1.3499,
      "step": 34100
    },
    {
      "epoch": 2006.4705882352941,
      "grad_norm": 20.156503677368164,
      "learning_rate": 2.993529411764706e-05,
      "loss": 1.3264,
      "step": 34110
    },
    {
      "epoch": 2007.0588235294117,
      "grad_norm": 23.68451690673828,
      "learning_rate": 2.9929411764705883e-05,
      "loss": 1.3827,
      "step": 34120
    },
    {
      "epoch": 2007.6470588235295,
      "grad_norm": 15.392404556274414,
      "learning_rate": 2.9923529411764706e-05,
      "loss": 1.4747,
      "step": 34130
    },
    {
      "epoch": 2008.235294117647,
      "grad_norm": 16.032224655151367,
      "learning_rate": 2.9917647058823533e-05,
      "loss": 1.5138,
      "step": 34140
    },
    {
      "epoch": 2008.8235294117646,
      "grad_norm": 16.525074005126953,
      "learning_rate": 2.9911764705882356e-05,
      "loss": 1.3279,
      "step": 34150
    },
    {
      "epoch": 2009.4117647058824,
      "grad_norm": 17.85892105102539,
      "learning_rate": 2.990588235294118e-05,
      "loss": 1.3537,
      "step": 34160
    },
    {
      "epoch": 2010.0,
      "grad_norm": 16.624923706054688,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 1.5632,
      "step": 34170
    },
    {
      "epoch": 2010.5882352941176,
      "grad_norm": 12.925684928894043,
      "learning_rate": 2.989411764705882e-05,
      "loss": 1.3904,
      "step": 34180
    },
    {
      "epoch": 2011.1764705882354,
      "grad_norm": 19.685461044311523,
      "learning_rate": 2.988823529411765e-05,
      "loss": 1.5005,
      "step": 34190
    },
    {
      "epoch": 2011.764705882353,
      "grad_norm": 23.955368041992188,
      "learning_rate": 2.9882352941176474e-05,
      "loss": 1.3307,
      "step": 34200
    },
    {
      "epoch": 2012.3529411764705,
      "grad_norm": 15.346173286437988,
      "learning_rate": 2.9876470588235294e-05,
      "loss": 1.2701,
      "step": 34210
    },
    {
      "epoch": 2012.9411764705883,
      "grad_norm": 16.098817825317383,
      "learning_rate": 2.9870588235294117e-05,
      "loss": 1.4463,
      "step": 34220
    },
    {
      "epoch": 2013.5294117647059,
      "grad_norm": 18.13133430480957,
      "learning_rate": 2.9864705882352943e-05,
      "loss": 1.4311,
      "step": 34230
    },
    {
      "epoch": 2014.1176470588234,
      "grad_norm": 21.376296997070312,
      "learning_rate": 2.9858823529411766e-05,
      "loss": 1.299,
      "step": 34240
    },
    {
      "epoch": 2014.7058823529412,
      "grad_norm": 19.251916885375977,
      "learning_rate": 2.985294117647059e-05,
      "loss": 1.53,
      "step": 34250
    },
    {
      "epoch": 2015.2941176470588,
      "grad_norm": 16.68663787841797,
      "learning_rate": 2.9847058823529412e-05,
      "loss": 1.4562,
      "step": 34260
    },
    {
      "epoch": 2015.8823529411766,
      "grad_norm": 17.14197540283203,
      "learning_rate": 2.984117647058824e-05,
      "loss": 1.4033,
      "step": 34270
    },
    {
      "epoch": 2016.4705882352941,
      "grad_norm": 22.209932327270508,
      "learning_rate": 2.983529411764706e-05,
      "loss": 1.361,
      "step": 34280
    },
    {
      "epoch": 2017.0588235294117,
      "grad_norm": 18.384464263916016,
      "learning_rate": 2.9829411764705884e-05,
      "loss": 1.4522,
      "step": 34290
    },
    {
      "epoch": 2017.6470588235295,
      "grad_norm": 16.422502517700195,
      "learning_rate": 2.9823529411764707e-05,
      "loss": 1.4413,
      "step": 34300
    },
    {
      "epoch": 2018.235294117647,
      "grad_norm": 19.52750587463379,
      "learning_rate": 2.9817647058823534e-05,
      "loss": 1.5197,
      "step": 34310
    },
    {
      "epoch": 2018.8235294117646,
      "grad_norm": 17.15406608581543,
      "learning_rate": 2.9811764705882357e-05,
      "loss": 1.4384,
      "step": 34320
    },
    {
      "epoch": 2019.4117647058824,
      "grad_norm": 20.838655471801758,
      "learning_rate": 2.9805882352941176e-05,
      "loss": 1.4908,
      "step": 34330
    },
    {
      "epoch": 2020.0,
      "grad_norm": 19.568193435668945,
      "learning_rate": 2.98e-05,
      "loss": 1.3796,
      "step": 34340
    },
    {
      "epoch": 2020.5882352941176,
      "grad_norm": 23.421966552734375,
      "learning_rate": 2.9794117647058822e-05,
      "loss": 1.3344,
      "step": 34350
    },
    {
      "epoch": 2021.1764705882354,
      "grad_norm": 14.444867134094238,
      "learning_rate": 2.978823529411765e-05,
      "loss": 1.3047,
      "step": 34360
    },
    {
      "epoch": 2021.764705882353,
      "grad_norm": 16.306678771972656,
      "learning_rate": 2.978235294117647e-05,
      "loss": 1.315,
      "step": 34370
    },
    {
      "epoch": 2022.3529411764705,
      "grad_norm": 14.786663055419922,
      "learning_rate": 2.9776470588235295e-05,
      "loss": 1.3887,
      "step": 34380
    },
    {
      "epoch": 2022.9411764705883,
      "grad_norm": 18.39568519592285,
      "learning_rate": 2.9770588235294118e-05,
      "loss": 1.4521,
      "step": 34390
    },
    {
      "epoch": 2023.5294117647059,
      "grad_norm": 18.90123748779297,
      "learning_rate": 2.9764705882352944e-05,
      "loss": 1.4796,
      "step": 34400
    },
    {
      "epoch": 2024.1176470588234,
      "grad_norm": 19.529253005981445,
      "learning_rate": 2.9758823529411767e-05,
      "loss": 1.248,
      "step": 34410
    },
    {
      "epoch": 2024.7058823529412,
      "grad_norm": 16.667579650878906,
      "learning_rate": 2.975294117647059e-05,
      "loss": 1.3948,
      "step": 34420
    },
    {
      "epoch": 2025.2941176470588,
      "grad_norm": 17.19620704650879,
      "learning_rate": 2.974705882352941e-05,
      "loss": 1.3475,
      "step": 34430
    },
    {
      "epoch": 2025.8823529411766,
      "grad_norm": 16.705617904663086,
      "learning_rate": 2.974117647058824e-05,
      "loss": 1.417,
      "step": 34440
    },
    {
      "epoch": 2026.4705882352941,
      "grad_norm": 13.221810340881348,
      "learning_rate": 2.9735294117647062e-05,
      "loss": 1.2267,
      "step": 34450
    },
    {
      "epoch": 2027.0588235294117,
      "grad_norm": 19.190547943115234,
      "learning_rate": 2.9729411764705882e-05,
      "loss": 1.3439,
      "step": 34460
    },
    {
      "epoch": 2027.6470588235295,
      "grad_norm": 16.39305877685547,
      "learning_rate": 2.9723529411764705e-05,
      "loss": 1.3766,
      "step": 34470
    },
    {
      "epoch": 2028.235294117647,
      "grad_norm": 14.852396965026855,
      "learning_rate": 2.9717647058823535e-05,
      "loss": 1.3783,
      "step": 34480
    },
    {
      "epoch": 2028.8235294117646,
      "grad_norm": 17.674448013305664,
      "learning_rate": 2.9711764705882354e-05,
      "loss": 1.3267,
      "step": 34490
    },
    {
      "epoch": 2029.4117647058824,
      "grad_norm": 19.30855369567871,
      "learning_rate": 2.9705882352941177e-05,
      "loss": 1.3928,
      "step": 34500
    },
    {
      "epoch": 2030.0,
      "grad_norm": 20.829824447631836,
      "learning_rate": 2.97e-05,
      "loss": 1.435,
      "step": 34510
    },
    {
      "epoch": 2030.5882352941176,
      "grad_norm": 16.555286407470703,
      "learning_rate": 2.9694117647058823e-05,
      "loss": 1.3286,
      "step": 34520
    },
    {
      "epoch": 2031.1764705882354,
      "grad_norm": 16.15996551513672,
      "learning_rate": 2.968823529411765e-05,
      "loss": 1.3339,
      "step": 34530
    },
    {
      "epoch": 2031.764705882353,
      "grad_norm": 17.614049911499023,
      "learning_rate": 2.9682352941176473e-05,
      "loss": 1.4131,
      "step": 34540
    },
    {
      "epoch": 2032.3529411764705,
      "grad_norm": 17.852684020996094,
      "learning_rate": 2.9676470588235296e-05,
      "loss": 1.4609,
      "step": 34550
    },
    {
      "epoch": 2032.9411764705883,
      "grad_norm": 22.476118087768555,
      "learning_rate": 2.9670588235294115e-05,
      "loss": 1.4348,
      "step": 34560
    },
    {
      "epoch": 2033.5294117647059,
      "grad_norm": 20.821880340576172,
      "learning_rate": 2.9664705882352945e-05,
      "loss": 1.5674,
      "step": 34570
    },
    {
      "epoch": 2034.1176470588234,
      "grad_norm": 19.87653350830078,
      "learning_rate": 2.9658823529411768e-05,
      "loss": 1.4461,
      "step": 34580
    },
    {
      "epoch": 2034.7058823529412,
      "grad_norm": 15.677770614624023,
      "learning_rate": 2.9652941176470588e-05,
      "loss": 1.3974,
      "step": 34590
    },
    {
      "epoch": 2035.2941176470588,
      "grad_norm": 15.761675834655762,
      "learning_rate": 2.964705882352941e-05,
      "loss": 1.3769,
      "step": 34600
    },
    {
      "epoch": 2035.8823529411766,
      "grad_norm": 16.55217170715332,
      "learning_rate": 2.964117647058824e-05,
      "loss": 1.4384,
      "step": 34610
    },
    {
      "epoch": 2036.4705882352941,
      "grad_norm": 19.766883850097656,
      "learning_rate": 2.963529411764706e-05,
      "loss": 1.4898,
      "step": 34620
    },
    {
      "epoch": 2037.0588235294117,
      "grad_norm": 15.830415725708008,
      "learning_rate": 2.9629411764705883e-05,
      "loss": 1.3882,
      "step": 34630
    },
    {
      "epoch": 2037.6470588235295,
      "grad_norm": 18.510385513305664,
      "learning_rate": 2.9623529411764706e-05,
      "loss": 1.2814,
      "step": 34640
    },
    {
      "epoch": 2038.235294117647,
      "grad_norm": 26.430570602416992,
      "learning_rate": 2.9617647058823532e-05,
      "loss": 1.3644,
      "step": 34650
    },
    {
      "epoch": 2038.8235294117646,
      "grad_norm": 14.278817176818848,
      "learning_rate": 2.9611764705882355e-05,
      "loss": 1.322,
      "step": 34660
    },
    {
      "epoch": 2039.4117647058824,
      "grad_norm": 18.354524612426758,
      "learning_rate": 2.9605882352941178e-05,
      "loss": 1.4214,
      "step": 34670
    },
    {
      "epoch": 2040.0,
      "grad_norm": 25.368534088134766,
      "learning_rate": 2.96e-05,
      "loss": 1.4609,
      "step": 34680
    },
    {
      "epoch": 2040.5882352941176,
      "grad_norm": 17.26015853881836,
      "learning_rate": 2.9594117647058828e-05,
      "loss": 1.4586,
      "step": 34690
    },
    {
      "epoch": 2041.1764705882354,
      "grad_norm": 22.734485626220703,
      "learning_rate": 2.958823529411765e-05,
      "loss": 1.3874,
      "step": 34700
    },
    {
      "epoch": 2041.764705882353,
      "grad_norm": 16.935922622680664,
      "learning_rate": 2.9582352941176474e-05,
      "loss": 1.4339,
      "step": 34710
    },
    {
      "epoch": 2042.3529411764705,
      "grad_norm": 15.335552215576172,
      "learning_rate": 2.9576470588235293e-05,
      "loss": 1.268,
      "step": 34720
    },
    {
      "epoch": 2042.9411764705883,
      "grad_norm": 17.22701072692871,
      "learning_rate": 2.9570588235294116e-05,
      "loss": 1.2933,
      "step": 34730
    },
    {
      "epoch": 2043.5294117647059,
      "grad_norm": 15.666301727294922,
      "learning_rate": 2.9564705882352946e-05,
      "loss": 1.339,
      "step": 34740
    },
    {
      "epoch": 2044.1176470588234,
      "grad_norm": 14.234583854675293,
      "learning_rate": 2.9558823529411766e-05,
      "loss": 1.4581,
      "step": 34750
    },
    {
      "epoch": 2044.7058823529412,
      "grad_norm": 17.424184799194336,
      "learning_rate": 2.955294117647059e-05,
      "loss": 1.2993,
      "step": 34760
    },
    {
      "epoch": 2045.2941176470588,
      "grad_norm": 17.426340103149414,
      "learning_rate": 2.954705882352941e-05,
      "loss": 1.3048,
      "step": 34770
    },
    {
      "epoch": 2045.8823529411766,
      "grad_norm": 16.617597579956055,
      "learning_rate": 2.9541176470588238e-05,
      "loss": 1.4303,
      "step": 34780
    },
    {
      "epoch": 2046.4705882352941,
      "grad_norm": 19.927215576171875,
      "learning_rate": 2.953529411764706e-05,
      "loss": 1.2307,
      "step": 34790
    },
    {
      "epoch": 2047.0588235294117,
      "grad_norm": 19.161958694458008,
      "learning_rate": 2.9529411764705884e-05,
      "loss": 1.2287,
      "step": 34800
    },
    {
      "epoch": 2047.6470588235295,
      "grad_norm": 18.94289207458496,
      "learning_rate": 2.9523529411764707e-05,
      "loss": 1.2719,
      "step": 34810
    },
    {
      "epoch": 2048.235294117647,
      "grad_norm": 21.542152404785156,
      "learning_rate": 2.9517647058823533e-05,
      "loss": 1.3448,
      "step": 34820
    },
    {
      "epoch": 2048.823529411765,
      "grad_norm": 19.996973037719727,
      "learning_rate": 2.9511764705882356e-05,
      "loss": 1.3146,
      "step": 34830
    },
    {
      "epoch": 2049.4117647058824,
      "grad_norm": 16.759475708007812,
      "learning_rate": 2.950588235294118e-05,
      "loss": 1.464,
      "step": 34840
    },
    {
      "epoch": 2050.0,
      "grad_norm": 21.351911544799805,
      "learning_rate": 2.95e-05,
      "loss": 1.3263,
      "step": 34850
    },
    {
      "epoch": 2050.5882352941176,
      "grad_norm": 21.62161636352539,
      "learning_rate": 2.949411764705883e-05,
      "loss": 1.4288,
      "step": 34860
    },
    {
      "epoch": 2051.176470588235,
      "grad_norm": 21.297590255737305,
      "learning_rate": 2.948823529411765e-05,
      "loss": 1.4898,
      "step": 34870
    },
    {
      "epoch": 2051.764705882353,
      "grad_norm": 20.480377197265625,
      "learning_rate": 2.948235294117647e-05,
      "loss": 1.4318,
      "step": 34880
    },
    {
      "epoch": 2052.3529411764707,
      "grad_norm": 17.40452766418457,
      "learning_rate": 2.9476470588235294e-05,
      "loss": 1.3697,
      "step": 34890
    },
    {
      "epoch": 2052.9411764705883,
      "grad_norm": 19.9794979095459,
      "learning_rate": 2.9470588235294117e-05,
      "loss": 1.3321,
      "step": 34900
    },
    {
      "epoch": 2053.529411764706,
      "grad_norm": 18.539396286010742,
      "learning_rate": 2.9464705882352943e-05,
      "loss": 1.3259,
      "step": 34910
    },
    {
      "epoch": 2054.1176470588234,
      "grad_norm": 19.57536506652832,
      "learning_rate": 2.9458823529411766e-05,
      "loss": 1.3807,
      "step": 34920
    },
    {
      "epoch": 2054.705882352941,
      "grad_norm": 16.191102981567383,
      "learning_rate": 2.945294117647059e-05,
      "loss": 1.4078,
      "step": 34930
    },
    {
      "epoch": 2055.294117647059,
      "grad_norm": 19.10843849182129,
      "learning_rate": 2.9447058823529412e-05,
      "loss": 1.3924,
      "step": 34940
    },
    {
      "epoch": 2055.8823529411766,
      "grad_norm": 23.02724838256836,
      "learning_rate": 2.944117647058824e-05,
      "loss": 1.3686,
      "step": 34950
    },
    {
      "epoch": 2056.470588235294,
      "grad_norm": 17.535877227783203,
      "learning_rate": 2.9435294117647062e-05,
      "loss": 1.3915,
      "step": 34960
    },
    {
      "epoch": 2057.0588235294117,
      "grad_norm": 15.910518646240234,
      "learning_rate": 2.9429411764705885e-05,
      "loss": 1.358,
      "step": 34970
    },
    {
      "epoch": 2057.6470588235293,
      "grad_norm": 19.012359619140625,
      "learning_rate": 2.9423529411764704e-05,
      "loss": 1.3264,
      "step": 34980
    },
    {
      "epoch": 2058.235294117647,
      "grad_norm": 15.877645492553711,
      "learning_rate": 2.9417647058823534e-05,
      "loss": 1.4536,
      "step": 34990
    },
    {
      "epoch": 2058.823529411765,
      "grad_norm": 14.237946510314941,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 1.3175,
      "step": 35000
    },
    {
      "epoch": 2059.4117647058824,
      "grad_norm": 15.819842338562012,
      "learning_rate": 2.9405882352941177e-05,
      "loss": 1.3984,
      "step": 35010
    },
    {
      "epoch": 2060.0,
      "grad_norm": 25.766889572143555,
      "learning_rate": 2.94e-05,
      "loss": 1.3468,
      "step": 35020
    },
    {
      "epoch": 2060.5882352941176,
      "grad_norm": 18.516298294067383,
      "learning_rate": 2.9394117647058826e-05,
      "loss": 1.4372,
      "step": 35030
    },
    {
      "epoch": 2061.176470588235,
      "grad_norm": 20.824480056762695,
      "learning_rate": 2.938823529411765e-05,
      "loss": 1.3751,
      "step": 35040
    },
    {
      "epoch": 2061.764705882353,
      "grad_norm": 16.229310989379883,
      "learning_rate": 2.9382352941176472e-05,
      "loss": 1.3028,
      "step": 35050
    },
    {
      "epoch": 2062.3529411764707,
      "grad_norm": 15.236063003540039,
      "learning_rate": 2.9376470588235295e-05,
      "loss": 1.2935,
      "step": 35060
    },
    {
      "epoch": 2062.9411764705883,
      "grad_norm": 13.710939407348633,
      "learning_rate": 2.9370588235294118e-05,
      "loss": 1.5243,
      "step": 35070
    },
    {
      "epoch": 2063.529411764706,
      "grad_norm": 22.184362411499023,
      "learning_rate": 2.9364705882352944e-05,
      "loss": 1.3074,
      "step": 35080
    },
    {
      "epoch": 2064.1176470588234,
      "grad_norm": 14.133041381835938,
      "learning_rate": 2.9358823529411767e-05,
      "loss": 1.4603,
      "step": 35090
    },
    {
      "epoch": 2064.705882352941,
      "grad_norm": 17.15983772277832,
      "learning_rate": 2.9352941176470587e-05,
      "loss": 1.3032,
      "step": 35100
    },
    {
      "epoch": 2065.294117647059,
      "grad_norm": 29.76264762878418,
      "learning_rate": 2.934705882352941e-05,
      "loss": 1.3011,
      "step": 35110
    },
    {
      "epoch": 2065.8823529411766,
      "grad_norm": 20.50299644470215,
      "learning_rate": 2.934117647058824e-05,
      "loss": 1.2197,
      "step": 35120
    },
    {
      "epoch": 2066.470588235294,
      "grad_norm": 16.76824951171875,
      "learning_rate": 2.933529411764706e-05,
      "loss": 1.4332,
      "step": 35130
    },
    {
      "epoch": 2067.0588235294117,
      "grad_norm": 23.173486709594727,
      "learning_rate": 2.9329411764705882e-05,
      "loss": 1.351,
      "step": 35140
    },
    {
      "epoch": 2067.6470588235293,
      "grad_norm": 17.194482803344727,
      "learning_rate": 2.9323529411764705e-05,
      "loss": 1.343,
      "step": 35150
    },
    {
      "epoch": 2068.235294117647,
      "grad_norm": 21.609451293945312,
      "learning_rate": 2.9317647058823532e-05,
      "loss": 1.3152,
      "step": 35160
    },
    {
      "epoch": 2068.823529411765,
      "grad_norm": 18.93007469177246,
      "learning_rate": 2.9311764705882355e-05,
      "loss": 1.3428,
      "step": 35170
    },
    {
      "epoch": 2069.4117647058824,
      "grad_norm": 21.200946807861328,
      "learning_rate": 2.9305882352941178e-05,
      "loss": 1.4754,
      "step": 35180
    },
    {
      "epoch": 2070.0,
      "grad_norm": 23.199954986572266,
      "learning_rate": 2.93e-05,
      "loss": 1.333,
      "step": 35190
    },
    {
      "epoch": 2070.5882352941176,
      "grad_norm": 14.359898567199707,
      "learning_rate": 2.9294117647058827e-05,
      "loss": 1.3664,
      "step": 35200
    },
    {
      "epoch": 2071.176470588235,
      "grad_norm": 18.510637283325195,
      "learning_rate": 2.928823529411765e-05,
      "loss": 1.4081,
      "step": 35210
    },
    {
      "epoch": 2071.764705882353,
      "grad_norm": 21.109926223754883,
      "learning_rate": 2.9282352941176473e-05,
      "loss": 1.4543,
      "step": 35220
    },
    {
      "epoch": 2072.3529411764707,
      "grad_norm": 14.944061279296875,
      "learning_rate": 2.9276470588235293e-05,
      "loss": 1.3099,
      "step": 35230
    },
    {
      "epoch": 2072.9411764705883,
      "grad_norm": 20.3192195892334,
      "learning_rate": 2.9270588235294116e-05,
      "loss": 1.3799,
      "step": 35240
    },
    {
      "epoch": 2073.529411764706,
      "grad_norm": 20.665746688842773,
      "learning_rate": 2.9264705882352945e-05,
      "loss": 1.3669,
      "step": 35250
    },
    {
      "epoch": 2074.1176470588234,
      "grad_norm": 20.23188018798828,
      "learning_rate": 2.9258823529411765e-05,
      "loss": 1.4168,
      "step": 35260
    },
    {
      "epoch": 2074.705882352941,
      "grad_norm": 22.941123962402344,
      "learning_rate": 2.9252941176470588e-05,
      "loss": 1.3874,
      "step": 35270
    },
    {
      "epoch": 2075.294117647059,
      "grad_norm": 17.250234603881836,
      "learning_rate": 2.924705882352941e-05,
      "loss": 1.3456,
      "step": 35280
    },
    {
      "epoch": 2075.8823529411766,
      "grad_norm": 20.23567008972168,
      "learning_rate": 2.9241176470588237e-05,
      "loss": 1.3017,
      "step": 35290
    },
    {
      "epoch": 2076.470588235294,
      "grad_norm": 18.941612243652344,
      "learning_rate": 2.923529411764706e-05,
      "loss": 1.3701,
      "step": 35300
    },
    {
      "epoch": 2077.0588235294117,
      "grad_norm": 21.89930534362793,
      "learning_rate": 2.9229411764705883e-05,
      "loss": 1.3293,
      "step": 35310
    },
    {
      "epoch": 2077.6470588235293,
      "grad_norm": 17.43813133239746,
      "learning_rate": 2.9223529411764706e-05,
      "loss": 1.437,
      "step": 35320
    },
    {
      "epoch": 2078.235294117647,
      "grad_norm": 20.103092193603516,
      "learning_rate": 2.9217647058823533e-05,
      "loss": 1.4287,
      "step": 35330
    },
    {
      "epoch": 2078.823529411765,
      "grad_norm": 16.602312088012695,
      "learning_rate": 2.9211764705882356e-05,
      "loss": 1.2689,
      "step": 35340
    },
    {
      "epoch": 2079.4117647058824,
      "grad_norm": 17.33932113647461,
      "learning_rate": 2.920588235294118e-05,
      "loss": 1.4051,
      "step": 35350
    },
    {
      "epoch": 2080.0,
      "grad_norm": 24.94896697998047,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 1.3866,
      "step": 35360
    },
    {
      "epoch": 2080.5882352941176,
      "grad_norm": 18.22679328918457,
      "learning_rate": 2.9194117647058828e-05,
      "loss": 1.4798,
      "step": 35370
    },
    {
      "epoch": 2081.176470588235,
      "grad_norm": 17.367162704467773,
      "learning_rate": 2.918823529411765e-05,
      "loss": 1.3237,
      "step": 35380
    },
    {
      "epoch": 2081.764705882353,
      "grad_norm": 19.913057327270508,
      "learning_rate": 2.918235294117647e-05,
      "loss": 1.3766,
      "step": 35390
    },
    {
      "epoch": 2082.3529411764707,
      "grad_norm": 16.964536666870117,
      "learning_rate": 2.9176470588235294e-05,
      "loss": 1.3987,
      "step": 35400
    },
    {
      "epoch": 2082.9411764705883,
      "grad_norm": 15.546357154846191,
      "learning_rate": 2.9170588235294123e-05,
      "loss": 1.4165,
      "step": 35410
    },
    {
      "epoch": 2083.529411764706,
      "grad_norm": 16.1253662109375,
      "learning_rate": 2.9164705882352943e-05,
      "loss": 1.3521,
      "step": 35420
    },
    {
      "epoch": 2084.1176470588234,
      "grad_norm": 19.987529754638672,
      "learning_rate": 2.9158823529411766e-05,
      "loss": 1.3721,
      "step": 35430
    },
    {
      "epoch": 2084.705882352941,
      "grad_norm": 20.215152740478516,
      "learning_rate": 2.915294117647059e-05,
      "loss": 1.4592,
      "step": 35440
    },
    {
      "epoch": 2085.294117647059,
      "grad_norm": 17.350187301635742,
      "learning_rate": 2.9147058823529412e-05,
      "loss": 1.3292,
      "step": 35450
    },
    {
      "epoch": 2085.8823529411766,
      "grad_norm": 17.868879318237305,
      "learning_rate": 2.9141176470588238e-05,
      "loss": 1.3585,
      "step": 35460
    },
    {
      "epoch": 2086.470588235294,
      "grad_norm": 21.050546646118164,
      "learning_rate": 2.913529411764706e-05,
      "loss": 1.3035,
      "step": 35470
    },
    {
      "epoch": 2087.0588235294117,
      "grad_norm": 21.34027862548828,
      "learning_rate": 2.9129411764705884e-05,
      "loss": 1.3812,
      "step": 35480
    },
    {
      "epoch": 2087.6470588235293,
      "grad_norm": 17.928119659423828,
      "learning_rate": 2.9123529411764704e-05,
      "loss": 1.3874,
      "step": 35490
    },
    {
      "epoch": 2088.235294117647,
      "grad_norm": 21.222257614135742,
      "learning_rate": 2.9117647058823534e-05,
      "loss": 1.446,
      "step": 35500
    },
    {
      "epoch": 2088.823529411765,
      "grad_norm": 21.742984771728516,
      "learning_rate": 2.9111764705882357e-05,
      "loss": 1.2487,
      "step": 35510
    },
    {
      "epoch": 2089.4117647058824,
      "grad_norm": 17.919424057006836,
      "learning_rate": 2.9105882352941176e-05,
      "loss": 1.5006,
      "step": 35520
    },
    {
      "epoch": 2090.0,
      "grad_norm": 25.573884963989258,
      "learning_rate": 2.91e-05,
      "loss": 1.2099,
      "step": 35530
    },
    {
      "epoch": 2090.5882352941176,
      "grad_norm": 19.677160263061523,
      "learning_rate": 2.9094117647058826e-05,
      "loss": 1.386,
      "step": 35540
    },
    {
      "epoch": 2091.176470588235,
      "grad_norm": 18.673524856567383,
      "learning_rate": 2.908823529411765e-05,
      "loss": 1.3668,
      "step": 35550
    },
    {
      "epoch": 2091.764705882353,
      "grad_norm": 21.683130264282227,
      "learning_rate": 2.908235294117647e-05,
      "loss": 1.4948,
      "step": 35560
    },
    {
      "epoch": 2092.3529411764707,
      "grad_norm": 13.357389450073242,
      "learning_rate": 2.9076470588235295e-05,
      "loss": 1.3436,
      "step": 35570
    },
    {
      "epoch": 2092.9411764705883,
      "grad_norm": 21.28722381591797,
      "learning_rate": 2.907058823529412e-05,
      "loss": 1.4385,
      "step": 35580
    },
    {
      "epoch": 2093.529411764706,
      "grad_norm": 18.133577346801758,
      "learning_rate": 2.9064705882352944e-05,
      "loss": 1.3489,
      "step": 35590
    },
    {
      "epoch": 2094.1176470588234,
      "grad_norm": 16.64915657043457,
      "learning_rate": 2.9058823529411767e-05,
      "loss": 1.4496,
      "step": 35600
    },
    {
      "epoch": 2094.705882352941,
      "grad_norm": 16.656742095947266,
      "learning_rate": 2.905294117647059e-05,
      "loss": 1.2108,
      "step": 35610
    },
    {
      "epoch": 2095.294117647059,
      "grad_norm": 18.007551193237305,
      "learning_rate": 2.904705882352941e-05,
      "loss": 1.3979,
      "step": 35620
    },
    {
      "epoch": 2095.8823529411766,
      "grad_norm": 19.199636459350586,
      "learning_rate": 2.904117647058824e-05,
      "loss": 1.3618,
      "step": 35630
    },
    {
      "epoch": 2096.470588235294,
      "grad_norm": 18.463973999023438,
      "learning_rate": 2.903529411764706e-05,
      "loss": 1.3526,
      "step": 35640
    },
    {
      "epoch": 2097.0588235294117,
      "grad_norm": 20.241703033447266,
      "learning_rate": 2.9029411764705882e-05,
      "loss": 1.3862,
      "step": 35650
    },
    {
      "epoch": 2097.6470588235293,
      "grad_norm": 16.709985733032227,
      "learning_rate": 2.9023529411764705e-05,
      "loss": 1.3422,
      "step": 35660
    },
    {
      "epoch": 2098.235294117647,
      "grad_norm": 17.29603385925293,
      "learning_rate": 2.901764705882353e-05,
      "loss": 1.2402,
      "step": 35670
    },
    {
      "epoch": 2098.823529411765,
      "grad_norm": 23.418821334838867,
      "learning_rate": 2.9011764705882354e-05,
      "loss": 1.4044,
      "step": 35680
    },
    {
      "epoch": 2099.4117647058824,
      "grad_norm": 20.50016975402832,
      "learning_rate": 2.9005882352941177e-05,
      "loss": 1.4461,
      "step": 35690
    },
    {
      "epoch": 2100.0,
      "grad_norm": 21.0565185546875,
      "learning_rate": 2.9e-05,
      "loss": 1.3479,
      "step": 35700
    },
    {
      "epoch": 2100.5882352941176,
      "grad_norm": 20.840646743774414,
      "learning_rate": 2.8994117647058827e-05,
      "loss": 1.3556,
      "step": 35710
    },
    {
      "epoch": 2101.176470588235,
      "grad_norm": 17.653972625732422,
      "learning_rate": 2.898823529411765e-05,
      "loss": 1.2795,
      "step": 35720
    },
    {
      "epoch": 2101.764705882353,
      "grad_norm": 16.153921127319336,
      "learning_rate": 2.8982352941176473e-05,
      "loss": 1.2125,
      "step": 35730
    },
    {
      "epoch": 2102.3529411764707,
      "grad_norm": 22.93238067626953,
      "learning_rate": 2.8976470588235292e-05,
      "loss": 1.5059,
      "step": 35740
    },
    {
      "epoch": 2102.9411764705883,
      "grad_norm": 19.982799530029297,
      "learning_rate": 2.8970588235294122e-05,
      "loss": 1.3492,
      "step": 35750
    },
    {
      "epoch": 2103.529411764706,
      "grad_norm": 22.72451400756836,
      "learning_rate": 2.8964705882352945e-05,
      "loss": 1.268,
      "step": 35760
    },
    {
      "epoch": 2104.1176470588234,
      "grad_norm": 17.752790451049805,
      "learning_rate": 2.8958823529411764e-05,
      "loss": 1.5081,
      "step": 35770
    },
    {
      "epoch": 2104.705882352941,
      "grad_norm": 19.25454330444336,
      "learning_rate": 2.8952941176470587e-05,
      "loss": 1.2441,
      "step": 35780
    },
    {
      "epoch": 2105.294117647059,
      "grad_norm": 17.07822608947754,
      "learning_rate": 2.894705882352941e-05,
      "loss": 1.3893,
      "step": 35790
    },
    {
      "epoch": 2105.8823529411766,
      "grad_norm": 15.906700134277344,
      "learning_rate": 2.8941176470588237e-05,
      "loss": 1.3285,
      "step": 35800
    },
    {
      "epoch": 2106.470588235294,
      "grad_norm": 17.616071701049805,
      "learning_rate": 2.893529411764706e-05,
      "loss": 1.4596,
      "step": 35810
    },
    {
      "epoch": 2107.0588235294117,
      "grad_norm": 24.587907791137695,
      "learning_rate": 2.8929411764705883e-05,
      "loss": 1.3846,
      "step": 35820
    },
    {
      "epoch": 2107.6470588235293,
      "grad_norm": 19.097383499145508,
      "learning_rate": 2.8923529411764706e-05,
      "loss": 1.3107,
      "step": 35830
    },
    {
      "epoch": 2108.235294117647,
      "grad_norm": 15.13016414642334,
      "learning_rate": 2.8917647058823532e-05,
      "loss": 1.2629,
      "step": 35840
    },
    {
      "epoch": 2108.823529411765,
      "grad_norm": 17.271116256713867,
      "learning_rate": 2.8911764705882355e-05,
      "loss": 1.2792,
      "step": 35850
    },
    {
      "epoch": 2109.4117647058824,
      "grad_norm": 19.590253829956055,
      "learning_rate": 2.8905882352941178e-05,
      "loss": 1.3668,
      "step": 35860
    },
    {
      "epoch": 2110.0,
      "grad_norm": 22.331466674804688,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 1.3209,
      "step": 35870
    },
    {
      "epoch": 2110.5882352941176,
      "grad_norm": 18.800966262817383,
      "learning_rate": 2.8894117647058828e-05,
      "loss": 1.3086,
      "step": 35880
    },
    {
      "epoch": 2111.176470588235,
      "grad_norm": 22.953121185302734,
      "learning_rate": 2.888823529411765e-05,
      "loss": 1.4186,
      "step": 35890
    },
    {
      "epoch": 2111.764705882353,
      "grad_norm": 18.51163101196289,
      "learning_rate": 2.888235294117647e-05,
      "loss": 1.3406,
      "step": 35900
    },
    {
      "epoch": 2112.3529411764707,
      "grad_norm": 15.32028865814209,
      "learning_rate": 2.8876470588235293e-05,
      "loss": 1.337,
      "step": 35910
    },
    {
      "epoch": 2112.9411764705883,
      "grad_norm": 17.472206115722656,
      "learning_rate": 2.8870588235294123e-05,
      "loss": 1.3724,
      "step": 35920
    },
    {
      "epoch": 2113.529411764706,
      "grad_norm": 17.223594665527344,
      "learning_rate": 2.8864705882352942e-05,
      "loss": 1.2097,
      "step": 35930
    },
    {
      "epoch": 2114.1176470588234,
      "grad_norm": 17.330333709716797,
      "learning_rate": 2.8858823529411765e-05,
      "loss": 1.2811,
      "step": 35940
    },
    {
      "epoch": 2114.705882352941,
      "grad_norm": 16.44722557067871,
      "learning_rate": 2.885294117647059e-05,
      "loss": 1.177,
      "step": 35950
    },
    {
      "epoch": 2115.294117647059,
      "grad_norm": 18.3892822265625,
      "learning_rate": 2.8847058823529415e-05,
      "loss": 1.3326,
      "step": 35960
    },
    {
      "epoch": 2115.8823529411766,
      "grad_norm": 21.9748477935791,
      "learning_rate": 2.8841176470588238e-05,
      "loss": 1.3099,
      "step": 35970
    },
    {
      "epoch": 2116.470588235294,
      "grad_norm": 19.248519897460938,
      "learning_rate": 2.883529411764706e-05,
      "loss": 1.4183,
      "step": 35980
    },
    {
      "epoch": 2117.0588235294117,
      "grad_norm": 19.06884002685547,
      "learning_rate": 2.8829411764705884e-05,
      "loss": 1.2691,
      "step": 35990
    },
    {
      "epoch": 2117.6470588235293,
      "grad_norm": 14.524165153503418,
      "learning_rate": 2.8823529411764703e-05,
      "loss": 1.2879,
      "step": 36000
    },
    {
      "epoch": 2118.235294117647,
      "grad_norm": 15.656730651855469,
      "learning_rate": 2.8817647058823533e-05,
      "loss": 1.3254,
      "step": 36010
    },
    {
      "epoch": 2118.823529411765,
      "grad_norm": 19.92099380493164,
      "learning_rate": 2.8811764705882356e-05,
      "loss": 1.3575,
      "step": 36020
    },
    {
      "epoch": 2119.4117647058824,
      "grad_norm": 11.779212951660156,
      "learning_rate": 2.8805882352941176e-05,
      "loss": 1.3961,
      "step": 36030
    },
    {
      "epoch": 2120.0,
      "grad_norm": 19.83892059326172,
      "learning_rate": 2.88e-05,
      "loss": 1.2345,
      "step": 36040
    },
    {
      "epoch": 2120.5882352941176,
      "grad_norm": 17.34585189819336,
      "learning_rate": 2.879411764705883e-05,
      "loss": 1.3291,
      "step": 36050
    },
    {
      "epoch": 2121.176470588235,
      "grad_norm": 19.95941162109375,
      "learning_rate": 2.8788235294117648e-05,
      "loss": 1.4103,
      "step": 36060
    },
    {
      "epoch": 2121.764705882353,
      "grad_norm": 16.429630279541016,
      "learning_rate": 2.878235294117647e-05,
      "loss": 1.3171,
      "step": 36070
    },
    {
      "epoch": 2122.3529411764707,
      "grad_norm": 17.549678802490234,
      "learning_rate": 2.8776470588235294e-05,
      "loss": 1.4022,
      "step": 36080
    },
    {
      "epoch": 2122.9411764705883,
      "grad_norm": 15.450345993041992,
      "learning_rate": 2.877058823529412e-05,
      "loss": 1.3306,
      "step": 36090
    },
    {
      "epoch": 2123.529411764706,
      "grad_norm": 20.791030883789062,
      "learning_rate": 2.8764705882352943e-05,
      "loss": 1.3789,
      "step": 36100
    },
    {
      "epoch": 2124.1176470588234,
      "grad_norm": 16.407835006713867,
      "learning_rate": 2.8758823529411766e-05,
      "loss": 1.4802,
      "step": 36110
    },
    {
      "epoch": 2124.705882352941,
      "grad_norm": 19.339542388916016,
      "learning_rate": 2.875294117647059e-05,
      "loss": 1.2266,
      "step": 36120
    },
    {
      "epoch": 2125.294117647059,
      "grad_norm": 19.402063369750977,
      "learning_rate": 2.8747058823529416e-05,
      "loss": 1.3296,
      "step": 36130
    },
    {
      "epoch": 2125.8823529411766,
      "grad_norm": 20.46519660949707,
      "learning_rate": 2.874117647058824e-05,
      "loss": 1.3241,
      "step": 36140
    },
    {
      "epoch": 2126.470588235294,
      "grad_norm": 15.799834251403809,
      "learning_rate": 2.8735294117647062e-05,
      "loss": 1.3155,
      "step": 36150
    },
    {
      "epoch": 2127.0588235294117,
      "grad_norm": 17.099899291992188,
      "learning_rate": 2.872941176470588e-05,
      "loss": 1.3625,
      "step": 36160
    },
    {
      "epoch": 2127.6470588235293,
      "grad_norm": 17.525686264038086,
      "learning_rate": 2.8723529411764704e-05,
      "loss": 1.3315,
      "step": 36170
    },
    {
      "epoch": 2128.235294117647,
      "grad_norm": 14.823684692382812,
      "learning_rate": 2.8717647058823534e-05,
      "loss": 1.2588,
      "step": 36180
    },
    {
      "epoch": 2128.823529411765,
      "grad_norm": 18.5057430267334,
      "learning_rate": 2.8711764705882354e-05,
      "loss": 1.2106,
      "step": 36190
    },
    {
      "epoch": 2129.4117647058824,
      "grad_norm": 18.831825256347656,
      "learning_rate": 2.8705882352941177e-05,
      "loss": 1.3456,
      "step": 36200
    },
    {
      "epoch": 2130.0,
      "grad_norm": 18.399044036865234,
      "learning_rate": 2.87e-05,
      "loss": 1.3098,
      "step": 36210
    },
    {
      "epoch": 2130.5882352941176,
      "grad_norm": 25.447751998901367,
      "learning_rate": 2.8694117647058826e-05,
      "loss": 1.4004,
      "step": 36220
    },
    {
      "epoch": 2131.176470588235,
      "grad_norm": 18.758153915405273,
      "learning_rate": 2.868823529411765e-05,
      "loss": 1.423,
      "step": 36230
    },
    {
      "epoch": 2131.764705882353,
      "grad_norm": 18.56061363220215,
      "learning_rate": 2.8682352941176472e-05,
      "loss": 1.3228,
      "step": 36240
    },
    {
      "epoch": 2132.3529411764707,
      "grad_norm": 15.578953742980957,
      "learning_rate": 2.8676470588235295e-05,
      "loss": 1.2028,
      "step": 36250
    },
    {
      "epoch": 2132.9411764705883,
      "grad_norm": 19.6751708984375,
      "learning_rate": 2.867058823529412e-05,
      "loss": 1.339,
      "step": 36260
    },
    {
      "epoch": 2133.529411764706,
      "grad_norm": 12.591325759887695,
      "learning_rate": 2.8664705882352944e-05,
      "loss": 1.3445,
      "step": 36270
    },
    {
      "epoch": 2134.1176470588234,
      "grad_norm": 12.57456111907959,
      "learning_rate": 2.8658823529411767e-05,
      "loss": 1.2515,
      "step": 36280
    },
    {
      "epoch": 2134.705882352941,
      "grad_norm": 16.718467712402344,
      "learning_rate": 2.8652941176470587e-05,
      "loss": 1.4467,
      "step": 36290
    },
    {
      "epoch": 2135.294117647059,
      "grad_norm": 16.167917251586914,
      "learning_rate": 2.8647058823529417e-05,
      "loss": 1.388,
      "step": 36300
    },
    {
      "epoch": 2135.8823529411766,
      "grad_norm": 17.27909278869629,
      "learning_rate": 2.8641176470588236e-05,
      "loss": 1.2561,
      "step": 36310
    },
    {
      "epoch": 2136.470588235294,
      "grad_norm": 20.984642028808594,
      "learning_rate": 2.863529411764706e-05,
      "loss": 1.3387,
      "step": 36320
    },
    {
      "epoch": 2137.0588235294117,
      "grad_norm": 15.16209888458252,
      "learning_rate": 2.8629411764705882e-05,
      "loss": 1.2065,
      "step": 36330
    },
    {
      "epoch": 2137.6470588235293,
      "grad_norm": 15.335216522216797,
      "learning_rate": 2.8623529411764705e-05,
      "loss": 1.1851,
      "step": 36340
    },
    {
      "epoch": 2138.235294117647,
      "grad_norm": 17.86932945251465,
      "learning_rate": 2.861764705882353e-05,
      "loss": 1.2931,
      "step": 36350
    },
    {
      "epoch": 2138.823529411765,
      "grad_norm": 18.70661163330078,
      "learning_rate": 2.8611764705882355e-05,
      "loss": 1.2936,
      "step": 36360
    },
    {
      "epoch": 2139.4117647058824,
      "grad_norm": 17.477983474731445,
      "learning_rate": 2.8605882352941178e-05,
      "loss": 1.1875,
      "step": 36370
    },
    {
      "epoch": 2140.0,
      "grad_norm": 17.911239624023438,
      "learning_rate": 2.86e-05,
      "loss": 1.4025,
      "step": 36380
    },
    {
      "epoch": 2140.5882352941176,
      "grad_norm": 19.264211654663086,
      "learning_rate": 2.8594117647058827e-05,
      "loss": 1.2985,
      "step": 36390
    },
    {
      "epoch": 2141.176470588235,
      "grad_norm": 18.960107803344727,
      "learning_rate": 2.858823529411765e-05,
      "loss": 1.3026,
      "step": 36400
    },
    {
      "epoch": 2141.764705882353,
      "grad_norm": 18.431827545166016,
      "learning_rate": 2.858235294117647e-05,
      "loss": 1.3902,
      "step": 36410
    },
    {
      "epoch": 2142.3529411764707,
      "grad_norm": 17.224971771240234,
      "learning_rate": 2.8576470588235293e-05,
      "loss": 1.3098,
      "step": 36420
    },
    {
      "epoch": 2142.9411764705883,
      "grad_norm": 19.84902572631836,
      "learning_rate": 2.8570588235294122e-05,
      "loss": 1.459,
      "step": 36430
    },
    {
      "epoch": 2143.529411764706,
      "grad_norm": 18.513643264770508,
      "learning_rate": 2.8564705882352942e-05,
      "loss": 1.3777,
      "step": 36440
    },
    {
      "epoch": 2144.1176470588234,
      "grad_norm": 19.80959701538086,
      "learning_rate": 2.8558823529411765e-05,
      "loss": 1.3212,
      "step": 36450
    },
    {
      "epoch": 2144.705882352941,
      "grad_norm": 14.864350318908691,
      "learning_rate": 2.8552941176470588e-05,
      "loss": 1.1854,
      "step": 36460
    },
    {
      "epoch": 2145.294117647059,
      "grad_norm": 16.31095314025879,
      "learning_rate": 2.8547058823529414e-05,
      "loss": 1.2685,
      "step": 36470
    },
    {
      "epoch": 2145.8823529411766,
      "grad_norm": 14.763413429260254,
      "learning_rate": 2.8541176470588237e-05,
      "loss": 1.2073,
      "step": 36480
    },
    {
      "epoch": 2146.470588235294,
      "grad_norm": 22.4279842376709,
      "learning_rate": 2.853529411764706e-05,
      "loss": 1.2736,
      "step": 36490
    },
    {
      "epoch": 2147.0588235294117,
      "grad_norm": 17.572187423706055,
      "learning_rate": 2.8529411764705883e-05,
      "loss": 1.4125,
      "step": 36500
    },
    {
      "epoch": 2147.6470588235293,
      "grad_norm": 16.559362411499023,
      "learning_rate": 2.8523529411764703e-05,
      "loss": 1.296,
      "step": 36510
    },
    {
      "epoch": 2148.235294117647,
      "grad_norm": 17.142358779907227,
      "learning_rate": 2.8517647058823533e-05,
      "loss": 1.3658,
      "step": 36520
    },
    {
      "epoch": 2148.823529411765,
      "grad_norm": 17.9344482421875,
      "learning_rate": 2.8511764705882356e-05,
      "loss": 1.286,
      "step": 36530
    },
    {
      "epoch": 2149.4117647058824,
      "grad_norm": 19.67767906188965,
      "learning_rate": 2.8505882352941175e-05,
      "loss": 1.3641,
      "step": 36540
    },
    {
      "epoch": 2150.0,
      "grad_norm": 19.0467472076416,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.364,
      "step": 36550
    },
    {
      "epoch": 2150.5882352941176,
      "grad_norm": 19.19136619567871,
      "learning_rate": 2.8494117647058828e-05,
      "loss": 1.3056,
      "step": 36560
    },
    {
      "epoch": 2151.176470588235,
      "grad_norm": 14.11813735961914,
      "learning_rate": 2.8488235294117648e-05,
      "loss": 1.2663,
      "step": 36570
    },
    {
      "epoch": 2151.764705882353,
      "grad_norm": 17.934955596923828,
      "learning_rate": 2.848235294117647e-05,
      "loss": 1.2648,
      "step": 36580
    },
    {
      "epoch": 2152.3529411764707,
      "grad_norm": 20.258853912353516,
      "learning_rate": 2.8476470588235294e-05,
      "loss": 1.2649,
      "step": 36590
    },
    {
      "epoch": 2152.9411764705883,
      "grad_norm": 21.98937225341797,
      "learning_rate": 2.847058823529412e-05,
      "loss": 1.4087,
      "step": 36600
    },
    {
      "epoch": 2153.529411764706,
      "grad_norm": 20.87953758239746,
      "learning_rate": 2.8464705882352943e-05,
      "loss": 1.3145,
      "step": 36610
    },
    {
      "epoch": 2154.1176470588234,
      "grad_norm": 23.39862632751465,
      "learning_rate": 2.8458823529411766e-05,
      "loss": 1.3994,
      "step": 36620
    },
    {
      "epoch": 2154.705882352941,
      "grad_norm": 15.766834259033203,
      "learning_rate": 2.845294117647059e-05,
      "loss": 1.2565,
      "step": 36630
    },
    {
      "epoch": 2155.294117647059,
      "grad_norm": 17.363418579101562,
      "learning_rate": 2.8447058823529415e-05,
      "loss": 1.398,
      "step": 36640
    },
    {
      "epoch": 2155.8823529411766,
      "grad_norm": 17.672014236450195,
      "learning_rate": 2.8441176470588238e-05,
      "loss": 1.3654,
      "step": 36650
    },
    {
      "epoch": 2156.470588235294,
      "grad_norm": 16.53522300720215,
      "learning_rate": 2.843529411764706e-05,
      "loss": 1.3007,
      "step": 36660
    },
    {
      "epoch": 2157.0588235294117,
      "grad_norm": 15.663076400756836,
      "learning_rate": 2.842941176470588e-05,
      "loss": 1.3,
      "step": 36670
    },
    {
      "epoch": 2157.6470588235293,
      "grad_norm": 16.7670955657959,
      "learning_rate": 2.842352941176471e-05,
      "loss": 1.3745,
      "step": 36680
    },
    {
      "epoch": 2158.235294117647,
      "grad_norm": 18.476476669311523,
      "learning_rate": 2.8417647058823534e-05,
      "loss": 1.3588,
      "step": 36690
    },
    {
      "epoch": 2158.823529411765,
      "grad_norm": 14.59622859954834,
      "learning_rate": 2.8411764705882353e-05,
      "loss": 1.2542,
      "step": 36700
    },
    {
      "epoch": 2159.4117647058824,
      "grad_norm": 15.536483764648438,
      "learning_rate": 2.8405882352941176e-05,
      "loss": 1.2989,
      "step": 36710
    },
    {
      "epoch": 2160.0,
      "grad_norm": 17.146329879760742,
      "learning_rate": 2.84e-05,
      "loss": 1.2259,
      "step": 36720
    },
    {
      "epoch": 2160.5882352941176,
      "grad_norm": 23.63907814025879,
      "learning_rate": 2.8394117647058826e-05,
      "loss": 1.4596,
      "step": 36730
    },
    {
      "epoch": 2161.176470588235,
      "grad_norm": 17.799232482910156,
      "learning_rate": 2.838823529411765e-05,
      "loss": 1.3236,
      "step": 36740
    },
    {
      "epoch": 2161.764705882353,
      "grad_norm": 20.55663299560547,
      "learning_rate": 2.838235294117647e-05,
      "loss": 1.3341,
      "step": 36750
    },
    {
      "epoch": 2162.3529411764707,
      "grad_norm": 16.88360595703125,
      "learning_rate": 2.8376470588235294e-05,
      "loss": 1.3852,
      "step": 36760
    },
    {
      "epoch": 2162.9411764705883,
      "grad_norm": 20.1021728515625,
      "learning_rate": 2.837058823529412e-05,
      "loss": 1.3259,
      "step": 36770
    },
    {
      "epoch": 2163.529411764706,
      "grad_norm": 15.748218536376953,
      "learning_rate": 2.8364705882352944e-05,
      "loss": 1.2475,
      "step": 36780
    },
    {
      "epoch": 2164.1176470588234,
      "grad_norm": 19.249954223632812,
      "learning_rate": 2.8358823529411767e-05,
      "loss": 1.2821,
      "step": 36790
    },
    {
      "epoch": 2164.705882352941,
      "grad_norm": 19.698305130004883,
      "learning_rate": 2.8352941176470586e-05,
      "loss": 1.3308,
      "step": 36800
    },
    {
      "epoch": 2165.294117647059,
      "grad_norm": 18.705270767211914,
      "learning_rate": 2.8347058823529416e-05,
      "loss": 1.2128,
      "step": 36810
    },
    {
      "epoch": 2165.8823529411766,
      "grad_norm": 14.586613655090332,
      "learning_rate": 2.834117647058824e-05,
      "loss": 1.1767,
      "step": 36820
    },
    {
      "epoch": 2166.470588235294,
      "grad_norm": 18.774194717407227,
      "learning_rate": 2.833529411764706e-05,
      "loss": 1.2167,
      "step": 36830
    },
    {
      "epoch": 2167.0588235294117,
      "grad_norm": 24.52147674560547,
      "learning_rate": 2.8329411764705882e-05,
      "loss": 1.3335,
      "step": 36840
    },
    {
      "epoch": 2167.6470588235293,
      "grad_norm": 19.78658103942871,
      "learning_rate": 2.832352941176471e-05,
      "loss": 1.3773,
      "step": 36850
    },
    {
      "epoch": 2168.235294117647,
      "grad_norm": 21.891700744628906,
      "learning_rate": 2.831764705882353e-05,
      "loss": 1.2673,
      "step": 36860
    },
    {
      "epoch": 2168.823529411765,
      "grad_norm": 18.74874496459961,
      "learning_rate": 2.8311764705882354e-05,
      "loss": 1.2751,
      "step": 36870
    },
    {
      "epoch": 2169.4117647058824,
      "grad_norm": 16.521520614624023,
      "learning_rate": 2.8305882352941177e-05,
      "loss": 1.4382,
      "step": 36880
    },
    {
      "epoch": 2170.0,
      "grad_norm": 17.618587493896484,
      "learning_rate": 2.83e-05,
      "loss": 1.2613,
      "step": 36890
    },
    {
      "epoch": 2170.5882352941176,
      "grad_norm": 20.069530487060547,
      "learning_rate": 2.8294117647058826e-05,
      "loss": 1.3563,
      "step": 36900
    },
    {
      "epoch": 2171.176470588235,
      "grad_norm": 17.631458282470703,
      "learning_rate": 2.828823529411765e-05,
      "loss": 1.2367,
      "step": 36910
    },
    {
      "epoch": 2171.764705882353,
      "grad_norm": 15.636363983154297,
      "learning_rate": 2.8282352941176472e-05,
      "loss": 1.2649,
      "step": 36920
    },
    {
      "epoch": 2172.3529411764707,
      "grad_norm": 17.224336624145508,
      "learning_rate": 2.8276470588235292e-05,
      "loss": 1.3888,
      "step": 36930
    },
    {
      "epoch": 2172.9411764705883,
      "grad_norm": 17.947364807128906,
      "learning_rate": 2.8270588235294122e-05,
      "loss": 1.3536,
      "step": 36940
    },
    {
      "epoch": 2173.529411764706,
      "grad_norm": 16.666786193847656,
      "learning_rate": 2.8264705882352945e-05,
      "loss": 1.3305,
      "step": 36950
    },
    {
      "epoch": 2174.1176470588234,
      "grad_norm": 18.535381317138672,
      "learning_rate": 2.8258823529411764e-05,
      "loss": 1.3207,
      "step": 36960
    },
    {
      "epoch": 2174.705882352941,
      "grad_norm": 16.835098266601562,
      "learning_rate": 2.8252941176470587e-05,
      "loss": 1.286,
      "step": 36970
    },
    {
      "epoch": 2175.294117647059,
      "grad_norm": 21.988449096679688,
      "learning_rate": 2.8247058823529414e-05,
      "loss": 1.3326,
      "step": 36980
    },
    {
      "epoch": 2175.8823529411766,
      "grad_norm": 21.77079200744629,
      "learning_rate": 2.8241176470588237e-05,
      "loss": 1.2469,
      "step": 36990
    },
    {
      "epoch": 2176.470588235294,
      "grad_norm": 20.845909118652344,
      "learning_rate": 2.823529411764706e-05,
      "loss": 1.1747,
      "step": 37000
    },
    {
      "epoch": 2177.0588235294117,
      "grad_norm": 17.66761016845703,
      "learning_rate": 2.8229411764705883e-05,
      "loss": 1.2838,
      "step": 37010
    },
    {
      "epoch": 2177.6470588235293,
      "grad_norm": 14.788056373596191,
      "learning_rate": 2.822352941176471e-05,
      "loss": 1.2988,
      "step": 37020
    },
    {
      "epoch": 2178.235294117647,
      "grad_norm": 16.406938552856445,
      "learning_rate": 2.8217647058823532e-05,
      "loss": 1.3297,
      "step": 37030
    },
    {
      "epoch": 2178.823529411765,
      "grad_norm": 15.609625816345215,
      "learning_rate": 2.8211764705882355e-05,
      "loss": 1.2567,
      "step": 37040
    },
    {
      "epoch": 2179.4117647058824,
      "grad_norm": 21.84956169128418,
      "learning_rate": 2.8205882352941178e-05,
      "loss": 1.3869,
      "step": 37050
    },
    {
      "epoch": 2180.0,
      "grad_norm": 21.5155029296875,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 1.1761,
      "step": 37060
    },
    {
      "epoch": 2180.5882352941176,
      "grad_norm": 18.79374122619629,
      "learning_rate": 2.8194117647058827e-05,
      "loss": 1.1688,
      "step": 37070
    },
    {
      "epoch": 2181.176470588235,
      "grad_norm": 17.914382934570312,
      "learning_rate": 2.8188235294117647e-05,
      "loss": 1.2263,
      "step": 37080
    },
    {
      "epoch": 2181.764705882353,
      "grad_norm": 13.493875503540039,
      "learning_rate": 2.818235294117647e-05,
      "loss": 1.3265,
      "step": 37090
    },
    {
      "epoch": 2182.3529411764707,
      "grad_norm": 16.458274841308594,
      "learning_rate": 2.8176470588235293e-05,
      "loss": 1.1998,
      "step": 37100
    },
    {
      "epoch": 2182.9411764705883,
      "grad_norm": 20.81494903564453,
      "learning_rate": 2.817058823529412e-05,
      "loss": 1.2996,
      "step": 37110
    },
    {
      "epoch": 2183.529411764706,
      "grad_norm": 19.959383010864258,
      "learning_rate": 2.8164705882352942e-05,
      "loss": 1.4367,
      "step": 37120
    },
    {
      "epoch": 2184.1176470588234,
      "grad_norm": 16.756086349487305,
      "learning_rate": 2.8158823529411765e-05,
      "loss": 1.3018,
      "step": 37130
    },
    {
      "epoch": 2184.705882352941,
      "grad_norm": 17.57549476623535,
      "learning_rate": 2.815294117647059e-05,
      "loss": 1.2806,
      "step": 37140
    },
    {
      "epoch": 2185.294117647059,
      "grad_norm": 13.604219436645508,
      "learning_rate": 2.8147058823529415e-05,
      "loss": 1.1957,
      "step": 37150
    },
    {
      "epoch": 2185.8823529411766,
      "grad_norm": 22.28972816467285,
      "learning_rate": 2.8141176470588238e-05,
      "loss": 1.3415,
      "step": 37160
    },
    {
      "epoch": 2186.470588235294,
      "grad_norm": 15.82841682434082,
      "learning_rate": 2.813529411764706e-05,
      "loss": 1.2015,
      "step": 37170
    },
    {
      "epoch": 2187.0588235294117,
      "grad_norm": 15.425568580627441,
      "learning_rate": 2.812941176470588e-05,
      "loss": 1.342,
      "step": 37180
    },
    {
      "epoch": 2187.6470588235293,
      "grad_norm": 19.822477340698242,
      "learning_rate": 2.812352941176471e-05,
      "loss": 1.2942,
      "step": 37190
    },
    {
      "epoch": 2188.235294117647,
      "grad_norm": 18.704776763916016,
      "learning_rate": 2.8117647058823533e-05,
      "loss": 1.2688,
      "step": 37200
    },
    {
      "epoch": 2188.823529411765,
      "grad_norm": 18.224180221557617,
      "learning_rate": 2.8111764705882353e-05,
      "loss": 1.2081,
      "step": 37210
    },
    {
      "epoch": 2189.4117647058824,
      "grad_norm": 16.685972213745117,
      "learning_rate": 2.8105882352941176e-05,
      "loss": 1.2159,
      "step": 37220
    },
    {
      "epoch": 2190.0,
      "grad_norm": 24.88582420349121,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 1.2581,
      "step": 37230
    },
    {
      "epoch": 2190.5882352941176,
      "grad_norm": 19.323665618896484,
      "learning_rate": 2.8094117647058825e-05,
      "loss": 1.2713,
      "step": 37240
    },
    {
      "epoch": 2191.176470588235,
      "grad_norm": 14.734621047973633,
      "learning_rate": 2.8088235294117648e-05,
      "loss": 1.2901,
      "step": 37250
    },
    {
      "epoch": 2191.764705882353,
      "grad_norm": 16.218170166015625,
      "learning_rate": 2.808235294117647e-05,
      "loss": 1.3528,
      "step": 37260
    },
    {
      "epoch": 2192.3529411764707,
      "grad_norm": 14.574509620666504,
      "learning_rate": 2.8076470588235294e-05,
      "loss": 1.2227,
      "step": 37270
    },
    {
      "epoch": 2192.9411764705883,
      "grad_norm": 21.85654067993164,
      "learning_rate": 2.807058823529412e-05,
      "loss": 1.3886,
      "step": 37280
    },
    {
      "epoch": 2193.529411764706,
      "grad_norm": 19.03534698486328,
      "learning_rate": 2.8064705882352943e-05,
      "loss": 1.1502,
      "step": 37290
    },
    {
      "epoch": 2194.1176470588234,
      "grad_norm": 13.115032196044922,
      "learning_rate": 2.8058823529411766e-05,
      "loss": 1.2881,
      "step": 37300
    },
    {
      "epoch": 2194.705882352941,
      "grad_norm": 18.640077590942383,
      "learning_rate": 2.8052941176470586e-05,
      "loss": 1.2133,
      "step": 37310
    },
    {
      "epoch": 2195.294117647059,
      "grad_norm": 15.375770568847656,
      "learning_rate": 2.8047058823529416e-05,
      "loss": 1.3564,
      "step": 37320
    },
    {
      "epoch": 2195.8823529411766,
      "grad_norm": 14.44899845123291,
      "learning_rate": 2.804117647058824e-05,
      "loss": 1.2307,
      "step": 37330
    },
    {
      "epoch": 2196.470588235294,
      "grad_norm": 22.59504508972168,
      "learning_rate": 2.8035294117647058e-05,
      "loss": 1.3926,
      "step": 37340
    },
    {
      "epoch": 2197.0588235294117,
      "grad_norm": 19.06051254272461,
      "learning_rate": 2.802941176470588e-05,
      "loss": 1.3665,
      "step": 37350
    },
    {
      "epoch": 2197.6470588235293,
      "grad_norm": 16.929956436157227,
      "learning_rate": 2.802352941176471e-05,
      "loss": 1.3275,
      "step": 37360
    },
    {
      "epoch": 2198.235294117647,
      "grad_norm": 18.080350875854492,
      "learning_rate": 2.801764705882353e-05,
      "loss": 1.2343,
      "step": 37370
    },
    {
      "epoch": 2198.823529411765,
      "grad_norm": 14.253710746765137,
      "learning_rate": 2.8011764705882354e-05,
      "loss": 1.2808,
      "step": 37380
    },
    {
      "epoch": 2199.4117647058824,
      "grad_norm": 17.55201530456543,
      "learning_rate": 2.8005882352941177e-05,
      "loss": 1.2545,
      "step": 37390
    },
    {
      "epoch": 2200.0,
      "grad_norm": 19.840349197387695,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.2278,
      "step": 37400
    },
    {
      "epoch": 2200.5882352941176,
      "grad_norm": 18.095172882080078,
      "learning_rate": 2.7994117647058826e-05,
      "loss": 1.2688,
      "step": 37410
    },
    {
      "epoch": 2201.176470588235,
      "grad_norm": 18.30397605895996,
      "learning_rate": 2.798823529411765e-05,
      "loss": 1.2903,
      "step": 37420
    },
    {
      "epoch": 2201.764705882353,
      "grad_norm": 20.782312393188477,
      "learning_rate": 2.7982352941176472e-05,
      "loss": 1.3436,
      "step": 37430
    },
    {
      "epoch": 2202.3529411764707,
      "grad_norm": 16.65201187133789,
      "learning_rate": 2.797647058823529e-05,
      "loss": 1.2795,
      "step": 37440
    },
    {
      "epoch": 2202.9411764705883,
      "grad_norm": 19.929990768432617,
      "learning_rate": 2.797058823529412e-05,
      "loss": 1.3372,
      "step": 37450
    },
    {
      "epoch": 2203.529411764706,
      "grad_norm": 24.655513763427734,
      "learning_rate": 2.7964705882352944e-05,
      "loss": 1.2341,
      "step": 37460
    },
    {
      "epoch": 2204.1176470588234,
      "grad_norm": 16.023889541625977,
      "learning_rate": 2.7958823529411764e-05,
      "loss": 1.228,
      "step": 37470
    },
    {
      "epoch": 2204.705882352941,
      "grad_norm": 15.16372299194336,
      "learning_rate": 2.7952941176470587e-05,
      "loss": 1.201,
      "step": 37480
    },
    {
      "epoch": 2205.294117647059,
      "grad_norm": 20.46542739868164,
      "learning_rate": 2.7947058823529417e-05,
      "loss": 1.2871,
      "step": 37490
    },
    {
      "epoch": 2205.8823529411766,
      "grad_norm": 19.2999267578125,
      "learning_rate": 2.7941176470588236e-05,
      "loss": 1.4137,
      "step": 37500
    },
    {
      "epoch": 2206.470588235294,
      "grad_norm": 14.448250770568848,
      "learning_rate": 2.793529411764706e-05,
      "loss": 1.2314,
      "step": 37510
    },
    {
      "epoch": 2207.0588235294117,
      "grad_norm": 18.688323974609375,
      "learning_rate": 2.7929411764705882e-05,
      "loss": 1.3943,
      "step": 37520
    },
    {
      "epoch": 2207.6470588235293,
      "grad_norm": 20.184295654296875,
      "learning_rate": 2.792352941176471e-05,
      "loss": 1.2539,
      "step": 37530
    },
    {
      "epoch": 2208.235294117647,
      "grad_norm": 22.437850952148438,
      "learning_rate": 2.791764705882353e-05,
      "loss": 1.2522,
      "step": 37540
    },
    {
      "epoch": 2208.823529411765,
      "grad_norm": 20.372591018676758,
      "learning_rate": 2.7911764705882355e-05,
      "loss": 1.2791,
      "step": 37550
    },
    {
      "epoch": 2209.4117647058824,
      "grad_norm": 20.393102645874023,
      "learning_rate": 2.7905882352941178e-05,
      "loss": 1.243,
      "step": 37560
    },
    {
      "epoch": 2210.0,
      "grad_norm": 22.66552734375,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 1.3284,
      "step": 37570
    },
    {
      "epoch": 2210.5882352941176,
      "grad_norm": 19.002666473388672,
      "learning_rate": 2.7894117647058827e-05,
      "loss": 1.3145,
      "step": 37580
    },
    {
      "epoch": 2211.176470588235,
      "grad_norm": 23.09986686706543,
      "learning_rate": 2.788823529411765e-05,
      "loss": 1.4369,
      "step": 37590
    },
    {
      "epoch": 2211.764705882353,
      "grad_norm": 18.598011016845703,
      "learning_rate": 2.788235294117647e-05,
      "loss": 1.2855,
      "step": 37600
    },
    {
      "epoch": 2212.3529411764707,
      "grad_norm": 17.041019439697266,
      "learning_rate": 2.7876470588235292e-05,
      "loss": 1.2892,
      "step": 37610
    },
    {
      "epoch": 2212.9411764705883,
      "grad_norm": 16.549060821533203,
      "learning_rate": 2.787058823529412e-05,
      "loss": 1.2612,
      "step": 37620
    },
    {
      "epoch": 2213.529411764706,
      "grad_norm": 20.03230094909668,
      "learning_rate": 2.7864705882352942e-05,
      "loss": 1.2249,
      "step": 37630
    },
    {
      "epoch": 2214.1176470588234,
      "grad_norm": 15.457401275634766,
      "learning_rate": 2.7858823529411765e-05,
      "loss": 1.3333,
      "step": 37640
    },
    {
      "epoch": 2214.705882352941,
      "grad_norm": 15.97722339630127,
      "learning_rate": 2.7852941176470588e-05,
      "loss": 1.324,
      "step": 37650
    },
    {
      "epoch": 2215.294117647059,
      "grad_norm": 20.05521011352539,
      "learning_rate": 2.7847058823529414e-05,
      "loss": 1.3633,
      "step": 37660
    },
    {
      "epoch": 2215.8823529411766,
      "grad_norm": 15.487687110900879,
      "learning_rate": 2.7841176470588237e-05,
      "loss": 1.2036,
      "step": 37670
    },
    {
      "epoch": 2216.470588235294,
      "grad_norm": 17.332563400268555,
      "learning_rate": 2.783529411764706e-05,
      "loss": 1.2956,
      "step": 37680
    },
    {
      "epoch": 2217.0588235294117,
      "grad_norm": 20.71070098876953,
      "learning_rate": 2.7829411764705883e-05,
      "loss": 1.2981,
      "step": 37690
    },
    {
      "epoch": 2217.6470588235293,
      "grad_norm": 23.93222427368164,
      "learning_rate": 2.782352941176471e-05,
      "loss": 1.3429,
      "step": 37700
    },
    {
      "epoch": 2218.235294117647,
      "grad_norm": 19.65669059753418,
      "learning_rate": 2.7817647058823533e-05,
      "loss": 1.3898,
      "step": 37710
    },
    {
      "epoch": 2218.823529411765,
      "grad_norm": 18.59520721435547,
      "learning_rate": 2.7811764705882356e-05,
      "loss": 1.3529,
      "step": 37720
    },
    {
      "epoch": 2219.4117647058824,
      "grad_norm": 21.222261428833008,
      "learning_rate": 2.7805882352941175e-05,
      "loss": 1.276,
      "step": 37730
    },
    {
      "epoch": 2220.0,
      "grad_norm": 30.518543243408203,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 1.3493,
      "step": 37740
    },
    {
      "epoch": 2220.5882352941176,
      "grad_norm": 18.6943302154541,
      "learning_rate": 2.7794117647058824e-05,
      "loss": 1.2792,
      "step": 37750
    },
    {
      "epoch": 2221.176470588235,
      "grad_norm": 18.675792694091797,
      "learning_rate": 2.7788235294117647e-05,
      "loss": 1.3078,
      "step": 37760
    },
    {
      "epoch": 2221.764705882353,
      "grad_norm": 24.999473571777344,
      "learning_rate": 2.778235294117647e-05,
      "loss": 1.2123,
      "step": 37770
    },
    {
      "epoch": 2222.3529411764707,
      "grad_norm": 18.389690399169922,
      "learning_rate": 2.7776470588235293e-05,
      "loss": 1.2862,
      "step": 37780
    },
    {
      "epoch": 2222.9411764705883,
      "grad_norm": 20.760013580322266,
      "learning_rate": 2.777058823529412e-05,
      "loss": 1.237,
      "step": 37790
    },
    {
      "epoch": 2223.529411764706,
      "grad_norm": 21.266721725463867,
      "learning_rate": 2.7764705882352943e-05,
      "loss": 1.3609,
      "step": 37800
    },
    {
      "epoch": 2224.1176470588234,
      "grad_norm": 18.784555435180664,
      "learning_rate": 2.7758823529411766e-05,
      "loss": 1.3403,
      "step": 37810
    },
    {
      "epoch": 2224.705882352941,
      "grad_norm": 19.197757720947266,
      "learning_rate": 2.775294117647059e-05,
      "loss": 1.2968,
      "step": 37820
    },
    {
      "epoch": 2225.294117647059,
      "grad_norm": 26.48535919189453,
      "learning_rate": 2.7747058823529415e-05,
      "loss": 1.1535,
      "step": 37830
    },
    {
      "epoch": 2225.8823529411766,
      "grad_norm": 20.761817932128906,
      "learning_rate": 2.7741176470588238e-05,
      "loss": 1.246,
      "step": 37840
    },
    {
      "epoch": 2226.470588235294,
      "grad_norm": 20.099946975708008,
      "learning_rate": 2.7735294117647058e-05,
      "loss": 1.2032,
      "step": 37850
    },
    {
      "epoch": 2227.0588235294117,
      "grad_norm": 20.846006393432617,
      "learning_rate": 2.772941176470588e-05,
      "loss": 1.2379,
      "step": 37860
    },
    {
      "epoch": 2227.6470588235293,
      "grad_norm": 25.498788833618164,
      "learning_rate": 2.772352941176471e-05,
      "loss": 1.2594,
      "step": 37870
    },
    {
      "epoch": 2228.235294117647,
      "grad_norm": 19.431062698364258,
      "learning_rate": 2.771764705882353e-05,
      "loss": 1.3554,
      "step": 37880
    },
    {
      "epoch": 2228.823529411765,
      "grad_norm": 20.31077003479004,
      "learning_rate": 2.7711764705882353e-05,
      "loss": 1.2195,
      "step": 37890
    },
    {
      "epoch": 2229.4117647058824,
      "grad_norm": 16.686410903930664,
      "learning_rate": 2.7705882352941176e-05,
      "loss": 1.2972,
      "step": 37900
    },
    {
      "epoch": 2230.0,
      "grad_norm": 25.526052474975586,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 1.3305,
      "step": 37910
    },
    {
      "epoch": 2230.5882352941176,
      "grad_norm": 14.209883689880371,
      "learning_rate": 2.7694117647058825e-05,
      "loss": 1.2243,
      "step": 37920
    },
    {
      "epoch": 2231.176470588235,
      "grad_norm": 19.82415771484375,
      "learning_rate": 2.768823529411765e-05,
      "loss": 1.3637,
      "step": 37930
    },
    {
      "epoch": 2231.764705882353,
      "grad_norm": 17.936744689941406,
      "learning_rate": 2.768235294117647e-05,
      "loss": 1.2386,
      "step": 37940
    },
    {
      "epoch": 2232.3529411764707,
      "grad_norm": 17.35837173461914,
      "learning_rate": 2.7676470588235298e-05,
      "loss": 1.2659,
      "step": 37950
    },
    {
      "epoch": 2232.9411764705883,
      "grad_norm": 18.716245651245117,
      "learning_rate": 2.767058823529412e-05,
      "loss": 1.1695,
      "step": 37960
    },
    {
      "epoch": 2233.529411764706,
      "grad_norm": 25.522531509399414,
      "learning_rate": 2.7664705882352944e-05,
      "loss": 1.0944,
      "step": 37970
    },
    {
      "epoch": 2234.1176470588234,
      "grad_norm": 20.439180374145508,
      "learning_rate": 2.7658823529411763e-05,
      "loss": 1.2874,
      "step": 37980
    },
    {
      "epoch": 2234.705882352941,
      "grad_norm": 14.865951538085938,
      "learning_rate": 2.7652941176470586e-05,
      "loss": 1.3326,
      "step": 37990
    },
    {
      "epoch": 2235.294117647059,
      "grad_norm": 17.410301208496094,
      "learning_rate": 2.7647058823529416e-05,
      "loss": 1.2687,
      "step": 38000
    },
    {
      "epoch": 2235.8823529411766,
      "grad_norm": 17.77263069152832,
      "learning_rate": 2.7641176470588236e-05,
      "loss": 1.2386,
      "step": 38010
    },
    {
      "epoch": 2236.470588235294,
      "grad_norm": 18.681259155273438,
      "learning_rate": 2.763529411764706e-05,
      "loss": 1.2893,
      "step": 38020
    },
    {
      "epoch": 2237.0588235294117,
      "grad_norm": 17.739215850830078,
      "learning_rate": 2.7629411764705882e-05,
      "loss": 1.31,
      "step": 38030
    },
    {
      "epoch": 2237.6470588235293,
      "grad_norm": 16.5164794921875,
      "learning_rate": 2.7623529411764708e-05,
      "loss": 1.1731,
      "step": 38040
    },
    {
      "epoch": 2238.235294117647,
      "grad_norm": 18.549530029296875,
      "learning_rate": 2.761764705882353e-05,
      "loss": 1.1961,
      "step": 38050
    },
    {
      "epoch": 2238.823529411765,
      "grad_norm": 19.940568923950195,
      "learning_rate": 2.7611764705882354e-05,
      "loss": 1.267,
      "step": 38060
    },
    {
      "epoch": 2239.4117647058824,
      "grad_norm": 19.695226669311523,
      "learning_rate": 2.7605882352941177e-05,
      "loss": 1.1899,
      "step": 38070
    },
    {
      "epoch": 2240.0,
      "grad_norm": 21.480728149414062,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 1.2326,
      "step": 38080
    },
    {
      "epoch": 2240.5882352941176,
      "grad_norm": 18.088272094726562,
      "learning_rate": 2.7594117647058826e-05,
      "loss": 1.3318,
      "step": 38090
    },
    {
      "epoch": 2241.176470588235,
      "grad_norm": 18.603734970092773,
      "learning_rate": 2.758823529411765e-05,
      "loss": 1.2948,
      "step": 38100
    },
    {
      "epoch": 2241.764705882353,
      "grad_norm": 21.734291076660156,
      "learning_rate": 2.758235294117647e-05,
      "loss": 1.3044,
      "step": 38110
    },
    {
      "epoch": 2242.3529411764707,
      "grad_norm": 17.602628707885742,
      "learning_rate": 2.75764705882353e-05,
      "loss": 1.1922,
      "step": 38120
    },
    {
      "epoch": 2242.9411764705883,
      "grad_norm": 23.980289459228516,
      "learning_rate": 2.7570588235294122e-05,
      "loss": 1.2164,
      "step": 38130
    },
    {
      "epoch": 2243.529411764706,
      "grad_norm": 23.99234962463379,
      "learning_rate": 2.756470588235294e-05,
      "loss": 1.2536,
      "step": 38140
    },
    {
      "epoch": 2244.1176470588234,
      "grad_norm": 16.334592819213867,
      "learning_rate": 2.7558823529411764e-05,
      "loss": 1.2437,
      "step": 38150
    },
    {
      "epoch": 2244.705882352941,
      "grad_norm": 19.218917846679688,
      "learning_rate": 2.7552941176470587e-05,
      "loss": 1.3625,
      "step": 38160
    },
    {
      "epoch": 2245.294117647059,
      "grad_norm": 17.438861846923828,
      "learning_rate": 2.7547058823529414e-05,
      "loss": 1.1781,
      "step": 38170
    },
    {
      "epoch": 2245.8823529411766,
      "grad_norm": 22.929983139038086,
      "learning_rate": 2.7541176470588237e-05,
      "loss": 1.2285,
      "step": 38180
    },
    {
      "epoch": 2246.470588235294,
      "grad_norm": 16.3708553314209,
      "learning_rate": 2.753529411764706e-05,
      "loss": 1.1776,
      "step": 38190
    },
    {
      "epoch": 2247.0588235294117,
      "grad_norm": 21.66005516052246,
      "learning_rate": 2.7529411764705883e-05,
      "loss": 1.3363,
      "step": 38200
    },
    {
      "epoch": 2247.6470588235293,
      "grad_norm": 17.166114807128906,
      "learning_rate": 2.752352941176471e-05,
      "loss": 1.2203,
      "step": 38210
    },
    {
      "epoch": 2248.235294117647,
      "grad_norm": 20.809833526611328,
      "learning_rate": 2.7517647058823532e-05,
      "loss": 1.2146,
      "step": 38220
    },
    {
      "epoch": 2248.823529411765,
      "grad_norm": 19.146020889282227,
      "learning_rate": 2.7511764705882355e-05,
      "loss": 1.272,
      "step": 38230
    },
    {
      "epoch": 2249.4117647058824,
      "grad_norm": 13.551795959472656,
      "learning_rate": 2.7505882352941175e-05,
      "loss": 1.1891,
      "step": 38240
    },
    {
      "epoch": 2250.0,
      "grad_norm": 18.712501525878906,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.221,
      "step": 38250
    },
    {
      "epoch": 2250.5882352941176,
      "grad_norm": 25.221906661987305,
      "learning_rate": 2.7494117647058827e-05,
      "loss": 1.3338,
      "step": 38260
    },
    {
      "epoch": 2251.176470588235,
      "grad_norm": 17.40143585205078,
      "learning_rate": 2.7488235294117647e-05,
      "loss": 1.2482,
      "step": 38270
    },
    {
      "epoch": 2251.764705882353,
      "grad_norm": 15.013260841369629,
      "learning_rate": 2.748235294117647e-05,
      "loss": 1.3195,
      "step": 38280
    },
    {
      "epoch": 2252.3529411764707,
      "grad_norm": 19.166170120239258,
      "learning_rate": 2.7476470588235296e-05,
      "loss": 1.3266,
      "step": 38290
    },
    {
      "epoch": 2252.9411764705883,
      "grad_norm": 16.819461822509766,
      "learning_rate": 2.747058823529412e-05,
      "loss": 1.3427,
      "step": 38300
    },
    {
      "epoch": 2253.529411764706,
      "grad_norm": 18.413978576660156,
      "learning_rate": 2.7464705882352942e-05,
      "loss": 1.2775,
      "step": 38310
    },
    {
      "epoch": 2254.1176470588234,
      "grad_norm": 19.350008010864258,
      "learning_rate": 2.7458823529411765e-05,
      "loss": 1.3313,
      "step": 38320
    },
    {
      "epoch": 2254.705882352941,
      "grad_norm": 14.163881301879883,
      "learning_rate": 2.7452941176470588e-05,
      "loss": 1.2417,
      "step": 38330
    },
    {
      "epoch": 2255.294117647059,
      "grad_norm": 22.42051124572754,
      "learning_rate": 2.7447058823529415e-05,
      "loss": 1.197,
      "step": 38340
    },
    {
      "epoch": 2255.8823529411766,
      "grad_norm": 16.727741241455078,
      "learning_rate": 2.7441176470588238e-05,
      "loss": 1.2369,
      "step": 38350
    },
    {
      "epoch": 2256.470588235294,
      "grad_norm": 15.403112411499023,
      "learning_rate": 2.743529411764706e-05,
      "loss": 1.3062,
      "step": 38360
    },
    {
      "epoch": 2257.0588235294117,
      "grad_norm": 17.860252380371094,
      "learning_rate": 2.742941176470588e-05,
      "loss": 1.2218,
      "step": 38370
    },
    {
      "epoch": 2257.6470588235293,
      "grad_norm": 18.03023338317871,
      "learning_rate": 2.742352941176471e-05,
      "loss": 1.2282,
      "step": 38380
    },
    {
      "epoch": 2258.235294117647,
      "grad_norm": 13.404698371887207,
      "learning_rate": 2.741764705882353e-05,
      "loss": 1.1902,
      "step": 38390
    },
    {
      "epoch": 2258.823529411765,
      "grad_norm": 21.9920597076416,
      "learning_rate": 2.7411764705882353e-05,
      "loss": 1.4348,
      "step": 38400
    },
    {
      "epoch": 2259.4117647058824,
      "grad_norm": 16.39796257019043,
      "learning_rate": 2.7405882352941176e-05,
      "loss": 1.2572,
      "step": 38410
    },
    {
      "epoch": 2260.0,
      "grad_norm": 21.523115158081055,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 1.1896,
      "step": 38420
    },
    {
      "epoch": 2260.5882352941176,
      "grad_norm": 20.065889358520508,
      "learning_rate": 2.7394117647058825e-05,
      "loss": 1.2632,
      "step": 38430
    },
    {
      "epoch": 2261.176470588235,
      "grad_norm": 20.857107162475586,
      "learning_rate": 2.7388235294117648e-05,
      "loss": 1.2432,
      "step": 38440
    },
    {
      "epoch": 2261.764705882353,
      "grad_norm": 18.434505462646484,
      "learning_rate": 2.738235294117647e-05,
      "loss": 1.2308,
      "step": 38450
    },
    {
      "epoch": 2262.3529411764707,
      "grad_norm": 15.672018051147461,
      "learning_rate": 2.7376470588235297e-05,
      "loss": 1.2569,
      "step": 38460
    },
    {
      "epoch": 2262.9411764705883,
      "grad_norm": 16.464996337890625,
      "learning_rate": 2.737058823529412e-05,
      "loss": 1.2678,
      "step": 38470
    },
    {
      "epoch": 2263.529411764706,
      "grad_norm": 20.11349105834961,
      "learning_rate": 2.7364705882352943e-05,
      "loss": 1.188,
      "step": 38480
    },
    {
      "epoch": 2264.1176470588234,
      "grad_norm": 24.114479064941406,
      "learning_rate": 2.7358823529411763e-05,
      "loss": 1.2401,
      "step": 38490
    },
    {
      "epoch": 2264.705882352941,
      "grad_norm": 17.794721603393555,
      "learning_rate": 2.7352941176470593e-05,
      "loss": 1.128,
      "step": 38500
    },
    {
      "epoch": 2265.294117647059,
      "grad_norm": 20.023582458496094,
      "learning_rate": 2.7347058823529416e-05,
      "loss": 1.2852,
      "step": 38510
    },
    {
      "epoch": 2265.8823529411766,
      "grad_norm": 19.571727752685547,
      "learning_rate": 2.7341176470588235e-05,
      "loss": 1.2837,
      "step": 38520
    },
    {
      "epoch": 2266.470588235294,
      "grad_norm": 15.479126930236816,
      "learning_rate": 2.7335294117647058e-05,
      "loss": 1.2424,
      "step": 38530
    },
    {
      "epoch": 2267.0588235294117,
      "grad_norm": 15.636307716369629,
      "learning_rate": 2.732941176470588e-05,
      "loss": 1.1952,
      "step": 38540
    },
    {
      "epoch": 2267.6470588235293,
      "grad_norm": 17.62818717956543,
      "learning_rate": 2.7323529411764708e-05,
      "loss": 1.2672,
      "step": 38550
    },
    {
      "epoch": 2268.235294117647,
      "grad_norm": 15.236886024475098,
      "learning_rate": 2.731764705882353e-05,
      "loss": 1.2362,
      "step": 38560
    },
    {
      "epoch": 2268.823529411765,
      "grad_norm": 19.42169189453125,
      "learning_rate": 2.7311764705882354e-05,
      "loss": 1.2513,
      "step": 38570
    },
    {
      "epoch": 2269.4117647058824,
      "grad_norm": 21.00442123413086,
      "learning_rate": 2.7305882352941177e-05,
      "loss": 1.2568,
      "step": 38580
    },
    {
      "epoch": 2270.0,
      "grad_norm": 19.820716857910156,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 1.3057,
      "step": 38590
    },
    {
      "epoch": 2270.5882352941176,
      "grad_norm": 21.62114906311035,
      "learning_rate": 2.7294117647058826e-05,
      "loss": 1.2504,
      "step": 38600
    },
    {
      "epoch": 2271.176470588235,
      "grad_norm": 21.581315994262695,
      "learning_rate": 2.728823529411765e-05,
      "loss": 1.3115,
      "step": 38610
    },
    {
      "epoch": 2271.764705882353,
      "grad_norm": 20.49178123474121,
      "learning_rate": 2.728235294117647e-05,
      "loss": 1.1467,
      "step": 38620
    },
    {
      "epoch": 2272.3529411764707,
      "grad_norm": 15.441740989685059,
      "learning_rate": 2.7276470588235298e-05,
      "loss": 1.2672,
      "step": 38630
    },
    {
      "epoch": 2272.9411764705883,
      "grad_norm": 15.79422664642334,
      "learning_rate": 2.727058823529412e-05,
      "loss": 1.22,
      "step": 38640
    },
    {
      "epoch": 2273.529411764706,
      "grad_norm": 18.665203094482422,
      "learning_rate": 2.726470588235294e-05,
      "loss": 1.1656,
      "step": 38650
    },
    {
      "epoch": 2274.1176470588234,
      "grad_norm": 19.20676040649414,
      "learning_rate": 2.7258823529411764e-05,
      "loss": 1.3399,
      "step": 38660
    },
    {
      "epoch": 2274.705882352941,
      "grad_norm": 19.1767520904541,
      "learning_rate": 2.7252941176470594e-05,
      "loss": 1.2307,
      "step": 38670
    },
    {
      "epoch": 2275.294117647059,
      "grad_norm": 17.071577072143555,
      "learning_rate": 2.7247058823529413e-05,
      "loss": 1.1954,
      "step": 38680
    },
    {
      "epoch": 2275.8823529411766,
      "grad_norm": 12.176839828491211,
      "learning_rate": 2.7241176470588236e-05,
      "loss": 1.2135,
      "step": 38690
    },
    {
      "epoch": 2276.470588235294,
      "grad_norm": 16.958446502685547,
      "learning_rate": 2.723529411764706e-05,
      "loss": 1.2212,
      "step": 38700
    },
    {
      "epoch": 2277.0588235294117,
      "grad_norm": 14.119548797607422,
      "learning_rate": 2.7229411764705882e-05,
      "loss": 1.3391,
      "step": 38710
    },
    {
      "epoch": 2277.6470588235293,
      "grad_norm": 16.645986557006836,
      "learning_rate": 2.722352941176471e-05,
      "loss": 1.3439,
      "step": 38720
    },
    {
      "epoch": 2278.235294117647,
      "grad_norm": 19.572221755981445,
      "learning_rate": 2.721764705882353e-05,
      "loss": 1.1464,
      "step": 38730
    },
    {
      "epoch": 2278.823529411765,
      "grad_norm": 17.072431564331055,
      "learning_rate": 2.7211764705882354e-05,
      "loss": 1.2868,
      "step": 38740
    },
    {
      "epoch": 2279.4117647058824,
      "grad_norm": 20.36493492126465,
      "learning_rate": 2.7205882352941174e-05,
      "loss": 1.2199,
      "step": 38750
    },
    {
      "epoch": 2280.0,
      "grad_norm": 22.15398597717285,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 1.223,
      "step": 38760
    },
    {
      "epoch": 2280.5882352941176,
      "grad_norm": 16.062868118286133,
      "learning_rate": 2.7194117647058827e-05,
      "loss": 1.268,
      "step": 38770
    },
    {
      "epoch": 2281.176470588235,
      "grad_norm": 19.469907760620117,
      "learning_rate": 2.7188235294117646e-05,
      "loss": 1.245,
      "step": 38780
    },
    {
      "epoch": 2281.764705882353,
      "grad_norm": 21.719398498535156,
      "learning_rate": 2.718235294117647e-05,
      "loss": 1.2573,
      "step": 38790
    },
    {
      "epoch": 2282.3529411764707,
      "grad_norm": 21.962158203125,
      "learning_rate": 2.71764705882353e-05,
      "loss": 1.393,
      "step": 38800
    },
    {
      "epoch": 2282.9411764705883,
      "grad_norm": 21.03978157043457,
      "learning_rate": 2.717058823529412e-05,
      "loss": 1.199,
      "step": 38810
    },
    {
      "epoch": 2283.529411764706,
      "grad_norm": 16.693260192871094,
      "learning_rate": 2.7164705882352942e-05,
      "loss": 1.2708,
      "step": 38820
    },
    {
      "epoch": 2284.1176470588234,
      "grad_norm": 24.980302810668945,
      "learning_rate": 2.7158823529411765e-05,
      "loss": 1.3169,
      "step": 38830
    },
    {
      "epoch": 2284.705882352941,
      "grad_norm": 17.53861427307129,
      "learning_rate": 2.715294117647059e-05,
      "loss": 1.2759,
      "step": 38840
    },
    {
      "epoch": 2285.294117647059,
      "grad_norm": 19.31300926208496,
      "learning_rate": 2.7147058823529414e-05,
      "loss": 1.3088,
      "step": 38850
    },
    {
      "epoch": 2285.8823529411766,
      "grad_norm": 16.572723388671875,
      "learning_rate": 2.7141176470588237e-05,
      "loss": 1.1174,
      "step": 38860
    },
    {
      "epoch": 2286.470588235294,
      "grad_norm": 17.29706382751465,
      "learning_rate": 2.713529411764706e-05,
      "loss": 1.272,
      "step": 38870
    },
    {
      "epoch": 2287.0588235294117,
      "grad_norm": 21.213459014892578,
      "learning_rate": 2.712941176470588e-05,
      "loss": 1.1612,
      "step": 38880
    },
    {
      "epoch": 2287.6470588235293,
      "grad_norm": 20.49714469909668,
      "learning_rate": 2.712352941176471e-05,
      "loss": 1.2714,
      "step": 38890
    },
    {
      "epoch": 2288.235294117647,
      "grad_norm": 20.295015335083008,
      "learning_rate": 2.7117647058823532e-05,
      "loss": 1.1457,
      "step": 38900
    },
    {
      "epoch": 2288.823529411765,
      "grad_norm": 25.64687728881836,
      "learning_rate": 2.7111764705882352e-05,
      "loss": 1.2154,
      "step": 38910
    },
    {
      "epoch": 2289.4117647058824,
      "grad_norm": 20.04859733581543,
      "learning_rate": 2.7105882352941175e-05,
      "loss": 1.2685,
      "step": 38920
    },
    {
      "epoch": 2290.0,
      "grad_norm": 26.4427490234375,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 1.2171,
      "step": 38930
    },
    {
      "epoch": 2290.5882352941176,
      "grad_norm": 16.079782485961914,
      "learning_rate": 2.7094117647058824e-05,
      "loss": 1.2855,
      "step": 38940
    },
    {
      "epoch": 2291.176470588235,
      "grad_norm": 16.700714111328125,
      "learning_rate": 2.7088235294117647e-05,
      "loss": 1.2333,
      "step": 38950
    },
    {
      "epoch": 2291.764705882353,
      "grad_norm": 19.74346351623535,
      "learning_rate": 2.708235294117647e-05,
      "loss": 1.2869,
      "step": 38960
    },
    {
      "epoch": 2292.3529411764707,
      "grad_norm": 16.068195343017578,
      "learning_rate": 2.7076470588235297e-05,
      "loss": 1.2054,
      "step": 38970
    },
    {
      "epoch": 2292.9411764705883,
      "grad_norm": 20.01767349243164,
      "learning_rate": 2.707058823529412e-05,
      "loss": 1.2731,
      "step": 38980
    },
    {
      "epoch": 2293.529411764706,
      "grad_norm": 26.095224380493164,
      "learning_rate": 2.7064705882352943e-05,
      "loss": 1.2632,
      "step": 38990
    },
    {
      "epoch": 2294.1176470588234,
      "grad_norm": 18.213979721069336,
      "learning_rate": 2.7058823529411766e-05,
      "loss": 1.323,
      "step": 39000
    },
    {
      "epoch": 2294.705882352941,
      "grad_norm": 23.3748722076416,
      "learning_rate": 2.7052941176470592e-05,
      "loss": 1.3118,
      "step": 39010
    },
    {
      "epoch": 2295.294117647059,
      "grad_norm": 17.95969581604004,
      "learning_rate": 2.7047058823529415e-05,
      "loss": 1.2825,
      "step": 39020
    },
    {
      "epoch": 2295.8823529411766,
      "grad_norm": 21.131423950195312,
      "learning_rate": 2.7041176470588238e-05,
      "loss": 1.1603,
      "step": 39030
    },
    {
      "epoch": 2296.470588235294,
      "grad_norm": 16.73587417602539,
      "learning_rate": 2.7035294117647058e-05,
      "loss": 1.2977,
      "step": 39040
    },
    {
      "epoch": 2297.0588235294117,
      "grad_norm": 19.040863037109375,
      "learning_rate": 2.702941176470588e-05,
      "loss": 1.2382,
      "step": 39050
    },
    {
      "epoch": 2297.6470588235293,
      "grad_norm": 22.664669036865234,
      "learning_rate": 2.7023529411764707e-05,
      "loss": 1.2772,
      "step": 39060
    },
    {
      "epoch": 2298.235294117647,
      "grad_norm": 18.95960807800293,
      "learning_rate": 2.701764705882353e-05,
      "loss": 1.2552,
      "step": 39070
    },
    {
      "epoch": 2298.823529411765,
      "grad_norm": 17.369234085083008,
      "learning_rate": 2.7011764705882353e-05,
      "loss": 1.2805,
      "step": 39080
    },
    {
      "epoch": 2299.4117647058824,
      "grad_norm": 18.429468154907227,
      "learning_rate": 2.7005882352941176e-05,
      "loss": 1.1746,
      "step": 39090
    },
    {
      "epoch": 2300.0,
      "grad_norm": 17.649438858032227,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.2319,
      "step": 39100
    },
    {
      "epoch": 2300.5882352941176,
      "grad_norm": 17.65772819519043,
      "learning_rate": 2.6994117647058825e-05,
      "loss": 1.3035,
      "step": 39110
    },
    {
      "epoch": 2301.176470588235,
      "grad_norm": 19.98236846923828,
      "learning_rate": 2.698823529411765e-05,
      "loss": 1.27,
      "step": 39120
    },
    {
      "epoch": 2301.764705882353,
      "grad_norm": 22.55002212524414,
      "learning_rate": 2.698235294117647e-05,
      "loss": 1.3263,
      "step": 39130
    },
    {
      "epoch": 2302.3529411764707,
      "grad_norm": 17.484041213989258,
      "learning_rate": 2.6976470588235298e-05,
      "loss": 1.1147,
      "step": 39140
    },
    {
      "epoch": 2302.9411764705883,
      "grad_norm": 19.75769805908203,
      "learning_rate": 2.697058823529412e-05,
      "loss": 1.2423,
      "step": 39150
    },
    {
      "epoch": 2303.529411764706,
      "grad_norm": 24.43017578125,
      "learning_rate": 2.696470588235294e-05,
      "loss": 1.2447,
      "step": 39160
    },
    {
      "epoch": 2304.1176470588234,
      "grad_norm": 17.220077514648438,
      "learning_rate": 2.6958823529411763e-05,
      "loss": 1.2123,
      "step": 39170
    },
    {
      "epoch": 2304.705882352941,
      "grad_norm": 24.736560821533203,
      "learning_rate": 2.6952941176470593e-05,
      "loss": 1.3051,
      "step": 39180
    },
    {
      "epoch": 2305.294117647059,
      "grad_norm": 20.201005935668945,
      "learning_rate": 2.6947058823529413e-05,
      "loss": 1.2445,
      "step": 39190
    },
    {
      "epoch": 2305.8823529411766,
      "grad_norm": 24.095388412475586,
      "learning_rate": 2.6941176470588236e-05,
      "loss": 1.3136,
      "step": 39200
    },
    {
      "epoch": 2306.470588235294,
      "grad_norm": 14.506634712219238,
      "learning_rate": 2.693529411764706e-05,
      "loss": 1.0353,
      "step": 39210
    },
    {
      "epoch": 2307.0588235294117,
      "grad_norm": 16.573925018310547,
      "learning_rate": 2.6929411764705885e-05,
      "loss": 1.2391,
      "step": 39220
    },
    {
      "epoch": 2307.6470588235293,
      "grad_norm": 18.312030792236328,
      "learning_rate": 2.6923529411764708e-05,
      "loss": 1.1476,
      "step": 39230
    },
    {
      "epoch": 2308.235294117647,
      "grad_norm": 17.54793357849121,
      "learning_rate": 2.691764705882353e-05,
      "loss": 1.2658,
      "step": 39240
    },
    {
      "epoch": 2308.823529411765,
      "grad_norm": 15.19461727142334,
      "learning_rate": 2.6911764705882354e-05,
      "loss": 1.1462,
      "step": 39250
    },
    {
      "epoch": 2309.4117647058824,
      "grad_norm": 16.303667068481445,
      "learning_rate": 2.6905882352941174e-05,
      "loss": 1.123,
      "step": 39260
    },
    {
      "epoch": 2310.0,
      "grad_norm": 24.91423225402832,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 1.2781,
      "step": 39270
    },
    {
      "epoch": 2310.5882352941176,
      "grad_norm": 23.8040771484375,
      "learning_rate": 2.6894117647058826e-05,
      "loss": 1.1771,
      "step": 39280
    },
    {
      "epoch": 2311.176470588235,
      "grad_norm": 17.486873626708984,
      "learning_rate": 2.6888235294117646e-05,
      "loss": 1.2635,
      "step": 39290
    },
    {
      "epoch": 2311.764705882353,
      "grad_norm": 14.222537994384766,
      "learning_rate": 2.688235294117647e-05,
      "loss": 1.2806,
      "step": 39300
    },
    {
      "epoch": 2312.3529411764707,
      "grad_norm": 21.82088851928711,
      "learning_rate": 2.68764705882353e-05,
      "loss": 1.1666,
      "step": 39310
    },
    {
      "epoch": 2312.9411764705883,
      "grad_norm": 21.378299713134766,
      "learning_rate": 2.6870588235294118e-05,
      "loss": 1.3043,
      "step": 39320
    },
    {
      "epoch": 2313.529411764706,
      "grad_norm": 17.627288818359375,
      "learning_rate": 2.686470588235294e-05,
      "loss": 1.2687,
      "step": 39330
    },
    {
      "epoch": 2314.1176470588234,
      "grad_norm": 19.678688049316406,
      "learning_rate": 2.6858823529411764e-05,
      "loss": 1.2634,
      "step": 39340
    },
    {
      "epoch": 2314.705882352941,
      "grad_norm": 20.345829010009766,
      "learning_rate": 2.685294117647059e-05,
      "loss": 1.3867,
      "step": 39350
    },
    {
      "epoch": 2315.294117647059,
      "grad_norm": 18.355070114135742,
      "learning_rate": 2.6847058823529414e-05,
      "loss": 1.2592,
      "step": 39360
    },
    {
      "epoch": 2315.8823529411766,
      "grad_norm": 19.301864624023438,
      "learning_rate": 2.6841176470588237e-05,
      "loss": 1.2475,
      "step": 39370
    },
    {
      "epoch": 2316.470588235294,
      "grad_norm": 18.85032081604004,
      "learning_rate": 2.683529411764706e-05,
      "loss": 1.2488,
      "step": 39380
    },
    {
      "epoch": 2317.0588235294117,
      "grad_norm": 17.26416015625,
      "learning_rate": 2.6829411764705886e-05,
      "loss": 1.1305,
      "step": 39390
    },
    {
      "epoch": 2317.6470588235293,
      "grad_norm": 23.02715301513672,
      "learning_rate": 2.682352941176471e-05,
      "loss": 1.2885,
      "step": 39400
    },
    {
      "epoch": 2318.235294117647,
      "grad_norm": 17.626312255859375,
      "learning_rate": 2.6817647058823532e-05,
      "loss": 1.2467,
      "step": 39410
    },
    {
      "epoch": 2318.823529411765,
      "grad_norm": 16.20330810546875,
      "learning_rate": 2.681176470588235e-05,
      "loss": 1.235,
      "step": 39420
    },
    {
      "epoch": 2319.4117647058824,
      "grad_norm": 20.550561904907227,
      "learning_rate": 2.6805882352941175e-05,
      "loss": 1.1772,
      "step": 39430
    },
    {
      "epoch": 2320.0,
      "grad_norm": 20.543354034423828,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 1.2019,
      "step": 39440
    },
    {
      "epoch": 2320.5882352941176,
      "grad_norm": 19.801847457885742,
      "learning_rate": 2.6794117647058824e-05,
      "loss": 1.3255,
      "step": 39450
    },
    {
      "epoch": 2321.176470588235,
      "grad_norm": 19.195741653442383,
      "learning_rate": 2.6788235294117647e-05,
      "loss": 1.2984,
      "step": 39460
    },
    {
      "epoch": 2321.764705882353,
      "grad_norm": 20.023088455200195,
      "learning_rate": 2.678235294117647e-05,
      "loss": 1.1858,
      "step": 39470
    },
    {
      "epoch": 2322.3529411764707,
      "grad_norm": 14.655903816223145,
      "learning_rate": 2.6776470588235296e-05,
      "loss": 1.2071,
      "step": 39480
    },
    {
      "epoch": 2322.9411764705883,
      "grad_norm": 17.475814819335938,
      "learning_rate": 2.677058823529412e-05,
      "loss": 1.2537,
      "step": 39490
    },
    {
      "epoch": 2323.529411764706,
      "grad_norm": 16.82248306274414,
      "learning_rate": 2.6764705882352942e-05,
      "loss": 1.2438,
      "step": 39500
    },
    {
      "epoch": 2324.1176470588234,
      "grad_norm": 23.48898696899414,
      "learning_rate": 2.6758823529411765e-05,
      "loss": 1.2609,
      "step": 39510
    },
    {
      "epoch": 2324.705882352941,
      "grad_norm": 19.249778747558594,
      "learning_rate": 2.675294117647059e-05,
      "loss": 1.2466,
      "step": 39520
    },
    {
      "epoch": 2325.294117647059,
      "grad_norm": 15.509305000305176,
      "learning_rate": 2.6747058823529415e-05,
      "loss": 1.0739,
      "step": 39530
    },
    {
      "epoch": 2325.8823529411766,
      "grad_norm": 19.264371871948242,
      "learning_rate": 2.6741176470588238e-05,
      "loss": 1.2328,
      "step": 39540
    },
    {
      "epoch": 2326.470588235294,
      "grad_norm": 21.94344711303711,
      "learning_rate": 2.6735294117647057e-05,
      "loss": 1.2821,
      "step": 39550
    },
    {
      "epoch": 2327.0588235294117,
      "grad_norm": 16.411914825439453,
      "learning_rate": 2.6729411764705887e-05,
      "loss": 1.1219,
      "step": 39560
    },
    {
      "epoch": 2327.6470588235293,
      "grad_norm": 22.24510383605957,
      "learning_rate": 2.672352941176471e-05,
      "loss": 1.2089,
      "step": 39570
    },
    {
      "epoch": 2328.235294117647,
      "grad_norm": 17.505455017089844,
      "learning_rate": 2.671764705882353e-05,
      "loss": 1.1432,
      "step": 39580
    },
    {
      "epoch": 2328.823529411765,
      "grad_norm": 13.954968452453613,
      "learning_rate": 2.6711764705882352e-05,
      "loss": 1.1684,
      "step": 39590
    },
    {
      "epoch": 2329.4117647058824,
      "grad_norm": 14.78909683227539,
      "learning_rate": 2.6705882352941175e-05,
      "loss": 1.1393,
      "step": 39600
    },
    {
      "epoch": 2330.0,
      "grad_norm": 24.385929107666016,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 1.272,
      "step": 39610
    },
    {
      "epoch": 2330.5882352941176,
      "grad_norm": 17.894521713256836,
      "learning_rate": 2.6694117647058825e-05,
      "loss": 1.3457,
      "step": 39620
    },
    {
      "epoch": 2331.176470588235,
      "grad_norm": 23.524423599243164,
      "learning_rate": 2.6688235294117648e-05,
      "loss": 1.3572,
      "step": 39630
    },
    {
      "epoch": 2331.764705882353,
      "grad_norm": 21.83895492553711,
      "learning_rate": 2.668235294117647e-05,
      "loss": 1.2432,
      "step": 39640
    },
    {
      "epoch": 2332.3529411764707,
      "grad_norm": 19.747236251831055,
      "learning_rate": 2.6676470588235297e-05,
      "loss": 1.2388,
      "step": 39650
    },
    {
      "epoch": 2332.9411764705883,
      "grad_norm": 21.203594207763672,
      "learning_rate": 2.667058823529412e-05,
      "loss": 1.1676,
      "step": 39660
    },
    {
      "epoch": 2333.529411764706,
      "grad_norm": 18.374845504760742,
      "learning_rate": 2.6664705882352943e-05,
      "loss": 1.1075,
      "step": 39670
    },
    {
      "epoch": 2334.1176470588234,
      "grad_norm": 16.940601348876953,
      "learning_rate": 2.6658823529411763e-05,
      "loss": 1.2701,
      "step": 39680
    },
    {
      "epoch": 2334.705882352941,
      "grad_norm": 17.47008514404297,
      "learning_rate": 2.6652941176470593e-05,
      "loss": 1.3148,
      "step": 39690
    },
    {
      "epoch": 2335.294117647059,
      "grad_norm": 15.673319816589355,
      "learning_rate": 2.6647058823529416e-05,
      "loss": 1.2116,
      "step": 39700
    },
    {
      "epoch": 2335.8823529411766,
      "grad_norm": 17.69371223449707,
      "learning_rate": 2.6641176470588235e-05,
      "loss": 1.205,
      "step": 39710
    },
    {
      "epoch": 2336.470588235294,
      "grad_norm": 19.607776641845703,
      "learning_rate": 2.6635294117647058e-05,
      "loss": 1.2323,
      "step": 39720
    },
    {
      "epoch": 2337.0588235294117,
      "grad_norm": 17.011335372924805,
      "learning_rate": 2.6629411764705884e-05,
      "loss": 1.1964,
      "step": 39730
    },
    {
      "epoch": 2337.6470588235293,
      "grad_norm": 18.922271728515625,
      "learning_rate": 2.6623529411764707e-05,
      "loss": 1.1967,
      "step": 39740
    },
    {
      "epoch": 2338.235294117647,
      "grad_norm": 19.734350204467773,
      "learning_rate": 2.661764705882353e-05,
      "loss": 1.1219,
      "step": 39750
    },
    {
      "epoch": 2338.823529411765,
      "grad_norm": 19.226516723632812,
      "learning_rate": 2.6611764705882353e-05,
      "loss": 1.2506,
      "step": 39760
    },
    {
      "epoch": 2339.4117647058824,
      "grad_norm": 16.60367774963379,
      "learning_rate": 2.660588235294118e-05,
      "loss": 1.1318,
      "step": 39770
    },
    {
      "epoch": 2340.0,
      "grad_norm": 29.64655303955078,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 1.2474,
      "step": 39780
    },
    {
      "epoch": 2340.5882352941176,
      "grad_norm": 15.86782455444336,
      "learning_rate": 2.6594117647058826e-05,
      "loss": 1.0895,
      "step": 39790
    },
    {
      "epoch": 2341.176470588235,
      "grad_norm": 22.455486297607422,
      "learning_rate": 2.658823529411765e-05,
      "loss": 1.2424,
      "step": 39800
    },
    {
      "epoch": 2341.764705882353,
      "grad_norm": 19.406660079956055,
      "learning_rate": 2.658235294117647e-05,
      "loss": 1.2982,
      "step": 39810
    },
    {
      "epoch": 2342.3529411764707,
      "grad_norm": 21.86713981628418,
      "learning_rate": 2.6576470588235298e-05,
      "loss": 1.2747,
      "step": 39820
    },
    {
      "epoch": 2342.9411764705883,
      "grad_norm": 25.741230010986328,
      "learning_rate": 2.6570588235294118e-05,
      "loss": 1.3286,
      "step": 39830
    },
    {
      "epoch": 2343.529411764706,
      "grad_norm": 17.141820907592773,
      "learning_rate": 2.656470588235294e-05,
      "loss": 1.1357,
      "step": 39840
    },
    {
      "epoch": 2344.1176470588234,
      "grad_norm": 15.211873054504395,
      "learning_rate": 2.6558823529411764e-05,
      "loss": 1.3154,
      "step": 39850
    },
    {
      "epoch": 2344.705882352941,
      "grad_norm": 21.26757049560547,
      "learning_rate": 2.655294117647059e-05,
      "loss": 1.2194,
      "step": 39860
    },
    {
      "epoch": 2345.294117647059,
      "grad_norm": 21.089426040649414,
      "learning_rate": 2.6547058823529413e-05,
      "loss": 1.2662,
      "step": 39870
    },
    {
      "epoch": 2345.8823529411766,
      "grad_norm": 22.784624099731445,
      "learning_rate": 2.6541176470588236e-05,
      "loss": 1.1493,
      "step": 39880
    },
    {
      "epoch": 2346.470588235294,
      "grad_norm": 20.35478401184082,
      "learning_rate": 2.653529411764706e-05,
      "loss": 1.2663,
      "step": 39890
    },
    {
      "epoch": 2347.0588235294117,
      "grad_norm": 23.50602912902832,
      "learning_rate": 2.6529411764705885e-05,
      "loss": 1.1174,
      "step": 39900
    },
    {
      "epoch": 2347.6470588235293,
      "grad_norm": 19.575475692749023,
      "learning_rate": 2.652352941176471e-05,
      "loss": 1.2438,
      "step": 39910
    },
    {
      "epoch": 2348.235294117647,
      "grad_norm": 21.58111572265625,
      "learning_rate": 2.651764705882353e-05,
      "loss": 1.1415,
      "step": 39920
    },
    {
      "epoch": 2348.823529411765,
      "grad_norm": 13.799461364746094,
      "learning_rate": 2.651176470588235e-05,
      "loss": 1.1746,
      "step": 39930
    },
    {
      "epoch": 2349.4117647058824,
      "grad_norm": 15.65870475769043,
      "learning_rate": 2.650588235294118e-05,
      "loss": 1.2504,
      "step": 39940
    },
    {
      "epoch": 2350.0,
      "grad_norm": 19.725475311279297,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.3564,
      "step": 39950
    },
    {
      "epoch": 2350.5882352941176,
      "grad_norm": 23.18973159790039,
      "learning_rate": 2.6494117647058823e-05,
      "loss": 1.2622,
      "step": 39960
    },
    {
      "epoch": 2351.176470588235,
      "grad_norm": 16.49591064453125,
      "learning_rate": 2.6488235294117646e-05,
      "loss": 1.2338,
      "step": 39970
    },
    {
      "epoch": 2351.764705882353,
      "grad_norm": 18.785795211791992,
      "learning_rate": 2.648235294117647e-05,
      "loss": 1.2671,
      "step": 39980
    },
    {
      "epoch": 2352.3529411764707,
      "grad_norm": 23.803543090820312,
      "learning_rate": 2.6476470588235296e-05,
      "loss": 1.2727,
      "step": 39990
    },
    {
      "epoch": 2352.9411764705883,
      "grad_norm": 17.1154842376709,
      "learning_rate": 2.647058823529412e-05,
      "loss": 1.1781,
      "step": 40000
    },
    {
      "epoch": 2353.529411764706,
      "grad_norm": 19.01802635192871,
      "learning_rate": 2.6464705882352942e-05,
      "loss": 1.1247,
      "step": 40010
    },
    {
      "epoch": 2354.1176470588234,
      "grad_norm": 16.677942276000977,
      "learning_rate": 2.6458823529411765e-05,
      "loss": 1.2396,
      "step": 40020
    },
    {
      "epoch": 2354.705882352941,
      "grad_norm": 17.108869552612305,
      "learning_rate": 2.645294117647059e-05,
      "loss": 1.2306,
      "step": 40030
    },
    {
      "epoch": 2355.294117647059,
      "grad_norm": 19.181137084960938,
      "learning_rate": 2.6447058823529414e-05,
      "loss": 1.2663,
      "step": 40040
    },
    {
      "epoch": 2355.8823529411766,
      "grad_norm": 17.857433319091797,
      "learning_rate": 2.6441176470588237e-05,
      "loss": 1.213,
      "step": 40050
    },
    {
      "epoch": 2356.470588235294,
      "grad_norm": 16.339553833007812,
      "learning_rate": 2.6435294117647057e-05,
      "loss": 1.1928,
      "step": 40060
    },
    {
      "epoch": 2357.0588235294117,
      "grad_norm": 19.25440788269043,
      "learning_rate": 2.6429411764705886e-05,
      "loss": 1.1926,
      "step": 40070
    },
    {
      "epoch": 2357.6470588235293,
      "grad_norm": 18.72115707397461,
      "learning_rate": 2.642352941176471e-05,
      "loss": 1.1169,
      "step": 40080
    },
    {
      "epoch": 2358.235294117647,
      "grad_norm": 22.687942504882812,
      "learning_rate": 2.641764705882353e-05,
      "loss": 1.1711,
      "step": 40090
    },
    {
      "epoch": 2358.823529411765,
      "grad_norm": 18.067068099975586,
      "learning_rate": 2.6411764705882352e-05,
      "loss": 1.2032,
      "step": 40100
    },
    {
      "epoch": 2359.4117647058824,
      "grad_norm": 16.815149307250977,
      "learning_rate": 2.6405882352941182e-05,
      "loss": 1.1968,
      "step": 40110
    },
    {
      "epoch": 2360.0,
      "grad_norm": 28.90366554260254,
      "learning_rate": 2.64e-05,
      "loss": 1.2185,
      "step": 40120
    },
    {
      "epoch": 2360.5882352941176,
      "grad_norm": 18.109804153442383,
      "learning_rate": 2.6394117647058824e-05,
      "loss": 1.1711,
      "step": 40130
    },
    {
      "epoch": 2361.176470588235,
      "grad_norm": 22.383771896362305,
      "learning_rate": 2.6388235294117647e-05,
      "loss": 1.2077,
      "step": 40140
    },
    {
      "epoch": 2361.764705882353,
      "grad_norm": 19.692703247070312,
      "learning_rate": 2.638235294117647e-05,
      "loss": 1.0626,
      "step": 40150
    },
    {
      "epoch": 2362.3529411764707,
      "grad_norm": 16.82971954345703,
      "learning_rate": 2.6376470588235297e-05,
      "loss": 1.1676,
      "step": 40160
    },
    {
      "epoch": 2362.9411764705883,
      "grad_norm": 20.983970642089844,
      "learning_rate": 2.637058823529412e-05,
      "loss": 1.1981,
      "step": 40170
    },
    {
      "epoch": 2363.529411764706,
      "grad_norm": 23.11709976196289,
      "learning_rate": 2.6364705882352943e-05,
      "loss": 1.2993,
      "step": 40180
    },
    {
      "epoch": 2364.1176470588234,
      "grad_norm": 18.996585845947266,
      "learning_rate": 2.6358823529411762e-05,
      "loss": 1.3358,
      "step": 40190
    },
    {
      "epoch": 2364.705882352941,
      "grad_norm": 18.59421730041504,
      "learning_rate": 2.6352941176470592e-05,
      "loss": 1.2927,
      "step": 40200
    },
    {
      "epoch": 2365.294117647059,
      "grad_norm": 19.500802993774414,
      "learning_rate": 2.6347058823529415e-05,
      "loss": 1.1549,
      "step": 40210
    },
    {
      "epoch": 2365.8823529411766,
      "grad_norm": 19.17159080505371,
      "learning_rate": 2.6341176470588235e-05,
      "loss": 1.177,
      "step": 40220
    },
    {
      "epoch": 2366.470588235294,
      "grad_norm": 20.512399673461914,
      "learning_rate": 2.6335294117647058e-05,
      "loss": 1.2537,
      "step": 40230
    },
    {
      "epoch": 2367.0588235294117,
      "grad_norm": 22.292335510253906,
      "learning_rate": 2.6329411764705887e-05,
      "loss": 1.1988,
      "step": 40240
    },
    {
      "epoch": 2367.6470588235293,
      "grad_norm": 27.09523582458496,
      "learning_rate": 2.6323529411764707e-05,
      "loss": 1.3414,
      "step": 40250
    },
    {
      "epoch": 2368.235294117647,
      "grad_norm": 18.629589080810547,
      "learning_rate": 2.631764705882353e-05,
      "loss": 1.0675,
      "step": 40260
    },
    {
      "epoch": 2368.823529411765,
      "grad_norm": 16.92624855041504,
      "learning_rate": 2.6311764705882353e-05,
      "loss": 1.1961,
      "step": 40270
    },
    {
      "epoch": 2369.4117647058824,
      "grad_norm": 21.339740753173828,
      "learning_rate": 2.630588235294118e-05,
      "loss": 1.2097,
      "step": 40280
    },
    {
      "epoch": 2370.0,
      "grad_norm": 22.91495132446289,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 1.2291,
      "step": 40290
    },
    {
      "epoch": 2370.5882352941176,
      "grad_norm": 22.77967071533203,
      "learning_rate": 2.6294117647058825e-05,
      "loss": 1.2294,
      "step": 40300
    },
    {
      "epoch": 2371.176470588235,
      "grad_norm": 20.212085723876953,
      "learning_rate": 2.6288235294117648e-05,
      "loss": 1.1781,
      "step": 40310
    },
    {
      "epoch": 2371.764705882353,
      "grad_norm": 19.921186447143555,
      "learning_rate": 2.6282352941176475e-05,
      "loss": 1.1255,
      "step": 40320
    },
    {
      "epoch": 2372.3529411764707,
      "grad_norm": 15.970841407775879,
      "learning_rate": 2.6276470588235298e-05,
      "loss": 1.0782,
      "step": 40330
    },
    {
      "epoch": 2372.9411764705883,
      "grad_norm": 17.405227661132812,
      "learning_rate": 2.627058823529412e-05,
      "loss": 1.2313,
      "step": 40340
    },
    {
      "epoch": 2373.529411764706,
      "grad_norm": 22.0474910736084,
      "learning_rate": 2.626470588235294e-05,
      "loss": 1.2767,
      "step": 40350
    },
    {
      "epoch": 2374.1176470588234,
      "grad_norm": 13.914324760437012,
      "learning_rate": 2.6258823529411763e-05,
      "loss": 1.218,
      "step": 40360
    },
    {
      "epoch": 2374.705882352941,
      "grad_norm": 15.93727970123291,
      "learning_rate": 2.625294117647059e-05,
      "loss": 1.2236,
      "step": 40370
    },
    {
      "epoch": 2375.294117647059,
      "grad_norm": 19.896785736083984,
      "learning_rate": 2.6247058823529413e-05,
      "loss": 1.148,
      "step": 40380
    },
    {
      "epoch": 2375.8823529411766,
      "grad_norm": 17.60886001586914,
      "learning_rate": 2.6241176470588236e-05,
      "loss": 0.9954,
      "step": 40390
    },
    {
      "epoch": 2376.470588235294,
      "grad_norm": 22.965883255004883,
      "learning_rate": 2.623529411764706e-05,
      "loss": 1.2113,
      "step": 40400
    },
    {
      "epoch": 2377.0588235294117,
      "grad_norm": 20.652624130249023,
      "learning_rate": 2.6229411764705885e-05,
      "loss": 1.1206,
      "step": 40410
    },
    {
      "epoch": 2377.6470588235293,
      "grad_norm": 15.094046592712402,
      "learning_rate": 2.6223529411764708e-05,
      "loss": 1.0311,
      "step": 40420
    },
    {
      "epoch": 2378.235294117647,
      "grad_norm": 19.06190299987793,
      "learning_rate": 2.621764705882353e-05,
      "loss": 1.0684,
      "step": 40430
    },
    {
      "epoch": 2378.823529411765,
      "grad_norm": 21.933298110961914,
      "learning_rate": 2.6211764705882354e-05,
      "loss": 1.1882,
      "step": 40440
    },
    {
      "epoch": 2379.4117647058824,
      "grad_norm": 18.147567749023438,
      "learning_rate": 2.620588235294118e-05,
      "loss": 1.1941,
      "step": 40450
    },
    {
      "epoch": 2380.0,
      "grad_norm": 21.79515266418457,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 1.2011,
      "step": 40460
    },
    {
      "epoch": 2380.5882352941176,
      "grad_norm": 14.684332847595215,
      "learning_rate": 2.6194117647058823e-05,
      "loss": 1.2738,
      "step": 40470
    },
    {
      "epoch": 2381.176470588235,
      "grad_norm": 19.566858291625977,
      "learning_rate": 2.6188235294117646e-05,
      "loss": 1.2814,
      "step": 40480
    },
    {
      "epoch": 2381.764705882353,
      "grad_norm": 14.931983947753906,
      "learning_rate": 2.6182352941176476e-05,
      "loss": 1.1128,
      "step": 40490
    },
    {
      "epoch": 2382.3529411764707,
      "grad_norm": 20.002025604248047,
      "learning_rate": 2.6176470588235295e-05,
      "loss": 1.2721,
      "step": 40500
    },
    {
      "epoch": 2382.9411764705883,
      "grad_norm": 20.472396850585938,
      "learning_rate": 2.6170588235294118e-05,
      "loss": 1.1444,
      "step": 40510
    },
    {
      "epoch": 2383.529411764706,
      "grad_norm": 23.486629486083984,
      "learning_rate": 2.616470588235294e-05,
      "loss": 1.2129,
      "step": 40520
    },
    {
      "epoch": 2384.1176470588234,
      "grad_norm": 20.52204132080078,
      "learning_rate": 2.6158823529411764e-05,
      "loss": 1.199,
      "step": 40530
    },
    {
      "epoch": 2384.705882352941,
      "grad_norm": 16.089330673217773,
      "learning_rate": 2.615294117647059e-05,
      "loss": 1.1518,
      "step": 40540
    },
    {
      "epoch": 2385.294117647059,
      "grad_norm": 17.58439826965332,
      "learning_rate": 2.6147058823529414e-05,
      "loss": 1.2863,
      "step": 40550
    },
    {
      "epoch": 2385.8823529411766,
      "grad_norm": 19.846416473388672,
      "learning_rate": 2.6141176470588237e-05,
      "loss": 1.1613,
      "step": 40560
    },
    {
      "epoch": 2386.470588235294,
      "grad_norm": 23.126312255859375,
      "learning_rate": 2.6135294117647056e-05,
      "loss": 1.2443,
      "step": 40570
    },
    {
      "epoch": 2387.0588235294117,
      "grad_norm": 17.215063095092773,
      "learning_rate": 2.6129411764705886e-05,
      "loss": 1.1637,
      "step": 40580
    },
    {
      "epoch": 2387.6470588235293,
      "grad_norm": 23.43136978149414,
      "learning_rate": 2.612352941176471e-05,
      "loss": 1.2648,
      "step": 40590
    },
    {
      "epoch": 2388.235294117647,
      "grad_norm": 21.78780746459961,
      "learning_rate": 2.611764705882353e-05,
      "loss": 1.325,
      "step": 40600
    },
    {
      "epoch": 2388.823529411765,
      "grad_norm": 17.00875473022461,
      "learning_rate": 2.611176470588235e-05,
      "loss": 1.1323,
      "step": 40610
    },
    {
      "epoch": 2389.4117647058824,
      "grad_norm": 24.115102767944336,
      "learning_rate": 2.610588235294118e-05,
      "loss": 1.2697,
      "step": 40620
    },
    {
      "epoch": 2390.0,
      "grad_norm": 17.702041625976562,
      "learning_rate": 2.61e-05,
      "loss": 1.1581,
      "step": 40630
    },
    {
      "epoch": 2390.5882352941176,
      "grad_norm": 16.675525665283203,
      "learning_rate": 2.6094117647058824e-05,
      "loss": 1.1875,
      "step": 40640
    },
    {
      "epoch": 2391.176470588235,
      "grad_norm": 17.9198055267334,
      "learning_rate": 2.6088235294117647e-05,
      "loss": 1.132,
      "step": 40650
    },
    {
      "epoch": 2391.764705882353,
      "grad_norm": 19.04227066040039,
      "learning_rate": 2.6082352941176473e-05,
      "loss": 1.2247,
      "step": 40660
    },
    {
      "epoch": 2392.3529411764707,
      "grad_norm": 17.285415649414062,
      "learning_rate": 2.6076470588235296e-05,
      "loss": 1.2313,
      "step": 40670
    },
    {
      "epoch": 2392.9411764705883,
      "grad_norm": 17.041641235351562,
      "learning_rate": 2.607058823529412e-05,
      "loss": 1.1696,
      "step": 40680
    },
    {
      "epoch": 2393.529411764706,
      "grad_norm": 14.577338218688965,
      "learning_rate": 2.6064705882352942e-05,
      "loss": 1.1135,
      "step": 40690
    },
    {
      "epoch": 2394.1176470588234,
      "grad_norm": 20.374784469604492,
      "learning_rate": 2.6058823529411762e-05,
      "loss": 1.2385,
      "step": 40700
    },
    {
      "epoch": 2394.705882352941,
      "grad_norm": 19.22112464904785,
      "learning_rate": 2.605294117647059e-05,
      "loss": 1.2408,
      "step": 40710
    },
    {
      "epoch": 2395.294117647059,
      "grad_norm": 21.709392547607422,
      "learning_rate": 2.6047058823529414e-05,
      "loss": 1.2993,
      "step": 40720
    },
    {
      "epoch": 2395.8823529411766,
      "grad_norm": 18.940629959106445,
      "learning_rate": 2.6041176470588234e-05,
      "loss": 1.1586,
      "step": 40730
    },
    {
      "epoch": 2396.470588235294,
      "grad_norm": 19.989309310913086,
      "learning_rate": 2.6035294117647057e-05,
      "loss": 1.0857,
      "step": 40740
    },
    {
      "epoch": 2397.0588235294117,
      "grad_norm": 19.966144561767578,
      "learning_rate": 2.6029411764705887e-05,
      "loss": 1.1395,
      "step": 40750
    },
    {
      "epoch": 2397.6470588235293,
      "grad_norm": 21.8541259765625,
      "learning_rate": 2.6023529411764706e-05,
      "loss": 1.1713,
      "step": 40760
    },
    {
      "epoch": 2398.235294117647,
      "grad_norm": 16.53429412841797,
      "learning_rate": 2.601764705882353e-05,
      "loss": 1.1677,
      "step": 40770
    },
    {
      "epoch": 2398.823529411765,
      "grad_norm": 21.539880752563477,
      "learning_rate": 2.6011764705882352e-05,
      "loss": 1.2713,
      "step": 40780
    },
    {
      "epoch": 2399.4117647058824,
      "grad_norm": 21.509824752807617,
      "learning_rate": 2.600588235294118e-05,
      "loss": 1.1672,
      "step": 40790
    },
    {
      "epoch": 2400.0,
      "grad_norm": 18.090045928955078,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.1323,
      "step": 40800
    },
    {
      "epoch": 2400.5882352941176,
      "grad_norm": 18.3690185546875,
      "learning_rate": 2.5994117647058825e-05,
      "loss": 1.1381,
      "step": 40810
    },
    {
      "epoch": 2401.176470588235,
      "grad_norm": 18.218795776367188,
      "learning_rate": 2.5988235294117648e-05,
      "loss": 1.2681,
      "step": 40820
    },
    {
      "epoch": 2401.764705882353,
      "grad_norm": 20.864349365234375,
      "learning_rate": 2.5982352941176474e-05,
      "loss": 1.2231,
      "step": 40830
    },
    {
      "epoch": 2402.3529411764707,
      "grad_norm": 19.51295280456543,
      "learning_rate": 2.5976470588235297e-05,
      "loss": 1.1732,
      "step": 40840
    },
    {
      "epoch": 2402.9411764705883,
      "grad_norm": 24.216724395751953,
      "learning_rate": 2.597058823529412e-05,
      "loss": 1.2293,
      "step": 40850
    },
    {
      "epoch": 2403.529411764706,
      "grad_norm": 22.126020431518555,
      "learning_rate": 2.596470588235294e-05,
      "loss": 1.2676,
      "step": 40860
    },
    {
      "epoch": 2404.1176470588234,
      "grad_norm": 18.868608474731445,
      "learning_rate": 2.5958823529411763e-05,
      "loss": 1.1609,
      "step": 40870
    },
    {
      "epoch": 2404.705882352941,
      "grad_norm": 14.785478591918945,
      "learning_rate": 2.5952941176470592e-05,
      "loss": 1.1368,
      "step": 40880
    },
    {
      "epoch": 2405.294117647059,
      "grad_norm": 22.13271141052246,
      "learning_rate": 2.5947058823529412e-05,
      "loss": 1.1661,
      "step": 40890
    },
    {
      "epoch": 2405.8823529411766,
      "grad_norm": 18.25286102294922,
      "learning_rate": 2.5941176470588235e-05,
      "loss": 1.1447,
      "step": 40900
    },
    {
      "epoch": 2406.470588235294,
      "grad_norm": 20.7169132232666,
      "learning_rate": 2.5935294117647058e-05,
      "loss": 1.1858,
      "step": 40910
    },
    {
      "epoch": 2407.0588235294117,
      "grad_norm": 19.155799865722656,
      "learning_rate": 2.5929411764705884e-05,
      "loss": 1.222,
      "step": 40920
    },
    {
      "epoch": 2407.6470588235293,
      "grad_norm": 17.613677978515625,
      "learning_rate": 2.5923529411764707e-05,
      "loss": 1.1125,
      "step": 40930
    },
    {
      "epoch": 2408.235294117647,
      "grad_norm": 18.71017837524414,
      "learning_rate": 2.591764705882353e-05,
      "loss": 1.1166,
      "step": 40940
    },
    {
      "epoch": 2408.823529411765,
      "grad_norm": 16.388153076171875,
      "learning_rate": 2.5911764705882353e-05,
      "loss": 1.0132,
      "step": 40950
    },
    {
      "epoch": 2409.4117647058824,
      "grad_norm": 17.77890968322754,
      "learning_rate": 2.590588235294118e-05,
      "loss": 1.1423,
      "step": 40960
    },
    {
      "epoch": 2410.0,
      "grad_norm": 20.07366371154785,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 1.13,
      "step": 40970
    },
    {
      "epoch": 2410.5882352941176,
      "grad_norm": 18.0759334564209,
      "learning_rate": 2.5894117647058826e-05,
      "loss": 1.2432,
      "step": 40980
    },
    {
      "epoch": 2411.176470588235,
      "grad_norm": 21.052433013916016,
      "learning_rate": 2.5888235294117645e-05,
      "loss": 1.2309,
      "step": 40990
    },
    {
      "epoch": 2411.764705882353,
      "grad_norm": 20.930347442626953,
      "learning_rate": 2.5882352941176475e-05,
      "loss": 1.2234,
      "step": 41000
    },
    {
      "epoch": 2412.3529411764707,
      "grad_norm": 17.216201782226562,
      "learning_rate": 2.5876470588235298e-05,
      "loss": 1.1804,
      "step": 41010
    },
    {
      "epoch": 2412.9411764705883,
      "grad_norm": 14.717926979064941,
      "learning_rate": 2.5870588235294118e-05,
      "loss": 1.2181,
      "step": 41020
    },
    {
      "epoch": 2413.529411764706,
      "grad_norm": 20.01834487915039,
      "learning_rate": 2.586470588235294e-05,
      "loss": 1.1408,
      "step": 41030
    },
    {
      "epoch": 2414.1176470588234,
      "grad_norm": 16.28571891784668,
      "learning_rate": 2.5858823529411767e-05,
      "loss": 1.1826,
      "step": 41040
    },
    {
      "epoch": 2414.705882352941,
      "grad_norm": 16.535839080810547,
      "learning_rate": 2.585294117647059e-05,
      "loss": 1.1807,
      "step": 41050
    },
    {
      "epoch": 2415.294117647059,
      "grad_norm": 19.4072265625,
      "learning_rate": 2.5847058823529413e-05,
      "loss": 1.1301,
      "step": 41060
    },
    {
      "epoch": 2415.8823529411766,
      "grad_norm": 21.529277801513672,
      "learning_rate": 2.5841176470588236e-05,
      "loss": 1.0857,
      "step": 41070
    },
    {
      "epoch": 2416.470588235294,
      "grad_norm": 23.448820114135742,
      "learning_rate": 2.583529411764706e-05,
      "loss": 1.2666,
      "step": 41080
    },
    {
      "epoch": 2417.0588235294117,
      "grad_norm": 19.333797454833984,
      "learning_rate": 2.5829411764705885e-05,
      "loss": 1.2261,
      "step": 41090
    },
    {
      "epoch": 2417.6470588235293,
      "grad_norm": 18.200166702270508,
      "learning_rate": 2.582352941176471e-05,
      "loss": 1.1301,
      "step": 41100
    },
    {
      "epoch": 2418.235294117647,
      "grad_norm": 24.734323501586914,
      "learning_rate": 2.581764705882353e-05,
      "loss": 1.0965,
      "step": 41110
    },
    {
      "epoch": 2418.823529411765,
      "grad_norm": 17.626853942871094,
      "learning_rate": 2.581176470588235e-05,
      "loss": 1.147,
      "step": 41120
    },
    {
      "epoch": 2419.4117647058824,
      "grad_norm": 23.86199378967285,
      "learning_rate": 2.580588235294118e-05,
      "loss": 1.174,
      "step": 41130
    },
    {
      "epoch": 2420.0,
      "grad_norm": 22.600202560424805,
      "learning_rate": 2.58e-05,
      "loss": 1.2163,
      "step": 41140
    },
    {
      "epoch": 2420.5882352941176,
      "grad_norm": 19.188770294189453,
      "learning_rate": 2.5794117647058823e-05,
      "loss": 1.1067,
      "step": 41150
    },
    {
      "epoch": 2421.176470588235,
      "grad_norm": 19.92756462097168,
      "learning_rate": 2.5788235294117646e-05,
      "loss": 1.2118,
      "step": 41160
    },
    {
      "epoch": 2421.764705882353,
      "grad_norm": 19.586484909057617,
      "learning_rate": 2.5782352941176473e-05,
      "loss": 1.1185,
      "step": 41170
    },
    {
      "epoch": 2422.3529411764707,
      "grad_norm": 19.372140884399414,
      "learning_rate": 2.5776470588235296e-05,
      "loss": 1.1734,
      "step": 41180
    },
    {
      "epoch": 2422.9411764705883,
      "grad_norm": 14.46992015838623,
      "learning_rate": 2.577058823529412e-05,
      "loss": 1.1641,
      "step": 41190
    },
    {
      "epoch": 2423.529411764706,
      "grad_norm": 17.073074340820312,
      "learning_rate": 2.576470588235294e-05,
      "loss": 1.1923,
      "step": 41200
    },
    {
      "epoch": 2424.1176470588234,
      "grad_norm": 19.25806999206543,
      "learning_rate": 2.5758823529411768e-05,
      "loss": 1.1342,
      "step": 41210
    },
    {
      "epoch": 2424.705882352941,
      "grad_norm": 16.34445571899414,
      "learning_rate": 2.575294117647059e-05,
      "loss": 1.157,
      "step": 41220
    },
    {
      "epoch": 2425.294117647059,
      "grad_norm": 15.121685028076172,
      "learning_rate": 2.5747058823529414e-05,
      "loss": 1.1333,
      "step": 41230
    },
    {
      "epoch": 2425.8823529411766,
      "grad_norm": 19.67616844177246,
      "learning_rate": 2.5741176470588234e-05,
      "loss": 1.2078,
      "step": 41240
    },
    {
      "epoch": 2426.470588235294,
      "grad_norm": 21.83034896850586,
      "learning_rate": 2.5735294117647057e-05,
      "loss": 1.1558,
      "step": 41250
    },
    {
      "epoch": 2427.0588235294117,
      "grad_norm": 25.533090591430664,
      "learning_rate": 2.5729411764705886e-05,
      "loss": 1.1679,
      "step": 41260
    },
    {
      "epoch": 2427.6470588235293,
      "grad_norm": 18.681306838989258,
      "learning_rate": 2.5723529411764706e-05,
      "loss": 1.1373,
      "step": 41270
    },
    {
      "epoch": 2428.235294117647,
      "grad_norm": 15.766763687133789,
      "learning_rate": 2.571764705882353e-05,
      "loss": 1.1204,
      "step": 41280
    },
    {
      "epoch": 2428.823529411765,
      "grad_norm": 19.398122787475586,
      "learning_rate": 2.5711764705882352e-05,
      "loss": 1.1892,
      "step": 41290
    },
    {
      "epoch": 2429.4117647058824,
      "grad_norm": 18.24831199645996,
      "learning_rate": 2.5705882352941178e-05,
      "loss": 1.0712,
      "step": 41300
    },
    {
      "epoch": 2430.0,
      "grad_norm": 23.766368865966797,
      "learning_rate": 2.57e-05,
      "loss": 1.1408,
      "step": 41310
    },
    {
      "epoch": 2430.5882352941176,
      "grad_norm": 21.639150619506836,
      "learning_rate": 2.5694117647058824e-05,
      "loss": 1.2658,
      "step": 41320
    },
    {
      "epoch": 2431.176470588235,
      "grad_norm": 16.40488624572754,
      "learning_rate": 2.5688235294117647e-05,
      "loss": 1.2262,
      "step": 41330
    },
    {
      "epoch": 2431.764705882353,
      "grad_norm": 19.712430953979492,
      "learning_rate": 2.5682352941176474e-05,
      "loss": 1.1282,
      "step": 41340
    },
    {
      "epoch": 2432.3529411764707,
      "grad_norm": 18.656002044677734,
      "learning_rate": 2.5676470588235297e-05,
      "loss": 1.1643,
      "step": 41350
    },
    {
      "epoch": 2432.9411764705883,
      "grad_norm": 15.931578636169434,
      "learning_rate": 2.567058823529412e-05,
      "loss": 1.1874,
      "step": 41360
    },
    {
      "epoch": 2433.529411764706,
      "grad_norm": 15.853094100952148,
      "learning_rate": 2.566470588235294e-05,
      "loss": 1.0584,
      "step": 41370
    },
    {
      "epoch": 2434.1176470588234,
      "grad_norm": 18.613048553466797,
      "learning_rate": 2.565882352941177e-05,
      "loss": 1.1956,
      "step": 41380
    },
    {
      "epoch": 2434.705882352941,
      "grad_norm": 19.654401779174805,
      "learning_rate": 2.5652941176470592e-05,
      "loss": 1.1291,
      "step": 41390
    },
    {
      "epoch": 2435.294117647059,
      "grad_norm": 18.470321655273438,
      "learning_rate": 2.564705882352941e-05,
      "loss": 1.2206,
      "step": 41400
    },
    {
      "epoch": 2435.8823529411766,
      "grad_norm": 15.807662963867188,
      "learning_rate": 2.5641176470588235e-05,
      "loss": 1.1326,
      "step": 41410
    },
    {
      "epoch": 2436.470588235294,
      "grad_norm": 17.78002166748047,
      "learning_rate": 2.5635294117647058e-05,
      "loss": 1.1788,
      "step": 41420
    },
    {
      "epoch": 2437.0588235294117,
      "grad_norm": 20.243356704711914,
      "learning_rate": 2.5629411764705884e-05,
      "loss": 1.097,
      "step": 41430
    },
    {
      "epoch": 2437.6470588235293,
      "grad_norm": 21.608409881591797,
      "learning_rate": 2.5623529411764707e-05,
      "loss": 1.1644,
      "step": 41440
    },
    {
      "epoch": 2438.235294117647,
      "grad_norm": 14.471895217895508,
      "learning_rate": 2.561764705882353e-05,
      "loss": 1.1734,
      "step": 41450
    },
    {
      "epoch": 2438.823529411765,
      "grad_norm": 19.60403060913086,
      "learning_rate": 2.5611764705882353e-05,
      "loss": 1.228,
      "step": 41460
    },
    {
      "epoch": 2439.4117647058824,
      "grad_norm": 22.772825241088867,
      "learning_rate": 2.560588235294118e-05,
      "loss": 1.1893,
      "step": 41470
    },
    {
      "epoch": 2440.0,
      "grad_norm": 22.52410888671875,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 1.1694,
      "step": 41480
    },
    {
      "epoch": 2440.5882352941176,
      "grad_norm": 19.62392234802246,
      "learning_rate": 2.5594117647058825e-05,
      "loss": 1.174,
      "step": 41490
    },
    {
      "epoch": 2441.176470588235,
      "grad_norm": 15.893924713134766,
      "learning_rate": 2.5588235294117645e-05,
      "loss": 1.0928,
      "step": 41500
    },
    {
      "epoch": 2441.764705882353,
      "grad_norm": 23.7130184173584,
      "learning_rate": 2.5582352941176475e-05,
      "loss": 1.2856,
      "step": 41510
    },
    {
      "epoch": 2442.3529411764707,
      "grad_norm": 20.02045249938965,
      "learning_rate": 2.5576470588235298e-05,
      "loss": 1.1726,
      "step": 41520
    },
    {
      "epoch": 2442.9411764705883,
      "grad_norm": 16.535139083862305,
      "learning_rate": 2.5570588235294117e-05,
      "loss": 1.1698,
      "step": 41530
    },
    {
      "epoch": 2443.529411764706,
      "grad_norm": 19.037839889526367,
      "learning_rate": 2.556470588235294e-05,
      "loss": 1.0903,
      "step": 41540
    },
    {
      "epoch": 2444.1176470588234,
      "grad_norm": 17.905717849731445,
      "learning_rate": 2.555882352941177e-05,
      "loss": 1.0693,
      "step": 41550
    },
    {
      "epoch": 2444.705882352941,
      "grad_norm": 19.735612869262695,
      "learning_rate": 2.555294117647059e-05,
      "loss": 1.207,
      "step": 41560
    },
    {
      "epoch": 2445.294117647059,
      "grad_norm": 17.662446975708008,
      "learning_rate": 2.5547058823529413e-05,
      "loss": 1.0995,
      "step": 41570
    },
    {
      "epoch": 2445.8823529411766,
      "grad_norm": 20.57464599609375,
      "learning_rate": 2.5541176470588235e-05,
      "loss": 1.1069,
      "step": 41580
    },
    {
      "epoch": 2446.470588235294,
      "grad_norm": 23.283777236938477,
      "learning_rate": 2.5535294117647062e-05,
      "loss": 1.2038,
      "step": 41590
    },
    {
      "epoch": 2447.0588235294117,
      "grad_norm": 20.29688835144043,
      "learning_rate": 2.5529411764705885e-05,
      "loss": 1.0873,
      "step": 41600
    },
    {
      "epoch": 2447.6470588235293,
      "grad_norm": 20.681589126586914,
      "learning_rate": 2.5523529411764708e-05,
      "loss": 1.0885,
      "step": 41610
    },
    {
      "epoch": 2448.235294117647,
      "grad_norm": 22.166385650634766,
      "learning_rate": 2.551764705882353e-05,
      "loss": 1.1982,
      "step": 41620
    },
    {
      "epoch": 2448.823529411765,
      "grad_norm": 13.833719253540039,
      "learning_rate": 2.551176470588235e-05,
      "loss": 1.1649,
      "step": 41630
    },
    {
      "epoch": 2449.4117647058824,
      "grad_norm": 18.374950408935547,
      "learning_rate": 2.550588235294118e-05,
      "loss": 1.1178,
      "step": 41640
    },
    {
      "epoch": 2450.0,
      "grad_norm": 23.604690551757812,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 1.2193,
      "step": 41650
    },
    {
      "epoch": 2450.5882352941176,
      "grad_norm": 19.137744903564453,
      "learning_rate": 2.5494117647058823e-05,
      "loss": 1.079,
      "step": 41660
    },
    {
      "epoch": 2451.176470588235,
      "grad_norm": 17.393566131591797,
      "learning_rate": 2.5488235294117646e-05,
      "loss": 0.9265,
      "step": 41670
    },
    {
      "epoch": 2451.764705882353,
      "grad_norm": 23.61165428161621,
      "learning_rate": 2.5482352941176476e-05,
      "loss": 1.2109,
      "step": 41680
    },
    {
      "epoch": 2452.3529411764707,
      "grad_norm": 23.48764991760254,
      "learning_rate": 2.5476470588235295e-05,
      "loss": 1.2473,
      "step": 41690
    },
    {
      "epoch": 2452.9411764705883,
      "grad_norm": 21.37618064880371,
      "learning_rate": 2.5470588235294118e-05,
      "loss": 1.138,
      "step": 41700
    },
    {
      "epoch": 2453.529411764706,
      "grad_norm": 25.84246826171875,
      "learning_rate": 2.546470588235294e-05,
      "loss": 1.1438,
      "step": 41710
    },
    {
      "epoch": 2454.1176470588234,
      "grad_norm": 24.16041374206543,
      "learning_rate": 2.5458823529411767e-05,
      "loss": 1.1618,
      "step": 41720
    },
    {
      "epoch": 2454.705882352941,
      "grad_norm": 22.868453979492188,
      "learning_rate": 2.545294117647059e-05,
      "loss": 1.193,
      "step": 41730
    },
    {
      "epoch": 2455.294117647059,
      "grad_norm": 17.92121696472168,
      "learning_rate": 2.5447058823529413e-05,
      "loss": 1.248,
      "step": 41740
    },
    {
      "epoch": 2455.8823529411766,
      "grad_norm": 15.873744010925293,
      "learning_rate": 2.5441176470588236e-05,
      "loss": 1.1138,
      "step": 41750
    },
    {
      "epoch": 2456.470588235294,
      "grad_norm": 21.82227897644043,
      "learning_rate": 2.5435294117647063e-05,
      "loss": 1.118,
      "step": 41760
    },
    {
      "epoch": 2457.0588235294117,
      "grad_norm": 15.778217315673828,
      "learning_rate": 2.5429411764705886e-05,
      "loss": 1.1791,
      "step": 41770
    },
    {
      "epoch": 2457.6470588235293,
      "grad_norm": 21.525157928466797,
      "learning_rate": 2.542352941176471e-05,
      "loss": 1.0244,
      "step": 41780
    },
    {
      "epoch": 2458.235294117647,
      "grad_norm": 19.580163955688477,
      "learning_rate": 2.541764705882353e-05,
      "loss": 1.0276,
      "step": 41790
    },
    {
      "epoch": 2458.823529411765,
      "grad_norm": 19.36709213256836,
      "learning_rate": 2.541176470588235e-05,
      "loss": 1.108,
      "step": 41800
    },
    {
      "epoch": 2459.4117647058824,
      "grad_norm": 17.623676300048828,
      "learning_rate": 2.5405882352941178e-05,
      "loss": 1.2298,
      "step": 41810
    },
    {
      "epoch": 2460.0,
      "grad_norm": 21.048583984375,
      "learning_rate": 2.54e-05,
      "loss": 1.0829,
      "step": 41820
    },
    {
      "epoch": 2460.5882352941176,
      "grad_norm": 21.711904525756836,
      "learning_rate": 2.5394117647058824e-05,
      "loss": 1.1489,
      "step": 41830
    },
    {
      "epoch": 2461.176470588235,
      "grad_norm": 20.93794059753418,
      "learning_rate": 2.5388235294117647e-05,
      "loss": 1.0627,
      "step": 41840
    },
    {
      "epoch": 2461.764705882353,
      "grad_norm": 23.176448822021484,
      "learning_rate": 2.5382352941176473e-05,
      "loss": 1.2512,
      "step": 41850
    },
    {
      "epoch": 2462.3529411764707,
      "grad_norm": 18.33908462524414,
      "learning_rate": 2.5376470588235296e-05,
      "loss": 1.1752,
      "step": 41860
    },
    {
      "epoch": 2462.9411764705883,
      "grad_norm": 20.431686401367188,
      "learning_rate": 2.537058823529412e-05,
      "loss": 1.1632,
      "step": 41870
    },
    {
      "epoch": 2463.529411764706,
      "grad_norm": 20.542072296142578,
      "learning_rate": 2.5364705882352942e-05,
      "loss": 1.1911,
      "step": 41880
    },
    {
      "epoch": 2464.1176470588234,
      "grad_norm": 21.262264251708984,
      "learning_rate": 2.535882352941177e-05,
      "loss": 1.1285,
      "step": 41890
    },
    {
      "epoch": 2464.705882352941,
      "grad_norm": 20.52128791809082,
      "learning_rate": 2.535294117647059e-05,
      "loss": 1.1755,
      "step": 41900
    },
    {
      "epoch": 2465.294117647059,
      "grad_norm": 20.18695068359375,
      "learning_rate": 2.534705882352941e-05,
      "loss": 1.0965,
      "step": 41910
    },
    {
      "epoch": 2465.8823529411766,
      "grad_norm": 20.66316032409668,
      "learning_rate": 2.5341176470588234e-05,
      "loss": 1.1819,
      "step": 41920
    },
    {
      "epoch": 2466.470588235294,
      "grad_norm": 16.04084014892578,
      "learning_rate": 2.5335294117647064e-05,
      "loss": 1.2069,
      "step": 41930
    },
    {
      "epoch": 2467.0588235294117,
      "grad_norm": 16.58664894104004,
      "learning_rate": 2.5329411764705883e-05,
      "loss": 1.2076,
      "step": 41940
    },
    {
      "epoch": 2467.6470588235293,
      "grad_norm": 15.149568557739258,
      "learning_rate": 2.5323529411764706e-05,
      "loss": 1.2296,
      "step": 41950
    },
    {
      "epoch": 2468.235294117647,
      "grad_norm": 16.300777435302734,
      "learning_rate": 2.531764705882353e-05,
      "loss": 1.1352,
      "step": 41960
    },
    {
      "epoch": 2468.823529411765,
      "grad_norm": 15.311619758605957,
      "learning_rate": 2.5311764705882352e-05,
      "loss": 1.1301,
      "step": 41970
    },
    {
      "epoch": 2469.4117647058824,
      "grad_norm": 23.423664093017578,
      "learning_rate": 2.530588235294118e-05,
      "loss": 1.2498,
      "step": 41980
    },
    {
      "epoch": 2470.0,
      "grad_norm": 25.532468795776367,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 1.1455,
      "step": 41990
    },
    {
      "epoch": 2470.5882352941176,
      "grad_norm": 17.209379196166992,
      "learning_rate": 2.5294117647058825e-05,
      "loss": 1.1245,
      "step": 42000
    },
    {
      "epoch": 2471.176470588235,
      "grad_norm": 19.704801559448242,
      "learning_rate": 2.5288235294117644e-05,
      "loss": 1.0634,
      "step": 42010
    },
    {
      "epoch": 2471.764705882353,
      "grad_norm": 16.15565299987793,
      "learning_rate": 2.5282352941176474e-05,
      "loss": 1.0156,
      "step": 42020
    },
    {
      "epoch": 2472.3529411764707,
      "grad_norm": 19.66166877746582,
      "learning_rate": 2.5276470588235297e-05,
      "loss": 1.0672,
      "step": 42030
    },
    {
      "epoch": 2472.9411764705883,
      "grad_norm": 19.434520721435547,
      "learning_rate": 2.5270588235294117e-05,
      "loss": 1.0397,
      "step": 42040
    },
    {
      "epoch": 2473.529411764706,
      "grad_norm": 21.78943634033203,
      "learning_rate": 2.526470588235294e-05,
      "loss": 1.1341,
      "step": 42050
    },
    {
      "epoch": 2474.1176470588234,
      "grad_norm": 18.10319709777832,
      "learning_rate": 2.525882352941177e-05,
      "loss": 1.1195,
      "step": 42060
    },
    {
      "epoch": 2474.705882352941,
      "grad_norm": 19.15669059753418,
      "learning_rate": 2.525294117647059e-05,
      "loss": 1.1204,
      "step": 42070
    },
    {
      "epoch": 2475.294117647059,
      "grad_norm": 20.49941062927246,
      "learning_rate": 2.5247058823529412e-05,
      "loss": 1.0823,
      "step": 42080
    },
    {
      "epoch": 2475.8823529411766,
      "grad_norm": 21.628005981445312,
      "learning_rate": 2.5241176470588235e-05,
      "loss": 1.1326,
      "step": 42090
    },
    {
      "epoch": 2476.470588235294,
      "grad_norm": 20.6678409576416,
      "learning_rate": 2.523529411764706e-05,
      "loss": 1.1812,
      "step": 42100
    },
    {
      "epoch": 2477.0588235294117,
      "grad_norm": 23.428075790405273,
      "learning_rate": 2.5229411764705884e-05,
      "loss": 1.1628,
      "step": 42110
    },
    {
      "epoch": 2477.6470588235293,
      "grad_norm": 20.852153778076172,
      "learning_rate": 2.5223529411764707e-05,
      "loss": 1.0861,
      "step": 42120
    },
    {
      "epoch": 2478.235294117647,
      "grad_norm": 23.69926643371582,
      "learning_rate": 2.521764705882353e-05,
      "loss": 1.1136,
      "step": 42130
    },
    {
      "epoch": 2478.823529411765,
      "grad_norm": 13.275202751159668,
      "learning_rate": 2.521176470588235e-05,
      "loss": 1.0865,
      "step": 42140
    },
    {
      "epoch": 2479.4117647058824,
      "grad_norm": 24.83111572265625,
      "learning_rate": 2.520588235294118e-05,
      "loss": 1.0946,
      "step": 42150
    },
    {
      "epoch": 2480.0,
      "grad_norm": 23.720447540283203,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 1.1046,
      "step": 42160
    },
    {
      "epoch": 2480.5882352941176,
      "grad_norm": 18.342985153198242,
      "learning_rate": 2.5194117647058822e-05,
      "loss": 1.1836,
      "step": 42170
    },
    {
      "epoch": 2481.176470588235,
      "grad_norm": 18.941387176513672,
      "learning_rate": 2.5188235294117645e-05,
      "loss": 1.1675,
      "step": 42180
    },
    {
      "epoch": 2481.764705882353,
      "grad_norm": 26.8563289642334,
      "learning_rate": 2.5182352941176475e-05,
      "loss": 1.267,
      "step": 42190
    },
    {
      "epoch": 2482.3529411764707,
      "grad_norm": 18.862590789794922,
      "learning_rate": 2.5176470588235295e-05,
      "loss": 1.1027,
      "step": 42200
    },
    {
      "epoch": 2482.9411764705883,
      "grad_norm": 18.295392990112305,
      "learning_rate": 2.5170588235294118e-05,
      "loss": 1.2281,
      "step": 42210
    },
    {
      "epoch": 2483.529411764706,
      "grad_norm": 18.517471313476562,
      "learning_rate": 2.516470588235294e-05,
      "loss": 1.1801,
      "step": 42220
    },
    {
      "epoch": 2484.1176470588234,
      "grad_norm": 19.10938835144043,
      "learning_rate": 2.5158823529411767e-05,
      "loss": 1.1398,
      "step": 42230
    },
    {
      "epoch": 2484.705882352941,
      "grad_norm": 19.479488372802734,
      "learning_rate": 2.515294117647059e-05,
      "loss": 1.1735,
      "step": 42240
    },
    {
      "epoch": 2485.294117647059,
      "grad_norm": 22.477218627929688,
      "learning_rate": 2.5147058823529413e-05,
      "loss": 1.0009,
      "step": 42250
    },
    {
      "epoch": 2485.8823529411766,
      "grad_norm": 16.779781341552734,
      "learning_rate": 2.5141176470588236e-05,
      "loss": 1.1038,
      "step": 42260
    },
    {
      "epoch": 2486.470588235294,
      "grad_norm": 18.024951934814453,
      "learning_rate": 2.5135294117647062e-05,
      "loss": 1.0872,
      "step": 42270
    },
    {
      "epoch": 2487.0588235294117,
      "grad_norm": 21.314693450927734,
      "learning_rate": 2.5129411764705885e-05,
      "loss": 1.1111,
      "step": 42280
    },
    {
      "epoch": 2487.6470588235293,
      "grad_norm": 16.09949493408203,
      "learning_rate": 2.5123529411764708e-05,
      "loss": 1.0602,
      "step": 42290
    },
    {
      "epoch": 2488.235294117647,
      "grad_norm": 17.294301986694336,
      "learning_rate": 2.5117647058823528e-05,
      "loss": 1.1501,
      "step": 42300
    },
    {
      "epoch": 2488.823529411765,
      "grad_norm": 18.183979034423828,
      "learning_rate": 2.5111764705882358e-05,
      "loss": 0.996,
      "step": 42310
    },
    {
      "epoch": 2489.4117647058824,
      "grad_norm": 16.999614715576172,
      "learning_rate": 2.510588235294118e-05,
      "loss": 1.1296,
      "step": 42320
    },
    {
      "epoch": 2490.0,
      "grad_norm": 28.82964324951172,
      "learning_rate": 2.51e-05,
      "loss": 1.2049,
      "step": 42330
    },
    {
      "epoch": 2490.5882352941176,
      "grad_norm": 18.120147705078125,
      "learning_rate": 2.5094117647058823e-05,
      "loss": 1.2064,
      "step": 42340
    },
    {
      "epoch": 2491.176470588235,
      "grad_norm": 20.676977157592773,
      "learning_rate": 2.5088235294117646e-05,
      "loss": 1.1678,
      "step": 42350
    },
    {
      "epoch": 2491.764705882353,
      "grad_norm": 22.696035385131836,
      "learning_rate": 2.5082352941176473e-05,
      "loss": 1.1893,
      "step": 42360
    },
    {
      "epoch": 2492.3529411764707,
      "grad_norm": 18.188261032104492,
      "learning_rate": 2.5076470588235296e-05,
      "loss": 1.1066,
      "step": 42370
    },
    {
      "epoch": 2492.9411764705883,
      "grad_norm": 19.69953155517578,
      "learning_rate": 2.507058823529412e-05,
      "loss": 1.167,
      "step": 42380
    },
    {
      "epoch": 2493.529411764706,
      "grad_norm": 19.70699119567871,
      "learning_rate": 2.506470588235294e-05,
      "loss": 1.1605,
      "step": 42390
    },
    {
      "epoch": 2494.1176470588234,
      "grad_norm": 21.464017868041992,
      "learning_rate": 2.5058823529411768e-05,
      "loss": 1.0759,
      "step": 42400
    },
    {
      "epoch": 2494.705882352941,
      "grad_norm": 21.93265724182129,
      "learning_rate": 2.505294117647059e-05,
      "loss": 1.0909,
      "step": 42410
    },
    {
      "epoch": 2495.294117647059,
      "grad_norm": 21.612600326538086,
      "learning_rate": 2.5047058823529414e-05,
      "loss": 1.154,
      "step": 42420
    },
    {
      "epoch": 2495.8823529411766,
      "grad_norm": 16.538467407226562,
      "learning_rate": 2.5041176470588234e-05,
      "loss": 1.0643,
      "step": 42430
    },
    {
      "epoch": 2496.470588235294,
      "grad_norm": 19.59859848022461,
      "learning_rate": 2.5035294117647063e-05,
      "loss": 1.1673,
      "step": 42440
    },
    {
      "epoch": 2497.0588235294117,
      "grad_norm": 22.626989364624023,
      "learning_rate": 2.5029411764705883e-05,
      "loss": 1.2202,
      "step": 42450
    },
    {
      "epoch": 2497.6470588235293,
      "grad_norm": 20.6734561920166,
      "learning_rate": 2.5023529411764706e-05,
      "loss": 1.0226,
      "step": 42460
    },
    {
      "epoch": 2498.235294117647,
      "grad_norm": 17.946928024291992,
      "learning_rate": 2.501764705882353e-05,
      "loss": 1.1498,
      "step": 42470
    },
    {
      "epoch": 2498.823529411765,
      "grad_norm": 23.310977935791016,
      "learning_rate": 2.5011764705882355e-05,
      "loss": 1.0798,
      "step": 42480
    },
    {
      "epoch": 2499.4117647058824,
      "grad_norm": 20.713884353637695,
      "learning_rate": 2.5005882352941178e-05,
      "loss": 1.0977,
      "step": 42490
    },
    {
      "epoch": 2500.0,
      "grad_norm": 30.021133422851562,
      "learning_rate": 2.5e-05,
      "loss": 1.1537,
      "step": 42500
    },
    {
      "epoch": 2500.5882352941176,
      "grad_norm": 19.655059814453125,
      "learning_rate": 2.4994117647058824e-05,
      "loss": 1.084,
      "step": 42510
    },
    {
      "epoch": 2501.176470588235,
      "grad_norm": 14.21422290802002,
      "learning_rate": 2.4988235294117647e-05,
      "loss": 1.1711,
      "step": 42520
    },
    {
      "epoch": 2501.764705882353,
      "grad_norm": 17.81607437133789,
      "learning_rate": 2.498235294117647e-05,
      "loss": 1.0661,
      "step": 42530
    },
    {
      "epoch": 2502.3529411764707,
      "grad_norm": 25.00946044921875,
      "learning_rate": 2.4976470588235297e-05,
      "loss": 1.1201,
      "step": 42540
    },
    {
      "epoch": 2502.9411764705883,
      "grad_norm": 23.328588485717773,
      "learning_rate": 2.497058823529412e-05,
      "loss": 1.103,
      "step": 42550
    },
    {
      "epoch": 2503.529411764706,
      "grad_norm": 16.6757755279541,
      "learning_rate": 2.4964705882352943e-05,
      "loss": 1.1995,
      "step": 42560
    },
    {
      "epoch": 2504.1176470588234,
      "grad_norm": 23.977752685546875,
      "learning_rate": 2.4958823529411766e-05,
      "loss": 1.1108,
      "step": 42570
    },
    {
      "epoch": 2504.705882352941,
      "grad_norm": 19.10553741455078,
      "learning_rate": 2.495294117647059e-05,
      "loss": 1.1277,
      "step": 42580
    },
    {
      "epoch": 2505.294117647059,
      "grad_norm": 18.667261123657227,
      "learning_rate": 2.494705882352941e-05,
      "loss": 1.1746,
      "step": 42590
    },
    {
      "epoch": 2505.8823529411766,
      "grad_norm": 27.313676834106445,
      "learning_rate": 2.4941176470588238e-05,
      "loss": 1.1079,
      "step": 42600
    },
    {
      "epoch": 2506.470588235294,
      "grad_norm": 18.66080093383789,
      "learning_rate": 2.493529411764706e-05,
      "loss": 1.155,
      "step": 42610
    },
    {
      "epoch": 2507.0588235294117,
      "grad_norm": 17.359025955200195,
      "learning_rate": 2.4929411764705884e-05,
      "loss": 1.1358,
      "step": 42620
    },
    {
      "epoch": 2507.6470588235293,
      "grad_norm": 19.87110710144043,
      "learning_rate": 2.4923529411764707e-05,
      "loss": 1.1975,
      "step": 42630
    },
    {
      "epoch": 2508.235294117647,
      "grad_norm": 21.812068939208984,
      "learning_rate": 2.4917647058823533e-05,
      "loss": 1.1501,
      "step": 42640
    },
    {
      "epoch": 2508.823529411765,
      "grad_norm": 16.704345703125,
      "learning_rate": 2.4911764705882353e-05,
      "loss": 1.0426,
      "step": 42650
    },
    {
      "epoch": 2509.4117647058824,
      "grad_norm": 22.03044319152832,
      "learning_rate": 2.490588235294118e-05,
      "loss": 1.1434,
      "step": 42660
    },
    {
      "epoch": 2510.0,
      "grad_norm": 23.438709259033203,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 1.2197,
      "step": 42670
    },
    {
      "epoch": 2510.5882352941176,
      "grad_norm": 21.639301300048828,
      "learning_rate": 2.4894117647058825e-05,
      "loss": 0.988,
      "step": 42680
    },
    {
      "epoch": 2511.176470588235,
      "grad_norm": 16.519437789916992,
      "learning_rate": 2.4888235294117648e-05,
      "loss": 1.0776,
      "step": 42690
    },
    {
      "epoch": 2511.764705882353,
      "grad_norm": 17.603626251220703,
      "learning_rate": 2.488235294117647e-05,
      "loss": 1.1566,
      "step": 42700
    },
    {
      "epoch": 2512.3529411764707,
      "grad_norm": 20.128883361816406,
      "learning_rate": 2.4876470588235294e-05,
      "loss": 1.0652,
      "step": 42710
    },
    {
      "epoch": 2512.9411764705883,
      "grad_norm": 21.337852478027344,
      "learning_rate": 2.4870588235294117e-05,
      "loss": 1.1404,
      "step": 42720
    },
    {
      "epoch": 2513.529411764706,
      "grad_norm": 18.143388748168945,
      "learning_rate": 2.4864705882352943e-05,
      "loss": 1.1213,
      "step": 42730
    },
    {
      "epoch": 2514.1176470588234,
      "grad_norm": 20.08731460571289,
      "learning_rate": 2.4858823529411766e-05,
      "loss": 0.9742,
      "step": 42740
    },
    {
      "epoch": 2514.705882352941,
      "grad_norm": 16.3709716796875,
      "learning_rate": 2.485294117647059e-05,
      "loss": 1.1045,
      "step": 42750
    },
    {
      "epoch": 2515.294117647059,
      "grad_norm": 26.218414306640625,
      "learning_rate": 2.4847058823529412e-05,
      "loss": 1.1433,
      "step": 42760
    },
    {
      "epoch": 2515.8823529411766,
      "grad_norm": 21.54303550720215,
      "learning_rate": 2.484117647058824e-05,
      "loss": 1.0492,
      "step": 42770
    },
    {
      "epoch": 2516.470588235294,
      "grad_norm": 20.955123901367188,
      "learning_rate": 2.483529411764706e-05,
      "loss": 1.1642,
      "step": 42780
    },
    {
      "epoch": 2517.0588235294117,
      "grad_norm": 19.026596069335938,
      "learning_rate": 2.4829411764705885e-05,
      "loss": 1.1451,
      "step": 42790
    },
    {
      "epoch": 2517.6470588235293,
      "grad_norm": 14.21475601196289,
      "learning_rate": 2.4823529411764708e-05,
      "loss": 1.0926,
      "step": 42800
    },
    {
      "epoch": 2518.235294117647,
      "grad_norm": 21.882204055786133,
      "learning_rate": 2.481764705882353e-05,
      "loss": 1.0886,
      "step": 42810
    },
    {
      "epoch": 2518.823529411765,
      "grad_norm": 20.45183753967285,
      "learning_rate": 2.4811764705882354e-05,
      "loss": 1.0878,
      "step": 42820
    },
    {
      "epoch": 2519.4117647058824,
      "grad_norm": 24.533416748046875,
      "learning_rate": 2.480588235294118e-05,
      "loss": 1.149,
      "step": 42830
    },
    {
      "epoch": 2520.0,
      "grad_norm": 20.143156051635742,
      "learning_rate": 2.48e-05,
      "loss": 1.131,
      "step": 42840
    },
    {
      "epoch": 2520.5882352941176,
      "grad_norm": 16.89576530456543,
      "learning_rate": 2.4794117647058826e-05,
      "loss": 1.1354,
      "step": 42850
    },
    {
      "epoch": 2521.176470588235,
      "grad_norm": 22.90983772277832,
      "learning_rate": 2.478823529411765e-05,
      "loss": 1.1098,
      "step": 42860
    },
    {
      "epoch": 2521.764705882353,
      "grad_norm": 16.830734252929688,
      "learning_rate": 2.4782352941176472e-05,
      "loss": 1.0366,
      "step": 42870
    },
    {
      "epoch": 2522.3529411764707,
      "grad_norm": 17.095788955688477,
      "learning_rate": 2.4776470588235295e-05,
      "loss": 0.9874,
      "step": 42880
    },
    {
      "epoch": 2522.9411764705883,
      "grad_norm": 19.399538040161133,
      "learning_rate": 2.4770588235294118e-05,
      "loss": 1.0557,
      "step": 42890
    },
    {
      "epoch": 2523.529411764706,
      "grad_norm": 23.378368377685547,
      "learning_rate": 2.476470588235294e-05,
      "loss": 1.0784,
      "step": 42900
    },
    {
      "epoch": 2524.1176470588234,
      "grad_norm": 20.89545249938965,
      "learning_rate": 2.4758823529411764e-05,
      "loss": 1.1986,
      "step": 42910
    },
    {
      "epoch": 2524.705882352941,
      "grad_norm": 20.03392219543457,
      "learning_rate": 2.475294117647059e-05,
      "loss": 1.0006,
      "step": 42920
    },
    {
      "epoch": 2525.294117647059,
      "grad_norm": 17.45736312866211,
      "learning_rate": 2.4747058823529413e-05,
      "loss": 1.007,
      "step": 42930
    },
    {
      "epoch": 2525.8823529411766,
      "grad_norm": 20.964860916137695,
      "learning_rate": 2.4741176470588236e-05,
      "loss": 1.0839,
      "step": 42940
    },
    {
      "epoch": 2526.470588235294,
      "grad_norm": 24.54486656188965,
      "learning_rate": 2.473529411764706e-05,
      "loss": 1.1178,
      "step": 42950
    },
    {
      "epoch": 2527.0588235294117,
      "grad_norm": 15.225577354431152,
      "learning_rate": 2.4729411764705886e-05,
      "loss": 1.1066,
      "step": 42960
    },
    {
      "epoch": 2527.6470588235293,
      "grad_norm": 21.140363693237305,
      "learning_rate": 2.4723529411764705e-05,
      "loss": 1.104,
      "step": 42970
    },
    {
      "epoch": 2528.235294117647,
      "grad_norm": 20.119470596313477,
      "learning_rate": 2.4717647058823532e-05,
      "loss": 1.1535,
      "step": 42980
    },
    {
      "epoch": 2528.823529411765,
      "grad_norm": 23.01618766784668,
      "learning_rate": 2.4711764705882355e-05,
      "loss": 1.0095,
      "step": 42990
    },
    {
      "epoch": 2529.4117647058824,
      "grad_norm": 22.183530807495117,
      "learning_rate": 2.4705882352941178e-05,
      "loss": 1.1777,
      "step": 43000
    },
    {
      "epoch": 2530.0,
      "grad_norm": 21.269594192504883,
      "learning_rate": 2.47e-05,
      "loss": 1.1604,
      "step": 43010
    },
    {
      "epoch": 2530.5882352941176,
      "grad_norm": 21.296619415283203,
      "learning_rate": 2.4694117647058827e-05,
      "loss": 1.0631,
      "step": 43020
    },
    {
      "epoch": 2531.176470588235,
      "grad_norm": 16.56370735168457,
      "learning_rate": 2.4688235294117647e-05,
      "loss": 1.1176,
      "step": 43030
    },
    {
      "epoch": 2531.764705882353,
      "grad_norm": 16.6712646484375,
      "learning_rate": 2.4682352941176473e-05,
      "loss": 1.1884,
      "step": 43040
    },
    {
      "epoch": 2532.3529411764707,
      "grad_norm": 17.42215919494629,
      "learning_rate": 2.4676470588235296e-05,
      "loss": 1.118,
      "step": 43050
    },
    {
      "epoch": 2532.9411764705883,
      "grad_norm": 24.903963088989258,
      "learning_rate": 2.467058823529412e-05,
      "loss": 1.1762,
      "step": 43060
    },
    {
      "epoch": 2533.529411764706,
      "grad_norm": 23.048097610473633,
      "learning_rate": 2.4664705882352942e-05,
      "loss": 1.0809,
      "step": 43070
    },
    {
      "epoch": 2534.1176470588234,
      "grad_norm": 15.746665000915527,
      "learning_rate": 2.4658823529411765e-05,
      "loss": 1.0374,
      "step": 43080
    },
    {
      "epoch": 2534.705882352941,
      "grad_norm": 16.086620330810547,
      "learning_rate": 2.465294117647059e-05,
      "loss": 1.1975,
      "step": 43090
    },
    {
      "epoch": 2535.294117647059,
      "grad_norm": 22.561389923095703,
      "learning_rate": 2.464705882352941e-05,
      "loss": 1.1686,
      "step": 43100
    },
    {
      "epoch": 2535.8823529411766,
      "grad_norm": 17.150972366333008,
      "learning_rate": 2.4641176470588237e-05,
      "loss": 1.1238,
      "step": 43110
    },
    {
      "epoch": 2536.470588235294,
      "grad_norm": 20.62549591064453,
      "learning_rate": 2.463529411764706e-05,
      "loss": 1.0643,
      "step": 43120
    },
    {
      "epoch": 2537.0588235294117,
      "grad_norm": 17.198749542236328,
      "learning_rate": 2.4629411764705883e-05,
      "loss": 1.1616,
      "step": 43130
    },
    {
      "epoch": 2537.6470588235293,
      "grad_norm": 18.660627365112305,
      "learning_rate": 2.4623529411764706e-05,
      "loss": 1.1672,
      "step": 43140
    },
    {
      "epoch": 2538.235294117647,
      "grad_norm": 24.932979583740234,
      "learning_rate": 2.4617647058823533e-05,
      "loss": 1.077,
      "step": 43150
    },
    {
      "epoch": 2538.823529411765,
      "grad_norm": 22.40386962890625,
      "learning_rate": 2.4611764705882352e-05,
      "loss": 1.0163,
      "step": 43160
    },
    {
      "epoch": 2539.4117647058824,
      "grad_norm": 25.260234832763672,
      "learning_rate": 2.460588235294118e-05,
      "loss": 1.1935,
      "step": 43170
    },
    {
      "epoch": 2540.0,
      "grad_norm": 23.00027084350586,
      "learning_rate": 2.46e-05,
      "loss": 1.1711,
      "step": 43180
    },
    {
      "epoch": 2540.5882352941176,
      "grad_norm": 21.924848556518555,
      "learning_rate": 2.4594117647058825e-05,
      "loss": 1.1333,
      "step": 43190
    },
    {
      "epoch": 2541.176470588235,
      "grad_norm": 16.431398391723633,
      "learning_rate": 2.4588235294117648e-05,
      "loss": 1.0916,
      "step": 43200
    },
    {
      "epoch": 2541.764705882353,
      "grad_norm": 18.16413116455078,
      "learning_rate": 2.4582352941176474e-05,
      "loss": 1.127,
      "step": 43210
    },
    {
      "epoch": 2542.3529411764707,
      "grad_norm": 21.20318031311035,
      "learning_rate": 2.4576470588235294e-05,
      "loss": 1.0781,
      "step": 43220
    },
    {
      "epoch": 2542.9411764705883,
      "grad_norm": 22.123748779296875,
      "learning_rate": 2.4570588235294117e-05,
      "loss": 1.2164,
      "step": 43230
    },
    {
      "epoch": 2543.529411764706,
      "grad_norm": 22.07718276977539,
      "learning_rate": 2.4564705882352943e-05,
      "loss": 0.9804,
      "step": 43240
    },
    {
      "epoch": 2544.1176470588234,
      "grad_norm": 19.166593551635742,
      "learning_rate": 2.4558823529411766e-05,
      "loss": 1.0807,
      "step": 43250
    },
    {
      "epoch": 2544.705882352941,
      "grad_norm": 21.96879768371582,
      "learning_rate": 2.455294117647059e-05,
      "loss": 1.0521,
      "step": 43260
    },
    {
      "epoch": 2545.294117647059,
      "grad_norm": 13.228264808654785,
      "learning_rate": 2.4547058823529412e-05,
      "loss": 1.0846,
      "step": 43270
    },
    {
      "epoch": 2545.8823529411766,
      "grad_norm": 19.836280822753906,
      "learning_rate": 2.4541176470588238e-05,
      "loss": 1.161,
      "step": 43280
    },
    {
      "epoch": 2546.470588235294,
      "grad_norm": 16.398794174194336,
      "learning_rate": 2.4535294117647058e-05,
      "loss": 1.0305,
      "step": 43290
    },
    {
      "epoch": 2547.0588235294117,
      "grad_norm": 22.244243621826172,
      "learning_rate": 2.4529411764705884e-05,
      "loss": 1.0452,
      "step": 43300
    },
    {
      "epoch": 2547.6470588235293,
      "grad_norm": 16.390186309814453,
      "learning_rate": 2.4523529411764707e-05,
      "loss": 1.1618,
      "step": 43310
    },
    {
      "epoch": 2548.235294117647,
      "grad_norm": 19.630125045776367,
      "learning_rate": 2.451764705882353e-05,
      "loss": 1.0921,
      "step": 43320
    },
    {
      "epoch": 2548.823529411765,
      "grad_norm": 16.896461486816406,
      "learning_rate": 2.4511764705882353e-05,
      "loss": 1.1247,
      "step": 43330
    },
    {
      "epoch": 2549.4117647058824,
      "grad_norm": 18.111236572265625,
      "learning_rate": 2.450588235294118e-05,
      "loss": 1.0978,
      "step": 43340
    },
    {
      "epoch": 2550.0,
      "grad_norm": 17.581390380859375,
      "learning_rate": 2.45e-05,
      "loss": 1.008,
      "step": 43350
    },
    {
      "epoch": 2550.5882352941176,
      "grad_norm": 19.570287704467773,
      "learning_rate": 2.4494117647058826e-05,
      "loss": 1.1267,
      "step": 43360
    },
    {
      "epoch": 2551.176470588235,
      "grad_norm": 20.83100700378418,
      "learning_rate": 2.448823529411765e-05,
      "loss": 1.095,
      "step": 43370
    },
    {
      "epoch": 2551.764705882353,
      "grad_norm": 17.713794708251953,
      "learning_rate": 2.448235294117647e-05,
      "loss": 1.0506,
      "step": 43380
    },
    {
      "epoch": 2552.3529411764707,
      "grad_norm": 20.95648193359375,
      "learning_rate": 2.4476470588235295e-05,
      "loss": 1.0823,
      "step": 43390
    },
    {
      "epoch": 2552.9411764705883,
      "grad_norm": 18.905445098876953,
      "learning_rate": 2.447058823529412e-05,
      "loss": 1.1417,
      "step": 43400
    },
    {
      "epoch": 2553.529411764706,
      "grad_norm": 17.625612258911133,
      "learning_rate": 2.4464705882352944e-05,
      "loss": 1.1582,
      "step": 43410
    },
    {
      "epoch": 2554.1176470588234,
      "grad_norm": 24.1228084564209,
      "learning_rate": 2.4458823529411764e-05,
      "loss": 1.1716,
      "step": 43420
    },
    {
      "epoch": 2554.705882352941,
      "grad_norm": 26.69850730895996,
      "learning_rate": 2.445294117647059e-05,
      "loss": 1.0232,
      "step": 43430
    },
    {
      "epoch": 2555.294117647059,
      "grad_norm": 15.829621315002441,
      "learning_rate": 2.4447058823529413e-05,
      "loss": 1.1117,
      "step": 43440
    },
    {
      "epoch": 2555.8823529411766,
      "grad_norm": 18.279417037963867,
      "learning_rate": 2.4441176470588236e-05,
      "loss": 1.1608,
      "step": 43450
    },
    {
      "epoch": 2556.470588235294,
      "grad_norm": 21.800247192382812,
      "learning_rate": 2.443529411764706e-05,
      "loss": 1.2231,
      "step": 43460
    },
    {
      "epoch": 2557.0588235294117,
      "grad_norm": 20.04367446899414,
      "learning_rate": 2.4429411764705885e-05,
      "loss": 1.1142,
      "step": 43470
    },
    {
      "epoch": 2557.6470588235293,
      "grad_norm": 12.871097564697266,
      "learning_rate": 2.4423529411764705e-05,
      "loss": 1.0558,
      "step": 43480
    },
    {
      "epoch": 2558.235294117647,
      "grad_norm": 20.749866485595703,
      "learning_rate": 2.441764705882353e-05,
      "loss": 1.1718,
      "step": 43490
    },
    {
      "epoch": 2558.823529411765,
      "grad_norm": 19.148950576782227,
      "learning_rate": 2.4411764705882354e-05,
      "loss": 1.024,
      "step": 43500
    },
    {
      "epoch": 2559.4117647058824,
      "grad_norm": 17.360515594482422,
      "learning_rate": 2.4405882352941177e-05,
      "loss": 1.1595,
      "step": 43510
    },
    {
      "epoch": 2560.0,
      "grad_norm": 19.958240509033203,
      "learning_rate": 2.44e-05,
      "loss": 1.0126,
      "step": 43520
    },
    {
      "epoch": 2560.5882352941176,
      "grad_norm": 16.36404037475586,
      "learning_rate": 2.4394117647058827e-05,
      "loss": 1.1416,
      "step": 43530
    },
    {
      "epoch": 2561.176470588235,
      "grad_norm": 14.795148849487305,
      "learning_rate": 2.438823529411765e-05,
      "loss": 1.1699,
      "step": 43540
    },
    {
      "epoch": 2561.764705882353,
      "grad_norm": 19.507543563842773,
      "learning_rate": 2.4382352941176473e-05,
      "loss": 1.247,
      "step": 43550
    },
    {
      "epoch": 2562.3529411764707,
      "grad_norm": 25.42254066467285,
      "learning_rate": 2.4376470588235296e-05,
      "loss": 1.1661,
      "step": 43560
    },
    {
      "epoch": 2562.9411764705883,
      "grad_norm": 15.866501808166504,
      "learning_rate": 2.437058823529412e-05,
      "loss": 1.1367,
      "step": 43570
    },
    {
      "epoch": 2563.529411764706,
      "grad_norm": 23.580631256103516,
      "learning_rate": 2.436470588235294e-05,
      "loss": 1.2867,
      "step": 43580
    },
    {
      "epoch": 2564.1176470588234,
      "grad_norm": 19.146099090576172,
      "learning_rate": 2.4358823529411764e-05,
      "loss": 1.1274,
      "step": 43590
    },
    {
      "epoch": 2564.705882352941,
      "grad_norm": 14.847042083740234,
      "learning_rate": 2.435294117647059e-05,
      "loss": 1.1425,
      "step": 43600
    },
    {
      "epoch": 2565.294117647059,
      "grad_norm": 20.46787452697754,
      "learning_rate": 2.434705882352941e-05,
      "loss": 1.1177,
      "step": 43610
    },
    {
      "epoch": 2565.8823529411766,
      "grad_norm": 15.596915245056152,
      "learning_rate": 2.4341176470588237e-05,
      "loss": 1.2339,
      "step": 43620
    },
    {
      "epoch": 2566.470588235294,
      "grad_norm": 21.70233726501465,
      "learning_rate": 2.433529411764706e-05,
      "loss": 1.1129,
      "step": 43630
    },
    {
      "epoch": 2567.0588235294117,
      "grad_norm": 15.135071754455566,
      "learning_rate": 2.4329411764705883e-05,
      "loss": 1.0658,
      "step": 43640
    },
    {
      "epoch": 2567.6470588235293,
      "grad_norm": 22.23728370666504,
      "learning_rate": 2.4323529411764706e-05,
      "loss": 1.147,
      "step": 43650
    },
    {
      "epoch": 2568.235294117647,
      "grad_norm": 17.553831100463867,
      "learning_rate": 2.4317647058823532e-05,
      "loss": 0.9884,
      "step": 43660
    },
    {
      "epoch": 2568.823529411765,
      "grad_norm": 20.581350326538086,
      "learning_rate": 2.4311764705882352e-05,
      "loss": 1.1537,
      "step": 43670
    },
    {
      "epoch": 2569.4117647058824,
      "grad_norm": 21.533973693847656,
      "learning_rate": 2.4305882352941178e-05,
      "loss": 1.2089,
      "step": 43680
    },
    {
      "epoch": 2570.0,
      "grad_norm": 18.14970588684082,
      "learning_rate": 2.43e-05,
      "loss": 1.1379,
      "step": 43690
    },
    {
      "epoch": 2570.5882352941176,
      "grad_norm": 21.712749481201172,
      "learning_rate": 2.4294117647058824e-05,
      "loss": 1.2011,
      "step": 43700
    },
    {
      "epoch": 2571.176470588235,
      "grad_norm": 19.31267738342285,
      "learning_rate": 2.4288235294117647e-05,
      "loss": 1.0712,
      "step": 43710
    },
    {
      "epoch": 2571.764705882353,
      "grad_norm": 19.11757469177246,
      "learning_rate": 2.4282352941176473e-05,
      "loss": 1.005,
      "step": 43720
    },
    {
      "epoch": 2572.3529411764707,
      "grad_norm": 18.549209594726562,
      "learning_rate": 2.4276470588235296e-05,
      "loss": 1.0646,
      "step": 43730
    },
    {
      "epoch": 2572.9411764705883,
      "grad_norm": 16.69318389892578,
      "learning_rate": 2.427058823529412e-05,
      "loss": 1.0494,
      "step": 43740
    },
    {
      "epoch": 2573.529411764706,
      "grad_norm": 25.072227478027344,
      "learning_rate": 2.4264705882352942e-05,
      "loss": 1.0803,
      "step": 43750
    },
    {
      "epoch": 2574.1176470588234,
      "grad_norm": 20.420591354370117,
      "learning_rate": 2.425882352941177e-05,
      "loss": 1.2001,
      "step": 43760
    },
    {
      "epoch": 2574.705882352941,
      "grad_norm": 18.05022621154785,
      "learning_rate": 2.425294117647059e-05,
      "loss": 1.0923,
      "step": 43770
    },
    {
      "epoch": 2575.294117647059,
      "grad_norm": 20.623891830444336,
      "learning_rate": 2.424705882352941e-05,
      "loss": 1.1231,
      "step": 43780
    },
    {
      "epoch": 2575.8823529411766,
      "grad_norm": 26.88809585571289,
      "learning_rate": 2.4241176470588238e-05,
      "loss": 1.218,
      "step": 43790
    },
    {
      "epoch": 2576.470588235294,
      "grad_norm": 15.708630561828613,
      "learning_rate": 2.4235294117647057e-05,
      "loss": 1.0173,
      "step": 43800
    },
    {
      "epoch": 2577.0588235294117,
      "grad_norm": 16.3330020904541,
      "learning_rate": 2.4229411764705884e-05,
      "loss": 1.122,
      "step": 43810
    },
    {
      "epoch": 2577.6470588235293,
      "grad_norm": 19.322628021240234,
      "learning_rate": 2.4223529411764707e-05,
      "loss": 1.0976,
      "step": 43820
    },
    {
      "epoch": 2578.235294117647,
      "grad_norm": 20.244083404541016,
      "learning_rate": 2.421764705882353e-05,
      "loss": 1.1422,
      "step": 43830
    },
    {
      "epoch": 2578.823529411765,
      "grad_norm": 20.399301528930664,
      "learning_rate": 2.4211764705882353e-05,
      "loss": 1.0427,
      "step": 43840
    },
    {
      "epoch": 2579.4117647058824,
      "grad_norm": 18.71479606628418,
      "learning_rate": 2.420588235294118e-05,
      "loss": 1.0184,
      "step": 43850
    },
    {
      "epoch": 2580.0,
      "grad_norm": 22.567651748657227,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 1.1896,
      "step": 43860
    },
    {
      "epoch": 2580.5882352941176,
      "grad_norm": 19.1320858001709,
      "learning_rate": 2.4194117647058825e-05,
      "loss": 1.0968,
      "step": 43870
    },
    {
      "epoch": 2581.176470588235,
      "grad_norm": 20.78367805480957,
      "learning_rate": 2.4188235294117648e-05,
      "loss": 1.0883,
      "step": 43880
    },
    {
      "epoch": 2581.764705882353,
      "grad_norm": 21.686063766479492,
      "learning_rate": 2.418235294117647e-05,
      "loss": 1.0328,
      "step": 43890
    },
    {
      "epoch": 2582.3529411764707,
      "grad_norm": 22.575307846069336,
      "learning_rate": 2.4176470588235294e-05,
      "loss": 1.1567,
      "step": 43900
    },
    {
      "epoch": 2582.9411764705883,
      "grad_norm": 22.40323829650879,
      "learning_rate": 2.417058823529412e-05,
      "loss": 1.2092,
      "step": 43910
    },
    {
      "epoch": 2583.529411764706,
      "grad_norm": 20.329147338867188,
      "learning_rate": 2.4164705882352943e-05,
      "loss": 1.1302,
      "step": 43920
    },
    {
      "epoch": 2584.1176470588234,
      "grad_norm": 17.935890197753906,
      "learning_rate": 2.4158823529411766e-05,
      "loss": 1.1194,
      "step": 43930
    },
    {
      "epoch": 2584.705882352941,
      "grad_norm": 19.650405883789062,
      "learning_rate": 2.415294117647059e-05,
      "loss": 1.0704,
      "step": 43940
    },
    {
      "epoch": 2585.294117647059,
      "grad_norm": 22.15087127685547,
      "learning_rate": 2.4147058823529416e-05,
      "loss": 1.0756,
      "step": 43950
    },
    {
      "epoch": 2585.8823529411766,
      "grad_norm": 20.856290817260742,
      "learning_rate": 2.4141176470588235e-05,
      "loss": 1.1056,
      "step": 43960
    },
    {
      "epoch": 2586.470588235294,
      "grad_norm": 16.579906463623047,
      "learning_rate": 2.413529411764706e-05,
      "loss": 1.0192,
      "step": 43970
    },
    {
      "epoch": 2587.0588235294117,
      "grad_norm": 18.27598762512207,
      "learning_rate": 2.4129411764705885e-05,
      "loss": 1.1442,
      "step": 43980
    },
    {
      "epoch": 2587.6470588235293,
      "grad_norm": 18.623802185058594,
      "learning_rate": 2.4123529411764704e-05,
      "loss": 1.075,
      "step": 43990
    },
    {
      "epoch": 2588.235294117647,
      "grad_norm": 21.817901611328125,
      "learning_rate": 2.411764705882353e-05,
      "loss": 1.0679,
      "step": 44000
    },
    {
      "epoch": 2588.823529411765,
      "grad_norm": 22.28530502319336,
      "learning_rate": 2.4111764705882354e-05,
      "loss": 1.0277,
      "step": 44010
    },
    {
      "epoch": 2589.4117647058824,
      "grad_norm": 20.081838607788086,
      "learning_rate": 2.4105882352941177e-05,
      "loss": 1.0853,
      "step": 44020
    },
    {
      "epoch": 2590.0,
      "grad_norm": 26.286483764648438,
      "learning_rate": 2.41e-05,
      "loss": 1.0832,
      "step": 44030
    },
    {
      "epoch": 2590.5882352941176,
      "grad_norm": 23.405672073364258,
      "learning_rate": 2.4094117647058826e-05,
      "loss": 1.1239,
      "step": 44040
    },
    {
      "epoch": 2591.176470588235,
      "grad_norm": 20.54500961303711,
      "learning_rate": 2.408823529411765e-05,
      "loss": 1.0641,
      "step": 44050
    },
    {
      "epoch": 2591.764705882353,
      "grad_norm": 23.145172119140625,
      "learning_rate": 2.4082352941176472e-05,
      "loss": 1.0511,
      "step": 44060
    },
    {
      "epoch": 2592.3529411764707,
      "grad_norm": 19.233409881591797,
      "learning_rate": 2.4076470588235295e-05,
      "loss": 1.0902,
      "step": 44070
    },
    {
      "epoch": 2592.9411764705883,
      "grad_norm": 16.612274169921875,
      "learning_rate": 2.407058823529412e-05,
      "loss": 1.0334,
      "step": 44080
    },
    {
      "epoch": 2593.529411764706,
      "grad_norm": 14.994044303894043,
      "learning_rate": 2.406470588235294e-05,
      "loss": 1.031,
      "step": 44090
    },
    {
      "epoch": 2594.1176470588234,
      "grad_norm": 18.762739181518555,
      "learning_rate": 2.4058823529411767e-05,
      "loss": 1.0085,
      "step": 44100
    },
    {
      "epoch": 2594.705882352941,
      "grad_norm": 22.577407836914062,
      "learning_rate": 2.405294117647059e-05,
      "loss": 1.1058,
      "step": 44110
    },
    {
      "epoch": 2595.294117647059,
      "grad_norm": 20.74305534362793,
      "learning_rate": 2.4047058823529413e-05,
      "loss": 1.2154,
      "step": 44120
    },
    {
      "epoch": 2595.8823529411766,
      "grad_norm": 11.754532814025879,
      "learning_rate": 2.4041176470588236e-05,
      "loss": 1.1076,
      "step": 44130
    },
    {
      "epoch": 2596.470588235294,
      "grad_norm": 21.415369033813477,
      "learning_rate": 2.403529411764706e-05,
      "loss": 1.098,
      "step": 44140
    },
    {
      "epoch": 2597.0588235294117,
      "grad_norm": 20.007431030273438,
      "learning_rate": 2.4029411764705882e-05,
      "loss": 1.0535,
      "step": 44150
    },
    {
      "epoch": 2597.6470588235293,
      "grad_norm": 26.065298080444336,
      "learning_rate": 2.4023529411764705e-05,
      "loss": 1.0688,
      "step": 44160
    },
    {
      "epoch": 2598.235294117647,
      "grad_norm": 16.980669021606445,
      "learning_rate": 2.401764705882353e-05,
      "loss": 1.1901,
      "step": 44170
    },
    {
      "epoch": 2598.823529411765,
      "grad_norm": 22.26192283630371,
      "learning_rate": 2.4011764705882355e-05,
      "loss": 1.0197,
      "step": 44180
    },
    {
      "epoch": 2599.4117647058824,
      "grad_norm": 22.03184700012207,
      "learning_rate": 2.4005882352941178e-05,
      "loss": 1.169,
      "step": 44190
    },
    {
      "epoch": 2600.0,
      "grad_norm": 25.20973014831543,
      "learning_rate": 2.4e-05,
      "loss": 1.0459,
      "step": 44200
    },
    {
      "epoch": 2600.5882352941176,
      "grad_norm": 15.626249313354492,
      "learning_rate": 2.3994117647058824e-05,
      "loss": 1.132,
      "step": 44210
    },
    {
      "epoch": 2601.176470588235,
      "grad_norm": 18.090314865112305,
      "learning_rate": 2.3988235294117647e-05,
      "loss": 1.0749,
      "step": 44220
    },
    {
      "epoch": 2601.764705882353,
      "grad_norm": 17.87357521057129,
      "learning_rate": 2.3982352941176473e-05,
      "loss": 1.2299,
      "step": 44230
    },
    {
      "epoch": 2602.3529411764707,
      "grad_norm": 28.90808868408203,
      "learning_rate": 2.3976470588235296e-05,
      "loss": 1.1859,
      "step": 44240
    },
    {
      "epoch": 2602.9411764705883,
      "grad_norm": 27.247371673583984,
      "learning_rate": 2.397058823529412e-05,
      "loss": 1.0663,
      "step": 44250
    },
    {
      "epoch": 2603.529411764706,
      "grad_norm": 18.73451042175293,
      "learning_rate": 2.3964705882352942e-05,
      "loss": 1.1097,
      "step": 44260
    },
    {
      "epoch": 2604.1176470588234,
      "grad_norm": 17.199337005615234,
      "learning_rate": 2.3958823529411768e-05,
      "loss": 1.1,
      "step": 44270
    },
    {
      "epoch": 2604.705882352941,
      "grad_norm": 19.63965606689453,
      "learning_rate": 2.3952941176470588e-05,
      "loss": 1.103,
      "step": 44280
    },
    {
      "epoch": 2605.294117647059,
      "grad_norm": 17.049123764038086,
      "learning_rate": 2.3947058823529414e-05,
      "loss": 1.0407,
      "step": 44290
    },
    {
      "epoch": 2605.8823529411766,
      "grad_norm": 19.289051055908203,
      "learning_rate": 2.3941176470588237e-05,
      "loss": 1.0475,
      "step": 44300
    },
    {
      "epoch": 2606.470588235294,
      "grad_norm": 22.21535301208496,
      "learning_rate": 2.393529411764706e-05,
      "loss": 1.0524,
      "step": 44310
    },
    {
      "epoch": 2607.0588235294117,
      "grad_norm": 16.871551513671875,
      "learning_rate": 2.3929411764705883e-05,
      "loss": 0.8981,
      "step": 44320
    },
    {
      "epoch": 2607.6470588235293,
      "grad_norm": 20.895143508911133,
      "learning_rate": 2.3923529411764706e-05,
      "loss": 1.1728,
      "step": 44330
    },
    {
      "epoch": 2608.235294117647,
      "grad_norm": 18.413576126098633,
      "learning_rate": 2.391764705882353e-05,
      "loss": 1.2326,
      "step": 44340
    },
    {
      "epoch": 2608.823529411765,
      "grad_norm": 22.842994689941406,
      "learning_rate": 2.3911764705882352e-05,
      "loss": 1.0631,
      "step": 44350
    },
    {
      "epoch": 2609.4117647058824,
      "grad_norm": 20.16722869873047,
      "learning_rate": 2.390588235294118e-05,
      "loss": 1.0685,
      "step": 44360
    },
    {
      "epoch": 2610.0,
      "grad_norm": 32.61635971069336,
      "learning_rate": 2.39e-05,
      "loss": 1.0661,
      "step": 44370
    },
    {
      "epoch": 2610.5882352941176,
      "grad_norm": 25.080427169799805,
      "learning_rate": 2.3894117647058825e-05,
      "loss": 1.0411,
      "step": 44380
    },
    {
      "epoch": 2611.176470588235,
      "grad_norm": 19.11369514465332,
      "learning_rate": 2.3888235294117648e-05,
      "loss": 1.1873,
      "step": 44390
    },
    {
      "epoch": 2611.764705882353,
      "grad_norm": 21.896329879760742,
      "learning_rate": 2.3882352941176474e-05,
      "loss": 0.9225,
      "step": 44400
    },
    {
      "epoch": 2612.3529411764707,
      "grad_norm": 20.912504196166992,
      "learning_rate": 2.3876470588235294e-05,
      "loss": 1.1449,
      "step": 44410
    },
    {
      "epoch": 2612.9411764705883,
      "grad_norm": 20.676095962524414,
      "learning_rate": 2.387058823529412e-05,
      "loss": 1.1388,
      "step": 44420
    },
    {
      "epoch": 2613.529411764706,
      "grad_norm": 22.364215850830078,
      "learning_rate": 2.3864705882352943e-05,
      "loss": 1.1589,
      "step": 44430
    },
    {
      "epoch": 2614.1176470588234,
      "grad_norm": 24.379995346069336,
      "learning_rate": 2.3858823529411766e-05,
      "loss": 1.1232,
      "step": 44440
    },
    {
      "epoch": 2614.705882352941,
      "grad_norm": 24.54593849182129,
      "learning_rate": 2.385294117647059e-05,
      "loss": 1.0651,
      "step": 44450
    },
    {
      "epoch": 2615.294117647059,
      "grad_norm": 21.30743980407715,
      "learning_rate": 2.3847058823529415e-05,
      "loss": 1.1132,
      "step": 44460
    },
    {
      "epoch": 2615.8823529411766,
      "grad_norm": 22.57996368408203,
      "learning_rate": 2.3841176470588235e-05,
      "loss": 1.1273,
      "step": 44470
    },
    {
      "epoch": 2616.470588235294,
      "grad_norm": 21.578550338745117,
      "learning_rate": 2.383529411764706e-05,
      "loss": 1.171,
      "step": 44480
    },
    {
      "epoch": 2617.0588235294117,
      "grad_norm": 24.955293655395508,
      "learning_rate": 2.3829411764705884e-05,
      "loss": 1.1318,
      "step": 44490
    },
    {
      "epoch": 2617.6470588235293,
      "grad_norm": 23.549654006958008,
      "learning_rate": 2.3823529411764707e-05,
      "loss": 1.0999,
      "step": 44500
    },
    {
      "epoch": 2618.235294117647,
      "grad_norm": 23.840192794799805,
      "learning_rate": 2.381764705882353e-05,
      "loss": 1.0825,
      "step": 44510
    },
    {
      "epoch": 2618.823529411765,
      "grad_norm": 17.53550148010254,
      "learning_rate": 2.3811764705882353e-05,
      "loss": 1.062,
      "step": 44520
    },
    {
      "epoch": 2619.4117647058824,
      "grad_norm": 17.07421112060547,
      "learning_rate": 2.380588235294118e-05,
      "loss": 0.9848,
      "step": 44530
    },
    {
      "epoch": 2620.0,
      "grad_norm": 21.37373161315918,
      "learning_rate": 2.38e-05,
      "loss": 1.1352,
      "step": 44540
    },
    {
      "epoch": 2620.5882352941176,
      "grad_norm": 17.701807022094727,
      "learning_rate": 2.3794117647058826e-05,
      "loss": 0.9709,
      "step": 44550
    },
    {
      "epoch": 2621.176470588235,
      "grad_norm": 24.998001098632812,
      "learning_rate": 2.378823529411765e-05,
      "loss": 1.1708,
      "step": 44560
    },
    {
      "epoch": 2621.764705882353,
      "grad_norm": 22.713298797607422,
      "learning_rate": 2.378235294117647e-05,
      "loss": 1.0748,
      "step": 44570
    },
    {
      "epoch": 2622.3529411764707,
      "grad_norm": 18.397071838378906,
      "learning_rate": 2.3776470588235294e-05,
      "loss": 1.2376,
      "step": 44580
    },
    {
      "epoch": 2622.9411764705883,
      "grad_norm": 16.998939514160156,
      "learning_rate": 2.377058823529412e-05,
      "loss": 1.0769,
      "step": 44590
    },
    {
      "epoch": 2623.529411764706,
      "grad_norm": 22.886146545410156,
      "learning_rate": 2.376470588235294e-05,
      "loss": 1.0705,
      "step": 44600
    },
    {
      "epoch": 2624.1176470588234,
      "grad_norm": 12.562962532043457,
      "learning_rate": 2.3758823529411767e-05,
      "loss": 1.0472,
      "step": 44610
    },
    {
      "epoch": 2624.705882352941,
      "grad_norm": 17.176237106323242,
      "learning_rate": 2.375294117647059e-05,
      "loss": 0.9413,
      "step": 44620
    },
    {
      "epoch": 2625.294117647059,
      "grad_norm": 17.520498275756836,
      "learning_rate": 2.3747058823529413e-05,
      "loss": 1.0723,
      "step": 44630
    },
    {
      "epoch": 2625.8823529411766,
      "grad_norm": 20.344772338867188,
      "learning_rate": 2.3741176470588236e-05,
      "loss": 1.1299,
      "step": 44640
    },
    {
      "epoch": 2626.470588235294,
      "grad_norm": 29.831594467163086,
      "learning_rate": 2.3735294117647062e-05,
      "loss": 1.1193,
      "step": 44650
    },
    {
      "epoch": 2627.0588235294117,
      "grad_norm": 18.0043888092041,
      "learning_rate": 2.3729411764705882e-05,
      "loss": 1.0449,
      "step": 44660
    },
    {
      "epoch": 2627.6470588235293,
      "grad_norm": 20.852468490600586,
      "learning_rate": 2.3723529411764708e-05,
      "loss": 0.8826,
      "step": 44670
    },
    {
      "epoch": 2628.235294117647,
      "grad_norm": 15.945155143737793,
      "learning_rate": 2.371764705882353e-05,
      "loss": 1.1045,
      "step": 44680
    },
    {
      "epoch": 2628.823529411765,
      "grad_norm": 15.847533226013184,
      "learning_rate": 2.3711764705882354e-05,
      "loss": 1.089,
      "step": 44690
    },
    {
      "epoch": 2629.4117647058824,
      "grad_norm": 19.88983726501465,
      "learning_rate": 2.3705882352941177e-05,
      "loss": 1.0412,
      "step": 44700
    },
    {
      "epoch": 2630.0,
      "grad_norm": 21.115585327148438,
      "learning_rate": 2.37e-05,
      "loss": 1.0831,
      "step": 44710
    },
    {
      "epoch": 2630.5882352941176,
      "grad_norm": 20.012155532836914,
      "learning_rate": 2.3694117647058826e-05,
      "loss": 1.1123,
      "step": 44720
    },
    {
      "epoch": 2631.176470588235,
      "grad_norm": 19.29732322692871,
      "learning_rate": 2.3688235294117646e-05,
      "loss": 1.0885,
      "step": 44730
    },
    {
      "epoch": 2631.764705882353,
      "grad_norm": 17.20387077331543,
      "learning_rate": 2.3682352941176472e-05,
      "loss": 1.0387,
      "step": 44740
    },
    {
      "epoch": 2632.3529411764707,
      "grad_norm": 20.173933029174805,
      "learning_rate": 2.3676470588235295e-05,
      "loss": 1.2134,
      "step": 44750
    },
    {
      "epoch": 2632.9411764705883,
      "grad_norm": 17.572879791259766,
      "learning_rate": 2.367058823529412e-05,
      "loss": 0.9532,
      "step": 44760
    },
    {
      "epoch": 2633.529411764706,
      "grad_norm": 19.960159301757812,
      "learning_rate": 2.366470588235294e-05,
      "loss": 1.0449,
      "step": 44770
    },
    {
      "epoch": 2634.1176470588234,
      "grad_norm": 26.223657608032227,
      "learning_rate": 2.3658823529411768e-05,
      "loss": 1.1797,
      "step": 44780
    },
    {
      "epoch": 2634.705882352941,
      "grad_norm": 20.336719512939453,
      "learning_rate": 2.3652941176470587e-05,
      "loss": 1.1199,
      "step": 44790
    },
    {
      "epoch": 2635.294117647059,
      "grad_norm": 18.677886962890625,
      "learning_rate": 2.3647058823529414e-05,
      "loss": 1.0384,
      "step": 44800
    },
    {
      "epoch": 2635.8823529411766,
      "grad_norm": 23.041765213012695,
      "learning_rate": 2.3641176470588237e-05,
      "loss": 1.0592,
      "step": 44810
    },
    {
      "epoch": 2636.470588235294,
      "grad_norm": 21.972166061401367,
      "learning_rate": 2.363529411764706e-05,
      "loss": 1.1027,
      "step": 44820
    },
    {
      "epoch": 2637.0588235294117,
      "grad_norm": 18.40620994567871,
      "learning_rate": 2.3629411764705883e-05,
      "loss": 1.0358,
      "step": 44830
    },
    {
      "epoch": 2637.6470588235293,
      "grad_norm": 21.97821617126465,
      "learning_rate": 2.362352941176471e-05,
      "loss": 1.1304,
      "step": 44840
    },
    {
      "epoch": 2638.235294117647,
      "grad_norm": 16.47743797302246,
      "learning_rate": 2.3617647058823532e-05,
      "loss": 0.9289,
      "step": 44850
    },
    {
      "epoch": 2638.823529411765,
      "grad_norm": 23.967634201049805,
      "learning_rate": 2.3611764705882355e-05,
      "loss": 1.2055,
      "step": 44860
    },
    {
      "epoch": 2639.4117647058824,
      "grad_norm": 19.63001823425293,
      "learning_rate": 2.3605882352941178e-05,
      "loss": 1.0562,
      "step": 44870
    },
    {
      "epoch": 2640.0,
      "grad_norm": 21.74704360961914,
      "learning_rate": 2.36e-05,
      "loss": 1.0179,
      "step": 44880
    },
    {
      "epoch": 2640.5882352941176,
      "grad_norm": 19.842771530151367,
      "learning_rate": 2.3594117647058824e-05,
      "loss": 1.0056,
      "step": 44890
    },
    {
      "epoch": 2641.176470588235,
      "grad_norm": 19.249887466430664,
      "learning_rate": 2.3588235294117647e-05,
      "loss": 1.149,
      "step": 44900
    },
    {
      "epoch": 2641.764705882353,
      "grad_norm": 21.514535903930664,
      "learning_rate": 2.3582352941176473e-05,
      "loss": 1.1518,
      "step": 44910
    },
    {
      "epoch": 2642.3529411764707,
      "grad_norm": 21.370819091796875,
      "learning_rate": 2.3576470588235293e-05,
      "loss": 1.0839,
      "step": 44920
    },
    {
      "epoch": 2642.9411764705883,
      "grad_norm": 20.1236572265625,
      "learning_rate": 2.357058823529412e-05,
      "loss": 1.0961,
      "step": 44930
    },
    {
      "epoch": 2643.529411764706,
      "grad_norm": 19.090490341186523,
      "learning_rate": 2.3564705882352942e-05,
      "loss": 1.087,
      "step": 44940
    },
    {
      "epoch": 2644.1176470588234,
      "grad_norm": 17.48206901550293,
      "learning_rate": 2.3558823529411765e-05,
      "loss": 1.0587,
      "step": 44950
    },
    {
      "epoch": 2644.705882352941,
      "grad_norm": 15.71345329284668,
      "learning_rate": 2.355294117647059e-05,
      "loss": 1.0148,
      "step": 44960
    },
    {
      "epoch": 2645.294117647059,
      "grad_norm": 22.48859214782715,
      "learning_rate": 2.3547058823529415e-05,
      "loss": 1.1513,
      "step": 44970
    },
    {
      "epoch": 2645.8823529411766,
      "grad_norm": 14.195734977722168,
      "learning_rate": 2.3541176470588234e-05,
      "loss": 1.0429,
      "step": 44980
    },
    {
      "epoch": 2646.470588235294,
      "grad_norm": 15.946599960327148,
      "learning_rate": 2.353529411764706e-05,
      "loss": 0.9656,
      "step": 44990
    },
    {
      "epoch": 2647.0588235294117,
      "grad_norm": 20.532163619995117,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 1.1172,
      "step": 45000
    },
    {
      "epoch": 2647.6470588235293,
      "grad_norm": 21.021087646484375,
      "learning_rate": 2.3523529411764707e-05,
      "loss": 1.1172,
      "step": 45010
    },
    {
      "epoch": 2648.235294117647,
      "grad_norm": 16.66372299194336,
      "learning_rate": 2.351764705882353e-05,
      "loss": 1.0857,
      "step": 45020
    },
    {
      "epoch": 2648.823529411765,
      "grad_norm": 19.501434326171875,
      "learning_rate": 2.3511764705882356e-05,
      "loss": 1.0067,
      "step": 45030
    },
    {
      "epoch": 2649.4117647058824,
      "grad_norm": 24.946489334106445,
      "learning_rate": 2.350588235294118e-05,
      "loss": 0.9878,
      "step": 45040
    },
    {
      "epoch": 2650.0,
      "grad_norm": 25.66925811767578,
      "learning_rate": 2.35e-05,
      "loss": 1.0008,
      "step": 45050
    },
    {
      "epoch": 2650.5882352941176,
      "grad_norm": 23.471111297607422,
      "learning_rate": 2.3494117647058825e-05,
      "loss": 1.1263,
      "step": 45060
    },
    {
      "epoch": 2651.176470588235,
      "grad_norm": 20.36573600769043,
      "learning_rate": 2.3488235294117648e-05,
      "loss": 1.074,
      "step": 45070
    },
    {
      "epoch": 2651.764705882353,
      "grad_norm": 27.470224380493164,
      "learning_rate": 2.348235294117647e-05,
      "loss": 1.0292,
      "step": 45080
    },
    {
      "epoch": 2652.3529411764707,
      "grad_norm": 20.72762107849121,
      "learning_rate": 2.3476470588235294e-05,
      "loss": 1.0602,
      "step": 45090
    },
    {
      "epoch": 2652.9411764705883,
      "grad_norm": 18.949621200561523,
      "learning_rate": 2.347058823529412e-05,
      "loss": 1.0659,
      "step": 45100
    },
    {
      "epoch": 2653.529411764706,
      "grad_norm": 20.02589225769043,
      "learning_rate": 2.346470588235294e-05,
      "loss": 1.0398,
      "step": 45110
    },
    {
      "epoch": 2654.1176470588234,
      "grad_norm": 13.175445556640625,
      "learning_rate": 2.3458823529411766e-05,
      "loss": 1.0366,
      "step": 45120
    },
    {
      "epoch": 2654.705882352941,
      "grad_norm": 21.35068702697754,
      "learning_rate": 2.345294117647059e-05,
      "loss": 0.9633,
      "step": 45130
    },
    {
      "epoch": 2655.294117647059,
      "grad_norm": 19.909046173095703,
      "learning_rate": 2.3447058823529412e-05,
      "loss": 0.976,
      "step": 45140
    },
    {
      "epoch": 2655.8823529411766,
      "grad_norm": 23.102819442749023,
      "learning_rate": 2.3441176470588235e-05,
      "loss": 1.0169,
      "step": 45150
    },
    {
      "epoch": 2656.470588235294,
      "grad_norm": 21.077775955200195,
      "learning_rate": 2.343529411764706e-05,
      "loss": 1.0702,
      "step": 45160
    },
    {
      "epoch": 2657.0588235294117,
      "grad_norm": 25.98876190185547,
      "learning_rate": 2.3429411764705885e-05,
      "loss": 1.0726,
      "step": 45170
    },
    {
      "epoch": 2657.6470588235293,
      "grad_norm": 21.977033615112305,
      "learning_rate": 2.3423529411764708e-05,
      "loss": 1.0514,
      "step": 45180
    },
    {
      "epoch": 2658.235294117647,
      "grad_norm": 22.43962287902832,
      "learning_rate": 2.341764705882353e-05,
      "loss": 0.9174,
      "step": 45190
    },
    {
      "epoch": 2658.823529411765,
      "grad_norm": 19.716379165649414,
      "learning_rate": 2.3411764705882354e-05,
      "loss": 1.0151,
      "step": 45200
    },
    {
      "epoch": 2659.4117647058824,
      "grad_norm": 18.347810745239258,
      "learning_rate": 2.3405882352941177e-05,
      "loss": 1.036,
      "step": 45210
    },
    {
      "epoch": 2660.0,
      "grad_norm": 25.56983184814453,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 1.1004,
      "step": 45220
    },
    {
      "epoch": 2660.5882352941176,
      "grad_norm": 21.43691635131836,
      "learning_rate": 2.3394117647058826e-05,
      "loss": 1.0202,
      "step": 45230
    },
    {
      "epoch": 2661.176470588235,
      "grad_norm": 19.405662536621094,
      "learning_rate": 2.3388235294117646e-05,
      "loss": 1.032,
      "step": 45240
    },
    {
      "epoch": 2661.764705882353,
      "grad_norm": 19.566099166870117,
      "learning_rate": 2.3382352941176472e-05,
      "loss": 1.0226,
      "step": 45250
    },
    {
      "epoch": 2662.3529411764707,
      "grad_norm": 18.67970085144043,
      "learning_rate": 2.3376470588235295e-05,
      "loss": 1.0387,
      "step": 45260
    },
    {
      "epoch": 2662.9411764705883,
      "grad_norm": 22.453725814819336,
      "learning_rate": 2.3370588235294118e-05,
      "loss": 1.1057,
      "step": 45270
    },
    {
      "epoch": 2663.529411764706,
      "grad_norm": 20.049394607543945,
      "learning_rate": 2.336470588235294e-05,
      "loss": 1.1294,
      "step": 45280
    },
    {
      "epoch": 2664.1176470588234,
      "grad_norm": 21.289127349853516,
      "learning_rate": 2.3358823529411767e-05,
      "loss": 1.122,
      "step": 45290
    },
    {
      "epoch": 2664.705882352941,
      "grad_norm": 24.023221969604492,
      "learning_rate": 2.3352941176470587e-05,
      "loss": 0.9259,
      "step": 45300
    },
    {
      "epoch": 2665.294117647059,
      "grad_norm": 22.648765563964844,
      "learning_rate": 2.3347058823529413e-05,
      "loss": 1.0097,
      "step": 45310
    },
    {
      "epoch": 2665.8823529411766,
      "grad_norm": 22.634471893310547,
      "learning_rate": 2.3341176470588236e-05,
      "loss": 0.9938,
      "step": 45320
    },
    {
      "epoch": 2666.470588235294,
      "grad_norm": 18.916135787963867,
      "learning_rate": 2.333529411764706e-05,
      "loss": 1.1708,
      "step": 45330
    },
    {
      "epoch": 2667.0588235294117,
      "grad_norm": 24.93132972717285,
      "learning_rate": 2.3329411764705882e-05,
      "loss": 1.049,
      "step": 45340
    },
    {
      "epoch": 2667.6470588235293,
      "grad_norm": 20.30126190185547,
      "learning_rate": 2.332352941176471e-05,
      "loss": 0.9854,
      "step": 45350
    },
    {
      "epoch": 2668.235294117647,
      "grad_norm": 16.630901336669922,
      "learning_rate": 2.331764705882353e-05,
      "loss": 0.9796,
      "step": 45360
    },
    {
      "epoch": 2668.823529411765,
      "grad_norm": 25.12505531311035,
      "learning_rate": 2.3311764705882355e-05,
      "loss": 1.0065,
      "step": 45370
    },
    {
      "epoch": 2669.4117647058824,
      "grad_norm": 16.02618980407715,
      "learning_rate": 2.3305882352941178e-05,
      "loss": 0.9937,
      "step": 45380
    },
    {
      "epoch": 2670.0,
      "grad_norm": 19.974674224853516,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 1.0155,
      "step": 45390
    },
    {
      "epoch": 2670.5882352941176,
      "grad_norm": 24.621397018432617,
      "learning_rate": 2.3294117647058824e-05,
      "loss": 1.0369,
      "step": 45400
    },
    {
      "epoch": 2671.176470588235,
      "grad_norm": 19.595558166503906,
      "learning_rate": 2.3288235294117647e-05,
      "loss": 1.1107,
      "step": 45410
    },
    {
      "epoch": 2671.764705882353,
      "grad_norm": 18.007625579833984,
      "learning_rate": 2.3282352941176473e-05,
      "loss": 1.005,
      "step": 45420
    },
    {
      "epoch": 2672.3529411764707,
      "grad_norm": 19.352073669433594,
      "learning_rate": 2.3276470588235292e-05,
      "loss": 1.0803,
      "step": 45430
    },
    {
      "epoch": 2672.9411764705883,
      "grad_norm": 22.0783634185791,
      "learning_rate": 2.327058823529412e-05,
      "loss": 1.0524,
      "step": 45440
    },
    {
      "epoch": 2673.529411764706,
      "grad_norm": 22.30328369140625,
      "learning_rate": 2.3264705882352942e-05,
      "loss": 1.0512,
      "step": 45450
    },
    {
      "epoch": 2674.1176470588234,
      "grad_norm": 21.062061309814453,
      "learning_rate": 2.3258823529411765e-05,
      "loss": 0.9493,
      "step": 45460
    },
    {
      "epoch": 2674.705882352941,
      "grad_norm": 11.538412094116211,
      "learning_rate": 2.3252941176470588e-05,
      "loss": 1.0256,
      "step": 45470
    },
    {
      "epoch": 2675.294117647059,
      "grad_norm": 15.509490013122559,
      "learning_rate": 2.3247058823529414e-05,
      "loss": 1.0971,
      "step": 45480
    },
    {
      "epoch": 2675.8823529411766,
      "grad_norm": 18.71320915222168,
      "learning_rate": 2.3241176470588237e-05,
      "loss": 0.9993,
      "step": 45490
    },
    {
      "epoch": 2676.470588235294,
      "grad_norm": 27.10514259338379,
      "learning_rate": 2.323529411764706e-05,
      "loss": 1.0489,
      "step": 45500
    },
    {
      "epoch": 2677.0588235294117,
      "grad_norm": 17.640031814575195,
      "learning_rate": 2.3229411764705883e-05,
      "loss": 1.1382,
      "step": 45510
    },
    {
      "epoch": 2677.6470588235293,
      "grad_norm": 19.927717208862305,
      "learning_rate": 2.322352941176471e-05,
      "loss": 1.0396,
      "step": 45520
    },
    {
      "epoch": 2678.235294117647,
      "grad_norm": 22.719087600708008,
      "learning_rate": 2.321764705882353e-05,
      "loss": 0.9058,
      "step": 45530
    },
    {
      "epoch": 2678.823529411765,
      "grad_norm": 21.14346694946289,
      "learning_rate": 2.3211764705882356e-05,
      "loss": 1.1268,
      "step": 45540
    },
    {
      "epoch": 2679.4117647058824,
      "grad_norm": 18.411283493041992,
      "learning_rate": 2.320588235294118e-05,
      "loss": 1.0922,
      "step": 45550
    },
    {
      "epoch": 2680.0,
      "grad_norm": 29.946632385253906,
      "learning_rate": 2.32e-05,
      "loss": 1.075,
      "step": 45560
    },
    {
      "epoch": 2680.5882352941176,
      "grad_norm": 21.88246726989746,
      "learning_rate": 2.3194117647058824e-05,
      "loss": 1.0261,
      "step": 45570
    },
    {
      "epoch": 2681.176470588235,
      "grad_norm": 22.785547256469727,
      "learning_rate": 2.318823529411765e-05,
      "loss": 1.0154,
      "step": 45580
    },
    {
      "epoch": 2681.764705882353,
      "grad_norm": 20.86421775817871,
      "learning_rate": 2.318235294117647e-05,
      "loss": 1.0542,
      "step": 45590
    },
    {
      "epoch": 2682.3529411764707,
      "grad_norm": 16.230316162109375,
      "learning_rate": 2.3176470588235293e-05,
      "loss": 1.0276,
      "step": 45600
    },
    {
      "epoch": 2682.9411764705883,
      "grad_norm": 16.17094612121582,
      "learning_rate": 2.317058823529412e-05,
      "loss": 1.0353,
      "step": 45610
    },
    {
      "epoch": 2683.529411764706,
      "grad_norm": 15.431907653808594,
      "learning_rate": 2.3164705882352943e-05,
      "loss": 1.1062,
      "step": 45620
    },
    {
      "epoch": 2684.1176470588234,
      "grad_norm": 18.039836883544922,
      "learning_rate": 2.3158823529411766e-05,
      "loss": 1.0109,
      "step": 45630
    },
    {
      "epoch": 2684.705882352941,
      "grad_norm": 29.1578369140625,
      "learning_rate": 2.315294117647059e-05,
      "loss": 1.0465,
      "step": 45640
    },
    {
      "epoch": 2685.294117647059,
      "grad_norm": 18.415027618408203,
      "learning_rate": 2.3147058823529412e-05,
      "loss": 0.9966,
      "step": 45650
    },
    {
      "epoch": 2685.8823529411766,
      "grad_norm": 21.180574417114258,
      "learning_rate": 2.3141176470588235e-05,
      "loss": 1.0985,
      "step": 45660
    },
    {
      "epoch": 2686.470588235294,
      "grad_norm": 13.891529083251953,
      "learning_rate": 2.313529411764706e-05,
      "loss": 0.9711,
      "step": 45670
    },
    {
      "epoch": 2687.0588235294117,
      "grad_norm": 16.80198097229004,
      "learning_rate": 2.3129411764705884e-05,
      "loss": 0.9912,
      "step": 45680
    },
    {
      "epoch": 2687.6470588235293,
      "grad_norm": 19.289169311523438,
      "learning_rate": 2.3123529411764707e-05,
      "loss": 0.9998,
      "step": 45690
    },
    {
      "epoch": 2688.235294117647,
      "grad_norm": 23.39661407470703,
      "learning_rate": 2.311764705882353e-05,
      "loss": 1.1005,
      "step": 45700
    },
    {
      "epoch": 2688.823529411765,
      "grad_norm": 21.299257278442383,
      "learning_rate": 2.3111764705882356e-05,
      "loss": 1.0564,
      "step": 45710
    },
    {
      "epoch": 2689.4117647058824,
      "grad_norm": 18.47065544128418,
      "learning_rate": 2.3105882352941176e-05,
      "loss": 0.9869,
      "step": 45720
    },
    {
      "epoch": 2690.0,
      "grad_norm": 18.430206298828125,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 1.1076,
      "step": 45730
    },
    {
      "epoch": 2690.5882352941176,
      "grad_norm": 20.70598602294922,
      "learning_rate": 2.3094117647058825e-05,
      "loss": 0.9863,
      "step": 45740
    },
    {
      "epoch": 2691.176470588235,
      "grad_norm": 21.511985778808594,
      "learning_rate": 2.308823529411765e-05,
      "loss": 1.0391,
      "step": 45750
    },
    {
      "epoch": 2691.764705882353,
      "grad_norm": 19.005517959594727,
      "learning_rate": 2.308235294117647e-05,
      "loss": 0.9897,
      "step": 45760
    },
    {
      "epoch": 2692.3529411764707,
      "grad_norm": 16.121213912963867,
      "learning_rate": 2.3076470588235294e-05,
      "loss": 0.9205,
      "step": 45770
    },
    {
      "epoch": 2692.9411764705883,
      "grad_norm": 24.553443908691406,
      "learning_rate": 2.3070588235294117e-05,
      "loss": 1.1037,
      "step": 45780
    },
    {
      "epoch": 2693.529411764706,
      "grad_norm": 20.57452964782715,
      "learning_rate": 2.306470588235294e-05,
      "loss": 1.0065,
      "step": 45790
    },
    {
      "epoch": 2694.1176470588234,
      "grad_norm": 22.69645881652832,
      "learning_rate": 2.3058823529411767e-05,
      "loss": 0.9788,
      "step": 45800
    },
    {
      "epoch": 2694.705882352941,
      "grad_norm": 18.504474639892578,
      "learning_rate": 2.305294117647059e-05,
      "loss": 1.0474,
      "step": 45810
    },
    {
      "epoch": 2695.294117647059,
      "grad_norm": 20.433815002441406,
      "learning_rate": 2.3047058823529413e-05,
      "loss": 1.0465,
      "step": 45820
    },
    {
      "epoch": 2695.8823529411766,
      "grad_norm": 19.19743537902832,
      "learning_rate": 2.3041176470588236e-05,
      "loss": 0.9761,
      "step": 45830
    },
    {
      "epoch": 2696.470588235294,
      "grad_norm": 19.442564010620117,
      "learning_rate": 2.3035294117647062e-05,
      "loss": 1.1299,
      "step": 45840
    },
    {
      "epoch": 2697.0588235294117,
      "grad_norm": 21.073503494262695,
      "learning_rate": 2.302941176470588e-05,
      "loss": 1.1055,
      "step": 45850
    },
    {
      "epoch": 2697.6470588235293,
      "grad_norm": 17.911048889160156,
      "learning_rate": 2.3023529411764708e-05,
      "loss": 1.1251,
      "step": 45860
    },
    {
      "epoch": 2698.235294117647,
      "grad_norm": 19.090862274169922,
      "learning_rate": 2.301764705882353e-05,
      "loss": 0.9995,
      "step": 45870
    },
    {
      "epoch": 2698.823529411765,
      "grad_norm": 14.614171028137207,
      "learning_rate": 2.3011764705882354e-05,
      "loss": 0.8903,
      "step": 45880
    },
    {
      "epoch": 2699.4117647058824,
      "grad_norm": 15.180209159851074,
      "learning_rate": 2.3005882352941177e-05,
      "loss": 0.9607,
      "step": 45890
    },
    {
      "epoch": 2700.0,
      "grad_norm": 19.652711868286133,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.981,
      "step": 45900
    },
    {
      "epoch": 2700.5882352941176,
      "grad_norm": 22.92522621154785,
      "learning_rate": 2.2994117647058823e-05,
      "loss": 1.11,
      "step": 45910
    },
    {
      "epoch": 2701.176470588235,
      "grad_norm": 21.061321258544922,
      "learning_rate": 2.298823529411765e-05,
      "loss": 1.083,
      "step": 45920
    },
    {
      "epoch": 2701.764705882353,
      "grad_norm": 18.593347549438477,
      "learning_rate": 2.2982352941176472e-05,
      "loss": 1.1083,
      "step": 45930
    },
    {
      "epoch": 2702.3529411764707,
      "grad_norm": 21.67474365234375,
      "learning_rate": 2.2976470588235295e-05,
      "loss": 1.1308,
      "step": 45940
    },
    {
      "epoch": 2702.9411764705883,
      "grad_norm": 24.62660026550293,
      "learning_rate": 2.297058823529412e-05,
      "loss": 1.0708,
      "step": 45950
    },
    {
      "epoch": 2703.529411764706,
      "grad_norm": 16.947858810424805,
      "learning_rate": 2.296470588235294e-05,
      "loss": 1.0202,
      "step": 45960
    },
    {
      "epoch": 2704.1176470588234,
      "grad_norm": 21.575428009033203,
      "learning_rate": 2.2958823529411764e-05,
      "loss": 1.051,
      "step": 45970
    },
    {
      "epoch": 2704.705882352941,
      "grad_norm": 23.451051712036133,
      "learning_rate": 2.2952941176470587e-05,
      "loss": 1.0181,
      "step": 45980
    },
    {
      "epoch": 2705.294117647059,
      "grad_norm": 16.198625564575195,
      "learning_rate": 2.2947058823529414e-05,
      "loss": 1.0028,
      "step": 45990
    },
    {
      "epoch": 2705.8823529411766,
      "grad_norm": 16.07636260986328,
      "learning_rate": 2.2941176470588237e-05,
      "loss": 1.002,
      "step": 46000
    },
    {
      "epoch": 2706.470588235294,
      "grad_norm": 25.314491271972656,
      "learning_rate": 2.293529411764706e-05,
      "loss": 1.0323,
      "step": 46010
    },
    {
      "epoch": 2707.0588235294117,
      "grad_norm": 18.496963500976562,
      "learning_rate": 2.2929411764705883e-05,
      "loss": 1.0928,
      "step": 46020
    },
    {
      "epoch": 2707.6470588235293,
      "grad_norm": 20.095619201660156,
      "learning_rate": 2.292352941176471e-05,
      "loss": 1.1769,
      "step": 46030
    },
    {
      "epoch": 2708.235294117647,
      "grad_norm": 16.860891342163086,
      "learning_rate": 2.291764705882353e-05,
      "loss": 0.949,
      "step": 46040
    },
    {
      "epoch": 2708.823529411765,
      "grad_norm": 19.85726547241211,
      "learning_rate": 2.2911764705882355e-05,
      "loss": 1.111,
      "step": 46050
    },
    {
      "epoch": 2709.4117647058824,
      "grad_norm": 23.336854934692383,
      "learning_rate": 2.2905882352941178e-05,
      "loss": 1.1233,
      "step": 46060
    },
    {
      "epoch": 2710.0,
      "grad_norm": 24.816938400268555,
      "learning_rate": 2.29e-05,
      "loss": 0.9731,
      "step": 46070
    },
    {
      "epoch": 2710.5882352941176,
      "grad_norm": 25.294782638549805,
      "learning_rate": 2.2894117647058824e-05,
      "loss": 1.1075,
      "step": 46080
    },
    {
      "epoch": 2711.176470588235,
      "grad_norm": 22.197341918945312,
      "learning_rate": 2.288823529411765e-05,
      "loss": 1.0006,
      "step": 46090
    },
    {
      "epoch": 2711.764705882353,
      "grad_norm": 17.889081954956055,
      "learning_rate": 2.288235294117647e-05,
      "loss": 0.9337,
      "step": 46100
    },
    {
      "epoch": 2712.3529411764707,
      "grad_norm": 12.099870681762695,
      "learning_rate": 2.2876470588235296e-05,
      "loss": 1.1241,
      "step": 46110
    },
    {
      "epoch": 2712.9411764705883,
      "grad_norm": 24.6585636138916,
      "learning_rate": 2.287058823529412e-05,
      "loss": 1.038,
      "step": 46120
    },
    {
      "epoch": 2713.529411764706,
      "grad_norm": 19.783447265625,
      "learning_rate": 2.2864705882352942e-05,
      "loss": 1.0947,
      "step": 46130
    },
    {
      "epoch": 2714.1176470588234,
      "grad_norm": 25.218280792236328,
      "learning_rate": 2.2858823529411765e-05,
      "loss": 1.0705,
      "step": 46140
    },
    {
      "epoch": 2714.705882352941,
      "grad_norm": 20.840229034423828,
      "learning_rate": 2.2852941176470588e-05,
      "loss": 0.9975,
      "step": 46150
    },
    {
      "epoch": 2715.294117647059,
      "grad_norm": 20.920181274414062,
      "learning_rate": 2.2847058823529415e-05,
      "loss": 1.0811,
      "step": 46160
    },
    {
      "epoch": 2715.8823529411766,
      "grad_norm": 20.065141677856445,
      "learning_rate": 2.2841176470588234e-05,
      "loss": 1.1495,
      "step": 46170
    },
    {
      "epoch": 2716.470588235294,
      "grad_norm": 24.61682891845703,
      "learning_rate": 2.283529411764706e-05,
      "loss": 1.031,
      "step": 46180
    },
    {
      "epoch": 2717.0588235294117,
      "grad_norm": 16.39386749267578,
      "learning_rate": 2.2829411764705884e-05,
      "loss": 1.0086,
      "step": 46190
    },
    {
      "epoch": 2717.6470588235293,
      "grad_norm": 24.2244873046875,
      "learning_rate": 2.2823529411764707e-05,
      "loss": 1.1483,
      "step": 46200
    },
    {
      "epoch": 2718.235294117647,
      "grad_norm": 20.434377670288086,
      "learning_rate": 2.281764705882353e-05,
      "loss": 1.1666,
      "step": 46210
    },
    {
      "epoch": 2718.823529411765,
      "grad_norm": 15.35322093963623,
      "learning_rate": 2.2811764705882356e-05,
      "loss": 0.9699,
      "step": 46220
    },
    {
      "epoch": 2719.4117647058824,
      "grad_norm": 18.35360336303711,
      "learning_rate": 2.2805882352941176e-05,
      "loss": 1.0004,
      "step": 46230
    },
    {
      "epoch": 2720.0,
      "grad_norm": 24.362661361694336,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.9802,
      "step": 46240
    },
    {
      "epoch": 2720.5882352941176,
      "grad_norm": 25.18894386291504,
      "learning_rate": 2.2794117647058825e-05,
      "loss": 0.9758,
      "step": 46250
    },
    {
      "epoch": 2721.176470588235,
      "grad_norm": 20.464954376220703,
      "learning_rate": 2.2788235294117648e-05,
      "loss": 1.0159,
      "step": 46260
    },
    {
      "epoch": 2721.764705882353,
      "grad_norm": 20.37639045715332,
      "learning_rate": 2.278235294117647e-05,
      "loss": 1.054,
      "step": 46270
    },
    {
      "epoch": 2722.3529411764707,
      "grad_norm": 20.65947723388672,
      "learning_rate": 2.2776470588235297e-05,
      "loss": 1.0628,
      "step": 46280
    },
    {
      "epoch": 2722.9411764705883,
      "grad_norm": 24.34458351135254,
      "learning_rate": 2.2770588235294117e-05,
      "loss": 1.0349,
      "step": 46290
    },
    {
      "epoch": 2723.529411764706,
      "grad_norm": 13.673859596252441,
      "learning_rate": 2.2764705882352943e-05,
      "loss": 0.9237,
      "step": 46300
    },
    {
      "epoch": 2724.1176470588234,
      "grad_norm": 19.88799476623535,
      "learning_rate": 2.2758823529411766e-05,
      "loss": 1.0601,
      "step": 46310
    },
    {
      "epoch": 2724.705882352941,
      "grad_norm": 23.873367309570312,
      "learning_rate": 2.275294117647059e-05,
      "loss": 1.0771,
      "step": 46320
    },
    {
      "epoch": 2725.294117647059,
      "grad_norm": 18.69017219543457,
      "learning_rate": 2.2747058823529412e-05,
      "loss": 1.0013,
      "step": 46330
    },
    {
      "epoch": 2725.8823529411766,
      "grad_norm": 20.503416061401367,
      "learning_rate": 2.2741176470588235e-05,
      "loss": 1.0749,
      "step": 46340
    },
    {
      "epoch": 2726.470588235294,
      "grad_norm": 20.190046310424805,
      "learning_rate": 2.273529411764706e-05,
      "loss": 0.9823,
      "step": 46350
    },
    {
      "epoch": 2727.0588235294117,
      "grad_norm": 23.860939025878906,
      "learning_rate": 2.272941176470588e-05,
      "loss": 0.9778,
      "step": 46360
    },
    {
      "epoch": 2727.6470588235293,
      "grad_norm": 15.562431335449219,
      "learning_rate": 2.2723529411764708e-05,
      "loss": 0.8976,
      "step": 46370
    },
    {
      "epoch": 2728.235294117647,
      "grad_norm": 22.4630184173584,
      "learning_rate": 2.271764705882353e-05,
      "loss": 0.9974,
      "step": 46380
    },
    {
      "epoch": 2728.823529411765,
      "grad_norm": 19.579328536987305,
      "learning_rate": 2.2711764705882354e-05,
      "loss": 0.977,
      "step": 46390
    },
    {
      "epoch": 2729.4117647058824,
      "grad_norm": 21.68474578857422,
      "learning_rate": 2.2705882352941177e-05,
      "loss": 1.1086,
      "step": 46400
    },
    {
      "epoch": 2730.0,
      "grad_norm": 19.415742874145508,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.9664,
      "step": 46410
    },
    {
      "epoch": 2730.5882352941176,
      "grad_norm": 19.00040054321289,
      "learning_rate": 2.2694117647058822e-05,
      "loss": 1.0198,
      "step": 46420
    },
    {
      "epoch": 2731.176470588235,
      "grad_norm": 25.23991584777832,
      "learning_rate": 2.268823529411765e-05,
      "loss": 1.0118,
      "step": 46430
    },
    {
      "epoch": 2731.764705882353,
      "grad_norm": 18.870004653930664,
      "learning_rate": 2.2682352941176472e-05,
      "loss": 1.099,
      "step": 46440
    },
    {
      "epoch": 2732.3529411764707,
      "grad_norm": 23.930917739868164,
      "learning_rate": 2.2676470588235295e-05,
      "loss": 1.0528,
      "step": 46450
    },
    {
      "epoch": 2732.9411764705883,
      "grad_norm": 20.700998306274414,
      "learning_rate": 2.2670588235294118e-05,
      "loss": 1.0644,
      "step": 46460
    },
    {
      "epoch": 2733.529411764706,
      "grad_norm": 23.473155975341797,
      "learning_rate": 2.2664705882352944e-05,
      "loss": 1.0115,
      "step": 46470
    },
    {
      "epoch": 2734.1176470588234,
      "grad_norm": 31.81479835510254,
      "learning_rate": 2.2658823529411767e-05,
      "loss": 1.0214,
      "step": 46480
    },
    {
      "epoch": 2734.705882352941,
      "grad_norm": 14.773909568786621,
      "learning_rate": 2.265294117647059e-05,
      "loss": 0.9617,
      "step": 46490
    },
    {
      "epoch": 2735.294117647059,
      "grad_norm": 19.18199920654297,
      "learning_rate": 2.2647058823529413e-05,
      "loss": 1.1782,
      "step": 46500
    },
    {
      "epoch": 2735.8823529411766,
      "grad_norm": 20.20052719116211,
      "learning_rate": 2.2641176470588236e-05,
      "loss": 0.9915,
      "step": 46510
    },
    {
      "epoch": 2736.470588235294,
      "grad_norm": 17.58582305908203,
      "learning_rate": 2.263529411764706e-05,
      "loss": 1.0228,
      "step": 46520
    },
    {
      "epoch": 2737.0588235294117,
      "grad_norm": 22.977031707763672,
      "learning_rate": 2.2629411764705882e-05,
      "loss": 0.898,
      "step": 46530
    },
    {
      "epoch": 2737.6470588235293,
      "grad_norm": 17.979066848754883,
      "learning_rate": 2.262352941176471e-05,
      "loss": 0.8985,
      "step": 46540
    },
    {
      "epoch": 2738.235294117647,
      "grad_norm": 25.00888442993164,
      "learning_rate": 2.2617647058823528e-05,
      "loss": 0.9572,
      "step": 46550
    },
    {
      "epoch": 2738.823529411765,
      "grad_norm": 20.11602783203125,
      "learning_rate": 2.2611764705882354e-05,
      "loss": 1.071,
      "step": 46560
    },
    {
      "epoch": 2739.4117647058824,
      "grad_norm": 23.11273193359375,
      "learning_rate": 2.2605882352941177e-05,
      "loss": 1.0544,
      "step": 46570
    },
    {
      "epoch": 2740.0,
      "grad_norm": 22.13412094116211,
      "learning_rate": 2.26e-05,
      "loss": 1.0675,
      "step": 46580
    },
    {
      "epoch": 2740.5882352941176,
      "grad_norm": 19.285539627075195,
      "learning_rate": 2.2594117647058823e-05,
      "loss": 0.9854,
      "step": 46590
    },
    {
      "epoch": 2741.176470588235,
      "grad_norm": 16.126665115356445,
      "learning_rate": 2.258823529411765e-05,
      "loss": 0.9431,
      "step": 46600
    },
    {
      "epoch": 2741.764705882353,
      "grad_norm": 24.372617721557617,
      "learning_rate": 2.2582352941176473e-05,
      "loss": 1.0278,
      "step": 46610
    },
    {
      "epoch": 2742.3529411764707,
      "grad_norm": 22.07051658630371,
      "learning_rate": 2.2576470588235296e-05,
      "loss": 1.0029,
      "step": 46620
    },
    {
      "epoch": 2742.9411764705883,
      "grad_norm": 23.075632095336914,
      "learning_rate": 2.257058823529412e-05,
      "loss": 0.9959,
      "step": 46630
    },
    {
      "epoch": 2743.529411764706,
      "grad_norm": 23.934795379638672,
      "learning_rate": 2.2564705882352942e-05,
      "loss": 1.1599,
      "step": 46640
    },
    {
      "epoch": 2744.1176470588234,
      "grad_norm": 16.956439971923828,
      "learning_rate": 2.2558823529411765e-05,
      "loss": 0.9731,
      "step": 46650
    },
    {
      "epoch": 2744.705882352941,
      "grad_norm": 14.735071182250977,
      "learning_rate": 2.255294117647059e-05,
      "loss": 0.9569,
      "step": 46660
    },
    {
      "epoch": 2745.294117647059,
      "grad_norm": 15.866992950439453,
      "learning_rate": 2.2547058823529414e-05,
      "loss": 1.0286,
      "step": 46670
    },
    {
      "epoch": 2745.8823529411766,
      "grad_norm": 20.4240665435791,
      "learning_rate": 2.2541176470588234e-05,
      "loss": 0.9479,
      "step": 46680
    },
    {
      "epoch": 2746.470588235294,
      "grad_norm": 21.313426971435547,
      "learning_rate": 2.253529411764706e-05,
      "loss": 1.0914,
      "step": 46690
    },
    {
      "epoch": 2747.0588235294117,
      "grad_norm": 25.99846839904785,
      "learning_rate": 2.2529411764705883e-05,
      "loss": 1.054,
      "step": 46700
    },
    {
      "epoch": 2747.6470588235293,
      "grad_norm": 23.688268661499023,
      "learning_rate": 2.2523529411764706e-05,
      "loss": 0.9609,
      "step": 46710
    },
    {
      "epoch": 2748.235294117647,
      "grad_norm": 18.925933837890625,
      "learning_rate": 2.251764705882353e-05,
      "loss": 1.0503,
      "step": 46720
    },
    {
      "epoch": 2748.823529411765,
      "grad_norm": 16.20839500427246,
      "learning_rate": 2.2511764705882355e-05,
      "loss": 0.9601,
      "step": 46730
    },
    {
      "epoch": 2749.4117647058824,
      "grad_norm": 19.985836029052734,
      "learning_rate": 2.2505882352941175e-05,
      "loss": 0.9411,
      "step": 46740
    },
    {
      "epoch": 2750.0,
      "grad_norm": 20.09398078918457,
      "learning_rate": 2.25e-05,
      "loss": 1.0454,
      "step": 46750
    },
    {
      "epoch": 2750.5882352941176,
      "grad_norm": 17.612642288208008,
      "learning_rate": 2.2494117647058824e-05,
      "loss": 0.9868,
      "step": 46760
    },
    {
      "epoch": 2751.176470588235,
      "grad_norm": 15.577988624572754,
      "learning_rate": 2.2488235294117647e-05,
      "loss": 1.1464,
      "step": 46770
    },
    {
      "epoch": 2751.764705882353,
      "grad_norm": 17.447086334228516,
      "learning_rate": 2.248235294117647e-05,
      "loss": 1.0273,
      "step": 46780
    },
    {
      "epoch": 2752.3529411764707,
      "grad_norm": 22.525897979736328,
      "learning_rate": 2.2476470588235297e-05,
      "loss": 0.978,
      "step": 46790
    },
    {
      "epoch": 2752.9411764705883,
      "grad_norm": 15.657690048217773,
      "learning_rate": 2.247058823529412e-05,
      "loss": 1.0141,
      "step": 46800
    },
    {
      "epoch": 2753.529411764706,
      "grad_norm": 22.334381103515625,
      "learning_rate": 2.2464705882352943e-05,
      "loss": 1.0189,
      "step": 46810
    },
    {
      "epoch": 2754.1176470588234,
      "grad_norm": 17.671733856201172,
      "learning_rate": 2.2458823529411766e-05,
      "loss": 1.052,
      "step": 46820
    },
    {
      "epoch": 2754.705882352941,
      "grad_norm": 20.889888763427734,
      "learning_rate": 2.2452941176470592e-05,
      "loss": 0.9502,
      "step": 46830
    },
    {
      "epoch": 2755.294117647059,
      "grad_norm": 20.22224235534668,
      "learning_rate": 2.2447058823529412e-05,
      "loss": 1.0401,
      "step": 46840
    },
    {
      "epoch": 2755.8823529411766,
      "grad_norm": 17.78863525390625,
      "learning_rate": 2.2441176470588238e-05,
      "loss": 1.0433,
      "step": 46850
    },
    {
      "epoch": 2756.470588235294,
      "grad_norm": 19.350051879882812,
      "learning_rate": 2.243529411764706e-05,
      "loss": 1.0918,
      "step": 46860
    },
    {
      "epoch": 2757.0588235294117,
      "grad_norm": 17.335548400878906,
      "learning_rate": 2.242941176470588e-05,
      "loss": 0.934,
      "step": 46870
    },
    {
      "epoch": 2757.6470588235293,
      "grad_norm": 23.17286491394043,
      "learning_rate": 2.2423529411764707e-05,
      "loss": 1.0621,
      "step": 46880
    },
    {
      "epoch": 2758.235294117647,
      "grad_norm": 23.68219757080078,
      "learning_rate": 2.241764705882353e-05,
      "loss": 1.0963,
      "step": 46890
    },
    {
      "epoch": 2758.823529411765,
      "grad_norm": 22.13780975341797,
      "learning_rate": 2.2411764705882353e-05,
      "loss": 0.8689,
      "step": 46900
    },
    {
      "epoch": 2759.4117647058824,
      "grad_norm": 24.04197120666504,
      "learning_rate": 2.2405882352941176e-05,
      "loss": 1.0274,
      "step": 46910
    },
    {
      "epoch": 2760.0,
      "grad_norm": 27.846343994140625,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.9652,
      "step": 46920
    },
    {
      "epoch": 2760.5882352941176,
      "grad_norm": 18.720252990722656,
      "learning_rate": 2.2394117647058825e-05,
      "loss": 0.9452,
      "step": 46930
    },
    {
      "epoch": 2761.176470588235,
      "grad_norm": 23.947185516357422,
      "learning_rate": 2.238823529411765e-05,
      "loss": 1.0497,
      "step": 46940
    },
    {
      "epoch": 2761.764705882353,
      "grad_norm": 21.021562576293945,
      "learning_rate": 2.238235294117647e-05,
      "loss": 0.9335,
      "step": 46950
    },
    {
      "epoch": 2762.3529411764707,
      "grad_norm": 24.684040069580078,
      "learning_rate": 2.2376470588235294e-05,
      "loss": 1.0569,
      "step": 46960
    },
    {
      "epoch": 2762.9411764705883,
      "grad_norm": 21.78142547607422,
      "learning_rate": 2.2370588235294117e-05,
      "loss": 0.9647,
      "step": 46970
    },
    {
      "epoch": 2763.529411764706,
      "grad_norm": 20.808393478393555,
      "learning_rate": 2.2364705882352944e-05,
      "loss": 1.0523,
      "step": 46980
    },
    {
      "epoch": 2764.1176470588234,
      "grad_norm": 19.72127914428711,
      "learning_rate": 2.2358823529411767e-05,
      "loss": 0.9965,
      "step": 46990
    },
    {
      "epoch": 2764.705882352941,
      "grad_norm": 21.527246475219727,
      "learning_rate": 2.235294117647059e-05,
      "loss": 0.9777,
      "step": 47000
    },
    {
      "epoch": 2765.294117647059,
      "grad_norm": 23.394630432128906,
      "learning_rate": 2.2347058823529413e-05,
      "loss": 1.0815,
      "step": 47010
    },
    {
      "epoch": 2765.8823529411766,
      "grad_norm": 20.91068458557129,
      "learning_rate": 2.234117647058824e-05,
      "loss": 1.1358,
      "step": 47020
    },
    {
      "epoch": 2766.470588235294,
      "grad_norm": 16.96358299255371,
      "learning_rate": 2.233529411764706e-05,
      "loss": 0.9368,
      "step": 47030
    },
    {
      "epoch": 2767.0588235294117,
      "grad_norm": 19.087966918945312,
      "learning_rate": 2.2329411764705885e-05,
      "loss": 1.1003,
      "step": 47040
    },
    {
      "epoch": 2767.6470588235293,
      "grad_norm": 17.461027145385742,
      "learning_rate": 2.2323529411764708e-05,
      "loss": 1.1285,
      "step": 47050
    },
    {
      "epoch": 2768.235294117647,
      "grad_norm": 16.547277450561523,
      "learning_rate": 2.2317647058823528e-05,
      "loss": 0.9451,
      "step": 47060
    },
    {
      "epoch": 2768.823529411765,
      "grad_norm": 20.398679733276367,
      "learning_rate": 2.2311764705882354e-05,
      "loss": 1.0902,
      "step": 47070
    },
    {
      "epoch": 2769.4117647058824,
      "grad_norm": 18.19169807434082,
      "learning_rate": 2.2305882352941177e-05,
      "loss": 1.0531,
      "step": 47080
    },
    {
      "epoch": 2770.0,
      "grad_norm": 22.362751007080078,
      "learning_rate": 2.23e-05,
      "loss": 0.9647,
      "step": 47090
    },
    {
      "epoch": 2770.5882352941176,
      "grad_norm": 20.0479793548584,
      "learning_rate": 2.2294117647058823e-05,
      "loss": 0.9861,
      "step": 47100
    },
    {
      "epoch": 2771.176470588235,
      "grad_norm": 17.460859298706055,
      "learning_rate": 2.228823529411765e-05,
      "loss": 1.033,
      "step": 47110
    },
    {
      "epoch": 2771.764705882353,
      "grad_norm": 16.6593074798584,
      "learning_rate": 2.2282352941176472e-05,
      "loss": 1.1545,
      "step": 47120
    },
    {
      "epoch": 2772.3529411764707,
      "grad_norm": 23.143739700317383,
      "learning_rate": 2.2276470588235295e-05,
      "loss": 0.9467,
      "step": 47130
    },
    {
      "epoch": 2772.9411764705883,
      "grad_norm": 22.607864379882812,
      "learning_rate": 2.2270588235294118e-05,
      "loss": 1.1186,
      "step": 47140
    },
    {
      "epoch": 2773.529411764706,
      "grad_norm": 15.631912231445312,
      "learning_rate": 2.2264705882352945e-05,
      "loss": 0.8857,
      "step": 47150
    },
    {
      "epoch": 2774.1176470588234,
      "grad_norm": 22.2476806640625,
      "learning_rate": 2.2258823529411764e-05,
      "loss": 0.9828,
      "step": 47160
    },
    {
      "epoch": 2774.705882352941,
      "grad_norm": 21.60190200805664,
      "learning_rate": 2.225294117647059e-05,
      "loss": 1.034,
      "step": 47170
    },
    {
      "epoch": 2775.294117647059,
      "grad_norm": 20.00520896911621,
      "learning_rate": 2.2247058823529414e-05,
      "loss": 0.9692,
      "step": 47180
    },
    {
      "epoch": 2775.8823529411766,
      "grad_norm": 15.489299774169922,
      "learning_rate": 2.2241176470588237e-05,
      "loss": 1.0065,
      "step": 47190
    },
    {
      "epoch": 2776.470588235294,
      "grad_norm": 21.9898624420166,
      "learning_rate": 2.223529411764706e-05,
      "loss": 1.0111,
      "step": 47200
    },
    {
      "epoch": 2777.0588235294117,
      "grad_norm": 18.04076385498047,
      "learning_rate": 2.2229411764705886e-05,
      "loss": 1.0202,
      "step": 47210
    },
    {
      "epoch": 2777.6470588235293,
      "grad_norm": 26.010013580322266,
      "learning_rate": 2.2223529411764706e-05,
      "loss": 1.0864,
      "step": 47220
    },
    {
      "epoch": 2778.235294117647,
      "grad_norm": 22.853649139404297,
      "learning_rate": 2.221764705882353e-05,
      "loss": 1.0384,
      "step": 47230
    },
    {
      "epoch": 2778.823529411765,
      "grad_norm": 16.773578643798828,
      "learning_rate": 2.2211764705882355e-05,
      "loss": 0.9696,
      "step": 47240
    },
    {
      "epoch": 2779.4117647058824,
      "grad_norm": 28.779756546020508,
      "learning_rate": 2.2205882352941178e-05,
      "loss": 1.0634,
      "step": 47250
    },
    {
      "epoch": 2780.0,
      "grad_norm": 23.273733139038086,
      "learning_rate": 2.22e-05,
      "loss": 1.1286,
      "step": 47260
    },
    {
      "epoch": 2780.5882352941176,
      "grad_norm": 19.973695755004883,
      "learning_rate": 2.2194117647058824e-05,
      "loss": 1.0267,
      "step": 47270
    },
    {
      "epoch": 2781.176470588235,
      "grad_norm": 21.39203453063965,
      "learning_rate": 2.2188235294117647e-05,
      "loss": 0.9896,
      "step": 47280
    },
    {
      "epoch": 2781.764705882353,
      "grad_norm": 21.5794677734375,
      "learning_rate": 2.218235294117647e-05,
      "loss": 0.9179,
      "step": 47290
    },
    {
      "epoch": 2782.3529411764707,
      "grad_norm": 25.09812355041504,
      "learning_rate": 2.2176470588235296e-05,
      "loss": 1.0075,
      "step": 47300
    },
    {
      "epoch": 2782.9411764705883,
      "grad_norm": 15.613481521606445,
      "learning_rate": 2.217058823529412e-05,
      "loss": 0.9572,
      "step": 47310
    },
    {
      "epoch": 2783.529411764706,
      "grad_norm": 22.2748966217041,
      "learning_rate": 2.2164705882352942e-05,
      "loss": 0.9928,
      "step": 47320
    },
    {
      "epoch": 2784.1176470588234,
      "grad_norm": 21.506044387817383,
      "learning_rate": 2.2158823529411765e-05,
      "loss": 1.059,
      "step": 47330
    },
    {
      "epoch": 2784.705882352941,
      "grad_norm": 23.201154708862305,
      "learning_rate": 2.215294117647059e-05,
      "loss": 0.9662,
      "step": 47340
    },
    {
      "epoch": 2785.294117647059,
      "grad_norm": 19.464393615722656,
      "learning_rate": 2.214705882352941e-05,
      "loss": 1.0155,
      "step": 47350
    },
    {
      "epoch": 2785.8823529411766,
      "grad_norm": 19.85651206970215,
      "learning_rate": 2.2141176470588238e-05,
      "loss": 1.0031,
      "step": 47360
    },
    {
      "epoch": 2786.470588235294,
      "grad_norm": 26.210826873779297,
      "learning_rate": 2.213529411764706e-05,
      "loss": 1.0859,
      "step": 47370
    },
    {
      "epoch": 2787.0588235294117,
      "grad_norm": 19.931671142578125,
      "learning_rate": 2.2129411764705884e-05,
      "loss": 1.1568,
      "step": 47380
    },
    {
      "epoch": 2787.6470588235293,
      "grad_norm": 21.758668899536133,
      "learning_rate": 2.2123529411764707e-05,
      "loss": 1.0693,
      "step": 47390
    },
    {
      "epoch": 2788.235294117647,
      "grad_norm": 22.308120727539062,
      "learning_rate": 2.2117647058823533e-05,
      "loss": 1.0361,
      "step": 47400
    },
    {
      "epoch": 2788.823529411765,
      "grad_norm": 19.52092933654785,
      "learning_rate": 2.2111764705882352e-05,
      "loss": 0.931,
      "step": 47410
    },
    {
      "epoch": 2789.4117647058824,
      "grad_norm": 17.87968635559082,
      "learning_rate": 2.2105882352941175e-05,
      "loss": 0.8652,
      "step": 47420
    },
    {
      "epoch": 2790.0,
      "grad_norm": 30.0042667388916,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.9648,
      "step": 47430
    },
    {
      "epoch": 2790.5882352941176,
      "grad_norm": 19.4920654296875,
      "learning_rate": 2.2094117647058825e-05,
      "loss": 1.0104,
      "step": 47440
    },
    {
      "epoch": 2791.176470588235,
      "grad_norm": 23.27086639404297,
      "learning_rate": 2.2088235294117648e-05,
      "loss": 1.0827,
      "step": 47450
    },
    {
      "epoch": 2791.764705882353,
      "grad_norm": 28.71368408203125,
      "learning_rate": 2.208235294117647e-05,
      "loss": 1.0023,
      "step": 47460
    },
    {
      "epoch": 2792.3529411764707,
      "grad_norm": 18.731176376342773,
      "learning_rate": 2.2076470588235297e-05,
      "loss": 1.0003,
      "step": 47470
    },
    {
      "epoch": 2792.9411764705883,
      "grad_norm": 20.69392204284668,
      "learning_rate": 2.2070588235294117e-05,
      "loss": 0.9751,
      "step": 47480
    },
    {
      "epoch": 2793.529411764706,
      "grad_norm": 16.04708480834961,
      "learning_rate": 2.2064705882352943e-05,
      "loss": 1.0306,
      "step": 47490
    },
    {
      "epoch": 2794.1176470588234,
      "grad_norm": 18.154756546020508,
      "learning_rate": 2.2058823529411766e-05,
      "loss": 1.0294,
      "step": 47500
    },
    {
      "epoch": 2794.705882352941,
      "grad_norm": 17.438007354736328,
      "learning_rate": 2.205294117647059e-05,
      "loss": 0.9329,
      "step": 47510
    },
    {
      "epoch": 2795.294117647059,
      "grad_norm": 18.976133346557617,
      "learning_rate": 2.2047058823529412e-05,
      "loss": 1.0182,
      "step": 47520
    },
    {
      "epoch": 2795.8823529411766,
      "grad_norm": 15.119144439697266,
      "learning_rate": 2.204117647058824e-05,
      "loss": 1.012,
      "step": 47530
    },
    {
      "epoch": 2796.470588235294,
      "grad_norm": 19.092498779296875,
      "learning_rate": 2.2035294117647058e-05,
      "loss": 0.9487,
      "step": 47540
    },
    {
      "epoch": 2797.0588235294117,
      "grad_norm": 23.79781723022461,
      "learning_rate": 2.2029411764705884e-05,
      "loss": 0.9677,
      "step": 47550
    },
    {
      "epoch": 2797.6470588235293,
      "grad_norm": 27.09321403503418,
      "learning_rate": 2.2023529411764707e-05,
      "loss": 1.1407,
      "step": 47560
    },
    {
      "epoch": 2798.235294117647,
      "grad_norm": 19.502281188964844,
      "learning_rate": 2.201764705882353e-05,
      "loss": 0.9685,
      "step": 47570
    },
    {
      "epoch": 2798.823529411765,
      "grad_norm": 20.280895233154297,
      "learning_rate": 2.2011764705882353e-05,
      "loss": 1.0857,
      "step": 47580
    },
    {
      "epoch": 2799.4117647058824,
      "grad_norm": 22.658857345581055,
      "learning_rate": 2.2005882352941176e-05,
      "loss": 0.9508,
      "step": 47590
    },
    {
      "epoch": 2800.0,
      "grad_norm": 27.067184448242188,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.0176,
      "step": 47600
    },
    {
      "epoch": 2800.5882352941176,
      "grad_norm": 20.377548217773438,
      "learning_rate": 2.1994117647058822e-05,
      "loss": 1.0368,
      "step": 47610
    },
    {
      "epoch": 2801.176470588235,
      "grad_norm": 23.753938674926758,
      "learning_rate": 2.198823529411765e-05,
      "loss": 0.9612,
      "step": 47620
    },
    {
      "epoch": 2801.764705882353,
      "grad_norm": 18.957307815551758,
      "learning_rate": 2.1982352941176472e-05,
      "loss": 1.0053,
      "step": 47630
    },
    {
      "epoch": 2802.3529411764707,
      "grad_norm": 18.93776512145996,
      "learning_rate": 2.1976470588235295e-05,
      "loss": 0.9357,
      "step": 47640
    },
    {
      "epoch": 2802.9411764705883,
      "grad_norm": 19.05642318725586,
      "learning_rate": 2.1970588235294118e-05,
      "loss": 1.02,
      "step": 47650
    },
    {
      "epoch": 2803.529411764706,
      "grad_norm": 17.738616943359375,
      "learning_rate": 2.1964705882352944e-05,
      "loss": 0.9797,
      "step": 47660
    },
    {
      "epoch": 2804.1176470588234,
      "grad_norm": 19.9725284576416,
      "learning_rate": 2.1958823529411764e-05,
      "loss": 0.9673,
      "step": 47670
    },
    {
      "epoch": 2804.705882352941,
      "grad_norm": 21.925426483154297,
      "learning_rate": 2.195294117647059e-05,
      "loss": 1.0735,
      "step": 47680
    },
    {
      "epoch": 2805.294117647059,
      "grad_norm": 22.018980026245117,
      "learning_rate": 2.1947058823529413e-05,
      "loss": 0.996,
      "step": 47690
    },
    {
      "epoch": 2805.8823529411766,
      "grad_norm": 18.64307403564453,
      "learning_rate": 2.1941176470588236e-05,
      "loss": 1.0483,
      "step": 47700
    },
    {
      "epoch": 2806.470588235294,
      "grad_norm": 25.28855323791504,
      "learning_rate": 2.193529411764706e-05,
      "loss": 1.016,
      "step": 47710
    },
    {
      "epoch": 2807.0588235294117,
      "grad_norm": 18.967456817626953,
      "learning_rate": 2.1929411764705885e-05,
      "loss": 0.8898,
      "step": 47720
    },
    {
      "epoch": 2807.6470588235293,
      "grad_norm": 19.846282958984375,
      "learning_rate": 2.1923529411764705e-05,
      "loss": 1.1046,
      "step": 47730
    },
    {
      "epoch": 2808.235294117647,
      "grad_norm": 16.618207931518555,
      "learning_rate": 2.191764705882353e-05,
      "loss": 0.9949,
      "step": 47740
    },
    {
      "epoch": 2808.823529411765,
      "grad_norm": 21.281538009643555,
      "learning_rate": 2.1911764705882354e-05,
      "loss": 1.1002,
      "step": 47750
    },
    {
      "epoch": 2809.4117647058824,
      "grad_norm": 26.187278747558594,
      "learning_rate": 2.1905882352941177e-05,
      "loss": 0.9774,
      "step": 47760
    },
    {
      "epoch": 2810.0,
      "grad_norm": 15.841132164001465,
      "learning_rate": 2.19e-05,
      "loss": 0.9832,
      "step": 47770
    },
    {
      "epoch": 2810.5882352941176,
      "grad_norm": 19.054607391357422,
      "learning_rate": 2.1894117647058823e-05,
      "loss": 1.0829,
      "step": 47780
    },
    {
      "epoch": 2811.176470588235,
      "grad_norm": 19.838743209838867,
      "learning_rate": 2.188823529411765e-05,
      "loss": 0.9548,
      "step": 47790
    },
    {
      "epoch": 2811.764705882353,
      "grad_norm": 23.668487548828125,
      "learning_rate": 2.188235294117647e-05,
      "loss": 0.9632,
      "step": 47800
    },
    {
      "epoch": 2812.3529411764707,
      "grad_norm": 18.882944107055664,
      "learning_rate": 2.1876470588235296e-05,
      "loss": 1.0157,
      "step": 47810
    },
    {
      "epoch": 2812.9411764705883,
      "grad_norm": 13.85898494720459,
      "learning_rate": 2.187058823529412e-05,
      "loss": 0.9029,
      "step": 47820
    },
    {
      "epoch": 2813.529411764706,
      "grad_norm": 23.824344635009766,
      "learning_rate": 2.1864705882352942e-05,
      "loss": 0.946,
      "step": 47830
    },
    {
      "epoch": 2814.1176470588234,
      "grad_norm": 21.629499435424805,
      "learning_rate": 2.1858823529411765e-05,
      "loss": 0.9888,
      "step": 47840
    },
    {
      "epoch": 2814.705882352941,
      "grad_norm": 25.01414680480957,
      "learning_rate": 2.185294117647059e-05,
      "loss": 1.0296,
      "step": 47850
    },
    {
      "epoch": 2815.294117647059,
      "grad_norm": 16.276155471801758,
      "learning_rate": 2.184705882352941e-05,
      "loss": 0.8875,
      "step": 47860
    },
    {
      "epoch": 2815.8823529411766,
      "grad_norm": 19.002548217773438,
      "learning_rate": 2.1841176470588237e-05,
      "loss": 1.0179,
      "step": 47870
    },
    {
      "epoch": 2816.470588235294,
      "grad_norm": 18.679582595825195,
      "learning_rate": 2.183529411764706e-05,
      "loss": 0.944,
      "step": 47880
    },
    {
      "epoch": 2817.0588235294117,
      "grad_norm": 17.728946685791016,
      "learning_rate": 2.1829411764705883e-05,
      "loss": 0.949,
      "step": 47890
    },
    {
      "epoch": 2817.6470588235293,
      "grad_norm": 21.2122859954834,
      "learning_rate": 2.1823529411764706e-05,
      "loss": 0.9746,
      "step": 47900
    },
    {
      "epoch": 2818.235294117647,
      "grad_norm": 25.63620376586914,
      "learning_rate": 2.1817647058823532e-05,
      "loss": 0.8701,
      "step": 47910
    },
    {
      "epoch": 2818.823529411765,
      "grad_norm": 19.052034378051758,
      "learning_rate": 2.1811764705882355e-05,
      "loss": 0.9837,
      "step": 47920
    },
    {
      "epoch": 2819.4117647058824,
      "grad_norm": 20.210342407226562,
      "learning_rate": 2.180588235294118e-05,
      "loss": 1.0644,
      "step": 47930
    },
    {
      "epoch": 2820.0,
      "grad_norm": 26.211753845214844,
      "learning_rate": 2.18e-05,
      "loss": 0.9683,
      "step": 47940
    },
    {
      "epoch": 2820.5882352941176,
      "grad_norm": 24.8303279876709,
      "learning_rate": 2.1794117647058824e-05,
      "loss": 1.0725,
      "step": 47950
    },
    {
      "epoch": 2821.176470588235,
      "grad_norm": 25.832290649414062,
      "learning_rate": 2.1788235294117647e-05,
      "loss": 0.9548,
      "step": 47960
    },
    {
      "epoch": 2821.764705882353,
      "grad_norm": 18.590713500976562,
      "learning_rate": 2.178235294117647e-05,
      "loss": 1.0897,
      "step": 47970
    },
    {
      "epoch": 2822.3529411764707,
      "grad_norm": 17.92982292175293,
      "learning_rate": 2.1776470588235297e-05,
      "loss": 1.0043,
      "step": 47980
    },
    {
      "epoch": 2822.9411764705883,
      "grad_norm": 24.594404220581055,
      "learning_rate": 2.1770588235294116e-05,
      "loss": 0.9469,
      "step": 47990
    },
    {
      "epoch": 2823.529411764706,
      "grad_norm": 16.401060104370117,
      "learning_rate": 2.1764705882352943e-05,
      "loss": 1.0021,
      "step": 48000
    },
    {
      "epoch": 2824.1176470588234,
      "grad_norm": 23.308656692504883,
      "learning_rate": 2.1758823529411766e-05,
      "loss": 0.9751,
      "step": 48010
    },
    {
      "epoch": 2824.705882352941,
      "grad_norm": 22.988080978393555,
      "learning_rate": 2.175294117647059e-05,
      "loss": 1.013,
      "step": 48020
    },
    {
      "epoch": 2825.294117647059,
      "grad_norm": 25.012266159057617,
      "learning_rate": 2.174705882352941e-05,
      "loss": 0.9729,
      "step": 48030
    },
    {
      "epoch": 2825.8823529411766,
      "grad_norm": 20.000696182250977,
      "learning_rate": 2.1741176470588238e-05,
      "loss": 0.9722,
      "step": 48040
    },
    {
      "epoch": 2826.470588235294,
      "grad_norm": 18.209062576293945,
      "learning_rate": 2.1735294117647058e-05,
      "loss": 1.0801,
      "step": 48050
    },
    {
      "epoch": 2827.0588235294117,
      "grad_norm": 18.312776565551758,
      "learning_rate": 2.1729411764705884e-05,
      "loss": 0.9642,
      "step": 48060
    },
    {
      "epoch": 2827.6470588235293,
      "grad_norm": 28.584253311157227,
      "learning_rate": 2.1723529411764707e-05,
      "loss": 1.0314,
      "step": 48070
    },
    {
      "epoch": 2828.235294117647,
      "grad_norm": 17.074522018432617,
      "learning_rate": 2.171764705882353e-05,
      "loss": 1.0013,
      "step": 48080
    },
    {
      "epoch": 2828.823529411765,
      "grad_norm": 25.97003746032715,
      "learning_rate": 2.1711764705882353e-05,
      "loss": 1.1098,
      "step": 48090
    },
    {
      "epoch": 2829.4117647058824,
      "grad_norm": 12.197793006896973,
      "learning_rate": 2.170588235294118e-05,
      "loss": 0.9694,
      "step": 48100
    },
    {
      "epoch": 2830.0,
      "grad_norm": 33.444393157958984,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 1.0672,
      "step": 48110
    },
    {
      "epoch": 2830.5882352941176,
      "grad_norm": 22.0842342376709,
      "learning_rate": 2.1694117647058825e-05,
      "loss": 0.9576,
      "step": 48120
    },
    {
      "epoch": 2831.176470588235,
      "grad_norm": 19.027864456176758,
      "learning_rate": 2.1688235294117648e-05,
      "loss": 1.0025,
      "step": 48130
    },
    {
      "epoch": 2831.764705882353,
      "grad_norm": 15.970908164978027,
      "learning_rate": 2.168235294117647e-05,
      "loss": 0.9696,
      "step": 48140
    },
    {
      "epoch": 2832.3529411764707,
      "grad_norm": 25.297426223754883,
      "learning_rate": 2.1676470588235294e-05,
      "loss": 0.8976,
      "step": 48150
    },
    {
      "epoch": 2832.9411764705883,
      "grad_norm": 19.871551513671875,
      "learning_rate": 2.1670588235294117e-05,
      "loss": 1.0171,
      "step": 48160
    },
    {
      "epoch": 2833.529411764706,
      "grad_norm": 20.09730339050293,
      "learning_rate": 2.1664705882352944e-05,
      "loss": 1.1612,
      "step": 48170
    },
    {
      "epoch": 2834.1176470588234,
      "grad_norm": 21.801380157470703,
      "learning_rate": 2.1658823529411763e-05,
      "loss": 0.9401,
      "step": 48180
    },
    {
      "epoch": 2834.705882352941,
      "grad_norm": 18.842853546142578,
      "learning_rate": 2.165294117647059e-05,
      "loss": 0.9485,
      "step": 48190
    },
    {
      "epoch": 2835.294117647059,
      "grad_norm": 20.648784637451172,
      "learning_rate": 2.1647058823529413e-05,
      "loss": 0.9838,
      "step": 48200
    },
    {
      "epoch": 2835.8823529411766,
      "grad_norm": 18.697856903076172,
      "learning_rate": 2.1641176470588236e-05,
      "loss": 0.8771,
      "step": 48210
    },
    {
      "epoch": 2836.470588235294,
      "grad_norm": 20.193147659301758,
      "learning_rate": 2.163529411764706e-05,
      "loss": 0.9984,
      "step": 48220
    },
    {
      "epoch": 2837.0588235294117,
      "grad_norm": 23.246328353881836,
      "learning_rate": 2.1629411764705885e-05,
      "loss": 1.0243,
      "step": 48230
    },
    {
      "epoch": 2837.6470588235293,
      "grad_norm": 15.378270149230957,
      "learning_rate": 2.1623529411764708e-05,
      "loss": 0.9604,
      "step": 48240
    },
    {
      "epoch": 2838.235294117647,
      "grad_norm": 22.709739685058594,
      "learning_rate": 2.161764705882353e-05,
      "loss": 0.9855,
      "step": 48250
    },
    {
      "epoch": 2838.823529411765,
      "grad_norm": 26.465576171875,
      "learning_rate": 2.1611764705882354e-05,
      "loss": 1.11,
      "step": 48260
    },
    {
      "epoch": 2839.4117647058824,
      "grad_norm": 29.483983993530273,
      "learning_rate": 2.1605882352941177e-05,
      "loss": 1.0451,
      "step": 48270
    },
    {
      "epoch": 2840.0,
      "grad_norm": 30.60151481628418,
      "learning_rate": 2.16e-05,
      "loss": 0.8825,
      "step": 48280
    },
    {
      "epoch": 2840.5882352941176,
      "grad_norm": 20.30644989013672,
      "learning_rate": 2.1594117647058826e-05,
      "loss": 0.9321,
      "step": 48290
    },
    {
      "epoch": 2841.176470588235,
      "grad_norm": 18.133546829223633,
      "learning_rate": 2.158823529411765e-05,
      "loss": 0.9816,
      "step": 48300
    },
    {
      "epoch": 2841.764705882353,
      "grad_norm": 22.054868698120117,
      "learning_rate": 2.1582352941176472e-05,
      "loss": 1.0183,
      "step": 48310
    },
    {
      "epoch": 2842.3529411764707,
      "grad_norm": 21.727968215942383,
      "learning_rate": 2.1576470588235295e-05,
      "loss": 0.9951,
      "step": 48320
    },
    {
      "epoch": 2842.9411764705883,
      "grad_norm": 25.93551254272461,
      "learning_rate": 2.1570588235294118e-05,
      "loss": 1.0557,
      "step": 48330
    },
    {
      "epoch": 2843.529411764706,
      "grad_norm": 19.561450958251953,
      "learning_rate": 2.156470588235294e-05,
      "loss": 0.8653,
      "step": 48340
    },
    {
      "epoch": 2844.1176470588234,
      "grad_norm": 24.69135856628418,
      "learning_rate": 2.1558823529411764e-05,
      "loss": 0.9599,
      "step": 48350
    },
    {
      "epoch": 2844.705882352941,
      "grad_norm": 22.609119415283203,
      "learning_rate": 2.155294117647059e-05,
      "loss": 0.9137,
      "step": 48360
    },
    {
      "epoch": 2845.294117647059,
      "grad_norm": 21.791446685791016,
      "learning_rate": 2.1547058823529414e-05,
      "loss": 1.0036,
      "step": 48370
    },
    {
      "epoch": 2845.8823529411766,
      "grad_norm": 18.2014102935791,
      "learning_rate": 2.1541176470588237e-05,
      "loss": 0.9323,
      "step": 48380
    },
    {
      "epoch": 2846.470588235294,
      "grad_norm": 20.398632049560547,
      "learning_rate": 2.153529411764706e-05,
      "loss": 0.9896,
      "step": 48390
    },
    {
      "epoch": 2847.0588235294117,
      "grad_norm": 23.213590621948242,
      "learning_rate": 2.1529411764705882e-05,
      "loss": 1.0879,
      "step": 48400
    },
    {
      "epoch": 2847.6470588235293,
      "grad_norm": 22.22688102722168,
      "learning_rate": 2.1523529411764705e-05,
      "loss": 1.177,
      "step": 48410
    },
    {
      "epoch": 2848.235294117647,
      "grad_norm": 22.620582580566406,
      "learning_rate": 2.1517647058823532e-05,
      "loss": 1.0834,
      "step": 48420
    },
    {
      "epoch": 2848.823529411765,
      "grad_norm": 23.32844352722168,
      "learning_rate": 2.1511764705882355e-05,
      "loss": 1.0302,
      "step": 48430
    },
    {
      "epoch": 2849.4117647058824,
      "grad_norm": 22.659883499145508,
      "learning_rate": 2.1505882352941178e-05,
      "loss": 0.9765,
      "step": 48440
    },
    {
      "epoch": 2850.0,
      "grad_norm": 30.211252212524414,
      "learning_rate": 2.15e-05,
      "loss": 1.0372,
      "step": 48450
    },
    {
      "epoch": 2850.5882352941176,
      "grad_norm": 17.42494010925293,
      "learning_rate": 2.1494117647058827e-05,
      "loss": 1.0366,
      "step": 48460
    },
    {
      "epoch": 2851.176470588235,
      "grad_norm": 17.449195861816406,
      "learning_rate": 2.1488235294117647e-05,
      "loss": 0.9618,
      "step": 48470
    },
    {
      "epoch": 2851.764705882353,
      "grad_norm": 22.401750564575195,
      "learning_rate": 2.1482352941176473e-05,
      "loss": 1.0161,
      "step": 48480
    },
    {
      "epoch": 2852.3529411764707,
      "grad_norm": 18.048538208007812,
      "learning_rate": 2.1476470588235296e-05,
      "loss": 1.0194,
      "step": 48490
    },
    {
      "epoch": 2852.9411764705883,
      "grad_norm": 20.491069793701172,
      "learning_rate": 2.1470588235294116e-05,
      "loss": 0.9641,
      "step": 48500
    },
    {
      "epoch": 2853.529411764706,
      "grad_norm": 23.974790573120117,
      "learning_rate": 2.1464705882352942e-05,
      "loss": 1.0313,
      "step": 48510
    },
    {
      "epoch": 2854.1176470588234,
      "grad_norm": 23.676673889160156,
      "learning_rate": 2.1458823529411765e-05,
      "loss": 0.8724,
      "step": 48520
    },
    {
      "epoch": 2854.705882352941,
      "grad_norm": 16.62413215637207,
      "learning_rate": 2.1452941176470588e-05,
      "loss": 0.9195,
      "step": 48530
    },
    {
      "epoch": 2855.294117647059,
      "grad_norm": 16.64291000366211,
      "learning_rate": 2.144705882352941e-05,
      "loss": 1.011,
      "step": 48540
    },
    {
      "epoch": 2855.8823529411766,
      "grad_norm": 17.830842971801758,
      "learning_rate": 2.1441176470588237e-05,
      "loss": 0.9638,
      "step": 48550
    },
    {
      "epoch": 2856.470588235294,
      "grad_norm": 19.471435546875,
      "learning_rate": 2.143529411764706e-05,
      "loss": 0.9986,
      "step": 48560
    },
    {
      "epoch": 2857.0588235294117,
      "grad_norm": 21.199661254882812,
      "learning_rate": 2.1429411764705883e-05,
      "loss": 0.9009,
      "step": 48570
    },
    {
      "epoch": 2857.6470588235293,
      "grad_norm": 16.665546417236328,
      "learning_rate": 2.1423529411764706e-05,
      "loss": 0.9325,
      "step": 48580
    },
    {
      "epoch": 2858.235294117647,
      "grad_norm": 19.40860366821289,
      "learning_rate": 2.1417647058823533e-05,
      "loss": 1.0259,
      "step": 48590
    },
    {
      "epoch": 2858.823529411765,
      "grad_norm": 16.1221923828125,
      "learning_rate": 2.1411764705882352e-05,
      "loss": 0.942,
      "step": 48600
    },
    {
      "epoch": 2859.4117647058824,
      "grad_norm": 16.182729721069336,
      "learning_rate": 2.140588235294118e-05,
      "loss": 0.9646,
      "step": 48610
    },
    {
      "epoch": 2860.0,
      "grad_norm": 21.914241790771484,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.9447,
      "step": 48620
    },
    {
      "epoch": 2860.5882352941176,
      "grad_norm": 23.662914276123047,
      "learning_rate": 2.1394117647058825e-05,
      "loss": 0.98,
      "step": 48630
    },
    {
      "epoch": 2861.176470588235,
      "grad_norm": 16.185237884521484,
      "learning_rate": 2.1388235294117648e-05,
      "loss": 0.9016,
      "step": 48640
    },
    {
      "epoch": 2861.764705882353,
      "grad_norm": 22.493581771850586,
      "learning_rate": 2.1382352941176474e-05,
      "loss": 0.9451,
      "step": 48650
    },
    {
      "epoch": 2862.3529411764707,
      "grad_norm": 20.180957794189453,
      "learning_rate": 2.1376470588235294e-05,
      "loss": 1.0776,
      "step": 48660
    },
    {
      "epoch": 2862.9411764705883,
      "grad_norm": 18.3342342376709,
      "learning_rate": 2.137058823529412e-05,
      "loss": 0.9831,
      "step": 48670
    },
    {
      "epoch": 2863.529411764706,
      "grad_norm": 18.785198211669922,
      "learning_rate": 2.1364705882352943e-05,
      "loss": 0.9964,
      "step": 48680
    },
    {
      "epoch": 2864.1176470588234,
      "grad_norm": 17.55327033996582,
      "learning_rate": 2.1358823529411766e-05,
      "loss": 0.9099,
      "step": 48690
    },
    {
      "epoch": 2864.705882352941,
      "grad_norm": 18.618579864501953,
      "learning_rate": 2.135294117647059e-05,
      "loss": 0.9456,
      "step": 48700
    },
    {
      "epoch": 2865.294117647059,
      "grad_norm": 18.85904884338379,
      "learning_rate": 2.1347058823529412e-05,
      "loss": 0.9751,
      "step": 48710
    },
    {
      "epoch": 2865.8823529411766,
      "grad_norm": 15.416664123535156,
      "learning_rate": 2.1341176470588235e-05,
      "loss": 1.0333,
      "step": 48720
    },
    {
      "epoch": 2866.470588235294,
      "grad_norm": 25.96405792236328,
      "learning_rate": 2.1335294117647058e-05,
      "loss": 0.9026,
      "step": 48730
    },
    {
      "epoch": 2867.0588235294117,
      "grad_norm": 21.473583221435547,
      "learning_rate": 2.1329411764705884e-05,
      "loss": 0.9293,
      "step": 48740
    },
    {
      "epoch": 2867.6470588235293,
      "grad_norm": 30.409292221069336,
      "learning_rate": 2.1323529411764707e-05,
      "loss": 0.951,
      "step": 48750
    },
    {
      "epoch": 2868.235294117647,
      "grad_norm": 20.02396011352539,
      "learning_rate": 2.131764705882353e-05,
      "loss": 1.0322,
      "step": 48760
    },
    {
      "epoch": 2868.823529411765,
      "grad_norm": 17.17321014404297,
      "learning_rate": 2.1311764705882353e-05,
      "loss": 1.074,
      "step": 48770
    },
    {
      "epoch": 2869.4117647058824,
      "grad_norm": 17.321346282958984,
      "learning_rate": 2.130588235294118e-05,
      "loss": 0.9735,
      "step": 48780
    },
    {
      "epoch": 2870.0,
      "grad_norm": 26.214746475219727,
      "learning_rate": 2.13e-05,
      "loss": 1.0779,
      "step": 48790
    },
    {
      "epoch": 2870.5882352941176,
      "grad_norm": 17.494224548339844,
      "learning_rate": 2.1294117647058826e-05,
      "loss": 0.9368,
      "step": 48800
    },
    {
      "epoch": 2871.176470588235,
      "grad_norm": 25.61102294921875,
      "learning_rate": 2.128823529411765e-05,
      "loss": 1.0745,
      "step": 48810
    },
    {
      "epoch": 2871.764705882353,
      "grad_norm": 18.636621475219727,
      "learning_rate": 2.1282352941176472e-05,
      "loss": 0.9865,
      "step": 48820
    },
    {
      "epoch": 2872.3529411764707,
      "grad_norm": 17.416086196899414,
      "learning_rate": 2.1276470588235295e-05,
      "loss": 0.9231,
      "step": 48830
    },
    {
      "epoch": 2872.9411764705883,
      "grad_norm": 19.37797737121582,
      "learning_rate": 2.127058823529412e-05,
      "loss": 1.1025,
      "step": 48840
    },
    {
      "epoch": 2873.529411764706,
      "grad_norm": 27.850139617919922,
      "learning_rate": 2.126470588235294e-05,
      "loss": 1.0316,
      "step": 48850
    },
    {
      "epoch": 2874.1176470588234,
      "grad_norm": 19.499584197998047,
      "learning_rate": 2.1258823529411764e-05,
      "loss": 1.0517,
      "step": 48860
    },
    {
      "epoch": 2874.705882352941,
      "grad_norm": 19.85467529296875,
      "learning_rate": 2.125294117647059e-05,
      "loss": 1.0609,
      "step": 48870
    },
    {
      "epoch": 2875.294117647059,
      "grad_norm": 15.13227367401123,
      "learning_rate": 2.1247058823529413e-05,
      "loss": 0.8472,
      "step": 48880
    },
    {
      "epoch": 2875.8823529411766,
      "grad_norm": 22.139362335205078,
      "learning_rate": 2.1241176470588236e-05,
      "loss": 1.0301,
      "step": 48890
    },
    {
      "epoch": 2876.470588235294,
      "grad_norm": 26.78314208984375,
      "learning_rate": 2.123529411764706e-05,
      "loss": 1.0528,
      "step": 48900
    },
    {
      "epoch": 2877.0588235294117,
      "grad_norm": 19.73822593688965,
      "learning_rate": 2.1229411764705885e-05,
      "loss": 1.0296,
      "step": 48910
    },
    {
      "epoch": 2877.6470588235293,
      "grad_norm": 25.664913177490234,
      "learning_rate": 2.1223529411764705e-05,
      "loss": 1.0181,
      "step": 48920
    },
    {
      "epoch": 2878.235294117647,
      "grad_norm": 28.04550552368164,
      "learning_rate": 2.121764705882353e-05,
      "loss": 0.9497,
      "step": 48930
    },
    {
      "epoch": 2878.823529411765,
      "grad_norm": 15.849635124206543,
      "learning_rate": 2.1211764705882354e-05,
      "loss": 0.9792,
      "step": 48940
    },
    {
      "epoch": 2879.4117647058824,
      "grad_norm": 15.746359825134277,
      "learning_rate": 2.1205882352941177e-05,
      "loss": 0.9824,
      "step": 48950
    },
    {
      "epoch": 2880.0,
      "grad_norm": 21.97227668762207,
      "learning_rate": 2.12e-05,
      "loss": 1.0323,
      "step": 48960
    },
    {
      "epoch": 2880.5882352941176,
      "grad_norm": 19.82682991027832,
      "learning_rate": 2.1194117647058827e-05,
      "loss": 0.9021,
      "step": 48970
    },
    {
      "epoch": 2881.176470588235,
      "grad_norm": 24.927568435668945,
      "learning_rate": 2.1188235294117646e-05,
      "loss": 1.0166,
      "step": 48980
    },
    {
      "epoch": 2881.764705882353,
      "grad_norm": 16.574323654174805,
      "learning_rate": 2.1182352941176473e-05,
      "loss": 0.9522,
      "step": 48990
    },
    {
      "epoch": 2882.3529411764707,
      "grad_norm": 17.930997848510742,
      "learning_rate": 2.1176470588235296e-05,
      "loss": 0.915,
      "step": 49000
    },
    {
      "epoch": 2882.9411764705883,
      "grad_norm": 25.06996726989746,
      "learning_rate": 2.117058823529412e-05,
      "loss": 0.9621,
      "step": 49010
    },
    {
      "epoch": 2883.529411764706,
      "grad_norm": 25.747406005859375,
      "learning_rate": 2.116470588235294e-05,
      "loss": 0.8563,
      "step": 49020
    },
    {
      "epoch": 2884.1176470588234,
      "grad_norm": 19.70223045349121,
      "learning_rate": 2.1158823529411768e-05,
      "loss": 0.998,
      "step": 49030
    },
    {
      "epoch": 2884.705882352941,
      "grad_norm": 19.983251571655273,
      "learning_rate": 2.1152941176470588e-05,
      "loss": 1.0698,
      "step": 49040
    },
    {
      "epoch": 2885.294117647059,
      "grad_norm": 22.565887451171875,
      "learning_rate": 2.114705882352941e-05,
      "loss": 0.9952,
      "step": 49050
    },
    {
      "epoch": 2885.8823529411766,
      "grad_norm": 19.713851928710938,
      "learning_rate": 2.1141176470588237e-05,
      "loss": 0.9406,
      "step": 49060
    },
    {
      "epoch": 2886.470588235294,
      "grad_norm": 18.357879638671875,
      "learning_rate": 2.113529411764706e-05,
      "loss": 0.9095,
      "step": 49070
    },
    {
      "epoch": 2887.0588235294117,
      "grad_norm": 21.533349990844727,
      "learning_rate": 2.1129411764705883e-05,
      "loss": 1.049,
      "step": 49080
    },
    {
      "epoch": 2887.6470588235293,
      "grad_norm": 25.836931228637695,
      "learning_rate": 2.1123529411764706e-05,
      "loss": 0.9993,
      "step": 49090
    },
    {
      "epoch": 2888.235294117647,
      "grad_norm": 20.28657341003418,
      "learning_rate": 2.1117647058823532e-05,
      "loss": 1.0768,
      "step": 49100
    },
    {
      "epoch": 2888.823529411765,
      "grad_norm": 17.039243698120117,
      "learning_rate": 2.1111764705882352e-05,
      "loss": 0.9077,
      "step": 49110
    },
    {
      "epoch": 2889.4117647058824,
      "grad_norm": 20.188180923461914,
      "learning_rate": 2.1105882352941178e-05,
      "loss": 0.9046,
      "step": 49120
    },
    {
      "epoch": 2890.0,
      "grad_norm": 22.981969833374023,
      "learning_rate": 2.11e-05,
      "loss": 0.9616,
      "step": 49130
    },
    {
      "epoch": 2890.5882352941176,
      "grad_norm": 19.25693702697754,
      "learning_rate": 2.1094117647058824e-05,
      "loss": 1.0047,
      "step": 49140
    },
    {
      "epoch": 2891.176470588235,
      "grad_norm": 23.327411651611328,
      "learning_rate": 2.1088235294117647e-05,
      "loss": 0.9606,
      "step": 49150
    },
    {
      "epoch": 2891.764705882353,
      "grad_norm": 18.957035064697266,
      "learning_rate": 2.1082352941176474e-05,
      "loss": 0.9727,
      "step": 49160
    },
    {
      "epoch": 2892.3529411764707,
      "grad_norm": 25.75652503967285,
      "learning_rate": 2.1076470588235293e-05,
      "loss": 1.0781,
      "step": 49170
    },
    {
      "epoch": 2892.9411764705883,
      "grad_norm": 15.617071151733398,
      "learning_rate": 2.107058823529412e-05,
      "loss": 1.0771,
      "step": 49180
    },
    {
      "epoch": 2893.529411764706,
      "grad_norm": 33.00416946411133,
      "learning_rate": 2.1064705882352943e-05,
      "loss": 0.9139,
      "step": 49190
    },
    {
      "epoch": 2894.1176470588234,
      "grad_norm": 26.014265060424805,
      "learning_rate": 2.1058823529411766e-05,
      "loss": 0.9558,
      "step": 49200
    },
    {
      "epoch": 2894.705882352941,
      "grad_norm": 24.23310089111328,
      "learning_rate": 2.105294117647059e-05,
      "loss": 0.9827,
      "step": 49210
    },
    {
      "epoch": 2895.294117647059,
      "grad_norm": 23.153026580810547,
      "learning_rate": 2.1047058823529415e-05,
      "loss": 0.946,
      "step": 49220
    },
    {
      "epoch": 2895.8823529411766,
      "grad_norm": 17.616336822509766,
      "learning_rate": 2.1041176470588238e-05,
      "loss": 0.9869,
      "step": 49230
    },
    {
      "epoch": 2896.470588235294,
      "grad_norm": 24.461050033569336,
      "learning_rate": 2.1035294117647058e-05,
      "loss": 0.8467,
      "step": 49240
    },
    {
      "epoch": 2897.0588235294117,
      "grad_norm": 21.591642379760742,
      "learning_rate": 2.1029411764705884e-05,
      "loss": 0.9653,
      "step": 49250
    },
    {
      "epoch": 2897.6470588235293,
      "grad_norm": 20.443645477294922,
      "learning_rate": 2.1023529411764707e-05,
      "loss": 1.0258,
      "step": 49260
    },
    {
      "epoch": 2898.235294117647,
      "grad_norm": 21.10272216796875,
      "learning_rate": 2.101764705882353e-05,
      "loss": 1.0389,
      "step": 49270
    },
    {
      "epoch": 2898.823529411765,
      "grad_norm": 20.96367835998535,
      "learning_rate": 2.1011764705882353e-05,
      "loss": 0.8993,
      "step": 49280
    },
    {
      "epoch": 2899.4117647058824,
      "grad_norm": 22.969528198242188,
      "learning_rate": 2.100588235294118e-05,
      "loss": 1.0252,
      "step": 49290
    },
    {
      "epoch": 2900.0,
      "grad_norm": 29.27279281616211,
      "learning_rate": 2.1e-05,
      "loss": 0.9108,
      "step": 49300
    },
    {
      "epoch": 2900.5882352941176,
      "grad_norm": 15.91678524017334,
      "learning_rate": 2.0994117647058825e-05,
      "loss": 0.8802,
      "step": 49310
    },
    {
      "epoch": 2901.176470588235,
      "grad_norm": 23.87656593322754,
      "learning_rate": 2.0988235294117648e-05,
      "loss": 0.8881,
      "step": 49320
    },
    {
      "epoch": 2901.764705882353,
      "grad_norm": 35.83978271484375,
      "learning_rate": 2.098235294117647e-05,
      "loss": 1.0202,
      "step": 49330
    },
    {
      "epoch": 2902.3529411764707,
      "grad_norm": 19.224023818969727,
      "learning_rate": 2.0976470588235294e-05,
      "loss": 0.9585,
      "step": 49340
    },
    {
      "epoch": 2902.9411764705883,
      "grad_norm": 23.392240524291992,
      "learning_rate": 2.097058823529412e-05,
      "loss": 0.8732,
      "step": 49350
    },
    {
      "epoch": 2903.529411764706,
      "grad_norm": 23.431428909301758,
      "learning_rate": 2.0964705882352944e-05,
      "loss": 1.0317,
      "step": 49360
    },
    {
      "epoch": 2904.1176470588234,
      "grad_norm": 22.134281158447266,
      "learning_rate": 2.0958823529411767e-05,
      "loss": 0.9917,
      "step": 49370
    },
    {
      "epoch": 2904.705882352941,
      "grad_norm": 18.019046783447266,
      "learning_rate": 2.095294117647059e-05,
      "loss": 0.8911,
      "step": 49380
    },
    {
      "epoch": 2905.294117647059,
      "grad_norm": 19.76167869567871,
      "learning_rate": 2.0947058823529413e-05,
      "loss": 1.0137,
      "step": 49390
    },
    {
      "epoch": 2905.8823529411766,
      "grad_norm": 20.571001052856445,
      "learning_rate": 2.0941176470588235e-05,
      "loss": 0.9926,
      "step": 49400
    },
    {
      "epoch": 2906.470588235294,
      "grad_norm": 24.4525089263916,
      "learning_rate": 2.093529411764706e-05,
      "loss": 0.9749,
      "step": 49410
    },
    {
      "epoch": 2907.0588235294117,
      "grad_norm": 14.613968849182129,
      "learning_rate": 2.0929411764705885e-05,
      "loss": 0.9329,
      "step": 49420
    },
    {
      "epoch": 2907.6470588235293,
      "grad_norm": 16.347610473632812,
      "learning_rate": 2.0923529411764704e-05,
      "loss": 0.967,
      "step": 49430
    },
    {
      "epoch": 2908.235294117647,
      "grad_norm": 17.50808334350586,
      "learning_rate": 2.091764705882353e-05,
      "loss": 0.902,
      "step": 49440
    },
    {
      "epoch": 2908.823529411765,
      "grad_norm": 22.08818244934082,
      "learning_rate": 2.0911764705882354e-05,
      "loss": 1.0448,
      "step": 49450
    },
    {
      "epoch": 2909.4117647058824,
      "grad_norm": 19.520919799804688,
      "learning_rate": 2.0905882352941177e-05,
      "loss": 0.9962,
      "step": 49460
    },
    {
      "epoch": 2910.0,
      "grad_norm": 17.05864906311035,
      "learning_rate": 2.09e-05,
      "loss": 1.0153,
      "step": 49470
    },
    {
      "epoch": 2910.5882352941176,
      "grad_norm": 14.151490211486816,
      "learning_rate": 2.0894117647058826e-05,
      "loss": 0.9212,
      "step": 49480
    },
    {
      "epoch": 2911.176470588235,
      "grad_norm": 19.010982513427734,
      "learning_rate": 2.0888235294117646e-05,
      "loss": 0.9475,
      "step": 49490
    },
    {
      "epoch": 2911.764705882353,
      "grad_norm": 24.3746337890625,
      "learning_rate": 2.0882352941176472e-05,
      "loss": 0.942,
      "step": 49500
    },
    {
      "epoch": 2912.3529411764707,
      "grad_norm": 22.740732192993164,
      "learning_rate": 2.0876470588235295e-05,
      "loss": 1.0568,
      "step": 49510
    },
    {
      "epoch": 2912.9411764705883,
      "grad_norm": 22.044191360473633,
      "learning_rate": 2.0870588235294118e-05,
      "loss": 0.9847,
      "step": 49520
    },
    {
      "epoch": 2913.529411764706,
      "grad_norm": 14.964698791503906,
      "learning_rate": 2.086470588235294e-05,
      "loss": 0.8739,
      "step": 49530
    },
    {
      "epoch": 2914.1176470588234,
      "grad_norm": 17.88096046447754,
      "learning_rate": 2.0858823529411767e-05,
      "loss": 0.9902,
      "step": 49540
    },
    {
      "epoch": 2914.705882352941,
      "grad_norm": 26.364131927490234,
      "learning_rate": 2.085294117647059e-05,
      "loss": 1.0668,
      "step": 49550
    },
    {
      "epoch": 2915.294117647059,
      "grad_norm": 27.86416244506836,
      "learning_rate": 2.0847058823529413e-05,
      "loss": 1.0189,
      "step": 49560
    },
    {
      "epoch": 2915.8823529411766,
      "grad_norm": 25.15541648864746,
      "learning_rate": 2.0841176470588236e-05,
      "loss": 0.8771,
      "step": 49570
    },
    {
      "epoch": 2916.470588235294,
      "grad_norm": 22.531150817871094,
      "learning_rate": 2.0835294117647063e-05,
      "loss": 0.9523,
      "step": 49580
    },
    {
      "epoch": 2917.0588235294117,
      "grad_norm": 21.498004913330078,
      "learning_rate": 2.0829411764705882e-05,
      "loss": 1.0047,
      "step": 49590
    },
    {
      "epoch": 2917.6470588235293,
      "grad_norm": 23.045320510864258,
      "learning_rate": 2.0823529411764705e-05,
      "loss": 0.9626,
      "step": 49600
    },
    {
      "epoch": 2918.235294117647,
      "grad_norm": 23.22955894470215,
      "learning_rate": 2.0817647058823532e-05,
      "loss": 1.0513,
      "step": 49610
    },
    {
      "epoch": 2918.823529411765,
      "grad_norm": 19.540111541748047,
      "learning_rate": 2.081176470588235e-05,
      "loss": 0.9942,
      "step": 49620
    },
    {
      "epoch": 2919.4117647058824,
      "grad_norm": 20.312803268432617,
      "learning_rate": 2.0805882352941178e-05,
      "loss": 1.1024,
      "step": 49630
    },
    {
      "epoch": 2920.0,
      "grad_norm": 24.91272735595703,
      "learning_rate": 2.08e-05,
      "loss": 0.9649,
      "step": 49640
    },
    {
      "epoch": 2920.5882352941176,
      "grad_norm": 20.840158462524414,
      "learning_rate": 2.0794117647058824e-05,
      "loss": 1.0322,
      "step": 49650
    },
    {
      "epoch": 2921.176470588235,
      "grad_norm": 17.81402587890625,
      "learning_rate": 2.0788235294117647e-05,
      "loss": 1.0319,
      "step": 49660
    },
    {
      "epoch": 2921.764705882353,
      "grad_norm": 18.829143524169922,
      "learning_rate": 2.0782352941176473e-05,
      "loss": 1.0256,
      "step": 49670
    },
    {
      "epoch": 2922.3529411764707,
      "grad_norm": 23.06839370727539,
      "learning_rate": 2.0776470588235296e-05,
      "loss": 1.045,
      "step": 49680
    },
    {
      "epoch": 2922.9411764705883,
      "grad_norm": 17.38434600830078,
      "learning_rate": 2.077058823529412e-05,
      "loss": 0.9456,
      "step": 49690
    },
    {
      "epoch": 2923.529411764706,
      "grad_norm": 18.001867294311523,
      "learning_rate": 2.0764705882352942e-05,
      "loss": 0.9066,
      "step": 49700
    },
    {
      "epoch": 2924.1176470588234,
      "grad_norm": 16.34397315979004,
      "learning_rate": 2.0758823529411765e-05,
      "loss": 1.0448,
      "step": 49710
    },
    {
      "epoch": 2924.705882352941,
      "grad_norm": 18.010160446166992,
      "learning_rate": 2.0752941176470588e-05,
      "loss": 0.9691,
      "step": 49720
    },
    {
      "epoch": 2925.294117647059,
      "grad_norm": 23.92428970336914,
      "learning_rate": 2.0747058823529414e-05,
      "loss": 0.9822,
      "step": 49730
    },
    {
      "epoch": 2925.8823529411766,
      "grad_norm": 20.562700271606445,
      "learning_rate": 2.0741176470588237e-05,
      "loss": 0.8801,
      "step": 49740
    },
    {
      "epoch": 2926.470588235294,
      "grad_norm": 21.49725341796875,
      "learning_rate": 2.073529411764706e-05,
      "loss": 0.9084,
      "step": 49750
    },
    {
      "epoch": 2927.0588235294117,
      "grad_norm": 15.80990219116211,
      "learning_rate": 2.0729411764705883e-05,
      "loss": 0.8852,
      "step": 49760
    },
    {
      "epoch": 2927.6470588235293,
      "grad_norm": 18.568952560424805,
      "learning_rate": 2.0723529411764706e-05,
      "loss": 0.9176,
      "step": 49770
    },
    {
      "epoch": 2928.235294117647,
      "grad_norm": 21.586942672729492,
      "learning_rate": 2.071764705882353e-05,
      "loss": 0.9825,
      "step": 49780
    },
    {
      "epoch": 2928.823529411765,
      "grad_norm": 20.840221405029297,
      "learning_rate": 2.0711764705882352e-05,
      "loss": 0.9138,
      "step": 49790
    },
    {
      "epoch": 2929.4117647058824,
      "grad_norm": 16.28948402404785,
      "learning_rate": 2.070588235294118e-05,
      "loss": 0.99,
      "step": 49800
    },
    {
      "epoch": 2930.0,
      "grad_norm": 24.579463958740234,
      "learning_rate": 2.07e-05,
      "loss": 0.9832,
      "step": 49810
    },
    {
      "epoch": 2930.5882352941176,
      "grad_norm": 17.893524169921875,
      "learning_rate": 2.0694117647058825e-05,
      "loss": 1.0544,
      "step": 49820
    },
    {
      "epoch": 2931.176470588235,
      "grad_norm": 20.135662078857422,
      "learning_rate": 2.0688235294117648e-05,
      "loss": 0.9825,
      "step": 49830
    },
    {
      "epoch": 2931.764705882353,
      "grad_norm": 22.49262046813965,
      "learning_rate": 2.068235294117647e-05,
      "loss": 0.943,
      "step": 49840
    },
    {
      "epoch": 2932.3529411764707,
      "grad_norm": 20.404010772705078,
      "learning_rate": 2.0676470588235294e-05,
      "loss": 1.0415,
      "step": 49850
    },
    {
      "epoch": 2932.9411764705883,
      "grad_norm": 19.87200164794922,
      "learning_rate": 2.067058823529412e-05,
      "loss": 0.922,
      "step": 49860
    },
    {
      "epoch": 2933.529411764706,
      "grad_norm": 20.82781982421875,
      "learning_rate": 2.0664705882352943e-05,
      "loss": 0.9292,
      "step": 49870
    },
    {
      "epoch": 2934.1176470588234,
      "grad_norm": 15.687366485595703,
      "learning_rate": 2.0658823529411766e-05,
      "loss": 0.8425,
      "step": 49880
    },
    {
      "epoch": 2934.705882352941,
      "grad_norm": 18.73416519165039,
      "learning_rate": 2.065294117647059e-05,
      "loss": 0.9521,
      "step": 49890
    },
    {
      "epoch": 2935.294117647059,
      "grad_norm": 19.65433120727539,
      "learning_rate": 2.0647058823529415e-05,
      "loss": 0.9993,
      "step": 49900
    },
    {
      "epoch": 2935.8823529411766,
      "grad_norm": 22.600669860839844,
      "learning_rate": 2.0641176470588235e-05,
      "loss": 0.8638,
      "step": 49910
    },
    {
      "epoch": 2936.470588235294,
      "grad_norm": 19.175935745239258,
      "learning_rate": 2.063529411764706e-05,
      "loss": 0.8925,
      "step": 49920
    },
    {
      "epoch": 2937.0588235294117,
      "grad_norm": 19.904083251953125,
      "learning_rate": 2.0629411764705884e-05,
      "loss": 0.9805,
      "step": 49930
    },
    {
      "epoch": 2937.6470588235293,
      "grad_norm": 29.049362182617188,
      "learning_rate": 2.0623529411764707e-05,
      "loss": 0.9038,
      "step": 49940
    },
    {
      "epoch": 2938.235294117647,
      "grad_norm": 18.746124267578125,
      "learning_rate": 2.061764705882353e-05,
      "loss": 0.8953,
      "step": 49950
    },
    {
      "epoch": 2938.823529411765,
      "grad_norm": 21.20207977294922,
      "learning_rate": 2.0611764705882353e-05,
      "loss": 0.92,
      "step": 49960
    },
    {
      "epoch": 2939.4117647058824,
      "grad_norm": 33.686981201171875,
      "learning_rate": 2.0605882352941176e-05,
      "loss": 0.9651,
      "step": 49970
    },
    {
      "epoch": 2940.0,
      "grad_norm": 20.735824584960938,
      "learning_rate": 2.06e-05,
      "loss": 0.9496,
      "step": 49980
    },
    {
      "epoch": 2940.5882352941176,
      "grad_norm": 22.762317657470703,
      "learning_rate": 2.0594117647058826e-05,
      "loss": 0.8998,
      "step": 49990
    },
    {
      "epoch": 2941.176470588235,
      "grad_norm": 22.222482681274414,
      "learning_rate": 2.058823529411765e-05,
      "loss": 0.9401,
      "step": 50000
    },
    {
      "epoch": 2941.764705882353,
      "grad_norm": 16.291624069213867,
      "learning_rate": 2.058235294117647e-05,
      "loss": 0.8879,
      "step": 50010
    },
    {
      "epoch": 2942.3529411764707,
      "grad_norm": 19.807239532470703,
      "learning_rate": 2.0576470588235295e-05,
      "loss": 0.8759,
      "step": 50020
    },
    {
      "epoch": 2942.9411764705883,
      "grad_norm": 31.59482765197754,
      "learning_rate": 2.0570588235294118e-05,
      "loss": 0.8706,
      "step": 50030
    },
    {
      "epoch": 2943.529411764706,
      "grad_norm": 15.559187889099121,
      "learning_rate": 2.056470588235294e-05,
      "loss": 0.8598,
      "step": 50040
    },
    {
      "epoch": 2944.1176470588234,
      "grad_norm": 24.67571258544922,
      "learning_rate": 2.0558823529411767e-05,
      "loss": 1.0506,
      "step": 50050
    },
    {
      "epoch": 2944.705882352941,
      "grad_norm": 19.225448608398438,
      "learning_rate": 2.055294117647059e-05,
      "loss": 0.9994,
      "step": 50060
    },
    {
      "epoch": 2945.294117647059,
      "grad_norm": 20.49274444580078,
      "learning_rate": 2.0547058823529413e-05,
      "loss": 0.8876,
      "step": 50070
    },
    {
      "epoch": 2945.8823529411766,
      "grad_norm": 18.57236671447754,
      "learning_rate": 2.0541176470588236e-05,
      "loss": 0.828,
      "step": 50080
    },
    {
      "epoch": 2946.470588235294,
      "grad_norm": 20.700918197631836,
      "learning_rate": 2.0535294117647062e-05,
      "loss": 0.9827,
      "step": 50090
    },
    {
      "epoch": 2947.0588235294117,
      "grad_norm": 27.298877716064453,
      "learning_rate": 2.0529411764705882e-05,
      "loss": 0.979,
      "step": 50100
    },
    {
      "epoch": 2947.6470588235293,
      "grad_norm": 23.137605667114258,
      "learning_rate": 2.0523529411764708e-05,
      "loss": 0.9339,
      "step": 50110
    },
    {
      "epoch": 2948.235294117647,
      "grad_norm": 22.037185668945312,
      "learning_rate": 2.051764705882353e-05,
      "loss": 0.9554,
      "step": 50120
    },
    {
      "epoch": 2948.823529411765,
      "grad_norm": 20.53981590270996,
      "learning_rate": 2.051176470588235e-05,
      "loss": 0.9006,
      "step": 50130
    },
    {
      "epoch": 2949.4117647058824,
      "grad_norm": 25.602252960205078,
      "learning_rate": 2.0505882352941177e-05,
      "loss": 0.9678,
      "step": 50140
    },
    {
      "epoch": 2950.0,
      "grad_norm": 26.58465576171875,
      "learning_rate": 2.05e-05,
      "loss": 0.9539,
      "step": 50150
    },
    {
      "epoch": 2950.5882352941176,
      "grad_norm": 20.295127868652344,
      "learning_rate": 2.0494117647058823e-05,
      "loss": 0.9384,
      "step": 50160
    },
    {
      "epoch": 2951.176470588235,
      "grad_norm": 23.279279708862305,
      "learning_rate": 2.0488235294117646e-05,
      "loss": 0.9272,
      "step": 50170
    },
    {
      "epoch": 2951.764705882353,
      "grad_norm": 19.1202335357666,
      "learning_rate": 2.0482352941176473e-05,
      "loss": 0.8327,
      "step": 50180
    },
    {
      "epoch": 2952.3529411764707,
      "grad_norm": 17.98978042602539,
      "learning_rate": 2.0476470588235296e-05,
      "loss": 0.8618,
      "step": 50190
    },
    {
      "epoch": 2952.9411764705883,
      "grad_norm": 22.110288619995117,
      "learning_rate": 2.047058823529412e-05,
      "loss": 0.9548,
      "step": 50200
    },
    {
      "epoch": 2953.529411764706,
      "grad_norm": 20.296655654907227,
      "learning_rate": 2.046470588235294e-05,
      "loss": 0.8793,
      "step": 50210
    },
    {
      "epoch": 2954.1176470588234,
      "grad_norm": 21.532331466674805,
      "learning_rate": 2.0458823529411768e-05,
      "loss": 0.9558,
      "step": 50220
    },
    {
      "epoch": 2954.705882352941,
      "grad_norm": 21.103853225708008,
      "learning_rate": 2.0452941176470588e-05,
      "loss": 0.9153,
      "step": 50230
    },
    {
      "epoch": 2955.294117647059,
      "grad_norm": 19.97756004333496,
      "learning_rate": 2.0447058823529414e-05,
      "loss": 0.8628,
      "step": 50240
    },
    {
      "epoch": 2955.8823529411766,
      "grad_norm": 20.608301162719727,
      "learning_rate": 2.0441176470588237e-05,
      "loss": 0.9029,
      "step": 50250
    },
    {
      "epoch": 2956.470588235294,
      "grad_norm": 21.0615234375,
      "learning_rate": 2.043529411764706e-05,
      "loss": 0.9266,
      "step": 50260
    },
    {
      "epoch": 2957.0588235294117,
      "grad_norm": 19.022912979125977,
      "learning_rate": 2.0429411764705883e-05,
      "loss": 0.8544,
      "step": 50270
    },
    {
      "epoch": 2957.6470588235293,
      "grad_norm": 19.98887062072754,
      "learning_rate": 2.042352941176471e-05,
      "loss": 0.9964,
      "step": 50280
    },
    {
      "epoch": 2958.235294117647,
      "grad_norm": 17.719932556152344,
      "learning_rate": 2.041764705882353e-05,
      "loss": 1.0119,
      "step": 50290
    },
    {
      "epoch": 2958.823529411765,
      "grad_norm": 19.666378021240234,
      "learning_rate": 2.0411764705882355e-05,
      "loss": 0.9483,
      "step": 50300
    },
    {
      "epoch": 2959.4117647058824,
      "grad_norm": 19.646331787109375,
      "learning_rate": 2.0405882352941178e-05,
      "loss": 0.9257,
      "step": 50310
    },
    {
      "epoch": 2960.0,
      "grad_norm": 20.334999084472656,
      "learning_rate": 2.04e-05,
      "loss": 0.995,
      "step": 50320
    },
    {
      "epoch": 2960.5882352941176,
      "grad_norm": 22.02135467529297,
      "learning_rate": 2.0394117647058824e-05,
      "loss": 1.05,
      "step": 50330
    },
    {
      "epoch": 2961.176470588235,
      "grad_norm": 15.647875785827637,
      "learning_rate": 2.0388235294117647e-05,
      "loss": 0.963,
      "step": 50340
    },
    {
      "epoch": 2961.764705882353,
      "grad_norm": 22.516040802001953,
      "learning_rate": 2.0382352941176474e-05,
      "loss": 0.9682,
      "step": 50350
    },
    {
      "epoch": 2962.3529411764707,
      "grad_norm": 20.261478424072266,
      "learning_rate": 2.0376470588235293e-05,
      "loss": 0.9396,
      "step": 50360
    },
    {
      "epoch": 2962.9411764705883,
      "grad_norm": 19.433237075805664,
      "learning_rate": 2.037058823529412e-05,
      "loss": 0.9259,
      "step": 50370
    },
    {
      "epoch": 2963.529411764706,
      "grad_norm": 29.96476936340332,
      "learning_rate": 2.0364705882352943e-05,
      "loss": 1.0012,
      "step": 50380
    },
    {
      "epoch": 2964.1176470588234,
      "grad_norm": 21.734373092651367,
      "learning_rate": 2.0358823529411765e-05,
      "loss": 0.9749,
      "step": 50390
    },
    {
      "epoch": 2964.705882352941,
      "grad_norm": 18.859277725219727,
      "learning_rate": 2.035294117647059e-05,
      "loss": 0.9792,
      "step": 50400
    },
    {
      "epoch": 2965.294117647059,
      "grad_norm": 26.74829864501953,
      "learning_rate": 2.0347058823529415e-05,
      "loss": 0.9982,
      "step": 50410
    },
    {
      "epoch": 2965.8823529411766,
      "grad_norm": 25.928241729736328,
      "learning_rate": 2.0341176470588234e-05,
      "loss": 0.9636,
      "step": 50420
    },
    {
      "epoch": 2966.470588235294,
      "grad_norm": 23.698516845703125,
      "learning_rate": 2.033529411764706e-05,
      "loss": 1.0252,
      "step": 50430
    },
    {
      "epoch": 2967.0588235294117,
      "grad_norm": 23.985767364501953,
      "learning_rate": 2.0329411764705884e-05,
      "loss": 1.0319,
      "step": 50440
    },
    {
      "epoch": 2967.6470588235293,
      "grad_norm": 31.84969711303711,
      "learning_rate": 2.0323529411764707e-05,
      "loss": 0.8967,
      "step": 50450
    },
    {
      "epoch": 2968.235294117647,
      "grad_norm": 24.03097152709961,
      "learning_rate": 2.031764705882353e-05,
      "loss": 0.9484,
      "step": 50460
    },
    {
      "epoch": 2968.823529411765,
      "grad_norm": 26.573162078857422,
      "learning_rate": 2.0311764705882356e-05,
      "loss": 1.0034,
      "step": 50470
    },
    {
      "epoch": 2969.4117647058824,
      "grad_norm": 16.373720169067383,
      "learning_rate": 2.0305882352941176e-05,
      "loss": 0.9753,
      "step": 50480
    },
    {
      "epoch": 2970.0,
      "grad_norm": 32.420860290527344,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 1.1107,
      "step": 50490
    },
    {
      "epoch": 2970.5882352941176,
      "grad_norm": 25.09794807434082,
      "learning_rate": 2.0294117647058825e-05,
      "loss": 0.9219,
      "step": 50500
    },
    {
      "epoch": 2971.176470588235,
      "grad_norm": 20.190486907958984,
      "learning_rate": 2.0288235294117648e-05,
      "loss": 0.9071,
      "step": 50510
    },
    {
      "epoch": 2971.764705882353,
      "grad_norm": 21.205554962158203,
      "learning_rate": 2.028235294117647e-05,
      "loss": 0.9138,
      "step": 50520
    },
    {
      "epoch": 2972.3529411764707,
      "grad_norm": 23.843156814575195,
      "learning_rate": 2.0276470588235294e-05,
      "loss": 1.0329,
      "step": 50530
    },
    {
      "epoch": 2972.9411764705883,
      "grad_norm": 23.245725631713867,
      "learning_rate": 2.027058823529412e-05,
      "loss": 1.0049,
      "step": 50540
    },
    {
      "epoch": 2973.529411764706,
      "grad_norm": 27.050765991210938,
      "learning_rate": 2.026470588235294e-05,
      "loss": 0.8804,
      "step": 50550
    },
    {
      "epoch": 2974.1176470588234,
      "grad_norm": 25.790220260620117,
      "learning_rate": 2.0258823529411766e-05,
      "loss": 0.8715,
      "step": 50560
    },
    {
      "epoch": 2974.705882352941,
      "grad_norm": 22.15288734436035,
      "learning_rate": 2.025294117647059e-05,
      "loss": 1.0548,
      "step": 50570
    },
    {
      "epoch": 2975.294117647059,
      "grad_norm": 19.799360275268555,
      "learning_rate": 2.0247058823529412e-05,
      "loss": 0.8371,
      "step": 50580
    },
    {
      "epoch": 2975.8823529411766,
      "grad_norm": 21.99616813659668,
      "learning_rate": 2.0241176470588235e-05,
      "loss": 1.0017,
      "step": 50590
    },
    {
      "epoch": 2976.470588235294,
      "grad_norm": 19.528730392456055,
      "learning_rate": 2.0235294117647062e-05,
      "loss": 0.8751,
      "step": 50600
    },
    {
      "epoch": 2977.0588235294117,
      "grad_norm": 24.055068969726562,
      "learning_rate": 2.022941176470588e-05,
      "loss": 0.9959,
      "step": 50610
    },
    {
      "epoch": 2977.6470588235293,
      "grad_norm": 18.392284393310547,
      "learning_rate": 2.0223529411764708e-05,
      "loss": 0.9303,
      "step": 50620
    },
    {
      "epoch": 2978.235294117647,
      "grad_norm": 26.966777801513672,
      "learning_rate": 2.021764705882353e-05,
      "loss": 1.0481,
      "step": 50630
    },
    {
      "epoch": 2978.823529411765,
      "grad_norm": 17.46756362915039,
      "learning_rate": 2.0211764705882354e-05,
      "loss": 0.9483,
      "step": 50640
    },
    {
      "epoch": 2979.4117647058824,
      "grad_norm": 25.20537567138672,
      "learning_rate": 2.0205882352941177e-05,
      "loss": 0.9282,
      "step": 50650
    },
    {
      "epoch": 2980.0,
      "grad_norm": 19.534378051757812,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.9364,
      "step": 50660
    },
    {
      "epoch": 2980.5882352941176,
      "grad_norm": 18.16761016845703,
      "learning_rate": 2.0194117647058826e-05,
      "loss": 0.8827,
      "step": 50670
    },
    {
      "epoch": 2981.176470588235,
      "grad_norm": 15.187955856323242,
      "learning_rate": 2.0188235294117646e-05,
      "loss": 0.9104,
      "step": 50680
    },
    {
      "epoch": 2981.764705882353,
      "grad_norm": 23.44085121154785,
      "learning_rate": 2.0182352941176472e-05,
      "loss": 0.8361,
      "step": 50690
    },
    {
      "epoch": 2982.3529411764707,
      "grad_norm": 24.932222366333008,
      "learning_rate": 2.0176470588235295e-05,
      "loss": 0.8859,
      "step": 50700
    },
    {
      "epoch": 2982.9411764705883,
      "grad_norm": 16.452865600585938,
      "learning_rate": 2.0170588235294118e-05,
      "loss": 0.8853,
      "step": 50710
    },
    {
      "epoch": 2983.529411764706,
      "grad_norm": 18.73712158203125,
      "learning_rate": 2.016470588235294e-05,
      "loss": 0.7572,
      "step": 50720
    },
    {
      "epoch": 2984.1176470588234,
      "grad_norm": 25.117734909057617,
      "learning_rate": 2.0158823529411767e-05,
      "loss": 0.9606,
      "step": 50730
    },
    {
      "epoch": 2984.705882352941,
      "grad_norm": 21.89423179626465,
      "learning_rate": 2.0152941176470587e-05,
      "loss": 1.0258,
      "step": 50740
    },
    {
      "epoch": 2985.294117647059,
      "grad_norm": 19.546056747436523,
      "learning_rate": 2.0147058823529413e-05,
      "loss": 0.9241,
      "step": 50750
    },
    {
      "epoch": 2985.8823529411766,
      "grad_norm": 25.095550537109375,
      "learning_rate": 2.0141176470588236e-05,
      "loss": 0.94,
      "step": 50760
    },
    {
      "epoch": 2986.470588235294,
      "grad_norm": 23.196678161621094,
      "learning_rate": 2.013529411764706e-05,
      "loss": 0.8999,
      "step": 50770
    },
    {
      "epoch": 2987.0588235294117,
      "grad_norm": 26.110450744628906,
      "learning_rate": 2.0129411764705882e-05,
      "loss": 0.9894,
      "step": 50780
    },
    {
      "epoch": 2987.6470588235293,
      "grad_norm": 15.305912017822266,
      "learning_rate": 2.012352941176471e-05,
      "loss": 0.9004,
      "step": 50790
    },
    {
      "epoch": 2988.235294117647,
      "grad_norm": 25.547182083129883,
      "learning_rate": 2.011764705882353e-05,
      "loss": 1.0003,
      "step": 50800
    },
    {
      "epoch": 2988.823529411765,
      "grad_norm": 20.73438835144043,
      "learning_rate": 2.0111764705882355e-05,
      "loss": 0.945,
      "step": 50810
    },
    {
      "epoch": 2989.4117647058824,
      "grad_norm": 25.162403106689453,
      "learning_rate": 2.0105882352941178e-05,
      "loss": 0.9822,
      "step": 50820
    },
    {
      "epoch": 2990.0,
      "grad_norm": 20.295780181884766,
      "learning_rate": 2.01e-05,
      "loss": 0.946,
      "step": 50830
    },
    {
      "epoch": 2990.5882352941176,
      "grad_norm": 24.570178985595703,
      "learning_rate": 2.0094117647058824e-05,
      "loss": 0.9715,
      "step": 50840
    },
    {
      "epoch": 2991.176470588235,
      "grad_norm": 17.248926162719727,
      "learning_rate": 2.008823529411765e-05,
      "loss": 0.9083,
      "step": 50850
    },
    {
      "epoch": 2991.764705882353,
      "grad_norm": 19.980609893798828,
      "learning_rate": 2.0082352941176473e-05,
      "loss": 0.8457,
      "step": 50860
    },
    {
      "epoch": 2992.3529411764707,
      "grad_norm": 21.208908081054688,
      "learning_rate": 2.0076470588235293e-05,
      "loss": 0.9788,
      "step": 50870
    },
    {
      "epoch": 2992.9411764705883,
      "grad_norm": 25.31065559387207,
      "learning_rate": 2.007058823529412e-05,
      "loss": 0.9923,
      "step": 50880
    },
    {
      "epoch": 2993.529411764706,
      "grad_norm": 20.584396362304688,
      "learning_rate": 2.0064705882352942e-05,
      "loss": 0.9213,
      "step": 50890
    },
    {
      "epoch": 2994.1176470588234,
      "grad_norm": 25.032119750976562,
      "learning_rate": 2.0058823529411765e-05,
      "loss": 0.9978,
      "step": 50900
    },
    {
      "epoch": 2994.705882352941,
      "grad_norm": 23.524049758911133,
      "learning_rate": 2.0052941176470588e-05,
      "loss": 1.0067,
      "step": 50910
    },
    {
      "epoch": 2995.294117647059,
      "grad_norm": 19.860595703125,
      "learning_rate": 2.0047058823529414e-05,
      "loss": 0.9669,
      "step": 50920
    },
    {
      "epoch": 2995.8823529411766,
      "grad_norm": 18.469839096069336,
      "learning_rate": 2.0041176470588234e-05,
      "loss": 0.8673,
      "step": 50930
    },
    {
      "epoch": 2996.470588235294,
      "grad_norm": 19.396034240722656,
      "learning_rate": 2.003529411764706e-05,
      "loss": 0.9001,
      "step": 50940
    },
    {
      "epoch": 2997.0588235294117,
      "grad_norm": 23.60087013244629,
      "learning_rate": 2.0029411764705883e-05,
      "loss": 0.8918,
      "step": 50950
    },
    {
      "epoch": 2997.6470588235293,
      "grad_norm": 17.87488555908203,
      "learning_rate": 2.0023529411764706e-05,
      "loss": 0.9846,
      "step": 50960
    },
    {
      "epoch": 2998.235294117647,
      "grad_norm": 21.498943328857422,
      "learning_rate": 2.001764705882353e-05,
      "loss": 0.8711,
      "step": 50970
    },
    {
      "epoch": 2998.823529411765,
      "grad_norm": 20.650732040405273,
      "learning_rate": 2.0011764705882356e-05,
      "loss": 0.944,
      "step": 50980
    },
    {
      "epoch": 2999.4117647058824,
      "grad_norm": 17.05586051940918,
      "learning_rate": 2.000588235294118e-05,
      "loss": 0.8938,
      "step": 50990
    },
    {
      "epoch": 3000.0,
      "grad_norm": 19.947330474853516,
      "learning_rate": 2e-05,
      "loss": 0.9273,
      "step": 51000
    },
    {
      "epoch": 3000.5882352941176,
      "grad_norm": 23.1184024810791,
      "learning_rate": 1.9994117647058825e-05,
      "loss": 0.9058,
      "step": 51010
    },
    {
      "epoch": 3001.176470588235,
      "grad_norm": 24.563961029052734,
      "learning_rate": 1.9988235294117648e-05,
      "loss": 0.9773,
      "step": 51020
    },
    {
      "epoch": 3001.764705882353,
      "grad_norm": 19.757184982299805,
      "learning_rate": 1.998235294117647e-05,
      "loss": 1.0041,
      "step": 51030
    },
    {
      "epoch": 3002.3529411764707,
      "grad_norm": 17.091087341308594,
      "learning_rate": 1.9976470588235294e-05,
      "loss": 0.9672,
      "step": 51040
    },
    {
      "epoch": 3002.9411764705883,
      "grad_norm": 19.64203453063965,
      "learning_rate": 1.997058823529412e-05,
      "loss": 0.8938,
      "step": 51050
    },
    {
      "epoch": 3003.529411764706,
      "grad_norm": 16.281766891479492,
      "learning_rate": 1.996470588235294e-05,
      "loss": 0.9793,
      "step": 51060
    },
    {
      "epoch": 3004.1176470588234,
      "grad_norm": 20.466796875,
      "learning_rate": 1.9958823529411766e-05,
      "loss": 0.8754,
      "step": 51070
    },
    {
      "epoch": 3004.705882352941,
      "grad_norm": 19.58702278137207,
      "learning_rate": 1.995294117647059e-05,
      "loss": 0.828,
      "step": 51080
    },
    {
      "epoch": 3005.294117647059,
      "grad_norm": 17.266433715820312,
      "learning_rate": 1.9947058823529412e-05,
      "loss": 1.0066,
      "step": 51090
    },
    {
      "epoch": 3005.8823529411766,
      "grad_norm": 22.109691619873047,
      "learning_rate": 1.9941176470588235e-05,
      "loss": 0.8876,
      "step": 51100
    },
    {
      "epoch": 3006.470588235294,
      "grad_norm": 26.739160537719727,
      "learning_rate": 1.993529411764706e-05,
      "loss": 0.9918,
      "step": 51110
    },
    {
      "epoch": 3007.0588235294117,
      "grad_norm": 19.933055877685547,
      "learning_rate": 1.992941176470588e-05,
      "loss": 0.9068,
      "step": 51120
    },
    {
      "epoch": 3007.6470588235293,
      "grad_norm": 19.57758903503418,
      "learning_rate": 1.9923529411764707e-05,
      "loss": 0.8861,
      "step": 51130
    },
    {
      "epoch": 3008.235294117647,
      "grad_norm": 15.524405479431152,
      "learning_rate": 1.991764705882353e-05,
      "loss": 0.8186,
      "step": 51140
    },
    {
      "epoch": 3008.823529411765,
      "grad_norm": 19.63860511779785,
      "learning_rate": 1.9911764705882353e-05,
      "loss": 0.8563,
      "step": 51150
    },
    {
      "epoch": 3009.4117647058824,
      "grad_norm": 21.96255111694336,
      "learning_rate": 1.9905882352941176e-05,
      "loss": 1.0211,
      "step": 51160
    },
    {
      "epoch": 3010.0,
      "grad_norm": 24.07232666015625,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.8713,
      "step": 51170
    },
    {
      "epoch": 3010.5882352941176,
      "grad_norm": 16.250125885009766,
      "learning_rate": 1.9894117647058826e-05,
      "loss": 0.9484,
      "step": 51180
    },
    {
      "epoch": 3011.176470588235,
      "grad_norm": 23.171478271484375,
      "learning_rate": 1.988823529411765e-05,
      "loss": 1.0581,
      "step": 51190
    },
    {
      "epoch": 3011.764705882353,
      "grad_norm": 18.272491455078125,
      "learning_rate": 1.988235294117647e-05,
      "loss": 0.9363,
      "step": 51200
    },
    {
      "epoch": 3012.3529411764707,
      "grad_norm": 18.95905876159668,
      "learning_rate": 1.9876470588235298e-05,
      "loss": 0.8996,
      "step": 51210
    },
    {
      "epoch": 3012.9411764705883,
      "grad_norm": 22.364572525024414,
      "learning_rate": 1.9870588235294118e-05,
      "loss": 1.0113,
      "step": 51220
    },
    {
      "epoch": 3013.529411764706,
      "grad_norm": 16.77420425415039,
      "learning_rate": 1.986470588235294e-05,
      "loss": 0.9762,
      "step": 51230
    },
    {
      "epoch": 3014.1176470588234,
      "grad_norm": 20.506591796875,
      "learning_rate": 1.9858823529411767e-05,
      "loss": 0.8946,
      "step": 51240
    },
    {
      "epoch": 3014.705882352941,
      "grad_norm": 19.600505828857422,
      "learning_rate": 1.9852941176470586e-05,
      "loss": 0.936,
      "step": 51250
    },
    {
      "epoch": 3015.294117647059,
      "grad_norm": 16.360042572021484,
      "learning_rate": 1.9847058823529413e-05,
      "loss": 0.9183,
      "step": 51260
    },
    {
      "epoch": 3015.8823529411766,
      "grad_norm": 21.85649871826172,
      "learning_rate": 1.9841176470588236e-05,
      "loss": 1.0212,
      "step": 51270
    },
    {
      "epoch": 3016.470588235294,
      "grad_norm": 18.96711540222168,
      "learning_rate": 1.983529411764706e-05,
      "loss": 1.0037,
      "step": 51280
    },
    {
      "epoch": 3017.0588235294117,
      "grad_norm": 28.912261962890625,
      "learning_rate": 1.9829411764705882e-05,
      "loss": 0.8732,
      "step": 51290
    },
    {
      "epoch": 3017.6470588235293,
      "grad_norm": 32.21446990966797,
      "learning_rate": 1.9823529411764708e-05,
      "loss": 0.9379,
      "step": 51300
    },
    {
      "epoch": 3018.235294117647,
      "grad_norm": 24.673009872436523,
      "learning_rate": 1.981764705882353e-05,
      "loss": 1.0156,
      "step": 51310
    },
    {
      "epoch": 3018.823529411765,
      "grad_norm": 23.029327392578125,
      "learning_rate": 1.9811764705882354e-05,
      "loss": 0.9018,
      "step": 51320
    },
    {
      "epoch": 3019.4117647058824,
      "grad_norm": 19.742595672607422,
      "learning_rate": 1.9805882352941177e-05,
      "loss": 0.9129,
      "step": 51330
    },
    {
      "epoch": 3020.0,
      "grad_norm": 24.422504425048828,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.9892,
      "step": 51340
    },
    {
      "epoch": 3020.5882352941176,
      "grad_norm": 22.837751388549805,
      "learning_rate": 1.9794117647058823e-05,
      "loss": 0.9906,
      "step": 51350
    },
    {
      "epoch": 3021.176470588235,
      "grad_norm": 17.163978576660156,
      "learning_rate": 1.978823529411765e-05,
      "loss": 0.9391,
      "step": 51360
    },
    {
      "epoch": 3021.764705882353,
      "grad_norm": 23.850505828857422,
      "learning_rate": 1.9782352941176473e-05,
      "loss": 0.9713,
      "step": 51370
    },
    {
      "epoch": 3022.3529411764707,
      "grad_norm": 19.06732940673828,
      "learning_rate": 1.9776470588235296e-05,
      "loss": 0.8426,
      "step": 51380
    },
    {
      "epoch": 3022.9411764705883,
      "grad_norm": 20.597965240478516,
      "learning_rate": 1.977058823529412e-05,
      "loss": 0.927,
      "step": 51390
    },
    {
      "epoch": 3023.529411764706,
      "grad_norm": 20.406341552734375,
      "learning_rate": 1.976470588235294e-05,
      "loss": 0.9567,
      "step": 51400
    },
    {
      "epoch": 3024.1176470588234,
      "grad_norm": 24.411571502685547,
      "learning_rate": 1.9758823529411764e-05,
      "loss": 0.8699,
      "step": 51410
    },
    {
      "epoch": 3024.705882352941,
      "grad_norm": 24.81358528137207,
      "learning_rate": 1.9752941176470587e-05,
      "loss": 0.8906,
      "step": 51420
    },
    {
      "epoch": 3025.294117647059,
      "grad_norm": 19.07250213623047,
      "learning_rate": 1.9747058823529414e-05,
      "loss": 0.889,
      "step": 51430
    },
    {
      "epoch": 3025.8823529411766,
      "grad_norm": 26.567079544067383,
      "learning_rate": 1.9741176470588237e-05,
      "loss": 0.9377,
      "step": 51440
    },
    {
      "epoch": 3026.470588235294,
      "grad_norm": 20.1675968170166,
      "learning_rate": 1.973529411764706e-05,
      "loss": 0.9129,
      "step": 51450
    },
    {
      "epoch": 3027.0588235294117,
      "grad_norm": 17.86907386779785,
      "learning_rate": 1.9729411764705883e-05,
      "loss": 0.9055,
      "step": 51460
    },
    {
      "epoch": 3027.6470588235293,
      "grad_norm": 14.159832954406738,
      "learning_rate": 1.9723529411764706e-05,
      "loss": 0.8872,
      "step": 51470
    },
    {
      "epoch": 3028.235294117647,
      "grad_norm": 25.003503799438477,
      "learning_rate": 1.971764705882353e-05,
      "loss": 1.0066,
      "step": 51480
    },
    {
      "epoch": 3028.823529411765,
      "grad_norm": 21.72437286376953,
      "learning_rate": 1.9711764705882355e-05,
      "loss": 0.945,
      "step": 51490
    },
    {
      "epoch": 3029.4117647058824,
      "grad_norm": 18.465213775634766,
      "learning_rate": 1.9705882352941178e-05,
      "loss": 0.9336,
      "step": 51500
    },
    {
      "epoch": 3030.0,
      "grad_norm": 19.400842666625977,
      "learning_rate": 1.97e-05,
      "loss": 0.9171,
      "step": 51510
    },
    {
      "epoch": 3030.5882352941176,
      "grad_norm": 23.65622901916504,
      "learning_rate": 1.9694117647058824e-05,
      "loss": 0.9408,
      "step": 51520
    },
    {
      "epoch": 3031.176470588235,
      "grad_norm": 15.824769973754883,
      "learning_rate": 1.968823529411765e-05,
      "loss": 1.0007,
      "step": 51530
    },
    {
      "epoch": 3031.764705882353,
      "grad_norm": 20.652257919311523,
      "learning_rate": 1.968235294117647e-05,
      "loss": 0.9493,
      "step": 51540
    },
    {
      "epoch": 3032.3529411764707,
      "grad_norm": 21.474966049194336,
      "learning_rate": 1.9676470588235296e-05,
      "loss": 0.9327,
      "step": 51550
    },
    {
      "epoch": 3032.9411764705883,
      "grad_norm": 27.7447452545166,
      "learning_rate": 1.967058823529412e-05,
      "loss": 0.9037,
      "step": 51560
    },
    {
      "epoch": 3033.529411764706,
      "grad_norm": 19.555744171142578,
      "learning_rate": 1.9664705882352942e-05,
      "loss": 0.9142,
      "step": 51570
    },
    {
      "epoch": 3034.1176470588234,
      "grad_norm": 18.410032272338867,
      "learning_rate": 1.9658823529411765e-05,
      "loss": 0.8735,
      "step": 51580
    },
    {
      "epoch": 3034.705882352941,
      "grad_norm": 21.299043655395508,
      "learning_rate": 1.965294117647059e-05,
      "loss": 0.9109,
      "step": 51590
    },
    {
      "epoch": 3035.294117647059,
      "grad_norm": 19.835037231445312,
      "learning_rate": 1.964705882352941e-05,
      "loss": 0.8348,
      "step": 51600
    },
    {
      "epoch": 3035.8823529411766,
      "grad_norm": 24.969459533691406,
      "learning_rate": 1.9641176470588234e-05,
      "loss": 0.8975,
      "step": 51610
    },
    {
      "epoch": 3036.470588235294,
      "grad_norm": 22.221527099609375,
      "learning_rate": 1.963529411764706e-05,
      "loss": 0.996,
      "step": 51620
    },
    {
      "epoch": 3037.0588235294117,
      "grad_norm": 19.25515365600586,
      "learning_rate": 1.9629411764705884e-05,
      "loss": 0.9265,
      "step": 51630
    },
    {
      "epoch": 3037.6470588235293,
      "grad_norm": 19.312435150146484,
      "learning_rate": 1.9623529411764707e-05,
      "loss": 0.994,
      "step": 51640
    },
    {
      "epoch": 3038.235294117647,
      "grad_norm": 22.48558235168457,
      "learning_rate": 1.961764705882353e-05,
      "loss": 0.8834,
      "step": 51650
    },
    {
      "epoch": 3038.823529411765,
      "grad_norm": 24.811092376708984,
      "learning_rate": 1.9611764705882356e-05,
      "loss": 0.9128,
      "step": 51660
    },
    {
      "epoch": 3039.4117647058824,
      "grad_norm": 23.013370513916016,
      "learning_rate": 1.9605882352941176e-05,
      "loss": 1.0023,
      "step": 51670
    },
    {
      "epoch": 3040.0,
      "grad_norm": 18.905536651611328,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.9517,
      "step": 51680
    },
    {
      "epoch": 3040.5882352941176,
      "grad_norm": 21.19060516357422,
      "learning_rate": 1.9594117647058825e-05,
      "loss": 0.9804,
      "step": 51690
    },
    {
      "epoch": 3041.176470588235,
      "grad_norm": 15.926421165466309,
      "learning_rate": 1.9588235294117648e-05,
      "loss": 0.8151,
      "step": 51700
    },
    {
      "epoch": 3041.764705882353,
      "grad_norm": 23.219894409179688,
      "learning_rate": 1.958235294117647e-05,
      "loss": 0.8325,
      "step": 51710
    },
    {
      "epoch": 3042.3529411764707,
      "grad_norm": 25.308401107788086,
      "learning_rate": 1.9576470588235297e-05,
      "loss": 0.8246,
      "step": 51720
    },
    {
      "epoch": 3042.9411764705883,
      "grad_norm": 17.12576675415039,
      "learning_rate": 1.9570588235294117e-05,
      "loss": 0.9948,
      "step": 51730
    },
    {
      "epoch": 3043.529411764706,
      "grad_norm": 14.84499454498291,
      "learning_rate": 1.9564705882352943e-05,
      "loss": 0.9448,
      "step": 51740
    },
    {
      "epoch": 3044.1176470588234,
      "grad_norm": 18.776113510131836,
      "learning_rate": 1.9558823529411766e-05,
      "loss": 0.8981,
      "step": 51750
    },
    {
      "epoch": 3044.705882352941,
      "grad_norm": 22.70569610595703,
      "learning_rate": 1.955294117647059e-05,
      "loss": 0.9371,
      "step": 51760
    },
    {
      "epoch": 3045.294117647059,
      "grad_norm": 18.555091857910156,
      "learning_rate": 1.9547058823529412e-05,
      "loss": 0.9223,
      "step": 51770
    },
    {
      "epoch": 3045.8823529411766,
      "grad_norm": 19.345304489135742,
      "learning_rate": 1.9541176470588235e-05,
      "loss": 0.9091,
      "step": 51780
    },
    {
      "epoch": 3046.470588235294,
      "grad_norm": 17.29046058654785,
      "learning_rate": 1.953529411764706e-05,
      "loss": 0.807,
      "step": 51790
    },
    {
      "epoch": 3047.0588235294117,
      "grad_norm": 17.835186004638672,
      "learning_rate": 1.952941176470588e-05,
      "loss": 0.9074,
      "step": 51800
    },
    {
      "epoch": 3047.6470588235293,
      "grad_norm": 13.70208740234375,
      "learning_rate": 1.9523529411764708e-05,
      "loss": 0.8172,
      "step": 51810
    },
    {
      "epoch": 3048.235294117647,
      "grad_norm": 19.023862838745117,
      "learning_rate": 1.951764705882353e-05,
      "loss": 0.879,
      "step": 51820
    },
    {
      "epoch": 3048.823529411765,
      "grad_norm": 20.10879898071289,
      "learning_rate": 1.9511764705882354e-05,
      "loss": 0.9692,
      "step": 51830
    },
    {
      "epoch": 3049.4117647058824,
      "grad_norm": 18.86395835876465,
      "learning_rate": 1.9505882352941177e-05,
      "loss": 0.8183,
      "step": 51840
    },
    {
      "epoch": 3050.0,
      "grad_norm": 19.82280921936035,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.8352,
      "step": 51850
    },
    {
      "epoch": 3050.5882352941176,
      "grad_norm": 18.43512725830078,
      "learning_rate": 1.9494117647058823e-05,
      "loss": 0.8795,
      "step": 51860
    },
    {
      "epoch": 3051.176470588235,
      "grad_norm": 19.139204025268555,
      "learning_rate": 1.948823529411765e-05,
      "loss": 0.9602,
      "step": 51870
    },
    {
      "epoch": 3051.764705882353,
      "grad_norm": 19.683046340942383,
      "learning_rate": 1.9482352941176472e-05,
      "loss": 0.8485,
      "step": 51880
    },
    {
      "epoch": 3052.3529411764707,
      "grad_norm": 17.2678165435791,
      "learning_rate": 1.9476470588235295e-05,
      "loss": 0.8862,
      "step": 51890
    },
    {
      "epoch": 3052.9411764705883,
      "grad_norm": 20.871601104736328,
      "learning_rate": 1.9470588235294118e-05,
      "loss": 0.8691,
      "step": 51900
    },
    {
      "epoch": 3053.529411764706,
      "grad_norm": 15.839323997497559,
      "learning_rate": 1.9464705882352944e-05,
      "loss": 0.9045,
      "step": 51910
    },
    {
      "epoch": 3054.1176470588234,
      "grad_norm": 24.06477928161621,
      "learning_rate": 1.9458823529411764e-05,
      "loss": 0.8546,
      "step": 51920
    },
    {
      "epoch": 3054.705882352941,
      "grad_norm": 26.05615997314453,
      "learning_rate": 1.945294117647059e-05,
      "loss": 0.8322,
      "step": 51930
    },
    {
      "epoch": 3055.294117647059,
      "grad_norm": 17.252687454223633,
      "learning_rate": 1.9447058823529413e-05,
      "loss": 0.8214,
      "step": 51940
    },
    {
      "epoch": 3055.8823529411766,
      "grad_norm": 19.82810401916504,
      "learning_rate": 1.9441176470588236e-05,
      "loss": 0.8791,
      "step": 51950
    },
    {
      "epoch": 3056.470588235294,
      "grad_norm": 16.529869079589844,
      "learning_rate": 1.943529411764706e-05,
      "loss": 0.9318,
      "step": 51960
    },
    {
      "epoch": 3057.0588235294117,
      "grad_norm": 17.41189956665039,
      "learning_rate": 1.9429411764705882e-05,
      "loss": 0.9058,
      "step": 51970
    },
    {
      "epoch": 3057.6470588235293,
      "grad_norm": 22.185684204101562,
      "learning_rate": 1.942352941176471e-05,
      "loss": 0.988,
      "step": 51980
    },
    {
      "epoch": 3058.235294117647,
      "grad_norm": 18.39524269104004,
      "learning_rate": 1.9417647058823528e-05,
      "loss": 0.8966,
      "step": 51990
    },
    {
      "epoch": 3058.823529411765,
      "grad_norm": 19.67519760131836,
      "learning_rate": 1.9411764705882355e-05,
      "loss": 0.8129,
      "step": 52000
    },
    {
      "epoch": 3059.4117647058824,
      "grad_norm": 20.83190155029297,
      "learning_rate": 1.9405882352941178e-05,
      "loss": 0.89,
      "step": 52010
    },
    {
      "epoch": 3060.0,
      "grad_norm": 25.65389060974121,
      "learning_rate": 1.94e-05,
      "loss": 0.8624,
      "step": 52020
    },
    {
      "epoch": 3060.5882352941176,
      "grad_norm": 18.097200393676758,
      "learning_rate": 1.9394117647058824e-05,
      "loss": 0.8953,
      "step": 52030
    },
    {
      "epoch": 3061.176470588235,
      "grad_norm": 20.257530212402344,
      "learning_rate": 1.938823529411765e-05,
      "loss": 1.0982,
      "step": 52040
    },
    {
      "epoch": 3061.764705882353,
      "grad_norm": 23.3338623046875,
      "learning_rate": 1.938235294117647e-05,
      "loss": 0.9302,
      "step": 52050
    },
    {
      "epoch": 3062.3529411764707,
      "grad_norm": 15.174067497253418,
      "learning_rate": 1.9376470588235296e-05,
      "loss": 0.7668,
      "step": 52060
    },
    {
      "epoch": 3062.9411764705883,
      "grad_norm": 19.26705551147461,
      "learning_rate": 1.937058823529412e-05,
      "loss": 0.8438,
      "step": 52070
    },
    {
      "epoch": 3063.529411764706,
      "grad_norm": 26.23103904724121,
      "learning_rate": 1.9364705882352942e-05,
      "loss": 0.9893,
      "step": 52080
    },
    {
      "epoch": 3064.1176470588234,
      "grad_norm": 25.91143035888672,
      "learning_rate": 1.9358823529411765e-05,
      "loss": 0.9508,
      "step": 52090
    },
    {
      "epoch": 3064.705882352941,
      "grad_norm": 19.885711669921875,
      "learning_rate": 1.935294117647059e-05,
      "loss": 0.923,
      "step": 52100
    },
    {
      "epoch": 3065.294117647059,
      "grad_norm": 14.198421478271484,
      "learning_rate": 1.934705882352941e-05,
      "loss": 0.8858,
      "step": 52110
    },
    {
      "epoch": 3065.8823529411766,
      "grad_norm": 21.026168823242188,
      "learning_rate": 1.9341176470588237e-05,
      "loss": 0.9648,
      "step": 52120
    },
    {
      "epoch": 3066.470588235294,
      "grad_norm": 19.880252838134766,
      "learning_rate": 1.933529411764706e-05,
      "loss": 0.8956,
      "step": 52130
    },
    {
      "epoch": 3067.0588235294117,
      "grad_norm": 20.835254669189453,
      "learning_rate": 1.9329411764705883e-05,
      "loss": 0.9095,
      "step": 52140
    },
    {
      "epoch": 3067.6470588235293,
      "grad_norm": 16.99220085144043,
      "learning_rate": 1.9323529411764706e-05,
      "loss": 0.9204,
      "step": 52150
    },
    {
      "epoch": 3068.235294117647,
      "grad_norm": 21.948240280151367,
      "learning_rate": 1.931764705882353e-05,
      "loss": 0.9465,
      "step": 52160
    },
    {
      "epoch": 3068.823529411765,
      "grad_norm": 21.4456729888916,
      "learning_rate": 1.9311764705882356e-05,
      "loss": 0.9132,
      "step": 52170
    },
    {
      "epoch": 3069.4117647058824,
      "grad_norm": 22.24957275390625,
      "learning_rate": 1.9305882352941175e-05,
      "loss": 0.8198,
      "step": 52180
    },
    {
      "epoch": 3070.0,
      "grad_norm": 25.760469436645508,
      "learning_rate": 1.93e-05,
      "loss": 0.8684,
      "step": 52190
    },
    {
      "epoch": 3070.5882352941176,
      "grad_norm": 19.217809677124023,
      "learning_rate": 1.9294117647058825e-05,
      "loss": 0.9723,
      "step": 52200
    },
    {
      "epoch": 3071.176470588235,
      "grad_norm": 23.11560821533203,
      "learning_rate": 1.9288235294117648e-05,
      "loss": 0.9,
      "step": 52210
    },
    {
      "epoch": 3071.764705882353,
      "grad_norm": 17.3206787109375,
      "learning_rate": 1.928235294117647e-05,
      "loss": 0.8804,
      "step": 52220
    },
    {
      "epoch": 3072.3529411764707,
      "grad_norm": 20.451791763305664,
      "learning_rate": 1.9276470588235297e-05,
      "loss": 0.9666,
      "step": 52230
    },
    {
      "epoch": 3072.9411764705883,
      "grad_norm": 19.499553680419922,
      "learning_rate": 1.9270588235294117e-05,
      "loss": 0.9343,
      "step": 52240
    },
    {
      "epoch": 3073.529411764706,
      "grad_norm": 19.646560668945312,
      "learning_rate": 1.9264705882352943e-05,
      "loss": 0.9257,
      "step": 52250
    },
    {
      "epoch": 3074.1176470588234,
      "grad_norm": 17.19183349609375,
      "learning_rate": 1.9258823529411766e-05,
      "loss": 0.9651,
      "step": 52260
    },
    {
      "epoch": 3074.705882352941,
      "grad_norm": 16.257436752319336,
      "learning_rate": 1.925294117647059e-05,
      "loss": 0.8262,
      "step": 52270
    },
    {
      "epoch": 3075.294117647059,
      "grad_norm": 17.46443748474121,
      "learning_rate": 1.9247058823529412e-05,
      "loss": 0.8987,
      "step": 52280
    },
    {
      "epoch": 3075.8823529411766,
      "grad_norm": 17.02307891845703,
      "learning_rate": 1.9241176470588238e-05,
      "loss": 1.0234,
      "step": 52290
    },
    {
      "epoch": 3076.470588235294,
      "grad_norm": 20.98985481262207,
      "learning_rate": 1.923529411764706e-05,
      "loss": 0.8628,
      "step": 52300
    },
    {
      "epoch": 3077.0588235294117,
      "grad_norm": 21.807998657226562,
      "learning_rate": 1.922941176470588e-05,
      "loss": 0.9341,
      "step": 52310
    },
    {
      "epoch": 3077.6470588235293,
      "grad_norm": 18.677072525024414,
      "learning_rate": 1.9223529411764707e-05,
      "loss": 0.8207,
      "step": 52320
    },
    {
      "epoch": 3078.235294117647,
      "grad_norm": 19.248497009277344,
      "learning_rate": 1.921764705882353e-05,
      "loss": 0.9817,
      "step": 52330
    },
    {
      "epoch": 3078.823529411765,
      "grad_norm": 22.860700607299805,
      "learning_rate": 1.9211764705882353e-05,
      "loss": 0.8964,
      "step": 52340
    },
    {
      "epoch": 3079.4117647058824,
      "grad_norm": 18.353378295898438,
      "learning_rate": 1.9205882352941176e-05,
      "loss": 0.843,
      "step": 52350
    },
    {
      "epoch": 3080.0,
      "grad_norm": 23.29218101501465,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.9193,
      "step": 52360
    },
    {
      "epoch": 3080.5882352941176,
      "grad_norm": 12.211675643920898,
      "learning_rate": 1.9194117647058822e-05,
      "loss": 0.8287,
      "step": 52370
    },
    {
      "epoch": 3081.176470588235,
      "grad_norm": 23.192270278930664,
      "learning_rate": 1.918823529411765e-05,
      "loss": 0.8963,
      "step": 52380
    },
    {
      "epoch": 3081.764705882353,
      "grad_norm": 24.174846649169922,
      "learning_rate": 1.918235294117647e-05,
      "loss": 0.9482,
      "step": 52390
    },
    {
      "epoch": 3082.3529411764707,
      "grad_norm": 19.994937896728516,
      "learning_rate": 1.9176470588235294e-05,
      "loss": 0.8766,
      "step": 52400
    },
    {
      "epoch": 3082.9411764705883,
      "grad_norm": 30.09276580810547,
      "learning_rate": 1.9170588235294117e-05,
      "loss": 1.0506,
      "step": 52410
    },
    {
      "epoch": 3083.529411764706,
      "grad_norm": 20.116830825805664,
      "learning_rate": 1.9164705882352944e-05,
      "loss": 0.8832,
      "step": 52420
    },
    {
      "epoch": 3084.1176470588234,
      "grad_norm": 15.381180763244629,
      "learning_rate": 1.9158823529411767e-05,
      "loss": 0.8263,
      "step": 52430
    },
    {
      "epoch": 3084.705882352941,
      "grad_norm": 19.147239685058594,
      "learning_rate": 1.915294117647059e-05,
      "loss": 0.817,
      "step": 52440
    },
    {
      "epoch": 3085.294117647059,
      "grad_norm": 21.070775985717773,
      "learning_rate": 1.9147058823529413e-05,
      "loss": 1.0473,
      "step": 52450
    },
    {
      "epoch": 3085.8823529411766,
      "grad_norm": 30.293048858642578,
      "learning_rate": 1.9141176470588236e-05,
      "loss": 1.0122,
      "step": 52460
    },
    {
      "epoch": 3086.470588235294,
      "grad_norm": 22.824447631835938,
      "learning_rate": 1.913529411764706e-05,
      "loss": 0.8683,
      "step": 52470
    },
    {
      "epoch": 3087.0588235294117,
      "grad_norm": 16.783748626708984,
      "learning_rate": 1.9129411764705885e-05,
      "loss": 0.9321,
      "step": 52480
    },
    {
      "epoch": 3087.6470588235293,
      "grad_norm": 22.331085205078125,
      "learning_rate": 1.9123529411764708e-05,
      "loss": 0.8956,
      "step": 52490
    },
    {
      "epoch": 3088.235294117647,
      "grad_norm": 16.584396362304688,
      "learning_rate": 1.9117647058823528e-05,
      "loss": 0.8638,
      "step": 52500
    },
    {
      "epoch": 3088.823529411765,
      "grad_norm": 23.274507522583008,
      "learning_rate": 1.9111764705882354e-05,
      "loss": 1.0443,
      "step": 52510
    },
    {
      "epoch": 3089.4117647058824,
      "grad_norm": 22.4871826171875,
      "learning_rate": 1.9105882352941177e-05,
      "loss": 0.984,
      "step": 52520
    },
    {
      "epoch": 3090.0,
      "grad_norm": 28.336666107177734,
      "learning_rate": 1.91e-05,
      "loss": 0.9905,
      "step": 52530
    },
    {
      "epoch": 3090.5882352941176,
      "grad_norm": 23.586273193359375,
      "learning_rate": 1.9094117647058823e-05,
      "loss": 0.8989,
      "step": 52540
    },
    {
      "epoch": 3091.176470588235,
      "grad_norm": 24.16362762451172,
      "learning_rate": 1.908823529411765e-05,
      "loss": 0.9622,
      "step": 52550
    },
    {
      "epoch": 3091.764705882353,
      "grad_norm": 17.5960750579834,
      "learning_rate": 1.908235294117647e-05,
      "loss": 0.8641,
      "step": 52560
    },
    {
      "epoch": 3092.3529411764707,
      "grad_norm": 25.854337692260742,
      "learning_rate": 1.9076470588235295e-05,
      "loss": 0.93,
      "step": 52570
    },
    {
      "epoch": 3092.9411764705883,
      "grad_norm": 21.815393447875977,
      "learning_rate": 1.907058823529412e-05,
      "loss": 0.9882,
      "step": 52580
    },
    {
      "epoch": 3093.529411764706,
      "grad_norm": 17.08470344543457,
      "learning_rate": 1.906470588235294e-05,
      "loss": 0.9128,
      "step": 52590
    },
    {
      "epoch": 3094.1176470588234,
      "grad_norm": 27.069366455078125,
      "learning_rate": 1.9058823529411764e-05,
      "loss": 0.8185,
      "step": 52600
    },
    {
      "epoch": 3094.705882352941,
      "grad_norm": 13.76904010772705,
      "learning_rate": 1.905294117647059e-05,
      "loss": 1.0342,
      "step": 52610
    },
    {
      "epoch": 3095.294117647059,
      "grad_norm": 18.482980728149414,
      "learning_rate": 1.9047058823529414e-05,
      "loss": 0.9488,
      "step": 52620
    },
    {
      "epoch": 3095.8823529411766,
      "grad_norm": 17.143569946289062,
      "learning_rate": 1.9041176470588237e-05,
      "loss": 0.7991,
      "step": 52630
    },
    {
      "epoch": 3096.470588235294,
      "grad_norm": 14.080952644348145,
      "learning_rate": 1.903529411764706e-05,
      "loss": 0.8469,
      "step": 52640
    },
    {
      "epoch": 3097.0588235294117,
      "grad_norm": 16.784351348876953,
      "learning_rate": 1.9029411764705886e-05,
      "loss": 0.9587,
      "step": 52650
    },
    {
      "epoch": 3097.6470588235293,
      "grad_norm": 28.57217025756836,
      "learning_rate": 1.9023529411764706e-05,
      "loss": 0.9695,
      "step": 52660
    },
    {
      "epoch": 3098.235294117647,
      "grad_norm": 19.867055892944336,
      "learning_rate": 1.9017647058823532e-05,
      "loss": 0.9222,
      "step": 52670
    },
    {
      "epoch": 3098.823529411765,
      "grad_norm": 17.804683685302734,
      "learning_rate": 1.9011764705882355e-05,
      "loss": 0.861,
      "step": 52680
    },
    {
      "epoch": 3099.4117647058824,
      "grad_norm": 17.5799617767334,
      "learning_rate": 1.9005882352941175e-05,
      "loss": 0.9598,
      "step": 52690
    },
    {
      "epoch": 3100.0,
      "grad_norm": 31.162837982177734,
      "learning_rate": 1.9e-05,
      "loss": 0.8162,
      "step": 52700
    },
    {
      "epoch": 3100.5882352941176,
      "grad_norm": 20.105186462402344,
      "learning_rate": 1.8994117647058824e-05,
      "loss": 1.0347,
      "step": 52710
    },
    {
      "epoch": 3101.176470588235,
      "grad_norm": 21.470693588256836,
      "learning_rate": 1.8988235294117647e-05,
      "loss": 0.792,
      "step": 52720
    },
    {
      "epoch": 3101.764705882353,
      "grad_norm": 14.948123931884766,
      "learning_rate": 1.898235294117647e-05,
      "loss": 0.9085,
      "step": 52730
    },
    {
      "epoch": 3102.3529411764707,
      "grad_norm": 23.85108757019043,
      "learning_rate": 1.8976470588235296e-05,
      "loss": 0.9443,
      "step": 52740
    },
    {
      "epoch": 3102.9411764705883,
      "grad_norm": 19.406709671020508,
      "learning_rate": 1.897058823529412e-05,
      "loss": 0.9048,
      "step": 52750
    },
    {
      "epoch": 3103.529411764706,
      "grad_norm": 16.272436141967773,
      "learning_rate": 1.8964705882352942e-05,
      "loss": 0.9534,
      "step": 52760
    },
    {
      "epoch": 3104.1176470588234,
      "grad_norm": 26.14600944519043,
      "learning_rate": 1.8958823529411765e-05,
      "loss": 0.8173,
      "step": 52770
    },
    {
      "epoch": 3104.705882352941,
      "grad_norm": 18.911087036132812,
      "learning_rate": 1.895294117647059e-05,
      "loss": 0.8701,
      "step": 52780
    },
    {
      "epoch": 3105.294117647059,
      "grad_norm": 29.051687240600586,
      "learning_rate": 1.894705882352941e-05,
      "loss": 0.9157,
      "step": 52790
    },
    {
      "epoch": 3105.8823529411766,
      "grad_norm": 17.69585609436035,
      "learning_rate": 1.8941176470588238e-05,
      "loss": 0.9034,
      "step": 52800
    },
    {
      "epoch": 3106.470588235294,
      "grad_norm": 21.355682373046875,
      "learning_rate": 1.893529411764706e-05,
      "loss": 0.8727,
      "step": 52810
    },
    {
      "epoch": 3107.0588235294117,
      "grad_norm": 19.375520706176758,
      "learning_rate": 1.8929411764705884e-05,
      "loss": 0.9205,
      "step": 52820
    },
    {
      "epoch": 3107.6470588235293,
      "grad_norm": 21.972658157348633,
      "learning_rate": 1.8923529411764707e-05,
      "loss": 0.9012,
      "step": 52830
    },
    {
      "epoch": 3108.235294117647,
      "grad_norm": 25.141178131103516,
      "learning_rate": 1.8917647058823533e-05,
      "loss": 0.9539,
      "step": 52840
    },
    {
      "epoch": 3108.823529411765,
      "grad_norm": 19.653799057006836,
      "learning_rate": 1.8911764705882353e-05,
      "loss": 0.9135,
      "step": 52850
    },
    {
      "epoch": 3109.4117647058824,
      "grad_norm": 19.984825134277344,
      "learning_rate": 1.8905882352941176e-05,
      "loss": 0.9118,
      "step": 52860
    },
    {
      "epoch": 3110.0,
      "grad_norm": 23.587278366088867,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.9647,
      "step": 52870
    },
    {
      "epoch": 3110.5882352941176,
      "grad_norm": 19.967439651489258,
      "learning_rate": 1.889411764705882e-05,
      "loss": 0.878,
      "step": 52880
    },
    {
      "epoch": 3111.176470588235,
      "grad_norm": 19.31165885925293,
      "learning_rate": 1.8888235294117648e-05,
      "loss": 0.8616,
      "step": 52890
    },
    {
      "epoch": 3111.764705882353,
      "grad_norm": 25.01107406616211,
      "learning_rate": 1.888235294117647e-05,
      "loss": 0.8816,
      "step": 52900
    },
    {
      "epoch": 3112.3529411764707,
      "grad_norm": 25.49704933166504,
      "learning_rate": 1.8876470588235294e-05,
      "loss": 0.8977,
      "step": 52910
    },
    {
      "epoch": 3112.9411764705883,
      "grad_norm": 25.2023868560791,
      "learning_rate": 1.8870588235294117e-05,
      "loss": 0.9379,
      "step": 52920
    },
    {
      "epoch": 3113.529411764706,
      "grad_norm": 24.304481506347656,
      "learning_rate": 1.8864705882352943e-05,
      "loss": 0.9121,
      "step": 52930
    },
    {
      "epoch": 3114.1176470588234,
      "grad_norm": 17.611125946044922,
      "learning_rate": 1.8858823529411766e-05,
      "loss": 0.8641,
      "step": 52940
    },
    {
      "epoch": 3114.705882352941,
      "grad_norm": 22.620677947998047,
      "learning_rate": 1.885294117647059e-05,
      "loss": 1.001,
      "step": 52950
    },
    {
      "epoch": 3115.294117647059,
      "grad_norm": 22.545528411865234,
      "learning_rate": 1.8847058823529412e-05,
      "loss": 0.8937,
      "step": 52960
    },
    {
      "epoch": 3115.8823529411766,
      "grad_norm": 24.429101943969727,
      "learning_rate": 1.884117647058824e-05,
      "loss": 0.8737,
      "step": 52970
    },
    {
      "epoch": 3116.470588235294,
      "grad_norm": 18.605497360229492,
      "learning_rate": 1.8835294117647058e-05,
      "loss": 0.8736,
      "step": 52980
    },
    {
      "epoch": 3117.0588235294117,
      "grad_norm": 18.96152687072754,
      "learning_rate": 1.8829411764705885e-05,
      "loss": 0.8731,
      "step": 52990
    },
    {
      "epoch": 3117.6470588235293,
      "grad_norm": 22.720544815063477,
      "learning_rate": 1.8823529411764708e-05,
      "loss": 0.9036,
      "step": 53000
    },
    {
      "epoch": 3118.235294117647,
      "grad_norm": 24.074739456176758,
      "learning_rate": 1.881764705882353e-05,
      "loss": 0.9682,
      "step": 53010
    },
    {
      "epoch": 3118.823529411765,
      "grad_norm": 14.587586402893066,
      "learning_rate": 1.8811764705882354e-05,
      "loss": 0.9391,
      "step": 53020
    },
    {
      "epoch": 3119.4117647058824,
      "grad_norm": 13.398476600646973,
      "learning_rate": 1.880588235294118e-05,
      "loss": 0.8663,
      "step": 53030
    },
    {
      "epoch": 3120.0,
      "grad_norm": 31.659536361694336,
      "learning_rate": 1.88e-05,
      "loss": 0.9248,
      "step": 53040
    },
    {
      "epoch": 3120.5882352941176,
      "grad_norm": 23.26058006286621,
      "learning_rate": 1.8794117647058823e-05,
      "loss": 0.933,
      "step": 53050
    },
    {
      "epoch": 3121.176470588235,
      "grad_norm": 25.18990707397461,
      "learning_rate": 1.878823529411765e-05,
      "loss": 0.8838,
      "step": 53060
    },
    {
      "epoch": 3121.764705882353,
      "grad_norm": 16.38652992248535,
      "learning_rate": 1.8782352941176472e-05,
      "loss": 0.8185,
      "step": 53070
    },
    {
      "epoch": 3122.3529411764707,
      "grad_norm": 19.681987762451172,
      "learning_rate": 1.8776470588235295e-05,
      "loss": 0.8244,
      "step": 53080
    },
    {
      "epoch": 3122.9411764705883,
      "grad_norm": 21.791486740112305,
      "learning_rate": 1.8770588235294118e-05,
      "loss": 0.9084,
      "step": 53090
    },
    {
      "epoch": 3123.529411764706,
      "grad_norm": 14.781272888183594,
      "learning_rate": 1.876470588235294e-05,
      "loss": 0.9125,
      "step": 53100
    },
    {
      "epoch": 3124.1176470588234,
      "grad_norm": 25.33546257019043,
      "learning_rate": 1.8758823529411764e-05,
      "loss": 0.9462,
      "step": 53110
    },
    {
      "epoch": 3124.705882352941,
      "grad_norm": 19.01686668395996,
      "learning_rate": 1.875294117647059e-05,
      "loss": 0.8495,
      "step": 53120
    },
    {
      "epoch": 3125.294117647059,
      "grad_norm": 25.86014175415039,
      "learning_rate": 1.8747058823529413e-05,
      "loss": 0.9792,
      "step": 53130
    },
    {
      "epoch": 3125.8823529411766,
      "grad_norm": 18.546611785888672,
      "learning_rate": 1.8741176470588236e-05,
      "loss": 0.9749,
      "step": 53140
    },
    {
      "epoch": 3126.470588235294,
      "grad_norm": 24.795597076416016,
      "learning_rate": 1.873529411764706e-05,
      "loss": 0.8847,
      "step": 53150
    },
    {
      "epoch": 3127.0588235294117,
      "grad_norm": 17.412248611450195,
      "learning_rate": 1.8729411764705886e-05,
      "loss": 0.9459,
      "step": 53160
    },
    {
      "epoch": 3127.6470588235293,
      "grad_norm": 23.380733489990234,
      "learning_rate": 1.8723529411764705e-05,
      "loss": 0.8368,
      "step": 53170
    },
    {
      "epoch": 3128.235294117647,
      "grad_norm": 20.881088256835938,
      "learning_rate": 1.871764705882353e-05,
      "loss": 0.844,
      "step": 53180
    },
    {
      "epoch": 3128.823529411765,
      "grad_norm": 20.633607864379883,
      "learning_rate": 1.8711764705882355e-05,
      "loss": 0.8888,
      "step": 53190
    },
    {
      "epoch": 3129.4117647058824,
      "grad_norm": 16.123266220092773,
      "learning_rate": 1.8705882352941178e-05,
      "loss": 0.9638,
      "step": 53200
    },
    {
      "epoch": 3130.0,
      "grad_norm": 36.45956039428711,
      "learning_rate": 1.87e-05,
      "loss": 0.9745,
      "step": 53210
    },
    {
      "epoch": 3130.5882352941176,
      "grad_norm": 17.175338745117188,
      "learning_rate": 1.8694117647058824e-05,
      "loss": 0.8752,
      "step": 53220
    },
    {
      "epoch": 3131.176470588235,
      "grad_norm": 16.810392379760742,
      "learning_rate": 1.8688235294117647e-05,
      "loss": 0.8575,
      "step": 53230
    },
    {
      "epoch": 3131.764705882353,
      "grad_norm": 19.56229591369629,
      "learning_rate": 1.868235294117647e-05,
      "loss": 0.834,
      "step": 53240
    },
    {
      "epoch": 3132.3529411764707,
      "grad_norm": 19.0711727142334,
      "learning_rate": 1.8676470588235296e-05,
      "loss": 0.825,
      "step": 53250
    },
    {
      "epoch": 3132.9411764705883,
      "grad_norm": 28.147747039794922,
      "learning_rate": 1.867058823529412e-05,
      "loss": 0.9685,
      "step": 53260
    },
    {
      "epoch": 3133.529411764706,
      "grad_norm": 26.440141677856445,
      "learning_rate": 1.8664705882352942e-05,
      "loss": 0.8714,
      "step": 53270
    },
    {
      "epoch": 3134.1176470588234,
      "grad_norm": 18.487197875976562,
      "learning_rate": 1.8658823529411765e-05,
      "loss": 0.7947,
      "step": 53280
    },
    {
      "epoch": 3134.705882352941,
      "grad_norm": 27.183868408203125,
      "learning_rate": 1.865294117647059e-05,
      "loss": 0.8983,
      "step": 53290
    },
    {
      "epoch": 3135.294117647059,
      "grad_norm": 27.540973663330078,
      "learning_rate": 1.864705882352941e-05,
      "loss": 0.8904,
      "step": 53300
    },
    {
      "epoch": 3135.8823529411766,
      "grad_norm": 25.75081443786621,
      "learning_rate": 1.8641176470588237e-05,
      "loss": 0.95,
      "step": 53310
    },
    {
      "epoch": 3136.470588235294,
      "grad_norm": 14.422837257385254,
      "learning_rate": 1.863529411764706e-05,
      "loss": 0.8126,
      "step": 53320
    },
    {
      "epoch": 3137.0588235294117,
      "grad_norm": 15.23298168182373,
      "learning_rate": 1.8629411764705883e-05,
      "loss": 0.8422,
      "step": 53330
    },
    {
      "epoch": 3137.6470588235293,
      "grad_norm": 21.860429763793945,
      "learning_rate": 1.8623529411764706e-05,
      "loss": 0.9623,
      "step": 53340
    },
    {
      "epoch": 3138.235294117647,
      "grad_norm": 23.35921859741211,
      "learning_rate": 1.8617647058823533e-05,
      "loss": 0.8677,
      "step": 53350
    },
    {
      "epoch": 3138.823529411765,
      "grad_norm": 27.24216651916504,
      "learning_rate": 1.8611764705882352e-05,
      "loss": 0.9146,
      "step": 53360
    },
    {
      "epoch": 3139.4117647058824,
      "grad_norm": 26.287723541259766,
      "learning_rate": 1.860588235294118e-05,
      "loss": 0.8493,
      "step": 53370
    },
    {
      "epoch": 3140.0,
      "grad_norm": 23.91568946838379,
      "learning_rate": 1.86e-05,
      "loss": 0.9743,
      "step": 53380
    },
    {
      "epoch": 3140.5882352941176,
      "grad_norm": 23.154403686523438,
      "learning_rate": 1.8594117647058824e-05,
      "loss": 0.8199,
      "step": 53390
    },
    {
      "epoch": 3141.176470588235,
      "grad_norm": 22.375476837158203,
      "learning_rate": 1.8588235294117647e-05,
      "loss": 0.8516,
      "step": 53400
    },
    {
      "epoch": 3141.764705882353,
      "grad_norm": 22.08530044555664,
      "learning_rate": 1.858235294117647e-05,
      "loss": 0.9174,
      "step": 53410
    },
    {
      "epoch": 3142.3529411764707,
      "grad_norm": 21.995338439941406,
      "learning_rate": 1.8576470588235297e-05,
      "loss": 0.922,
      "step": 53420
    },
    {
      "epoch": 3142.9411764705883,
      "grad_norm": 11.811844825744629,
      "learning_rate": 1.8570588235294116e-05,
      "loss": 0.8822,
      "step": 53430
    },
    {
      "epoch": 3143.529411764706,
      "grad_norm": 18.913732528686523,
      "learning_rate": 1.8564705882352943e-05,
      "loss": 0.8299,
      "step": 53440
    },
    {
      "epoch": 3144.1176470588234,
      "grad_norm": 22.503101348876953,
      "learning_rate": 1.8558823529411766e-05,
      "loss": 0.8371,
      "step": 53450
    },
    {
      "epoch": 3144.705882352941,
      "grad_norm": 19.713674545288086,
      "learning_rate": 1.855294117647059e-05,
      "loss": 0.8438,
      "step": 53460
    },
    {
      "epoch": 3145.294117647059,
      "grad_norm": 25.25566864013672,
      "learning_rate": 1.8547058823529412e-05,
      "loss": 0.8871,
      "step": 53470
    },
    {
      "epoch": 3145.8823529411766,
      "grad_norm": 20.462081909179688,
      "learning_rate": 1.8541176470588238e-05,
      "loss": 0.8992,
      "step": 53480
    },
    {
      "epoch": 3146.470588235294,
      "grad_norm": 23.604360580444336,
      "learning_rate": 1.8535294117647058e-05,
      "loss": 0.9037,
      "step": 53490
    },
    {
      "epoch": 3147.0588235294117,
      "grad_norm": 23.40813446044922,
      "learning_rate": 1.8529411764705884e-05,
      "loss": 0.9217,
      "step": 53500
    },
    {
      "epoch": 3147.6470588235293,
      "grad_norm": 21.414539337158203,
      "learning_rate": 1.8523529411764707e-05,
      "loss": 0.858,
      "step": 53510
    },
    {
      "epoch": 3148.235294117647,
      "grad_norm": 20.83148193359375,
      "learning_rate": 1.851764705882353e-05,
      "loss": 0.7974,
      "step": 53520
    },
    {
      "epoch": 3148.823529411765,
      "grad_norm": 18.3940486907959,
      "learning_rate": 1.8511764705882353e-05,
      "loss": 0.8704,
      "step": 53530
    },
    {
      "epoch": 3149.4117647058824,
      "grad_norm": 21.383989334106445,
      "learning_rate": 1.850588235294118e-05,
      "loss": 0.8429,
      "step": 53540
    },
    {
      "epoch": 3150.0,
      "grad_norm": 25.813838958740234,
      "learning_rate": 1.85e-05,
      "loss": 0.8636,
      "step": 53550
    },
    {
      "epoch": 3150.5882352941176,
      "grad_norm": 16.61638641357422,
      "learning_rate": 1.8494117647058825e-05,
      "loss": 0.8582,
      "step": 53560
    },
    {
      "epoch": 3151.176470588235,
      "grad_norm": 17.221332550048828,
      "learning_rate": 1.848823529411765e-05,
      "loss": 0.92,
      "step": 53570
    },
    {
      "epoch": 3151.764705882353,
      "grad_norm": 21.321069717407227,
      "learning_rate": 1.848235294117647e-05,
      "loss": 0.9495,
      "step": 53580
    },
    {
      "epoch": 3152.3529411764707,
      "grad_norm": 23.5107364654541,
      "learning_rate": 1.8476470588235294e-05,
      "loss": 0.9093,
      "step": 53590
    },
    {
      "epoch": 3152.9411764705883,
      "grad_norm": 21.470945358276367,
      "learning_rate": 1.8470588235294117e-05,
      "loss": 0.9279,
      "step": 53600
    },
    {
      "epoch": 3153.529411764706,
      "grad_norm": 26.023284912109375,
      "learning_rate": 1.8464705882352944e-05,
      "loss": 0.8794,
      "step": 53610
    },
    {
      "epoch": 3154.1176470588234,
      "grad_norm": 19.453706741333008,
      "learning_rate": 1.8458823529411763e-05,
      "loss": 1.0461,
      "step": 53620
    },
    {
      "epoch": 3154.705882352941,
      "grad_norm": 19.164602279663086,
      "learning_rate": 1.845294117647059e-05,
      "loss": 0.9347,
      "step": 53630
    },
    {
      "epoch": 3155.294117647059,
      "grad_norm": 20.62653350830078,
      "learning_rate": 1.8447058823529413e-05,
      "loss": 0.8895,
      "step": 53640
    },
    {
      "epoch": 3155.8823529411766,
      "grad_norm": 17.49382209777832,
      "learning_rate": 1.8441176470588236e-05,
      "loss": 0.9242,
      "step": 53650
    },
    {
      "epoch": 3156.470588235294,
      "grad_norm": 21.541568756103516,
      "learning_rate": 1.843529411764706e-05,
      "loss": 0.8564,
      "step": 53660
    },
    {
      "epoch": 3157.0588235294117,
      "grad_norm": 22.42154884338379,
      "learning_rate": 1.8429411764705885e-05,
      "loss": 0.9708,
      "step": 53670
    },
    {
      "epoch": 3157.6470588235293,
      "grad_norm": 16.939565658569336,
      "learning_rate": 1.8423529411764705e-05,
      "loss": 0.9167,
      "step": 53680
    },
    {
      "epoch": 3158.235294117647,
      "grad_norm": 24.717918395996094,
      "learning_rate": 1.841764705882353e-05,
      "loss": 0.8315,
      "step": 53690
    },
    {
      "epoch": 3158.823529411765,
      "grad_norm": 23.432750701904297,
      "learning_rate": 1.8411764705882354e-05,
      "loss": 0.9834,
      "step": 53700
    },
    {
      "epoch": 3159.4117647058824,
      "grad_norm": 20.572355270385742,
      "learning_rate": 1.8405882352941177e-05,
      "loss": 0.8599,
      "step": 53710
    },
    {
      "epoch": 3160.0,
      "grad_norm": 19.026567459106445,
      "learning_rate": 1.84e-05,
      "loss": 0.8732,
      "step": 53720
    },
    {
      "epoch": 3160.5882352941176,
      "grad_norm": 18.596569061279297,
      "learning_rate": 1.8394117647058826e-05,
      "loss": 0.8381,
      "step": 53730
    },
    {
      "epoch": 3161.176470588235,
      "grad_norm": 18.662128448486328,
      "learning_rate": 1.838823529411765e-05,
      "loss": 0.8464,
      "step": 53740
    },
    {
      "epoch": 3161.764705882353,
      "grad_norm": 16.376218795776367,
      "learning_rate": 1.8382352941176472e-05,
      "loss": 0.8662,
      "step": 53750
    },
    {
      "epoch": 3162.3529411764707,
      "grad_norm": 15.879439353942871,
      "learning_rate": 1.8376470588235295e-05,
      "loss": 0.9592,
      "step": 53760
    },
    {
      "epoch": 3162.9411764705883,
      "grad_norm": 19.223587036132812,
      "learning_rate": 1.837058823529412e-05,
      "loss": 0.9245,
      "step": 53770
    },
    {
      "epoch": 3163.529411764706,
      "grad_norm": 22.34562873840332,
      "learning_rate": 1.836470588235294e-05,
      "loss": 0.894,
      "step": 53780
    },
    {
      "epoch": 3164.1176470588234,
      "grad_norm": 21.234390258789062,
      "learning_rate": 1.8358823529411764e-05,
      "loss": 0.8458,
      "step": 53790
    },
    {
      "epoch": 3164.705882352941,
      "grad_norm": 21.443920135498047,
      "learning_rate": 1.835294117647059e-05,
      "loss": 0.9187,
      "step": 53800
    },
    {
      "epoch": 3165.294117647059,
      "grad_norm": 18.856891632080078,
      "learning_rate": 1.834705882352941e-05,
      "loss": 0.7935,
      "step": 53810
    },
    {
      "epoch": 3165.8823529411766,
      "grad_norm": 22.137256622314453,
      "learning_rate": 1.8341176470588237e-05,
      "loss": 0.7806,
      "step": 53820
    },
    {
      "epoch": 3166.470588235294,
      "grad_norm": 16.63753890991211,
      "learning_rate": 1.833529411764706e-05,
      "loss": 0.867,
      "step": 53830
    },
    {
      "epoch": 3167.0588235294117,
      "grad_norm": 21.206228256225586,
      "learning_rate": 1.8329411764705883e-05,
      "loss": 1.0377,
      "step": 53840
    },
    {
      "epoch": 3167.6470588235293,
      "grad_norm": 26.48120880126953,
      "learning_rate": 1.8323529411764706e-05,
      "loss": 0.8052,
      "step": 53850
    },
    {
      "epoch": 3168.235294117647,
      "grad_norm": 22.6495361328125,
      "learning_rate": 1.8317647058823532e-05,
      "loss": 0.9935,
      "step": 53860
    },
    {
      "epoch": 3168.823529411765,
      "grad_norm": 17.1870059967041,
      "learning_rate": 1.831176470588235e-05,
      "loss": 0.8702,
      "step": 53870
    },
    {
      "epoch": 3169.4117647058824,
      "grad_norm": 17.875873565673828,
      "learning_rate": 1.8305882352941178e-05,
      "loss": 0.8535,
      "step": 53880
    },
    {
      "epoch": 3170.0,
      "grad_norm": 25.784536361694336,
      "learning_rate": 1.83e-05,
      "loss": 0.8964,
      "step": 53890
    },
    {
      "epoch": 3170.5882352941176,
      "grad_norm": 25.433443069458008,
      "learning_rate": 1.8294117647058824e-05,
      "loss": 0.8346,
      "step": 53900
    },
    {
      "epoch": 3171.176470588235,
      "grad_norm": 18.301973342895508,
      "learning_rate": 1.8288235294117647e-05,
      "loss": 0.9395,
      "step": 53910
    },
    {
      "epoch": 3171.764705882353,
      "grad_norm": 17.344865798950195,
      "learning_rate": 1.8282352941176473e-05,
      "loss": 0.9043,
      "step": 53920
    },
    {
      "epoch": 3172.3529411764707,
      "grad_norm": 21.81406593322754,
      "learning_rate": 1.8276470588235296e-05,
      "loss": 0.9636,
      "step": 53930
    },
    {
      "epoch": 3172.9411764705883,
      "grad_norm": 17.695707321166992,
      "learning_rate": 1.827058823529412e-05,
      "loss": 0.8981,
      "step": 53940
    },
    {
      "epoch": 3173.529411764706,
      "grad_norm": 25.455883026123047,
      "learning_rate": 1.8264705882352942e-05,
      "loss": 0.8379,
      "step": 53950
    },
    {
      "epoch": 3174.1176470588234,
      "grad_norm": 16.736656188964844,
      "learning_rate": 1.8258823529411765e-05,
      "loss": 0.83,
      "step": 53960
    },
    {
      "epoch": 3174.705882352941,
      "grad_norm": 19.6177921295166,
      "learning_rate": 1.8252941176470588e-05,
      "loss": 0.8462,
      "step": 53970
    },
    {
      "epoch": 3175.294117647059,
      "grad_norm": 21.237037658691406,
      "learning_rate": 1.824705882352941e-05,
      "loss": 0.8082,
      "step": 53980
    },
    {
      "epoch": 3175.8823529411766,
      "grad_norm": 17.364824295043945,
      "learning_rate": 1.8241176470588238e-05,
      "loss": 0.8738,
      "step": 53990
    },
    {
      "epoch": 3176.470588235294,
      "grad_norm": 22.2352352142334,
      "learning_rate": 1.8235294117647057e-05,
      "loss": 0.9492,
      "step": 54000
    },
    {
      "epoch": 3177.0588235294117,
      "grad_norm": 14.967162132263184,
      "learning_rate": 1.8229411764705884e-05,
      "loss": 0.9222,
      "step": 54010
    },
    {
      "epoch": 3177.6470588235293,
      "grad_norm": 19.7406005859375,
      "learning_rate": 1.8223529411764707e-05,
      "loss": 0.834,
      "step": 54020
    },
    {
      "epoch": 3178.235294117647,
      "grad_norm": 19.304927825927734,
      "learning_rate": 1.821764705882353e-05,
      "loss": 0.8427,
      "step": 54030
    },
    {
      "epoch": 3178.823529411765,
      "grad_norm": 16.885581970214844,
      "learning_rate": 1.8211764705882353e-05,
      "loss": 0.8254,
      "step": 54040
    },
    {
      "epoch": 3179.4117647058824,
      "grad_norm": 18.8577880859375,
      "learning_rate": 1.820588235294118e-05,
      "loss": 1.0247,
      "step": 54050
    },
    {
      "epoch": 3180.0,
      "grad_norm": 23.767526626586914,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.8087,
      "step": 54060
    },
    {
      "epoch": 3180.5882352941176,
      "grad_norm": 16.386743545532227,
      "learning_rate": 1.8194117647058825e-05,
      "loss": 0.7121,
      "step": 54070
    },
    {
      "epoch": 3181.176470588235,
      "grad_norm": 24.784208297729492,
      "learning_rate": 1.8188235294117648e-05,
      "loss": 0.9606,
      "step": 54080
    },
    {
      "epoch": 3181.764705882353,
      "grad_norm": 16.920133590698242,
      "learning_rate": 1.8182352941176474e-05,
      "loss": 0.9657,
      "step": 54090
    },
    {
      "epoch": 3182.3529411764707,
      "grad_norm": 16.51521110534668,
      "learning_rate": 1.8176470588235294e-05,
      "loss": 0.8946,
      "step": 54100
    },
    {
      "epoch": 3182.9411764705883,
      "grad_norm": 22.233661651611328,
      "learning_rate": 1.817058823529412e-05,
      "loss": 0.9428,
      "step": 54110
    },
    {
      "epoch": 3183.529411764706,
      "grad_norm": 23.61311149597168,
      "learning_rate": 1.8164705882352943e-05,
      "loss": 0.8727,
      "step": 54120
    },
    {
      "epoch": 3184.1176470588234,
      "grad_norm": 20.753128051757812,
      "learning_rate": 1.8158823529411763e-05,
      "loss": 0.7999,
      "step": 54130
    },
    {
      "epoch": 3184.705882352941,
      "grad_norm": 20.070451736450195,
      "learning_rate": 1.815294117647059e-05,
      "loss": 0.8526,
      "step": 54140
    },
    {
      "epoch": 3185.294117647059,
      "grad_norm": 20.897220611572266,
      "learning_rate": 1.8147058823529412e-05,
      "loss": 0.8067,
      "step": 54150
    },
    {
      "epoch": 3185.8823529411766,
      "grad_norm": 19.754209518432617,
      "learning_rate": 1.8141176470588235e-05,
      "loss": 0.8525,
      "step": 54160
    },
    {
      "epoch": 3186.470588235294,
      "grad_norm": 20.908302307128906,
      "learning_rate": 1.8135294117647058e-05,
      "loss": 0.9043,
      "step": 54170
    },
    {
      "epoch": 3187.0588235294117,
      "grad_norm": 21.368249893188477,
      "learning_rate": 1.8129411764705885e-05,
      "loss": 0.895,
      "step": 54180
    },
    {
      "epoch": 3187.6470588235293,
      "grad_norm": 19.472259521484375,
      "learning_rate": 1.8123529411764708e-05,
      "loss": 1.025,
      "step": 54190
    },
    {
      "epoch": 3188.235294117647,
      "grad_norm": 16.3515567779541,
      "learning_rate": 1.811764705882353e-05,
      "loss": 0.9435,
      "step": 54200
    },
    {
      "epoch": 3188.823529411765,
      "grad_norm": 22.016254425048828,
      "learning_rate": 1.8111764705882354e-05,
      "loss": 0.8816,
      "step": 54210
    },
    {
      "epoch": 3189.4117647058824,
      "grad_norm": 22.274213790893555,
      "learning_rate": 1.8105882352941177e-05,
      "loss": 0.9703,
      "step": 54220
    },
    {
      "epoch": 3190.0,
      "grad_norm": 20.972850799560547,
      "learning_rate": 1.81e-05,
      "loss": 0.945,
      "step": 54230
    },
    {
      "epoch": 3190.5882352941176,
      "grad_norm": 20.49668312072754,
      "learning_rate": 1.8094117647058826e-05,
      "loss": 0.8748,
      "step": 54240
    },
    {
      "epoch": 3191.176470588235,
      "grad_norm": 24.44738006591797,
      "learning_rate": 1.808823529411765e-05,
      "loss": 0.9845,
      "step": 54250
    },
    {
      "epoch": 3191.764705882353,
      "grad_norm": 23.41337776184082,
      "learning_rate": 1.8082352941176472e-05,
      "loss": 0.8044,
      "step": 54260
    },
    {
      "epoch": 3192.3529411764707,
      "grad_norm": 16.918611526489258,
      "learning_rate": 1.8076470588235295e-05,
      "loss": 0.9203,
      "step": 54270
    },
    {
      "epoch": 3192.9411764705883,
      "grad_norm": 22.942291259765625,
      "learning_rate": 1.807058823529412e-05,
      "loss": 0.907,
      "step": 54280
    },
    {
      "epoch": 3193.529411764706,
      "grad_norm": 21.763750076293945,
      "learning_rate": 1.806470588235294e-05,
      "loss": 0.811,
      "step": 54290
    },
    {
      "epoch": 3194.1176470588234,
      "grad_norm": 22.835506439208984,
      "learning_rate": 1.8058823529411767e-05,
      "loss": 0.8736,
      "step": 54300
    },
    {
      "epoch": 3194.705882352941,
      "grad_norm": 25.925304412841797,
      "learning_rate": 1.805294117647059e-05,
      "loss": 0.7883,
      "step": 54310
    },
    {
      "epoch": 3195.294117647059,
      "grad_norm": 18.791257858276367,
      "learning_rate": 1.804705882352941e-05,
      "loss": 0.8168,
      "step": 54320
    },
    {
      "epoch": 3195.8823529411766,
      "grad_norm": 20.451255798339844,
      "learning_rate": 1.8041176470588236e-05,
      "loss": 0.8467,
      "step": 54330
    },
    {
      "epoch": 3196.470588235294,
      "grad_norm": 16.781410217285156,
      "learning_rate": 1.803529411764706e-05,
      "loss": 0.7659,
      "step": 54340
    },
    {
      "epoch": 3197.0588235294117,
      "grad_norm": 21.500282287597656,
      "learning_rate": 1.8029411764705882e-05,
      "loss": 0.9497,
      "step": 54350
    },
    {
      "epoch": 3197.6470588235293,
      "grad_norm": 16.140310287475586,
      "learning_rate": 1.8023529411764705e-05,
      "loss": 0.9966,
      "step": 54360
    },
    {
      "epoch": 3198.235294117647,
      "grad_norm": 21.53479766845703,
      "learning_rate": 1.801764705882353e-05,
      "loss": 0.8521,
      "step": 54370
    },
    {
      "epoch": 3198.823529411765,
      "grad_norm": 16.67768096923828,
      "learning_rate": 1.8011764705882354e-05,
      "loss": 0.8718,
      "step": 54380
    },
    {
      "epoch": 3199.4117647058824,
      "grad_norm": 23.415386199951172,
      "learning_rate": 1.8005882352941177e-05,
      "loss": 0.8759,
      "step": 54390
    },
    {
      "epoch": 3200.0,
      "grad_norm": 15.407715797424316,
      "learning_rate": 1.8e-05,
      "loss": 0.7943,
      "step": 54400
    },
    {
      "epoch": 3200.5882352941176,
      "grad_norm": 17.492347717285156,
      "learning_rate": 1.7994117647058827e-05,
      "loss": 0.9321,
      "step": 54410
    },
    {
      "epoch": 3201.176470588235,
      "grad_norm": 16.985523223876953,
      "learning_rate": 1.7988235294117646e-05,
      "loss": 0.8005,
      "step": 54420
    },
    {
      "epoch": 3201.764705882353,
      "grad_norm": 19.998010635375977,
      "learning_rate": 1.7982352941176473e-05,
      "loss": 0.8234,
      "step": 54430
    },
    {
      "epoch": 3202.3529411764707,
      "grad_norm": 22.423927307128906,
      "learning_rate": 1.7976470588235296e-05,
      "loss": 0.9632,
      "step": 54440
    },
    {
      "epoch": 3202.9411764705883,
      "grad_norm": 21.74225616455078,
      "learning_rate": 1.797058823529412e-05,
      "loss": 0.9254,
      "step": 54450
    },
    {
      "epoch": 3203.529411764706,
      "grad_norm": 28.192380905151367,
      "learning_rate": 1.7964705882352942e-05,
      "loss": 0.9084,
      "step": 54460
    },
    {
      "epoch": 3204.1176470588234,
      "grad_norm": 21.087488174438477,
      "learning_rate": 1.7958823529411768e-05,
      "loss": 0.8465,
      "step": 54470
    },
    {
      "epoch": 3204.705882352941,
      "grad_norm": 16.31039047241211,
      "learning_rate": 1.7952941176470588e-05,
      "loss": 0.9352,
      "step": 54480
    },
    {
      "epoch": 3205.294117647059,
      "grad_norm": 15.184508323669434,
      "learning_rate": 1.794705882352941e-05,
      "loss": 0.8684,
      "step": 54490
    },
    {
      "epoch": 3205.8823529411766,
      "grad_norm": 24.14138412475586,
      "learning_rate": 1.7941176470588237e-05,
      "loss": 0.9526,
      "step": 54500
    },
    {
      "epoch": 3206.470588235294,
      "grad_norm": 17.07686424255371,
      "learning_rate": 1.793529411764706e-05,
      "loss": 0.8777,
      "step": 54510
    },
    {
      "epoch": 3207.0588235294117,
      "grad_norm": 15.781815528869629,
      "learning_rate": 1.7929411764705883e-05,
      "loss": 0.7766,
      "step": 54520
    },
    {
      "epoch": 3207.6470588235293,
      "grad_norm": 24.37467384338379,
      "learning_rate": 1.7923529411764706e-05,
      "loss": 0.978,
      "step": 54530
    },
    {
      "epoch": 3208.235294117647,
      "grad_norm": 17.297094345092773,
      "learning_rate": 1.791764705882353e-05,
      "loss": 0.9683,
      "step": 54540
    },
    {
      "epoch": 3208.823529411765,
      "grad_norm": 20.19449806213379,
      "learning_rate": 1.7911764705882352e-05,
      "loss": 0.8229,
      "step": 54550
    },
    {
      "epoch": 3209.4117647058824,
      "grad_norm": 19.520492553710938,
      "learning_rate": 1.790588235294118e-05,
      "loss": 0.8672,
      "step": 54560
    },
    {
      "epoch": 3210.0,
      "grad_norm": 24.005908966064453,
      "learning_rate": 1.79e-05,
      "loss": 0.7993,
      "step": 54570
    },
    {
      "epoch": 3210.5882352941176,
      "grad_norm": 16.58230972290039,
      "learning_rate": 1.7894117647058824e-05,
      "loss": 0.8913,
      "step": 54580
    },
    {
      "epoch": 3211.176470588235,
      "grad_norm": 18.586273193359375,
      "learning_rate": 1.7888235294117647e-05,
      "loss": 0.8731,
      "step": 54590
    },
    {
      "epoch": 3211.764705882353,
      "grad_norm": 16.019405364990234,
      "learning_rate": 1.7882352941176474e-05,
      "loss": 0.8729,
      "step": 54600
    },
    {
      "epoch": 3212.3529411764707,
      "grad_norm": 19.39674949645996,
      "learning_rate": 1.7876470588235293e-05,
      "loss": 0.8577,
      "step": 54610
    },
    {
      "epoch": 3212.9411764705883,
      "grad_norm": 24.2770938873291,
      "learning_rate": 1.787058823529412e-05,
      "loss": 0.901,
      "step": 54620
    },
    {
      "epoch": 3213.529411764706,
      "grad_norm": 17.99188804626465,
      "learning_rate": 1.7864705882352943e-05,
      "loss": 0.7333,
      "step": 54630
    },
    {
      "epoch": 3214.1176470588234,
      "grad_norm": 28.43675422668457,
      "learning_rate": 1.7858823529411766e-05,
      "loss": 0.7669,
      "step": 54640
    },
    {
      "epoch": 3214.705882352941,
      "grad_norm": 26.537981033325195,
      "learning_rate": 1.785294117647059e-05,
      "loss": 0.874,
      "step": 54650
    },
    {
      "epoch": 3215.294117647059,
      "grad_norm": 21.121414184570312,
      "learning_rate": 1.7847058823529415e-05,
      "loss": 0.9145,
      "step": 54660
    },
    {
      "epoch": 3215.8823529411766,
      "grad_norm": 20.05597686767578,
      "learning_rate": 1.7841176470588235e-05,
      "loss": 0.9582,
      "step": 54670
    },
    {
      "epoch": 3216.470588235294,
      "grad_norm": 18.98604965209961,
      "learning_rate": 1.7835294117647058e-05,
      "loss": 0.8183,
      "step": 54680
    },
    {
      "epoch": 3217.0588235294117,
      "grad_norm": 21.464906692504883,
      "learning_rate": 1.7829411764705884e-05,
      "loss": 0.9401,
      "step": 54690
    },
    {
      "epoch": 3217.6470588235293,
      "grad_norm": 22.39347267150879,
      "learning_rate": 1.7823529411764707e-05,
      "loss": 0.9314,
      "step": 54700
    },
    {
      "epoch": 3218.235294117647,
      "grad_norm": 18.76217269897461,
      "learning_rate": 1.781764705882353e-05,
      "loss": 0.7406,
      "step": 54710
    },
    {
      "epoch": 3218.823529411765,
      "grad_norm": 27.093994140625,
      "learning_rate": 1.7811764705882353e-05,
      "loss": 0.8469,
      "step": 54720
    },
    {
      "epoch": 3219.4117647058824,
      "grad_norm": 22.306425094604492,
      "learning_rate": 1.780588235294118e-05,
      "loss": 0.9374,
      "step": 54730
    },
    {
      "epoch": 3220.0,
      "grad_norm": 34.7822380065918,
      "learning_rate": 1.78e-05,
      "loss": 0.916,
      "step": 54740
    },
    {
      "epoch": 3220.5882352941176,
      "grad_norm": 17.557947158813477,
      "learning_rate": 1.7794117647058825e-05,
      "loss": 0.7758,
      "step": 54750
    },
    {
      "epoch": 3221.176470588235,
      "grad_norm": 19.687122344970703,
      "learning_rate": 1.778823529411765e-05,
      "loss": 0.7802,
      "step": 54760
    },
    {
      "epoch": 3221.764705882353,
      "grad_norm": 20.710224151611328,
      "learning_rate": 1.778235294117647e-05,
      "loss": 0.9753,
      "step": 54770
    },
    {
      "epoch": 3222.3529411764707,
      "grad_norm": 17.344938278198242,
      "learning_rate": 1.7776470588235294e-05,
      "loss": 0.885,
      "step": 54780
    },
    {
      "epoch": 3222.9411764705883,
      "grad_norm": 22.22663688659668,
      "learning_rate": 1.777058823529412e-05,
      "loss": 0.8507,
      "step": 54790
    },
    {
      "epoch": 3223.529411764706,
      "grad_norm": 18.68990135192871,
      "learning_rate": 1.776470588235294e-05,
      "loss": 0.9337,
      "step": 54800
    },
    {
      "epoch": 3224.1176470588234,
      "grad_norm": 16.422195434570312,
      "learning_rate": 1.7758823529411767e-05,
      "loss": 0.8878,
      "step": 54810
    },
    {
      "epoch": 3224.705882352941,
      "grad_norm": 19.196918487548828,
      "learning_rate": 1.775294117647059e-05,
      "loss": 0.8063,
      "step": 54820
    },
    {
      "epoch": 3225.294117647059,
      "grad_norm": 25.841819763183594,
      "learning_rate": 1.7747058823529413e-05,
      "loss": 0.871,
      "step": 54830
    },
    {
      "epoch": 3225.8823529411766,
      "grad_norm": 26.109888076782227,
      "learning_rate": 1.7741176470588236e-05,
      "loss": 0.916,
      "step": 54840
    },
    {
      "epoch": 3226.470588235294,
      "grad_norm": 19.561506271362305,
      "learning_rate": 1.7735294117647062e-05,
      "loss": 0.8167,
      "step": 54850
    },
    {
      "epoch": 3227.0588235294117,
      "grad_norm": 17.55306625366211,
      "learning_rate": 1.772941176470588e-05,
      "loss": 0.9,
      "step": 54860
    },
    {
      "epoch": 3227.6470588235293,
      "grad_norm": 19.086894989013672,
      "learning_rate": 1.7723529411764705e-05,
      "loss": 0.8951,
      "step": 54870
    },
    {
      "epoch": 3228.235294117647,
      "grad_norm": 24.35015869140625,
      "learning_rate": 1.771764705882353e-05,
      "loss": 0.8921,
      "step": 54880
    },
    {
      "epoch": 3228.823529411765,
      "grad_norm": 21.474018096923828,
      "learning_rate": 1.7711764705882354e-05,
      "loss": 1.0297,
      "step": 54890
    },
    {
      "epoch": 3229.4117647058824,
      "grad_norm": 21.07802963256836,
      "learning_rate": 1.7705882352941177e-05,
      "loss": 0.9256,
      "step": 54900
    },
    {
      "epoch": 3230.0,
      "grad_norm": 21.924142837524414,
      "learning_rate": 1.77e-05,
      "loss": 0.9102,
      "step": 54910
    },
    {
      "epoch": 3230.5882352941176,
      "grad_norm": 13.457402229309082,
      "learning_rate": 1.7694117647058826e-05,
      "loss": 0.9168,
      "step": 54920
    },
    {
      "epoch": 3231.176470588235,
      "grad_norm": 20.31218910217285,
      "learning_rate": 1.7688235294117646e-05,
      "loss": 0.8803,
      "step": 54930
    },
    {
      "epoch": 3231.764705882353,
      "grad_norm": 22.384784698486328,
      "learning_rate": 1.7682352941176472e-05,
      "loss": 0.8872,
      "step": 54940
    },
    {
      "epoch": 3232.3529411764707,
      "grad_norm": 23.83170509338379,
      "learning_rate": 1.7676470588235295e-05,
      "loss": 0.8408,
      "step": 54950
    },
    {
      "epoch": 3232.9411764705883,
      "grad_norm": 19.137802124023438,
      "learning_rate": 1.7670588235294118e-05,
      "loss": 0.8857,
      "step": 54960
    },
    {
      "epoch": 3233.529411764706,
      "grad_norm": 16.621353149414062,
      "learning_rate": 1.766470588235294e-05,
      "loss": 0.7976,
      "step": 54970
    },
    {
      "epoch": 3234.1176470588234,
      "grad_norm": 26.660240173339844,
      "learning_rate": 1.7658823529411768e-05,
      "loss": 0.9537,
      "step": 54980
    },
    {
      "epoch": 3234.705882352941,
      "grad_norm": 22.576091766357422,
      "learning_rate": 1.7652941176470587e-05,
      "loss": 0.7958,
      "step": 54990
    },
    {
      "epoch": 3235.294117647059,
      "grad_norm": 16.725467681884766,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 0.8866,
      "step": 55000
    },
    {
      "epoch": 3235.8823529411766,
      "grad_norm": 15.469308853149414,
      "learning_rate": 1.7641176470588237e-05,
      "loss": 0.7726,
      "step": 55010
    },
    {
      "epoch": 3236.470588235294,
      "grad_norm": 25.441286087036133,
      "learning_rate": 1.763529411764706e-05,
      "loss": 0.9243,
      "step": 55020
    },
    {
      "epoch": 3237.0588235294117,
      "grad_norm": 23.622297286987305,
      "learning_rate": 1.7629411764705883e-05,
      "loss": 0.8496,
      "step": 55030
    },
    {
      "epoch": 3237.6470588235293,
      "grad_norm": 17.434415817260742,
      "learning_rate": 1.7623529411764706e-05,
      "loss": 0.8961,
      "step": 55040
    },
    {
      "epoch": 3238.235294117647,
      "grad_norm": 23.945173263549805,
      "learning_rate": 1.7617647058823532e-05,
      "loss": 0.8822,
      "step": 55050
    },
    {
      "epoch": 3238.823529411765,
      "grad_norm": 14.190530776977539,
      "learning_rate": 1.761176470588235e-05,
      "loss": 0.8692,
      "step": 55060
    },
    {
      "epoch": 3239.4117647058824,
      "grad_norm": 22.038061141967773,
      "learning_rate": 1.7605882352941178e-05,
      "loss": 0.8218,
      "step": 55070
    },
    {
      "epoch": 3240.0,
      "grad_norm": 20.922243118286133,
      "learning_rate": 1.76e-05,
      "loss": 0.8427,
      "step": 55080
    },
    {
      "epoch": 3240.5882352941176,
      "grad_norm": 21.102279663085938,
      "learning_rate": 1.7594117647058824e-05,
      "loss": 0.9101,
      "step": 55090
    },
    {
      "epoch": 3241.176470588235,
      "grad_norm": 22.042236328125,
      "learning_rate": 1.7588235294117647e-05,
      "loss": 0.8585,
      "step": 55100
    },
    {
      "epoch": 3241.764705882353,
      "grad_norm": 18.086528778076172,
      "learning_rate": 1.7582352941176473e-05,
      "loss": 0.7585,
      "step": 55110
    },
    {
      "epoch": 3242.3529411764707,
      "grad_norm": 21.14422035217285,
      "learning_rate": 1.7576470588235293e-05,
      "loss": 0.8789,
      "step": 55120
    },
    {
      "epoch": 3242.9411764705883,
      "grad_norm": 17.538604736328125,
      "learning_rate": 1.757058823529412e-05,
      "loss": 0.8339,
      "step": 55130
    },
    {
      "epoch": 3243.529411764706,
      "grad_norm": 18.960172653198242,
      "learning_rate": 1.7564705882352942e-05,
      "loss": 0.7783,
      "step": 55140
    },
    {
      "epoch": 3244.1176470588234,
      "grad_norm": 21.01045036315918,
      "learning_rate": 1.7558823529411765e-05,
      "loss": 0.9037,
      "step": 55150
    },
    {
      "epoch": 3244.705882352941,
      "grad_norm": 20.57516860961914,
      "learning_rate": 1.7552941176470588e-05,
      "loss": 0.9099,
      "step": 55160
    },
    {
      "epoch": 3245.294117647059,
      "grad_norm": 20.764549255371094,
      "learning_rate": 1.7547058823529415e-05,
      "loss": 0.8651,
      "step": 55170
    },
    {
      "epoch": 3245.8823529411766,
      "grad_norm": 17.305376052856445,
      "learning_rate": 1.7541176470588238e-05,
      "loss": 0.8961,
      "step": 55180
    },
    {
      "epoch": 3246.470588235294,
      "grad_norm": 17.57526969909668,
      "learning_rate": 1.753529411764706e-05,
      "loss": 0.8457,
      "step": 55190
    },
    {
      "epoch": 3247.0588235294117,
      "grad_norm": 18.755102157592773,
      "learning_rate": 1.7529411764705884e-05,
      "loss": 0.8971,
      "step": 55200
    },
    {
      "epoch": 3247.6470588235293,
      "grad_norm": 20.652484893798828,
      "learning_rate": 1.7523529411764707e-05,
      "loss": 0.7358,
      "step": 55210
    },
    {
      "epoch": 3248.235294117647,
      "grad_norm": 23.17117691040039,
      "learning_rate": 1.751764705882353e-05,
      "loss": 0.8657,
      "step": 55220
    },
    {
      "epoch": 3248.823529411765,
      "grad_norm": 20.628211975097656,
      "learning_rate": 1.7511764705882352e-05,
      "loss": 0.9004,
      "step": 55230
    },
    {
      "epoch": 3249.4117647058824,
      "grad_norm": 22.1848201751709,
      "learning_rate": 1.750588235294118e-05,
      "loss": 0.8452,
      "step": 55240
    },
    {
      "epoch": 3250.0,
      "grad_norm": 21.643657684326172,
      "learning_rate": 1.75e-05,
      "loss": 0.801,
      "step": 55250
    },
    {
      "epoch": 3250.5882352941176,
      "grad_norm": 19.221830368041992,
      "learning_rate": 1.7494117647058825e-05,
      "loss": 0.8428,
      "step": 55260
    },
    {
      "epoch": 3251.176470588235,
      "grad_norm": 16.61375617980957,
      "learning_rate": 1.7488235294117648e-05,
      "loss": 0.8602,
      "step": 55270
    },
    {
      "epoch": 3251.764705882353,
      "grad_norm": 16.757282257080078,
      "learning_rate": 1.748235294117647e-05,
      "loss": 0.8762,
      "step": 55280
    },
    {
      "epoch": 3252.3529411764707,
      "grad_norm": 23.43737030029297,
      "learning_rate": 1.7476470588235294e-05,
      "loss": 0.8249,
      "step": 55290
    },
    {
      "epoch": 3252.9411764705883,
      "grad_norm": 25.486656188964844,
      "learning_rate": 1.747058823529412e-05,
      "loss": 0.8442,
      "step": 55300
    },
    {
      "epoch": 3253.529411764706,
      "grad_norm": 18.118587493896484,
      "learning_rate": 1.746470588235294e-05,
      "loss": 0.7846,
      "step": 55310
    },
    {
      "epoch": 3254.1176470588234,
      "grad_norm": 23.606565475463867,
      "learning_rate": 1.7458823529411766e-05,
      "loss": 0.9108,
      "step": 55320
    },
    {
      "epoch": 3254.705882352941,
      "grad_norm": 16.203264236450195,
      "learning_rate": 1.745294117647059e-05,
      "loss": 0.8449,
      "step": 55330
    },
    {
      "epoch": 3255.294117647059,
      "grad_norm": 18.12259864807129,
      "learning_rate": 1.7447058823529412e-05,
      "loss": 0.958,
      "step": 55340
    },
    {
      "epoch": 3255.8823529411766,
      "grad_norm": 17.729434967041016,
      "learning_rate": 1.7441176470588235e-05,
      "loss": 0.9129,
      "step": 55350
    },
    {
      "epoch": 3256.470588235294,
      "grad_norm": 15.241325378417969,
      "learning_rate": 1.743529411764706e-05,
      "loss": 0.7207,
      "step": 55360
    },
    {
      "epoch": 3257.0588235294117,
      "grad_norm": 17.440914154052734,
      "learning_rate": 1.7429411764705884e-05,
      "loss": 0.8577,
      "step": 55370
    },
    {
      "epoch": 3257.6470588235293,
      "grad_norm": 23.996435165405273,
      "learning_rate": 1.7423529411764707e-05,
      "loss": 0.8552,
      "step": 55380
    },
    {
      "epoch": 3258.235294117647,
      "grad_norm": 21.481645584106445,
      "learning_rate": 1.741764705882353e-05,
      "loss": 0.9088,
      "step": 55390
    },
    {
      "epoch": 3258.823529411765,
      "grad_norm": 21.53229522705078,
      "learning_rate": 1.7411764705882353e-05,
      "loss": 0.8665,
      "step": 55400
    },
    {
      "epoch": 3259.4117647058824,
      "grad_norm": 18.882848739624023,
      "learning_rate": 1.7405882352941176e-05,
      "loss": 0.8438,
      "step": 55410
    },
    {
      "epoch": 3260.0,
      "grad_norm": 22.379417419433594,
      "learning_rate": 1.74e-05,
      "loss": 0.8383,
      "step": 55420
    },
    {
      "epoch": 3260.5882352941176,
      "grad_norm": 16.9696044921875,
      "learning_rate": 1.7394117647058826e-05,
      "loss": 0.8791,
      "step": 55430
    },
    {
      "epoch": 3261.176470588235,
      "grad_norm": 20.12047004699707,
      "learning_rate": 1.7388235294117645e-05,
      "loss": 0.7122,
      "step": 55440
    },
    {
      "epoch": 3261.764705882353,
      "grad_norm": 21.06791114807129,
      "learning_rate": 1.7382352941176472e-05,
      "loss": 0.9839,
      "step": 55450
    },
    {
      "epoch": 3262.3529411764707,
      "grad_norm": 24.851539611816406,
      "learning_rate": 1.7376470588235295e-05,
      "loss": 0.7816,
      "step": 55460
    },
    {
      "epoch": 3262.9411764705883,
      "grad_norm": 20.168394088745117,
      "learning_rate": 1.7370588235294118e-05,
      "loss": 0.9832,
      "step": 55470
    },
    {
      "epoch": 3263.529411764706,
      "grad_norm": 17.769149780273438,
      "learning_rate": 1.736470588235294e-05,
      "loss": 0.8531,
      "step": 55480
    },
    {
      "epoch": 3264.1176470588234,
      "grad_norm": 12.878722190856934,
      "learning_rate": 1.7358823529411767e-05,
      "loss": 0.7723,
      "step": 55490
    },
    {
      "epoch": 3264.705882352941,
      "grad_norm": 18.939180374145508,
      "learning_rate": 1.735294117647059e-05,
      "loss": 0.921,
      "step": 55500
    },
    {
      "epoch": 3265.294117647059,
      "grad_norm": 16.436960220336914,
      "learning_rate": 1.7347058823529413e-05,
      "loss": 0.7758,
      "step": 55510
    },
    {
      "epoch": 3265.8823529411766,
      "grad_norm": 20.891014099121094,
      "learning_rate": 1.7341176470588236e-05,
      "loss": 0.8797,
      "step": 55520
    },
    {
      "epoch": 3266.470588235294,
      "grad_norm": 20.58661460876465,
      "learning_rate": 1.733529411764706e-05,
      "loss": 0.8441,
      "step": 55530
    },
    {
      "epoch": 3267.0588235294117,
      "grad_norm": 21.346010208129883,
      "learning_rate": 1.7329411764705882e-05,
      "loss": 0.8263,
      "step": 55540
    },
    {
      "epoch": 3267.6470588235293,
      "grad_norm": 18.68360137939453,
      "learning_rate": 1.732352941176471e-05,
      "loss": 0.9014,
      "step": 55550
    },
    {
      "epoch": 3268.235294117647,
      "grad_norm": 25.309080123901367,
      "learning_rate": 1.731764705882353e-05,
      "loss": 0.8704,
      "step": 55560
    },
    {
      "epoch": 3268.823529411765,
      "grad_norm": 21.899526596069336,
      "learning_rate": 1.7311764705882354e-05,
      "loss": 0.8078,
      "step": 55570
    },
    {
      "epoch": 3269.4117647058824,
      "grad_norm": 15.151288986206055,
      "learning_rate": 1.7305882352941177e-05,
      "loss": 0.7745,
      "step": 55580
    },
    {
      "epoch": 3270.0,
      "grad_norm": 23.84050178527832,
      "learning_rate": 1.73e-05,
      "loss": 0.7228,
      "step": 55590
    },
    {
      "epoch": 3270.5882352941176,
      "grad_norm": 21.910112380981445,
      "learning_rate": 1.7294117647058823e-05,
      "loss": 0.8798,
      "step": 55600
    },
    {
      "epoch": 3271.176470588235,
      "grad_norm": 24.085155487060547,
      "learning_rate": 1.7288235294117646e-05,
      "loss": 0.9022,
      "step": 55610
    },
    {
      "epoch": 3271.764705882353,
      "grad_norm": 27.762727737426758,
      "learning_rate": 1.7282352941176473e-05,
      "loss": 0.9628,
      "step": 55620
    },
    {
      "epoch": 3272.3529411764707,
      "grad_norm": 17.159473419189453,
      "learning_rate": 1.7276470588235292e-05,
      "loss": 0.7802,
      "step": 55630
    },
    {
      "epoch": 3272.9411764705883,
      "grad_norm": 18.18006706237793,
      "learning_rate": 1.727058823529412e-05,
      "loss": 0.885,
      "step": 55640
    },
    {
      "epoch": 3273.529411764706,
      "grad_norm": 18.916399002075195,
      "learning_rate": 1.7264705882352942e-05,
      "loss": 0.8138,
      "step": 55650
    },
    {
      "epoch": 3274.1176470588234,
      "grad_norm": 20.906070709228516,
      "learning_rate": 1.7258823529411765e-05,
      "loss": 0.8959,
      "step": 55660
    },
    {
      "epoch": 3274.705882352941,
      "grad_norm": 22.14883041381836,
      "learning_rate": 1.7252941176470588e-05,
      "loss": 0.9571,
      "step": 55670
    },
    {
      "epoch": 3275.294117647059,
      "grad_norm": 20.990203857421875,
      "learning_rate": 1.7247058823529414e-05,
      "loss": 0.8626,
      "step": 55680
    },
    {
      "epoch": 3275.8823529411766,
      "grad_norm": 25.303083419799805,
      "learning_rate": 1.7241176470588237e-05,
      "loss": 0.9305,
      "step": 55690
    },
    {
      "epoch": 3276.470588235294,
      "grad_norm": 23.38688850402832,
      "learning_rate": 1.723529411764706e-05,
      "loss": 0.7934,
      "step": 55700
    },
    {
      "epoch": 3277.0588235294117,
      "grad_norm": 18.437620162963867,
      "learning_rate": 1.7229411764705883e-05,
      "loss": 0.9553,
      "step": 55710
    },
    {
      "epoch": 3277.6470588235293,
      "grad_norm": 22.18018341064453,
      "learning_rate": 1.722352941176471e-05,
      "loss": 0.8763,
      "step": 55720
    },
    {
      "epoch": 3278.235294117647,
      "grad_norm": 19.165700912475586,
      "learning_rate": 1.721764705882353e-05,
      "loss": 0.7548,
      "step": 55730
    },
    {
      "epoch": 3278.823529411765,
      "grad_norm": 21.85272216796875,
      "learning_rate": 1.7211764705882355e-05,
      "loss": 0.9762,
      "step": 55740
    },
    {
      "epoch": 3279.4117647058824,
      "grad_norm": 23.551578521728516,
      "learning_rate": 1.720588235294118e-05,
      "loss": 0.7921,
      "step": 55750
    },
    {
      "epoch": 3280.0,
      "grad_norm": 23.999605178833008,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.8788,
      "step": 55760
    },
    {
      "epoch": 3280.5882352941176,
      "grad_norm": 24.089750289916992,
      "learning_rate": 1.7194117647058824e-05,
      "loss": 0.9875,
      "step": 55770
    },
    {
      "epoch": 3281.176470588235,
      "grad_norm": 20.083477020263672,
      "learning_rate": 1.7188235294117647e-05,
      "loss": 0.9469,
      "step": 55780
    },
    {
      "epoch": 3281.764705882353,
      "grad_norm": 26.077417373657227,
      "learning_rate": 1.718235294117647e-05,
      "loss": 0.8921,
      "step": 55790
    },
    {
      "epoch": 3282.3529411764707,
      "grad_norm": 20.863170623779297,
      "learning_rate": 1.7176470588235293e-05,
      "loss": 0.7907,
      "step": 55800
    },
    {
      "epoch": 3282.9411764705883,
      "grad_norm": 25.77458381652832,
      "learning_rate": 1.717058823529412e-05,
      "loss": 0.8494,
      "step": 55810
    },
    {
      "epoch": 3283.529411764706,
      "grad_norm": 21.48360824584961,
      "learning_rate": 1.7164705882352943e-05,
      "loss": 0.8411,
      "step": 55820
    },
    {
      "epoch": 3284.1176470588234,
      "grad_norm": 17.326234817504883,
      "learning_rate": 1.7158823529411766e-05,
      "loss": 0.8846,
      "step": 55830
    },
    {
      "epoch": 3284.705882352941,
      "grad_norm": 28.19487953186035,
      "learning_rate": 1.715294117647059e-05,
      "loss": 0.8206,
      "step": 55840
    },
    {
      "epoch": 3285.294117647059,
      "grad_norm": 17.648658752441406,
      "learning_rate": 1.714705882352941e-05,
      "loss": 0.8218,
      "step": 55850
    },
    {
      "epoch": 3285.8823529411766,
      "grad_norm": 25.62409019470215,
      "learning_rate": 1.7141176470588235e-05,
      "loss": 0.8497,
      "step": 55860
    },
    {
      "epoch": 3286.470588235294,
      "grad_norm": 16.817419052124023,
      "learning_rate": 1.713529411764706e-05,
      "loss": 0.9205,
      "step": 55870
    },
    {
      "epoch": 3287.0588235294117,
      "grad_norm": 21.85613250732422,
      "learning_rate": 1.7129411764705884e-05,
      "loss": 0.8226,
      "step": 55880
    },
    {
      "epoch": 3287.6470588235293,
      "grad_norm": 23.353853225708008,
      "learning_rate": 1.7123529411764707e-05,
      "loss": 0.8911,
      "step": 55890
    },
    {
      "epoch": 3288.235294117647,
      "grad_norm": 28.643165588378906,
      "learning_rate": 1.711764705882353e-05,
      "loss": 0.8991,
      "step": 55900
    },
    {
      "epoch": 3288.823529411765,
      "grad_norm": 19.989208221435547,
      "learning_rate": 1.7111764705882356e-05,
      "loss": 0.7915,
      "step": 55910
    },
    {
      "epoch": 3289.4117647058824,
      "grad_norm": 16.53631019592285,
      "learning_rate": 1.7105882352941176e-05,
      "loss": 0.8716,
      "step": 55920
    },
    {
      "epoch": 3290.0,
      "grad_norm": 22.53527069091797,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.8976,
      "step": 55930
    },
    {
      "epoch": 3290.5882352941176,
      "grad_norm": 19.791391372680664,
      "learning_rate": 1.7094117647058825e-05,
      "loss": 0.8678,
      "step": 55940
    },
    {
      "epoch": 3291.176470588235,
      "grad_norm": 27.570032119750977,
      "learning_rate": 1.7088235294117645e-05,
      "loss": 0.7931,
      "step": 55950
    },
    {
      "epoch": 3291.764705882353,
      "grad_norm": 17.91231918334961,
      "learning_rate": 1.708235294117647e-05,
      "loss": 0.8264,
      "step": 55960
    },
    {
      "epoch": 3292.3529411764707,
      "grad_norm": 18.344961166381836,
      "learning_rate": 1.7076470588235294e-05,
      "loss": 0.8597,
      "step": 55970
    },
    {
      "epoch": 3292.9411764705883,
      "grad_norm": 23.210681915283203,
      "learning_rate": 1.7070588235294117e-05,
      "loss": 0.8358,
      "step": 55980
    },
    {
      "epoch": 3293.529411764706,
      "grad_norm": 23.852928161621094,
      "learning_rate": 1.706470588235294e-05,
      "loss": 0.8658,
      "step": 55990
    },
    {
      "epoch": 3294.1176470588234,
      "grad_norm": 23.211973190307617,
      "learning_rate": 1.7058823529411767e-05,
      "loss": 0.8211,
      "step": 56000
    },
    {
      "epoch": 3294.705882352941,
      "grad_norm": 18.092241287231445,
      "learning_rate": 1.705294117647059e-05,
      "loss": 0.8234,
      "step": 56010
    },
    {
      "epoch": 3295.294117647059,
      "grad_norm": 20.261613845825195,
      "learning_rate": 1.7047058823529413e-05,
      "loss": 0.8833,
      "step": 56020
    },
    {
      "epoch": 3295.8823529411766,
      "grad_norm": 25.046342849731445,
      "learning_rate": 1.7041176470588236e-05,
      "loss": 0.9145,
      "step": 56030
    },
    {
      "epoch": 3296.470588235294,
      "grad_norm": 20.52972412109375,
      "learning_rate": 1.7035294117647062e-05,
      "loss": 0.7361,
      "step": 56040
    },
    {
      "epoch": 3297.0588235294117,
      "grad_norm": 19.836069107055664,
      "learning_rate": 1.702941176470588e-05,
      "loss": 0.7861,
      "step": 56050
    },
    {
      "epoch": 3297.6470588235293,
      "grad_norm": 22.109012603759766,
      "learning_rate": 1.7023529411764708e-05,
      "loss": 0.7846,
      "step": 56060
    },
    {
      "epoch": 3298.235294117647,
      "grad_norm": 25.267980575561523,
      "learning_rate": 1.701764705882353e-05,
      "loss": 0.7445,
      "step": 56070
    },
    {
      "epoch": 3298.823529411765,
      "grad_norm": 18.007165908813477,
      "learning_rate": 1.7011764705882354e-05,
      "loss": 0.8781,
      "step": 56080
    },
    {
      "epoch": 3299.4117647058824,
      "grad_norm": 29.2622127532959,
      "learning_rate": 1.7005882352941177e-05,
      "loss": 0.9319,
      "step": 56090
    },
    {
      "epoch": 3300.0,
      "grad_norm": 20.298288345336914,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.7716,
      "step": 56100
    },
    {
      "epoch": 3300.5882352941176,
      "grad_norm": 21.380643844604492,
      "learning_rate": 1.6994117647058823e-05,
      "loss": 0.8627,
      "step": 56110
    },
    {
      "epoch": 3301.176470588235,
      "grad_norm": 19.65178108215332,
      "learning_rate": 1.698823529411765e-05,
      "loss": 0.9936,
      "step": 56120
    },
    {
      "epoch": 3301.764705882353,
      "grad_norm": 21.81378173828125,
      "learning_rate": 1.6982352941176472e-05,
      "loss": 0.9336,
      "step": 56130
    },
    {
      "epoch": 3302.3529411764707,
      "grad_norm": 17.488510131835938,
      "learning_rate": 1.6976470588235295e-05,
      "loss": 0.9101,
      "step": 56140
    },
    {
      "epoch": 3302.9411764705883,
      "grad_norm": 17.23503875732422,
      "learning_rate": 1.6970588235294118e-05,
      "loss": 0.8955,
      "step": 56150
    },
    {
      "epoch": 3303.529411764706,
      "grad_norm": 15.516153335571289,
      "learning_rate": 1.696470588235294e-05,
      "loss": 0.7827,
      "step": 56160
    },
    {
      "epoch": 3304.1176470588234,
      "grad_norm": 17.078227996826172,
      "learning_rate": 1.6958823529411768e-05,
      "loss": 0.81,
      "step": 56170
    },
    {
      "epoch": 3304.705882352941,
      "grad_norm": 24.81496810913086,
      "learning_rate": 1.6952941176470587e-05,
      "loss": 0.8627,
      "step": 56180
    },
    {
      "epoch": 3305.294117647059,
      "grad_norm": 16.103759765625,
      "learning_rate": 1.6947058823529414e-05,
      "loss": 0.8012,
      "step": 56190
    },
    {
      "epoch": 3305.8823529411766,
      "grad_norm": 19.162019729614258,
      "learning_rate": 1.6941176470588237e-05,
      "loss": 0.8427,
      "step": 56200
    },
    {
      "epoch": 3306.470588235294,
      "grad_norm": 19.5113468170166,
      "learning_rate": 1.693529411764706e-05,
      "loss": 0.8135,
      "step": 56210
    },
    {
      "epoch": 3307.0588235294117,
      "grad_norm": 17.315635681152344,
      "learning_rate": 1.6929411764705882e-05,
      "loss": 0.8748,
      "step": 56220
    },
    {
      "epoch": 3307.6470588235293,
      "grad_norm": 21.63734245300293,
      "learning_rate": 1.692352941176471e-05,
      "loss": 0.8351,
      "step": 56230
    },
    {
      "epoch": 3308.235294117647,
      "grad_norm": 16.686702728271484,
      "learning_rate": 1.691764705882353e-05,
      "loss": 0.9132,
      "step": 56240
    },
    {
      "epoch": 3308.823529411765,
      "grad_norm": 27.388315200805664,
      "learning_rate": 1.6911764705882355e-05,
      "loss": 0.8615,
      "step": 56250
    },
    {
      "epoch": 3309.4117647058824,
      "grad_norm": 17.544986724853516,
      "learning_rate": 1.6905882352941178e-05,
      "loss": 0.8166,
      "step": 56260
    },
    {
      "epoch": 3310.0,
      "grad_norm": 32.00606155395508,
      "learning_rate": 1.69e-05,
      "loss": 0.9313,
      "step": 56270
    },
    {
      "epoch": 3310.5882352941176,
      "grad_norm": 23.6120662689209,
      "learning_rate": 1.6894117647058824e-05,
      "loss": 0.8797,
      "step": 56280
    },
    {
      "epoch": 3311.176470588235,
      "grad_norm": 20.962018966674805,
      "learning_rate": 1.688823529411765e-05,
      "loss": 0.8301,
      "step": 56290
    },
    {
      "epoch": 3311.764705882353,
      "grad_norm": 21.52889633178711,
      "learning_rate": 1.688235294117647e-05,
      "loss": 0.8641,
      "step": 56300
    },
    {
      "epoch": 3312.3529411764707,
      "grad_norm": 22.240772247314453,
      "learning_rate": 1.6876470588235293e-05,
      "loss": 0.7797,
      "step": 56310
    },
    {
      "epoch": 3312.9411764705883,
      "grad_norm": 22.298450469970703,
      "learning_rate": 1.687058823529412e-05,
      "loss": 0.9151,
      "step": 56320
    },
    {
      "epoch": 3313.529411764706,
      "grad_norm": 24.110450744628906,
      "learning_rate": 1.6864705882352942e-05,
      "loss": 0.7621,
      "step": 56330
    },
    {
      "epoch": 3314.1176470588234,
      "grad_norm": 12.51446533203125,
      "learning_rate": 1.6858823529411765e-05,
      "loss": 0.8032,
      "step": 56340
    },
    {
      "epoch": 3314.705882352941,
      "grad_norm": 20.8297061920166,
      "learning_rate": 1.6852941176470588e-05,
      "loss": 0.8241,
      "step": 56350
    },
    {
      "epoch": 3315.294117647059,
      "grad_norm": 15.400410652160645,
      "learning_rate": 1.6847058823529414e-05,
      "loss": 0.8426,
      "step": 56360
    },
    {
      "epoch": 3315.8823529411766,
      "grad_norm": 25.257186889648438,
      "learning_rate": 1.6841176470588234e-05,
      "loss": 0.7746,
      "step": 56370
    },
    {
      "epoch": 3316.470588235294,
      "grad_norm": 19.082141876220703,
      "learning_rate": 1.683529411764706e-05,
      "loss": 0.7372,
      "step": 56380
    },
    {
      "epoch": 3317.0588235294117,
      "grad_norm": 23.658702850341797,
      "learning_rate": 1.6829411764705883e-05,
      "loss": 0.8492,
      "step": 56390
    },
    {
      "epoch": 3317.6470588235293,
      "grad_norm": 19.826684951782227,
      "learning_rate": 1.6823529411764706e-05,
      "loss": 0.946,
      "step": 56400
    },
    {
      "epoch": 3318.235294117647,
      "grad_norm": 19.184602737426758,
      "learning_rate": 1.681764705882353e-05,
      "loss": 0.7965,
      "step": 56410
    },
    {
      "epoch": 3318.823529411765,
      "grad_norm": 18.656354904174805,
      "learning_rate": 1.6811764705882356e-05,
      "loss": 0.7852,
      "step": 56420
    },
    {
      "epoch": 3319.4117647058824,
      "grad_norm": 22.149457931518555,
      "learning_rate": 1.6805882352941175e-05,
      "loss": 0.8307,
      "step": 56430
    },
    {
      "epoch": 3320.0,
      "grad_norm": 38.248260498046875,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.8801,
      "step": 56440
    },
    {
      "epoch": 3320.5882352941176,
      "grad_norm": 18.281091690063477,
      "learning_rate": 1.6794117647058825e-05,
      "loss": 0.8477,
      "step": 56450
    },
    {
      "epoch": 3321.176470588235,
      "grad_norm": 16.968677520751953,
      "learning_rate": 1.6788235294117648e-05,
      "loss": 0.8909,
      "step": 56460
    },
    {
      "epoch": 3321.764705882353,
      "grad_norm": 25.127370834350586,
      "learning_rate": 1.678235294117647e-05,
      "loss": 0.8611,
      "step": 56470
    },
    {
      "epoch": 3322.3529411764707,
      "grad_norm": 21.177335739135742,
      "learning_rate": 1.6776470588235297e-05,
      "loss": 0.7832,
      "step": 56480
    },
    {
      "epoch": 3322.9411764705883,
      "grad_norm": 19.944499969482422,
      "learning_rate": 1.677058823529412e-05,
      "loss": 0.8325,
      "step": 56490
    },
    {
      "epoch": 3323.529411764706,
      "grad_norm": 16.64441680908203,
      "learning_rate": 1.676470588235294e-05,
      "loss": 0.8553,
      "step": 56500
    },
    {
      "epoch": 3324.1176470588234,
      "grad_norm": 16.80165672302246,
      "learning_rate": 1.6758823529411766e-05,
      "loss": 0.8654,
      "step": 56510
    },
    {
      "epoch": 3324.705882352941,
      "grad_norm": 24.27324676513672,
      "learning_rate": 1.675294117647059e-05,
      "loss": 0.8832,
      "step": 56520
    },
    {
      "epoch": 3325.294117647059,
      "grad_norm": 22.28628158569336,
      "learning_rate": 1.6747058823529412e-05,
      "loss": 0.8946,
      "step": 56530
    },
    {
      "epoch": 3325.8823529411766,
      "grad_norm": 22.485748291015625,
      "learning_rate": 1.6741176470588235e-05,
      "loss": 0.8946,
      "step": 56540
    },
    {
      "epoch": 3326.470588235294,
      "grad_norm": 21.005624771118164,
      "learning_rate": 1.673529411764706e-05,
      "loss": 0.8277,
      "step": 56550
    },
    {
      "epoch": 3327.0588235294117,
      "grad_norm": 21.247333526611328,
      "learning_rate": 1.672941176470588e-05,
      "loss": 0.9379,
      "step": 56560
    },
    {
      "epoch": 3327.6470588235293,
      "grad_norm": 25.042959213256836,
      "learning_rate": 1.6723529411764707e-05,
      "loss": 0.7323,
      "step": 56570
    },
    {
      "epoch": 3328.235294117647,
      "grad_norm": 20.646596908569336,
      "learning_rate": 1.671764705882353e-05,
      "loss": 0.7574,
      "step": 56580
    },
    {
      "epoch": 3328.823529411765,
      "grad_norm": 22.002986907958984,
      "learning_rate": 1.6711764705882353e-05,
      "loss": 0.8991,
      "step": 56590
    },
    {
      "epoch": 3329.4117647058824,
      "grad_norm": 19.418359756469727,
      "learning_rate": 1.6705882352941176e-05,
      "loss": 0.9915,
      "step": 56600
    },
    {
      "epoch": 3330.0,
      "grad_norm": 25.740341186523438,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.8248,
      "step": 56610
    },
    {
      "epoch": 3330.5882352941176,
      "grad_norm": 19.47715187072754,
      "learning_rate": 1.6694117647058822e-05,
      "loss": 0.8074,
      "step": 56620
    },
    {
      "epoch": 3331.176470588235,
      "grad_norm": 23.535036087036133,
      "learning_rate": 1.668823529411765e-05,
      "loss": 0.8936,
      "step": 56630
    },
    {
      "epoch": 3331.764705882353,
      "grad_norm": 24.999977111816406,
      "learning_rate": 1.6682352941176472e-05,
      "loss": 0.775,
      "step": 56640
    },
    {
      "epoch": 3332.3529411764707,
      "grad_norm": 20.21112632751465,
      "learning_rate": 1.6676470588235295e-05,
      "loss": 0.8712,
      "step": 56650
    },
    {
      "epoch": 3332.9411764705883,
      "grad_norm": 15.951081275939941,
      "learning_rate": 1.6670588235294118e-05,
      "loss": 1.0163,
      "step": 56660
    },
    {
      "epoch": 3333.529411764706,
      "grad_norm": 14.902759552001953,
      "learning_rate": 1.666470588235294e-05,
      "loss": 0.8174,
      "step": 56670
    },
    {
      "epoch": 3334.1176470588234,
      "grad_norm": 23.53375244140625,
      "learning_rate": 1.6658823529411767e-05,
      "loss": 0.8725,
      "step": 56680
    },
    {
      "epoch": 3334.705882352941,
      "grad_norm": 20.088319778442383,
      "learning_rate": 1.6652941176470587e-05,
      "loss": 0.7998,
      "step": 56690
    },
    {
      "epoch": 3335.294117647059,
      "grad_norm": 17.595584869384766,
      "learning_rate": 1.6647058823529413e-05,
      "loss": 0.7605,
      "step": 56700
    },
    {
      "epoch": 3335.8823529411766,
      "grad_norm": 19.400890350341797,
      "learning_rate": 1.6641176470588236e-05,
      "loss": 0.8499,
      "step": 56710
    },
    {
      "epoch": 3336.470588235294,
      "grad_norm": 20.645462036132812,
      "learning_rate": 1.663529411764706e-05,
      "loss": 0.8117,
      "step": 56720
    },
    {
      "epoch": 3337.0588235294117,
      "grad_norm": 18.28506088256836,
      "learning_rate": 1.6629411764705882e-05,
      "loss": 0.8764,
      "step": 56730
    },
    {
      "epoch": 3337.6470588235293,
      "grad_norm": 27.292011260986328,
      "learning_rate": 1.662352941176471e-05,
      "loss": 0.8514,
      "step": 56740
    },
    {
      "epoch": 3338.235294117647,
      "grad_norm": 21.002056121826172,
      "learning_rate": 1.6617647058823528e-05,
      "loss": 0.844,
      "step": 56750
    },
    {
      "epoch": 3338.823529411765,
      "grad_norm": 19.33551597595215,
      "learning_rate": 1.6611764705882354e-05,
      "loss": 0.7862,
      "step": 56760
    },
    {
      "epoch": 3339.4117647058824,
      "grad_norm": 24.346694946289062,
      "learning_rate": 1.6605882352941177e-05,
      "loss": 0.82,
      "step": 56770
    },
    {
      "epoch": 3340.0,
      "grad_norm": 23.6087703704834,
      "learning_rate": 1.66e-05,
      "loss": 0.8117,
      "step": 56780
    },
    {
      "epoch": 3340.5882352941176,
      "grad_norm": 16.03629493713379,
      "learning_rate": 1.6594117647058823e-05,
      "loss": 0.7365,
      "step": 56790
    },
    {
      "epoch": 3341.176470588235,
      "grad_norm": 19.79296112060547,
      "learning_rate": 1.658823529411765e-05,
      "loss": 0.8197,
      "step": 56800
    },
    {
      "epoch": 3341.764705882353,
      "grad_norm": 22.318490982055664,
      "learning_rate": 1.6582352941176473e-05,
      "loss": 0.7716,
      "step": 56810
    },
    {
      "epoch": 3342.3529411764707,
      "grad_norm": 28.48079490661621,
      "learning_rate": 1.6576470588235296e-05,
      "loss": 0.8735,
      "step": 56820
    },
    {
      "epoch": 3342.9411764705883,
      "grad_norm": 18.07309341430664,
      "learning_rate": 1.657058823529412e-05,
      "loss": 0.7796,
      "step": 56830
    },
    {
      "epoch": 3343.529411764706,
      "grad_norm": 23.85523223876953,
      "learning_rate": 1.656470588235294e-05,
      "loss": 0.802,
      "step": 56840
    },
    {
      "epoch": 3344.1176470588234,
      "grad_norm": 21.127134323120117,
      "learning_rate": 1.6558823529411765e-05,
      "loss": 0.8268,
      "step": 56850
    },
    {
      "epoch": 3344.705882352941,
      "grad_norm": 17.55326271057129,
      "learning_rate": 1.6552941176470588e-05,
      "loss": 0.7986,
      "step": 56860
    },
    {
      "epoch": 3345.294117647059,
      "grad_norm": 17.79108428955078,
      "learning_rate": 1.6547058823529414e-05,
      "loss": 0.8322,
      "step": 56870
    },
    {
      "epoch": 3345.8823529411766,
      "grad_norm": 20.75422477722168,
      "learning_rate": 1.6541176470588234e-05,
      "loss": 0.783,
      "step": 56880
    },
    {
      "epoch": 3346.470588235294,
      "grad_norm": 23.724058151245117,
      "learning_rate": 1.653529411764706e-05,
      "loss": 0.7147,
      "step": 56890
    },
    {
      "epoch": 3347.0588235294117,
      "grad_norm": 19.455291748046875,
      "learning_rate": 1.6529411764705883e-05,
      "loss": 0.8286,
      "step": 56900
    },
    {
      "epoch": 3347.6470588235293,
      "grad_norm": 20.41352081298828,
      "learning_rate": 1.6523529411764706e-05,
      "loss": 0.7996,
      "step": 56910
    },
    {
      "epoch": 3348.235294117647,
      "grad_norm": 24.501686096191406,
      "learning_rate": 1.651764705882353e-05,
      "loss": 0.9176,
      "step": 56920
    },
    {
      "epoch": 3348.823529411765,
      "grad_norm": 27.12961196899414,
      "learning_rate": 1.6511764705882355e-05,
      "loss": 0.8528,
      "step": 56930
    },
    {
      "epoch": 3349.4117647058824,
      "grad_norm": 18.88514518737793,
      "learning_rate": 1.6505882352941175e-05,
      "loss": 0.8619,
      "step": 56940
    },
    {
      "epoch": 3350.0,
      "grad_norm": 15.978726387023926,
      "learning_rate": 1.65e-05,
      "loss": 0.8543,
      "step": 56950
    },
    {
      "epoch": 3350.5882352941176,
      "grad_norm": 26.202552795410156,
      "learning_rate": 1.6494117647058824e-05,
      "loss": 0.807,
      "step": 56960
    },
    {
      "epoch": 3351.176470588235,
      "grad_norm": 21.291296005249023,
      "learning_rate": 1.6488235294117647e-05,
      "loss": 0.8546,
      "step": 56970
    },
    {
      "epoch": 3351.764705882353,
      "grad_norm": 22.22345733642578,
      "learning_rate": 1.648235294117647e-05,
      "loss": 0.7885,
      "step": 56980
    },
    {
      "epoch": 3352.3529411764707,
      "grad_norm": 26.79520606994629,
      "learning_rate": 1.6476470588235297e-05,
      "loss": 0.8432,
      "step": 56990
    },
    {
      "epoch": 3352.9411764705883,
      "grad_norm": 17.336627960205078,
      "learning_rate": 1.647058823529412e-05,
      "loss": 0.8051,
      "step": 57000
    },
    {
      "epoch": 3353.529411764706,
      "grad_norm": 18.815696716308594,
      "learning_rate": 1.6464705882352943e-05,
      "loss": 0.9187,
      "step": 57010
    },
    {
      "epoch": 3354.1176470588234,
      "grad_norm": 20.414682388305664,
      "learning_rate": 1.6458823529411766e-05,
      "loss": 0.8723,
      "step": 57020
    },
    {
      "epoch": 3354.705882352941,
      "grad_norm": 28.66095542907715,
      "learning_rate": 1.645294117647059e-05,
      "loss": 0.8432,
      "step": 57030
    },
    {
      "epoch": 3355.294117647059,
      "grad_norm": 20.022951126098633,
      "learning_rate": 1.644705882352941e-05,
      "loss": 0.8815,
      "step": 57040
    },
    {
      "epoch": 3355.8823529411766,
      "grad_norm": 21.268321990966797,
      "learning_rate": 1.6441176470588235e-05,
      "loss": 0.8742,
      "step": 57050
    },
    {
      "epoch": 3356.470588235294,
      "grad_norm": 23.59160041809082,
      "learning_rate": 1.643529411764706e-05,
      "loss": 0.8465,
      "step": 57060
    },
    {
      "epoch": 3357.0588235294117,
      "grad_norm": 19.751110076904297,
      "learning_rate": 1.642941176470588e-05,
      "loss": 0.8531,
      "step": 57070
    },
    {
      "epoch": 3357.6470588235293,
      "grad_norm": 17.267749786376953,
      "learning_rate": 1.6423529411764707e-05,
      "loss": 0.9176,
      "step": 57080
    },
    {
      "epoch": 3358.235294117647,
      "grad_norm": 24.475250244140625,
      "learning_rate": 1.641764705882353e-05,
      "loss": 0.8282,
      "step": 57090
    },
    {
      "epoch": 3358.823529411765,
      "grad_norm": 23.293581008911133,
      "learning_rate": 1.6411764705882353e-05,
      "loss": 0.8908,
      "step": 57100
    },
    {
      "epoch": 3359.4117647058824,
      "grad_norm": 20.47808265686035,
      "learning_rate": 1.6405882352941176e-05,
      "loss": 0.8016,
      "step": 57110
    },
    {
      "epoch": 3360.0,
      "grad_norm": 24.193078994750977,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.7996,
      "step": 57120
    },
    {
      "epoch": 3360.5882352941176,
      "grad_norm": 20.952144622802734,
      "learning_rate": 1.6394117647058825e-05,
      "loss": 0.828,
      "step": 57130
    },
    {
      "epoch": 3361.176470588235,
      "grad_norm": 29.422189712524414,
      "learning_rate": 1.6388235294117648e-05,
      "loss": 0.7962,
      "step": 57140
    },
    {
      "epoch": 3361.764705882353,
      "grad_norm": 23.241249084472656,
      "learning_rate": 1.638235294117647e-05,
      "loss": 0.8894,
      "step": 57150
    },
    {
      "epoch": 3362.3529411764707,
      "grad_norm": 19.389692306518555,
      "learning_rate": 1.6376470588235298e-05,
      "loss": 0.8019,
      "step": 57160
    },
    {
      "epoch": 3362.9411764705883,
      "grad_norm": 19.5718994140625,
      "learning_rate": 1.6370588235294117e-05,
      "loss": 0.8482,
      "step": 57170
    },
    {
      "epoch": 3363.529411764706,
      "grad_norm": 24.170761108398438,
      "learning_rate": 1.6364705882352944e-05,
      "loss": 0.7901,
      "step": 57180
    },
    {
      "epoch": 3364.1176470588234,
      "grad_norm": 19.5249080657959,
      "learning_rate": 1.6358823529411767e-05,
      "loss": 0.862,
      "step": 57190
    },
    {
      "epoch": 3364.705882352941,
      "grad_norm": 20.705774307250977,
      "learning_rate": 1.635294117647059e-05,
      "loss": 0.8381,
      "step": 57200
    },
    {
      "epoch": 3365.294117647059,
      "grad_norm": 23.200809478759766,
      "learning_rate": 1.6347058823529413e-05,
      "loss": 0.8035,
      "step": 57210
    },
    {
      "epoch": 3365.8823529411766,
      "grad_norm": 26.502986907958984,
      "learning_rate": 1.6341176470588235e-05,
      "loss": 0.8382,
      "step": 57220
    },
    {
      "epoch": 3366.470588235294,
      "grad_norm": 17.818235397338867,
      "learning_rate": 1.633529411764706e-05,
      "loss": 0.8861,
      "step": 57230
    },
    {
      "epoch": 3367.0588235294117,
      "grad_norm": 25.515899658203125,
      "learning_rate": 1.632941176470588e-05,
      "loss": 0.815,
      "step": 57240
    },
    {
      "epoch": 3367.6470588235293,
      "grad_norm": 24.043956756591797,
      "learning_rate": 1.6323529411764708e-05,
      "loss": 0.8184,
      "step": 57250
    },
    {
      "epoch": 3368.235294117647,
      "grad_norm": 15.66261100769043,
      "learning_rate": 1.631764705882353e-05,
      "loss": 0.7386,
      "step": 57260
    },
    {
      "epoch": 3368.823529411765,
      "grad_norm": 23.791471481323242,
      "learning_rate": 1.6311764705882354e-05,
      "loss": 0.9773,
      "step": 57270
    },
    {
      "epoch": 3369.4117647058824,
      "grad_norm": 19.221532821655273,
      "learning_rate": 1.6305882352941177e-05,
      "loss": 0.8712,
      "step": 57280
    },
    {
      "epoch": 3370.0,
      "grad_norm": 21.160024642944336,
      "learning_rate": 1.63e-05,
      "loss": 0.7599,
      "step": 57290
    },
    {
      "epoch": 3370.5882352941176,
      "grad_norm": 13.473731994628906,
      "learning_rate": 1.6294117647058823e-05,
      "loss": 0.8218,
      "step": 57300
    },
    {
      "epoch": 3371.176470588235,
      "grad_norm": 14.611140251159668,
      "learning_rate": 1.628823529411765e-05,
      "loss": 0.8892,
      "step": 57310
    },
    {
      "epoch": 3371.764705882353,
      "grad_norm": 14.020744323730469,
      "learning_rate": 1.6282352941176472e-05,
      "loss": 0.8128,
      "step": 57320
    },
    {
      "epoch": 3372.3529411764707,
      "grad_norm": 16.318660736083984,
      "learning_rate": 1.6276470588235295e-05,
      "loss": 0.8143,
      "step": 57330
    },
    {
      "epoch": 3372.9411764705883,
      "grad_norm": 15.373407363891602,
      "learning_rate": 1.6270588235294118e-05,
      "loss": 0.8223,
      "step": 57340
    },
    {
      "epoch": 3373.529411764706,
      "grad_norm": 18.992782592773438,
      "learning_rate": 1.6264705882352944e-05,
      "loss": 0.8716,
      "step": 57350
    },
    {
      "epoch": 3374.1176470588234,
      "grad_norm": 19.318538665771484,
      "learning_rate": 1.6258823529411764e-05,
      "loss": 0.8555,
      "step": 57360
    },
    {
      "epoch": 3374.705882352941,
      "grad_norm": 19.094717025756836,
      "learning_rate": 1.625294117647059e-05,
      "loss": 0.7107,
      "step": 57370
    },
    {
      "epoch": 3375.294117647059,
      "grad_norm": 20.661529541015625,
      "learning_rate": 1.6247058823529413e-05,
      "loss": 0.8802,
      "step": 57380
    },
    {
      "epoch": 3375.8823529411766,
      "grad_norm": 20.076040267944336,
      "learning_rate": 1.6241176470588236e-05,
      "loss": 0.8128,
      "step": 57390
    },
    {
      "epoch": 3376.470588235294,
      "grad_norm": 22.138492584228516,
      "learning_rate": 1.623529411764706e-05,
      "loss": 0.8562,
      "step": 57400
    },
    {
      "epoch": 3377.0588235294117,
      "grad_norm": 22.70802116394043,
      "learning_rate": 1.6229411764705882e-05,
      "loss": 0.833,
      "step": 57410
    },
    {
      "epoch": 3377.6470588235293,
      "grad_norm": 23.660388946533203,
      "learning_rate": 1.6223529411764705e-05,
      "loss": 0.7618,
      "step": 57420
    },
    {
      "epoch": 3378.235294117647,
      "grad_norm": 21.638093948364258,
      "learning_rate": 1.621764705882353e-05,
      "loss": 0.8535,
      "step": 57430
    },
    {
      "epoch": 3378.823529411765,
      "grad_norm": 19.24884033203125,
      "learning_rate": 1.6211764705882355e-05,
      "loss": 0.8731,
      "step": 57440
    },
    {
      "epoch": 3379.4117647058824,
      "grad_norm": 22.2987117767334,
      "learning_rate": 1.6205882352941178e-05,
      "loss": 0.9118,
      "step": 57450
    },
    {
      "epoch": 3380.0,
      "grad_norm": 25.297292709350586,
      "learning_rate": 1.62e-05,
      "loss": 0.8854,
      "step": 57460
    },
    {
      "epoch": 3380.5882352941176,
      "grad_norm": 24.17264747619629,
      "learning_rate": 1.6194117647058824e-05,
      "loss": 0.8518,
      "step": 57470
    },
    {
      "epoch": 3381.176470588235,
      "grad_norm": 22.81936264038086,
      "learning_rate": 1.618823529411765e-05,
      "loss": 0.9208,
      "step": 57480
    },
    {
      "epoch": 3381.764705882353,
      "grad_norm": 25.169424057006836,
      "learning_rate": 1.618235294117647e-05,
      "loss": 0.8627,
      "step": 57490
    },
    {
      "epoch": 3382.3529411764707,
      "grad_norm": 22.257919311523438,
      "learning_rate": 1.6176470588235296e-05,
      "loss": 0.8828,
      "step": 57500
    },
    {
      "epoch": 3382.9411764705883,
      "grad_norm": 17.22735023498535,
      "learning_rate": 1.617058823529412e-05,
      "loss": 0.8083,
      "step": 57510
    },
    {
      "epoch": 3383.529411764706,
      "grad_norm": 19.289438247680664,
      "learning_rate": 1.6164705882352942e-05,
      "loss": 0.8279,
      "step": 57520
    },
    {
      "epoch": 3384.1176470588234,
      "grad_norm": 34.20969009399414,
      "learning_rate": 1.6158823529411765e-05,
      "loss": 0.9023,
      "step": 57530
    },
    {
      "epoch": 3384.705882352941,
      "grad_norm": 20.487096786499023,
      "learning_rate": 1.615294117647059e-05,
      "loss": 0.7711,
      "step": 57540
    },
    {
      "epoch": 3385.294117647059,
      "grad_norm": 13.8168306350708,
      "learning_rate": 1.614705882352941e-05,
      "loss": 0.8414,
      "step": 57550
    },
    {
      "epoch": 3385.8823529411766,
      "grad_norm": 20.198272705078125,
      "learning_rate": 1.6141176470588237e-05,
      "loss": 0.8206,
      "step": 57560
    },
    {
      "epoch": 3386.470588235294,
      "grad_norm": 19.968647003173828,
      "learning_rate": 1.613529411764706e-05,
      "loss": 0.8757,
      "step": 57570
    },
    {
      "epoch": 3387.0588235294117,
      "grad_norm": 25.98538589477539,
      "learning_rate": 1.6129411764705883e-05,
      "loss": 0.8833,
      "step": 57580
    },
    {
      "epoch": 3387.6470588235293,
      "grad_norm": 25.408061981201172,
      "learning_rate": 1.6123529411764706e-05,
      "loss": 0.7646,
      "step": 57590
    },
    {
      "epoch": 3388.235294117647,
      "grad_norm": 25.774179458618164,
      "learning_rate": 1.611764705882353e-05,
      "loss": 0.8664,
      "step": 57600
    },
    {
      "epoch": 3388.823529411765,
      "grad_norm": 23.731128692626953,
      "learning_rate": 1.6111764705882352e-05,
      "loss": 0.7237,
      "step": 57610
    },
    {
      "epoch": 3389.4117647058824,
      "grad_norm": 22.792457580566406,
      "learning_rate": 1.6105882352941175e-05,
      "loss": 0.7945,
      "step": 57620
    },
    {
      "epoch": 3390.0,
      "grad_norm": 21.582860946655273,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.8161,
      "step": 57630
    },
    {
      "epoch": 3390.5882352941176,
      "grad_norm": 13.635114669799805,
      "learning_rate": 1.6094117647058825e-05,
      "loss": 0.8151,
      "step": 57640
    },
    {
      "epoch": 3391.176470588235,
      "grad_norm": 20.181625366210938,
      "learning_rate": 1.6088235294117648e-05,
      "loss": 0.7063,
      "step": 57650
    },
    {
      "epoch": 3391.764705882353,
      "grad_norm": 23.797761917114258,
      "learning_rate": 1.608235294117647e-05,
      "loss": 0.8075,
      "step": 57660
    },
    {
      "epoch": 3392.3529411764707,
      "grad_norm": 19.141916275024414,
      "learning_rate": 1.6076470588235297e-05,
      "loss": 0.8195,
      "step": 57670
    },
    {
      "epoch": 3392.9411764705883,
      "grad_norm": 20.689971923828125,
      "learning_rate": 1.6070588235294117e-05,
      "loss": 0.783,
      "step": 57680
    },
    {
      "epoch": 3393.529411764706,
      "grad_norm": 17.32563018798828,
      "learning_rate": 1.6064705882352943e-05,
      "loss": 0.8237,
      "step": 57690
    },
    {
      "epoch": 3394.1176470588234,
      "grad_norm": 21.043310165405273,
      "learning_rate": 1.6058823529411766e-05,
      "loss": 0.8654,
      "step": 57700
    },
    {
      "epoch": 3394.705882352941,
      "grad_norm": 22.315793991088867,
      "learning_rate": 1.605294117647059e-05,
      "loss": 0.8621,
      "step": 57710
    },
    {
      "epoch": 3395.294117647059,
      "grad_norm": 22.06415367126465,
      "learning_rate": 1.6047058823529412e-05,
      "loss": 0.8191,
      "step": 57720
    },
    {
      "epoch": 3395.8823529411766,
      "grad_norm": 16.140003204345703,
      "learning_rate": 1.604117647058824e-05,
      "loss": 0.7105,
      "step": 57730
    },
    {
      "epoch": 3396.470588235294,
      "grad_norm": 24.566673278808594,
      "learning_rate": 1.6035294117647058e-05,
      "loss": 0.7971,
      "step": 57740
    },
    {
      "epoch": 3397.0588235294117,
      "grad_norm": 20.193119049072266,
      "learning_rate": 1.6029411764705884e-05,
      "loss": 0.7561,
      "step": 57750
    },
    {
      "epoch": 3397.6470588235293,
      "grad_norm": 19.441652297973633,
      "learning_rate": 1.6023529411764707e-05,
      "loss": 0.8324,
      "step": 57760
    },
    {
      "epoch": 3398.235294117647,
      "grad_norm": 18.34944725036621,
      "learning_rate": 1.601764705882353e-05,
      "loss": 0.8759,
      "step": 57770
    },
    {
      "epoch": 3398.823529411765,
      "grad_norm": 25.1971492767334,
      "learning_rate": 1.6011764705882353e-05,
      "loss": 0.8446,
      "step": 57780
    },
    {
      "epoch": 3399.4117647058824,
      "grad_norm": 23.212217330932617,
      "learning_rate": 1.6005882352941176e-05,
      "loss": 0.7549,
      "step": 57790
    },
    {
      "epoch": 3400.0,
      "grad_norm": 27.478862762451172,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.827,
      "step": 57800
    },
    {
      "epoch": 3400.5882352941176,
      "grad_norm": 20.538558959960938,
      "learning_rate": 1.5994117647058822e-05,
      "loss": 0.8365,
      "step": 57810
    },
    {
      "epoch": 3401.176470588235,
      "grad_norm": 20.59119415283203,
      "learning_rate": 1.598823529411765e-05,
      "loss": 0.8205,
      "step": 57820
    },
    {
      "epoch": 3401.764705882353,
      "grad_norm": 14.602782249450684,
      "learning_rate": 1.598235294117647e-05,
      "loss": 0.7783,
      "step": 57830
    },
    {
      "epoch": 3402.3529411764707,
      "grad_norm": 24.34177589416504,
      "learning_rate": 1.5976470588235295e-05,
      "loss": 0.8216,
      "step": 57840
    },
    {
      "epoch": 3402.9411764705883,
      "grad_norm": 23.383750915527344,
      "learning_rate": 1.5970588235294118e-05,
      "loss": 0.7994,
      "step": 57850
    },
    {
      "epoch": 3403.529411764706,
      "grad_norm": 16.38304901123047,
      "learning_rate": 1.5964705882352944e-05,
      "loss": 0.7501,
      "step": 57860
    },
    {
      "epoch": 3404.1176470588234,
      "grad_norm": 26.367300033569336,
      "learning_rate": 1.5958823529411764e-05,
      "loss": 0.7846,
      "step": 57870
    },
    {
      "epoch": 3404.705882352941,
      "grad_norm": 18.3131160736084,
      "learning_rate": 1.595294117647059e-05,
      "loss": 0.8628,
      "step": 57880
    },
    {
      "epoch": 3405.294117647059,
      "grad_norm": 21.18074607849121,
      "learning_rate": 1.5947058823529413e-05,
      "loss": 0.9098,
      "step": 57890
    },
    {
      "epoch": 3405.8823529411766,
      "grad_norm": 19.673843383789062,
      "learning_rate": 1.5941176470588236e-05,
      "loss": 0.8078,
      "step": 57900
    },
    {
      "epoch": 3406.470588235294,
      "grad_norm": 17.957923889160156,
      "learning_rate": 1.593529411764706e-05,
      "loss": 0.7745,
      "step": 57910
    },
    {
      "epoch": 3407.0588235294117,
      "grad_norm": 31.315805435180664,
      "learning_rate": 1.5929411764705885e-05,
      "loss": 0.8515,
      "step": 57920
    },
    {
      "epoch": 3407.6470588235293,
      "grad_norm": 24.75242805480957,
      "learning_rate": 1.5923529411764705e-05,
      "loss": 0.8253,
      "step": 57930
    },
    {
      "epoch": 3408.235294117647,
      "grad_norm": 26.775957107543945,
      "learning_rate": 1.5917647058823528e-05,
      "loss": 0.8566,
      "step": 57940
    },
    {
      "epoch": 3408.823529411765,
      "grad_norm": 21.707368850708008,
      "learning_rate": 1.5911764705882354e-05,
      "loss": 0.8363,
      "step": 57950
    },
    {
      "epoch": 3409.4117647058824,
      "grad_norm": 19.783475875854492,
      "learning_rate": 1.5905882352941177e-05,
      "loss": 0.8483,
      "step": 57960
    },
    {
      "epoch": 3410.0,
      "grad_norm": 19.24407386779785,
      "learning_rate": 1.59e-05,
      "loss": 0.7801,
      "step": 57970
    },
    {
      "epoch": 3410.5882352941176,
      "grad_norm": 20.011180877685547,
      "learning_rate": 1.5894117647058823e-05,
      "loss": 0.8565,
      "step": 57980
    },
    {
      "epoch": 3411.176470588235,
      "grad_norm": 24.6345272064209,
      "learning_rate": 1.588823529411765e-05,
      "loss": 0.7774,
      "step": 57990
    },
    {
      "epoch": 3411.764705882353,
      "grad_norm": 20.29096221923828,
      "learning_rate": 1.588235294117647e-05,
      "loss": 0.8898,
      "step": 58000
    },
    {
      "epoch": 3412.3529411764707,
      "grad_norm": 16.852872848510742,
      "learning_rate": 1.5876470588235296e-05,
      "loss": 0.7857,
      "step": 58010
    },
    {
      "epoch": 3412.9411764705883,
      "grad_norm": 18.55028533935547,
      "learning_rate": 1.587058823529412e-05,
      "loss": 0.7507,
      "step": 58020
    },
    {
      "epoch": 3413.529411764706,
      "grad_norm": 25.8789005279541,
      "learning_rate": 1.586470588235294e-05,
      "loss": 0.8146,
      "step": 58030
    },
    {
      "epoch": 3414.1176470588234,
      "grad_norm": 20.318300247192383,
      "learning_rate": 1.5858823529411765e-05,
      "loss": 0.7961,
      "step": 58040
    },
    {
      "epoch": 3414.705882352941,
      "grad_norm": 13.080755233764648,
      "learning_rate": 1.585294117647059e-05,
      "loss": 0.8023,
      "step": 58050
    },
    {
      "epoch": 3415.294117647059,
      "grad_norm": 17.700288772583008,
      "learning_rate": 1.584705882352941e-05,
      "loss": 0.7531,
      "step": 58060
    },
    {
      "epoch": 3415.8823529411766,
      "grad_norm": 20.615713119506836,
      "learning_rate": 1.5841176470588237e-05,
      "loss": 0.9784,
      "step": 58070
    },
    {
      "epoch": 3416.470588235294,
      "grad_norm": 22.273408889770508,
      "learning_rate": 1.583529411764706e-05,
      "loss": 0.9604,
      "step": 58080
    },
    {
      "epoch": 3417.0588235294117,
      "grad_norm": 21.284406661987305,
      "learning_rate": 1.5829411764705883e-05,
      "loss": 0.7372,
      "step": 58090
    },
    {
      "epoch": 3417.6470588235293,
      "grad_norm": 20.259124755859375,
      "learning_rate": 1.5823529411764706e-05,
      "loss": 0.7821,
      "step": 58100
    },
    {
      "epoch": 3418.235294117647,
      "grad_norm": 22.73550033569336,
      "learning_rate": 1.5817647058823532e-05,
      "loss": 0.8004,
      "step": 58110
    },
    {
      "epoch": 3418.823529411765,
      "grad_norm": 26.327280044555664,
      "learning_rate": 1.5811764705882355e-05,
      "loss": 0.8435,
      "step": 58120
    },
    {
      "epoch": 3419.4117647058824,
      "grad_norm": 17.120954513549805,
      "learning_rate": 1.5805882352941175e-05,
      "loss": 0.8025,
      "step": 58130
    },
    {
      "epoch": 3420.0,
      "grad_norm": 19.758262634277344,
      "learning_rate": 1.58e-05,
      "loss": 0.8257,
      "step": 58140
    },
    {
      "epoch": 3420.5882352941176,
      "grad_norm": 20.331836700439453,
      "learning_rate": 1.5794117647058824e-05,
      "loss": 0.8734,
      "step": 58150
    },
    {
      "epoch": 3421.176470588235,
      "grad_norm": 23.8172664642334,
      "learning_rate": 1.5788235294117647e-05,
      "loss": 0.8173,
      "step": 58160
    },
    {
      "epoch": 3421.764705882353,
      "grad_norm": 18.22696304321289,
      "learning_rate": 1.578235294117647e-05,
      "loss": 0.7908,
      "step": 58170
    },
    {
      "epoch": 3422.3529411764707,
      "grad_norm": 25.58456802368164,
      "learning_rate": 1.5776470588235297e-05,
      "loss": 0.8842,
      "step": 58180
    },
    {
      "epoch": 3422.9411764705883,
      "grad_norm": 21.7387752532959,
      "learning_rate": 1.5770588235294116e-05,
      "loss": 0.8279,
      "step": 58190
    },
    {
      "epoch": 3423.529411764706,
      "grad_norm": 24.099037170410156,
      "learning_rate": 1.5764705882352943e-05,
      "loss": 0.7428,
      "step": 58200
    },
    {
      "epoch": 3424.1176470588234,
      "grad_norm": 20.647417068481445,
      "learning_rate": 1.5758823529411765e-05,
      "loss": 0.8345,
      "step": 58210
    },
    {
      "epoch": 3424.705882352941,
      "grad_norm": 13.827418327331543,
      "learning_rate": 1.575294117647059e-05,
      "loss": 0.8198,
      "step": 58220
    },
    {
      "epoch": 3425.294117647059,
      "grad_norm": 17.392784118652344,
      "learning_rate": 1.574705882352941e-05,
      "loss": 0.8419,
      "step": 58230
    },
    {
      "epoch": 3425.8823529411766,
      "grad_norm": 19.100378036499023,
      "learning_rate": 1.5741176470588238e-05,
      "loss": 0.9332,
      "step": 58240
    },
    {
      "epoch": 3426.470588235294,
      "grad_norm": 17.2970027923584,
      "learning_rate": 1.573529411764706e-05,
      "loss": 0.8738,
      "step": 58250
    },
    {
      "epoch": 3427.0588235294117,
      "grad_norm": 20.157394409179688,
      "learning_rate": 1.5729411764705884e-05,
      "loss": 0.7901,
      "step": 58260
    },
    {
      "epoch": 3427.6470588235293,
      "grad_norm": 22.219215393066406,
      "learning_rate": 1.5723529411764707e-05,
      "loss": 0.8794,
      "step": 58270
    },
    {
      "epoch": 3428.235294117647,
      "grad_norm": 26.98482894897461,
      "learning_rate": 1.571764705882353e-05,
      "loss": 0.751,
      "step": 58280
    },
    {
      "epoch": 3428.823529411765,
      "grad_norm": 24.64654541015625,
      "learning_rate": 1.5711764705882353e-05,
      "loss": 0.9047,
      "step": 58290
    },
    {
      "epoch": 3429.4117647058824,
      "grad_norm": 22.00967025756836,
      "learning_rate": 1.570588235294118e-05,
      "loss": 0.7538,
      "step": 58300
    },
    {
      "epoch": 3430.0,
      "grad_norm": 20.06038475036621,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.8884,
      "step": 58310
    },
    {
      "epoch": 3430.5882352941176,
      "grad_norm": 17.279850006103516,
      "learning_rate": 1.5694117647058822e-05,
      "loss": 0.8523,
      "step": 58320
    },
    {
      "epoch": 3431.176470588235,
      "grad_norm": 18.402542114257812,
      "learning_rate": 1.5688235294117648e-05,
      "loss": 0.8013,
      "step": 58330
    },
    {
      "epoch": 3431.764705882353,
      "grad_norm": 15.926928520202637,
      "learning_rate": 1.568235294117647e-05,
      "loss": 0.9261,
      "step": 58340
    },
    {
      "epoch": 3432.3529411764707,
      "grad_norm": 24.01911735534668,
      "learning_rate": 1.5676470588235294e-05,
      "loss": 0.7365,
      "step": 58350
    },
    {
      "epoch": 3432.9411764705883,
      "grad_norm": 13.257966995239258,
      "learning_rate": 1.5670588235294117e-05,
      "loss": 0.8161,
      "step": 58360
    },
    {
      "epoch": 3433.529411764706,
      "grad_norm": 14.219029426574707,
      "learning_rate": 1.5664705882352943e-05,
      "loss": 0.8284,
      "step": 58370
    },
    {
      "epoch": 3434.1176470588234,
      "grad_norm": 19.33255386352539,
      "learning_rate": 1.5658823529411763e-05,
      "loss": 0.7646,
      "step": 58380
    },
    {
      "epoch": 3434.705882352941,
      "grad_norm": 23.766841888427734,
      "learning_rate": 1.565294117647059e-05,
      "loss": 0.845,
      "step": 58390
    },
    {
      "epoch": 3435.294117647059,
      "grad_norm": 16.059823989868164,
      "learning_rate": 1.5647058823529412e-05,
      "loss": 0.8431,
      "step": 58400
    },
    {
      "epoch": 3435.8823529411766,
      "grad_norm": 19.66119956970215,
      "learning_rate": 1.5641176470588235e-05,
      "loss": 0.8205,
      "step": 58410
    },
    {
      "epoch": 3436.470588235294,
      "grad_norm": 19.379594802856445,
      "learning_rate": 1.563529411764706e-05,
      "loss": 0.8443,
      "step": 58420
    },
    {
      "epoch": 3437.0588235294117,
      "grad_norm": 22.73430061340332,
      "learning_rate": 1.5629411764705885e-05,
      "loss": 0.7736,
      "step": 58430
    },
    {
      "epoch": 3437.6470588235293,
      "grad_norm": 36.84958267211914,
      "learning_rate": 1.5623529411764708e-05,
      "loss": 0.7484,
      "step": 58440
    },
    {
      "epoch": 3438.235294117647,
      "grad_norm": 25.935453414916992,
      "learning_rate": 1.561764705882353e-05,
      "loss": 0.8902,
      "step": 58450
    },
    {
      "epoch": 3438.823529411765,
      "grad_norm": 19.835115432739258,
      "learning_rate": 1.5611764705882354e-05,
      "loss": 0.7893,
      "step": 58460
    },
    {
      "epoch": 3439.4117647058824,
      "grad_norm": 20.897306442260742,
      "learning_rate": 1.560588235294118e-05,
      "loss": 0.7508,
      "step": 58470
    },
    {
      "epoch": 3440.0,
      "grad_norm": 18.907806396484375,
      "learning_rate": 1.56e-05,
      "loss": 0.8337,
      "step": 58480
    },
    {
      "epoch": 3440.5882352941176,
      "grad_norm": 13.435164451599121,
      "learning_rate": 1.5594117647058823e-05,
      "loss": 0.7604,
      "step": 58490
    },
    {
      "epoch": 3441.176470588235,
      "grad_norm": 25.482624053955078,
      "learning_rate": 1.558823529411765e-05,
      "loss": 0.7947,
      "step": 58500
    },
    {
      "epoch": 3441.764705882353,
      "grad_norm": 30.356576919555664,
      "learning_rate": 1.558235294117647e-05,
      "loss": 0.9283,
      "step": 58510
    },
    {
      "epoch": 3442.3529411764707,
      "grad_norm": 24.576807022094727,
      "learning_rate": 1.5576470588235295e-05,
      "loss": 0.7778,
      "step": 58520
    },
    {
      "epoch": 3442.9411764705883,
      "grad_norm": 18.295129776000977,
      "learning_rate": 1.5570588235294118e-05,
      "loss": 0.8078,
      "step": 58530
    },
    {
      "epoch": 3443.529411764706,
      "grad_norm": 17.563865661621094,
      "learning_rate": 1.556470588235294e-05,
      "loss": 0.8402,
      "step": 58540
    },
    {
      "epoch": 3444.1176470588234,
      "grad_norm": 22.528812408447266,
      "learning_rate": 1.5558823529411764e-05,
      "loss": 0.8322,
      "step": 58550
    },
    {
      "epoch": 3444.705882352941,
      "grad_norm": 22.671518325805664,
      "learning_rate": 1.555294117647059e-05,
      "loss": 0.8981,
      "step": 58560
    },
    {
      "epoch": 3445.294117647059,
      "grad_norm": 20.639007568359375,
      "learning_rate": 1.5547058823529413e-05,
      "loss": 0.8072,
      "step": 58570
    },
    {
      "epoch": 3445.8823529411766,
      "grad_norm": 23.064472198486328,
      "learning_rate": 1.5541176470588236e-05,
      "loss": 0.9025,
      "step": 58580
    },
    {
      "epoch": 3446.470588235294,
      "grad_norm": 14.79331111907959,
      "learning_rate": 1.553529411764706e-05,
      "loss": 0.894,
      "step": 58590
    },
    {
      "epoch": 3447.0588235294117,
      "grad_norm": 20.970260620117188,
      "learning_rate": 1.5529411764705882e-05,
      "loss": 0.8265,
      "step": 58600
    },
    {
      "epoch": 3447.6470588235293,
      "grad_norm": 20.476665496826172,
      "learning_rate": 1.5523529411764705e-05,
      "loss": 0.8932,
      "step": 58610
    },
    {
      "epoch": 3448.235294117647,
      "grad_norm": 22.16655731201172,
      "learning_rate": 1.5517647058823532e-05,
      "loss": 0.894,
      "step": 58620
    },
    {
      "epoch": 3448.823529411765,
      "grad_norm": 18.070104598999023,
      "learning_rate": 1.5511764705882355e-05,
      "loss": 0.7618,
      "step": 58630
    },
    {
      "epoch": 3449.4117647058824,
      "grad_norm": 19.149431228637695,
      "learning_rate": 1.5505882352941178e-05,
      "loss": 0.8301,
      "step": 58640
    },
    {
      "epoch": 3450.0,
      "grad_norm": 24.334869384765625,
      "learning_rate": 1.55e-05,
      "loss": 0.8267,
      "step": 58650
    },
    {
      "epoch": 3450.5882352941176,
      "grad_norm": 18.754274368286133,
      "learning_rate": 1.5494117647058827e-05,
      "loss": 0.832,
      "step": 58660
    },
    {
      "epoch": 3451.176470588235,
      "grad_norm": 15.85132122039795,
      "learning_rate": 1.5488235294117647e-05,
      "loss": 0.7945,
      "step": 58670
    },
    {
      "epoch": 3451.764705882353,
      "grad_norm": 21.607290267944336,
      "learning_rate": 1.548235294117647e-05,
      "loss": 0.7835,
      "step": 58680
    },
    {
      "epoch": 3452.3529411764707,
      "grad_norm": 22.935245513916016,
      "learning_rate": 1.5476470588235296e-05,
      "loss": 0.8825,
      "step": 58690
    },
    {
      "epoch": 3452.9411764705883,
      "grad_norm": 18.36577033996582,
      "learning_rate": 1.5470588235294116e-05,
      "loss": 0.816,
      "step": 58700
    },
    {
      "epoch": 3453.529411764706,
      "grad_norm": 25.337312698364258,
      "learning_rate": 1.5464705882352942e-05,
      "loss": 0.8155,
      "step": 58710
    },
    {
      "epoch": 3454.1176470588234,
      "grad_norm": 24.5040225982666,
      "learning_rate": 1.5458823529411765e-05,
      "loss": 0.7378,
      "step": 58720
    },
    {
      "epoch": 3454.705882352941,
      "grad_norm": 23.27249526977539,
      "learning_rate": 1.5452941176470588e-05,
      "loss": 0.7528,
      "step": 58730
    },
    {
      "epoch": 3455.294117647059,
      "grad_norm": 18.078811645507812,
      "learning_rate": 1.544705882352941e-05,
      "loss": 0.8311,
      "step": 58740
    },
    {
      "epoch": 3455.8823529411766,
      "grad_norm": 27.330522537231445,
      "learning_rate": 1.5441176470588237e-05,
      "loss": 0.9138,
      "step": 58750
    },
    {
      "epoch": 3456.470588235294,
      "grad_norm": 24.586597442626953,
      "learning_rate": 1.543529411764706e-05,
      "loss": 0.9273,
      "step": 58760
    },
    {
      "epoch": 3457.0588235294117,
      "grad_norm": 23.098543167114258,
      "learning_rate": 1.5429411764705883e-05,
      "loss": 0.8276,
      "step": 58770
    },
    {
      "epoch": 3457.6470588235293,
      "grad_norm": 22.397319793701172,
      "learning_rate": 1.5423529411764706e-05,
      "loss": 0.8413,
      "step": 58780
    },
    {
      "epoch": 3458.235294117647,
      "grad_norm": 19.99289894104004,
      "learning_rate": 1.5417647058823533e-05,
      "loss": 0.7998,
      "step": 58790
    },
    {
      "epoch": 3458.823529411765,
      "grad_norm": 30.384183883666992,
      "learning_rate": 1.5411764705882352e-05,
      "loss": 0.8199,
      "step": 58800
    },
    {
      "epoch": 3459.4117647058824,
      "grad_norm": 21.902057647705078,
      "learning_rate": 1.540588235294118e-05,
      "loss": 0.7916,
      "step": 58810
    },
    {
      "epoch": 3460.0,
      "grad_norm": 27.2833309173584,
      "learning_rate": 1.54e-05,
      "loss": 0.7615,
      "step": 58820
    },
    {
      "epoch": 3460.5882352941176,
      "grad_norm": 15.607229232788086,
      "learning_rate": 1.5394117647058825e-05,
      "loss": 0.7643,
      "step": 58830
    },
    {
      "epoch": 3461.176470588235,
      "grad_norm": 24.124195098876953,
      "learning_rate": 1.5388235294117648e-05,
      "loss": 0.8848,
      "step": 58840
    },
    {
      "epoch": 3461.764705882353,
      "grad_norm": 15.270302772521973,
      "learning_rate": 1.538235294117647e-05,
      "loss": 0.8286,
      "step": 58850
    },
    {
      "epoch": 3462.3529411764707,
      "grad_norm": 18.580699920654297,
      "learning_rate": 1.5376470588235294e-05,
      "loss": 0.8851,
      "step": 58860
    },
    {
      "epoch": 3462.9411764705883,
      "grad_norm": 18.74712371826172,
      "learning_rate": 1.5370588235294117e-05,
      "loss": 0.8443,
      "step": 58870
    },
    {
      "epoch": 3463.529411764706,
      "grad_norm": 16.363021850585938,
      "learning_rate": 1.5364705882352943e-05,
      "loss": 0.8996,
      "step": 58880
    },
    {
      "epoch": 3464.1176470588234,
      "grad_norm": 22.858325958251953,
      "learning_rate": 1.5358823529411766e-05,
      "loss": 0.7864,
      "step": 58890
    },
    {
      "epoch": 3464.705882352941,
      "grad_norm": 17.56058120727539,
      "learning_rate": 1.535294117647059e-05,
      "loss": 0.8447,
      "step": 58900
    },
    {
      "epoch": 3465.294117647059,
      "grad_norm": 23.303789138793945,
      "learning_rate": 1.5347058823529412e-05,
      "loss": 0.765,
      "step": 58910
    },
    {
      "epoch": 3465.8823529411766,
      "grad_norm": 23.754201889038086,
      "learning_rate": 1.5341176470588238e-05,
      "loss": 0.9123,
      "step": 58920
    },
    {
      "epoch": 3466.470588235294,
      "grad_norm": 17.519325256347656,
      "learning_rate": 1.5335294117647058e-05,
      "loss": 0.8199,
      "step": 58930
    },
    {
      "epoch": 3467.0588235294117,
      "grad_norm": 26.84009552001953,
      "learning_rate": 1.5329411764705884e-05,
      "loss": 0.8247,
      "step": 58940
    },
    {
      "epoch": 3467.6470588235293,
      "grad_norm": 29.391345977783203,
      "learning_rate": 1.5323529411764707e-05,
      "loss": 0.8773,
      "step": 58950
    },
    {
      "epoch": 3468.235294117647,
      "grad_norm": 18.302715301513672,
      "learning_rate": 1.531764705882353e-05,
      "loss": 0.7204,
      "step": 58960
    },
    {
      "epoch": 3468.823529411765,
      "grad_norm": 20.039247512817383,
      "learning_rate": 1.5311764705882353e-05,
      "loss": 0.8586,
      "step": 58970
    },
    {
      "epoch": 3469.4117647058824,
      "grad_norm": 18.24381446838379,
      "learning_rate": 1.530588235294118e-05,
      "loss": 0.9022,
      "step": 58980
    },
    {
      "epoch": 3470.0,
      "grad_norm": 35.292259216308594,
      "learning_rate": 1.53e-05,
      "loss": 0.9173,
      "step": 58990
    },
    {
      "epoch": 3470.5882352941176,
      "grad_norm": 22.070480346679688,
      "learning_rate": 1.5294117647058826e-05,
      "loss": 0.7117,
      "step": 59000
    },
    {
      "epoch": 3471.176470588235,
      "grad_norm": 20.708181381225586,
      "learning_rate": 1.528823529411765e-05,
      "loss": 0.8811,
      "step": 59010
    },
    {
      "epoch": 3471.764705882353,
      "grad_norm": 21.820873260498047,
      "learning_rate": 1.528235294117647e-05,
      "loss": 0.7896,
      "step": 59020
    },
    {
      "epoch": 3472.3529411764707,
      "grad_norm": 17.11573600769043,
      "learning_rate": 1.5276470588235295e-05,
      "loss": 0.7786,
      "step": 59030
    },
    {
      "epoch": 3472.9411764705883,
      "grad_norm": 17.2767333984375,
      "learning_rate": 1.5270588235294118e-05,
      "loss": 0.8469,
      "step": 59040
    },
    {
      "epoch": 3473.529411764706,
      "grad_norm": 30.237178802490234,
      "learning_rate": 1.526470588235294e-05,
      "loss": 0.8667,
      "step": 59050
    },
    {
      "epoch": 3474.1176470588234,
      "grad_norm": 18.27979278564453,
      "learning_rate": 1.5258823529411764e-05,
      "loss": 0.8079,
      "step": 59060
    },
    {
      "epoch": 3474.705882352941,
      "grad_norm": 21.391626358032227,
      "learning_rate": 1.525294117647059e-05,
      "loss": 0.7659,
      "step": 59070
    },
    {
      "epoch": 3475.294117647059,
      "grad_norm": 19.3489990234375,
      "learning_rate": 1.5247058823529411e-05,
      "loss": 0.7075,
      "step": 59080
    },
    {
      "epoch": 3475.8823529411766,
      "grad_norm": 25.422605514526367,
      "learning_rate": 1.5241176470588236e-05,
      "loss": 0.948,
      "step": 59090
    },
    {
      "epoch": 3476.470588235294,
      "grad_norm": 22.351957321166992,
      "learning_rate": 1.5235294117647059e-05,
      "loss": 0.8264,
      "step": 59100
    },
    {
      "epoch": 3477.0588235294117,
      "grad_norm": 26.91204833984375,
      "learning_rate": 1.5229411764705884e-05,
      "loss": 0.7943,
      "step": 59110
    },
    {
      "epoch": 3477.6470588235293,
      "grad_norm": 24.553977966308594,
      "learning_rate": 1.5223529411764707e-05,
      "loss": 0.7703,
      "step": 59120
    },
    {
      "epoch": 3478.235294117647,
      "grad_norm": 20.198062896728516,
      "learning_rate": 1.5217647058823531e-05,
      "loss": 0.7801,
      "step": 59130
    },
    {
      "epoch": 3478.823529411765,
      "grad_norm": 19.726943969726562,
      "learning_rate": 1.5211764705882352e-05,
      "loss": 0.9068,
      "step": 59140
    },
    {
      "epoch": 3479.4117647058824,
      "grad_norm": 21.299158096313477,
      "learning_rate": 1.5205882352941179e-05,
      "loss": 0.8455,
      "step": 59150
    },
    {
      "epoch": 3480.0,
      "grad_norm": 23.494958877563477,
      "learning_rate": 1.52e-05,
      "loss": 0.8754,
      "step": 59160
    },
    {
      "epoch": 3480.5882352941176,
      "grad_norm": 21.578832626342773,
      "learning_rate": 1.5194117647058825e-05,
      "loss": 0.8599,
      "step": 59170
    },
    {
      "epoch": 3481.176470588235,
      "grad_norm": 20.300437927246094,
      "learning_rate": 1.5188235294117648e-05,
      "loss": 0.8178,
      "step": 59180
    },
    {
      "epoch": 3481.764705882353,
      "grad_norm": 17.0825138092041,
      "learning_rate": 1.5182352941176473e-05,
      "loss": 0.8053,
      "step": 59190
    },
    {
      "epoch": 3482.3529411764707,
      "grad_norm": 19.194381713867188,
      "learning_rate": 1.5176470588235295e-05,
      "loss": 0.7889,
      "step": 59200
    },
    {
      "epoch": 3482.9411764705883,
      "grad_norm": 19.251083374023438,
      "learning_rate": 1.5170588235294117e-05,
      "loss": 0.7933,
      "step": 59210
    },
    {
      "epoch": 3483.529411764706,
      "grad_norm": 26.487215042114258,
      "learning_rate": 1.5164705882352941e-05,
      "loss": 0.8706,
      "step": 59220
    },
    {
      "epoch": 3484.1176470588234,
      "grad_norm": 27.528051376342773,
      "learning_rate": 1.5158823529411764e-05,
      "loss": 0.7824,
      "step": 59230
    },
    {
      "epoch": 3484.705882352941,
      "grad_norm": 20.13408851623535,
      "learning_rate": 1.5152941176470589e-05,
      "loss": 0.7731,
      "step": 59240
    },
    {
      "epoch": 3485.294117647059,
      "grad_norm": 24.201276779174805,
      "learning_rate": 1.5147058823529412e-05,
      "loss": 0.8269,
      "step": 59250
    },
    {
      "epoch": 3485.8823529411766,
      "grad_norm": 19.283828735351562,
      "learning_rate": 1.5141176470588237e-05,
      "loss": 0.7043,
      "step": 59260
    },
    {
      "epoch": 3486.470588235294,
      "grad_norm": 23.201719284057617,
      "learning_rate": 1.5135294117647058e-05,
      "loss": 0.7148,
      "step": 59270
    },
    {
      "epoch": 3487.0588235294117,
      "grad_norm": 22.859949111938477,
      "learning_rate": 1.5129411764705883e-05,
      "loss": 0.9065,
      "step": 59280
    },
    {
      "epoch": 3487.6470588235293,
      "grad_norm": 22.147384643554688,
      "learning_rate": 1.5123529411764706e-05,
      "loss": 0.8442,
      "step": 59290
    },
    {
      "epoch": 3488.235294117647,
      "grad_norm": 21.29922866821289,
      "learning_rate": 1.511764705882353e-05,
      "loss": 0.8309,
      "step": 59300
    },
    {
      "epoch": 3488.823529411765,
      "grad_norm": 23.375267028808594,
      "learning_rate": 1.5111764705882353e-05,
      "loss": 0.8257,
      "step": 59310
    },
    {
      "epoch": 3489.4117647058824,
      "grad_norm": 18.7404842376709,
      "learning_rate": 1.5105882352941178e-05,
      "loss": 0.8163,
      "step": 59320
    },
    {
      "epoch": 3490.0,
      "grad_norm": 19.093280792236328,
      "learning_rate": 1.51e-05,
      "loss": 0.7745,
      "step": 59330
    },
    {
      "epoch": 3490.5882352941176,
      "grad_norm": 22.03750228881836,
      "learning_rate": 1.5094117647058826e-05,
      "loss": 0.842,
      "step": 59340
    },
    {
      "epoch": 3491.176470588235,
      "grad_norm": 20.994125366210938,
      "learning_rate": 1.5088235294117647e-05,
      "loss": 0.8525,
      "step": 59350
    },
    {
      "epoch": 3491.764705882353,
      "grad_norm": 22.249086380004883,
      "learning_rate": 1.5082352941176472e-05,
      "loss": 0.803,
      "step": 59360
    },
    {
      "epoch": 3492.3529411764707,
      "grad_norm": 20.042007446289062,
      "learning_rate": 1.5076470588235295e-05,
      "loss": 0.757,
      "step": 59370
    },
    {
      "epoch": 3492.9411764705883,
      "grad_norm": 18.707809448242188,
      "learning_rate": 1.507058823529412e-05,
      "loss": 0.7594,
      "step": 59380
    },
    {
      "epoch": 3493.529411764706,
      "grad_norm": 18.99557876586914,
      "learning_rate": 1.5064705882352942e-05,
      "loss": 0.8133,
      "step": 59390
    },
    {
      "epoch": 3494.1176470588234,
      "grad_norm": 14.490240097045898,
      "learning_rate": 1.5058823529411764e-05,
      "loss": 0.78,
      "step": 59400
    },
    {
      "epoch": 3494.705882352941,
      "grad_norm": 25.643835067749023,
      "learning_rate": 1.5052941176470588e-05,
      "loss": 0.8008,
      "step": 59410
    },
    {
      "epoch": 3495.294117647059,
      "grad_norm": 34.273040771484375,
      "learning_rate": 1.5047058823529411e-05,
      "loss": 0.8035,
      "step": 59420
    },
    {
      "epoch": 3495.8823529411766,
      "grad_norm": 22.241212844848633,
      "learning_rate": 1.5041176470588236e-05,
      "loss": 0.7332,
      "step": 59430
    },
    {
      "epoch": 3496.470588235294,
      "grad_norm": 26.859670639038086,
      "learning_rate": 1.5035294117647059e-05,
      "loss": 0.7813,
      "step": 59440
    },
    {
      "epoch": 3497.0588235294117,
      "grad_norm": 22.040470123291016,
      "learning_rate": 1.5029411764705884e-05,
      "loss": 0.7188,
      "step": 59450
    },
    {
      "epoch": 3497.6470588235293,
      "grad_norm": 21.960617065429688,
      "learning_rate": 1.5023529411764705e-05,
      "loss": 0.7336,
      "step": 59460
    },
    {
      "epoch": 3498.235294117647,
      "grad_norm": 21.45025634765625,
      "learning_rate": 1.5017647058823531e-05,
      "loss": 0.824,
      "step": 59470
    },
    {
      "epoch": 3498.823529411765,
      "grad_norm": 30.664522171020508,
      "learning_rate": 1.5011764705882353e-05,
      "loss": 0.8413,
      "step": 59480
    },
    {
      "epoch": 3499.4117647058824,
      "grad_norm": 19.910572052001953,
      "learning_rate": 1.5005882352941177e-05,
      "loss": 0.847,
      "step": 59490
    },
    {
      "epoch": 3500.0,
      "grad_norm": 24.36012077331543,
      "learning_rate": 1.5e-05,
      "loss": 0.6609,
      "step": 59500
    },
    {
      "epoch": 3500.5882352941176,
      "grad_norm": 17.253501892089844,
      "learning_rate": 1.4994117647058825e-05,
      "loss": 0.7823,
      "step": 59510
    },
    {
      "epoch": 3501.176470588235,
      "grad_norm": 23.109651565551758,
      "learning_rate": 1.4988235294117648e-05,
      "loss": 0.8534,
      "step": 59520
    },
    {
      "epoch": 3501.764705882353,
      "grad_norm": 18.79453468322754,
      "learning_rate": 1.4982352941176473e-05,
      "loss": 0.763,
      "step": 59530
    },
    {
      "epoch": 3502.3529411764707,
      "grad_norm": 18.236736297607422,
      "learning_rate": 1.4976470588235294e-05,
      "loss": 0.7733,
      "step": 59540
    },
    {
      "epoch": 3502.9411764705883,
      "grad_norm": 15.258187294006348,
      "learning_rate": 1.497058823529412e-05,
      "loss": 0.8148,
      "step": 59550
    },
    {
      "epoch": 3503.529411764706,
      "grad_norm": 15.37331485748291,
      "learning_rate": 1.4964705882352942e-05,
      "loss": 0.7152,
      "step": 59560
    },
    {
      "epoch": 3504.1176470588234,
      "grad_norm": 19.46794319152832,
      "learning_rate": 1.4958823529411766e-05,
      "loss": 0.8543,
      "step": 59570
    },
    {
      "epoch": 3504.705882352941,
      "grad_norm": 19.178735733032227,
      "learning_rate": 1.495294117647059e-05,
      "loss": 0.776,
      "step": 59580
    },
    {
      "epoch": 3505.294117647059,
      "grad_norm": 24.848066329956055,
      "learning_rate": 1.494705882352941e-05,
      "loss": 0.8704,
      "step": 59590
    },
    {
      "epoch": 3505.8823529411766,
      "grad_norm": 24.0417537689209,
      "learning_rate": 1.4941176470588237e-05,
      "loss": 0.8536,
      "step": 59600
    },
    {
      "epoch": 3506.470588235294,
      "grad_norm": 21.244415283203125,
      "learning_rate": 1.4935294117647058e-05,
      "loss": 0.8218,
      "step": 59610
    },
    {
      "epoch": 3507.0588235294117,
      "grad_norm": 20.761207580566406,
      "learning_rate": 1.4929411764705883e-05,
      "loss": 0.777,
      "step": 59620
    },
    {
      "epoch": 3507.6470588235293,
      "grad_norm": 19.700031280517578,
      "learning_rate": 1.4923529411764706e-05,
      "loss": 0.7372,
      "step": 59630
    },
    {
      "epoch": 3508.235294117647,
      "grad_norm": 20.271411895751953,
      "learning_rate": 1.491764705882353e-05,
      "loss": 0.7797,
      "step": 59640
    },
    {
      "epoch": 3508.823529411765,
      "grad_norm": 19.506946563720703,
      "learning_rate": 1.4911764705882354e-05,
      "loss": 0.8455,
      "step": 59650
    },
    {
      "epoch": 3509.4117647058824,
      "grad_norm": 20.000276565551758,
      "learning_rate": 1.4905882352941178e-05,
      "loss": 0.7699,
      "step": 59660
    },
    {
      "epoch": 3510.0,
      "grad_norm": 33.81155014038086,
      "learning_rate": 1.49e-05,
      "loss": 0.8701,
      "step": 59670
    },
    {
      "epoch": 3510.5882352941176,
      "grad_norm": 19.80653953552246,
      "learning_rate": 1.4894117647058824e-05,
      "loss": 0.8416,
      "step": 59680
    },
    {
      "epoch": 3511.176470588235,
      "grad_norm": 22.1516170501709,
      "learning_rate": 1.4888235294117647e-05,
      "loss": 0.7679,
      "step": 59690
    },
    {
      "epoch": 3511.764705882353,
      "grad_norm": 22.85277557373047,
      "learning_rate": 1.4882352941176472e-05,
      "loss": 0.7876,
      "step": 59700
    },
    {
      "epoch": 3512.3529411764707,
      "grad_norm": 19.099281311035156,
      "learning_rate": 1.4876470588235295e-05,
      "loss": 0.8294,
      "step": 59710
    },
    {
      "epoch": 3512.9411764705883,
      "grad_norm": 18.306114196777344,
      "learning_rate": 1.487058823529412e-05,
      "loss": 0.7655,
      "step": 59720
    },
    {
      "epoch": 3513.529411764706,
      "grad_norm": 20.96232795715332,
      "learning_rate": 1.4864705882352941e-05,
      "loss": 0.7797,
      "step": 59730
    },
    {
      "epoch": 3514.1176470588234,
      "grad_norm": 17.623693466186523,
      "learning_rate": 1.4858823529411767e-05,
      "loss": 0.7391,
      "step": 59740
    },
    {
      "epoch": 3514.705882352941,
      "grad_norm": 28.268918991088867,
      "learning_rate": 1.4852941176470589e-05,
      "loss": 0.8164,
      "step": 59750
    },
    {
      "epoch": 3515.294117647059,
      "grad_norm": 22.942991256713867,
      "learning_rate": 1.4847058823529412e-05,
      "loss": 0.7709,
      "step": 59760
    },
    {
      "epoch": 3515.8823529411766,
      "grad_norm": 24.718372344970703,
      "learning_rate": 1.4841176470588236e-05,
      "loss": 0.8694,
      "step": 59770
    },
    {
      "epoch": 3516.470588235294,
      "grad_norm": 26.82160758972168,
      "learning_rate": 1.4835294117647058e-05,
      "loss": 0.8773,
      "step": 59780
    },
    {
      "epoch": 3517.0588235294117,
      "grad_norm": 19.30507469177246,
      "learning_rate": 1.4829411764705884e-05,
      "loss": 0.7127,
      "step": 59790
    },
    {
      "epoch": 3517.6470588235293,
      "grad_norm": 22.035900115966797,
      "learning_rate": 1.4823529411764705e-05,
      "loss": 0.7921,
      "step": 59800
    },
    {
      "epoch": 3518.235294117647,
      "grad_norm": 22.997791290283203,
      "learning_rate": 1.481764705882353e-05,
      "loss": 0.8603,
      "step": 59810
    },
    {
      "epoch": 3518.823529411765,
      "grad_norm": 26.80372428894043,
      "learning_rate": 1.4811764705882353e-05,
      "loss": 0.8166,
      "step": 59820
    },
    {
      "epoch": 3519.4117647058824,
      "grad_norm": 22.193777084350586,
      "learning_rate": 1.4805882352941178e-05,
      "loss": 0.6841,
      "step": 59830
    },
    {
      "epoch": 3520.0,
      "grad_norm": 16.601184844970703,
      "learning_rate": 1.48e-05,
      "loss": 0.8394,
      "step": 59840
    },
    {
      "epoch": 3520.5882352941176,
      "grad_norm": 16.155332565307617,
      "learning_rate": 1.4794117647058825e-05,
      "loss": 0.8613,
      "step": 59850
    },
    {
      "epoch": 3521.176470588235,
      "grad_norm": 15.981032371520996,
      "learning_rate": 1.4788235294117647e-05,
      "loss": 0.6862,
      "step": 59860
    },
    {
      "epoch": 3521.764705882353,
      "grad_norm": 26.180482864379883,
      "learning_rate": 1.4782352941176473e-05,
      "loss": 0.8177,
      "step": 59870
    },
    {
      "epoch": 3522.3529411764707,
      "grad_norm": 22.147727966308594,
      "learning_rate": 1.4776470588235294e-05,
      "loss": 0.8222,
      "step": 59880
    },
    {
      "epoch": 3522.9411764705883,
      "grad_norm": 25.036022186279297,
      "learning_rate": 1.4770588235294119e-05,
      "loss": 0.7373,
      "step": 59890
    },
    {
      "epoch": 3523.529411764706,
      "grad_norm": 24.14061737060547,
      "learning_rate": 1.4764705882352942e-05,
      "loss": 0.7228,
      "step": 59900
    },
    {
      "epoch": 3524.1176470588234,
      "grad_norm": 24.403532028198242,
      "learning_rate": 1.4758823529411767e-05,
      "loss": 0.7871,
      "step": 59910
    },
    {
      "epoch": 3524.705882352941,
      "grad_norm": 24.160377502441406,
      "learning_rate": 1.475294117647059e-05,
      "loss": 0.8863,
      "step": 59920
    },
    {
      "epoch": 3525.294117647059,
      "grad_norm": 21.6961669921875,
      "learning_rate": 1.4747058823529414e-05,
      "loss": 0.9229,
      "step": 59930
    },
    {
      "epoch": 3525.8823529411766,
      "grad_norm": 15.875289916992188,
      "learning_rate": 1.4741176470588236e-05,
      "loss": 0.8531,
      "step": 59940
    },
    {
      "epoch": 3526.470588235294,
      "grad_norm": 23.788097381591797,
      "learning_rate": 1.4735294117647059e-05,
      "loss": 0.8356,
      "step": 59950
    },
    {
      "epoch": 3527.0588235294117,
      "grad_norm": 20.33127212524414,
      "learning_rate": 1.4729411764705883e-05,
      "loss": 0.7728,
      "step": 59960
    },
    {
      "epoch": 3527.6470588235293,
      "grad_norm": 16.16371726989746,
      "learning_rate": 1.4723529411764706e-05,
      "loss": 0.8139,
      "step": 59970
    },
    {
      "epoch": 3528.235294117647,
      "grad_norm": 21.73666000366211,
      "learning_rate": 1.4717647058823531e-05,
      "loss": 0.8624,
      "step": 59980
    },
    {
      "epoch": 3528.823529411765,
      "grad_norm": 18.977346420288086,
      "learning_rate": 1.4711764705882352e-05,
      "loss": 0.9053,
      "step": 59990
    },
    {
      "epoch": 3529.4117647058824,
      "grad_norm": 21.70342254638672,
      "learning_rate": 1.4705882352941177e-05,
      "loss": 0.7518,
      "step": 60000
    },
    {
      "epoch": 3530.0,
      "grad_norm": 21.048664093017578,
      "learning_rate": 1.47e-05,
      "loss": 0.8415,
      "step": 60010
    },
    {
      "epoch": 3530.5882352941176,
      "grad_norm": 13.205038070678711,
      "learning_rate": 1.4694117647058825e-05,
      "loss": 0.7029,
      "step": 60020
    },
    {
      "epoch": 3531.176470588235,
      "grad_norm": 24.414539337158203,
      "learning_rate": 1.4688235294117648e-05,
      "loss": 0.8127,
      "step": 60030
    },
    {
      "epoch": 3531.764705882353,
      "grad_norm": 23.130468368530273,
      "learning_rate": 1.4682352941176472e-05,
      "loss": 0.9,
      "step": 60040
    },
    {
      "epoch": 3532.3529411764707,
      "grad_norm": 13.977684020996094,
      "learning_rate": 1.4676470588235294e-05,
      "loss": 0.7978,
      "step": 60050
    },
    {
      "epoch": 3532.9411764705883,
      "grad_norm": 19.226343154907227,
      "learning_rate": 1.467058823529412e-05,
      "loss": 0.7341,
      "step": 60060
    },
    {
      "epoch": 3533.529411764706,
      "grad_norm": 24.773380279541016,
      "learning_rate": 1.4664705882352941e-05,
      "loss": 0.9767,
      "step": 60070
    },
    {
      "epoch": 3534.1176470588234,
      "grad_norm": 17.686264038085938,
      "learning_rate": 1.4658823529411766e-05,
      "loss": 0.7362,
      "step": 60080
    },
    {
      "epoch": 3534.705882352941,
      "grad_norm": 17.74374008178711,
      "learning_rate": 1.4652941176470589e-05,
      "loss": 0.7698,
      "step": 60090
    },
    {
      "epoch": 3535.294117647059,
      "grad_norm": 22.40766143798828,
      "learning_rate": 1.4647058823529414e-05,
      "loss": 0.7435,
      "step": 60100
    },
    {
      "epoch": 3535.8823529411766,
      "grad_norm": 29.145978927612305,
      "learning_rate": 1.4641176470588237e-05,
      "loss": 0.8066,
      "step": 60110
    },
    {
      "epoch": 3536.470588235294,
      "grad_norm": 20.988059997558594,
      "learning_rate": 1.4635294117647058e-05,
      "loss": 0.8414,
      "step": 60120
    },
    {
      "epoch": 3537.0588235294117,
      "grad_norm": 19.800884246826172,
      "learning_rate": 1.4629411764705882e-05,
      "loss": 0.7809,
      "step": 60130
    },
    {
      "epoch": 3537.6470588235293,
      "grad_norm": 20.766687393188477,
      "learning_rate": 1.4623529411764705e-05,
      "loss": 0.7979,
      "step": 60140
    },
    {
      "epoch": 3538.235294117647,
      "grad_norm": 20.93494987487793,
      "learning_rate": 1.461764705882353e-05,
      "loss": 0.8434,
      "step": 60150
    },
    {
      "epoch": 3538.823529411765,
      "grad_norm": 24.233531951904297,
      "learning_rate": 1.4611764705882353e-05,
      "loss": 0.7832,
      "step": 60160
    },
    {
      "epoch": 3539.4117647058824,
      "grad_norm": 23.683813095092773,
      "learning_rate": 1.4605882352941178e-05,
      "loss": 0.8157,
      "step": 60170
    },
    {
      "epoch": 3540.0,
      "grad_norm": 23.009042739868164,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.8037,
      "step": 60180
    },
    {
      "epoch": 3540.5882352941176,
      "grad_norm": 19.049644470214844,
      "learning_rate": 1.4594117647058826e-05,
      "loss": 0.7083,
      "step": 60190
    },
    {
      "epoch": 3541.176470588235,
      "grad_norm": 24.95003890991211,
      "learning_rate": 1.4588235294117647e-05,
      "loss": 0.8527,
      "step": 60200
    },
    {
      "epoch": 3541.764705882353,
      "grad_norm": 24.240619659423828,
      "learning_rate": 1.4582352941176471e-05,
      "loss": 0.7293,
      "step": 60210
    },
    {
      "epoch": 3542.3529411764707,
      "grad_norm": 15.786250114440918,
      "learning_rate": 1.4576470588235294e-05,
      "loss": 0.7854,
      "step": 60220
    },
    {
      "epoch": 3542.9411764705883,
      "grad_norm": 20.59397315979004,
      "learning_rate": 1.4570588235294119e-05,
      "loss": 0.8053,
      "step": 60230
    },
    {
      "epoch": 3543.529411764706,
      "grad_norm": 18.61089515686035,
      "learning_rate": 1.4564705882352942e-05,
      "loss": 0.6708,
      "step": 60240
    },
    {
      "epoch": 3544.1176470588234,
      "grad_norm": 23.270999908447266,
      "learning_rate": 1.4558823529411767e-05,
      "loss": 0.8153,
      "step": 60250
    },
    {
      "epoch": 3544.705882352941,
      "grad_norm": 18.652629852294922,
      "learning_rate": 1.4552941176470588e-05,
      "loss": 0.7283,
      "step": 60260
    },
    {
      "epoch": 3545.294117647059,
      "grad_norm": 25.349123001098633,
      "learning_rate": 1.4547058823529413e-05,
      "loss": 0.7956,
      "step": 60270
    },
    {
      "epoch": 3545.8823529411766,
      "grad_norm": 17.27260398864746,
      "learning_rate": 1.4541176470588236e-05,
      "loss": 0.8313,
      "step": 60280
    },
    {
      "epoch": 3546.470588235294,
      "grad_norm": 20.74867057800293,
      "learning_rate": 1.453529411764706e-05,
      "loss": 0.8269,
      "step": 60290
    },
    {
      "epoch": 3547.0588235294117,
      "grad_norm": 20.75346565246582,
      "learning_rate": 1.4529411764705883e-05,
      "loss": 0.8785,
      "step": 60300
    },
    {
      "epoch": 3547.6470588235293,
      "grad_norm": 24.73322105407715,
      "learning_rate": 1.4523529411764705e-05,
      "loss": 0.7879,
      "step": 60310
    },
    {
      "epoch": 3548.235294117647,
      "grad_norm": 22.42517852783203,
      "learning_rate": 1.451764705882353e-05,
      "loss": 0.891,
      "step": 60320
    },
    {
      "epoch": 3548.823529411765,
      "grad_norm": 20.92888641357422,
      "learning_rate": 1.4511764705882352e-05,
      "loss": 0.831,
      "step": 60330
    },
    {
      "epoch": 3549.4117647058824,
      "grad_norm": 22.72154426574707,
      "learning_rate": 1.4505882352941177e-05,
      "loss": 0.7872,
      "step": 60340
    },
    {
      "epoch": 3550.0,
      "grad_norm": 17.962080001831055,
      "learning_rate": 1.45e-05,
      "loss": 0.7578,
      "step": 60350
    },
    {
      "epoch": 3550.5882352941176,
      "grad_norm": 23.89864730834961,
      "learning_rate": 1.4494117647058825e-05,
      "loss": 0.7725,
      "step": 60360
    },
    {
      "epoch": 3551.176470588235,
      "grad_norm": 29.49785804748535,
      "learning_rate": 1.4488235294117646e-05,
      "loss": 0.8165,
      "step": 60370
    },
    {
      "epoch": 3551.764705882353,
      "grad_norm": 14.149751663208008,
      "learning_rate": 1.4482352941176472e-05,
      "loss": 0.662,
      "step": 60380
    },
    {
      "epoch": 3552.3529411764707,
      "grad_norm": 20.885700225830078,
      "learning_rate": 1.4476470588235294e-05,
      "loss": 0.7033,
      "step": 60390
    },
    {
      "epoch": 3552.9411764705883,
      "grad_norm": 21.879390716552734,
      "learning_rate": 1.4470588235294118e-05,
      "loss": 0.8595,
      "step": 60400
    },
    {
      "epoch": 3553.529411764706,
      "grad_norm": 23.8007755279541,
      "learning_rate": 1.4464705882352941e-05,
      "loss": 0.82,
      "step": 60410
    },
    {
      "epoch": 3554.1176470588234,
      "grad_norm": 22.699068069458008,
      "learning_rate": 1.4458823529411766e-05,
      "loss": 0.8138,
      "step": 60420
    },
    {
      "epoch": 3554.705882352941,
      "grad_norm": 24.54437255859375,
      "learning_rate": 1.4452941176470589e-05,
      "loss": 0.8142,
      "step": 60430
    },
    {
      "epoch": 3555.294117647059,
      "grad_norm": 23.717905044555664,
      "learning_rate": 1.4447058823529414e-05,
      "loss": 0.8758,
      "step": 60440
    },
    {
      "epoch": 3555.8823529411766,
      "grad_norm": 21.048126220703125,
      "learning_rate": 1.4441176470588235e-05,
      "loss": 0.8747,
      "step": 60450
    },
    {
      "epoch": 3556.470588235294,
      "grad_norm": 20.952251434326172,
      "learning_rate": 1.4435294117647061e-05,
      "loss": 0.7034,
      "step": 60460
    },
    {
      "epoch": 3557.0588235294117,
      "grad_norm": 16.92565155029297,
      "learning_rate": 1.4429411764705883e-05,
      "loss": 0.8281,
      "step": 60470
    },
    {
      "epoch": 3557.6470588235293,
      "grad_norm": 15.634552955627441,
      "learning_rate": 1.4423529411764707e-05,
      "loss": 0.7094,
      "step": 60480
    },
    {
      "epoch": 3558.235294117647,
      "grad_norm": 20.67137908935547,
      "learning_rate": 1.441764705882353e-05,
      "loss": 0.7042,
      "step": 60490
    },
    {
      "epoch": 3558.823529411765,
      "grad_norm": 18.903656005859375,
      "learning_rate": 1.4411764705882352e-05,
      "loss": 0.8709,
      "step": 60500
    },
    {
      "epoch": 3559.4117647058824,
      "grad_norm": 19.39081382751465,
      "learning_rate": 1.4405882352941178e-05,
      "loss": 0.7541,
      "step": 60510
    },
    {
      "epoch": 3560.0,
      "grad_norm": 34.64303970336914,
      "learning_rate": 1.44e-05,
      "loss": 0.8606,
      "step": 60520
    },
    {
      "epoch": 3560.5882352941176,
      "grad_norm": 21.552579879760742,
      "learning_rate": 1.4394117647058824e-05,
      "loss": 0.9114,
      "step": 60530
    },
    {
      "epoch": 3561.176470588235,
      "grad_norm": 23.638994216918945,
      "learning_rate": 1.4388235294117647e-05,
      "loss": 0.701,
      "step": 60540
    },
    {
      "epoch": 3561.764705882353,
      "grad_norm": 23.316343307495117,
      "learning_rate": 1.4382352941176472e-05,
      "loss": 0.7728,
      "step": 60550
    },
    {
      "epoch": 3562.3529411764707,
      "grad_norm": 21.36653709411621,
      "learning_rate": 1.4376470588235295e-05,
      "loss": 0.7737,
      "step": 60560
    },
    {
      "epoch": 3562.9411764705883,
      "grad_norm": 21.449962615966797,
      "learning_rate": 1.437058823529412e-05,
      "loss": 0.8115,
      "step": 60570
    },
    {
      "epoch": 3563.529411764706,
      "grad_norm": 27.31066131591797,
      "learning_rate": 1.436470588235294e-05,
      "loss": 0.8407,
      "step": 60580
    },
    {
      "epoch": 3564.1176470588234,
      "grad_norm": 19.886810302734375,
      "learning_rate": 1.4358823529411767e-05,
      "loss": 0.7208,
      "step": 60590
    },
    {
      "epoch": 3564.705882352941,
      "grad_norm": 20.00287628173828,
      "learning_rate": 1.4352941176470588e-05,
      "loss": 0.7519,
      "step": 60600
    },
    {
      "epoch": 3565.294117647059,
      "grad_norm": 20.93634033203125,
      "learning_rate": 1.4347058823529413e-05,
      "loss": 0.9017,
      "step": 60610
    },
    {
      "epoch": 3565.8823529411766,
      "grad_norm": 20.532320022583008,
      "learning_rate": 1.4341176470588236e-05,
      "loss": 0.8215,
      "step": 60620
    },
    {
      "epoch": 3566.470588235294,
      "grad_norm": 18.552486419677734,
      "learning_rate": 1.433529411764706e-05,
      "loss": 0.7591,
      "step": 60630
    },
    {
      "epoch": 3567.0588235294117,
      "grad_norm": 24.134197235107422,
      "learning_rate": 1.4329411764705884e-05,
      "loss": 0.7222,
      "step": 60640
    },
    {
      "epoch": 3567.6470588235293,
      "grad_norm": 21.110410690307617,
      "learning_rate": 1.4323529411764708e-05,
      "loss": 0.8556,
      "step": 60650
    },
    {
      "epoch": 3568.235294117647,
      "grad_norm": 26.208797454833984,
      "learning_rate": 1.431764705882353e-05,
      "loss": 0.8795,
      "step": 60660
    },
    {
      "epoch": 3568.823529411765,
      "grad_norm": 15.86374568939209,
      "learning_rate": 1.4311764705882353e-05,
      "loss": 0.72,
      "step": 60670
    },
    {
      "epoch": 3569.4117647058824,
      "grad_norm": 15.270601272583008,
      "learning_rate": 1.4305882352941177e-05,
      "loss": 0.8096,
      "step": 60680
    },
    {
      "epoch": 3570.0,
      "grad_norm": 26.529190063476562,
      "learning_rate": 1.43e-05,
      "loss": 0.8446,
      "step": 60690
    },
    {
      "epoch": 3570.5882352941176,
      "grad_norm": 18.894861221313477,
      "learning_rate": 1.4294117647058825e-05,
      "loss": 0.8316,
      "step": 60700
    },
    {
      "epoch": 3571.176470588235,
      "grad_norm": 17.672563552856445,
      "learning_rate": 1.4288235294117646e-05,
      "loss": 0.8626,
      "step": 60710
    },
    {
      "epoch": 3571.764705882353,
      "grad_norm": 24.587223052978516,
      "learning_rate": 1.4282352941176471e-05,
      "loss": 0.8514,
      "step": 60720
    },
    {
      "epoch": 3572.3529411764707,
      "grad_norm": 14.954214096069336,
      "learning_rate": 1.4276470588235294e-05,
      "loss": 0.8464,
      "step": 60730
    },
    {
      "epoch": 3572.9411764705883,
      "grad_norm": 22.37742805480957,
      "learning_rate": 1.4270588235294119e-05,
      "loss": 0.7977,
      "step": 60740
    },
    {
      "epoch": 3573.529411764706,
      "grad_norm": 30.29615020751953,
      "learning_rate": 1.4264705882352942e-05,
      "loss": 0.8204,
      "step": 60750
    },
    {
      "epoch": 3574.1176470588234,
      "grad_norm": 20.661571502685547,
      "learning_rate": 1.4258823529411766e-05,
      "loss": 0.8307,
      "step": 60760
    },
    {
      "epoch": 3574.705882352941,
      "grad_norm": 19.37163543701172,
      "learning_rate": 1.4252941176470588e-05,
      "loss": 0.8036,
      "step": 60770
    },
    {
      "epoch": 3575.294117647059,
      "grad_norm": 21.623577117919922,
      "learning_rate": 1.4247058823529414e-05,
      "loss": 0.8013,
      "step": 60780
    },
    {
      "epoch": 3575.8823529411766,
      "grad_norm": 26.86762046813965,
      "learning_rate": 1.4241176470588235e-05,
      "loss": 0.8445,
      "step": 60790
    },
    {
      "epoch": 3576.470588235294,
      "grad_norm": 20.693538665771484,
      "learning_rate": 1.423529411764706e-05,
      "loss": 0.7525,
      "step": 60800
    },
    {
      "epoch": 3577.0588235294117,
      "grad_norm": 19.370378494262695,
      "learning_rate": 1.4229411764705883e-05,
      "loss": 0.8218,
      "step": 60810
    },
    {
      "epoch": 3577.6470588235293,
      "grad_norm": 18.542299270629883,
      "learning_rate": 1.4223529411764708e-05,
      "loss": 0.8288,
      "step": 60820
    },
    {
      "epoch": 3578.235294117647,
      "grad_norm": 20.650964736938477,
      "learning_rate": 1.421764705882353e-05,
      "loss": 0.7666,
      "step": 60830
    },
    {
      "epoch": 3578.823529411765,
      "grad_norm": 21.631425857543945,
      "learning_rate": 1.4211764705882355e-05,
      "loss": 0.7966,
      "step": 60840
    },
    {
      "epoch": 3579.4117647058824,
      "grad_norm": 30.120824813842773,
      "learning_rate": 1.4205882352941177e-05,
      "loss": 0.873,
      "step": 60850
    },
    {
      "epoch": 3580.0,
      "grad_norm": 24.837059020996094,
      "learning_rate": 1.42e-05,
      "loss": 0.7162,
      "step": 60860
    },
    {
      "epoch": 3580.5882352941176,
      "grad_norm": 18.114282608032227,
      "learning_rate": 1.4194117647058824e-05,
      "loss": 0.6998,
      "step": 60870
    },
    {
      "epoch": 3581.176470588235,
      "grad_norm": 26.507532119750977,
      "learning_rate": 1.4188235294117647e-05,
      "loss": 0.8137,
      "step": 60880
    },
    {
      "epoch": 3581.764705882353,
      "grad_norm": 18.940593719482422,
      "learning_rate": 1.4182352941176472e-05,
      "loss": 0.7421,
      "step": 60890
    },
    {
      "epoch": 3582.3529411764707,
      "grad_norm": 17.291852951049805,
      "learning_rate": 1.4176470588235293e-05,
      "loss": 0.8101,
      "step": 60900
    },
    {
      "epoch": 3582.9411764705883,
      "grad_norm": 21.058181762695312,
      "learning_rate": 1.417058823529412e-05,
      "loss": 0.7568,
      "step": 60910
    },
    {
      "epoch": 3583.529411764706,
      "grad_norm": 20.196577072143555,
      "learning_rate": 1.4164705882352941e-05,
      "loss": 0.7973,
      "step": 60920
    },
    {
      "epoch": 3584.1176470588234,
      "grad_norm": 18.192859649658203,
      "learning_rate": 1.4158823529411766e-05,
      "loss": 0.7889,
      "step": 60930
    },
    {
      "epoch": 3584.705882352941,
      "grad_norm": 14.717724800109863,
      "learning_rate": 1.4152941176470589e-05,
      "loss": 0.8227,
      "step": 60940
    },
    {
      "epoch": 3585.294117647059,
      "grad_norm": 24.481298446655273,
      "learning_rate": 1.4147058823529413e-05,
      "loss": 0.8554,
      "step": 60950
    },
    {
      "epoch": 3585.8823529411766,
      "grad_norm": 18.63845443725586,
      "learning_rate": 1.4141176470588236e-05,
      "loss": 0.8075,
      "step": 60960
    },
    {
      "epoch": 3586.470588235294,
      "grad_norm": 17.708803176879883,
      "learning_rate": 1.4135294117647061e-05,
      "loss": 0.8275,
      "step": 60970
    },
    {
      "epoch": 3587.0588235294117,
      "grad_norm": 17.82027244567871,
      "learning_rate": 1.4129411764705882e-05,
      "loss": 0.7321,
      "step": 60980
    },
    {
      "epoch": 3587.6470588235293,
      "grad_norm": 20.09937286376953,
      "learning_rate": 1.4123529411764707e-05,
      "loss": 0.8797,
      "step": 60990
    },
    {
      "epoch": 3588.235294117647,
      "grad_norm": 23.077800750732422,
      "learning_rate": 1.411764705882353e-05,
      "loss": 0.7926,
      "step": 61000
    },
    {
      "epoch": 3588.823529411765,
      "grad_norm": 15.155680656433105,
      "learning_rate": 1.4111764705882355e-05,
      "loss": 0.8292,
      "step": 61010
    },
    {
      "epoch": 3589.4117647058824,
      "grad_norm": 19.419553756713867,
      "learning_rate": 1.4105882352941178e-05,
      "loss": 0.7838,
      "step": 61020
    },
    {
      "epoch": 3590.0,
      "grad_norm": 30.334789276123047,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.7973,
      "step": 61030
    },
    {
      "epoch": 3590.5882352941176,
      "grad_norm": 17.25446319580078,
      "learning_rate": 1.4094117647058824e-05,
      "loss": 0.7417,
      "step": 61040
    },
    {
      "epoch": 3591.176470588235,
      "grad_norm": 13.590620994567871,
      "learning_rate": 1.4088235294117647e-05,
      "loss": 0.7742,
      "step": 61050
    },
    {
      "epoch": 3591.764705882353,
      "grad_norm": 21.76936912536621,
      "learning_rate": 1.4082352941176471e-05,
      "loss": 0.8489,
      "step": 61060
    },
    {
      "epoch": 3592.3529411764707,
      "grad_norm": 22.948101043701172,
      "learning_rate": 1.4076470588235294e-05,
      "loss": 0.9085,
      "step": 61070
    },
    {
      "epoch": 3592.9411764705883,
      "grad_norm": 22.640857696533203,
      "learning_rate": 1.4070588235294119e-05,
      "loss": 0.7537,
      "step": 61080
    },
    {
      "epoch": 3593.529411764706,
      "grad_norm": 25.652076721191406,
      "learning_rate": 1.406470588235294e-05,
      "loss": 0.8764,
      "step": 61090
    },
    {
      "epoch": 3594.1176470588234,
      "grad_norm": 21.835481643676758,
      "learning_rate": 1.4058823529411767e-05,
      "loss": 0.7137,
      "step": 61100
    },
    {
      "epoch": 3594.705882352941,
      "grad_norm": 27.14285659790039,
      "learning_rate": 1.4052941176470588e-05,
      "loss": 0.6967,
      "step": 61110
    },
    {
      "epoch": 3595.294117647059,
      "grad_norm": 16.315279006958008,
      "learning_rate": 1.4047058823529412e-05,
      "loss": 0.8246,
      "step": 61120
    },
    {
      "epoch": 3595.8823529411766,
      "grad_norm": 18.146242141723633,
      "learning_rate": 1.4041176470588235e-05,
      "loss": 0.824,
      "step": 61130
    },
    {
      "epoch": 3596.470588235294,
      "grad_norm": 22.214420318603516,
      "learning_rate": 1.403529411764706e-05,
      "loss": 0.7783,
      "step": 61140
    },
    {
      "epoch": 3597.0588235294117,
      "grad_norm": 20.042137145996094,
      "learning_rate": 1.4029411764705883e-05,
      "loss": 0.7491,
      "step": 61150
    },
    {
      "epoch": 3597.6470588235293,
      "grad_norm": 22.85879135131836,
      "learning_rate": 1.4023529411764708e-05,
      "loss": 0.7246,
      "step": 61160
    },
    {
      "epoch": 3598.235294117647,
      "grad_norm": 16.359230041503906,
      "learning_rate": 1.4017647058823529e-05,
      "loss": 0.8876,
      "step": 61170
    },
    {
      "epoch": 3598.823529411765,
      "grad_norm": 26.671213150024414,
      "learning_rate": 1.4011764705882356e-05,
      "loss": 0.6625,
      "step": 61180
    },
    {
      "epoch": 3599.4117647058824,
      "grad_norm": 20.545522689819336,
      "learning_rate": 1.4005882352941177e-05,
      "loss": 0.8721,
      "step": 61190
    },
    {
      "epoch": 3600.0,
      "grad_norm": 29.918174743652344,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.7384,
      "step": 61200
    },
    {
      "epoch": 3600.5882352941176,
      "grad_norm": 25.597002029418945,
      "learning_rate": 1.3994117647058824e-05,
      "loss": 0.7799,
      "step": 61210
    },
    {
      "epoch": 3601.176470588235,
      "grad_norm": 26.986289978027344,
      "learning_rate": 1.3988235294117646e-05,
      "loss": 0.8643,
      "step": 61220
    },
    {
      "epoch": 3601.764705882353,
      "grad_norm": 26.8492374420166,
      "learning_rate": 1.3982352941176472e-05,
      "loss": 0.7617,
      "step": 61230
    },
    {
      "epoch": 3602.3529411764707,
      "grad_norm": 19.978530883789062,
      "learning_rate": 1.3976470588235293e-05,
      "loss": 0.7862,
      "step": 61240
    },
    {
      "epoch": 3602.9411764705883,
      "grad_norm": 22.450843811035156,
      "learning_rate": 1.3970588235294118e-05,
      "loss": 0.7611,
      "step": 61250
    },
    {
      "epoch": 3603.529411764706,
      "grad_norm": 18.65876007080078,
      "learning_rate": 1.3964705882352941e-05,
      "loss": 0.7824,
      "step": 61260
    },
    {
      "epoch": 3604.1176470588234,
      "grad_norm": 22.119457244873047,
      "learning_rate": 1.3958823529411766e-05,
      "loss": 0.7155,
      "step": 61270
    },
    {
      "epoch": 3604.705882352941,
      "grad_norm": 22.40454864501953,
      "learning_rate": 1.3952941176470589e-05,
      "loss": 0.8421,
      "step": 61280
    },
    {
      "epoch": 3605.294117647059,
      "grad_norm": 17.991647720336914,
      "learning_rate": 1.3947058823529413e-05,
      "loss": 0.8503,
      "step": 61290
    },
    {
      "epoch": 3605.8823529411766,
      "grad_norm": 23.08837890625,
      "learning_rate": 1.3941176470588235e-05,
      "loss": 0.7946,
      "step": 61300
    },
    {
      "epoch": 3606.470588235294,
      "grad_norm": 22.96519660949707,
      "learning_rate": 1.393529411764706e-05,
      "loss": 0.769,
      "step": 61310
    },
    {
      "epoch": 3607.0588235294117,
      "grad_norm": 20.225595474243164,
      "learning_rate": 1.3929411764705882e-05,
      "loss": 0.7946,
      "step": 61320
    },
    {
      "epoch": 3607.6470588235293,
      "grad_norm": 19.07347869873047,
      "learning_rate": 1.3923529411764707e-05,
      "loss": 0.6533,
      "step": 61330
    },
    {
      "epoch": 3608.235294117647,
      "grad_norm": 21.41583251953125,
      "learning_rate": 1.391764705882353e-05,
      "loss": 0.7504,
      "step": 61340
    },
    {
      "epoch": 3608.823529411765,
      "grad_norm": 19.86639404296875,
      "learning_rate": 1.3911764705882355e-05,
      "loss": 0.691,
      "step": 61350
    },
    {
      "epoch": 3609.4117647058824,
      "grad_norm": 20.726016998291016,
      "learning_rate": 1.3905882352941178e-05,
      "loss": 0.7947,
      "step": 61360
    },
    {
      "epoch": 3610.0,
      "grad_norm": 22.854793548583984,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.7569,
      "step": 61370
    },
    {
      "epoch": 3610.5882352941176,
      "grad_norm": 28.553199768066406,
      "learning_rate": 1.3894117647058824e-05,
      "loss": 0.828,
      "step": 61380
    },
    {
      "epoch": 3611.176470588235,
      "grad_norm": 21.59650993347168,
      "learning_rate": 1.3888235294117647e-05,
      "loss": 0.7495,
      "step": 61390
    },
    {
      "epoch": 3611.764705882353,
      "grad_norm": 21.72237205505371,
      "learning_rate": 1.3882352941176471e-05,
      "loss": 0.7324,
      "step": 61400
    },
    {
      "epoch": 3612.3529411764707,
      "grad_norm": 20.266658782958984,
      "learning_rate": 1.3876470588235294e-05,
      "loss": 0.8378,
      "step": 61410
    },
    {
      "epoch": 3612.9411764705883,
      "grad_norm": 21.10024070739746,
      "learning_rate": 1.3870588235294119e-05,
      "loss": 0.8762,
      "step": 61420
    },
    {
      "epoch": 3613.529411764706,
      "grad_norm": 18.88778305053711,
      "learning_rate": 1.386470588235294e-05,
      "loss": 0.7319,
      "step": 61430
    },
    {
      "epoch": 3614.1176470588234,
      "grad_norm": 21.82868194580078,
      "learning_rate": 1.3858823529411765e-05,
      "loss": 0.6398,
      "step": 61440
    },
    {
      "epoch": 3614.705882352941,
      "grad_norm": 13.361122131347656,
      "learning_rate": 1.3852941176470588e-05,
      "loss": 0.7872,
      "step": 61450
    },
    {
      "epoch": 3615.294117647059,
      "grad_norm": 19.705276489257812,
      "learning_rate": 1.3847058823529413e-05,
      "loss": 0.7868,
      "step": 61460
    },
    {
      "epoch": 3615.8823529411766,
      "grad_norm": 23.000083923339844,
      "learning_rate": 1.3841176470588236e-05,
      "loss": 0.7837,
      "step": 61470
    },
    {
      "epoch": 3616.470588235294,
      "grad_norm": 19.719345092773438,
      "learning_rate": 1.383529411764706e-05,
      "loss": 0.704,
      "step": 61480
    },
    {
      "epoch": 3617.0588235294117,
      "grad_norm": 14.439779281616211,
      "learning_rate": 1.3829411764705882e-05,
      "loss": 0.7436,
      "step": 61490
    },
    {
      "epoch": 3617.6470588235293,
      "grad_norm": 24.976491928100586,
      "learning_rate": 1.3823529411764708e-05,
      "loss": 0.81,
      "step": 61500
    },
    {
      "epoch": 3618.235294117647,
      "grad_norm": 20.198287963867188,
      "learning_rate": 1.381764705882353e-05,
      "loss": 0.8029,
      "step": 61510
    },
    {
      "epoch": 3618.823529411765,
      "grad_norm": 21.45217514038086,
      "learning_rate": 1.3811764705882354e-05,
      "loss": 0.7439,
      "step": 61520
    },
    {
      "epoch": 3619.4117647058824,
      "grad_norm": 19.906993865966797,
      "learning_rate": 1.3805882352941177e-05,
      "loss": 0.8667,
      "step": 61530
    },
    {
      "epoch": 3620.0,
      "grad_norm": 25.234725952148438,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.818,
      "step": 61540
    },
    {
      "epoch": 3620.5882352941176,
      "grad_norm": 20.385868072509766,
      "learning_rate": 1.3794117647058825e-05,
      "loss": 0.822,
      "step": 61550
    },
    {
      "epoch": 3621.176470588235,
      "grad_norm": 19.84138298034668,
      "learning_rate": 1.378823529411765e-05,
      "loss": 0.8095,
      "step": 61560
    },
    {
      "epoch": 3621.764705882353,
      "grad_norm": 14.62546443939209,
      "learning_rate": 1.378235294117647e-05,
      "loss": 0.7604,
      "step": 61570
    },
    {
      "epoch": 3622.3529411764707,
      "grad_norm": 17.034032821655273,
      "learning_rate": 1.3776470588235294e-05,
      "loss": 0.8678,
      "step": 61580
    },
    {
      "epoch": 3622.9411764705883,
      "grad_norm": 20.839391708374023,
      "learning_rate": 1.3770588235294118e-05,
      "loss": 0.7212,
      "step": 61590
    },
    {
      "epoch": 3623.529411764706,
      "grad_norm": 18.317283630371094,
      "learning_rate": 1.3764705882352941e-05,
      "loss": 0.7849,
      "step": 61600
    },
    {
      "epoch": 3624.1176470588234,
      "grad_norm": 22.260042190551758,
      "learning_rate": 1.3758823529411766e-05,
      "loss": 0.7976,
      "step": 61610
    },
    {
      "epoch": 3624.705882352941,
      "grad_norm": 20.255571365356445,
      "learning_rate": 1.3752941176470587e-05,
      "loss": 0.777,
      "step": 61620
    },
    {
      "epoch": 3625.294117647059,
      "grad_norm": 19.82292366027832,
      "learning_rate": 1.3747058823529414e-05,
      "loss": 0.6888,
      "step": 61630
    },
    {
      "epoch": 3625.8823529411766,
      "grad_norm": 21.443424224853516,
      "learning_rate": 1.3741176470588235e-05,
      "loss": 0.7983,
      "step": 61640
    },
    {
      "epoch": 3626.470588235294,
      "grad_norm": 14.477194786071777,
      "learning_rate": 1.373529411764706e-05,
      "loss": 0.704,
      "step": 61650
    },
    {
      "epoch": 3627.0588235294117,
      "grad_norm": 27.0710391998291,
      "learning_rate": 1.3729411764705883e-05,
      "loss": 0.8104,
      "step": 61660
    },
    {
      "epoch": 3627.6470588235293,
      "grad_norm": 21.22603988647461,
      "learning_rate": 1.3723529411764707e-05,
      "loss": 0.7698,
      "step": 61670
    },
    {
      "epoch": 3628.235294117647,
      "grad_norm": 28.806583404541016,
      "learning_rate": 1.371764705882353e-05,
      "loss": 0.7498,
      "step": 61680
    },
    {
      "epoch": 3628.823529411765,
      "grad_norm": 19.480422973632812,
      "learning_rate": 1.3711764705882355e-05,
      "loss": 0.777,
      "step": 61690
    },
    {
      "epoch": 3629.4117647058824,
      "grad_norm": 22.43824577331543,
      "learning_rate": 1.3705882352941176e-05,
      "loss": 0.9008,
      "step": 61700
    },
    {
      "epoch": 3630.0,
      "grad_norm": 22.6851806640625,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.857,
      "step": 61710
    },
    {
      "epoch": 3630.5882352941176,
      "grad_norm": 22.835508346557617,
      "learning_rate": 1.3694117647058824e-05,
      "loss": 0.7373,
      "step": 61720
    },
    {
      "epoch": 3631.176470588235,
      "grad_norm": 28.039417266845703,
      "learning_rate": 1.3688235294117649e-05,
      "loss": 0.7868,
      "step": 61730
    },
    {
      "epoch": 3631.764705882353,
      "grad_norm": 16.295513153076172,
      "learning_rate": 1.3682352941176472e-05,
      "loss": 0.7194,
      "step": 61740
    },
    {
      "epoch": 3632.3529411764707,
      "grad_norm": 25.44768714904785,
      "learning_rate": 1.3676470588235296e-05,
      "loss": 0.6996,
      "step": 61750
    },
    {
      "epoch": 3632.9411764705883,
      "grad_norm": 25.32904815673828,
      "learning_rate": 1.3670588235294118e-05,
      "loss": 0.7315,
      "step": 61760
    },
    {
      "epoch": 3633.529411764706,
      "grad_norm": 19.012195587158203,
      "learning_rate": 1.366470588235294e-05,
      "loss": 0.7922,
      "step": 61770
    },
    {
      "epoch": 3634.1176470588234,
      "grad_norm": 17.292526245117188,
      "learning_rate": 1.3658823529411765e-05,
      "loss": 0.8775,
      "step": 61780
    },
    {
      "epoch": 3634.705882352941,
      "grad_norm": 15.715625762939453,
      "learning_rate": 1.3652941176470588e-05,
      "loss": 0.7229,
      "step": 61790
    },
    {
      "epoch": 3635.294117647059,
      "grad_norm": 18.41152000427246,
      "learning_rate": 1.3647058823529413e-05,
      "loss": 0.7451,
      "step": 61800
    },
    {
      "epoch": 3635.8823529411766,
      "grad_norm": 17.96358299255371,
      "learning_rate": 1.3641176470588234e-05,
      "loss": 0.7662,
      "step": 61810
    },
    {
      "epoch": 3636.470588235294,
      "grad_norm": 20.680837631225586,
      "learning_rate": 1.363529411764706e-05,
      "loss": 0.8074,
      "step": 61820
    },
    {
      "epoch": 3637.0588235294117,
      "grad_norm": 15.453636169433594,
      "learning_rate": 1.3629411764705882e-05,
      "loss": 0.7259,
      "step": 61830
    },
    {
      "epoch": 3637.6470588235293,
      "grad_norm": 32.74085235595703,
      "learning_rate": 1.3623529411764707e-05,
      "loss": 0.8049,
      "step": 61840
    },
    {
      "epoch": 3638.235294117647,
      "grad_norm": 23.656190872192383,
      "learning_rate": 1.361764705882353e-05,
      "loss": 0.8216,
      "step": 61850
    },
    {
      "epoch": 3638.823529411765,
      "grad_norm": 19.07415199279785,
      "learning_rate": 1.3611764705882354e-05,
      "loss": 0.8616,
      "step": 61860
    },
    {
      "epoch": 3639.4117647058824,
      "grad_norm": 27.101776123046875,
      "learning_rate": 1.3605882352941177e-05,
      "loss": 0.8862,
      "step": 61870
    },
    {
      "epoch": 3640.0,
      "grad_norm": 21.62466812133789,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.7822,
      "step": 61880
    },
    {
      "epoch": 3640.5882352941176,
      "grad_norm": 19.763607025146484,
      "learning_rate": 1.3594117647058823e-05,
      "loss": 0.7291,
      "step": 61890
    },
    {
      "epoch": 3641.176470588235,
      "grad_norm": 13.310340881347656,
      "learning_rate": 1.358823529411765e-05,
      "loss": 0.6554,
      "step": 61900
    },
    {
      "epoch": 3641.764705882353,
      "grad_norm": 19.868061065673828,
      "learning_rate": 1.3582352941176471e-05,
      "loss": 0.7641,
      "step": 61910
    },
    {
      "epoch": 3642.3529411764707,
      "grad_norm": 22.138957977294922,
      "learning_rate": 1.3576470588235296e-05,
      "loss": 0.8166,
      "step": 61920
    },
    {
      "epoch": 3642.9411764705883,
      "grad_norm": 31.057809829711914,
      "learning_rate": 1.3570588235294119e-05,
      "loss": 0.74,
      "step": 61930
    },
    {
      "epoch": 3643.529411764706,
      "grad_norm": 16.767675399780273,
      "learning_rate": 1.356470588235294e-05,
      "loss": 0.7405,
      "step": 61940
    },
    {
      "epoch": 3644.1176470588234,
      "grad_norm": 19.463809967041016,
      "learning_rate": 1.3558823529411766e-05,
      "loss": 0.7971,
      "step": 61950
    },
    {
      "epoch": 3644.705882352941,
      "grad_norm": 27.51186752319336,
      "learning_rate": 1.3552941176470588e-05,
      "loss": 0.7476,
      "step": 61960
    },
    {
      "epoch": 3645.294117647059,
      "grad_norm": 22.908538818359375,
      "learning_rate": 1.3547058823529412e-05,
      "loss": 0.8113,
      "step": 61970
    },
    {
      "epoch": 3645.8823529411766,
      "grad_norm": 21.40324592590332,
      "learning_rate": 1.3541176470588235e-05,
      "loss": 0.7053,
      "step": 61980
    },
    {
      "epoch": 3646.470588235294,
      "grad_norm": 20.661876678466797,
      "learning_rate": 1.353529411764706e-05,
      "loss": 0.7955,
      "step": 61990
    },
    {
      "epoch": 3647.0588235294117,
      "grad_norm": 19.561954498291016,
      "learning_rate": 1.3529411764705883e-05,
      "loss": 0.6926,
      "step": 62000
    },
    {
      "epoch": 3647.6470588235293,
      "grad_norm": 27.750059127807617,
      "learning_rate": 1.3523529411764708e-05,
      "loss": 0.8614,
      "step": 62010
    },
    {
      "epoch": 3648.235294117647,
      "grad_norm": 15.261580467224121,
      "learning_rate": 1.3517647058823529e-05,
      "loss": 0.7832,
      "step": 62020
    },
    {
      "epoch": 3648.823529411765,
      "grad_norm": 21.07808494567871,
      "learning_rate": 1.3511764705882354e-05,
      "loss": 0.8138,
      "step": 62030
    },
    {
      "epoch": 3649.4117647058824,
      "grad_norm": 25.839868545532227,
      "learning_rate": 1.3505882352941177e-05,
      "loss": 0.6998,
      "step": 62040
    },
    {
      "epoch": 3650.0,
      "grad_norm": 23.94936180114746,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.7404,
      "step": 62050
    },
    {
      "epoch": 3650.5882352941176,
      "grad_norm": 25.958681106567383,
      "learning_rate": 1.3494117647058824e-05,
      "loss": 0.8241,
      "step": 62060
    },
    {
      "epoch": 3651.176470588235,
      "grad_norm": 23.227535247802734,
      "learning_rate": 1.3488235294117649e-05,
      "loss": 0.812,
      "step": 62070
    },
    {
      "epoch": 3651.764705882353,
      "grad_norm": 20.711734771728516,
      "learning_rate": 1.348235294117647e-05,
      "loss": 0.7929,
      "step": 62080
    },
    {
      "epoch": 3652.3529411764707,
      "grad_norm": 14.629043579101562,
      "learning_rate": 1.3476470588235297e-05,
      "loss": 0.8602,
      "step": 62090
    },
    {
      "epoch": 3652.9411764705883,
      "grad_norm": 21.93080711364746,
      "learning_rate": 1.3470588235294118e-05,
      "loss": 0.7579,
      "step": 62100
    },
    {
      "epoch": 3653.529411764706,
      "grad_norm": 29.09722137451172,
      "learning_rate": 1.3464705882352943e-05,
      "loss": 0.8306,
      "step": 62110
    },
    {
      "epoch": 3654.1176470588234,
      "grad_norm": 25.086450576782227,
      "learning_rate": 1.3458823529411765e-05,
      "loss": 0.8213,
      "step": 62120
    },
    {
      "epoch": 3654.705882352941,
      "grad_norm": 22.241867065429688,
      "learning_rate": 1.3452941176470587e-05,
      "loss": 0.6165,
      "step": 62130
    },
    {
      "epoch": 3655.294117647059,
      "grad_norm": 18.7526798248291,
      "learning_rate": 1.3447058823529413e-05,
      "loss": 0.7429,
      "step": 62140
    },
    {
      "epoch": 3655.8823529411766,
      "grad_norm": 21.554790496826172,
      "learning_rate": 1.3441176470588234e-05,
      "loss": 0.8013,
      "step": 62150
    },
    {
      "epoch": 3656.470588235294,
      "grad_norm": 26.57084083557129,
      "learning_rate": 1.3435294117647059e-05,
      "loss": 0.7476,
      "step": 62160
    },
    {
      "epoch": 3657.0588235294117,
      "grad_norm": 26.56462860107422,
      "learning_rate": 1.3429411764705882e-05,
      "loss": 0.8022,
      "step": 62170
    },
    {
      "epoch": 3657.6470588235293,
      "grad_norm": 18.06553840637207,
      "learning_rate": 1.3423529411764707e-05,
      "loss": 0.8666,
      "step": 62180
    },
    {
      "epoch": 3658.235294117647,
      "grad_norm": 20.900774002075195,
      "learning_rate": 1.341764705882353e-05,
      "loss": 0.8373,
      "step": 62190
    },
    {
      "epoch": 3658.823529411765,
      "grad_norm": 14.526566505432129,
      "learning_rate": 1.3411764705882354e-05,
      "loss": 0.7286,
      "step": 62200
    },
    {
      "epoch": 3659.4117647058824,
      "grad_norm": 22.753646850585938,
      "learning_rate": 1.3405882352941176e-05,
      "loss": 0.8367,
      "step": 62210
    },
    {
      "epoch": 3660.0,
      "grad_norm": 25.52641487121582,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.7473,
      "step": 62220
    },
    {
      "epoch": 3660.5882352941176,
      "grad_norm": 26.299789428710938,
      "learning_rate": 1.3394117647058823e-05,
      "loss": 0.7753,
      "step": 62230
    },
    {
      "epoch": 3661.176470588235,
      "grad_norm": 21.28313446044922,
      "learning_rate": 1.3388235294117648e-05,
      "loss": 0.7505,
      "step": 62240
    },
    {
      "epoch": 3661.764705882353,
      "grad_norm": 18.638341903686523,
      "learning_rate": 1.3382352941176471e-05,
      "loss": 0.7436,
      "step": 62250
    },
    {
      "epoch": 3662.3529411764707,
      "grad_norm": 15.922268867492676,
      "learning_rate": 1.3376470588235296e-05,
      "loss": 0.7162,
      "step": 62260
    },
    {
      "epoch": 3662.9411764705883,
      "grad_norm": 25.824172973632812,
      "learning_rate": 1.3370588235294119e-05,
      "loss": 0.7838,
      "step": 62270
    },
    {
      "epoch": 3663.529411764706,
      "grad_norm": 23.00680923461914,
      "learning_rate": 1.3364705882352943e-05,
      "loss": 0.7349,
      "step": 62280
    },
    {
      "epoch": 3664.1176470588234,
      "grad_norm": 20.76776885986328,
      "learning_rate": 1.3358823529411765e-05,
      "loss": 0.8415,
      "step": 62290
    },
    {
      "epoch": 3664.705882352941,
      "grad_norm": 14.964710235595703,
      "learning_rate": 1.3352941176470588e-05,
      "loss": 0.7556,
      "step": 62300
    },
    {
      "epoch": 3665.294117647059,
      "grad_norm": 22.267133712768555,
      "learning_rate": 1.3347058823529412e-05,
      "loss": 0.7429,
      "step": 62310
    },
    {
      "epoch": 3665.8823529411766,
      "grad_norm": 19.403507232666016,
      "learning_rate": 1.3341176470588235e-05,
      "loss": 0.7966,
      "step": 62320
    },
    {
      "epoch": 3666.470588235294,
      "grad_norm": 19.41851043701172,
      "learning_rate": 1.333529411764706e-05,
      "loss": 0.7023,
      "step": 62330
    },
    {
      "epoch": 3667.0588235294117,
      "grad_norm": 24.150968551635742,
      "learning_rate": 1.3329411764705881e-05,
      "loss": 0.7918,
      "step": 62340
    },
    {
      "epoch": 3667.6470588235293,
      "grad_norm": 26.78360939025879,
      "learning_rate": 1.3323529411764708e-05,
      "loss": 0.7681,
      "step": 62350
    },
    {
      "epoch": 3668.235294117647,
      "grad_norm": 16.971025466918945,
      "learning_rate": 1.3317647058823529e-05,
      "loss": 0.7074,
      "step": 62360
    },
    {
      "epoch": 3668.823529411765,
      "grad_norm": 19.276643753051758,
      "learning_rate": 1.3311764705882354e-05,
      "loss": 0.6957,
      "step": 62370
    },
    {
      "epoch": 3669.4117647058824,
      "grad_norm": 21.295080184936523,
      "learning_rate": 1.3305882352941177e-05,
      "loss": 0.7496,
      "step": 62380
    },
    {
      "epoch": 3670.0,
      "grad_norm": 28.107065200805664,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.7502,
      "step": 62390
    },
    {
      "epoch": 3670.5882352941176,
      "grad_norm": 19.692441940307617,
      "learning_rate": 1.3294117647058824e-05,
      "loss": 0.7511,
      "step": 62400
    },
    {
      "epoch": 3671.176470588235,
      "grad_norm": 17.135494232177734,
      "learning_rate": 1.3288235294117649e-05,
      "loss": 0.8514,
      "step": 62410
    },
    {
      "epoch": 3671.764705882353,
      "grad_norm": 24.05477523803711,
      "learning_rate": 1.328235294117647e-05,
      "loss": 0.7411,
      "step": 62420
    },
    {
      "epoch": 3672.3529411764707,
      "grad_norm": 16.333209991455078,
      "learning_rate": 1.3276470588235295e-05,
      "loss": 0.7267,
      "step": 62430
    },
    {
      "epoch": 3672.9411764705883,
      "grad_norm": 18.947406768798828,
      "learning_rate": 1.3270588235294118e-05,
      "loss": 0.8283,
      "step": 62440
    },
    {
      "epoch": 3673.529411764706,
      "grad_norm": 28.070598602294922,
      "learning_rate": 1.3264705882352943e-05,
      "loss": 0.7812,
      "step": 62450
    },
    {
      "epoch": 3674.1176470588234,
      "grad_norm": 19.57622528076172,
      "learning_rate": 1.3258823529411766e-05,
      "loss": 0.8191,
      "step": 62460
    },
    {
      "epoch": 3674.705882352941,
      "grad_norm": 15.591634750366211,
      "learning_rate": 1.325294117647059e-05,
      "loss": 0.6998,
      "step": 62470
    },
    {
      "epoch": 3675.294117647059,
      "grad_norm": 17.711231231689453,
      "learning_rate": 1.3247058823529412e-05,
      "loss": 0.6979,
      "step": 62480
    },
    {
      "epoch": 3675.8823529411766,
      "grad_norm": 21.070280075073242,
      "learning_rate": 1.3241176470588235e-05,
      "loss": 0.7122,
      "step": 62490
    },
    {
      "epoch": 3676.470588235294,
      "grad_norm": 20.880008697509766,
      "learning_rate": 1.323529411764706e-05,
      "loss": 0.7663,
      "step": 62500
    },
    {
      "epoch": 3677.0588235294117,
      "grad_norm": 26.95964813232422,
      "learning_rate": 1.3229411764705882e-05,
      "loss": 0.8577,
      "step": 62510
    },
    {
      "epoch": 3677.6470588235293,
      "grad_norm": 24.817508697509766,
      "learning_rate": 1.3223529411764707e-05,
      "loss": 0.8675,
      "step": 62520
    },
    {
      "epoch": 3678.235294117647,
      "grad_norm": 25.1844482421875,
      "learning_rate": 1.3217647058823528e-05,
      "loss": 0.7958,
      "step": 62530
    },
    {
      "epoch": 3678.823529411765,
      "grad_norm": 25.073522567749023,
      "learning_rate": 1.3211764705882355e-05,
      "loss": 0.8407,
      "step": 62540
    },
    {
      "epoch": 3679.4117647058824,
      "grad_norm": 22.582813262939453,
      "learning_rate": 1.3205882352941176e-05,
      "loss": 0.6205,
      "step": 62550
    },
    {
      "epoch": 3680.0,
      "grad_norm": 28.57861328125,
      "learning_rate": 1.32e-05,
      "loss": 0.7417,
      "step": 62560
    },
    {
      "epoch": 3680.5882352941176,
      "grad_norm": 21.91160774230957,
      "learning_rate": 1.3194117647058824e-05,
      "loss": 0.7069,
      "step": 62570
    },
    {
      "epoch": 3681.176470588235,
      "grad_norm": 21.999887466430664,
      "learning_rate": 1.3188235294117648e-05,
      "loss": 0.7023,
      "step": 62580
    },
    {
      "epoch": 3681.764705882353,
      "grad_norm": 24.5253963470459,
      "learning_rate": 1.3182352941176471e-05,
      "loss": 0.7382,
      "step": 62590
    },
    {
      "epoch": 3682.3529411764707,
      "grad_norm": 15.42878532409668,
      "learning_rate": 1.3176470588235296e-05,
      "loss": 0.7114,
      "step": 62600
    },
    {
      "epoch": 3682.9411764705883,
      "grad_norm": 20.7491512298584,
      "learning_rate": 1.3170588235294117e-05,
      "loss": 0.7905,
      "step": 62610
    },
    {
      "epoch": 3683.529411764706,
      "grad_norm": 24.841190338134766,
      "learning_rate": 1.3164705882352944e-05,
      "loss": 0.7769,
      "step": 62620
    },
    {
      "epoch": 3684.1176470588234,
      "grad_norm": 20.378915786743164,
      "learning_rate": 1.3158823529411765e-05,
      "loss": 0.7434,
      "step": 62630
    },
    {
      "epoch": 3684.705882352941,
      "grad_norm": 24.2434024810791,
      "learning_rate": 1.315294117647059e-05,
      "loss": 0.7009,
      "step": 62640
    },
    {
      "epoch": 3685.294117647059,
      "grad_norm": 22.945077896118164,
      "learning_rate": 1.3147058823529413e-05,
      "loss": 0.8213,
      "step": 62650
    },
    {
      "epoch": 3685.8823529411766,
      "grad_norm": 18.790748596191406,
      "learning_rate": 1.3141176470588237e-05,
      "loss": 0.8999,
      "step": 62660
    },
    {
      "epoch": 3686.470588235294,
      "grad_norm": 23.131671905517578,
      "learning_rate": 1.313529411764706e-05,
      "loss": 0.7503,
      "step": 62670
    },
    {
      "epoch": 3687.0588235294117,
      "grad_norm": 19.764162063598633,
      "learning_rate": 1.3129411764705882e-05,
      "loss": 0.6905,
      "step": 62680
    },
    {
      "epoch": 3687.6470588235293,
      "grad_norm": 22.55392074584961,
      "learning_rate": 1.3123529411764706e-05,
      "loss": 0.7685,
      "step": 62690
    },
    {
      "epoch": 3688.235294117647,
      "grad_norm": 18.28360939025879,
      "learning_rate": 1.311764705882353e-05,
      "loss": 0.7558,
      "step": 62700
    },
    {
      "epoch": 3688.823529411765,
      "grad_norm": 18.342618942260742,
      "learning_rate": 1.3111764705882354e-05,
      "loss": 0.7051,
      "step": 62710
    },
    {
      "epoch": 3689.4117647058824,
      "grad_norm": 23.781282424926758,
      "learning_rate": 1.3105882352941177e-05,
      "loss": 0.7667,
      "step": 62720
    },
    {
      "epoch": 3690.0,
      "grad_norm": 27.348173141479492,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.6981,
      "step": 62730
    },
    {
      "epoch": 3690.5882352941176,
      "grad_norm": 22.623756408691406,
      "learning_rate": 1.3094117647058823e-05,
      "loss": 0.732,
      "step": 62740
    },
    {
      "epoch": 3691.176470588235,
      "grad_norm": 22.84754180908203,
      "learning_rate": 1.3088235294117648e-05,
      "loss": 0.7531,
      "step": 62750
    },
    {
      "epoch": 3691.764705882353,
      "grad_norm": 22.49549674987793,
      "learning_rate": 1.308235294117647e-05,
      "loss": 0.8491,
      "step": 62760
    },
    {
      "epoch": 3692.3529411764707,
      "grad_norm": 20.453163146972656,
      "learning_rate": 1.3076470588235295e-05,
      "loss": 0.8562,
      "step": 62770
    },
    {
      "epoch": 3692.9411764705883,
      "grad_norm": 15.609254837036133,
      "learning_rate": 1.3070588235294118e-05,
      "loss": 0.7348,
      "step": 62780
    },
    {
      "epoch": 3693.529411764706,
      "grad_norm": 17.580793380737305,
      "learning_rate": 1.3064705882352943e-05,
      "loss": 0.7075,
      "step": 62790
    },
    {
      "epoch": 3694.1176470588234,
      "grad_norm": 15.330788612365723,
      "learning_rate": 1.3058823529411764e-05,
      "loss": 0.7205,
      "step": 62800
    },
    {
      "epoch": 3694.705882352941,
      "grad_norm": 20.812467575073242,
      "learning_rate": 1.305294117647059e-05,
      "loss": 0.8533,
      "step": 62810
    },
    {
      "epoch": 3695.294117647059,
      "grad_norm": 28.33057403564453,
      "learning_rate": 1.3047058823529412e-05,
      "loss": 0.7314,
      "step": 62820
    },
    {
      "epoch": 3695.8823529411766,
      "grad_norm": 19.240318298339844,
      "learning_rate": 1.3041176470588237e-05,
      "loss": 0.7451,
      "step": 62830
    },
    {
      "epoch": 3696.470588235294,
      "grad_norm": 19.809070587158203,
      "learning_rate": 1.303529411764706e-05,
      "loss": 0.7864,
      "step": 62840
    },
    {
      "epoch": 3697.0588235294117,
      "grad_norm": 18.574844360351562,
      "learning_rate": 1.3029411764705881e-05,
      "loss": 0.7277,
      "step": 62850
    },
    {
      "epoch": 3697.6470588235293,
      "grad_norm": 22.735239028930664,
      "learning_rate": 1.3023529411764707e-05,
      "loss": 0.8131,
      "step": 62860
    },
    {
      "epoch": 3698.235294117647,
      "grad_norm": 24.738632202148438,
      "learning_rate": 1.3017647058823529e-05,
      "loss": 0.7347,
      "step": 62870
    },
    {
      "epoch": 3698.823529411765,
      "grad_norm": 23.54398536682129,
      "learning_rate": 1.3011764705882353e-05,
      "loss": 0.8845,
      "step": 62880
    },
    {
      "epoch": 3699.4117647058824,
      "grad_norm": 13.001996040344238,
      "learning_rate": 1.3005882352941176e-05,
      "loss": 0.7725,
      "step": 62890
    },
    {
      "epoch": 3700.0,
      "grad_norm": 19.759206771850586,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.7179,
      "step": 62900
    },
    {
      "epoch": 3700.5882352941176,
      "grad_norm": 22.809350967407227,
      "learning_rate": 1.2994117647058824e-05,
      "loss": 0.847,
      "step": 62910
    },
    {
      "epoch": 3701.176470588235,
      "grad_norm": 26.030811309814453,
      "learning_rate": 1.2988235294117649e-05,
      "loss": 0.7545,
      "step": 62920
    },
    {
      "epoch": 3701.764705882353,
      "grad_norm": 20.894046783447266,
      "learning_rate": 1.298235294117647e-05,
      "loss": 0.6438,
      "step": 62930
    },
    {
      "epoch": 3702.3529411764707,
      "grad_norm": 26.50372886657715,
      "learning_rate": 1.2976470588235296e-05,
      "loss": 0.7469,
      "step": 62940
    },
    {
      "epoch": 3702.9411764705883,
      "grad_norm": 29.808265686035156,
      "learning_rate": 1.2970588235294118e-05,
      "loss": 0.8251,
      "step": 62950
    },
    {
      "epoch": 3703.529411764706,
      "grad_norm": 15.091438293457031,
      "learning_rate": 1.2964705882352942e-05,
      "loss": 0.7375,
      "step": 62960
    },
    {
      "epoch": 3704.1176470588234,
      "grad_norm": 21.44444465637207,
      "learning_rate": 1.2958823529411765e-05,
      "loss": 0.7059,
      "step": 62970
    },
    {
      "epoch": 3704.705882352941,
      "grad_norm": 21.824975967407227,
      "learning_rate": 1.295294117647059e-05,
      "loss": 0.6628,
      "step": 62980
    },
    {
      "epoch": 3705.294117647059,
      "grad_norm": 25.037771224975586,
      "learning_rate": 1.2947058823529413e-05,
      "loss": 0.8892,
      "step": 62990
    },
    {
      "epoch": 3705.8823529411766,
      "grad_norm": 21.685070037841797,
      "learning_rate": 1.2941176470588238e-05,
      "loss": 0.7738,
      "step": 63000
    },
    {
      "epoch": 3706.470588235294,
      "grad_norm": 14.349505424499512,
      "learning_rate": 1.2935294117647059e-05,
      "loss": 0.7918,
      "step": 63010
    },
    {
      "epoch": 3707.0588235294117,
      "grad_norm": 24.914758682250977,
      "learning_rate": 1.2929411764705884e-05,
      "loss": 0.724,
      "step": 63020
    },
    {
      "epoch": 3707.6470588235293,
      "grad_norm": 23.982017517089844,
      "learning_rate": 1.2923529411764707e-05,
      "loss": 0.6749,
      "step": 63030
    },
    {
      "epoch": 3708.235294117647,
      "grad_norm": 22.67151641845703,
      "learning_rate": 1.291764705882353e-05,
      "loss": 0.78,
      "step": 63040
    },
    {
      "epoch": 3708.823529411765,
      "grad_norm": 23.073949813842773,
      "learning_rate": 1.2911764705882354e-05,
      "loss": 0.761,
      "step": 63050
    },
    {
      "epoch": 3709.4117647058824,
      "grad_norm": 26.377260208129883,
      "learning_rate": 1.2905882352941175e-05,
      "loss": 0.7924,
      "step": 63060
    },
    {
      "epoch": 3710.0,
      "grad_norm": 18.643342971801758,
      "learning_rate": 1.29e-05,
      "loss": 0.772,
      "step": 63070
    },
    {
      "epoch": 3710.5882352941176,
      "grad_norm": 21.292369842529297,
      "learning_rate": 1.2894117647058823e-05,
      "loss": 0.6684,
      "step": 63080
    },
    {
      "epoch": 3711.176470588235,
      "grad_norm": 21.544654846191406,
      "learning_rate": 1.2888235294117648e-05,
      "loss": 0.8407,
      "step": 63090
    },
    {
      "epoch": 3711.764705882353,
      "grad_norm": 20.254955291748047,
      "learning_rate": 1.288235294117647e-05,
      "loss": 0.896,
      "step": 63100
    },
    {
      "epoch": 3712.3529411764707,
      "grad_norm": 15.65444278717041,
      "learning_rate": 1.2876470588235295e-05,
      "loss": 0.7037,
      "step": 63110
    },
    {
      "epoch": 3712.9411764705883,
      "grad_norm": 21.978816986083984,
      "learning_rate": 1.2870588235294117e-05,
      "loss": 0.7693,
      "step": 63120
    },
    {
      "epoch": 3713.529411764706,
      "grad_norm": 20.835285186767578,
      "learning_rate": 1.2864705882352943e-05,
      "loss": 0.7729,
      "step": 63130
    },
    {
      "epoch": 3714.1176470588234,
      "grad_norm": 22.148635864257812,
      "learning_rate": 1.2858823529411764e-05,
      "loss": 0.751,
      "step": 63140
    },
    {
      "epoch": 3714.705882352941,
      "grad_norm": 27.109840393066406,
      "learning_rate": 1.2852941176470589e-05,
      "loss": 0.7409,
      "step": 63150
    },
    {
      "epoch": 3715.294117647059,
      "grad_norm": 16.726686477661133,
      "learning_rate": 1.2847058823529412e-05,
      "loss": 0.7342,
      "step": 63160
    },
    {
      "epoch": 3715.8823529411766,
      "grad_norm": 22.22182273864746,
      "learning_rate": 1.2841176470588237e-05,
      "loss": 0.7737,
      "step": 63170
    },
    {
      "epoch": 3716.470588235294,
      "grad_norm": 23.211671829223633,
      "learning_rate": 1.283529411764706e-05,
      "loss": 0.7779,
      "step": 63180
    },
    {
      "epoch": 3717.0588235294117,
      "grad_norm": 18.354204177856445,
      "learning_rate": 1.2829411764705884e-05,
      "loss": 0.7406,
      "step": 63190
    },
    {
      "epoch": 3717.6470588235293,
      "grad_norm": 19.487428665161133,
      "learning_rate": 1.2823529411764706e-05,
      "loss": 0.6912,
      "step": 63200
    },
    {
      "epoch": 3718.235294117647,
      "grad_norm": 24.540861129760742,
      "learning_rate": 1.2817647058823529e-05,
      "loss": 0.8069,
      "step": 63210
    },
    {
      "epoch": 3718.823529411765,
      "grad_norm": 27.517004013061523,
      "learning_rate": 1.2811764705882353e-05,
      "loss": 0.8127,
      "step": 63220
    },
    {
      "epoch": 3719.4117647058824,
      "grad_norm": 21.10961151123047,
      "learning_rate": 1.2805882352941176e-05,
      "loss": 0.7081,
      "step": 63230
    },
    {
      "epoch": 3720.0,
      "grad_norm": 25.641674041748047,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.8232,
      "step": 63240
    },
    {
      "epoch": 3720.5882352941176,
      "grad_norm": 24.128002166748047,
      "learning_rate": 1.2794117647058822e-05,
      "loss": 0.7931,
      "step": 63250
    },
    {
      "epoch": 3721.176470588235,
      "grad_norm": 17.525320053100586,
      "learning_rate": 1.2788235294117649e-05,
      "loss": 0.817,
      "step": 63260
    },
    {
      "epoch": 3721.764705882353,
      "grad_norm": 17.13449478149414,
      "learning_rate": 1.278235294117647e-05,
      "loss": 0.6615,
      "step": 63270
    },
    {
      "epoch": 3722.3529411764707,
      "grad_norm": 20.19481086730957,
      "learning_rate": 1.2776470588235295e-05,
      "loss": 0.7513,
      "step": 63280
    },
    {
      "epoch": 3722.9411764705883,
      "grad_norm": 22.046911239624023,
      "learning_rate": 1.2770588235294118e-05,
      "loss": 0.8429,
      "step": 63290
    },
    {
      "epoch": 3723.529411764706,
      "grad_norm": 22.186500549316406,
      "learning_rate": 1.2764705882352942e-05,
      "loss": 0.7698,
      "step": 63300
    },
    {
      "epoch": 3724.1176470588234,
      "grad_norm": 24.387035369873047,
      "learning_rate": 1.2758823529411765e-05,
      "loss": 0.7351,
      "step": 63310
    },
    {
      "epoch": 3724.705882352941,
      "grad_norm": 16.086015701293945,
      "learning_rate": 1.275294117647059e-05,
      "loss": 0.7723,
      "step": 63320
    },
    {
      "epoch": 3725.294117647059,
      "grad_norm": 16.55500602722168,
      "learning_rate": 1.2747058823529411e-05,
      "loss": 0.7362,
      "step": 63330
    },
    {
      "epoch": 3725.8823529411766,
      "grad_norm": 15.520401000976562,
      "learning_rate": 1.2741176470588238e-05,
      "loss": 0.7308,
      "step": 63340
    },
    {
      "epoch": 3726.470588235294,
      "grad_norm": 25.724288940429688,
      "learning_rate": 1.2735294117647059e-05,
      "loss": 0.8066,
      "step": 63350
    },
    {
      "epoch": 3727.0588235294117,
      "grad_norm": 31.76655387878418,
      "learning_rate": 1.2729411764705884e-05,
      "loss": 0.7736,
      "step": 63360
    },
    {
      "epoch": 3727.6470588235293,
      "grad_norm": 17.819944381713867,
      "learning_rate": 1.2723529411764707e-05,
      "loss": 0.863,
      "step": 63370
    },
    {
      "epoch": 3728.235294117647,
      "grad_norm": 14.703999519348145,
      "learning_rate": 1.2717647058823531e-05,
      "loss": 0.6523,
      "step": 63380
    },
    {
      "epoch": 3728.823529411765,
      "grad_norm": 20.94954490661621,
      "learning_rate": 1.2711764705882354e-05,
      "loss": 0.7106,
      "step": 63390
    },
    {
      "epoch": 3729.4117647058824,
      "grad_norm": 16.489177703857422,
      "learning_rate": 1.2705882352941176e-05,
      "loss": 0.7135,
      "step": 63400
    },
    {
      "epoch": 3730.0,
      "grad_norm": 13.378555297851562,
      "learning_rate": 1.27e-05,
      "loss": 0.6823,
      "step": 63410
    },
    {
      "epoch": 3730.5882352941176,
      "grad_norm": 27.220956802368164,
      "learning_rate": 1.2694117647058823e-05,
      "loss": 0.7353,
      "step": 63420
    },
    {
      "epoch": 3731.176470588235,
      "grad_norm": 21.608041763305664,
      "learning_rate": 1.2688235294117648e-05,
      "loss": 0.7416,
      "step": 63430
    },
    {
      "epoch": 3731.764705882353,
      "grad_norm": 20.610877990722656,
      "learning_rate": 1.2682352941176471e-05,
      "loss": 0.7936,
      "step": 63440
    },
    {
      "epoch": 3732.3529411764707,
      "grad_norm": 16.945850372314453,
      "learning_rate": 1.2676470588235296e-05,
      "loss": 0.7874,
      "step": 63450
    },
    {
      "epoch": 3732.9411764705883,
      "grad_norm": 20.394887924194336,
      "learning_rate": 1.2670588235294117e-05,
      "loss": 0.6755,
      "step": 63460
    },
    {
      "epoch": 3733.529411764706,
      "grad_norm": 27.395418167114258,
      "learning_rate": 1.2664705882352942e-05,
      "loss": 0.7934,
      "step": 63470
    },
    {
      "epoch": 3734.1176470588234,
      "grad_norm": 19.177705764770508,
      "learning_rate": 1.2658823529411765e-05,
      "loss": 0.7416,
      "step": 63480
    },
    {
      "epoch": 3734.705882352941,
      "grad_norm": 21.5003662109375,
      "learning_rate": 1.265294117647059e-05,
      "loss": 0.8147,
      "step": 63490
    },
    {
      "epoch": 3735.294117647059,
      "grad_norm": 22.017803192138672,
      "learning_rate": 1.2647058823529412e-05,
      "loss": 0.7952,
      "step": 63500
    },
    {
      "epoch": 3735.8823529411766,
      "grad_norm": 26.346567153930664,
      "learning_rate": 1.2641176470588237e-05,
      "loss": 0.7074,
      "step": 63510
    },
    {
      "epoch": 3736.470588235294,
      "grad_norm": 24.575305938720703,
      "learning_rate": 1.2635294117647058e-05,
      "loss": 0.7285,
      "step": 63520
    },
    {
      "epoch": 3737.0588235294117,
      "grad_norm": 17.121675491333008,
      "learning_rate": 1.2629411764705885e-05,
      "loss": 0.8176,
      "step": 63530
    },
    {
      "epoch": 3737.6470588235293,
      "grad_norm": 21.777118682861328,
      "learning_rate": 1.2623529411764706e-05,
      "loss": 0.739,
      "step": 63540
    },
    {
      "epoch": 3738.235294117647,
      "grad_norm": 19.39360237121582,
      "learning_rate": 1.261764705882353e-05,
      "loss": 0.6827,
      "step": 63550
    },
    {
      "epoch": 3738.823529411765,
      "grad_norm": 17.939287185668945,
      "learning_rate": 1.2611764705882354e-05,
      "loss": 0.7882,
      "step": 63560
    },
    {
      "epoch": 3739.4117647058824,
      "grad_norm": 22.99060821533203,
      "learning_rate": 1.2605882352941175e-05,
      "loss": 0.6153,
      "step": 63570
    },
    {
      "epoch": 3740.0,
      "grad_norm": 19.026918411254883,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.8049,
      "step": 63580
    },
    {
      "epoch": 3740.5882352941176,
      "grad_norm": 20.242586135864258,
      "learning_rate": 1.2594117647058823e-05,
      "loss": 0.8411,
      "step": 63590
    },
    {
      "epoch": 3741.176470588235,
      "grad_norm": 21.90810203552246,
      "learning_rate": 1.2588235294117647e-05,
      "loss": 0.6521,
      "step": 63600
    },
    {
      "epoch": 3741.764705882353,
      "grad_norm": 12.025497436523438,
      "learning_rate": 1.258235294117647e-05,
      "loss": 0.6588,
      "step": 63610
    },
    {
      "epoch": 3742.3529411764707,
      "grad_norm": 21.09552001953125,
      "learning_rate": 1.2576470588235295e-05,
      "loss": 0.7514,
      "step": 63620
    },
    {
      "epoch": 3742.9411764705883,
      "grad_norm": 12.377735137939453,
      "learning_rate": 1.2570588235294118e-05,
      "loss": 0.7066,
      "step": 63630
    },
    {
      "epoch": 3743.529411764706,
      "grad_norm": 17.26906967163086,
      "learning_rate": 1.2564705882352943e-05,
      "loss": 0.8315,
      "step": 63640
    },
    {
      "epoch": 3744.1176470588234,
      "grad_norm": 20.794666290283203,
      "learning_rate": 1.2558823529411764e-05,
      "loss": 0.7527,
      "step": 63650
    },
    {
      "epoch": 3744.705882352941,
      "grad_norm": 16.410627365112305,
      "learning_rate": 1.255294117647059e-05,
      "loss": 0.7606,
      "step": 63660
    },
    {
      "epoch": 3745.294117647059,
      "grad_norm": 15.257015228271484,
      "learning_rate": 1.2547058823529412e-05,
      "loss": 0.7752,
      "step": 63670
    },
    {
      "epoch": 3745.8823529411766,
      "grad_norm": 19.570411682128906,
      "learning_rate": 1.2541176470588236e-05,
      "loss": 0.7505,
      "step": 63680
    },
    {
      "epoch": 3746.470588235294,
      "grad_norm": 18.551937103271484,
      "learning_rate": 1.253529411764706e-05,
      "loss": 0.6761,
      "step": 63690
    },
    {
      "epoch": 3747.0588235294117,
      "grad_norm": 18.949506759643555,
      "learning_rate": 1.2529411764705884e-05,
      "loss": 0.7514,
      "step": 63700
    },
    {
      "epoch": 3747.6470588235293,
      "grad_norm": 20.24761199951172,
      "learning_rate": 1.2523529411764707e-05,
      "loss": 0.7206,
      "step": 63710
    },
    {
      "epoch": 3748.235294117647,
      "grad_norm": 20.363954544067383,
      "learning_rate": 1.2517647058823532e-05,
      "loss": 0.7005,
      "step": 63720
    },
    {
      "epoch": 3748.823529411765,
      "grad_norm": 29.079315185546875,
      "learning_rate": 1.2511764705882353e-05,
      "loss": 0.8112,
      "step": 63730
    },
    {
      "epoch": 3749.4117647058824,
      "grad_norm": 15.829730987548828,
      "learning_rate": 1.2505882352941178e-05,
      "loss": 0.7307,
      "step": 63740
    },
    {
      "epoch": 3750.0,
      "grad_norm": 20.868019104003906,
      "learning_rate": 1.25e-05,
      "loss": 0.6985,
      "step": 63750
    },
    {
      "epoch": 3750.5882352941176,
      "grad_norm": 25.209707260131836,
      "learning_rate": 1.2494117647058824e-05,
      "loss": 0.7961,
      "step": 63760
    },
    {
      "epoch": 3751.176470588235,
      "grad_norm": 16.598955154418945,
      "learning_rate": 1.2488235294117648e-05,
      "loss": 0.7551,
      "step": 63770
    },
    {
      "epoch": 3751.764705882353,
      "grad_norm": 23.431203842163086,
      "learning_rate": 1.2482352941176471e-05,
      "loss": 0.8165,
      "step": 63780
    },
    {
      "epoch": 3752.3529411764707,
      "grad_norm": 19.943851470947266,
      "learning_rate": 1.2476470588235294e-05,
      "loss": 0.8065,
      "step": 63790
    },
    {
      "epoch": 3752.9411764705883,
      "grad_norm": 20.606712341308594,
      "learning_rate": 1.2470588235294119e-05,
      "loss": 0.6783,
      "step": 63800
    },
    {
      "epoch": 3753.529411764706,
      "grad_norm": 21.595048904418945,
      "learning_rate": 1.2464705882352942e-05,
      "loss": 0.7024,
      "step": 63810
    },
    {
      "epoch": 3754.1176470588234,
      "grad_norm": 23.88934326171875,
      "learning_rate": 1.2458823529411767e-05,
      "loss": 0.7958,
      "step": 63820
    },
    {
      "epoch": 3754.705882352941,
      "grad_norm": 27.189449310302734,
      "learning_rate": 1.245294117647059e-05,
      "loss": 0.6928,
      "step": 63830
    },
    {
      "epoch": 3755.294117647059,
      "grad_norm": 19.54404640197754,
      "learning_rate": 1.2447058823529413e-05,
      "loss": 0.6838,
      "step": 63840
    },
    {
      "epoch": 3755.8823529411766,
      "grad_norm": 25.36655616760254,
      "learning_rate": 1.2441176470588236e-05,
      "loss": 0.7836,
      "step": 63850
    },
    {
      "epoch": 3756.470588235294,
      "grad_norm": 17.8167667388916,
      "learning_rate": 1.2435294117647059e-05,
      "loss": 0.7228,
      "step": 63860
    },
    {
      "epoch": 3757.0588235294117,
      "grad_norm": 16.601438522338867,
      "learning_rate": 1.2429411764705883e-05,
      "loss": 0.7773,
      "step": 63870
    },
    {
      "epoch": 3757.6470588235293,
      "grad_norm": 17.382949829101562,
      "learning_rate": 1.2423529411764706e-05,
      "loss": 0.7558,
      "step": 63880
    },
    {
      "epoch": 3758.235294117647,
      "grad_norm": 28.324365615844727,
      "learning_rate": 1.241764705882353e-05,
      "loss": 0.7728,
      "step": 63890
    },
    {
      "epoch": 3758.823529411765,
      "grad_norm": 22.11911964416504,
      "learning_rate": 1.2411764705882354e-05,
      "loss": 0.6948,
      "step": 63900
    },
    {
      "epoch": 3759.4117647058824,
      "grad_norm": 22.051393508911133,
      "learning_rate": 1.2405882352941177e-05,
      "loss": 0.7316,
      "step": 63910
    },
    {
      "epoch": 3760.0,
      "grad_norm": 28.320356369018555,
      "learning_rate": 1.24e-05,
      "loss": 0.9232,
      "step": 63920
    },
    {
      "epoch": 3760.5882352941176,
      "grad_norm": 13.909289360046387,
      "learning_rate": 1.2394117647058825e-05,
      "loss": 0.7068,
      "step": 63930
    },
    {
      "epoch": 3761.176470588235,
      "grad_norm": 17.346721649169922,
      "learning_rate": 1.2388235294117648e-05,
      "loss": 0.7511,
      "step": 63940
    },
    {
      "epoch": 3761.764705882353,
      "grad_norm": 21.643245697021484,
      "learning_rate": 1.238235294117647e-05,
      "loss": 0.659,
      "step": 63950
    },
    {
      "epoch": 3762.3529411764707,
      "grad_norm": 19.3520565032959,
      "learning_rate": 1.2376470588235295e-05,
      "loss": 0.7786,
      "step": 63960
    },
    {
      "epoch": 3762.9411764705883,
      "grad_norm": 14.521740913391113,
      "learning_rate": 1.2370588235294118e-05,
      "loss": 0.7211,
      "step": 63970
    },
    {
      "epoch": 3763.529411764706,
      "grad_norm": 16.69365692138672,
      "learning_rate": 1.2364705882352943e-05,
      "loss": 0.661,
      "step": 63980
    },
    {
      "epoch": 3764.1176470588234,
      "grad_norm": 19.245954513549805,
      "learning_rate": 1.2358823529411766e-05,
      "loss": 0.7686,
      "step": 63990
    },
    {
      "epoch": 3764.705882352941,
      "grad_norm": 19.006895065307617,
      "learning_rate": 1.2352941176470589e-05,
      "loss": 0.7404,
      "step": 64000
    },
    {
      "epoch": 3765.294117647059,
      "grad_norm": 14.902976036071777,
      "learning_rate": 1.2347058823529414e-05,
      "loss": 0.689,
      "step": 64010
    },
    {
      "epoch": 3765.8823529411766,
      "grad_norm": 15.162941932678223,
      "learning_rate": 1.2341176470588237e-05,
      "loss": 0.6947,
      "step": 64020
    },
    {
      "epoch": 3766.470588235294,
      "grad_norm": 24.698314666748047,
      "learning_rate": 1.233529411764706e-05,
      "loss": 0.9221,
      "step": 64030
    },
    {
      "epoch": 3767.0588235294117,
      "grad_norm": 25.14436912536621,
      "learning_rate": 1.2329411764705882e-05,
      "loss": 0.7223,
      "step": 64040
    },
    {
      "epoch": 3767.6470588235293,
      "grad_norm": 20.166231155395508,
      "learning_rate": 1.2323529411764705e-05,
      "loss": 0.6498,
      "step": 64050
    },
    {
      "epoch": 3768.235294117647,
      "grad_norm": 21.787723541259766,
      "learning_rate": 1.231764705882353e-05,
      "loss": 0.7029,
      "step": 64060
    },
    {
      "epoch": 3768.823529411765,
      "grad_norm": 19.01958465576172,
      "learning_rate": 1.2311764705882353e-05,
      "loss": 0.8322,
      "step": 64070
    },
    {
      "epoch": 3769.4117647058824,
      "grad_norm": 19.579500198364258,
      "learning_rate": 1.2305882352941176e-05,
      "loss": 0.8625,
      "step": 64080
    },
    {
      "epoch": 3770.0,
      "grad_norm": 27.096328735351562,
      "learning_rate": 1.23e-05,
      "loss": 0.7175,
      "step": 64090
    },
    {
      "epoch": 3770.5882352941176,
      "grad_norm": 22.003690719604492,
      "learning_rate": 1.2294117647058824e-05,
      "loss": 0.6818,
      "step": 64100
    },
    {
      "epoch": 3771.176470588235,
      "grad_norm": 18.211225509643555,
      "learning_rate": 1.2288235294117647e-05,
      "loss": 0.8712,
      "step": 64110
    },
    {
      "epoch": 3771.764705882353,
      "grad_norm": 24.180294036865234,
      "learning_rate": 1.2282352941176471e-05,
      "loss": 0.6375,
      "step": 64120
    },
    {
      "epoch": 3772.3529411764707,
      "grad_norm": 13.440367698669434,
      "learning_rate": 1.2276470588235294e-05,
      "loss": 0.7565,
      "step": 64130
    },
    {
      "epoch": 3772.9411764705883,
      "grad_norm": 24.590600967407227,
      "learning_rate": 1.2270588235294119e-05,
      "loss": 0.7451,
      "step": 64140
    },
    {
      "epoch": 3773.529411764706,
      "grad_norm": 19.540109634399414,
      "learning_rate": 1.2264705882352942e-05,
      "loss": 0.7421,
      "step": 64150
    },
    {
      "epoch": 3774.1176470588234,
      "grad_norm": 22.009071350097656,
      "learning_rate": 1.2258823529411765e-05,
      "loss": 0.6791,
      "step": 64160
    },
    {
      "epoch": 3774.705882352941,
      "grad_norm": 20.824792861938477,
      "learning_rate": 1.225294117647059e-05,
      "loss": 0.7298,
      "step": 64170
    },
    {
      "epoch": 3775.294117647059,
      "grad_norm": 19.329256057739258,
      "learning_rate": 1.2247058823529413e-05,
      "loss": 0.6362,
      "step": 64180
    },
    {
      "epoch": 3775.8823529411766,
      "grad_norm": 22.064071655273438,
      "learning_rate": 1.2241176470588236e-05,
      "loss": 0.83,
      "step": 64190
    },
    {
      "epoch": 3776.470588235294,
      "grad_norm": 24.392723083496094,
      "learning_rate": 1.223529411764706e-05,
      "loss": 0.8458,
      "step": 64200
    },
    {
      "epoch": 3777.0588235294117,
      "grad_norm": 13.743629455566406,
      "learning_rate": 1.2229411764705882e-05,
      "loss": 0.7498,
      "step": 64210
    },
    {
      "epoch": 3777.6470588235293,
      "grad_norm": 20.538373947143555,
      "learning_rate": 1.2223529411764706e-05,
      "loss": 0.7175,
      "step": 64220
    },
    {
      "epoch": 3778.235294117647,
      "grad_norm": 29.341156005859375,
      "learning_rate": 1.221764705882353e-05,
      "loss": 0.8609,
      "step": 64230
    },
    {
      "epoch": 3778.823529411765,
      "grad_norm": 23.853723526000977,
      "learning_rate": 1.2211764705882352e-05,
      "loss": 0.7946,
      "step": 64240
    },
    {
      "epoch": 3779.4117647058824,
      "grad_norm": 25.44240951538086,
      "learning_rate": 1.2205882352941177e-05,
      "loss": 0.6986,
      "step": 64250
    },
    {
      "epoch": 3780.0,
      "grad_norm": 22.361879348754883,
      "learning_rate": 1.22e-05,
      "loss": 0.6539,
      "step": 64260
    },
    {
      "epoch": 3780.5882352941176,
      "grad_norm": 16.69234275817871,
      "learning_rate": 1.2194117647058825e-05,
      "loss": 0.8259,
      "step": 64270
    },
    {
      "epoch": 3781.176470588235,
      "grad_norm": 15.931702613830566,
      "learning_rate": 1.2188235294117648e-05,
      "loss": 0.7518,
      "step": 64280
    },
    {
      "epoch": 3781.764705882353,
      "grad_norm": 20.18389892578125,
      "learning_rate": 1.218235294117647e-05,
      "loss": 0.7628,
      "step": 64290
    },
    {
      "epoch": 3782.3529411764707,
      "grad_norm": 28.022167205810547,
      "learning_rate": 1.2176470588235295e-05,
      "loss": 0.8335,
      "step": 64300
    },
    {
      "epoch": 3782.9411764705883,
      "grad_norm": 18.920547485351562,
      "learning_rate": 1.2170588235294118e-05,
      "loss": 0.7368,
      "step": 64310
    },
    {
      "epoch": 3783.529411764706,
      "grad_norm": 18.150197982788086,
      "learning_rate": 1.2164705882352941e-05,
      "loss": 0.7649,
      "step": 64320
    },
    {
      "epoch": 3784.1176470588234,
      "grad_norm": 19.95574378967285,
      "learning_rate": 1.2158823529411766e-05,
      "loss": 0.6447,
      "step": 64330
    },
    {
      "epoch": 3784.705882352941,
      "grad_norm": 22.960155487060547,
      "learning_rate": 1.2152941176470589e-05,
      "loss": 0.7624,
      "step": 64340
    },
    {
      "epoch": 3785.294117647059,
      "grad_norm": 27.173080444335938,
      "learning_rate": 1.2147058823529412e-05,
      "loss": 0.7335,
      "step": 64350
    },
    {
      "epoch": 3785.8823529411766,
      "grad_norm": 23.636856079101562,
      "learning_rate": 1.2141176470588237e-05,
      "loss": 0.7004,
      "step": 64360
    },
    {
      "epoch": 3786.470588235294,
      "grad_norm": 17.673376083374023,
      "learning_rate": 1.213529411764706e-05,
      "loss": 0.7107,
      "step": 64370
    },
    {
      "epoch": 3787.0588235294117,
      "grad_norm": 15.380389213562012,
      "learning_rate": 1.2129411764705884e-05,
      "loss": 0.6863,
      "step": 64380
    },
    {
      "epoch": 3787.6470588235293,
      "grad_norm": 23.854055404663086,
      "learning_rate": 1.2123529411764706e-05,
      "loss": 0.7495,
      "step": 64390
    },
    {
      "epoch": 3788.235294117647,
      "grad_norm": 19.942550659179688,
      "learning_rate": 1.2117647058823529e-05,
      "loss": 0.7361,
      "step": 64400
    },
    {
      "epoch": 3788.823529411765,
      "grad_norm": 17.919952392578125,
      "learning_rate": 1.2111764705882353e-05,
      "loss": 0.6909,
      "step": 64410
    },
    {
      "epoch": 3789.4117647058824,
      "grad_norm": 17.551239013671875,
      "learning_rate": 1.2105882352941176e-05,
      "loss": 0.843,
      "step": 64420
    },
    {
      "epoch": 3790.0,
      "grad_norm": 23.475709915161133,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.843,
      "step": 64430
    },
    {
      "epoch": 3790.5882352941176,
      "grad_norm": 24.547672271728516,
      "learning_rate": 1.2094117647058824e-05,
      "loss": 0.7321,
      "step": 64440
    },
    {
      "epoch": 3791.176470588235,
      "grad_norm": 17.597867965698242,
      "learning_rate": 1.2088235294117647e-05,
      "loss": 0.7302,
      "step": 64450
    },
    {
      "epoch": 3791.764705882353,
      "grad_norm": 19.866363525390625,
      "learning_rate": 1.2082352941176472e-05,
      "loss": 0.7943,
      "step": 64460
    },
    {
      "epoch": 3792.3529411764707,
      "grad_norm": 21.911090850830078,
      "learning_rate": 1.2076470588235295e-05,
      "loss": 0.7607,
      "step": 64470
    },
    {
      "epoch": 3792.9411764705883,
      "grad_norm": 25.503984451293945,
      "learning_rate": 1.2070588235294118e-05,
      "loss": 0.7985,
      "step": 64480
    },
    {
      "epoch": 3793.529411764706,
      "grad_norm": 21.80356788635254,
      "learning_rate": 1.2064705882352942e-05,
      "loss": 0.7975,
      "step": 64490
    },
    {
      "epoch": 3794.1176470588234,
      "grad_norm": 23.675859451293945,
      "learning_rate": 1.2058823529411765e-05,
      "loss": 0.7555,
      "step": 64500
    },
    {
      "epoch": 3794.705882352941,
      "grad_norm": 14.147421836853027,
      "learning_rate": 1.2052941176470588e-05,
      "loss": 0.7088,
      "step": 64510
    },
    {
      "epoch": 3795.294117647059,
      "grad_norm": 21.085224151611328,
      "learning_rate": 1.2047058823529413e-05,
      "loss": 0.7832,
      "step": 64520
    },
    {
      "epoch": 3795.8823529411766,
      "grad_norm": 23.915708541870117,
      "learning_rate": 1.2041176470588236e-05,
      "loss": 0.7346,
      "step": 64530
    },
    {
      "epoch": 3796.470588235294,
      "grad_norm": 20.409284591674805,
      "learning_rate": 1.203529411764706e-05,
      "loss": 0.6965,
      "step": 64540
    },
    {
      "epoch": 3797.0588235294117,
      "grad_norm": 21.608362197875977,
      "learning_rate": 1.2029411764705884e-05,
      "loss": 0.6462,
      "step": 64550
    },
    {
      "epoch": 3797.6470588235293,
      "grad_norm": 20.24817657470703,
      "learning_rate": 1.2023529411764707e-05,
      "loss": 0.6574,
      "step": 64560
    },
    {
      "epoch": 3798.235294117647,
      "grad_norm": 16.282499313354492,
      "learning_rate": 1.201764705882353e-05,
      "loss": 0.6293,
      "step": 64570
    },
    {
      "epoch": 3798.823529411765,
      "grad_norm": 22.73192596435547,
      "learning_rate": 1.2011764705882353e-05,
      "loss": 0.8042,
      "step": 64580
    },
    {
      "epoch": 3799.4117647058824,
      "grad_norm": 24.139163970947266,
      "learning_rate": 1.2005882352941177e-05,
      "loss": 0.7906,
      "step": 64590
    },
    {
      "epoch": 3800.0,
      "grad_norm": 27.219423294067383,
      "learning_rate": 1.2e-05,
      "loss": 0.8019,
      "step": 64600
    },
    {
      "epoch": 3800.5882352941176,
      "grad_norm": 24.69775390625,
      "learning_rate": 1.1994117647058823e-05,
      "loss": 0.735,
      "step": 64610
    },
    {
      "epoch": 3801.176470588235,
      "grad_norm": 16.482362747192383,
      "learning_rate": 1.1988235294117648e-05,
      "loss": 0.6119,
      "step": 64620
    },
    {
      "epoch": 3801.764705882353,
      "grad_norm": 22.313844680786133,
      "learning_rate": 1.1982352941176471e-05,
      "loss": 0.7396,
      "step": 64630
    },
    {
      "epoch": 3802.3529411764707,
      "grad_norm": 21.510692596435547,
      "learning_rate": 1.1976470588235294e-05,
      "loss": 0.8566,
      "step": 64640
    },
    {
      "epoch": 3802.9411764705883,
      "grad_norm": 20.46393394470215,
      "learning_rate": 1.1970588235294119e-05,
      "loss": 0.7365,
      "step": 64650
    },
    {
      "epoch": 3803.529411764706,
      "grad_norm": 21.034114837646484,
      "learning_rate": 1.1964705882352942e-05,
      "loss": 0.7606,
      "step": 64660
    },
    {
      "epoch": 3804.1176470588234,
      "grad_norm": 31.263275146484375,
      "learning_rate": 1.1958823529411765e-05,
      "loss": 0.7943,
      "step": 64670
    },
    {
      "epoch": 3804.705882352941,
      "grad_norm": 27.547822952270508,
      "learning_rate": 1.195294117647059e-05,
      "loss": 0.8882,
      "step": 64680
    },
    {
      "epoch": 3805.294117647059,
      "grad_norm": 23.073623657226562,
      "learning_rate": 1.1947058823529412e-05,
      "loss": 0.6464,
      "step": 64690
    },
    {
      "epoch": 3805.8823529411766,
      "grad_norm": 23.71945571899414,
      "learning_rate": 1.1941176470588237e-05,
      "loss": 0.7438,
      "step": 64700
    },
    {
      "epoch": 3806.470588235294,
      "grad_norm": 22.03532600402832,
      "learning_rate": 1.193529411764706e-05,
      "loss": 0.8788,
      "step": 64710
    },
    {
      "epoch": 3807.0588235294117,
      "grad_norm": 21.374670028686523,
      "learning_rate": 1.1929411764705883e-05,
      "loss": 0.7564,
      "step": 64720
    },
    {
      "epoch": 3807.6470588235293,
      "grad_norm": 20.81732940673828,
      "learning_rate": 1.1923529411764708e-05,
      "loss": 0.7335,
      "step": 64730
    },
    {
      "epoch": 3808.235294117647,
      "grad_norm": 28.947601318359375,
      "learning_rate": 1.191764705882353e-05,
      "loss": 0.762,
      "step": 64740
    },
    {
      "epoch": 3808.823529411765,
      "grad_norm": 16.71800422668457,
      "learning_rate": 1.1911764705882354e-05,
      "loss": 0.7632,
      "step": 64750
    },
    {
      "epoch": 3809.4117647058824,
      "grad_norm": 24.29575538635254,
      "learning_rate": 1.1905882352941177e-05,
      "loss": 0.8786,
      "step": 64760
    },
    {
      "epoch": 3810.0,
      "grad_norm": 42.26334762573242,
      "learning_rate": 1.19e-05,
      "loss": 0.7893,
      "step": 64770
    },
    {
      "epoch": 3810.5882352941176,
      "grad_norm": 19.827768325805664,
      "learning_rate": 1.1894117647058824e-05,
      "loss": 0.8087,
      "step": 64780
    },
    {
      "epoch": 3811.176470588235,
      "grad_norm": 20.470582962036133,
      "learning_rate": 1.1888235294117647e-05,
      "loss": 0.7441,
      "step": 64790
    },
    {
      "epoch": 3811.764705882353,
      "grad_norm": 26.577604293823242,
      "learning_rate": 1.188235294117647e-05,
      "loss": 0.7383,
      "step": 64800
    },
    {
      "epoch": 3812.3529411764707,
      "grad_norm": 27.690393447875977,
      "learning_rate": 1.1876470588235295e-05,
      "loss": 0.8309,
      "step": 64810
    },
    {
      "epoch": 3812.9411764705883,
      "grad_norm": 20.582935333251953,
      "learning_rate": 1.1870588235294118e-05,
      "loss": 0.712,
      "step": 64820
    },
    {
      "epoch": 3813.529411764706,
      "grad_norm": 24.069732666015625,
      "learning_rate": 1.1864705882352941e-05,
      "loss": 0.6769,
      "step": 64830
    },
    {
      "epoch": 3814.1176470588234,
      "grad_norm": 17.8093204498291,
      "learning_rate": 1.1858823529411766e-05,
      "loss": 0.6097,
      "step": 64840
    },
    {
      "epoch": 3814.705882352941,
      "grad_norm": 19.715621948242188,
      "learning_rate": 1.1852941176470589e-05,
      "loss": 0.6702,
      "step": 64850
    },
    {
      "epoch": 3815.294117647059,
      "grad_norm": 17.76186752319336,
      "learning_rate": 1.1847058823529413e-05,
      "loss": 0.7433,
      "step": 64860
    },
    {
      "epoch": 3815.8823529411766,
      "grad_norm": 16.1990909576416,
      "learning_rate": 1.1841176470588236e-05,
      "loss": 0.7482,
      "step": 64870
    },
    {
      "epoch": 3816.470588235294,
      "grad_norm": 22.11319351196289,
      "learning_rate": 1.183529411764706e-05,
      "loss": 0.804,
      "step": 64880
    },
    {
      "epoch": 3817.0588235294117,
      "grad_norm": 24.327198028564453,
      "learning_rate": 1.1829411764705884e-05,
      "loss": 0.749,
      "step": 64890
    },
    {
      "epoch": 3817.6470588235293,
      "grad_norm": 13.70911979675293,
      "learning_rate": 1.1823529411764707e-05,
      "loss": 0.6737,
      "step": 64900
    },
    {
      "epoch": 3818.235294117647,
      "grad_norm": 19.34405517578125,
      "learning_rate": 1.181764705882353e-05,
      "loss": 0.7835,
      "step": 64910
    },
    {
      "epoch": 3818.823529411765,
      "grad_norm": 16.468210220336914,
      "learning_rate": 1.1811764705882355e-05,
      "loss": 0.7104,
      "step": 64920
    },
    {
      "epoch": 3819.4117647058824,
      "grad_norm": 21.84387969970703,
      "learning_rate": 1.1805882352941178e-05,
      "loss": 0.6927,
      "step": 64930
    },
    {
      "epoch": 3820.0,
      "grad_norm": 23.740386962890625,
      "learning_rate": 1.18e-05,
      "loss": 0.7551,
      "step": 64940
    },
    {
      "epoch": 3820.5882352941176,
      "grad_norm": 20.8516845703125,
      "learning_rate": 1.1794117647058824e-05,
      "loss": 0.7393,
      "step": 64950
    },
    {
      "epoch": 3821.176470588235,
      "grad_norm": 19.061365127563477,
      "learning_rate": 1.1788235294117647e-05,
      "loss": 0.7165,
      "step": 64960
    },
    {
      "epoch": 3821.764705882353,
      "grad_norm": 15.3170804977417,
      "learning_rate": 1.1782352941176471e-05,
      "loss": 0.7143,
      "step": 64970
    },
    {
      "epoch": 3822.3529411764707,
      "grad_norm": 19.433975219726562,
      "learning_rate": 1.1776470588235294e-05,
      "loss": 0.7501,
      "step": 64980
    },
    {
      "epoch": 3822.9411764705883,
      "grad_norm": 17.60466957092285,
      "learning_rate": 1.1770588235294117e-05,
      "loss": 0.7441,
      "step": 64990
    },
    {
      "epoch": 3823.529411764706,
      "grad_norm": 23.678346633911133,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 0.7467,
      "step": 65000
    },
    {
      "epoch": 3824.1176470588234,
      "grad_norm": 21.582599639892578,
      "learning_rate": 1.1758823529411765e-05,
      "loss": 0.7238,
      "step": 65010
    },
    {
      "epoch": 3824.705882352941,
      "grad_norm": 17.580699920654297,
      "learning_rate": 1.175294117647059e-05,
      "loss": 0.7036,
      "step": 65020
    },
    {
      "epoch": 3825.294117647059,
      "grad_norm": 19.40852928161621,
      "learning_rate": 1.1747058823529412e-05,
      "loss": 0.6616,
      "step": 65030
    },
    {
      "epoch": 3825.8823529411766,
      "grad_norm": 29.499265670776367,
      "learning_rate": 1.1741176470588235e-05,
      "loss": 0.7309,
      "step": 65040
    },
    {
      "epoch": 3826.470588235294,
      "grad_norm": 20.85392189025879,
      "learning_rate": 1.173529411764706e-05,
      "loss": 0.8214,
      "step": 65050
    },
    {
      "epoch": 3827.0588235294117,
      "grad_norm": 23.33136749267578,
      "learning_rate": 1.1729411764705883e-05,
      "loss": 0.7738,
      "step": 65060
    },
    {
      "epoch": 3827.6470588235293,
      "grad_norm": 17.25176239013672,
      "learning_rate": 1.1723529411764706e-05,
      "loss": 0.6888,
      "step": 65070
    },
    {
      "epoch": 3828.235294117647,
      "grad_norm": 27.64543342590332,
      "learning_rate": 1.171764705882353e-05,
      "loss": 0.7561,
      "step": 65080
    },
    {
      "epoch": 3828.823529411765,
      "grad_norm": 18.70127296447754,
      "learning_rate": 1.1711764705882354e-05,
      "loss": 0.7226,
      "step": 65090
    },
    {
      "epoch": 3829.4117647058824,
      "grad_norm": 20.629213333129883,
      "learning_rate": 1.1705882352941177e-05,
      "loss": 0.8198,
      "step": 65100
    },
    {
      "epoch": 3830.0,
      "grad_norm": 18.493635177612305,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.8441,
      "step": 65110
    },
    {
      "epoch": 3830.5882352941176,
      "grad_norm": 20.951425552368164,
      "learning_rate": 1.1694117647058823e-05,
      "loss": 0.6764,
      "step": 65120
    },
    {
      "epoch": 3831.176470588235,
      "grad_norm": 15.684967994689941,
      "learning_rate": 1.1688235294117647e-05,
      "loss": 0.7253,
      "step": 65130
    },
    {
      "epoch": 3831.764705882353,
      "grad_norm": 22.565359115600586,
      "learning_rate": 1.168235294117647e-05,
      "loss": 0.7367,
      "step": 65140
    },
    {
      "epoch": 3832.3529411764707,
      "grad_norm": 28.343090057373047,
      "learning_rate": 1.1676470588235293e-05,
      "loss": 0.7527,
      "step": 65150
    },
    {
      "epoch": 3832.9411764705883,
      "grad_norm": 16.920530319213867,
      "learning_rate": 1.1670588235294118e-05,
      "loss": 0.7688,
      "step": 65160
    },
    {
      "epoch": 3833.529411764706,
      "grad_norm": 19.844539642333984,
      "learning_rate": 1.1664705882352941e-05,
      "loss": 0.675,
      "step": 65170
    },
    {
      "epoch": 3834.1176470588234,
      "grad_norm": 26.14698600769043,
      "learning_rate": 1.1658823529411766e-05,
      "loss": 0.7105,
      "step": 65180
    },
    {
      "epoch": 3834.705882352941,
      "grad_norm": 24.060075759887695,
      "learning_rate": 1.1652941176470589e-05,
      "loss": 0.684,
      "step": 65190
    },
    {
      "epoch": 3835.294117647059,
      "grad_norm": 27.104215621948242,
      "learning_rate": 1.1647058823529412e-05,
      "loss": 0.8248,
      "step": 65200
    },
    {
      "epoch": 3835.8823529411766,
      "grad_norm": 22.557619094848633,
      "learning_rate": 1.1641176470588236e-05,
      "loss": 0.7391,
      "step": 65210
    },
    {
      "epoch": 3836.470588235294,
      "grad_norm": 17.787216186523438,
      "learning_rate": 1.163529411764706e-05,
      "loss": 0.6581,
      "step": 65220
    },
    {
      "epoch": 3837.0588235294117,
      "grad_norm": 21.430078506469727,
      "learning_rate": 1.1629411764705882e-05,
      "loss": 0.7457,
      "step": 65230
    },
    {
      "epoch": 3837.6470588235293,
      "grad_norm": 16.183687210083008,
      "learning_rate": 1.1623529411764707e-05,
      "loss": 0.7407,
      "step": 65240
    },
    {
      "epoch": 3838.235294117647,
      "grad_norm": 24.358699798583984,
      "learning_rate": 1.161764705882353e-05,
      "loss": 0.7109,
      "step": 65250
    },
    {
      "epoch": 3838.823529411765,
      "grad_norm": 20.18724250793457,
      "learning_rate": 1.1611764705882355e-05,
      "loss": 0.6577,
      "step": 65260
    },
    {
      "epoch": 3839.4117647058824,
      "grad_norm": 22.84859848022461,
      "learning_rate": 1.1605882352941178e-05,
      "loss": 0.7607,
      "step": 65270
    },
    {
      "epoch": 3840.0,
      "grad_norm": 26.758365631103516,
      "learning_rate": 1.16e-05,
      "loss": 0.7254,
      "step": 65280
    },
    {
      "epoch": 3840.5882352941176,
      "grad_norm": 23.831851959228516,
      "learning_rate": 1.1594117647058825e-05,
      "loss": 0.7324,
      "step": 65290
    },
    {
      "epoch": 3841.176470588235,
      "grad_norm": 19.141342163085938,
      "learning_rate": 1.1588235294117647e-05,
      "loss": 0.8048,
      "step": 65300
    },
    {
      "epoch": 3841.764705882353,
      "grad_norm": 24.160476684570312,
      "learning_rate": 1.1582352941176471e-05,
      "loss": 0.8519,
      "step": 65310
    },
    {
      "epoch": 3842.3529411764707,
      "grad_norm": 17.265859603881836,
      "learning_rate": 1.1576470588235294e-05,
      "loss": 0.6759,
      "step": 65320
    },
    {
      "epoch": 3842.9411764705883,
      "grad_norm": 22.103313446044922,
      "learning_rate": 1.1570588235294117e-05,
      "loss": 0.808,
      "step": 65330
    },
    {
      "epoch": 3843.529411764706,
      "grad_norm": 20.516691207885742,
      "learning_rate": 1.1564705882352942e-05,
      "loss": 0.64,
      "step": 65340
    },
    {
      "epoch": 3844.1176470588234,
      "grad_norm": 26.984054565429688,
      "learning_rate": 1.1558823529411765e-05,
      "loss": 0.7811,
      "step": 65350
    },
    {
      "epoch": 3844.705882352941,
      "grad_norm": 23.396881103515625,
      "learning_rate": 1.1552941176470588e-05,
      "loss": 0.7363,
      "step": 65360
    },
    {
      "epoch": 3845.294117647059,
      "grad_norm": 19.241350173950195,
      "learning_rate": 1.1547058823529413e-05,
      "loss": 0.7198,
      "step": 65370
    },
    {
      "epoch": 3845.8823529411766,
      "grad_norm": 19.532976150512695,
      "learning_rate": 1.1541176470588236e-05,
      "loss": 0.6526,
      "step": 65380
    },
    {
      "epoch": 3846.470588235294,
      "grad_norm": 18.858888626098633,
      "learning_rate": 1.1535294117647059e-05,
      "loss": 0.7921,
      "step": 65390
    },
    {
      "epoch": 3847.0588235294117,
      "grad_norm": 24.039684295654297,
      "learning_rate": 1.1529411764705883e-05,
      "loss": 0.6838,
      "step": 65400
    },
    {
      "epoch": 3847.6470588235293,
      "grad_norm": 20.07727813720703,
      "learning_rate": 1.1523529411764706e-05,
      "loss": 0.67,
      "step": 65410
    },
    {
      "epoch": 3848.235294117647,
      "grad_norm": 14.625092506408691,
      "learning_rate": 1.1517647058823531e-05,
      "loss": 0.78,
      "step": 65420
    },
    {
      "epoch": 3848.823529411765,
      "grad_norm": 24.94809913635254,
      "learning_rate": 1.1511764705882354e-05,
      "loss": 0.7292,
      "step": 65430
    },
    {
      "epoch": 3849.4117647058824,
      "grad_norm": 20.414522171020508,
      "learning_rate": 1.1505882352941177e-05,
      "loss": 0.7349,
      "step": 65440
    },
    {
      "epoch": 3850.0,
      "grad_norm": 30.341114044189453,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.7379,
      "step": 65450
    },
    {
      "epoch": 3850.5882352941176,
      "grad_norm": 20.213104248046875,
      "learning_rate": 1.1494117647058825e-05,
      "loss": 0.685,
      "step": 65460
    },
    {
      "epoch": 3851.176470588235,
      "grad_norm": 16.92842674255371,
      "learning_rate": 1.1488235294117648e-05,
      "loss": 0.6505,
      "step": 65470
    },
    {
      "epoch": 3851.764705882353,
      "grad_norm": 21.804824829101562,
      "learning_rate": 1.148235294117647e-05,
      "loss": 0.6643,
      "step": 65480
    },
    {
      "epoch": 3852.3529411764707,
      "grad_norm": 25.918413162231445,
      "learning_rate": 1.1476470588235294e-05,
      "loss": 0.7192,
      "step": 65490
    },
    {
      "epoch": 3852.9411764705883,
      "grad_norm": 29.766340255737305,
      "learning_rate": 1.1470588235294118e-05,
      "loss": 0.7379,
      "step": 65500
    },
    {
      "epoch": 3853.529411764706,
      "grad_norm": 17.46042251586914,
      "learning_rate": 1.1464705882352941e-05,
      "loss": 0.7366,
      "step": 65510
    },
    {
      "epoch": 3854.1176470588234,
      "grad_norm": 21.97212791442871,
      "learning_rate": 1.1458823529411764e-05,
      "loss": 0.7509,
      "step": 65520
    },
    {
      "epoch": 3854.705882352941,
      "grad_norm": 16.052173614501953,
      "learning_rate": 1.1452941176470589e-05,
      "loss": 0.7014,
      "step": 65530
    },
    {
      "epoch": 3855.294117647059,
      "grad_norm": 14.657522201538086,
      "learning_rate": 1.1447058823529412e-05,
      "loss": 0.7015,
      "step": 65540
    },
    {
      "epoch": 3855.8823529411766,
      "grad_norm": 17.915639877319336,
      "learning_rate": 1.1441176470588235e-05,
      "loss": 0.815,
      "step": 65550
    },
    {
      "epoch": 3856.470588235294,
      "grad_norm": 21.13311004638672,
      "learning_rate": 1.143529411764706e-05,
      "loss": 0.7364,
      "step": 65560
    },
    {
      "epoch": 3857.0588235294117,
      "grad_norm": 16.113052368164062,
      "learning_rate": 1.1429411764705883e-05,
      "loss": 0.6724,
      "step": 65570
    },
    {
      "epoch": 3857.6470588235293,
      "grad_norm": 18.075178146362305,
      "learning_rate": 1.1423529411764707e-05,
      "loss": 0.7553,
      "step": 65580
    },
    {
      "epoch": 3858.235294117647,
      "grad_norm": 15.93358325958252,
      "learning_rate": 1.141764705882353e-05,
      "loss": 0.7114,
      "step": 65590
    },
    {
      "epoch": 3858.823529411765,
      "grad_norm": 19.90308380126953,
      "learning_rate": 1.1411764705882353e-05,
      "loss": 0.7172,
      "step": 65600
    },
    {
      "epoch": 3859.4117647058824,
      "grad_norm": 28.153064727783203,
      "learning_rate": 1.1405882352941178e-05,
      "loss": 0.7299,
      "step": 65610
    },
    {
      "epoch": 3860.0,
      "grad_norm": 26.593835830688477,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.7245,
      "step": 65620
    },
    {
      "epoch": 3860.5882352941176,
      "grad_norm": 19.769418716430664,
      "learning_rate": 1.1394117647058824e-05,
      "loss": 0.6948,
      "step": 65630
    },
    {
      "epoch": 3861.176470588235,
      "grad_norm": 18.117307662963867,
      "learning_rate": 1.1388235294117649e-05,
      "loss": 0.7421,
      "step": 65640
    },
    {
      "epoch": 3861.764705882353,
      "grad_norm": 22.685401916503906,
      "learning_rate": 1.1382352941176472e-05,
      "loss": 0.7028,
      "step": 65650
    },
    {
      "epoch": 3862.3529411764707,
      "grad_norm": 17.162212371826172,
      "learning_rate": 1.1376470588235295e-05,
      "loss": 0.7913,
      "step": 65660
    },
    {
      "epoch": 3862.9411764705883,
      "grad_norm": 21.728870391845703,
      "learning_rate": 1.1370588235294118e-05,
      "loss": 0.6068,
      "step": 65670
    },
    {
      "epoch": 3863.529411764706,
      "grad_norm": 19.58157730102539,
      "learning_rate": 1.136470588235294e-05,
      "loss": 0.6807,
      "step": 65680
    },
    {
      "epoch": 3864.1176470588234,
      "grad_norm": 27.400936126708984,
      "learning_rate": 1.1358823529411765e-05,
      "loss": 0.6162,
      "step": 65690
    },
    {
      "epoch": 3864.705882352941,
      "grad_norm": 22.30249786376953,
      "learning_rate": 1.1352941176470588e-05,
      "loss": 0.7306,
      "step": 65700
    },
    {
      "epoch": 3865.294117647059,
      "grad_norm": 20.72628402709961,
      "learning_rate": 1.1347058823529411e-05,
      "loss": 0.755,
      "step": 65710
    },
    {
      "epoch": 3865.8823529411766,
      "grad_norm": 19.97687339782715,
      "learning_rate": 1.1341176470588236e-05,
      "loss": 0.7942,
      "step": 65720
    },
    {
      "epoch": 3866.470588235294,
      "grad_norm": 24.15677261352539,
      "learning_rate": 1.1335294117647059e-05,
      "loss": 0.8466,
      "step": 65730
    },
    {
      "epoch": 3867.0588235294117,
      "grad_norm": 22.377525329589844,
      "learning_rate": 1.1329411764705884e-05,
      "loss": 0.7355,
      "step": 65740
    },
    {
      "epoch": 3867.6470588235293,
      "grad_norm": 17.295944213867188,
      "learning_rate": 1.1323529411764707e-05,
      "loss": 0.6896,
      "step": 65750
    },
    {
      "epoch": 3868.235294117647,
      "grad_norm": 23.026105880737305,
      "learning_rate": 1.131764705882353e-05,
      "loss": 0.7209,
      "step": 65760
    },
    {
      "epoch": 3868.823529411765,
      "grad_norm": 17.780582427978516,
      "learning_rate": 1.1311764705882354e-05,
      "loss": 0.6793,
      "step": 65770
    },
    {
      "epoch": 3869.4117647058824,
      "grad_norm": 26.24125862121582,
      "learning_rate": 1.1305882352941177e-05,
      "loss": 0.8148,
      "step": 65780
    },
    {
      "epoch": 3870.0,
      "grad_norm": 25.169925689697266,
      "learning_rate": 1.13e-05,
      "loss": 0.8016,
      "step": 65790
    },
    {
      "epoch": 3870.5882352941176,
      "grad_norm": 20.095251083374023,
      "learning_rate": 1.1294117647058825e-05,
      "loss": 0.6429,
      "step": 65800
    },
    {
      "epoch": 3871.176470588235,
      "grad_norm": 32.13078689575195,
      "learning_rate": 1.1288235294117648e-05,
      "loss": 0.6891,
      "step": 65810
    },
    {
      "epoch": 3871.764705882353,
      "grad_norm": 19.722869873046875,
      "learning_rate": 1.1282352941176471e-05,
      "loss": 0.7134,
      "step": 65820
    },
    {
      "epoch": 3872.3529411764707,
      "grad_norm": 17.289884567260742,
      "learning_rate": 1.1276470588235296e-05,
      "loss": 0.7468,
      "step": 65830
    },
    {
      "epoch": 3872.9411764705883,
      "grad_norm": 25.0331974029541,
      "learning_rate": 1.1270588235294117e-05,
      "loss": 0.7736,
      "step": 65840
    },
    {
      "epoch": 3873.529411764706,
      "grad_norm": 19.239391326904297,
      "learning_rate": 1.1264705882352942e-05,
      "loss": 0.694,
      "step": 65850
    },
    {
      "epoch": 3874.1176470588234,
      "grad_norm": 16.14647102355957,
      "learning_rate": 1.1258823529411765e-05,
      "loss": 0.7371,
      "step": 65860
    },
    {
      "epoch": 3874.705882352941,
      "grad_norm": 23.863941192626953,
      "learning_rate": 1.1252941176470588e-05,
      "loss": 0.8017,
      "step": 65870
    },
    {
      "epoch": 3875.294117647059,
      "grad_norm": 18.34628677368164,
      "learning_rate": 1.1247058823529412e-05,
      "loss": 0.7422,
      "step": 65880
    },
    {
      "epoch": 3875.8823529411766,
      "grad_norm": 21.399211883544922,
      "learning_rate": 1.1241176470588235e-05,
      "loss": 0.6468,
      "step": 65890
    },
    {
      "epoch": 3876.470588235294,
      "grad_norm": 21.828983306884766,
      "learning_rate": 1.123529411764706e-05,
      "loss": 0.716,
      "step": 65900
    },
    {
      "epoch": 3877.0588235294117,
      "grad_norm": 27.231836318969727,
      "learning_rate": 1.1229411764705883e-05,
      "loss": 0.8751,
      "step": 65910
    },
    {
      "epoch": 3877.6470588235293,
      "grad_norm": 22.16227912902832,
      "learning_rate": 1.1223529411764706e-05,
      "loss": 0.722,
      "step": 65920
    },
    {
      "epoch": 3878.235294117647,
      "grad_norm": 23.527536392211914,
      "learning_rate": 1.121764705882353e-05,
      "loss": 0.6535,
      "step": 65930
    },
    {
      "epoch": 3878.823529411765,
      "grad_norm": 22.994626998901367,
      "learning_rate": 1.1211764705882354e-05,
      "loss": 0.7969,
      "step": 65940
    },
    {
      "epoch": 3879.4117647058824,
      "grad_norm": 25.121105194091797,
      "learning_rate": 1.1205882352941177e-05,
      "loss": 0.7498,
      "step": 65950
    },
    {
      "epoch": 3880.0,
      "grad_norm": 19.533470153808594,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.7001,
      "step": 65960
    },
    {
      "epoch": 3880.5882352941176,
      "grad_norm": 14.608922958374023,
      "learning_rate": 1.1194117647058824e-05,
      "loss": 0.7425,
      "step": 65970
    },
    {
      "epoch": 3881.176470588235,
      "grad_norm": 31.22246742248535,
      "learning_rate": 1.1188235294117647e-05,
      "loss": 0.7717,
      "step": 65980
    },
    {
      "epoch": 3881.764705882353,
      "grad_norm": 19.128637313842773,
      "learning_rate": 1.1182352941176472e-05,
      "loss": 0.6434,
      "step": 65990
    },
    {
      "epoch": 3882.3529411764707,
      "grad_norm": 21.94136619567871,
      "learning_rate": 1.1176470588235295e-05,
      "loss": 0.6467,
      "step": 66000
    },
    {
      "epoch": 3882.9411764705883,
      "grad_norm": 21.43056869506836,
      "learning_rate": 1.117058823529412e-05,
      "loss": 0.69,
      "step": 66010
    },
    {
      "epoch": 3883.529411764706,
      "grad_norm": 21.6136417388916,
      "learning_rate": 1.1164705882352943e-05,
      "loss": 0.8337,
      "step": 66020
    },
    {
      "epoch": 3884.1176470588234,
      "grad_norm": 26.93834114074707,
      "learning_rate": 1.1158823529411764e-05,
      "loss": 0.7696,
      "step": 66030
    },
    {
      "epoch": 3884.705882352941,
      "grad_norm": 21.93485450744629,
      "learning_rate": 1.1152941176470588e-05,
      "loss": 0.7996,
      "step": 66040
    },
    {
      "epoch": 3885.294117647059,
      "grad_norm": 28.06964111328125,
      "learning_rate": 1.1147058823529411e-05,
      "loss": 0.7104,
      "step": 66050
    },
    {
      "epoch": 3885.8823529411766,
      "grad_norm": 20.54081153869629,
      "learning_rate": 1.1141176470588236e-05,
      "loss": 0.7248,
      "step": 66060
    },
    {
      "epoch": 3886.470588235294,
      "grad_norm": 25.144250869750977,
      "learning_rate": 1.1135294117647059e-05,
      "loss": 0.6675,
      "step": 66070
    },
    {
      "epoch": 3887.0588235294117,
      "grad_norm": 22.155487060546875,
      "learning_rate": 1.1129411764705882e-05,
      "loss": 0.6894,
      "step": 66080
    },
    {
      "epoch": 3887.6470588235293,
      "grad_norm": 32.54263687133789,
      "learning_rate": 1.1123529411764707e-05,
      "loss": 0.8312,
      "step": 66090
    },
    {
      "epoch": 3888.235294117647,
      "grad_norm": 26.160886764526367,
      "learning_rate": 1.111764705882353e-05,
      "loss": 0.6959,
      "step": 66100
    },
    {
      "epoch": 3888.823529411765,
      "grad_norm": 15.123133659362793,
      "learning_rate": 1.1111764705882353e-05,
      "loss": 0.817,
      "step": 66110
    },
    {
      "epoch": 3889.4117647058824,
      "grad_norm": 18.500925064086914,
      "learning_rate": 1.1105882352941177e-05,
      "loss": 0.7147,
      "step": 66120
    },
    {
      "epoch": 3890.0,
      "grad_norm": 20.1789493560791,
      "learning_rate": 1.11e-05,
      "loss": 0.7625,
      "step": 66130
    },
    {
      "epoch": 3890.5882352941176,
      "grad_norm": 13.36484146118164,
      "learning_rate": 1.1094117647058823e-05,
      "loss": 0.7492,
      "step": 66140
    },
    {
      "epoch": 3891.176470588235,
      "grad_norm": 19.57636070251465,
      "learning_rate": 1.1088235294117648e-05,
      "loss": 0.7674,
      "step": 66150
    },
    {
      "epoch": 3891.764705882353,
      "grad_norm": 18.337339401245117,
      "learning_rate": 1.1082352941176471e-05,
      "loss": 0.7401,
      "step": 66160
    },
    {
      "epoch": 3892.3529411764707,
      "grad_norm": 23.89906883239746,
      "learning_rate": 1.1076470588235296e-05,
      "loss": 0.7797,
      "step": 66170
    },
    {
      "epoch": 3892.9411764705883,
      "grad_norm": 26.762252807617188,
      "learning_rate": 1.1070588235294119e-05,
      "loss": 0.7697,
      "step": 66180
    },
    {
      "epoch": 3893.529411764706,
      "grad_norm": 24.775047302246094,
      "learning_rate": 1.1064705882352942e-05,
      "loss": 0.7594,
      "step": 66190
    },
    {
      "epoch": 3894.1176470588234,
      "grad_norm": 21.99782371520996,
      "learning_rate": 1.1058823529411766e-05,
      "loss": 0.8212,
      "step": 66200
    },
    {
      "epoch": 3894.705882352941,
      "grad_norm": 22.392391204833984,
      "learning_rate": 1.1052941176470588e-05,
      "loss": 0.6938,
      "step": 66210
    },
    {
      "epoch": 3895.294117647059,
      "grad_norm": 22.521547317504883,
      "learning_rate": 1.1047058823529412e-05,
      "loss": 0.7165,
      "step": 66220
    },
    {
      "epoch": 3895.8823529411766,
      "grad_norm": 20.499988555908203,
      "learning_rate": 1.1041176470588235e-05,
      "loss": 0.818,
      "step": 66230
    },
    {
      "epoch": 3896.470588235294,
      "grad_norm": 26.316795349121094,
      "learning_rate": 1.1035294117647058e-05,
      "loss": 0.7141,
      "step": 66240
    },
    {
      "epoch": 3897.0588235294117,
      "grad_norm": 17.115026473999023,
      "learning_rate": 1.1029411764705883e-05,
      "loss": 0.5557,
      "step": 66250
    },
    {
      "epoch": 3897.6470588235293,
      "grad_norm": 18.639389038085938,
      "learning_rate": 1.1023529411764706e-05,
      "loss": 0.7477,
      "step": 66260
    },
    {
      "epoch": 3898.235294117647,
      "grad_norm": 18.731964111328125,
      "learning_rate": 1.1017647058823529e-05,
      "loss": 0.681,
      "step": 66270
    },
    {
      "epoch": 3898.823529411765,
      "grad_norm": 25.028308868408203,
      "learning_rate": 1.1011764705882354e-05,
      "loss": 0.7209,
      "step": 66280
    },
    {
      "epoch": 3899.4117647058824,
      "grad_norm": 17.7503662109375,
      "learning_rate": 1.1005882352941177e-05,
      "loss": 0.6643,
      "step": 66290
    },
    {
      "epoch": 3900.0,
      "grad_norm": 25.108654022216797,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.7174,
      "step": 66300
    },
    {
      "epoch": 3900.5882352941176,
      "grad_norm": 20.62039566040039,
      "learning_rate": 1.0994117647058824e-05,
      "loss": 0.6907,
      "step": 66310
    },
    {
      "epoch": 3901.176470588235,
      "grad_norm": 14.134644508361816,
      "learning_rate": 1.0988235294117647e-05,
      "loss": 0.7353,
      "step": 66320
    },
    {
      "epoch": 3901.764705882353,
      "grad_norm": 21.057403564453125,
      "learning_rate": 1.0982352941176472e-05,
      "loss": 0.7428,
      "step": 66330
    },
    {
      "epoch": 3902.3529411764707,
      "grad_norm": 15.365516662597656,
      "learning_rate": 1.0976470588235295e-05,
      "loss": 0.7773,
      "step": 66340
    },
    {
      "epoch": 3902.9411764705883,
      "grad_norm": 18.89404296875,
      "learning_rate": 1.0970588235294118e-05,
      "loss": 0.6281,
      "step": 66350
    },
    {
      "epoch": 3903.529411764706,
      "grad_norm": 24.91090965270996,
      "learning_rate": 1.0964705882352943e-05,
      "loss": 0.8631,
      "step": 66360
    },
    {
      "epoch": 3904.1176470588234,
      "grad_norm": 17.797199249267578,
      "learning_rate": 1.0958823529411766e-05,
      "loss": 0.6453,
      "step": 66370
    },
    {
      "epoch": 3904.705882352941,
      "grad_norm": 22.969207763671875,
      "learning_rate": 1.0952941176470589e-05,
      "loss": 0.7203,
      "step": 66380
    },
    {
      "epoch": 3905.294117647059,
      "grad_norm": 16.379484176635742,
      "learning_rate": 1.0947058823529412e-05,
      "loss": 0.7223,
      "step": 66390
    },
    {
      "epoch": 3905.8823529411766,
      "grad_norm": 17.48320198059082,
      "learning_rate": 1.0941176470588235e-05,
      "loss": 0.7204,
      "step": 66400
    },
    {
      "epoch": 3906.470588235294,
      "grad_norm": 19.634952545166016,
      "learning_rate": 1.093529411764706e-05,
      "loss": 0.7368,
      "step": 66410
    },
    {
      "epoch": 3907.0588235294117,
      "grad_norm": 22.53367042541504,
      "learning_rate": 1.0929411764705882e-05,
      "loss": 0.7605,
      "step": 66420
    },
    {
      "epoch": 3907.6470588235293,
      "grad_norm": 23.942323684692383,
      "learning_rate": 1.0923529411764705e-05,
      "loss": 0.7636,
      "step": 66430
    },
    {
      "epoch": 3908.235294117647,
      "grad_norm": 16.618703842163086,
      "learning_rate": 1.091764705882353e-05,
      "loss": 0.6302,
      "step": 66440
    },
    {
      "epoch": 3908.823529411765,
      "grad_norm": 23.985422134399414,
      "learning_rate": 1.0911764705882353e-05,
      "loss": 0.7689,
      "step": 66450
    },
    {
      "epoch": 3909.4117647058824,
      "grad_norm": 21.903417587280273,
      "learning_rate": 1.0905882352941178e-05,
      "loss": 0.7328,
      "step": 66460
    },
    {
      "epoch": 3910.0,
      "grad_norm": 25.222082138061523,
      "learning_rate": 1.09e-05,
      "loss": 0.6801,
      "step": 66470
    },
    {
      "epoch": 3910.5882352941176,
      "grad_norm": 23.955801010131836,
      "learning_rate": 1.0894117647058824e-05,
      "loss": 0.7619,
      "step": 66480
    },
    {
      "epoch": 3911.176470588235,
      "grad_norm": 19.792695999145508,
      "learning_rate": 1.0888235294117648e-05,
      "loss": 0.7178,
      "step": 66490
    },
    {
      "epoch": 3911.764705882353,
      "grad_norm": 20.18166732788086,
      "learning_rate": 1.0882352941176471e-05,
      "loss": 0.7108,
      "step": 66500
    },
    {
      "epoch": 3912.3529411764707,
      "grad_norm": 18.089954376220703,
      "learning_rate": 1.0876470588235294e-05,
      "loss": 0.6331,
      "step": 66510
    },
    {
      "epoch": 3912.9411764705883,
      "grad_norm": 17.12310028076172,
      "learning_rate": 1.0870588235294119e-05,
      "loss": 0.6883,
      "step": 66520
    },
    {
      "epoch": 3913.529411764706,
      "grad_norm": 24.51459312438965,
      "learning_rate": 1.0864705882352942e-05,
      "loss": 0.8551,
      "step": 66530
    },
    {
      "epoch": 3914.1176470588234,
      "grad_norm": 23.400636672973633,
      "learning_rate": 1.0858823529411765e-05,
      "loss": 0.692,
      "step": 66540
    },
    {
      "epoch": 3914.705882352941,
      "grad_norm": 18.92755889892578,
      "learning_rate": 1.085294117647059e-05,
      "loss": 0.7394,
      "step": 66550
    },
    {
      "epoch": 3915.294117647059,
      "grad_norm": 19.739606857299805,
      "learning_rate": 1.0847058823529413e-05,
      "loss": 0.6917,
      "step": 66560
    },
    {
      "epoch": 3915.8823529411766,
      "grad_norm": 22.328113555908203,
      "learning_rate": 1.0841176470588236e-05,
      "loss": 0.6401,
      "step": 66570
    },
    {
      "epoch": 3916.470588235294,
      "grad_norm": 15.94355583190918,
      "learning_rate": 1.0835294117647059e-05,
      "loss": 0.6626,
      "step": 66580
    },
    {
      "epoch": 3917.0588235294117,
      "grad_norm": 20.332538604736328,
      "learning_rate": 1.0829411764705882e-05,
      "loss": 0.7158,
      "step": 66590
    },
    {
      "epoch": 3917.6470588235293,
      "grad_norm": 20.53542137145996,
      "learning_rate": 1.0823529411764706e-05,
      "loss": 0.7717,
      "step": 66600
    },
    {
      "epoch": 3918.235294117647,
      "grad_norm": 19.20563316345215,
      "learning_rate": 1.081764705882353e-05,
      "loss": 0.7728,
      "step": 66610
    },
    {
      "epoch": 3918.823529411765,
      "grad_norm": 21.84908103942871,
      "learning_rate": 1.0811764705882354e-05,
      "loss": 0.6081,
      "step": 66620
    },
    {
      "epoch": 3919.4117647058824,
      "grad_norm": 23.676036834716797,
      "learning_rate": 1.0805882352941177e-05,
      "loss": 0.63,
      "step": 66630
    },
    {
      "epoch": 3920.0,
      "grad_norm": 27.76312255859375,
      "learning_rate": 1.08e-05,
      "loss": 0.7706,
      "step": 66640
    },
    {
      "epoch": 3920.5882352941176,
      "grad_norm": 22.80388641357422,
      "learning_rate": 1.0794117647058825e-05,
      "loss": 0.7272,
      "step": 66650
    },
    {
      "epoch": 3921.176470588235,
      "grad_norm": 25.574329376220703,
      "learning_rate": 1.0788235294117648e-05,
      "loss": 0.8066,
      "step": 66660
    },
    {
      "epoch": 3921.764705882353,
      "grad_norm": 31.00550079345703,
      "learning_rate": 1.078235294117647e-05,
      "loss": 0.7211,
      "step": 66670
    },
    {
      "epoch": 3922.3529411764707,
      "grad_norm": 23.433326721191406,
      "learning_rate": 1.0776470588235295e-05,
      "loss": 0.6541,
      "step": 66680
    },
    {
      "epoch": 3922.9411764705883,
      "grad_norm": 23.342985153198242,
      "learning_rate": 1.0770588235294118e-05,
      "loss": 0.6823,
      "step": 66690
    },
    {
      "epoch": 3923.529411764706,
      "grad_norm": 19.166799545288086,
      "learning_rate": 1.0764705882352941e-05,
      "loss": 0.7915,
      "step": 66700
    },
    {
      "epoch": 3924.1176470588234,
      "grad_norm": 16.418468475341797,
      "learning_rate": 1.0758823529411766e-05,
      "loss": 0.6827,
      "step": 66710
    },
    {
      "epoch": 3924.705882352941,
      "grad_norm": 20.476688385009766,
      "learning_rate": 1.0752941176470589e-05,
      "loss": 0.6872,
      "step": 66720
    },
    {
      "epoch": 3925.294117647059,
      "grad_norm": 20.007957458496094,
      "learning_rate": 1.0747058823529414e-05,
      "loss": 0.6682,
      "step": 66730
    },
    {
      "epoch": 3925.8823529411766,
      "grad_norm": 22.331735610961914,
      "learning_rate": 1.0741176470588237e-05,
      "loss": 0.7324,
      "step": 66740
    },
    {
      "epoch": 3926.470588235294,
      "grad_norm": 19.530136108398438,
      "learning_rate": 1.0735294117647058e-05,
      "loss": 0.7014,
      "step": 66750
    },
    {
      "epoch": 3927.0588235294117,
      "grad_norm": 20.783367156982422,
      "learning_rate": 1.0729411764705883e-05,
      "loss": 0.8422,
      "step": 66760
    },
    {
      "epoch": 3927.6470588235293,
      "grad_norm": 18.026134490966797,
      "learning_rate": 1.0723529411764706e-05,
      "loss": 0.7663,
      "step": 66770
    },
    {
      "epoch": 3928.235294117647,
      "grad_norm": 21.405851364135742,
      "learning_rate": 1.071764705882353e-05,
      "loss": 0.8162,
      "step": 66780
    },
    {
      "epoch": 3928.823529411765,
      "grad_norm": 20.50847816467285,
      "learning_rate": 1.0711764705882353e-05,
      "loss": 0.7335,
      "step": 66790
    },
    {
      "epoch": 3929.4117647058824,
      "grad_norm": 35.89430618286133,
      "learning_rate": 1.0705882352941176e-05,
      "loss": 0.6713,
      "step": 66800
    },
    {
      "epoch": 3930.0,
      "grad_norm": 17.739315032958984,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.617,
      "step": 66810
    },
    {
      "epoch": 3930.5882352941176,
      "grad_norm": 29.449552536010742,
      "learning_rate": 1.0694117647058824e-05,
      "loss": 0.6417,
      "step": 66820
    },
    {
      "epoch": 3931.176470588235,
      "grad_norm": 22.968494415283203,
      "learning_rate": 1.0688235294117647e-05,
      "loss": 0.7241,
      "step": 66830
    },
    {
      "epoch": 3931.764705882353,
      "grad_norm": 24.468168258666992,
      "learning_rate": 1.0682352941176472e-05,
      "loss": 0.7231,
      "step": 66840
    },
    {
      "epoch": 3932.3529411764707,
      "grad_norm": 26.08898162841797,
      "learning_rate": 1.0676470588235295e-05,
      "loss": 0.6635,
      "step": 66850
    },
    {
      "epoch": 3932.9411764705883,
      "grad_norm": 19.82215118408203,
      "learning_rate": 1.0670588235294118e-05,
      "loss": 0.789,
      "step": 66860
    },
    {
      "epoch": 3933.529411764706,
      "grad_norm": 26.65337371826172,
      "learning_rate": 1.0664705882352942e-05,
      "loss": 0.856,
      "step": 66870
    },
    {
      "epoch": 3934.1176470588234,
      "grad_norm": 21.393953323364258,
      "learning_rate": 1.0658823529411765e-05,
      "loss": 0.7413,
      "step": 66880
    },
    {
      "epoch": 3934.705882352941,
      "grad_norm": 18.284034729003906,
      "learning_rate": 1.065294117647059e-05,
      "loss": 0.6233,
      "step": 66890
    },
    {
      "epoch": 3935.294117647059,
      "grad_norm": 20.329280853271484,
      "learning_rate": 1.0647058823529413e-05,
      "loss": 0.6869,
      "step": 66900
    },
    {
      "epoch": 3935.8823529411766,
      "grad_norm": 20.74604606628418,
      "learning_rate": 1.0641176470588236e-05,
      "loss": 0.6343,
      "step": 66910
    },
    {
      "epoch": 3936.470588235294,
      "grad_norm": 17.166484832763672,
      "learning_rate": 1.063529411764706e-05,
      "loss": 0.814,
      "step": 66920
    },
    {
      "epoch": 3937.0588235294117,
      "grad_norm": 24.96382713317871,
      "learning_rate": 1.0629411764705882e-05,
      "loss": 0.6893,
      "step": 66930
    },
    {
      "epoch": 3937.6470588235293,
      "grad_norm": 19.441816329956055,
      "learning_rate": 1.0623529411764707e-05,
      "loss": 0.7654,
      "step": 66940
    },
    {
      "epoch": 3938.235294117647,
      "grad_norm": 21.024715423583984,
      "learning_rate": 1.061764705882353e-05,
      "loss": 0.761,
      "step": 66950
    },
    {
      "epoch": 3938.823529411765,
      "grad_norm": 18.345565795898438,
      "learning_rate": 1.0611764705882352e-05,
      "loss": 0.6971,
      "step": 66960
    },
    {
      "epoch": 3939.4117647058824,
      "grad_norm": 16.233375549316406,
      "learning_rate": 1.0605882352941177e-05,
      "loss": 0.6753,
      "step": 66970
    },
    {
      "epoch": 3940.0,
      "grad_norm": 19.74289321899414,
      "learning_rate": 1.06e-05,
      "loss": 0.7006,
      "step": 66980
    },
    {
      "epoch": 3940.5882352941176,
      "grad_norm": 16.146440505981445,
      "learning_rate": 1.0594117647058823e-05,
      "loss": 0.7126,
      "step": 66990
    },
    {
      "epoch": 3941.176470588235,
      "grad_norm": 23.16229248046875,
      "learning_rate": 1.0588235294117648e-05,
      "loss": 0.8027,
      "step": 67000
    },
    {
      "epoch": 3941.764705882353,
      "grad_norm": 15.576476097106934,
      "learning_rate": 1.058235294117647e-05,
      "loss": 0.6725,
      "step": 67010
    },
    {
      "epoch": 3942.3529411764707,
      "grad_norm": 27.028722763061523,
      "learning_rate": 1.0576470588235294e-05,
      "loss": 0.8361,
      "step": 67020
    },
    {
      "epoch": 3942.9411764705883,
      "grad_norm": 23.74467658996582,
      "learning_rate": 1.0570588235294118e-05,
      "loss": 0.6646,
      "step": 67030
    },
    {
      "epoch": 3943.529411764706,
      "grad_norm": 21.3026065826416,
      "learning_rate": 1.0564705882352941e-05,
      "loss": 0.7608,
      "step": 67040
    },
    {
      "epoch": 3944.1176470588234,
      "grad_norm": 19.026729583740234,
      "learning_rate": 1.0558823529411766e-05,
      "loss": 0.848,
      "step": 67050
    },
    {
      "epoch": 3944.705882352941,
      "grad_norm": 21.636884689331055,
      "learning_rate": 1.0552941176470589e-05,
      "loss": 0.7662,
      "step": 67060
    },
    {
      "epoch": 3945.294117647059,
      "grad_norm": 26.687013626098633,
      "learning_rate": 1.0547058823529412e-05,
      "loss": 0.7525,
      "step": 67070
    },
    {
      "epoch": 3945.8823529411766,
      "grad_norm": 19.971263885498047,
      "learning_rate": 1.0541176470588237e-05,
      "loss": 0.6133,
      "step": 67080
    },
    {
      "epoch": 3946.470588235294,
      "grad_norm": 21.502164840698242,
      "learning_rate": 1.053529411764706e-05,
      "loss": 0.7738,
      "step": 67090
    },
    {
      "epoch": 3947.0588235294117,
      "grad_norm": 18.73470687866211,
      "learning_rate": 1.0529411764705883e-05,
      "loss": 0.7585,
      "step": 67100
    },
    {
      "epoch": 3947.6470588235293,
      "grad_norm": 21.994409561157227,
      "learning_rate": 1.0523529411764707e-05,
      "loss": 0.7522,
      "step": 67110
    },
    {
      "epoch": 3948.235294117647,
      "grad_norm": 24.911258697509766,
      "learning_rate": 1.0517647058823529e-05,
      "loss": 0.7588,
      "step": 67120
    },
    {
      "epoch": 3948.823529411765,
      "grad_norm": 19.07596778869629,
      "learning_rate": 1.0511764705882353e-05,
      "loss": 0.764,
      "step": 67130
    },
    {
      "epoch": 3949.4117647058824,
      "grad_norm": 26.023521423339844,
      "learning_rate": 1.0505882352941176e-05,
      "loss": 0.7371,
      "step": 67140
    },
    {
      "epoch": 3950.0,
      "grad_norm": 26.7641544342041,
      "learning_rate": 1.05e-05,
      "loss": 0.6666,
      "step": 67150
    },
    {
      "epoch": 3950.5882352941176,
      "grad_norm": 18.94304847717285,
      "learning_rate": 1.0494117647058824e-05,
      "loss": 0.6103,
      "step": 67160
    },
    {
      "epoch": 3951.176470588235,
      "grad_norm": 12.749011993408203,
      "learning_rate": 1.0488235294117647e-05,
      "loss": 0.6666,
      "step": 67170
    },
    {
      "epoch": 3951.764705882353,
      "grad_norm": 24.751596450805664,
      "learning_rate": 1.0482352941176472e-05,
      "loss": 0.7382,
      "step": 67180
    },
    {
      "epoch": 3952.3529411764707,
      "grad_norm": 22.591859817504883,
      "learning_rate": 1.0476470588235295e-05,
      "loss": 0.7402,
      "step": 67190
    },
    {
      "epoch": 3952.9411764705883,
      "grad_norm": 16.230403900146484,
      "learning_rate": 1.0470588235294118e-05,
      "loss": 0.7994,
      "step": 67200
    },
    {
      "epoch": 3953.529411764706,
      "grad_norm": 19.45880699157715,
      "learning_rate": 1.0464705882352942e-05,
      "loss": 0.6667,
      "step": 67210
    },
    {
      "epoch": 3954.1176470588234,
      "grad_norm": 22.818347930908203,
      "learning_rate": 1.0458823529411765e-05,
      "loss": 0.7302,
      "step": 67220
    },
    {
      "epoch": 3954.705882352941,
      "grad_norm": 18.481760025024414,
      "learning_rate": 1.0452941176470588e-05,
      "loss": 0.7241,
      "step": 67230
    },
    {
      "epoch": 3955.294117647059,
      "grad_norm": 18.380046844482422,
      "learning_rate": 1.0447058823529413e-05,
      "loss": 0.7318,
      "step": 67240
    },
    {
      "epoch": 3955.8823529411766,
      "grad_norm": 17.03023910522461,
      "learning_rate": 1.0441176470588236e-05,
      "loss": 0.7835,
      "step": 67250
    },
    {
      "epoch": 3956.470588235294,
      "grad_norm": 19.717037200927734,
      "learning_rate": 1.0435294117647059e-05,
      "loss": 0.6683,
      "step": 67260
    },
    {
      "epoch": 3957.0588235294117,
      "grad_norm": 22.708642959594727,
      "learning_rate": 1.0429411764705884e-05,
      "loss": 0.6824,
      "step": 67270
    },
    {
      "epoch": 3957.6470588235293,
      "grad_norm": 15.932808876037598,
      "learning_rate": 1.0423529411764707e-05,
      "loss": 0.7201,
      "step": 67280
    },
    {
      "epoch": 3958.235294117647,
      "grad_norm": 14.48218059539795,
      "learning_rate": 1.0417647058823531e-05,
      "loss": 0.6544,
      "step": 67290
    },
    {
      "epoch": 3958.823529411765,
      "grad_norm": 22.233440399169922,
      "learning_rate": 1.0411764705882353e-05,
      "loss": 0.7252,
      "step": 67300
    },
    {
      "epoch": 3959.4117647058824,
      "grad_norm": 16.40821075439453,
      "learning_rate": 1.0405882352941176e-05,
      "loss": 0.6637,
      "step": 67310
    },
    {
      "epoch": 3960.0,
      "grad_norm": 24.97271156311035,
      "learning_rate": 1.04e-05,
      "loss": 0.6587,
      "step": 67320
    },
    {
      "epoch": 3960.5882352941176,
      "grad_norm": 12.89801025390625,
      "learning_rate": 1.0394117647058823e-05,
      "loss": 0.6301,
      "step": 67330
    },
    {
      "epoch": 3961.176470588235,
      "grad_norm": 29.24464225769043,
      "learning_rate": 1.0388235294117648e-05,
      "loss": 0.7158,
      "step": 67340
    },
    {
      "epoch": 3961.764705882353,
      "grad_norm": 22.562969207763672,
      "learning_rate": 1.0382352941176471e-05,
      "loss": 0.7378,
      "step": 67350
    },
    {
      "epoch": 3962.3529411764707,
      "grad_norm": 16.700239181518555,
      "learning_rate": 1.0376470588235294e-05,
      "loss": 0.6837,
      "step": 67360
    },
    {
      "epoch": 3962.9411764705883,
      "grad_norm": 23.151212692260742,
      "learning_rate": 1.0370588235294119e-05,
      "loss": 0.8181,
      "step": 67370
    },
    {
      "epoch": 3963.529411764706,
      "grad_norm": 22.894868850708008,
      "learning_rate": 1.0364705882352942e-05,
      "loss": 0.7056,
      "step": 67380
    },
    {
      "epoch": 3964.1176470588234,
      "grad_norm": 15.425339698791504,
      "learning_rate": 1.0358823529411765e-05,
      "loss": 0.7579,
      "step": 67390
    },
    {
      "epoch": 3964.705882352941,
      "grad_norm": 19.61574935913086,
      "learning_rate": 1.035294117647059e-05,
      "loss": 0.798,
      "step": 67400
    },
    {
      "epoch": 3965.294117647059,
      "grad_norm": 26.721887588500977,
      "learning_rate": 1.0347058823529412e-05,
      "loss": 0.683,
      "step": 67410
    },
    {
      "epoch": 3965.8823529411766,
      "grad_norm": 23.738971710205078,
      "learning_rate": 1.0341176470588235e-05,
      "loss": 0.7347,
      "step": 67420
    },
    {
      "epoch": 3966.470588235294,
      "grad_norm": 21.41853904724121,
      "learning_rate": 1.033529411764706e-05,
      "loss": 0.662,
      "step": 67430
    },
    {
      "epoch": 3967.0588235294117,
      "grad_norm": 22.976415634155273,
      "learning_rate": 1.0329411764705883e-05,
      "loss": 0.6265,
      "step": 67440
    },
    {
      "epoch": 3967.6470588235293,
      "grad_norm": 19.88738441467285,
      "learning_rate": 1.0323529411764708e-05,
      "loss": 0.7337,
      "step": 67450
    },
    {
      "epoch": 3968.235294117647,
      "grad_norm": 22.077329635620117,
      "learning_rate": 1.031764705882353e-05,
      "loss": 0.7788,
      "step": 67460
    },
    {
      "epoch": 3968.823529411765,
      "grad_norm": 20.456878662109375,
      "learning_rate": 1.0311764705882354e-05,
      "loss": 0.6664,
      "step": 67470
    },
    {
      "epoch": 3969.4117647058824,
      "grad_norm": 21.660062789916992,
      "learning_rate": 1.0305882352941177e-05,
      "loss": 0.7953,
      "step": 67480
    },
    {
      "epoch": 3970.0,
      "grad_norm": 19.928695678710938,
      "learning_rate": 1.03e-05,
      "loss": 0.8157,
      "step": 67490
    },
    {
      "epoch": 3970.5882352941176,
      "grad_norm": 31.769094467163086,
      "learning_rate": 1.0294117647058824e-05,
      "loss": 0.756,
      "step": 67500
    },
    {
      "epoch": 3971.176470588235,
      "grad_norm": 23.60542869567871,
      "learning_rate": 1.0288235294117647e-05,
      "loss": 0.6577,
      "step": 67510
    },
    {
      "epoch": 3971.764705882353,
      "grad_norm": 22.120203018188477,
      "learning_rate": 1.028235294117647e-05,
      "loss": 0.7761,
      "step": 67520
    },
    {
      "epoch": 3972.3529411764707,
      "grad_norm": 17.127119064331055,
      "learning_rate": 1.0276470588235295e-05,
      "loss": 0.6507,
      "step": 67530
    },
    {
      "epoch": 3972.9411764705883,
      "grad_norm": 15.377542495727539,
      "learning_rate": 1.0270588235294118e-05,
      "loss": 0.6443,
      "step": 67540
    },
    {
      "epoch": 3973.529411764706,
      "grad_norm": 16.663253784179688,
      "learning_rate": 1.0264705882352941e-05,
      "loss": 0.7079,
      "step": 67550
    },
    {
      "epoch": 3974.1176470588234,
      "grad_norm": 18.764389038085938,
      "learning_rate": 1.0258823529411766e-05,
      "loss": 0.7229,
      "step": 67560
    },
    {
      "epoch": 3974.705882352941,
      "grad_norm": 17.87566375732422,
      "learning_rate": 1.0252941176470589e-05,
      "loss": 0.6705,
      "step": 67570
    },
    {
      "epoch": 3975.294117647059,
      "grad_norm": 19.339420318603516,
      "learning_rate": 1.0247058823529412e-05,
      "loss": 0.7258,
      "step": 67580
    },
    {
      "epoch": 3975.8823529411766,
      "grad_norm": 16.70892333984375,
      "learning_rate": 1.0241176470588236e-05,
      "loss": 0.7774,
      "step": 67590
    },
    {
      "epoch": 3976.470588235294,
      "grad_norm": 30.049890518188477,
      "learning_rate": 1.023529411764706e-05,
      "loss": 0.6927,
      "step": 67600
    },
    {
      "epoch": 3977.0588235294117,
      "grad_norm": 19.16172218322754,
      "learning_rate": 1.0229411764705884e-05,
      "loss": 0.7284,
      "step": 67610
    },
    {
      "epoch": 3977.6470588235293,
      "grad_norm": 18.562238693237305,
      "learning_rate": 1.0223529411764707e-05,
      "loss": 0.7394,
      "step": 67620
    },
    {
      "epoch": 3978.235294117647,
      "grad_norm": 28.576993942260742,
      "learning_rate": 1.021764705882353e-05,
      "loss": 0.7074,
      "step": 67630
    },
    {
      "epoch": 3978.823529411765,
      "grad_norm": 19.810827255249023,
      "learning_rate": 1.0211764705882355e-05,
      "loss": 0.7553,
      "step": 67640
    },
    {
      "epoch": 3979.4117647058824,
      "grad_norm": 26.196123123168945,
      "learning_rate": 1.0205882352941178e-05,
      "loss": 0.771,
      "step": 67650
    },
    {
      "epoch": 3980.0,
      "grad_norm": 23.35931968688965,
      "learning_rate": 1.02e-05,
      "loss": 0.741,
      "step": 67660
    },
    {
      "epoch": 3980.5882352941176,
      "grad_norm": 26.579334259033203,
      "learning_rate": 1.0194117647058824e-05,
      "loss": 0.7713,
      "step": 67670
    },
    {
      "epoch": 3981.176470588235,
      "grad_norm": 21.935455322265625,
      "learning_rate": 1.0188235294117647e-05,
      "loss": 0.6621,
      "step": 67680
    },
    {
      "epoch": 3981.764705882353,
      "grad_norm": 18.374357223510742,
      "learning_rate": 1.0182352941176471e-05,
      "loss": 0.6902,
      "step": 67690
    },
    {
      "epoch": 3982.3529411764707,
      "grad_norm": 18.534643173217773,
      "learning_rate": 1.0176470588235294e-05,
      "loss": 0.7732,
      "step": 67700
    },
    {
      "epoch": 3982.9411764705883,
      "grad_norm": 18.918970108032227,
      "learning_rate": 1.0170588235294117e-05,
      "loss": 0.7345,
      "step": 67710
    },
    {
      "epoch": 3983.529411764706,
      "grad_norm": 20.580324172973633,
      "learning_rate": 1.0164705882352942e-05,
      "loss": 0.8225,
      "step": 67720
    },
    {
      "epoch": 3984.1176470588234,
      "grad_norm": 20.243907928466797,
      "learning_rate": 1.0158823529411765e-05,
      "loss": 0.7177,
      "step": 67730
    },
    {
      "epoch": 3984.705882352941,
      "grad_norm": 18.842113494873047,
      "learning_rate": 1.0152941176470588e-05,
      "loss": 0.7026,
      "step": 67740
    },
    {
      "epoch": 3985.294117647059,
      "grad_norm": 19.40604591369629,
      "learning_rate": 1.0147058823529413e-05,
      "loss": 0.7568,
      "step": 67750
    },
    {
      "epoch": 3985.8823529411766,
      "grad_norm": 13.10440444946289,
      "learning_rate": 1.0141176470588236e-05,
      "loss": 0.6914,
      "step": 67760
    },
    {
      "epoch": 3986.470588235294,
      "grad_norm": 20.244342803955078,
      "learning_rate": 1.013529411764706e-05,
      "loss": 0.744,
      "step": 67770
    },
    {
      "epoch": 3987.0588235294117,
      "grad_norm": 25.453184127807617,
      "learning_rate": 1.0129411764705883e-05,
      "loss": 0.6624,
      "step": 67780
    },
    {
      "epoch": 3987.6470588235293,
      "grad_norm": 19.048912048339844,
      "learning_rate": 1.0123529411764706e-05,
      "loss": 0.6831,
      "step": 67790
    },
    {
      "epoch": 3988.235294117647,
      "grad_norm": 15.221961975097656,
      "learning_rate": 1.0117647058823531e-05,
      "loss": 0.6521,
      "step": 67800
    },
    {
      "epoch": 3988.823529411765,
      "grad_norm": 18.400970458984375,
      "learning_rate": 1.0111764705882354e-05,
      "loss": 0.7108,
      "step": 67810
    },
    {
      "epoch": 3989.4117647058824,
      "grad_norm": 18.739622116088867,
      "learning_rate": 1.0105882352941177e-05,
      "loss": 0.6594,
      "step": 67820
    },
    {
      "epoch": 3990.0,
      "grad_norm": 14.983809471130371,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.6981,
      "step": 67830
    },
    {
      "epoch": 3990.5882352941176,
      "grad_norm": 16.7607364654541,
      "learning_rate": 1.0094117647058823e-05,
      "loss": 0.6934,
      "step": 67840
    },
    {
      "epoch": 3991.176470588235,
      "grad_norm": 22.535097122192383,
      "learning_rate": 1.0088235294117648e-05,
      "loss": 0.7349,
      "step": 67850
    },
    {
      "epoch": 3991.764705882353,
      "grad_norm": 19.640687942504883,
      "learning_rate": 1.008235294117647e-05,
      "loss": 0.7176,
      "step": 67860
    },
    {
      "epoch": 3992.3529411764707,
      "grad_norm": 21.70176124572754,
      "learning_rate": 1.0076470588235294e-05,
      "loss": 0.6854,
      "step": 67870
    },
    {
      "epoch": 3992.9411764705883,
      "grad_norm": 18.315673828125,
      "learning_rate": 1.0070588235294118e-05,
      "loss": 0.6967,
      "step": 67880
    },
    {
      "epoch": 3993.529411764706,
      "grad_norm": 22.447595596313477,
      "learning_rate": 1.0064705882352941e-05,
      "loss": 0.6951,
      "step": 67890
    },
    {
      "epoch": 3994.1176470588234,
      "grad_norm": 21.721294403076172,
      "learning_rate": 1.0058823529411764e-05,
      "loss": 0.726,
      "step": 67900
    },
    {
      "epoch": 3994.705882352941,
      "grad_norm": 17.346872329711914,
      "learning_rate": 1.0052941176470589e-05,
      "loss": 0.7376,
      "step": 67910
    },
    {
      "epoch": 3995.294117647059,
      "grad_norm": 26.296096801757812,
      "learning_rate": 1.0047058823529412e-05,
      "loss": 0.6356,
      "step": 67920
    },
    {
      "epoch": 3995.8823529411766,
      "grad_norm": 23.40431785583496,
      "learning_rate": 1.0041176470588237e-05,
      "loss": 0.7521,
      "step": 67930
    },
    {
      "epoch": 3996.470588235294,
      "grad_norm": 21.737939834594727,
      "learning_rate": 1.003529411764706e-05,
      "loss": 0.6996,
      "step": 67940
    },
    {
      "epoch": 3997.0588235294117,
      "grad_norm": 17.860240936279297,
      "learning_rate": 1.0029411764705882e-05,
      "loss": 0.6956,
      "step": 67950
    },
    {
      "epoch": 3997.6470588235293,
      "grad_norm": 22.685482025146484,
      "learning_rate": 1.0023529411764707e-05,
      "loss": 0.774,
      "step": 67960
    },
    {
      "epoch": 3998.235294117647,
      "grad_norm": 19.74540138244629,
      "learning_rate": 1.001764705882353e-05,
      "loss": 0.7408,
      "step": 67970
    },
    {
      "epoch": 3998.823529411765,
      "grad_norm": 21.648500442504883,
      "learning_rate": 1.0011764705882353e-05,
      "loss": 0.6104,
      "step": 67980
    },
    {
      "epoch": 3999.4117647058824,
      "grad_norm": 22.386507034301758,
      "learning_rate": 1.0005882352941178e-05,
      "loss": 0.6479,
      "step": 67990
    },
    {
      "epoch": 4000.0,
      "grad_norm": 16.080089569091797,
      "learning_rate": 1e-05,
      "loss": 0.6613,
      "step": 68000
    },
    {
      "epoch": 4000.5882352941176,
      "grad_norm": 26.522823333740234,
      "learning_rate": 9.994117647058824e-06,
      "loss": 0.7679,
      "step": 68010
    },
    {
      "epoch": 4001.176470588235,
      "grad_norm": 27.55204200744629,
      "learning_rate": 9.988235294117647e-06,
      "loss": 0.7609,
      "step": 68020
    },
    {
      "epoch": 4001.764705882353,
      "grad_norm": 21.04703140258789,
      "learning_rate": 9.98235294117647e-06,
      "loss": 0.8059,
      "step": 68030
    },
    {
      "epoch": 4002.3529411764707,
      "grad_norm": 16.620662689208984,
      "learning_rate": 9.976470588235294e-06,
      "loss": 0.6748,
      "step": 68040
    },
    {
      "epoch": 4002.9411764705883,
      "grad_norm": 25.035924911499023,
      "learning_rate": 9.970588235294117e-06,
      "loss": 0.7316,
      "step": 68050
    },
    {
      "epoch": 4003.529411764706,
      "grad_norm": 20.470569610595703,
      "learning_rate": 9.96470588235294e-06,
      "loss": 0.6646,
      "step": 68060
    },
    {
      "epoch": 4004.1176470588234,
      "grad_norm": 25.57818031311035,
      "learning_rate": 9.958823529411765e-06,
      "loss": 0.6378,
      "step": 68070
    },
    {
      "epoch": 4004.705882352941,
      "grad_norm": 19.554058074951172,
      "learning_rate": 9.952941176470588e-06,
      "loss": 0.8046,
      "step": 68080
    },
    {
      "epoch": 4005.294117647059,
      "grad_norm": 26.47772979736328,
      "learning_rate": 9.947058823529413e-06,
      "loss": 0.7459,
      "step": 68090
    },
    {
      "epoch": 4005.8823529411766,
      "grad_norm": 21.363601684570312,
      "learning_rate": 9.941176470588236e-06,
      "loss": 0.7645,
      "step": 68100
    },
    {
      "epoch": 4006.470588235294,
      "grad_norm": 25.89225196838379,
      "learning_rate": 9.935294117647059e-06,
      "loss": 0.7356,
      "step": 68110
    },
    {
      "epoch": 4007.0588235294117,
      "grad_norm": 22.635488510131836,
      "learning_rate": 9.929411764705883e-06,
      "loss": 0.8332,
      "step": 68120
    },
    {
      "epoch": 4007.6470588235293,
      "grad_norm": 20.882726669311523,
      "learning_rate": 9.923529411764706e-06,
      "loss": 0.7096,
      "step": 68130
    },
    {
      "epoch": 4008.235294117647,
      "grad_norm": 27.548057556152344,
      "learning_rate": 9.91764705882353e-06,
      "loss": 0.6954,
      "step": 68140
    },
    {
      "epoch": 4008.823529411765,
      "grad_norm": 20.913766860961914,
      "learning_rate": 9.911764705882354e-06,
      "loss": 0.7956,
      "step": 68150
    },
    {
      "epoch": 4009.4117647058824,
      "grad_norm": 28.795642852783203,
      "learning_rate": 9.905882352941177e-06,
      "loss": 0.6469,
      "step": 68160
    },
    {
      "epoch": 4010.0,
      "grad_norm": 30.76288604736328,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.7692,
      "step": 68170
    },
    {
      "epoch": 4010.5882352941176,
      "grad_norm": 18.896373748779297,
      "learning_rate": 9.894117647058825e-06,
      "loss": 0.7294,
      "step": 68180
    },
    {
      "epoch": 4011.176470588235,
      "grad_norm": 19.14653778076172,
      "learning_rate": 9.888235294117648e-06,
      "loss": 0.6785,
      "step": 68190
    },
    {
      "epoch": 4011.764705882353,
      "grad_norm": 21.43660545349121,
      "learning_rate": 9.88235294117647e-06,
      "loss": 0.7144,
      "step": 68200
    },
    {
      "epoch": 4012.3529411764707,
      "grad_norm": 17.139263153076172,
      "learning_rate": 9.876470588235294e-06,
      "loss": 0.613,
      "step": 68210
    },
    {
      "epoch": 4012.9411764705883,
      "grad_norm": 19.49799156188965,
      "learning_rate": 9.870588235294118e-06,
      "loss": 0.7249,
      "step": 68220
    },
    {
      "epoch": 4013.529411764706,
      "grad_norm": 19.72269058227539,
      "learning_rate": 9.864705882352941e-06,
      "loss": 0.7093,
      "step": 68230
    },
    {
      "epoch": 4014.1176470588234,
      "grad_norm": 25.905105590820312,
      "learning_rate": 9.858823529411764e-06,
      "loss": 0.7164,
      "step": 68240
    },
    {
      "epoch": 4014.705882352941,
      "grad_norm": 14.759604454040527,
      "learning_rate": 9.852941176470589e-06,
      "loss": 0.6611,
      "step": 68250
    },
    {
      "epoch": 4015.294117647059,
      "grad_norm": 21.27250862121582,
      "learning_rate": 9.847058823529412e-06,
      "loss": 0.6695,
      "step": 68260
    },
    {
      "epoch": 4015.8823529411766,
      "grad_norm": 21.684001922607422,
      "learning_rate": 9.841176470588235e-06,
      "loss": 0.7391,
      "step": 68270
    },
    {
      "epoch": 4016.470588235294,
      "grad_norm": 23.66178321838379,
      "learning_rate": 9.83529411764706e-06,
      "loss": 0.6322,
      "step": 68280
    },
    {
      "epoch": 4017.0588235294117,
      "grad_norm": 26.64759063720703,
      "learning_rate": 9.829411764705883e-06,
      "loss": 0.858,
      "step": 68290
    },
    {
      "epoch": 4017.6470588235293,
      "grad_norm": 15.983674049377441,
      "learning_rate": 9.823529411764706e-06,
      "loss": 0.6629,
      "step": 68300
    },
    {
      "epoch": 4018.235294117647,
      "grad_norm": 18.008750915527344,
      "learning_rate": 9.81764705882353e-06,
      "loss": 0.6669,
      "step": 68310
    },
    {
      "epoch": 4018.823529411765,
      "grad_norm": 22.98038101196289,
      "learning_rate": 9.811764705882353e-06,
      "loss": 0.7476,
      "step": 68320
    },
    {
      "epoch": 4019.4117647058824,
      "grad_norm": 18.692350387573242,
      "learning_rate": 9.805882352941178e-06,
      "loss": 0.7646,
      "step": 68330
    },
    {
      "epoch": 4020.0,
      "grad_norm": 24.75939178466797,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.597,
      "step": 68340
    },
    {
      "epoch": 4020.5882352941176,
      "grad_norm": 14.89437484741211,
      "learning_rate": 9.794117647058824e-06,
      "loss": 0.8194,
      "step": 68350
    },
    {
      "epoch": 4021.176470588235,
      "grad_norm": 27.909902572631836,
      "learning_rate": 9.788235294117649e-06,
      "loss": 0.674,
      "step": 68360
    },
    {
      "epoch": 4021.764705882353,
      "grad_norm": 24.789459228515625,
      "learning_rate": 9.782352941176472e-06,
      "loss": 0.6857,
      "step": 68370
    },
    {
      "epoch": 4022.3529411764707,
      "grad_norm": 18.392202377319336,
      "learning_rate": 9.776470588235295e-06,
      "loss": 0.8029,
      "step": 68380
    },
    {
      "epoch": 4022.9411764705883,
      "grad_norm": 17.069293975830078,
      "learning_rate": 9.770588235294118e-06,
      "loss": 0.6599,
      "step": 68390
    },
    {
      "epoch": 4023.529411764706,
      "grad_norm": 23.661033630371094,
      "learning_rate": 9.76470588235294e-06,
      "loss": 0.6342,
      "step": 68400
    },
    {
      "epoch": 4024.1176470588234,
      "grad_norm": 18.660043716430664,
      "learning_rate": 9.758823529411765e-06,
      "loss": 0.7722,
      "step": 68410
    },
    {
      "epoch": 4024.705882352941,
      "grad_norm": 19.017440795898438,
      "learning_rate": 9.752941176470588e-06,
      "loss": 0.7453,
      "step": 68420
    },
    {
      "epoch": 4025.294117647059,
      "grad_norm": 18.253028869628906,
      "learning_rate": 9.747058823529411e-06,
      "loss": 0.7068,
      "step": 68430
    },
    {
      "epoch": 4025.8823529411766,
      "grad_norm": 24.964479446411133,
      "learning_rate": 9.741176470588236e-06,
      "loss": 0.7396,
      "step": 68440
    },
    {
      "epoch": 4026.470588235294,
      "grad_norm": 14.213245391845703,
      "learning_rate": 9.735294117647059e-06,
      "loss": 0.7003,
      "step": 68450
    },
    {
      "epoch": 4027.0588235294117,
      "grad_norm": 24.830913543701172,
      "learning_rate": 9.729411764705882e-06,
      "loss": 0.6804,
      "step": 68460
    },
    {
      "epoch": 4027.6470588235293,
      "grad_norm": 20.513269424438477,
      "learning_rate": 9.723529411764707e-06,
      "loss": 0.6825,
      "step": 68470
    },
    {
      "epoch": 4028.235294117647,
      "grad_norm": 14.70068645477295,
      "learning_rate": 9.71764705882353e-06,
      "loss": 0.7654,
      "step": 68480
    },
    {
      "epoch": 4028.823529411765,
      "grad_norm": 22.44984245300293,
      "learning_rate": 9.711764705882354e-06,
      "loss": 0.7954,
      "step": 68490
    },
    {
      "epoch": 4029.4117647058824,
      "grad_norm": 30.164817810058594,
      "learning_rate": 9.705882352941177e-06,
      "loss": 0.865,
      "step": 68500
    },
    {
      "epoch": 4030.0,
      "grad_norm": 25.66921615600586,
      "learning_rate": 9.7e-06,
      "loss": 0.7107,
      "step": 68510
    },
    {
      "epoch": 4030.5882352941176,
      "grad_norm": 15.536648750305176,
      "learning_rate": 9.694117647058825e-06,
      "loss": 0.7534,
      "step": 68520
    },
    {
      "epoch": 4031.176470588235,
      "grad_norm": 19.643566131591797,
      "learning_rate": 9.688235294117648e-06,
      "loss": 0.6482,
      "step": 68530
    },
    {
      "epoch": 4031.764705882353,
      "grad_norm": 22.260732650756836,
      "learning_rate": 9.682352941176471e-06,
      "loss": 0.7107,
      "step": 68540
    },
    {
      "epoch": 4032.3529411764707,
      "grad_norm": 21.117647171020508,
      "learning_rate": 9.676470588235296e-06,
      "loss": 0.6506,
      "step": 68550
    },
    {
      "epoch": 4032.9411764705883,
      "grad_norm": 15.837964057922363,
      "learning_rate": 9.670588235294119e-06,
      "loss": 0.6635,
      "step": 68560
    },
    {
      "epoch": 4033.529411764706,
      "grad_norm": 27.51644515991211,
      "learning_rate": 9.664705882352942e-06,
      "loss": 0.7846,
      "step": 68570
    },
    {
      "epoch": 4034.1176470588234,
      "grad_norm": 16.643869400024414,
      "learning_rate": 9.658823529411765e-06,
      "loss": 0.7467,
      "step": 68580
    },
    {
      "epoch": 4034.705882352941,
      "grad_norm": 29.2122745513916,
      "learning_rate": 9.652941176470588e-06,
      "loss": 0.7029,
      "step": 68590
    },
    {
      "epoch": 4035.294117647059,
      "grad_norm": 24.832847595214844,
      "learning_rate": 9.647058823529412e-06,
      "loss": 0.6818,
      "step": 68600
    },
    {
      "epoch": 4035.8823529411766,
      "grad_norm": 25.939184188842773,
      "learning_rate": 9.641176470588235e-06,
      "loss": 0.6663,
      "step": 68610
    },
    {
      "epoch": 4036.470588235294,
      "grad_norm": 13.924416542053223,
      "learning_rate": 9.635294117647058e-06,
      "loss": 0.733,
      "step": 68620
    },
    {
      "epoch": 4037.0588235294117,
      "grad_norm": 18.522953033447266,
      "learning_rate": 9.629411764705883e-06,
      "loss": 0.6985,
      "step": 68630
    },
    {
      "epoch": 4037.6470588235293,
      "grad_norm": 16.03609848022461,
      "learning_rate": 9.623529411764706e-06,
      "loss": 0.6485,
      "step": 68640
    },
    {
      "epoch": 4038.235294117647,
      "grad_norm": 19.45688819885254,
      "learning_rate": 9.61764705882353e-06,
      "loss": 0.6699,
      "step": 68650
    },
    {
      "epoch": 4038.823529411765,
      "grad_norm": 20.59441566467285,
      "learning_rate": 9.611764705882354e-06,
      "loss": 0.7295,
      "step": 68660
    },
    {
      "epoch": 4039.4117647058824,
      "grad_norm": 20.785078048706055,
      "learning_rate": 9.605882352941177e-06,
      "loss": 0.7278,
      "step": 68670
    },
    {
      "epoch": 4040.0,
      "grad_norm": 20.535472869873047,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.6869,
      "step": 68680
    },
    {
      "epoch": 4040.5882352941176,
      "grad_norm": 16.413427352905273,
      "learning_rate": 9.594117647058824e-06,
      "loss": 0.6788,
      "step": 68690
    },
    {
      "epoch": 4041.176470588235,
      "grad_norm": 13.893475532531738,
      "learning_rate": 9.588235294117647e-06,
      "loss": 0.7798,
      "step": 68700
    },
    {
      "epoch": 4041.764705882353,
      "grad_norm": 22.007890701293945,
      "learning_rate": 9.582352941176472e-06,
      "loss": 0.6822,
      "step": 68710
    },
    {
      "epoch": 4042.3529411764707,
      "grad_norm": 23.406902313232422,
      "learning_rate": 9.576470588235295e-06,
      "loss": 0.7981,
      "step": 68720
    },
    {
      "epoch": 4042.9411764705883,
      "grad_norm": 17.17794418334961,
      "learning_rate": 9.570588235294118e-06,
      "loss": 0.7348,
      "step": 68730
    },
    {
      "epoch": 4043.529411764706,
      "grad_norm": 17.891246795654297,
      "learning_rate": 9.564705882352943e-06,
      "loss": 0.6812,
      "step": 68740
    },
    {
      "epoch": 4044.1176470588234,
      "grad_norm": 20.694934844970703,
      "learning_rate": 9.558823529411764e-06,
      "loss": 0.6975,
      "step": 68750
    },
    {
      "epoch": 4044.705882352941,
      "grad_norm": 18.293354034423828,
      "learning_rate": 9.552941176470589e-06,
      "loss": 0.7394,
      "step": 68760
    },
    {
      "epoch": 4045.294117647059,
      "grad_norm": 19.53188133239746,
      "learning_rate": 9.547058823529412e-06,
      "loss": 0.6449,
      "step": 68770
    },
    {
      "epoch": 4045.8823529411766,
      "grad_norm": 20.074466705322266,
      "learning_rate": 9.541176470588235e-06,
      "loss": 0.6284,
      "step": 68780
    },
    {
      "epoch": 4046.470588235294,
      "grad_norm": 23.865564346313477,
      "learning_rate": 9.53529411764706e-06,
      "loss": 0.7018,
      "step": 68790
    },
    {
      "epoch": 4047.0588235294117,
      "grad_norm": 13.173876762390137,
      "learning_rate": 9.529411764705882e-06,
      "loss": 0.6014,
      "step": 68800
    },
    {
      "epoch": 4047.6470588235293,
      "grad_norm": 16.04241371154785,
      "learning_rate": 9.523529411764707e-06,
      "loss": 0.6813,
      "step": 68810
    },
    {
      "epoch": 4048.235294117647,
      "grad_norm": 24.49127960205078,
      "learning_rate": 9.51764705882353e-06,
      "loss": 0.7061,
      "step": 68820
    },
    {
      "epoch": 4048.823529411765,
      "grad_norm": 18.759004592895508,
      "learning_rate": 9.511764705882353e-06,
      "loss": 0.7368,
      "step": 68830
    },
    {
      "epoch": 4049.4117647058824,
      "grad_norm": 18.1121768951416,
      "learning_rate": 9.505882352941178e-06,
      "loss": 0.6777,
      "step": 68840
    },
    {
      "epoch": 4050.0,
      "grad_norm": 27.937978744506836,
      "learning_rate": 9.5e-06,
      "loss": 0.6702,
      "step": 68850
    },
    {
      "epoch": 4050.5882352941176,
      "grad_norm": 25.09351348876953,
      "learning_rate": 9.494117647058824e-06,
      "loss": 0.6747,
      "step": 68860
    },
    {
      "epoch": 4051.176470588235,
      "grad_norm": 18.51099967956543,
      "learning_rate": 9.488235294117648e-06,
      "loss": 0.6836,
      "step": 68870
    },
    {
      "epoch": 4051.764705882353,
      "grad_norm": 25.91522979736328,
      "learning_rate": 9.482352941176471e-06,
      "loss": 0.6636,
      "step": 68880
    },
    {
      "epoch": 4052.3529411764707,
      "grad_norm": 24.362876892089844,
      "learning_rate": 9.476470588235294e-06,
      "loss": 0.6783,
      "step": 68890
    },
    {
      "epoch": 4052.9411764705883,
      "grad_norm": 19.386995315551758,
      "learning_rate": 9.470588235294119e-06,
      "loss": 0.6782,
      "step": 68900
    },
    {
      "epoch": 4053.529411764706,
      "grad_norm": 20.36835289001465,
      "learning_rate": 9.464705882352942e-06,
      "loss": 0.7275,
      "step": 68910
    },
    {
      "epoch": 4054.1176470588234,
      "grad_norm": 22.791784286499023,
      "learning_rate": 9.458823529411767e-06,
      "loss": 0.6431,
      "step": 68920
    },
    {
      "epoch": 4054.705882352941,
      "grad_norm": 21.858823776245117,
      "learning_rate": 9.452941176470588e-06,
      "loss": 0.8127,
      "step": 68930
    },
    {
      "epoch": 4055.294117647059,
      "grad_norm": 22.046545028686523,
      "learning_rate": 9.44705882352941e-06,
      "loss": 0.6621,
      "step": 68940
    },
    {
      "epoch": 4055.8823529411766,
      "grad_norm": 18.19107437133789,
      "learning_rate": 9.441176470588235e-06,
      "loss": 0.7566,
      "step": 68950
    },
    {
      "epoch": 4056.470588235294,
      "grad_norm": 25.52054786682129,
      "learning_rate": 9.435294117647058e-06,
      "loss": 0.8316,
      "step": 68960
    },
    {
      "epoch": 4057.0588235294117,
      "grad_norm": 16.87027931213379,
      "learning_rate": 9.429411764705883e-06,
      "loss": 0.7105,
      "step": 68970
    },
    {
      "epoch": 4057.6470588235293,
      "grad_norm": 27.685285568237305,
      "learning_rate": 9.423529411764706e-06,
      "loss": 0.7456,
      "step": 68980
    },
    {
      "epoch": 4058.235294117647,
      "grad_norm": 12.988039016723633,
      "learning_rate": 9.417647058823529e-06,
      "loss": 0.7137,
      "step": 68990
    },
    {
      "epoch": 4058.823529411765,
      "grad_norm": 16.060415267944336,
      "learning_rate": 9.411764705882354e-06,
      "loss": 0.7567,
      "step": 69000
    },
    {
      "epoch": 4059.4117647058824,
      "grad_norm": 23.4769344329834,
      "learning_rate": 9.405882352941177e-06,
      "loss": 0.6786,
      "step": 69010
    },
    {
      "epoch": 4060.0,
      "grad_norm": 28.810075759887695,
      "learning_rate": 9.4e-06,
      "loss": 0.7106,
      "step": 69020
    },
    {
      "epoch": 4060.5882352941176,
      "grad_norm": 19.53105354309082,
      "learning_rate": 9.394117647058824e-06,
      "loss": 0.6683,
      "step": 69030
    },
    {
      "epoch": 4061.176470588235,
      "grad_norm": 23.791566848754883,
      "learning_rate": 9.388235294117647e-06,
      "loss": 0.6685,
      "step": 69040
    },
    {
      "epoch": 4061.764705882353,
      "grad_norm": 19.91838264465332,
      "learning_rate": 9.38235294117647e-06,
      "loss": 0.6589,
      "step": 69050
    },
    {
      "epoch": 4062.3529411764707,
      "grad_norm": 26.051010131835938,
      "learning_rate": 9.376470588235295e-06,
      "loss": 0.7154,
      "step": 69060
    },
    {
      "epoch": 4062.9411764705883,
      "grad_norm": 27.041833877563477,
      "learning_rate": 9.370588235294118e-06,
      "loss": 0.705,
      "step": 69070
    },
    {
      "epoch": 4063.529411764706,
      "grad_norm": 15.660883903503418,
      "learning_rate": 9.364705882352943e-06,
      "loss": 0.6754,
      "step": 69080
    },
    {
      "epoch": 4064.1176470588234,
      "grad_norm": 15.338528633117676,
      "learning_rate": 9.358823529411766e-06,
      "loss": 0.7101,
      "step": 69090
    },
    {
      "epoch": 4064.705882352941,
      "grad_norm": 24.467140197753906,
      "learning_rate": 9.352941176470589e-06,
      "loss": 0.7775,
      "step": 69100
    },
    {
      "epoch": 4065.294117647059,
      "grad_norm": 23.65118408203125,
      "learning_rate": 9.347058823529412e-06,
      "loss": 0.7247,
      "step": 69110
    },
    {
      "epoch": 4065.8823529411766,
      "grad_norm": 23.74659538269043,
      "learning_rate": 9.341176470588235e-06,
      "loss": 0.6991,
      "step": 69120
    },
    {
      "epoch": 4066.470588235294,
      "grad_norm": 27.50457000732422,
      "learning_rate": 9.33529411764706e-06,
      "loss": 0.6891,
      "step": 69130
    },
    {
      "epoch": 4067.0588235294117,
      "grad_norm": 21.377904891967773,
      "learning_rate": 9.329411764705882e-06,
      "loss": 0.6617,
      "step": 69140
    },
    {
      "epoch": 4067.6470588235293,
      "grad_norm": 22.213102340698242,
      "learning_rate": 9.323529411764705e-06,
      "loss": 0.74,
      "step": 69150
    },
    {
      "epoch": 4068.235294117647,
      "grad_norm": 20.477312088012695,
      "learning_rate": 9.31764705882353e-06,
      "loss": 0.7933,
      "step": 69160
    },
    {
      "epoch": 4068.823529411765,
      "grad_norm": 19.19050407409668,
      "learning_rate": 9.311764705882353e-06,
      "loss": 0.861,
      "step": 69170
    },
    {
      "epoch": 4069.4117647058824,
      "grad_norm": 12.33918285369873,
      "learning_rate": 9.305882352941176e-06,
      "loss": 0.6828,
      "step": 69180
    },
    {
      "epoch": 4070.0,
      "grad_norm": 25.5314884185791,
      "learning_rate": 9.3e-06,
      "loss": 0.7496,
      "step": 69190
    },
    {
      "epoch": 4070.5882352941176,
      "grad_norm": 26.964309692382812,
      "learning_rate": 9.294117647058824e-06,
      "loss": 0.6827,
      "step": 69200
    },
    {
      "epoch": 4071.176470588235,
      "grad_norm": 24.814037322998047,
      "learning_rate": 9.288235294117648e-06,
      "loss": 0.7022,
      "step": 69210
    },
    {
      "epoch": 4071.764705882353,
      "grad_norm": 18.774181365966797,
      "learning_rate": 9.282352941176471e-06,
      "loss": 0.6737,
      "step": 69220
    },
    {
      "epoch": 4072.3529411764707,
      "grad_norm": 25.432788848876953,
      "learning_rate": 9.276470588235294e-06,
      "loss": 0.6614,
      "step": 69230
    },
    {
      "epoch": 4072.9411764705883,
      "grad_norm": 16.30672836303711,
      "learning_rate": 9.270588235294119e-06,
      "loss": 0.7799,
      "step": 69240
    },
    {
      "epoch": 4073.529411764706,
      "grad_norm": 23.07886505126953,
      "learning_rate": 9.264705882352942e-06,
      "loss": 0.6336,
      "step": 69250
    },
    {
      "epoch": 4074.1176470588234,
      "grad_norm": 28.19605827331543,
      "learning_rate": 9.258823529411765e-06,
      "loss": 0.6871,
      "step": 69260
    },
    {
      "epoch": 4074.705882352941,
      "grad_norm": 18.82128143310547,
      "learning_rate": 9.25294117647059e-06,
      "loss": 0.6776,
      "step": 69270
    },
    {
      "epoch": 4075.294117647059,
      "grad_norm": 16.991363525390625,
      "learning_rate": 9.247058823529413e-06,
      "loss": 0.5873,
      "step": 69280
    },
    {
      "epoch": 4075.8823529411766,
      "grad_norm": 18.999544143676758,
      "learning_rate": 9.241176470588236e-06,
      "loss": 0.8048,
      "step": 69290
    },
    {
      "epoch": 4076.470588235294,
      "grad_norm": 16.070222854614258,
      "learning_rate": 9.235294117647059e-06,
      "loss": 0.7197,
      "step": 69300
    },
    {
      "epoch": 4077.0588235294117,
      "grad_norm": 23.302188873291016,
      "learning_rate": 9.229411764705882e-06,
      "loss": 0.7127,
      "step": 69310
    },
    {
      "epoch": 4077.6470588235293,
      "grad_norm": 17.79323959350586,
      "learning_rate": 9.223529411764706e-06,
      "loss": 0.6179,
      "step": 69320
    },
    {
      "epoch": 4078.235294117647,
      "grad_norm": 22.429533004760742,
      "learning_rate": 9.21764705882353e-06,
      "loss": 0.7256,
      "step": 69330
    },
    {
      "epoch": 4078.823529411765,
      "grad_norm": 25.575828552246094,
      "learning_rate": 9.211764705882352e-06,
      "loss": 0.7667,
      "step": 69340
    },
    {
      "epoch": 4079.4117647058824,
      "grad_norm": 20.627227783203125,
      "learning_rate": 9.205882352941177e-06,
      "loss": 0.6718,
      "step": 69350
    },
    {
      "epoch": 4080.0,
      "grad_norm": 21.022993087768555,
      "learning_rate": 9.2e-06,
      "loss": 0.7181,
      "step": 69360
    },
    {
      "epoch": 4080.5882352941176,
      "grad_norm": 15.107136726379395,
      "learning_rate": 9.194117647058825e-06,
      "loss": 0.7539,
      "step": 69370
    },
    {
      "epoch": 4081.176470588235,
      "grad_norm": 25.07815933227539,
      "learning_rate": 9.188235294117648e-06,
      "loss": 0.6833,
      "step": 69380
    },
    {
      "epoch": 4081.764705882353,
      "grad_norm": 19.384143829345703,
      "learning_rate": 9.18235294117647e-06,
      "loss": 0.7094,
      "step": 69390
    },
    {
      "epoch": 4082.3529411764707,
      "grad_norm": 30.383081436157227,
      "learning_rate": 9.176470588235295e-06,
      "loss": 0.6515,
      "step": 69400
    },
    {
      "epoch": 4082.9411764705883,
      "grad_norm": 21.637868881225586,
      "learning_rate": 9.170588235294118e-06,
      "loss": 0.8322,
      "step": 69410
    },
    {
      "epoch": 4083.529411764706,
      "grad_norm": 16.68578338623047,
      "learning_rate": 9.164705882352941e-06,
      "loss": 0.6981,
      "step": 69420
    },
    {
      "epoch": 4084.1176470588234,
      "grad_norm": 17.103445053100586,
      "learning_rate": 9.158823529411766e-06,
      "loss": 0.681,
      "step": 69430
    },
    {
      "epoch": 4084.705882352941,
      "grad_norm": 24.635095596313477,
      "learning_rate": 9.152941176470589e-06,
      "loss": 0.8025,
      "step": 69440
    },
    {
      "epoch": 4085.294117647059,
      "grad_norm": 17.954557418823242,
      "learning_rate": 9.147058823529412e-06,
      "loss": 0.6661,
      "step": 69450
    },
    {
      "epoch": 4085.8823529411766,
      "grad_norm": 19.55298614501953,
      "learning_rate": 9.141176470588237e-06,
      "loss": 0.7269,
      "step": 69460
    },
    {
      "epoch": 4086.470588235294,
      "grad_norm": 16.71798324584961,
      "learning_rate": 9.13529411764706e-06,
      "loss": 0.6155,
      "step": 69470
    },
    {
      "epoch": 4087.0588235294117,
      "grad_norm": 19.360742568969727,
      "learning_rate": 9.129411764705883e-06,
      "loss": 0.6137,
      "step": 69480
    },
    {
      "epoch": 4087.6470588235293,
      "grad_norm": 17.93509292602539,
      "learning_rate": 9.123529411764706e-06,
      "loss": 0.6478,
      "step": 69490
    },
    {
      "epoch": 4088.235294117647,
      "grad_norm": 17.175559997558594,
      "learning_rate": 9.117647058823529e-06,
      "loss": 0.7715,
      "step": 69500
    },
    {
      "epoch": 4088.823529411765,
      "grad_norm": 11.311761856079102,
      "learning_rate": 9.111764705882353e-06,
      "loss": 0.5581,
      "step": 69510
    },
    {
      "epoch": 4089.4117647058824,
      "grad_norm": 23.487619400024414,
      "learning_rate": 9.105882352941176e-06,
      "loss": 0.7081,
      "step": 69520
    },
    {
      "epoch": 4090.0,
      "grad_norm": 26.09476089477539,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.7643,
      "step": 69530
    },
    {
      "epoch": 4090.5882352941176,
      "grad_norm": 18.744752883911133,
      "learning_rate": 9.094117647058824e-06,
      "loss": 0.7191,
      "step": 69540
    },
    {
      "epoch": 4091.176470588235,
      "grad_norm": 23.820158004760742,
      "learning_rate": 9.088235294117647e-06,
      "loss": 0.753,
      "step": 69550
    },
    {
      "epoch": 4091.764705882353,
      "grad_norm": 19.047304153442383,
      "learning_rate": 9.082352941176472e-06,
      "loss": 0.6968,
      "step": 69560
    },
    {
      "epoch": 4092.3529411764707,
      "grad_norm": 26.54572868347168,
      "learning_rate": 9.076470588235295e-06,
      "loss": 0.729,
      "step": 69570
    },
    {
      "epoch": 4092.9411764705883,
      "grad_norm": 22.225048065185547,
      "learning_rate": 9.070588235294118e-06,
      "loss": 0.6838,
      "step": 69580
    },
    {
      "epoch": 4093.529411764706,
      "grad_norm": 22.364303588867188,
      "learning_rate": 9.064705882352942e-06,
      "loss": 0.7223,
      "step": 69590
    },
    {
      "epoch": 4094.1176470588234,
      "grad_norm": 21.32742691040039,
      "learning_rate": 9.058823529411765e-06,
      "loss": 0.7346,
      "step": 69600
    },
    {
      "epoch": 4094.705882352941,
      "grad_norm": 20.025501251220703,
      "learning_rate": 9.052941176470588e-06,
      "loss": 0.6905,
      "step": 69610
    },
    {
      "epoch": 4095.294117647059,
      "grad_norm": 22.953460693359375,
      "learning_rate": 9.047058823529413e-06,
      "loss": 0.6483,
      "step": 69620
    },
    {
      "epoch": 4095.8823529411766,
      "grad_norm": 24.527687072753906,
      "learning_rate": 9.041176470588236e-06,
      "loss": 0.6582,
      "step": 69630
    },
    {
      "epoch": 4096.470588235294,
      "grad_norm": 20.978548049926758,
      "learning_rate": 9.03529411764706e-06,
      "loss": 0.7244,
      "step": 69640
    },
    {
      "epoch": 4097.058823529412,
      "grad_norm": 22.328632354736328,
      "learning_rate": 9.029411764705884e-06,
      "loss": 0.6378,
      "step": 69650
    },
    {
      "epoch": 4097.64705882353,
      "grad_norm": 23.244441986083984,
      "learning_rate": 9.023529411764705e-06,
      "loss": 0.6576,
      "step": 69660
    },
    {
      "epoch": 4098.235294117647,
      "grad_norm": 19.50588035583496,
      "learning_rate": 9.01764705882353e-06,
      "loss": 0.8415,
      "step": 69670
    },
    {
      "epoch": 4098.823529411765,
      "grad_norm": 25.301013946533203,
      "learning_rate": 9.011764705882353e-06,
      "loss": 0.7435,
      "step": 69680
    },
    {
      "epoch": 4099.411764705882,
      "grad_norm": 19.45838737487793,
      "learning_rate": 9.005882352941177e-06,
      "loss": 0.6147,
      "step": 69690
    },
    {
      "epoch": 4100.0,
      "grad_norm": 23.53481674194336,
      "learning_rate": 9e-06,
      "loss": 0.6626,
      "step": 69700
    },
    {
      "epoch": 4100.588235294118,
      "grad_norm": 21.1882381439209,
      "learning_rate": 8.994117647058823e-06,
      "loss": 0.7114,
      "step": 69710
    },
    {
      "epoch": 4101.176470588235,
      "grad_norm": 15.95492935180664,
      "learning_rate": 8.988235294117648e-06,
      "loss": 0.6956,
      "step": 69720
    },
    {
      "epoch": 4101.764705882353,
      "grad_norm": 23.143857955932617,
      "learning_rate": 8.982352941176471e-06,
      "loss": 0.6757,
      "step": 69730
    },
    {
      "epoch": 4102.35294117647,
      "grad_norm": 19.493175506591797,
      "learning_rate": 8.976470588235294e-06,
      "loss": 0.7027,
      "step": 69740
    },
    {
      "epoch": 4102.941176470588,
      "grad_norm": 22.020421981811523,
      "learning_rate": 8.970588235294119e-06,
      "loss": 0.7415,
      "step": 69750
    },
    {
      "epoch": 4103.529411764706,
      "grad_norm": 21.09626579284668,
      "learning_rate": 8.964705882352942e-06,
      "loss": 0.7282,
      "step": 69760
    },
    {
      "epoch": 4104.117647058823,
      "grad_norm": 18.21415138244629,
      "learning_rate": 8.958823529411765e-06,
      "loss": 0.7118,
      "step": 69770
    },
    {
      "epoch": 4104.705882352941,
      "grad_norm": 19.443456649780273,
      "learning_rate": 8.95294117647059e-06,
      "loss": 0.6573,
      "step": 69780
    },
    {
      "epoch": 4105.294117647059,
      "grad_norm": 25.013078689575195,
      "learning_rate": 8.947058823529412e-06,
      "loss": 0.7608,
      "step": 69790
    },
    {
      "epoch": 4105.882352941177,
      "grad_norm": 24.580814361572266,
      "learning_rate": 8.941176470588237e-06,
      "loss": 0.7373,
      "step": 69800
    },
    {
      "epoch": 4106.470588235294,
      "grad_norm": 16.232175827026367,
      "learning_rate": 8.93529411764706e-06,
      "loss": 0.7308,
      "step": 69810
    },
    {
      "epoch": 4107.058823529412,
      "grad_norm": 26.21803855895996,
      "learning_rate": 8.929411764705883e-06,
      "loss": 0.7155,
      "step": 69820
    },
    {
      "epoch": 4107.64705882353,
      "grad_norm": 19.1959228515625,
      "learning_rate": 8.923529411764708e-06,
      "loss": 0.6635,
      "step": 69830
    },
    {
      "epoch": 4108.235294117647,
      "grad_norm": 16.596147537231445,
      "learning_rate": 8.917647058823529e-06,
      "loss": 0.6016,
      "step": 69840
    },
    {
      "epoch": 4108.823529411765,
      "grad_norm": 27.63397789001465,
      "learning_rate": 8.911764705882354e-06,
      "loss": 0.7654,
      "step": 69850
    },
    {
      "epoch": 4109.411764705882,
      "grad_norm": 22.472614288330078,
      "learning_rate": 8.905882352941177e-06,
      "loss": 0.7451,
      "step": 69860
    },
    {
      "epoch": 4110.0,
      "grad_norm": 18.609983444213867,
      "learning_rate": 8.9e-06,
      "loss": 0.6864,
      "step": 69870
    },
    {
      "epoch": 4110.588235294118,
      "grad_norm": 23.18146514892578,
      "learning_rate": 8.894117647058824e-06,
      "loss": 0.6423,
      "step": 69880
    },
    {
      "epoch": 4111.176470588235,
      "grad_norm": 19.74856948852539,
      "learning_rate": 8.888235294117647e-06,
      "loss": 0.6183,
      "step": 69890
    },
    {
      "epoch": 4111.764705882353,
      "grad_norm": 22.093660354614258,
      "learning_rate": 8.88235294117647e-06,
      "loss": 0.8086,
      "step": 69900
    },
    {
      "epoch": 4112.35294117647,
      "grad_norm": 22.026893615722656,
      "learning_rate": 8.876470588235295e-06,
      "loss": 0.7151,
      "step": 69910
    },
    {
      "epoch": 4112.941176470588,
      "grad_norm": 17.601211547851562,
      "learning_rate": 8.870588235294118e-06,
      "loss": 0.7403,
      "step": 69920
    },
    {
      "epoch": 4113.529411764706,
      "grad_norm": 15.255900382995605,
      "learning_rate": 8.86470588235294e-06,
      "loss": 0.7046,
      "step": 69930
    },
    {
      "epoch": 4114.117647058823,
      "grad_norm": 17.568941116333008,
      "learning_rate": 8.858823529411765e-06,
      "loss": 0.683,
      "step": 69940
    },
    {
      "epoch": 4114.705882352941,
      "grad_norm": 18.08374786376953,
      "learning_rate": 8.852941176470588e-06,
      "loss": 0.7557,
      "step": 69950
    },
    {
      "epoch": 4115.294117647059,
      "grad_norm": 21.550024032592773,
      "learning_rate": 8.847058823529413e-06,
      "loss": 0.668,
      "step": 69960
    },
    {
      "epoch": 4115.882352941177,
      "grad_norm": 20.248720169067383,
      "learning_rate": 8.841176470588236e-06,
      "loss": 0.6866,
      "step": 69970
    },
    {
      "epoch": 4116.470588235294,
      "grad_norm": 17.222591400146484,
      "learning_rate": 8.835294117647059e-06,
      "loss": 0.5775,
      "step": 69980
    },
    {
      "epoch": 4117.058823529412,
      "grad_norm": 28.714927673339844,
      "learning_rate": 8.829411764705884e-06,
      "loss": 0.7565,
      "step": 69990
    },
    {
      "epoch": 4117.64705882353,
      "grad_norm": 17.444713592529297,
      "learning_rate": 8.823529411764707e-06,
      "loss": 0.6762,
      "step": 70000
    },
    {
      "epoch": 4118.235294117647,
      "grad_norm": 25.18830108642578,
      "learning_rate": 8.81764705882353e-06,
      "loss": 0.6953,
      "step": 70010
    },
    {
      "epoch": 4118.823529411765,
      "grad_norm": 25.328460693359375,
      "learning_rate": 8.811764705882353e-06,
      "loss": 0.7058,
      "step": 70020
    },
    {
      "epoch": 4119.411764705882,
      "grad_norm": 20.635093688964844,
      "learning_rate": 8.805882352941176e-06,
      "loss": 0.7557,
      "step": 70030
    },
    {
      "epoch": 4120.0,
      "grad_norm": 30.45897102355957,
      "learning_rate": 8.8e-06,
      "loss": 0.6318,
      "step": 70040
    },
    {
      "epoch": 4120.588235294118,
      "grad_norm": 23.453937530517578,
      "learning_rate": 8.794117647058823e-06,
      "loss": 0.7242,
      "step": 70050
    },
    {
      "epoch": 4121.176470588235,
      "grad_norm": 19.625476837158203,
      "learning_rate": 8.788235294117646e-06,
      "loss": 0.6596,
      "step": 70060
    },
    {
      "epoch": 4121.764705882353,
      "grad_norm": 20.266820907592773,
      "learning_rate": 8.782352941176471e-06,
      "loss": 0.6635,
      "step": 70070
    },
    {
      "epoch": 4122.35294117647,
      "grad_norm": 25.44997215270996,
      "learning_rate": 8.776470588235294e-06,
      "loss": 0.7545,
      "step": 70080
    },
    {
      "epoch": 4122.941176470588,
      "grad_norm": 20.394161224365234,
      "learning_rate": 8.770588235294119e-06,
      "loss": 0.7202,
      "step": 70090
    },
    {
      "epoch": 4123.529411764706,
      "grad_norm": 22.026412963867188,
      "learning_rate": 8.764705882352942e-06,
      "loss": 0.6061,
      "step": 70100
    },
    {
      "epoch": 4124.117647058823,
      "grad_norm": 22.053783416748047,
      "learning_rate": 8.758823529411765e-06,
      "loss": 0.7054,
      "step": 70110
    },
    {
      "epoch": 4124.705882352941,
      "grad_norm": 23.2063045501709,
      "learning_rate": 8.75294117647059e-06,
      "loss": 0.593,
      "step": 70120
    },
    {
      "epoch": 4125.294117647059,
      "grad_norm": 20.490318298339844,
      "learning_rate": 8.747058823529412e-06,
      "loss": 0.7212,
      "step": 70130
    },
    {
      "epoch": 4125.882352941177,
      "grad_norm": 15.504643440246582,
      "learning_rate": 8.741176470588235e-06,
      "loss": 0.7498,
      "step": 70140
    },
    {
      "epoch": 4126.470588235294,
      "grad_norm": 19.285419464111328,
      "learning_rate": 8.73529411764706e-06,
      "loss": 0.6738,
      "step": 70150
    },
    {
      "epoch": 4127.058823529412,
      "grad_norm": 20.697368621826172,
      "learning_rate": 8.729411764705883e-06,
      "loss": 0.6991,
      "step": 70160
    },
    {
      "epoch": 4127.64705882353,
      "grad_norm": 24.16704559326172,
      "learning_rate": 8.723529411764706e-06,
      "loss": 0.7632,
      "step": 70170
    },
    {
      "epoch": 4128.235294117647,
      "grad_norm": 17.755918502807617,
      "learning_rate": 8.71764705882353e-06,
      "loss": 0.6036,
      "step": 70180
    },
    {
      "epoch": 4128.823529411765,
      "grad_norm": 20.34183692932129,
      "learning_rate": 8.711764705882354e-06,
      "loss": 0.7351,
      "step": 70190
    },
    {
      "epoch": 4129.411764705882,
      "grad_norm": 22.525257110595703,
      "learning_rate": 8.705882352941177e-06,
      "loss": 0.7932,
      "step": 70200
    },
    {
      "epoch": 4130.0,
      "grad_norm": 28.811492919921875,
      "learning_rate": 8.7e-06,
      "loss": 0.7791,
      "step": 70210
    },
    {
      "epoch": 4130.588235294118,
      "grad_norm": 20.05706787109375,
      "learning_rate": 8.694117647058823e-06,
      "loss": 0.7231,
      "step": 70220
    },
    {
      "epoch": 4131.176470588235,
      "grad_norm": 19.89406394958496,
      "learning_rate": 8.688235294117647e-06,
      "loss": 0.6166,
      "step": 70230
    },
    {
      "epoch": 4131.764705882353,
      "grad_norm": 16.141721725463867,
      "learning_rate": 8.68235294117647e-06,
      "loss": 0.6944,
      "step": 70240
    },
    {
      "epoch": 4132.35294117647,
      "grad_norm": 25.466352462768555,
      "learning_rate": 8.676470588235295e-06,
      "loss": 0.7274,
      "step": 70250
    },
    {
      "epoch": 4132.941176470588,
      "grad_norm": 21.987598419189453,
      "learning_rate": 8.670588235294118e-06,
      "loss": 0.6369,
      "step": 70260
    },
    {
      "epoch": 4133.529411764706,
      "grad_norm": 16.298974990844727,
      "learning_rate": 8.664705882352941e-06,
      "loss": 0.7078,
      "step": 70270
    },
    {
      "epoch": 4134.117647058823,
      "grad_norm": 18.310157775878906,
      "learning_rate": 8.658823529411766e-06,
      "loss": 0.7023,
      "step": 70280
    },
    {
      "epoch": 4134.705882352941,
      "grad_norm": 19.406763076782227,
      "learning_rate": 8.652941176470589e-06,
      "loss": 0.7314,
      "step": 70290
    },
    {
      "epoch": 4135.294117647059,
      "grad_norm": 17.681232452392578,
      "learning_rate": 8.647058823529412e-06,
      "loss": 0.693,
      "step": 70300
    },
    {
      "epoch": 4135.882352941177,
      "grad_norm": 15.873333930969238,
      "learning_rate": 8.641176470588236e-06,
      "loss": 0.6146,
      "step": 70310
    },
    {
      "epoch": 4136.470588235294,
      "grad_norm": 21.429738998413086,
      "learning_rate": 8.63529411764706e-06,
      "loss": 0.6964,
      "step": 70320
    },
    {
      "epoch": 4137.058823529412,
      "grad_norm": 25.25082778930664,
      "learning_rate": 8.629411764705882e-06,
      "loss": 0.7614,
      "step": 70330
    },
    {
      "epoch": 4137.64705882353,
      "grad_norm": 23.113201141357422,
      "learning_rate": 8.623529411764707e-06,
      "loss": 0.7138,
      "step": 70340
    },
    {
      "epoch": 4138.235294117647,
      "grad_norm": 16.69923973083496,
      "learning_rate": 8.61764705882353e-06,
      "loss": 0.6931,
      "step": 70350
    },
    {
      "epoch": 4138.823529411765,
      "grad_norm": 22.651836395263672,
      "learning_rate": 8.611764705882355e-06,
      "loss": 0.7479,
      "step": 70360
    },
    {
      "epoch": 4139.411764705882,
      "grad_norm": 24.847530364990234,
      "learning_rate": 8.605882352941178e-06,
      "loss": 0.7063,
      "step": 70370
    },
    {
      "epoch": 4140.0,
      "grad_norm": 25.422760009765625,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.7736,
      "step": 70380
    },
    {
      "epoch": 4140.588235294118,
      "grad_norm": 19.46260643005371,
      "learning_rate": 8.594117647058824e-06,
      "loss": 0.766,
      "step": 70390
    },
    {
      "epoch": 4141.176470588235,
      "grad_norm": 13.815185546875,
      "learning_rate": 8.588235294117647e-06,
      "loss": 0.6759,
      "step": 70400
    },
    {
      "epoch": 4141.764705882353,
      "grad_norm": 16.672927856445312,
      "learning_rate": 8.582352941176471e-06,
      "loss": 0.7778,
      "step": 70410
    },
    {
      "epoch": 4142.35294117647,
      "grad_norm": 22.765443801879883,
      "learning_rate": 8.576470588235294e-06,
      "loss": 0.7352,
      "step": 70420
    },
    {
      "epoch": 4142.941176470588,
      "grad_norm": 23.532184600830078,
      "learning_rate": 8.570588235294117e-06,
      "loss": 0.7283,
      "step": 70430
    },
    {
      "epoch": 4143.529411764706,
      "grad_norm": 18.141765594482422,
      "learning_rate": 8.564705882352942e-06,
      "loss": 0.7246,
      "step": 70440
    },
    {
      "epoch": 4144.117647058823,
      "grad_norm": 21.61427879333496,
      "learning_rate": 8.558823529411765e-06,
      "loss": 0.695,
      "step": 70450
    },
    {
      "epoch": 4144.705882352941,
      "grad_norm": 20.543411254882812,
      "learning_rate": 8.552941176470588e-06,
      "loss": 0.6626,
      "step": 70460
    },
    {
      "epoch": 4145.294117647059,
      "grad_norm": 26.56031608581543,
      "learning_rate": 8.547058823529413e-06,
      "loss": 0.5945,
      "step": 70470
    },
    {
      "epoch": 4145.882352941177,
      "grad_norm": 20.05847930908203,
      "learning_rate": 8.541176470588236e-06,
      "loss": 0.5956,
      "step": 70480
    },
    {
      "epoch": 4146.470588235294,
      "grad_norm": 20.938695907592773,
      "learning_rate": 8.535294117647059e-06,
      "loss": 0.7276,
      "step": 70490
    },
    {
      "epoch": 4147.058823529412,
      "grad_norm": 21.735868453979492,
      "learning_rate": 8.529411764705883e-06,
      "loss": 0.6597,
      "step": 70500
    },
    {
      "epoch": 4147.64705882353,
      "grad_norm": 18.206480026245117,
      "learning_rate": 8.523529411764706e-06,
      "loss": 0.5778,
      "step": 70510
    },
    {
      "epoch": 4148.235294117647,
      "grad_norm": 22.109539031982422,
      "learning_rate": 8.517647058823531e-06,
      "loss": 0.7063,
      "step": 70520
    },
    {
      "epoch": 4148.823529411765,
      "grad_norm": 19.722862243652344,
      "learning_rate": 8.511764705882354e-06,
      "loss": 0.7326,
      "step": 70530
    },
    {
      "epoch": 4149.411764705882,
      "grad_norm": 28.042207717895508,
      "learning_rate": 8.505882352941177e-06,
      "loss": 0.7345,
      "step": 70540
    },
    {
      "epoch": 4150.0,
      "grad_norm": 31.565471649169922,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.7797,
      "step": 70550
    },
    {
      "epoch": 4150.588235294118,
      "grad_norm": 14.881963729858398,
      "learning_rate": 8.494117647058825e-06,
      "loss": 0.6797,
      "step": 70560
    },
    {
      "epoch": 4151.176470588235,
      "grad_norm": 15.817502975463867,
      "learning_rate": 8.488235294117648e-06,
      "loss": 0.6688,
      "step": 70570
    },
    {
      "epoch": 4151.764705882353,
      "grad_norm": 20.304893493652344,
      "learning_rate": 8.48235294117647e-06,
      "loss": 0.65,
      "step": 70580
    },
    {
      "epoch": 4152.35294117647,
      "grad_norm": 18.8300838470459,
      "learning_rate": 8.476470588235294e-06,
      "loss": 0.6915,
      "step": 70590
    },
    {
      "epoch": 4152.941176470588,
      "grad_norm": 17.762060165405273,
      "learning_rate": 8.470588235294118e-06,
      "loss": 0.6579,
      "step": 70600
    },
    {
      "epoch": 4153.529411764706,
      "grad_norm": 22.672643661499023,
      "learning_rate": 8.464705882352941e-06,
      "loss": 0.8285,
      "step": 70610
    },
    {
      "epoch": 4154.117647058823,
      "grad_norm": 19.223644256591797,
      "learning_rate": 8.458823529411764e-06,
      "loss": 0.6538,
      "step": 70620
    },
    {
      "epoch": 4154.705882352941,
      "grad_norm": 19.753082275390625,
      "learning_rate": 8.452941176470589e-06,
      "loss": 0.6894,
      "step": 70630
    },
    {
      "epoch": 4155.294117647059,
      "grad_norm": 15.508224487304688,
      "learning_rate": 8.447058823529412e-06,
      "loss": 0.7536,
      "step": 70640
    },
    {
      "epoch": 4155.882352941177,
      "grad_norm": 28.298744201660156,
      "learning_rate": 8.441176470588235e-06,
      "loss": 0.7462,
      "step": 70650
    },
    {
      "epoch": 4156.470588235294,
      "grad_norm": 18.122146606445312,
      "learning_rate": 8.43529411764706e-06,
      "loss": 0.6862,
      "step": 70660
    },
    {
      "epoch": 4157.058823529412,
      "grad_norm": 27.11752700805664,
      "learning_rate": 8.429411764705883e-06,
      "loss": 0.7413,
      "step": 70670
    },
    {
      "epoch": 4157.64705882353,
      "grad_norm": 21.89739418029785,
      "learning_rate": 8.423529411764707e-06,
      "loss": 0.6417,
      "step": 70680
    },
    {
      "epoch": 4158.235294117647,
      "grad_norm": 23.34836769104004,
      "learning_rate": 8.41764705882353e-06,
      "loss": 0.8099,
      "step": 70690
    },
    {
      "epoch": 4158.823529411765,
      "grad_norm": 14.827939987182617,
      "learning_rate": 8.411764705882353e-06,
      "loss": 0.6778,
      "step": 70700
    },
    {
      "epoch": 4159.411764705882,
      "grad_norm": 19.30605697631836,
      "learning_rate": 8.405882352941178e-06,
      "loss": 0.7049,
      "step": 70710
    },
    {
      "epoch": 4160.0,
      "grad_norm": 22.511850357055664,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.692,
      "step": 70720
    },
    {
      "epoch": 4160.588235294118,
      "grad_norm": 17.063968658447266,
      "learning_rate": 8.394117647058824e-06,
      "loss": 0.7185,
      "step": 70730
    },
    {
      "epoch": 4161.176470588235,
      "grad_norm": 21.073728561401367,
      "learning_rate": 8.388235294117649e-06,
      "loss": 0.7624,
      "step": 70740
    },
    {
      "epoch": 4161.764705882353,
      "grad_norm": 17.64246940612793,
      "learning_rate": 8.38235294117647e-06,
      "loss": 0.6742,
      "step": 70750
    },
    {
      "epoch": 4162.35294117647,
      "grad_norm": 13.427899360656738,
      "learning_rate": 8.376470588235295e-06,
      "loss": 0.6185,
      "step": 70760
    },
    {
      "epoch": 4162.941176470588,
      "grad_norm": 22.813655853271484,
      "learning_rate": 8.370588235294118e-06,
      "loss": 0.7289,
      "step": 70770
    },
    {
      "epoch": 4163.529411764706,
      "grad_norm": 23.426538467407227,
      "learning_rate": 8.36470588235294e-06,
      "loss": 0.6855,
      "step": 70780
    },
    {
      "epoch": 4164.117647058823,
      "grad_norm": 20.38798713684082,
      "learning_rate": 8.358823529411765e-06,
      "loss": 0.6914,
      "step": 70790
    },
    {
      "epoch": 4164.705882352941,
      "grad_norm": 14.844524383544922,
      "learning_rate": 8.352941176470588e-06,
      "loss": 0.6648,
      "step": 70800
    },
    {
      "epoch": 4165.294117647059,
      "grad_norm": 16.535539627075195,
      "learning_rate": 8.347058823529411e-06,
      "loss": 0.7637,
      "step": 70810
    },
    {
      "epoch": 4165.882352941177,
      "grad_norm": 19.806150436401367,
      "learning_rate": 8.341176470588236e-06,
      "loss": 0.6466,
      "step": 70820
    },
    {
      "epoch": 4166.470588235294,
      "grad_norm": 21.94366455078125,
      "learning_rate": 8.335294117647059e-06,
      "loss": 0.7019,
      "step": 70830
    },
    {
      "epoch": 4167.058823529412,
      "grad_norm": 19.420568466186523,
      "learning_rate": 8.329411764705884e-06,
      "loss": 0.6352,
      "step": 70840
    },
    {
      "epoch": 4167.64705882353,
      "grad_norm": 19.36850357055664,
      "learning_rate": 8.323529411764707e-06,
      "loss": 0.6725,
      "step": 70850
    },
    {
      "epoch": 4168.235294117647,
      "grad_norm": 17.785900115966797,
      "learning_rate": 8.31764705882353e-06,
      "loss": 0.5373,
      "step": 70860
    },
    {
      "epoch": 4168.823529411765,
      "grad_norm": 25.081693649291992,
      "learning_rate": 8.311764705882354e-06,
      "loss": 0.7814,
      "step": 70870
    },
    {
      "epoch": 4169.411764705882,
      "grad_norm": 16.500505447387695,
      "learning_rate": 8.305882352941177e-06,
      "loss": 0.742,
      "step": 70880
    },
    {
      "epoch": 4170.0,
      "grad_norm": 25.4918212890625,
      "learning_rate": 8.3e-06,
      "loss": 0.716,
      "step": 70890
    },
    {
      "epoch": 4170.588235294118,
      "grad_norm": 23.419174194335938,
      "learning_rate": 8.294117647058825e-06,
      "loss": 0.7216,
      "step": 70900
    },
    {
      "epoch": 4171.176470588235,
      "grad_norm": 25.346128463745117,
      "learning_rate": 8.288235294117648e-06,
      "loss": 0.6804,
      "step": 70910
    },
    {
      "epoch": 4171.764705882353,
      "grad_norm": 23.68937110900879,
      "learning_rate": 8.28235294117647e-06,
      "loss": 0.6028,
      "step": 70920
    },
    {
      "epoch": 4172.35294117647,
      "grad_norm": 23.953495025634766,
      "learning_rate": 8.276470588235294e-06,
      "loss": 0.7583,
      "step": 70930
    },
    {
      "epoch": 4172.941176470588,
      "grad_norm": 25.35043716430664,
      "learning_rate": 8.270588235294117e-06,
      "loss": 0.7441,
      "step": 70940
    },
    {
      "epoch": 4173.529411764706,
      "grad_norm": 20.78586196899414,
      "learning_rate": 8.264705882352941e-06,
      "loss": 0.6612,
      "step": 70950
    },
    {
      "epoch": 4174.117647058823,
      "grad_norm": 17.76708221435547,
      "learning_rate": 8.258823529411764e-06,
      "loss": 0.7178,
      "step": 70960
    },
    {
      "epoch": 4174.705882352941,
      "grad_norm": 22.798120498657227,
      "learning_rate": 8.252941176470587e-06,
      "loss": 0.7322,
      "step": 70970
    },
    {
      "epoch": 4175.294117647059,
      "grad_norm": 23.088726043701172,
      "learning_rate": 8.247058823529412e-06,
      "loss": 0.6984,
      "step": 70980
    },
    {
      "epoch": 4175.882352941177,
      "grad_norm": 22.695280075073242,
      "learning_rate": 8.241176470588235e-06,
      "loss": 0.7744,
      "step": 70990
    },
    {
      "epoch": 4176.470588235294,
      "grad_norm": 14.89077377319336,
      "learning_rate": 8.23529411764706e-06,
      "loss": 0.6602,
      "step": 71000
    },
    {
      "epoch": 4177.058823529412,
      "grad_norm": 23.9558162689209,
      "learning_rate": 8.229411764705883e-06,
      "loss": 0.7151,
      "step": 71010
    },
    {
      "epoch": 4177.64705882353,
      "grad_norm": 20.00860023498535,
      "learning_rate": 8.223529411764706e-06,
      "loss": 0.6761,
      "step": 71020
    },
    {
      "epoch": 4178.235294117647,
      "grad_norm": 20.358558654785156,
      "learning_rate": 8.21764705882353e-06,
      "loss": 0.6798,
      "step": 71030
    },
    {
      "epoch": 4178.823529411765,
      "grad_norm": 20.87546730041504,
      "learning_rate": 8.211764705882353e-06,
      "loss": 0.7058,
      "step": 71040
    },
    {
      "epoch": 4179.411764705882,
      "grad_norm": 23.26833724975586,
      "learning_rate": 8.205882352941176e-06,
      "loss": 0.6419,
      "step": 71050
    },
    {
      "epoch": 4180.0,
      "grad_norm": 23.664033889770508,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.7985,
      "step": 71060
    },
    {
      "epoch": 4180.588235294118,
      "grad_norm": 24.010499954223633,
      "learning_rate": 8.194117647058824e-06,
      "loss": 0.6954,
      "step": 71070
    },
    {
      "epoch": 4181.176470588235,
      "grad_norm": 28.83731460571289,
      "learning_rate": 8.188235294117649e-06,
      "loss": 0.6504,
      "step": 71080
    },
    {
      "epoch": 4181.764705882353,
      "grad_norm": 18.448211669921875,
      "learning_rate": 8.182352941176472e-06,
      "loss": 0.7128,
      "step": 71090
    },
    {
      "epoch": 4182.35294117647,
      "grad_norm": 25.583324432373047,
      "learning_rate": 8.176470588235295e-06,
      "loss": 0.7368,
      "step": 71100
    },
    {
      "epoch": 4182.941176470588,
      "grad_norm": 26.466236114501953,
      "learning_rate": 8.170588235294118e-06,
      "loss": 0.6909,
      "step": 71110
    },
    {
      "epoch": 4183.529411764706,
      "grad_norm": 26.981426239013672,
      "learning_rate": 8.16470588235294e-06,
      "loss": 0.7117,
      "step": 71120
    },
    {
      "epoch": 4184.117647058823,
      "grad_norm": 21.381425857543945,
      "learning_rate": 8.158823529411765e-06,
      "loss": 0.7636,
      "step": 71130
    },
    {
      "epoch": 4184.705882352941,
      "grad_norm": 21.502676010131836,
      "learning_rate": 8.152941176470588e-06,
      "loss": 0.811,
      "step": 71140
    },
    {
      "epoch": 4185.294117647059,
      "grad_norm": 22.191564559936523,
      "learning_rate": 8.147058823529411e-06,
      "loss": 0.6843,
      "step": 71150
    },
    {
      "epoch": 4185.882352941177,
      "grad_norm": 20.6470890045166,
      "learning_rate": 8.141176470588236e-06,
      "loss": 0.6099,
      "step": 71160
    },
    {
      "epoch": 4186.470588235294,
      "grad_norm": 23.600875854492188,
      "learning_rate": 8.135294117647059e-06,
      "loss": 0.7138,
      "step": 71170
    },
    {
      "epoch": 4187.058823529412,
      "grad_norm": 26.308515548706055,
      "learning_rate": 8.129411764705882e-06,
      "loss": 0.6999,
      "step": 71180
    },
    {
      "epoch": 4187.64705882353,
      "grad_norm": 21.017772674560547,
      "learning_rate": 8.123529411764707e-06,
      "loss": 0.6576,
      "step": 71190
    },
    {
      "epoch": 4188.235294117647,
      "grad_norm": 21.72694206237793,
      "learning_rate": 8.11764705882353e-06,
      "loss": 0.7609,
      "step": 71200
    },
    {
      "epoch": 4188.823529411765,
      "grad_norm": 25.231361389160156,
      "learning_rate": 8.111764705882353e-06,
      "loss": 0.6248,
      "step": 71210
    },
    {
      "epoch": 4189.411764705882,
      "grad_norm": 21.658191680908203,
      "learning_rate": 8.105882352941177e-06,
      "loss": 0.6993,
      "step": 71220
    },
    {
      "epoch": 4190.0,
      "grad_norm": 24.51018524169922,
      "learning_rate": 8.1e-06,
      "loss": 0.7155,
      "step": 71230
    },
    {
      "epoch": 4190.588235294118,
      "grad_norm": 17.796789169311523,
      "learning_rate": 8.094117647058825e-06,
      "loss": 0.6628,
      "step": 71240
    },
    {
      "epoch": 4191.176470588235,
      "grad_norm": 17.380714416503906,
      "learning_rate": 8.088235294117648e-06,
      "loss": 0.6361,
      "step": 71250
    },
    {
      "epoch": 4191.764705882353,
      "grad_norm": 24.270259857177734,
      "learning_rate": 8.082352941176471e-06,
      "loss": 0.7152,
      "step": 71260
    },
    {
      "epoch": 4192.35294117647,
      "grad_norm": 16.74210548400879,
      "learning_rate": 8.076470588235296e-06,
      "loss": 0.7005,
      "step": 71270
    },
    {
      "epoch": 4192.941176470588,
      "grad_norm": 20.049585342407227,
      "learning_rate": 8.070588235294119e-06,
      "loss": 0.7038,
      "step": 71280
    },
    {
      "epoch": 4193.529411764706,
      "grad_norm": 25.66118621826172,
      "learning_rate": 8.064705882352942e-06,
      "loss": 0.7667,
      "step": 71290
    },
    {
      "epoch": 4194.117647058823,
      "grad_norm": 14.37170124053955,
      "learning_rate": 8.058823529411765e-06,
      "loss": 0.8176,
      "step": 71300
    },
    {
      "epoch": 4194.705882352941,
      "grad_norm": 19.07341766357422,
      "learning_rate": 8.052941176470588e-06,
      "loss": 0.6881,
      "step": 71310
    },
    {
      "epoch": 4195.294117647059,
      "grad_norm": 18.51995277404785,
      "learning_rate": 8.047058823529412e-06,
      "loss": 0.7069,
      "step": 71320
    },
    {
      "epoch": 4195.882352941177,
      "grad_norm": 19.686288833618164,
      "learning_rate": 8.041176470588235e-06,
      "loss": 0.7023,
      "step": 71330
    },
    {
      "epoch": 4196.470588235294,
      "grad_norm": 14.783105850219727,
      "learning_rate": 8.035294117647058e-06,
      "loss": 0.566,
      "step": 71340
    },
    {
      "epoch": 4197.058823529412,
      "grad_norm": 23.0203800201416,
      "learning_rate": 8.029411764705883e-06,
      "loss": 0.7071,
      "step": 71350
    },
    {
      "epoch": 4197.64705882353,
      "grad_norm": 16.302988052368164,
      "learning_rate": 8.023529411764706e-06,
      "loss": 0.65,
      "step": 71360
    },
    {
      "epoch": 4198.235294117647,
      "grad_norm": 15.911361694335938,
      "learning_rate": 8.017647058823529e-06,
      "loss": 0.6879,
      "step": 71370
    },
    {
      "epoch": 4198.823529411765,
      "grad_norm": 17.848121643066406,
      "learning_rate": 8.011764705882354e-06,
      "loss": 0.6911,
      "step": 71380
    },
    {
      "epoch": 4199.411764705882,
      "grad_norm": 26.904394149780273,
      "learning_rate": 8.005882352941177e-06,
      "loss": 0.7553,
      "step": 71390
    },
    {
      "epoch": 4200.0,
      "grad_norm": 21.1768741607666,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6422,
      "step": 71400
    },
    {
      "epoch": 4200.588235294118,
      "grad_norm": 22.96457862854004,
      "learning_rate": 7.994117647058824e-06,
      "loss": 0.6957,
      "step": 71410
    },
    {
      "epoch": 4201.176470588235,
      "grad_norm": 21.30171775817871,
      "learning_rate": 7.988235294117647e-06,
      "loss": 0.5969,
      "step": 71420
    },
    {
      "epoch": 4201.764705882353,
      "grad_norm": 18.582895278930664,
      "learning_rate": 7.982352941176472e-06,
      "loss": 0.6241,
      "step": 71430
    },
    {
      "epoch": 4202.35294117647,
      "grad_norm": 24.264989852905273,
      "learning_rate": 7.976470588235295e-06,
      "loss": 0.7737,
      "step": 71440
    },
    {
      "epoch": 4202.941176470588,
      "grad_norm": 18.724058151245117,
      "learning_rate": 7.970588235294118e-06,
      "loss": 0.5869,
      "step": 71450
    },
    {
      "epoch": 4203.529411764706,
      "grad_norm": 22.10564613342285,
      "learning_rate": 7.964705882352943e-06,
      "loss": 0.7333,
      "step": 71460
    },
    {
      "epoch": 4204.117647058823,
      "grad_norm": 25.84427261352539,
      "learning_rate": 7.958823529411764e-06,
      "loss": 0.7491,
      "step": 71470
    },
    {
      "epoch": 4204.705882352941,
      "grad_norm": 16.527530670166016,
      "learning_rate": 7.952941176470589e-06,
      "loss": 0.7177,
      "step": 71480
    },
    {
      "epoch": 4205.294117647059,
      "grad_norm": 28.90618133544922,
      "learning_rate": 7.947058823529412e-06,
      "loss": 0.7214,
      "step": 71490
    },
    {
      "epoch": 4205.882352941177,
      "grad_norm": 27.0775146484375,
      "learning_rate": 7.941176470588235e-06,
      "loss": 0.6597,
      "step": 71500
    },
    {
      "epoch": 4206.470588235294,
      "grad_norm": 19.76046371459961,
      "learning_rate": 7.93529411764706e-06,
      "loss": 0.7221,
      "step": 71510
    },
    {
      "epoch": 4207.058823529412,
      "grad_norm": 17.248754501342773,
      "learning_rate": 7.929411764705882e-06,
      "loss": 0.6236,
      "step": 71520
    },
    {
      "epoch": 4207.64705882353,
      "grad_norm": 19.28413200378418,
      "learning_rate": 7.923529411764705e-06,
      "loss": 0.7979,
      "step": 71530
    },
    {
      "epoch": 4208.235294117647,
      "grad_norm": 20.575952529907227,
      "learning_rate": 7.91764705882353e-06,
      "loss": 0.7824,
      "step": 71540
    },
    {
      "epoch": 4208.823529411765,
      "grad_norm": 16.66358757019043,
      "learning_rate": 7.911764705882353e-06,
      "loss": 0.8253,
      "step": 71550
    },
    {
      "epoch": 4209.411764705882,
      "grad_norm": 18.681306838989258,
      "learning_rate": 7.905882352941178e-06,
      "loss": 0.7388,
      "step": 71560
    },
    {
      "epoch": 4210.0,
      "grad_norm": 14.949203491210938,
      "learning_rate": 7.9e-06,
      "loss": 0.7473,
      "step": 71570
    },
    {
      "epoch": 4210.588235294118,
      "grad_norm": 23.39531707763672,
      "learning_rate": 7.894117647058824e-06,
      "loss": 0.7008,
      "step": 71580
    },
    {
      "epoch": 4211.176470588235,
      "grad_norm": 23.329843521118164,
      "learning_rate": 7.888235294117648e-06,
      "loss": 0.676,
      "step": 71590
    },
    {
      "epoch": 4211.764705882353,
      "grad_norm": 16.466495513916016,
      "learning_rate": 7.882352941176471e-06,
      "loss": 0.7294,
      "step": 71600
    },
    {
      "epoch": 4212.35294117647,
      "grad_norm": 21.554973602294922,
      "learning_rate": 7.876470588235294e-06,
      "loss": 0.6418,
      "step": 71610
    },
    {
      "epoch": 4212.941176470588,
      "grad_norm": 24.941625595092773,
      "learning_rate": 7.870588235294119e-06,
      "loss": 0.7427,
      "step": 71620
    },
    {
      "epoch": 4213.529411764706,
      "grad_norm": 26.50326919555664,
      "learning_rate": 7.864705882352942e-06,
      "loss": 0.7271,
      "step": 71630
    },
    {
      "epoch": 4214.117647058823,
      "grad_norm": 18.936573028564453,
      "learning_rate": 7.858823529411765e-06,
      "loss": 0.7376,
      "step": 71640
    },
    {
      "epoch": 4214.705882352941,
      "grad_norm": 20.288301467895508,
      "learning_rate": 7.85294117647059e-06,
      "loss": 0.74,
      "step": 71650
    },
    {
      "epoch": 4215.294117647059,
      "grad_norm": 20.595993041992188,
      "learning_rate": 7.847058823529411e-06,
      "loss": 0.6845,
      "step": 71660
    },
    {
      "epoch": 4215.882352941177,
      "grad_norm": 24.447343826293945,
      "learning_rate": 7.841176470588236e-06,
      "loss": 0.6597,
      "step": 71670
    },
    {
      "epoch": 4216.470588235294,
      "grad_norm": 24.573463439941406,
      "learning_rate": 7.835294117647059e-06,
      "loss": 0.7316,
      "step": 71680
    },
    {
      "epoch": 4217.058823529412,
      "grad_norm": 26.899959564208984,
      "learning_rate": 7.829411764705882e-06,
      "loss": 0.6882,
      "step": 71690
    },
    {
      "epoch": 4217.64705882353,
      "grad_norm": 23.644161224365234,
      "learning_rate": 7.823529411764706e-06,
      "loss": 0.7589,
      "step": 71700
    },
    {
      "epoch": 4218.235294117647,
      "grad_norm": 22.676424026489258,
      "learning_rate": 7.81764705882353e-06,
      "loss": 0.6638,
      "step": 71710
    },
    {
      "epoch": 4218.823529411765,
      "grad_norm": 17.0308895111084,
      "learning_rate": 7.811764705882354e-06,
      "loss": 0.5607,
      "step": 71720
    },
    {
      "epoch": 4219.411764705882,
      "grad_norm": 14.500944137573242,
      "learning_rate": 7.805882352941177e-06,
      "loss": 0.6328,
      "step": 71730
    },
    {
      "epoch": 4220.0,
      "grad_norm": 12.506668090820312,
      "learning_rate": 7.8e-06,
      "loss": 0.67,
      "step": 71740
    },
    {
      "epoch": 4220.588235294118,
      "grad_norm": 20.06719207763672,
      "learning_rate": 7.794117647058825e-06,
      "loss": 0.6645,
      "step": 71750
    },
    {
      "epoch": 4221.176470588235,
      "grad_norm": 25.159761428833008,
      "learning_rate": 7.788235294117648e-06,
      "loss": 0.6791,
      "step": 71760
    },
    {
      "epoch": 4221.764705882353,
      "grad_norm": 23.11337661743164,
      "learning_rate": 7.78235294117647e-06,
      "loss": 0.6936,
      "step": 71770
    },
    {
      "epoch": 4222.35294117647,
      "grad_norm": 17.53388023376465,
      "learning_rate": 7.776470588235295e-06,
      "loss": 0.7404,
      "step": 71780
    },
    {
      "epoch": 4222.941176470588,
      "grad_norm": 22.38300132751465,
      "learning_rate": 7.770588235294118e-06,
      "loss": 0.6637,
      "step": 71790
    },
    {
      "epoch": 4223.529411764706,
      "grad_norm": 23.105051040649414,
      "learning_rate": 7.764705882352941e-06,
      "loss": 0.7233,
      "step": 71800
    },
    {
      "epoch": 4224.117647058823,
      "grad_norm": 20.422714233398438,
      "learning_rate": 7.758823529411766e-06,
      "loss": 0.6138,
      "step": 71810
    },
    {
      "epoch": 4224.705882352941,
      "grad_norm": 28.182132720947266,
      "learning_rate": 7.752941176470589e-06,
      "loss": 0.7499,
      "step": 71820
    },
    {
      "epoch": 4225.294117647059,
      "grad_norm": 16.701086044311523,
      "learning_rate": 7.747058823529414e-06,
      "loss": 0.7365,
      "step": 71830
    },
    {
      "epoch": 4225.882352941177,
      "grad_norm": 18.084962844848633,
      "learning_rate": 7.741176470588235e-06,
      "loss": 0.69,
      "step": 71840
    },
    {
      "epoch": 4226.470588235294,
      "grad_norm": 21.817800521850586,
      "learning_rate": 7.735294117647058e-06,
      "loss": 0.6815,
      "step": 71850
    },
    {
      "epoch": 4227.058823529412,
      "grad_norm": 18.767162322998047,
      "learning_rate": 7.729411764705882e-06,
      "loss": 0.7071,
      "step": 71860
    },
    {
      "epoch": 4227.64705882353,
      "grad_norm": 28.574705123901367,
      "learning_rate": 7.723529411764705e-06,
      "loss": 0.6984,
      "step": 71870
    },
    {
      "epoch": 4228.235294117647,
      "grad_norm": 22.952865600585938,
      "learning_rate": 7.71764705882353e-06,
      "loss": 0.7462,
      "step": 71880
    },
    {
      "epoch": 4228.823529411765,
      "grad_norm": 20.38776969909668,
      "learning_rate": 7.711764705882353e-06,
      "loss": 0.6481,
      "step": 71890
    },
    {
      "epoch": 4229.411764705882,
      "grad_norm": 17.256820678710938,
      "learning_rate": 7.705882352941176e-06,
      "loss": 0.6091,
      "step": 71900
    },
    {
      "epoch": 4230.0,
      "grad_norm": 26.61147117614746,
      "learning_rate": 7.7e-06,
      "loss": 0.7772,
      "step": 71910
    },
    {
      "epoch": 4230.588235294118,
      "grad_norm": 21.8322811126709,
      "learning_rate": 7.694117647058824e-06,
      "loss": 0.6976,
      "step": 71920
    },
    {
      "epoch": 4231.176470588235,
      "grad_norm": 15.906357765197754,
      "learning_rate": 7.688235294117647e-06,
      "loss": 0.604,
      "step": 71930
    },
    {
      "epoch": 4231.764705882353,
      "grad_norm": 23.3449649810791,
      "learning_rate": 7.682352941176471e-06,
      "loss": 0.7906,
      "step": 71940
    },
    {
      "epoch": 4232.35294117647,
      "grad_norm": 22.95050048828125,
      "learning_rate": 7.676470588235294e-06,
      "loss": 0.7021,
      "step": 71950
    },
    {
      "epoch": 4232.941176470588,
      "grad_norm": 27.12662124633789,
      "learning_rate": 7.670588235294119e-06,
      "loss": 0.6458,
      "step": 71960
    },
    {
      "epoch": 4233.529411764706,
      "grad_norm": 12.595946311950684,
      "learning_rate": 7.664705882352942e-06,
      "loss": 0.6788,
      "step": 71970
    },
    {
      "epoch": 4234.117647058823,
      "grad_norm": 18.13767433166504,
      "learning_rate": 7.658823529411765e-06,
      "loss": 0.7126,
      "step": 71980
    },
    {
      "epoch": 4234.705882352941,
      "grad_norm": 30.37238311767578,
      "learning_rate": 7.65294117647059e-06,
      "loss": 0.7743,
      "step": 71990
    },
    {
      "epoch": 4235.294117647059,
      "grad_norm": 18.040149688720703,
      "learning_rate": 7.647058823529413e-06,
      "loss": 0.7817,
      "step": 72000
    },
    {
      "epoch": 4235.882352941177,
      "grad_norm": 18.618135452270508,
      "learning_rate": 7.641176470588236e-06,
      "loss": 0.7135,
      "step": 72010
    },
    {
      "epoch": 4236.470588235294,
      "grad_norm": 16.052406311035156,
      "learning_rate": 7.635294117647059e-06,
      "loss": 0.7138,
      "step": 72020
    },
    {
      "epoch": 4237.058823529412,
      "grad_norm": 14.426055908203125,
      "learning_rate": 7.629411764705882e-06,
      "loss": 0.6505,
      "step": 72030
    },
    {
      "epoch": 4237.64705882353,
      "grad_norm": 15.649613380432129,
      "learning_rate": 7.623529411764706e-06,
      "loss": 0.6763,
      "step": 72040
    },
    {
      "epoch": 4238.235294117647,
      "grad_norm": 22.098264694213867,
      "learning_rate": 7.617647058823529e-06,
      "loss": 0.781,
      "step": 72050
    },
    {
      "epoch": 4238.823529411765,
      "grad_norm": 21.977773666381836,
      "learning_rate": 7.611764705882353e-06,
      "loss": 0.8839,
      "step": 72060
    },
    {
      "epoch": 4239.411764705882,
      "grad_norm": 18.078460693359375,
      "learning_rate": 7.605882352941176e-06,
      "loss": 0.5955,
      "step": 72070
    },
    {
      "epoch": 4240.0,
      "grad_norm": 30.630739212036133,
      "learning_rate": 7.6e-06,
      "loss": 0.5812,
      "step": 72080
    },
    {
      "epoch": 4240.588235294118,
      "grad_norm": 22.876527786254883,
      "learning_rate": 7.594117647058824e-06,
      "loss": 0.9012,
      "step": 72090
    },
    {
      "epoch": 4241.176470588235,
      "grad_norm": 18.246068954467773,
      "learning_rate": 7.588235294117648e-06,
      "loss": 0.7713,
      "step": 72100
    },
    {
      "epoch": 4241.764705882353,
      "grad_norm": 21.623348236083984,
      "learning_rate": 7.582352941176471e-06,
      "loss": 0.6355,
      "step": 72110
    },
    {
      "epoch": 4242.35294117647,
      "grad_norm": 21.975603103637695,
      "learning_rate": 7.5764705882352946e-06,
      "loss": 0.7674,
      "step": 72120
    },
    {
      "epoch": 4242.941176470588,
      "grad_norm": 23.859590530395508,
      "learning_rate": 7.570588235294118e-06,
      "loss": 0.6985,
      "step": 72130
    },
    {
      "epoch": 4243.529411764706,
      "grad_norm": 18.365386962890625,
      "learning_rate": 7.564705882352941e-06,
      "loss": 0.7151,
      "step": 72140
    },
    {
      "epoch": 4244.117647058823,
      "grad_norm": 26.736133575439453,
      "learning_rate": 7.558823529411765e-06,
      "loss": 0.7357,
      "step": 72150
    },
    {
      "epoch": 4244.705882352941,
      "grad_norm": 20.73977279663086,
      "learning_rate": 7.552941176470589e-06,
      "loss": 0.7847,
      "step": 72160
    },
    {
      "epoch": 4245.294117647059,
      "grad_norm": 25.223079681396484,
      "learning_rate": 7.547058823529413e-06,
      "loss": 0.6916,
      "step": 72170
    },
    {
      "epoch": 4245.882352941177,
      "grad_norm": 22.78131103515625,
      "learning_rate": 7.541176470588236e-06,
      "loss": 0.7552,
      "step": 72180
    },
    {
      "epoch": 4246.470588235294,
      "grad_norm": 20.46139907836914,
      "learning_rate": 7.53529411764706e-06,
      "loss": 0.6489,
      "step": 72190
    },
    {
      "epoch": 4247.058823529412,
      "grad_norm": 18.441741943359375,
      "learning_rate": 7.529411764705882e-06,
      "loss": 0.7449,
      "step": 72200
    },
    {
      "epoch": 4247.64705882353,
      "grad_norm": 16.289426803588867,
      "learning_rate": 7.523529411764706e-06,
      "loss": 0.608,
      "step": 72210
    },
    {
      "epoch": 4248.235294117647,
      "grad_norm": 21.131084442138672,
      "learning_rate": 7.5176470588235295e-06,
      "loss": 0.7043,
      "step": 72220
    },
    {
      "epoch": 4248.823529411765,
      "grad_norm": 17.384912490844727,
      "learning_rate": 7.5117647058823525e-06,
      "loss": 0.6183,
      "step": 72230
    },
    {
      "epoch": 4249.411764705882,
      "grad_norm": 19.19712257385254,
      "learning_rate": 7.505882352941176e-06,
      "loss": 0.6649,
      "step": 72240
    },
    {
      "epoch": 4250.0,
      "grad_norm": 28.159351348876953,
      "learning_rate": 7.5e-06,
      "loss": 0.7111,
      "step": 72250
    },
    {
      "epoch": 4250.588235294118,
      "grad_norm": 20.256256103515625,
      "learning_rate": 7.494117647058824e-06,
      "loss": 0.6798,
      "step": 72260
    },
    {
      "epoch": 4251.176470588235,
      "grad_norm": 20.171897888183594,
      "learning_rate": 7.488235294117647e-06,
      "loss": 0.6619,
      "step": 72270
    },
    {
      "epoch": 4251.764705882353,
      "grad_norm": 16.311079025268555,
      "learning_rate": 7.482352941176471e-06,
      "loss": 0.6907,
      "step": 72280
    },
    {
      "epoch": 4252.35294117647,
      "grad_norm": 28.715702056884766,
      "learning_rate": 7.476470588235295e-06,
      "loss": 0.7774,
      "step": 72290
    },
    {
      "epoch": 4252.941176470588,
      "grad_norm": 21.158100128173828,
      "learning_rate": 7.4705882352941185e-06,
      "loss": 0.7434,
      "step": 72300
    },
    {
      "epoch": 4253.529411764706,
      "grad_norm": 16.168521881103516,
      "learning_rate": 7.4647058823529415e-06,
      "loss": 0.594,
      "step": 72310
    },
    {
      "epoch": 4254.117647058823,
      "grad_norm": 22.676651000976562,
      "learning_rate": 7.458823529411765e-06,
      "loss": 0.688,
      "step": 72320
    },
    {
      "epoch": 4254.705882352941,
      "grad_norm": 18.99517822265625,
      "learning_rate": 7.452941176470589e-06,
      "loss": 0.6717,
      "step": 72330
    },
    {
      "epoch": 4255.294117647059,
      "grad_norm": 23.265399932861328,
      "learning_rate": 7.447058823529412e-06,
      "loss": 0.632,
      "step": 72340
    },
    {
      "epoch": 4255.882352941177,
      "grad_norm": 20.456113815307617,
      "learning_rate": 7.441176470588236e-06,
      "loss": 0.6946,
      "step": 72350
    },
    {
      "epoch": 4256.470588235294,
      "grad_norm": 19.510177612304688,
      "learning_rate": 7.43529411764706e-06,
      "loss": 0.6408,
      "step": 72360
    },
    {
      "epoch": 4257.058823529412,
      "grad_norm": 19.155317306518555,
      "learning_rate": 7.429411764705884e-06,
      "loss": 0.6431,
      "step": 72370
    },
    {
      "epoch": 4257.64705882353,
      "grad_norm": 15.456766128540039,
      "learning_rate": 7.423529411764706e-06,
      "loss": 0.6771,
      "step": 72380
    },
    {
      "epoch": 4258.235294117647,
      "grad_norm": 25.884830474853516,
      "learning_rate": 7.417647058823529e-06,
      "loss": 0.7131,
      "step": 72390
    },
    {
      "epoch": 4258.823529411765,
      "grad_norm": 20.07244300842285,
      "learning_rate": 7.411764705882353e-06,
      "loss": 0.6794,
      "step": 72400
    },
    {
      "epoch": 4259.411764705882,
      "grad_norm": 20.113615036010742,
      "learning_rate": 7.4058823529411765e-06,
      "loss": 0.7006,
      "step": 72410
    },
    {
      "epoch": 4260.0,
      "grad_norm": 20.761293411254883,
      "learning_rate": 7.4e-06,
      "loss": 0.6852,
      "step": 72420
    },
    {
      "epoch": 4260.588235294118,
      "grad_norm": 13.486790657043457,
      "learning_rate": 7.394117647058823e-06,
      "loss": 0.7074,
      "step": 72430
    },
    {
      "epoch": 4261.176470588235,
      "grad_norm": 15.163887023925781,
      "learning_rate": 7.388235294117647e-06,
      "loss": 0.7208,
      "step": 72440
    },
    {
      "epoch": 4261.764705882353,
      "grad_norm": 22.055997848510742,
      "learning_rate": 7.382352941176471e-06,
      "loss": 0.7264,
      "step": 72450
    },
    {
      "epoch": 4262.35294117647,
      "grad_norm": 19.19780731201172,
      "learning_rate": 7.376470588235295e-06,
      "loss": 0.8123,
      "step": 72460
    },
    {
      "epoch": 4262.941176470588,
      "grad_norm": 21.61691665649414,
      "learning_rate": 7.370588235294118e-06,
      "loss": 0.6645,
      "step": 72470
    },
    {
      "epoch": 4263.529411764706,
      "grad_norm": 20.024370193481445,
      "learning_rate": 7.364705882352942e-06,
      "loss": 0.7331,
      "step": 72480
    },
    {
      "epoch": 4264.117647058823,
      "grad_norm": 19.145771026611328,
      "learning_rate": 7.3588235294117655e-06,
      "loss": 0.6791,
      "step": 72490
    },
    {
      "epoch": 4264.705882352941,
      "grad_norm": 17.81479835510254,
      "learning_rate": 7.3529411764705884e-06,
      "loss": 0.649,
      "step": 72500
    },
    {
      "epoch": 4265.294117647059,
      "grad_norm": 18.480459213256836,
      "learning_rate": 7.347058823529412e-06,
      "loss": 0.6848,
      "step": 72510
    },
    {
      "epoch": 4265.882352941177,
      "grad_norm": 25.216550827026367,
      "learning_rate": 7.341176470588236e-06,
      "loss": 0.7294,
      "step": 72520
    },
    {
      "epoch": 4266.470588235294,
      "grad_norm": 22.35197639465332,
      "learning_rate": 7.33529411764706e-06,
      "loss": 0.6622,
      "step": 72530
    },
    {
      "epoch": 4267.058823529412,
      "grad_norm": 25.115333557128906,
      "learning_rate": 7.329411764705883e-06,
      "loss": 0.6816,
      "step": 72540
    },
    {
      "epoch": 4267.64705882353,
      "grad_norm": 20.988422393798828,
      "learning_rate": 7.323529411764707e-06,
      "loss": 0.6779,
      "step": 72550
    },
    {
      "epoch": 4268.235294117647,
      "grad_norm": 25.392715454101562,
      "learning_rate": 7.317647058823529e-06,
      "loss": 0.6435,
      "step": 72560
    },
    {
      "epoch": 4268.823529411765,
      "grad_norm": 18.250383377075195,
      "learning_rate": 7.311764705882353e-06,
      "loss": 0.6478,
      "step": 72570
    },
    {
      "epoch": 4269.411764705882,
      "grad_norm": 25.148639678955078,
      "learning_rate": 7.3058823529411766e-06,
      "loss": 0.675,
      "step": 72580
    },
    {
      "epoch": 4270.0,
      "grad_norm": 17.13520050048828,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.7865,
      "step": 72590
    },
    {
      "epoch": 4270.588235294118,
      "grad_norm": 28.88852310180664,
      "learning_rate": 7.294117647058823e-06,
      "loss": 0.8349,
      "step": 72600
    },
    {
      "epoch": 4271.176470588235,
      "grad_norm": 16.815998077392578,
      "learning_rate": 7.288235294117647e-06,
      "loss": 0.6748,
      "step": 72610
    },
    {
      "epoch": 4271.764705882353,
      "grad_norm": 27.43999481201172,
      "learning_rate": 7.282352941176471e-06,
      "loss": 0.7268,
      "step": 72620
    },
    {
      "epoch": 4272.35294117647,
      "grad_norm": 20.185070037841797,
      "learning_rate": 7.276470588235294e-06,
      "loss": 0.7393,
      "step": 72630
    },
    {
      "epoch": 4272.941176470588,
      "grad_norm": 17.42542266845703,
      "learning_rate": 7.270588235294118e-06,
      "loss": 0.6273,
      "step": 72640
    },
    {
      "epoch": 4273.529411764706,
      "grad_norm": 26.69028091430664,
      "learning_rate": 7.264705882352942e-06,
      "loss": 0.789,
      "step": 72650
    },
    {
      "epoch": 4274.117647058823,
      "grad_norm": 17.063995361328125,
      "learning_rate": 7.258823529411765e-06,
      "loss": 0.6744,
      "step": 72660
    },
    {
      "epoch": 4274.705882352941,
      "grad_norm": 17.64811134338379,
      "learning_rate": 7.2529411764705885e-06,
      "loss": 0.7078,
      "step": 72670
    },
    {
      "epoch": 4275.294117647059,
      "grad_norm": 24.82516860961914,
      "learning_rate": 7.247058823529412e-06,
      "loss": 0.7271,
      "step": 72680
    },
    {
      "epoch": 4275.882352941177,
      "grad_norm": 16.96483612060547,
      "learning_rate": 7.241176470588236e-06,
      "loss": 0.6258,
      "step": 72690
    },
    {
      "epoch": 4276.470588235294,
      "grad_norm": 22.268877029418945,
      "learning_rate": 7.235294117647059e-06,
      "loss": 0.7111,
      "step": 72700
    },
    {
      "epoch": 4277.058823529412,
      "grad_norm": 24.116376876831055,
      "learning_rate": 7.229411764705883e-06,
      "loss": 0.6573,
      "step": 72710
    },
    {
      "epoch": 4277.64705882353,
      "grad_norm": 17.95402717590332,
      "learning_rate": 7.223529411764707e-06,
      "loss": 0.8065,
      "step": 72720
    },
    {
      "epoch": 4278.235294117647,
      "grad_norm": 18.727832794189453,
      "learning_rate": 7.217647058823531e-06,
      "loss": 0.7544,
      "step": 72730
    },
    {
      "epoch": 4278.823529411765,
      "grad_norm": 25.125375747680664,
      "learning_rate": 7.211764705882354e-06,
      "loss": 0.6966,
      "step": 72740
    },
    {
      "epoch": 4279.411764705882,
      "grad_norm": 20.24555778503418,
      "learning_rate": 7.205882352941176e-06,
      "loss": 0.6503,
      "step": 72750
    },
    {
      "epoch": 4280.0,
      "grad_norm": 18.678380966186523,
      "learning_rate": 7.2e-06,
      "loss": 0.5816,
      "step": 72760
    },
    {
      "epoch": 4280.588235294118,
      "grad_norm": 23.068838119506836,
      "learning_rate": 7.1941176470588235e-06,
      "loss": 0.6894,
      "step": 72770
    },
    {
      "epoch": 4281.176470588235,
      "grad_norm": 14.679646492004395,
      "learning_rate": 7.188235294117647e-06,
      "loss": 0.6697,
      "step": 72780
    },
    {
      "epoch": 4281.764705882353,
      "grad_norm": 20.579132080078125,
      "learning_rate": 7.18235294117647e-06,
      "loss": 0.7079,
      "step": 72790
    },
    {
      "epoch": 4282.35294117647,
      "grad_norm": 23.58995819091797,
      "learning_rate": 7.176470588235294e-06,
      "loss": 0.774,
      "step": 72800
    },
    {
      "epoch": 4282.941176470588,
      "grad_norm": 19.965543746948242,
      "learning_rate": 7.170588235294118e-06,
      "loss": 0.6349,
      "step": 72810
    },
    {
      "epoch": 4283.529411764706,
      "grad_norm": 16.886215209960938,
      "learning_rate": 7.164705882352942e-06,
      "loss": 0.7699,
      "step": 72820
    },
    {
      "epoch": 4284.117647058823,
      "grad_norm": 21.386241912841797,
      "learning_rate": 7.158823529411765e-06,
      "loss": 0.7145,
      "step": 72830
    },
    {
      "epoch": 4284.705882352941,
      "grad_norm": 22.237689971923828,
      "learning_rate": 7.152941176470589e-06,
      "loss": 0.6746,
      "step": 72840
    },
    {
      "epoch": 4285.294117647059,
      "grad_norm": 23.98528289794922,
      "learning_rate": 7.1470588235294125e-06,
      "loss": 0.6787,
      "step": 72850
    },
    {
      "epoch": 4285.882352941177,
      "grad_norm": 23.106672286987305,
      "learning_rate": 7.1411764705882355e-06,
      "loss": 0.7223,
      "step": 72860
    },
    {
      "epoch": 4286.470588235294,
      "grad_norm": 20.10831642150879,
      "learning_rate": 7.135294117647059e-06,
      "loss": 0.6405,
      "step": 72870
    },
    {
      "epoch": 4287.058823529412,
      "grad_norm": 25.728395462036133,
      "learning_rate": 7.129411764705883e-06,
      "loss": 0.7366,
      "step": 72880
    },
    {
      "epoch": 4287.64705882353,
      "grad_norm": 16.524412155151367,
      "learning_rate": 7.123529411764707e-06,
      "loss": 0.5356,
      "step": 72890
    },
    {
      "epoch": 4288.235294117647,
      "grad_norm": 19.855915069580078,
      "learning_rate": 7.11764705882353e-06,
      "loss": 0.6831,
      "step": 72900
    },
    {
      "epoch": 4288.823529411765,
      "grad_norm": 23.51921844482422,
      "learning_rate": 7.111764705882354e-06,
      "loss": 0.7795,
      "step": 72910
    },
    {
      "epoch": 4289.411764705882,
      "grad_norm": 20.026451110839844,
      "learning_rate": 7.105882352941178e-06,
      "loss": 0.6509,
      "step": 72920
    },
    {
      "epoch": 4290.0,
      "grad_norm": 24.827016830444336,
      "learning_rate": 7.1e-06,
      "loss": 0.713,
      "step": 72930
    },
    {
      "epoch": 4290.588235294118,
      "grad_norm": 19.801429748535156,
      "learning_rate": 7.094117647058824e-06,
      "loss": 0.7342,
      "step": 72940
    },
    {
      "epoch": 4291.176470588235,
      "grad_norm": 19.73606300354004,
      "learning_rate": 7.088235294117647e-06,
      "loss": 0.6112,
      "step": 72950
    },
    {
      "epoch": 4291.764705882353,
      "grad_norm": 26.676929473876953,
      "learning_rate": 7.0823529411764704e-06,
      "loss": 0.6334,
      "step": 72960
    },
    {
      "epoch": 4292.35294117647,
      "grad_norm": 20.686748504638672,
      "learning_rate": 7.076470588235294e-06,
      "loss": 0.7082,
      "step": 72970
    },
    {
      "epoch": 4292.941176470588,
      "grad_norm": 20.949657440185547,
      "learning_rate": 7.070588235294118e-06,
      "loss": 0.6482,
      "step": 72980
    },
    {
      "epoch": 4293.529411764706,
      "grad_norm": 19.871458053588867,
      "learning_rate": 7.064705882352941e-06,
      "loss": 0.5998,
      "step": 72990
    },
    {
      "epoch": 4294.117647058823,
      "grad_norm": 19.14378547668457,
      "learning_rate": 7.058823529411765e-06,
      "loss": 0.7514,
      "step": 73000
    },
    {
      "epoch": 4294.705882352941,
      "grad_norm": 23.298845291137695,
      "learning_rate": 7.052941176470589e-06,
      "loss": 0.8063,
      "step": 73010
    },
    {
      "epoch": 4295.294117647059,
      "grad_norm": 23.464202880859375,
      "learning_rate": 7.047058823529412e-06,
      "loss": 0.7455,
      "step": 73020
    },
    {
      "epoch": 4295.882352941177,
      "grad_norm": 16.260828018188477,
      "learning_rate": 7.041176470588236e-06,
      "loss": 0.7694,
      "step": 73030
    },
    {
      "epoch": 4296.470588235294,
      "grad_norm": 16.11425018310547,
      "learning_rate": 7.0352941176470594e-06,
      "loss": 0.7956,
      "step": 73040
    },
    {
      "epoch": 4297.058823529412,
      "grad_norm": 23.662803649902344,
      "learning_rate": 7.029411764705883e-06,
      "loss": 0.7037,
      "step": 73050
    },
    {
      "epoch": 4297.64705882353,
      "grad_norm": 19.203828811645508,
      "learning_rate": 7.023529411764706e-06,
      "loss": 0.7061,
      "step": 73060
    },
    {
      "epoch": 4298.235294117647,
      "grad_norm": 27.577512741088867,
      "learning_rate": 7.01764705882353e-06,
      "loss": 0.7415,
      "step": 73070
    },
    {
      "epoch": 4298.823529411765,
      "grad_norm": 24.62699317932129,
      "learning_rate": 7.011764705882354e-06,
      "loss": 0.6857,
      "step": 73080
    },
    {
      "epoch": 4299.411764705882,
      "grad_norm": 20.395591735839844,
      "learning_rate": 7.005882352941178e-06,
      "loss": 0.677,
      "step": 73090
    },
    {
      "epoch": 4300.0,
      "grad_norm": 18.266494750976562,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.6364,
      "step": 73100
    },
    {
      "epoch": 4300.588235294118,
      "grad_norm": 23.438257217407227,
      "learning_rate": 6.994117647058823e-06,
      "loss": 0.6567,
      "step": 73110
    },
    {
      "epoch": 4301.176470588235,
      "grad_norm": 20.01299476623535,
      "learning_rate": 6.988235294117647e-06,
      "loss": 0.6886,
      "step": 73120
    },
    {
      "epoch": 4301.764705882353,
      "grad_norm": 29.17496109008789,
      "learning_rate": 6.9823529411764706e-06,
      "loss": 0.6826,
      "step": 73130
    },
    {
      "epoch": 4302.35294117647,
      "grad_norm": 16.46335220336914,
      "learning_rate": 6.976470588235294e-06,
      "loss": 0.6337,
      "step": 73140
    },
    {
      "epoch": 4302.941176470588,
      "grad_norm": 23.96010971069336,
      "learning_rate": 6.970588235294117e-06,
      "loss": 0.6851,
      "step": 73150
    },
    {
      "epoch": 4303.529411764706,
      "grad_norm": 19.19589614868164,
      "learning_rate": 6.964705882352941e-06,
      "loss": 0.7063,
      "step": 73160
    },
    {
      "epoch": 4304.117647058823,
      "grad_norm": 21.72139549255371,
      "learning_rate": 6.958823529411765e-06,
      "loss": 0.6537,
      "step": 73170
    },
    {
      "epoch": 4304.705882352941,
      "grad_norm": 19.695940017700195,
      "learning_rate": 6.952941176470589e-06,
      "loss": 0.6716,
      "step": 73180
    },
    {
      "epoch": 4305.294117647059,
      "grad_norm": 16.749408721923828,
      "learning_rate": 6.947058823529412e-06,
      "loss": 0.7147,
      "step": 73190
    },
    {
      "epoch": 4305.882352941177,
      "grad_norm": 23.204124450683594,
      "learning_rate": 6.941176470588236e-06,
      "loss": 0.6341,
      "step": 73200
    },
    {
      "epoch": 4306.470588235294,
      "grad_norm": 15.229482650756836,
      "learning_rate": 6.9352941176470595e-06,
      "loss": 0.6413,
      "step": 73210
    },
    {
      "epoch": 4307.058823529412,
      "grad_norm": 16.886573791503906,
      "learning_rate": 6.9294117647058825e-06,
      "loss": 0.7072,
      "step": 73220
    },
    {
      "epoch": 4307.64705882353,
      "grad_norm": 21.316347122192383,
      "learning_rate": 6.923529411764706e-06,
      "loss": 0.7318,
      "step": 73230
    },
    {
      "epoch": 4308.235294117647,
      "grad_norm": 24.605316162109375,
      "learning_rate": 6.91764705882353e-06,
      "loss": 0.6899,
      "step": 73240
    },
    {
      "epoch": 4308.823529411765,
      "grad_norm": 17.113935470581055,
      "learning_rate": 6.911764705882354e-06,
      "loss": 0.652,
      "step": 73250
    },
    {
      "epoch": 4309.411764705882,
      "grad_norm": 14.990729331970215,
      "learning_rate": 6.905882352941177e-06,
      "loss": 0.5976,
      "step": 73260
    },
    {
      "epoch": 4310.0,
      "grad_norm": 24.248254776000977,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.7694,
      "step": 73270
    },
    {
      "epoch": 4310.588235294118,
      "grad_norm": 19.4940242767334,
      "learning_rate": 6.894117647058825e-06,
      "loss": 0.8278,
      "step": 73280
    },
    {
      "epoch": 4311.176470588235,
      "grad_norm": 21.597858428955078,
      "learning_rate": 6.888235294117647e-06,
      "loss": 0.6648,
      "step": 73290
    },
    {
      "epoch": 4311.764705882353,
      "grad_norm": 20.859603881835938,
      "learning_rate": 6.882352941176471e-06,
      "loss": 0.7828,
      "step": 73300
    },
    {
      "epoch": 4312.35294117647,
      "grad_norm": 19.596099853515625,
      "learning_rate": 6.876470588235294e-06,
      "loss": 0.7315,
      "step": 73310
    },
    {
      "epoch": 4312.941176470588,
      "grad_norm": 21.085506439208984,
      "learning_rate": 6.8705882352941175e-06,
      "loss": 0.6557,
      "step": 73320
    },
    {
      "epoch": 4313.529411764706,
      "grad_norm": 26.266603469848633,
      "learning_rate": 6.864705882352941e-06,
      "loss": 0.6683,
      "step": 73330
    },
    {
      "epoch": 4314.117647058823,
      "grad_norm": 28.45814323425293,
      "learning_rate": 6.858823529411765e-06,
      "loss": 0.7011,
      "step": 73340
    },
    {
      "epoch": 4314.705882352941,
      "grad_norm": 17.706716537475586,
      "learning_rate": 6.852941176470588e-06,
      "loss": 0.6782,
      "step": 73350
    },
    {
      "epoch": 4315.294117647059,
      "grad_norm": 22.77265167236328,
      "learning_rate": 6.847058823529412e-06,
      "loss": 0.6567,
      "step": 73360
    },
    {
      "epoch": 4315.882352941177,
      "grad_norm": 19.023235321044922,
      "learning_rate": 6.841176470588236e-06,
      "loss": 0.7344,
      "step": 73370
    },
    {
      "epoch": 4316.470588235294,
      "grad_norm": 26.944364547729492,
      "learning_rate": 6.835294117647059e-06,
      "loss": 0.7893,
      "step": 73380
    },
    {
      "epoch": 4317.058823529412,
      "grad_norm": 21.448362350463867,
      "learning_rate": 6.829411764705883e-06,
      "loss": 0.645,
      "step": 73390
    },
    {
      "epoch": 4317.64705882353,
      "grad_norm": 19.437110900878906,
      "learning_rate": 6.8235294117647065e-06,
      "loss": 0.7001,
      "step": 73400
    },
    {
      "epoch": 4318.235294117647,
      "grad_norm": 17.280996322631836,
      "learning_rate": 6.81764705882353e-06,
      "loss": 0.6888,
      "step": 73410
    },
    {
      "epoch": 4318.823529411765,
      "grad_norm": 23.964866638183594,
      "learning_rate": 6.811764705882353e-06,
      "loss": 0.6827,
      "step": 73420
    },
    {
      "epoch": 4319.411764705882,
      "grad_norm": 24.88981819152832,
      "learning_rate": 6.805882352941177e-06,
      "loss": 0.7104,
      "step": 73430
    },
    {
      "epoch": 4320.0,
      "grad_norm": 16.513458251953125,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.6633,
      "step": 73440
    },
    {
      "epoch": 4320.588235294118,
      "grad_norm": 20.251855850219727,
      "learning_rate": 6.794117647058825e-06,
      "loss": 0.5658,
      "step": 73450
    },
    {
      "epoch": 4321.176470588235,
      "grad_norm": 25.662687301635742,
      "learning_rate": 6.788235294117648e-06,
      "loss": 0.7023,
      "step": 73460
    },
    {
      "epoch": 4321.764705882353,
      "grad_norm": 16.31786346435547,
      "learning_rate": 6.78235294117647e-06,
      "loss": 0.6683,
      "step": 73470
    },
    {
      "epoch": 4322.35294117647,
      "grad_norm": 20.003917694091797,
      "learning_rate": 6.776470588235294e-06,
      "loss": 0.7371,
      "step": 73480
    },
    {
      "epoch": 4322.941176470588,
      "grad_norm": 20.2654972076416,
      "learning_rate": 6.770588235294118e-06,
      "loss": 0.6148,
      "step": 73490
    },
    {
      "epoch": 4323.529411764706,
      "grad_norm": 14.52500057220459,
      "learning_rate": 6.7647058823529414e-06,
      "loss": 0.7592,
      "step": 73500
    },
    {
      "epoch": 4324.117647058823,
      "grad_norm": 28.783601760864258,
      "learning_rate": 6.758823529411764e-06,
      "loss": 0.647,
      "step": 73510
    },
    {
      "epoch": 4324.705882352941,
      "grad_norm": 20.78766632080078,
      "learning_rate": 6.752941176470588e-06,
      "loss": 0.5935,
      "step": 73520
    },
    {
      "epoch": 4325.294117647059,
      "grad_norm": 17.074007034301758,
      "learning_rate": 6.747058823529412e-06,
      "loss": 0.7081,
      "step": 73530
    },
    {
      "epoch": 4325.882352941177,
      "grad_norm": 18.231430053710938,
      "learning_rate": 6.741176470588235e-06,
      "loss": 0.6687,
      "step": 73540
    },
    {
      "epoch": 4326.470588235294,
      "grad_norm": 18.2645320892334,
      "learning_rate": 6.735294117647059e-06,
      "loss": 0.7445,
      "step": 73550
    },
    {
      "epoch": 4327.058823529412,
      "grad_norm": 22.746082305908203,
      "learning_rate": 6.729411764705883e-06,
      "loss": 0.7291,
      "step": 73560
    },
    {
      "epoch": 4327.64705882353,
      "grad_norm": 23.661996841430664,
      "learning_rate": 6.723529411764707e-06,
      "loss": 0.6678,
      "step": 73570
    },
    {
      "epoch": 4328.235294117647,
      "grad_norm": 21.07821273803711,
      "learning_rate": 6.7176470588235296e-06,
      "loss": 0.7274,
      "step": 73580
    },
    {
      "epoch": 4328.823529411765,
      "grad_norm": 25.029592514038086,
      "learning_rate": 6.711764705882353e-06,
      "loss": 0.767,
      "step": 73590
    },
    {
      "epoch": 4329.411764705882,
      "grad_norm": 18.40399169921875,
      "learning_rate": 6.705882352941177e-06,
      "loss": 0.6302,
      "step": 73600
    },
    {
      "epoch": 4330.0,
      "grad_norm": 16.0758056640625,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.636,
      "step": 73610
    },
    {
      "epoch": 4330.588235294118,
      "grad_norm": 22.341110229492188,
      "learning_rate": 6.694117647058824e-06,
      "loss": 0.6911,
      "step": 73620
    },
    {
      "epoch": 4331.176470588235,
      "grad_norm": 17.73148536682129,
      "learning_rate": 6.688235294117648e-06,
      "loss": 0.6726,
      "step": 73630
    },
    {
      "epoch": 4331.764705882353,
      "grad_norm": 20.598892211914062,
      "learning_rate": 6.682352941176472e-06,
      "loss": 0.6886,
      "step": 73640
    },
    {
      "epoch": 4332.35294117647,
      "grad_norm": 21.66366958618164,
      "learning_rate": 6.676470588235294e-06,
      "loss": 0.592,
      "step": 73650
    },
    {
      "epoch": 4332.941176470588,
      "grad_norm": 24.5139102935791,
      "learning_rate": 6.670588235294118e-06,
      "loss": 0.682,
      "step": 73660
    },
    {
      "epoch": 4333.529411764706,
      "grad_norm": 24.701383590698242,
      "learning_rate": 6.664705882352941e-06,
      "loss": 0.6455,
      "step": 73670
    },
    {
      "epoch": 4334.117647058823,
      "grad_norm": 18.066547393798828,
      "learning_rate": 6.6588235294117645e-06,
      "loss": 0.6043,
      "step": 73680
    },
    {
      "epoch": 4334.705882352941,
      "grad_norm": 24.519975662231445,
      "learning_rate": 6.652941176470588e-06,
      "loss": 0.6841,
      "step": 73690
    },
    {
      "epoch": 4335.294117647059,
      "grad_norm": 16.80276107788086,
      "learning_rate": 6.647058823529412e-06,
      "loss": 0.7483,
      "step": 73700
    },
    {
      "epoch": 4335.882352941177,
      "grad_norm": 30.241117477416992,
      "learning_rate": 6.641176470588235e-06,
      "loss": 0.6716,
      "step": 73710
    },
    {
      "epoch": 4336.470588235294,
      "grad_norm": 23.515304565429688,
      "learning_rate": 6.635294117647059e-06,
      "loss": 0.6471,
      "step": 73720
    },
    {
      "epoch": 4337.058823529412,
      "grad_norm": 18.95060920715332,
      "learning_rate": 6.629411764705883e-06,
      "loss": 0.6271,
      "step": 73730
    },
    {
      "epoch": 4337.64705882353,
      "grad_norm": 18.547706604003906,
      "learning_rate": 6.623529411764706e-06,
      "loss": 0.655,
      "step": 73740
    },
    {
      "epoch": 4338.235294117647,
      "grad_norm": 21.55657196044922,
      "learning_rate": 6.61764705882353e-06,
      "loss": 0.687,
      "step": 73750
    },
    {
      "epoch": 4338.823529411765,
      "grad_norm": 23.119434356689453,
      "learning_rate": 6.6117647058823535e-06,
      "loss": 0.7125,
      "step": 73760
    },
    {
      "epoch": 4339.411764705882,
      "grad_norm": 15.766263961791992,
      "learning_rate": 6.605882352941177e-06,
      "loss": 0.5616,
      "step": 73770
    },
    {
      "epoch": 4340.0,
      "grad_norm": 26.204423904418945,
      "learning_rate": 6.6e-06,
      "loss": 0.7598,
      "step": 73780
    },
    {
      "epoch": 4340.588235294118,
      "grad_norm": 26.062925338745117,
      "learning_rate": 6.594117647058824e-06,
      "loss": 0.6295,
      "step": 73790
    },
    {
      "epoch": 4341.176470588235,
      "grad_norm": 24.061676025390625,
      "learning_rate": 6.588235294117648e-06,
      "loss": 0.6821,
      "step": 73800
    },
    {
      "epoch": 4341.764705882353,
      "grad_norm": 18.121444702148438,
      "learning_rate": 6.582352941176472e-06,
      "loss": 0.7862,
      "step": 73810
    },
    {
      "epoch": 4342.35294117647,
      "grad_norm": 14.968281745910645,
      "learning_rate": 6.576470588235295e-06,
      "loss": 0.7679,
      "step": 73820
    },
    {
      "epoch": 4342.941176470588,
      "grad_norm": 18.462501525878906,
      "learning_rate": 6.570588235294119e-06,
      "loss": 0.6336,
      "step": 73830
    },
    {
      "epoch": 4343.529411764706,
      "grad_norm": 22.686325073242188,
      "learning_rate": 6.564705882352941e-06,
      "loss": 0.6272,
      "step": 73840
    },
    {
      "epoch": 4344.117647058823,
      "grad_norm": 16.946683883666992,
      "learning_rate": 6.558823529411765e-06,
      "loss": 0.591,
      "step": 73850
    },
    {
      "epoch": 4344.705882352941,
      "grad_norm": 21.531949996948242,
      "learning_rate": 6.5529411764705885e-06,
      "loss": 0.7519,
      "step": 73860
    },
    {
      "epoch": 4345.294117647059,
      "grad_norm": 17.901016235351562,
      "learning_rate": 6.5470588235294115e-06,
      "loss": 0.6032,
      "step": 73870
    },
    {
      "epoch": 4345.882352941177,
      "grad_norm": 22.426956176757812,
      "learning_rate": 6.541176470588235e-06,
      "loss": 0.7026,
      "step": 73880
    },
    {
      "epoch": 4346.470588235294,
      "grad_norm": 17.921432495117188,
      "learning_rate": 6.535294117647059e-06,
      "loss": 0.7145,
      "step": 73890
    },
    {
      "epoch": 4347.058823529412,
      "grad_norm": 17.973379135131836,
      "learning_rate": 6.529411764705882e-06,
      "loss": 0.6668,
      "step": 73900
    },
    {
      "epoch": 4347.64705882353,
      "grad_norm": 16.885683059692383,
      "learning_rate": 6.523529411764706e-06,
      "loss": 0.6682,
      "step": 73910
    },
    {
      "epoch": 4348.235294117647,
      "grad_norm": 18.679283142089844,
      "learning_rate": 6.51764705882353e-06,
      "loss": 0.6864,
      "step": 73920
    },
    {
      "epoch": 4348.823529411765,
      "grad_norm": 24.01922035217285,
      "learning_rate": 6.511764705882354e-06,
      "loss": 0.6908,
      "step": 73930
    },
    {
      "epoch": 4349.411764705882,
      "grad_norm": 20.984493255615234,
      "learning_rate": 6.505882352941177e-06,
      "loss": 0.738,
      "step": 73940
    },
    {
      "epoch": 4350.0,
      "grad_norm": 25.161237716674805,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.7594,
      "step": 73950
    },
    {
      "epoch": 4350.588235294118,
      "grad_norm": 21.451967239379883,
      "learning_rate": 6.494117647058824e-06,
      "loss": 0.7679,
      "step": 73960
    },
    {
      "epoch": 4351.176470588235,
      "grad_norm": 20.623210906982422,
      "learning_rate": 6.488235294117648e-06,
      "loss": 0.7143,
      "step": 73970
    },
    {
      "epoch": 4351.764705882353,
      "grad_norm": 18.12101173400879,
      "learning_rate": 6.482352941176471e-06,
      "loss": 0.6582,
      "step": 73980
    },
    {
      "epoch": 4352.35294117647,
      "grad_norm": 24.33515739440918,
      "learning_rate": 6.476470588235295e-06,
      "loss": 0.6815,
      "step": 73990
    },
    {
      "epoch": 4352.941176470588,
      "grad_norm": 20.077655792236328,
      "learning_rate": 6.470588235294119e-06,
      "loss": 0.6694,
      "step": 74000
    },
    {
      "epoch": 4353.529411764706,
      "grad_norm": 22.702199935913086,
      "learning_rate": 6.464705882352942e-06,
      "loss": 0.8113,
      "step": 74010
    },
    {
      "epoch": 4354.117647058823,
      "grad_norm": 17.110414505004883,
      "learning_rate": 6.458823529411765e-06,
      "loss": 0.5972,
      "step": 74020
    },
    {
      "epoch": 4354.705882352941,
      "grad_norm": 21.791847229003906,
      "learning_rate": 6.452941176470588e-06,
      "loss": 0.7426,
      "step": 74030
    },
    {
      "epoch": 4355.294117647059,
      "grad_norm": 31.5744686126709,
      "learning_rate": 6.4470588235294116e-06,
      "loss": 0.681,
      "step": 74040
    },
    {
      "epoch": 4355.882352941177,
      "grad_norm": 23.406675338745117,
      "learning_rate": 6.441176470588235e-06,
      "loss": 0.645,
      "step": 74050
    },
    {
      "epoch": 4356.470588235294,
      "grad_norm": 22.830625534057617,
      "learning_rate": 6.435294117647058e-06,
      "loss": 0.6688,
      "step": 74060
    },
    {
      "epoch": 4357.058823529412,
      "grad_norm": 24.0545711517334,
      "learning_rate": 6.429411764705882e-06,
      "loss": 0.645,
      "step": 74070
    },
    {
      "epoch": 4357.64705882353,
      "grad_norm": 20.176719665527344,
      "learning_rate": 6.423529411764706e-06,
      "loss": 0.7179,
      "step": 74080
    },
    {
      "epoch": 4358.235294117647,
      "grad_norm": 20.033573150634766,
      "learning_rate": 6.41764705882353e-06,
      "loss": 0.5859,
      "step": 74090
    },
    {
      "epoch": 4358.823529411765,
      "grad_norm": 21.516660690307617,
      "learning_rate": 6.411764705882353e-06,
      "loss": 0.7016,
      "step": 74100
    },
    {
      "epoch": 4359.411764705882,
      "grad_norm": 31.832340240478516,
      "learning_rate": 6.405882352941177e-06,
      "loss": 0.692,
      "step": 74110
    },
    {
      "epoch": 4360.0,
      "grad_norm": 29.75131607055664,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.8409,
      "step": 74120
    },
    {
      "epoch": 4360.588235294118,
      "grad_norm": 14.31973648071289,
      "learning_rate": 6.394117647058824e-06,
      "loss": 0.5639,
      "step": 74130
    },
    {
      "epoch": 4361.176470588235,
      "grad_norm": 20.25469398498535,
      "learning_rate": 6.388235294117647e-06,
      "loss": 0.7659,
      "step": 74140
    },
    {
      "epoch": 4361.764705882353,
      "grad_norm": 19.730052947998047,
      "learning_rate": 6.382352941176471e-06,
      "loss": 0.6637,
      "step": 74150
    },
    {
      "epoch": 4362.35294117647,
      "grad_norm": 24.653072357177734,
      "learning_rate": 6.376470588235295e-06,
      "loss": 0.7201,
      "step": 74160
    },
    {
      "epoch": 4362.941176470588,
      "grad_norm": 19.089061737060547,
      "learning_rate": 6.370588235294119e-06,
      "loss": 0.6917,
      "step": 74170
    },
    {
      "epoch": 4363.529411764706,
      "grad_norm": 19.889047622680664,
      "learning_rate": 6.364705882352942e-06,
      "loss": 0.6257,
      "step": 74180
    },
    {
      "epoch": 4364.117647058823,
      "grad_norm": 17.862783432006836,
      "learning_rate": 6.358823529411766e-06,
      "loss": 0.6872,
      "step": 74190
    },
    {
      "epoch": 4364.705882352941,
      "grad_norm": 17.963895797729492,
      "learning_rate": 6.352941176470588e-06,
      "loss": 0.7214,
      "step": 74200
    },
    {
      "epoch": 4365.294117647059,
      "grad_norm": 18.710962295532227,
      "learning_rate": 6.347058823529412e-06,
      "loss": 0.738,
      "step": 74210
    },
    {
      "epoch": 4365.882352941177,
      "grad_norm": 16.700437545776367,
      "learning_rate": 6.3411764705882355e-06,
      "loss": 0.6187,
      "step": 74220
    },
    {
      "epoch": 4366.470588235294,
      "grad_norm": 27.107057571411133,
      "learning_rate": 6.3352941176470585e-06,
      "loss": 0.6929,
      "step": 74230
    },
    {
      "epoch": 4367.058823529412,
      "grad_norm": 17.270328521728516,
      "learning_rate": 6.329411764705882e-06,
      "loss": 0.7169,
      "step": 74240
    },
    {
      "epoch": 4367.64705882353,
      "grad_norm": 20.272573471069336,
      "learning_rate": 6.323529411764706e-06,
      "loss": 0.7184,
      "step": 74250
    },
    {
      "epoch": 4368.235294117647,
      "grad_norm": 17.93413734436035,
      "learning_rate": 6.317647058823529e-06,
      "loss": 0.8078,
      "step": 74260
    },
    {
      "epoch": 4368.823529411765,
      "grad_norm": 20.246604919433594,
      "learning_rate": 6.311764705882353e-06,
      "loss": 0.749,
      "step": 74270
    },
    {
      "epoch": 4369.411764705882,
      "grad_norm": 16.692338943481445,
      "learning_rate": 6.305882352941177e-06,
      "loss": 0.5927,
      "step": 74280
    },
    {
      "epoch": 4370.0,
      "grad_norm": 18.1882381439209,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.6202,
      "step": 74290
    },
    {
      "epoch": 4370.588235294118,
      "grad_norm": 17.474489212036133,
      "learning_rate": 6.294117647058824e-06,
      "loss": 0.5352,
      "step": 74300
    },
    {
      "epoch": 4371.176470588235,
      "grad_norm": 17.31137466430664,
      "learning_rate": 6.2882352941176475e-06,
      "loss": 0.7554,
      "step": 74310
    },
    {
      "epoch": 4371.764705882353,
      "grad_norm": 19.845285415649414,
      "learning_rate": 6.282352941176471e-06,
      "loss": 0.7428,
      "step": 74320
    },
    {
      "epoch": 4372.35294117647,
      "grad_norm": 20.80001449584961,
      "learning_rate": 6.276470588235295e-06,
      "loss": 0.6723,
      "step": 74330
    },
    {
      "epoch": 4372.941176470588,
      "grad_norm": 14.777592658996582,
      "learning_rate": 6.270588235294118e-06,
      "loss": 0.6295,
      "step": 74340
    },
    {
      "epoch": 4373.529411764706,
      "grad_norm": 21.459590911865234,
      "learning_rate": 6.264705882352942e-06,
      "loss": 0.7161,
      "step": 74350
    },
    {
      "epoch": 4374.117647058823,
      "grad_norm": 21.49175453186035,
      "learning_rate": 6.258823529411766e-06,
      "loss": 0.7256,
      "step": 74360
    },
    {
      "epoch": 4374.705882352941,
      "grad_norm": 21.832656860351562,
      "learning_rate": 6.252941176470589e-06,
      "loss": 0.6797,
      "step": 74370
    },
    {
      "epoch": 4375.294117647059,
      "grad_norm": 18.971282958984375,
      "learning_rate": 6.247058823529412e-06,
      "loss": 0.6054,
      "step": 74380
    },
    {
      "epoch": 4375.882352941177,
      "grad_norm": 15.68424129486084,
      "learning_rate": 6.241176470588236e-06,
      "loss": 0.7106,
      "step": 74390
    },
    {
      "epoch": 4376.470588235294,
      "grad_norm": 27.323009490966797,
      "learning_rate": 6.2352941176470595e-06,
      "loss": 0.7401,
      "step": 74400
    },
    {
      "epoch": 4377.058823529412,
      "grad_norm": 17.053205490112305,
      "learning_rate": 6.229411764705883e-06,
      "loss": 0.7303,
      "step": 74410
    },
    {
      "epoch": 4377.64705882353,
      "grad_norm": 20.179794311523438,
      "learning_rate": 6.223529411764706e-06,
      "loss": 0.6862,
      "step": 74420
    },
    {
      "epoch": 4378.235294117647,
      "grad_norm": 21.5302677154541,
      "learning_rate": 6.217647058823529e-06,
      "loss": 0.7027,
      "step": 74430
    },
    {
      "epoch": 4378.823529411765,
      "grad_norm": 18.977996826171875,
      "learning_rate": 6.211764705882353e-06,
      "loss": 0.6785,
      "step": 74440
    },
    {
      "epoch": 4379.411764705882,
      "grad_norm": 23.3353271484375,
      "learning_rate": 6.205882352941177e-06,
      "loss": 0.6162,
      "step": 74450
    },
    {
      "epoch": 4380.0,
      "grad_norm": 17.385805130004883,
      "learning_rate": 6.2e-06,
      "loss": 0.6278,
      "step": 74460
    },
    {
      "epoch": 4380.588235294118,
      "grad_norm": 21.671018600463867,
      "learning_rate": 6.194117647058824e-06,
      "loss": 0.7318,
      "step": 74470
    },
    {
      "epoch": 4381.176470588235,
      "grad_norm": 20.57021141052246,
      "learning_rate": 6.188235294117648e-06,
      "loss": 0.6499,
      "step": 74480
    },
    {
      "epoch": 4381.764705882353,
      "grad_norm": 20.85881996154785,
      "learning_rate": 6.1823529411764714e-06,
      "loss": 0.6521,
      "step": 74490
    },
    {
      "epoch": 4382.35294117647,
      "grad_norm": 29.7940673828125,
      "learning_rate": 6.1764705882352944e-06,
      "loss": 0.7539,
      "step": 74500
    },
    {
      "epoch": 4382.941176470588,
      "grad_norm": 19.23013687133789,
      "learning_rate": 6.170588235294118e-06,
      "loss": 0.7199,
      "step": 74510
    },
    {
      "epoch": 4383.529411764706,
      "grad_norm": 24.239246368408203,
      "learning_rate": 6.164705882352941e-06,
      "loss": 0.7233,
      "step": 74520
    },
    {
      "epoch": 4384.117647058823,
      "grad_norm": 20.75728988647461,
      "learning_rate": 6.158823529411765e-06,
      "loss": 0.6559,
      "step": 74530
    },
    {
      "epoch": 4384.705882352941,
      "grad_norm": 19.444416046142578,
      "learning_rate": 6.152941176470588e-06,
      "loss": 0.6426,
      "step": 74540
    },
    {
      "epoch": 4385.294117647059,
      "grad_norm": 16.655664443969727,
      "learning_rate": 6.147058823529412e-06,
      "loss": 0.6794,
      "step": 74550
    },
    {
      "epoch": 4385.882352941177,
      "grad_norm": 26.16045379638672,
      "learning_rate": 6.141176470588236e-06,
      "loss": 0.6888,
      "step": 74560
    },
    {
      "epoch": 4386.470588235294,
      "grad_norm": 21.134010314941406,
      "learning_rate": 6.1352941176470596e-06,
      "loss": 0.682,
      "step": 74570
    },
    {
      "epoch": 4387.058823529412,
      "grad_norm": 22.084447860717773,
      "learning_rate": 6.1294117647058826e-06,
      "loss": 0.7066,
      "step": 74580
    },
    {
      "epoch": 4387.64705882353,
      "grad_norm": 17.929594039916992,
      "learning_rate": 6.123529411764706e-06,
      "loss": 0.6316,
      "step": 74590
    },
    {
      "epoch": 4388.235294117647,
      "grad_norm": 30.40053939819336,
      "learning_rate": 6.11764705882353e-06,
      "loss": 0.7299,
      "step": 74600
    },
    {
      "epoch": 4388.823529411765,
      "grad_norm": 19.54010581970215,
      "learning_rate": 6.111764705882353e-06,
      "loss": 0.6197,
      "step": 74610
    },
    {
      "epoch": 4389.411764705882,
      "grad_norm": 20.386613845825195,
      "learning_rate": 6.105882352941176e-06,
      "loss": 0.7073,
      "step": 74620
    },
    {
      "epoch": 4390.0,
      "grad_norm": 22.393735885620117,
      "learning_rate": 6.1e-06,
      "loss": 0.748,
      "step": 74630
    },
    {
      "epoch": 4390.588235294118,
      "grad_norm": 19.630327224731445,
      "learning_rate": 6.094117647058824e-06,
      "loss": 0.666,
      "step": 74640
    },
    {
      "epoch": 4391.176470588235,
      "grad_norm": 20.029953002929688,
      "learning_rate": 6.088235294117648e-06,
      "loss": 0.618,
      "step": 74650
    },
    {
      "epoch": 4391.764705882353,
      "grad_norm": 21.221101760864258,
      "learning_rate": 6.082352941176471e-06,
      "loss": 0.6856,
      "step": 74660
    },
    {
      "epoch": 4392.35294117647,
      "grad_norm": 22.02678871154785,
      "learning_rate": 6.0764705882352945e-06,
      "loss": 0.5746,
      "step": 74670
    },
    {
      "epoch": 4392.941176470588,
      "grad_norm": 17.025833129882812,
      "learning_rate": 6.070588235294118e-06,
      "loss": 0.6206,
      "step": 74680
    },
    {
      "epoch": 4393.529411764706,
      "grad_norm": 24.35648536682129,
      "learning_rate": 6.064705882352942e-06,
      "loss": 0.6675,
      "step": 74690
    },
    {
      "epoch": 4394.117647058823,
      "grad_norm": 22.94660186767578,
      "learning_rate": 6.058823529411764e-06,
      "loss": 0.7253,
      "step": 74700
    },
    {
      "epoch": 4394.705882352941,
      "grad_norm": 17.236459732055664,
      "learning_rate": 6.052941176470588e-06,
      "loss": 0.6597,
      "step": 74710
    },
    {
      "epoch": 4395.294117647059,
      "grad_norm": 19.506237030029297,
      "learning_rate": 6.047058823529412e-06,
      "loss": 0.769,
      "step": 74720
    },
    {
      "epoch": 4395.882352941177,
      "grad_norm": 18.67070198059082,
      "learning_rate": 6.041176470588236e-06,
      "loss": 0.6291,
      "step": 74730
    },
    {
      "epoch": 4396.470588235294,
      "grad_norm": 19.8372859954834,
      "learning_rate": 6.035294117647059e-06,
      "loss": 0.6774,
      "step": 74740
    },
    {
      "epoch": 4397.058823529412,
      "grad_norm": 22.606163024902344,
      "learning_rate": 6.029411764705883e-06,
      "loss": 0.6271,
      "step": 74750
    },
    {
      "epoch": 4397.64705882353,
      "grad_norm": 15.933119773864746,
      "learning_rate": 6.0235294117647065e-06,
      "loss": 0.5966,
      "step": 74760
    },
    {
      "epoch": 4398.235294117647,
      "grad_norm": 17.43507957458496,
      "learning_rate": 6.01764705882353e-06,
      "loss": 0.6858,
      "step": 74770
    },
    {
      "epoch": 4398.823529411765,
      "grad_norm": 13.81148624420166,
      "learning_rate": 6.011764705882353e-06,
      "loss": 0.6357,
      "step": 74780
    },
    {
      "epoch": 4399.411764705882,
      "grad_norm": 19.857057571411133,
      "learning_rate": 6.005882352941176e-06,
      "loss": 0.5931,
      "step": 74790
    },
    {
      "epoch": 4400.0,
      "grad_norm": 23.04606819152832,
      "learning_rate": 6e-06,
      "loss": 0.7291,
      "step": 74800
    },
    {
      "epoch": 4400.588235294118,
      "grad_norm": 20.70757293701172,
      "learning_rate": 5.994117647058824e-06,
      "loss": 0.726,
      "step": 74810
    },
    {
      "epoch": 4401.176470588235,
      "grad_norm": 13.875274658203125,
      "learning_rate": 5.988235294117647e-06,
      "loss": 0.6262,
      "step": 74820
    },
    {
      "epoch": 4401.764705882353,
      "grad_norm": 23.545366287231445,
      "learning_rate": 5.982352941176471e-06,
      "loss": 0.5538,
      "step": 74830
    },
    {
      "epoch": 4402.35294117647,
      "grad_norm": 15.396697044372559,
      "learning_rate": 5.976470588235295e-06,
      "loss": 0.7154,
      "step": 74840
    },
    {
      "epoch": 4402.941176470588,
      "grad_norm": 20.3082275390625,
      "learning_rate": 5.9705882352941185e-06,
      "loss": 0.7477,
      "step": 74850
    },
    {
      "epoch": 4403.529411764706,
      "grad_norm": 25.614349365234375,
      "learning_rate": 5.9647058823529415e-06,
      "loss": 0.7533,
      "step": 74860
    },
    {
      "epoch": 4404.117647058823,
      "grad_norm": 26.600250244140625,
      "learning_rate": 5.958823529411765e-06,
      "loss": 0.697,
      "step": 74870
    },
    {
      "epoch": 4404.705882352941,
      "grad_norm": 25.079856872558594,
      "learning_rate": 5.952941176470588e-06,
      "loss": 0.7317,
      "step": 74880
    },
    {
      "epoch": 4405.294117647059,
      "grad_norm": 20.329307556152344,
      "learning_rate": 5.947058823529412e-06,
      "loss": 0.6662,
      "step": 74890
    },
    {
      "epoch": 4405.882352941177,
      "grad_norm": 20.506685256958008,
      "learning_rate": 5.941176470588235e-06,
      "loss": 0.5961,
      "step": 74900
    },
    {
      "epoch": 4406.470588235294,
      "grad_norm": 22.953763961791992,
      "learning_rate": 5.935294117647059e-06,
      "loss": 0.6135,
      "step": 74910
    },
    {
      "epoch": 4407.058823529412,
      "grad_norm": 20.281463623046875,
      "learning_rate": 5.929411764705883e-06,
      "loss": 0.6484,
      "step": 74920
    },
    {
      "epoch": 4407.64705882353,
      "grad_norm": 21.79135513305664,
      "learning_rate": 5.923529411764707e-06,
      "loss": 0.6553,
      "step": 74930
    },
    {
      "epoch": 4408.235294117647,
      "grad_norm": 25.21354103088379,
      "learning_rate": 5.91764705882353e-06,
      "loss": 0.6158,
      "step": 74940
    },
    {
      "epoch": 4408.823529411765,
      "grad_norm": 25.359556198120117,
      "learning_rate": 5.9117647058823534e-06,
      "loss": 0.7451,
      "step": 74950
    },
    {
      "epoch": 4409.411764705882,
      "grad_norm": 17.077545166015625,
      "learning_rate": 5.905882352941177e-06,
      "loss": 0.6593,
      "step": 74960
    },
    {
      "epoch": 4410.0,
      "grad_norm": 27.62118148803711,
      "learning_rate": 5.9e-06,
      "loss": 0.5812,
      "step": 74970
    },
    {
      "epoch": 4410.588235294118,
      "grad_norm": 16.143753051757812,
      "learning_rate": 5.894117647058823e-06,
      "loss": 0.6855,
      "step": 74980
    },
    {
      "epoch": 4411.176470588235,
      "grad_norm": 19.689292907714844,
      "learning_rate": 5.888235294117647e-06,
      "loss": 0.71,
      "step": 74990
    },
    {
      "epoch": 4411.764705882353,
      "grad_norm": 20.638912200927734,
      "learning_rate": 5.882352941176471e-06,
      "loss": 0.7423,
      "step": 75000
    },
    {
      "epoch": 4412.35294117647,
      "grad_norm": 21.6375789642334,
      "learning_rate": 5.876470588235295e-06,
      "loss": 0.6345,
      "step": 75010
    },
    {
      "epoch": 4412.941176470588,
      "grad_norm": 27.613201141357422,
      "learning_rate": 5.870588235294118e-06,
      "loss": 0.6628,
      "step": 75020
    },
    {
      "epoch": 4413.529411764706,
      "grad_norm": 25.97325325012207,
      "learning_rate": 5.864705882352942e-06,
      "loss": 0.6003,
      "step": 75030
    },
    {
      "epoch": 4414.117647058823,
      "grad_norm": 25.635988235473633,
      "learning_rate": 5.858823529411765e-06,
      "loss": 0.6913,
      "step": 75040
    },
    {
      "epoch": 4414.705882352941,
      "grad_norm": 33.15498352050781,
      "learning_rate": 5.852941176470588e-06,
      "loss": 0.6812,
      "step": 75050
    },
    {
      "epoch": 4415.294117647059,
      "grad_norm": 16.33087730407715,
      "learning_rate": 5.847058823529411e-06,
      "loss": 0.7011,
      "step": 75060
    },
    {
      "epoch": 4415.882352941177,
      "grad_norm": 21.451168060302734,
      "learning_rate": 5.841176470588235e-06,
      "loss": 0.6925,
      "step": 75070
    },
    {
      "epoch": 4416.470588235294,
      "grad_norm": 16.127912521362305,
      "learning_rate": 5.835294117647059e-06,
      "loss": 0.7999,
      "step": 75080
    },
    {
      "epoch": 4417.058823529412,
      "grad_norm": 29.58277702331543,
      "learning_rate": 5.829411764705883e-06,
      "loss": 0.6309,
      "step": 75090
    },
    {
      "epoch": 4417.64705882353,
      "grad_norm": 18.091175079345703,
      "learning_rate": 5.823529411764706e-06,
      "loss": 0.6223,
      "step": 75100
    },
    {
      "epoch": 4418.235294117647,
      "grad_norm": 15.41511058807373,
      "learning_rate": 5.81764705882353e-06,
      "loss": 0.7192,
      "step": 75110
    },
    {
      "epoch": 4418.823529411765,
      "grad_norm": 23.043331146240234,
      "learning_rate": 5.8117647058823536e-06,
      "loss": 0.7145,
      "step": 75120
    },
    {
      "epoch": 4419.411764705882,
      "grad_norm": 24.822738647460938,
      "learning_rate": 5.805882352941177e-06,
      "loss": 0.5662,
      "step": 75130
    },
    {
      "epoch": 4420.0,
      "grad_norm": 15.204808235168457,
      "learning_rate": 5.8e-06,
      "loss": 0.7076,
      "step": 75140
    },
    {
      "epoch": 4420.588235294118,
      "grad_norm": 19.715049743652344,
      "learning_rate": 5.794117647058823e-06,
      "loss": 0.7561,
      "step": 75150
    },
    {
      "epoch": 4421.176470588235,
      "grad_norm": 20.89969253540039,
      "learning_rate": 5.788235294117647e-06,
      "loss": 0.6819,
      "step": 75160
    },
    {
      "epoch": 4421.764705882353,
      "grad_norm": 22.594919204711914,
      "learning_rate": 5.782352941176471e-06,
      "loss": 0.6871,
      "step": 75170
    },
    {
      "epoch": 4422.35294117647,
      "grad_norm": 22.50676918029785,
      "learning_rate": 5.776470588235294e-06,
      "loss": 0.6325,
      "step": 75180
    },
    {
      "epoch": 4422.941176470588,
      "grad_norm": 20.243196487426758,
      "learning_rate": 5.770588235294118e-06,
      "loss": 0.6212,
      "step": 75190
    },
    {
      "epoch": 4423.529411764706,
      "grad_norm": 33.05891418457031,
      "learning_rate": 5.764705882352942e-06,
      "loss": 0.7321,
      "step": 75200
    },
    {
      "epoch": 4424.117647058823,
      "grad_norm": 23.763282775878906,
      "learning_rate": 5.7588235294117655e-06,
      "loss": 0.7883,
      "step": 75210
    },
    {
      "epoch": 4424.705882352941,
      "grad_norm": 22.812707901000977,
      "learning_rate": 5.7529411764705885e-06,
      "loss": 0.7292,
      "step": 75220
    },
    {
      "epoch": 4425.294117647059,
      "grad_norm": 25.739835739135742,
      "learning_rate": 5.747058823529412e-06,
      "loss": 0.6671,
      "step": 75230
    },
    {
      "epoch": 4425.882352941177,
      "grad_norm": 17.77474594116211,
      "learning_rate": 5.741176470588235e-06,
      "loss": 0.6551,
      "step": 75240
    },
    {
      "epoch": 4426.470588235294,
      "grad_norm": 30.922313690185547,
      "learning_rate": 5.735294117647059e-06,
      "loss": 0.8157,
      "step": 75250
    },
    {
      "epoch": 4427.058823529412,
      "grad_norm": 33.4113655090332,
      "learning_rate": 5.729411764705882e-06,
      "loss": 0.7479,
      "step": 75260
    },
    {
      "epoch": 4427.64705882353,
      "grad_norm": 18.900732040405273,
      "learning_rate": 5.723529411764706e-06,
      "loss": 0.7921,
      "step": 75270
    },
    {
      "epoch": 4428.235294117647,
      "grad_norm": 17.319934844970703,
      "learning_rate": 5.71764705882353e-06,
      "loss": 0.7695,
      "step": 75280
    },
    {
      "epoch": 4428.823529411765,
      "grad_norm": 19.97711181640625,
      "learning_rate": 5.711764705882354e-06,
      "loss": 0.6441,
      "step": 75290
    },
    {
      "epoch": 4429.411764705882,
      "grad_norm": 16.884628295898438,
      "learning_rate": 5.705882352941177e-06,
      "loss": 0.5426,
      "step": 75300
    },
    {
      "epoch": 4430.0,
      "grad_norm": 22.878986358642578,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.6977,
      "step": 75310
    },
    {
      "epoch": 4430.588235294118,
      "grad_norm": 19.770845413208008,
      "learning_rate": 5.694117647058824e-06,
      "loss": 0.6961,
      "step": 75320
    },
    {
      "epoch": 4431.176470588235,
      "grad_norm": 18.50286293029785,
      "learning_rate": 5.688235294117647e-06,
      "loss": 0.5958,
      "step": 75330
    },
    {
      "epoch": 4431.764705882353,
      "grad_norm": 16.627145767211914,
      "learning_rate": 5.68235294117647e-06,
      "loss": 0.7107,
      "step": 75340
    },
    {
      "epoch": 4432.35294117647,
      "grad_norm": 14.106624603271484,
      "learning_rate": 5.676470588235294e-06,
      "loss": 0.6734,
      "step": 75350
    },
    {
      "epoch": 4432.941176470588,
      "grad_norm": 22.297243118286133,
      "learning_rate": 5.670588235294118e-06,
      "loss": 0.7115,
      "step": 75360
    },
    {
      "epoch": 4433.529411764706,
      "grad_norm": 21.624317169189453,
      "learning_rate": 5.664705882352942e-06,
      "loss": 0.6901,
      "step": 75370
    },
    {
      "epoch": 4434.117647058823,
      "grad_norm": 21.0484619140625,
      "learning_rate": 5.658823529411765e-06,
      "loss": 0.7673,
      "step": 75380
    },
    {
      "epoch": 4434.705882352941,
      "grad_norm": 21.417999267578125,
      "learning_rate": 5.652941176470589e-06,
      "loss": 0.6467,
      "step": 75390
    },
    {
      "epoch": 4435.294117647059,
      "grad_norm": 18.109363555908203,
      "learning_rate": 5.6470588235294125e-06,
      "loss": 0.7063,
      "step": 75400
    },
    {
      "epoch": 4435.882352941177,
      "grad_norm": 24.091554641723633,
      "learning_rate": 5.6411764705882354e-06,
      "loss": 0.6309,
      "step": 75410
    },
    {
      "epoch": 4436.470588235294,
      "grad_norm": 15.410264015197754,
      "learning_rate": 5.6352941176470584e-06,
      "loss": 0.6225,
      "step": 75420
    },
    {
      "epoch": 4437.058823529412,
      "grad_norm": 24.12478256225586,
      "learning_rate": 5.629411764705882e-06,
      "loss": 0.7279,
      "step": 75430
    },
    {
      "epoch": 4437.64705882353,
      "grad_norm": 18.754735946655273,
      "learning_rate": 5.623529411764706e-06,
      "loss": 0.6877,
      "step": 75440
    },
    {
      "epoch": 4438.235294117647,
      "grad_norm": 20.691282272338867,
      "learning_rate": 5.61764705882353e-06,
      "loss": 0.5612,
      "step": 75450
    },
    {
      "epoch": 4438.823529411765,
      "grad_norm": 16.13395118713379,
      "learning_rate": 5.611764705882353e-06,
      "loss": 0.6468,
      "step": 75460
    },
    {
      "epoch": 4439.411764705882,
      "grad_norm": 18.56679344177246,
      "learning_rate": 5.605882352941177e-06,
      "loss": 0.6206,
      "step": 75470
    },
    {
      "epoch": 4440.0,
      "grad_norm": 29.312387466430664,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.6986,
      "step": 75480
    },
    {
      "epoch": 4440.588235294118,
      "grad_norm": 20.209012985229492,
      "learning_rate": 5.594117647058824e-06,
      "loss": 0.6544,
      "step": 75490
    },
    {
      "epoch": 4441.176470588235,
      "grad_norm": 18.770383834838867,
      "learning_rate": 5.588235294117647e-06,
      "loss": 0.6102,
      "step": 75500
    },
    {
      "epoch": 4441.764705882353,
      "grad_norm": 28.391551971435547,
      "learning_rate": 5.582352941176471e-06,
      "loss": 0.7284,
      "step": 75510
    },
    {
      "epoch": 4442.35294117647,
      "grad_norm": 22.021825790405273,
      "learning_rate": 5.576470588235294e-06,
      "loss": 0.7462,
      "step": 75520
    },
    {
      "epoch": 4442.941176470588,
      "grad_norm": 21.70414924621582,
      "learning_rate": 5.570588235294118e-06,
      "loss": 0.7125,
      "step": 75530
    },
    {
      "epoch": 4443.529411764706,
      "grad_norm": 23.76520538330078,
      "learning_rate": 5.564705882352941e-06,
      "loss": 0.6896,
      "step": 75540
    },
    {
      "epoch": 4444.117647058823,
      "grad_norm": 20.25228500366211,
      "learning_rate": 5.558823529411765e-06,
      "loss": 0.7525,
      "step": 75550
    },
    {
      "epoch": 4444.705882352941,
      "grad_norm": 24.954509735107422,
      "learning_rate": 5.552941176470589e-06,
      "loss": 0.6656,
      "step": 75560
    },
    {
      "epoch": 4445.294117647059,
      "grad_norm": 17.6577205657959,
      "learning_rate": 5.547058823529412e-06,
      "loss": 0.6523,
      "step": 75570
    },
    {
      "epoch": 4445.882352941177,
      "grad_norm": 29.60810661315918,
      "learning_rate": 5.5411764705882356e-06,
      "loss": 0.7974,
      "step": 75580
    },
    {
      "epoch": 4446.470588235294,
      "grad_norm": 19.78853416442871,
      "learning_rate": 5.535294117647059e-06,
      "loss": 0.639,
      "step": 75590
    },
    {
      "epoch": 4447.058823529412,
      "grad_norm": 19.49395751953125,
      "learning_rate": 5.529411764705883e-06,
      "loss": 0.7096,
      "step": 75600
    },
    {
      "epoch": 4447.64705882353,
      "grad_norm": 26.215442657470703,
      "learning_rate": 5.523529411764706e-06,
      "loss": 0.8081,
      "step": 75610
    },
    {
      "epoch": 4448.235294117647,
      "grad_norm": 20.713882446289062,
      "learning_rate": 5.517647058823529e-06,
      "loss": 0.6443,
      "step": 75620
    },
    {
      "epoch": 4448.823529411765,
      "grad_norm": 16.46500015258789,
      "learning_rate": 5.511764705882353e-06,
      "loss": 0.6557,
      "step": 75630
    },
    {
      "epoch": 4449.411764705882,
      "grad_norm": 19.919544219970703,
      "learning_rate": 5.505882352941177e-06,
      "loss": 0.7173,
      "step": 75640
    },
    {
      "epoch": 4450.0,
      "grad_norm": 31.496809005737305,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.7917,
      "step": 75650
    },
    {
      "epoch": 4450.588235294118,
      "grad_norm": 19.43218231201172,
      "learning_rate": 5.494117647058824e-06,
      "loss": 0.6632,
      "step": 75660
    },
    {
      "epoch": 4451.176470588235,
      "grad_norm": 27.69441795349121,
      "learning_rate": 5.4882352941176475e-06,
      "loss": 0.7791,
      "step": 75670
    },
    {
      "epoch": 4451.764705882353,
      "grad_norm": 22.470495223999023,
      "learning_rate": 5.482352941176471e-06,
      "loss": 0.5683,
      "step": 75680
    },
    {
      "epoch": 4452.35294117647,
      "grad_norm": 16.648014068603516,
      "learning_rate": 5.476470588235294e-06,
      "loss": 0.6955,
      "step": 75690
    },
    {
      "epoch": 4452.941176470588,
      "grad_norm": 29.21344566345215,
      "learning_rate": 5.470588235294117e-06,
      "loss": 0.6954,
      "step": 75700
    },
    {
      "epoch": 4453.529411764706,
      "grad_norm": 21.567502975463867,
      "learning_rate": 5.464705882352941e-06,
      "loss": 0.6681,
      "step": 75710
    },
    {
      "epoch": 4454.117647058823,
      "grad_norm": 15.978641510009766,
      "learning_rate": 5.458823529411765e-06,
      "loss": 0.6151,
      "step": 75720
    },
    {
      "epoch": 4454.705882352941,
      "grad_norm": 19.411014556884766,
      "learning_rate": 5.452941176470589e-06,
      "loss": 0.6599,
      "step": 75730
    },
    {
      "epoch": 4455.294117647059,
      "grad_norm": 19.255678176879883,
      "learning_rate": 5.447058823529412e-06,
      "loss": 0.6607,
      "step": 75740
    },
    {
      "epoch": 4455.882352941177,
      "grad_norm": 20.115455627441406,
      "learning_rate": 5.441176470588236e-06,
      "loss": 0.628,
      "step": 75750
    },
    {
      "epoch": 4456.470588235294,
      "grad_norm": 25.150461196899414,
      "learning_rate": 5.4352941176470595e-06,
      "loss": 0.7252,
      "step": 75760
    },
    {
      "epoch": 4457.058823529412,
      "grad_norm": 24.259510040283203,
      "learning_rate": 5.4294117647058825e-06,
      "loss": 0.6873,
      "step": 75770
    },
    {
      "epoch": 4457.64705882353,
      "grad_norm": 30.549345016479492,
      "learning_rate": 5.423529411764706e-06,
      "loss": 0.7423,
      "step": 75780
    },
    {
      "epoch": 4458.235294117647,
      "grad_norm": 22.21023941040039,
      "learning_rate": 5.417647058823529e-06,
      "loss": 0.6722,
      "step": 75790
    },
    {
      "epoch": 4458.823529411765,
      "grad_norm": 16.723237991333008,
      "learning_rate": 5.411764705882353e-06,
      "loss": 0.6621,
      "step": 75800
    },
    {
      "epoch": 4459.411764705882,
      "grad_norm": 33.23750686645508,
      "learning_rate": 5.405882352941177e-06,
      "loss": 0.6185,
      "step": 75810
    },
    {
      "epoch": 4460.0,
      "grad_norm": 23.29714012145996,
      "learning_rate": 5.4e-06,
      "loss": 0.6563,
      "step": 75820
    },
    {
      "epoch": 4460.588235294118,
      "grad_norm": 15.295329093933105,
      "learning_rate": 5.394117647058824e-06,
      "loss": 0.7061,
      "step": 75830
    },
    {
      "epoch": 4461.176470588235,
      "grad_norm": 18.03829002380371,
      "learning_rate": 5.388235294117648e-06,
      "loss": 0.6383,
      "step": 75840
    },
    {
      "epoch": 4461.764705882353,
      "grad_norm": 26.72564697265625,
      "learning_rate": 5.382352941176471e-06,
      "loss": 0.7182,
      "step": 75850
    },
    {
      "epoch": 4462.35294117647,
      "grad_norm": 26.44936180114746,
      "learning_rate": 5.3764705882352945e-06,
      "loss": 0.6776,
      "step": 75860
    },
    {
      "epoch": 4462.941176470588,
      "grad_norm": 25.29776382446289,
      "learning_rate": 5.370588235294118e-06,
      "loss": 0.7099,
      "step": 75870
    },
    {
      "epoch": 4463.529411764706,
      "grad_norm": 25.54405403137207,
      "learning_rate": 5.364705882352941e-06,
      "loss": 0.6642,
      "step": 75880
    },
    {
      "epoch": 4464.117647058823,
      "grad_norm": 17.443023681640625,
      "learning_rate": 5.358823529411765e-06,
      "loss": 0.6093,
      "step": 75890
    },
    {
      "epoch": 4464.705882352941,
      "grad_norm": 21.85476303100586,
      "learning_rate": 5.352941176470588e-06,
      "loss": 0.6963,
      "step": 75900
    },
    {
      "epoch": 4465.294117647059,
      "grad_norm": 20.120285034179688,
      "learning_rate": 5.347058823529412e-06,
      "loss": 0.6648,
      "step": 75910
    },
    {
      "epoch": 4465.882352941177,
      "grad_norm": 22.91265296936035,
      "learning_rate": 5.341176470588236e-06,
      "loss": 0.6817,
      "step": 75920
    },
    {
      "epoch": 4466.470588235294,
      "grad_norm": 22.42176628112793,
      "learning_rate": 5.335294117647059e-06,
      "loss": 0.7385,
      "step": 75930
    },
    {
      "epoch": 4467.058823529412,
      "grad_norm": 21.8270320892334,
      "learning_rate": 5.329411764705883e-06,
      "loss": 0.7552,
      "step": 75940
    },
    {
      "epoch": 4467.64705882353,
      "grad_norm": 23.869384765625,
      "learning_rate": 5.3235294117647064e-06,
      "loss": 0.7235,
      "step": 75950
    },
    {
      "epoch": 4468.235294117647,
      "grad_norm": 21.642480850219727,
      "learning_rate": 5.31764705882353e-06,
      "loss": 0.6668,
      "step": 75960
    },
    {
      "epoch": 4468.823529411765,
      "grad_norm": 13.652121543884277,
      "learning_rate": 5.311764705882353e-06,
      "loss": 0.6119,
      "step": 75970
    },
    {
      "epoch": 4469.411764705882,
      "grad_norm": 22.34727668762207,
      "learning_rate": 5.305882352941176e-06,
      "loss": 0.6201,
      "step": 75980
    },
    {
      "epoch": 4470.0,
      "grad_norm": 26.26856231689453,
      "learning_rate": 5.3e-06,
      "loss": 0.7151,
      "step": 75990
    },
    {
      "epoch": 4470.588235294118,
      "grad_norm": 16.32878875732422,
      "learning_rate": 5.294117647058824e-06,
      "loss": 0.6855,
      "step": 76000
    },
    {
      "epoch": 4471.176470588235,
      "grad_norm": 25.10845184326172,
      "learning_rate": 5.288235294117647e-06,
      "loss": 0.6402,
      "step": 76010
    },
    {
      "epoch": 4471.764705882353,
      "grad_norm": 17.525115966796875,
      "learning_rate": 5.282352941176471e-06,
      "loss": 0.6239,
      "step": 76020
    },
    {
      "epoch": 4472.35294117647,
      "grad_norm": 20.35788917541504,
      "learning_rate": 5.2764705882352946e-06,
      "loss": 0.6234,
      "step": 76030
    },
    {
      "epoch": 4472.941176470588,
      "grad_norm": 19.415983200073242,
      "learning_rate": 5.270588235294118e-06,
      "loss": 0.6741,
      "step": 76040
    },
    {
      "epoch": 4473.529411764706,
      "grad_norm": 13.852335929870605,
      "learning_rate": 5.264705882352941e-06,
      "loss": 0.6396,
      "step": 76050
    },
    {
      "epoch": 4474.117647058823,
      "grad_norm": 16.43083953857422,
      "learning_rate": 5.258823529411764e-06,
      "loss": 0.7528,
      "step": 76060
    },
    {
      "epoch": 4474.705882352941,
      "grad_norm": 21.14583969116211,
      "learning_rate": 5.252941176470588e-06,
      "loss": 0.689,
      "step": 76070
    },
    {
      "epoch": 4475.294117647059,
      "grad_norm": 13.969027519226074,
      "learning_rate": 5.247058823529412e-06,
      "loss": 0.6758,
      "step": 76080
    },
    {
      "epoch": 4475.882352941177,
      "grad_norm": 14.79305648803711,
      "learning_rate": 5.241176470588236e-06,
      "loss": 0.6968,
      "step": 76090
    },
    {
      "epoch": 4476.470588235294,
      "grad_norm": 20.589221954345703,
      "learning_rate": 5.235294117647059e-06,
      "loss": 0.6792,
      "step": 76100
    },
    {
      "epoch": 4477.058823529412,
      "grad_norm": 21.989274978637695,
      "learning_rate": 5.229411764705883e-06,
      "loss": 0.6198,
      "step": 76110
    },
    {
      "epoch": 4477.64705882353,
      "grad_norm": 18.40380859375,
      "learning_rate": 5.2235294117647065e-06,
      "loss": 0.7288,
      "step": 76120
    },
    {
      "epoch": 4478.235294117647,
      "grad_norm": 20.682523727416992,
      "learning_rate": 5.2176470588235295e-06,
      "loss": 0.6856,
      "step": 76130
    },
    {
      "epoch": 4478.823529411765,
      "grad_norm": 18.53056526184082,
      "learning_rate": 5.211764705882353e-06,
      "loss": 0.6168,
      "step": 76140
    },
    {
      "epoch": 4479.411764705882,
      "grad_norm": 18.628250122070312,
      "learning_rate": 5.205882352941176e-06,
      "loss": 0.7147,
      "step": 76150
    },
    {
      "epoch": 4480.0,
      "grad_norm": 19.66092300415039,
      "learning_rate": 5.2e-06,
      "loss": 0.6626,
      "step": 76160
    },
    {
      "epoch": 4480.588235294118,
      "grad_norm": 19.472021102905273,
      "learning_rate": 5.194117647058824e-06,
      "loss": 0.6648,
      "step": 76170
    },
    {
      "epoch": 4481.176470588235,
      "grad_norm": 18.3391170501709,
      "learning_rate": 5.188235294117647e-06,
      "loss": 0.662,
      "step": 76180
    },
    {
      "epoch": 4481.764705882353,
      "grad_norm": 20.729143142700195,
      "learning_rate": 5.182352941176471e-06,
      "loss": 0.6275,
      "step": 76190
    },
    {
      "epoch": 4482.35294117647,
      "grad_norm": 25.543582916259766,
      "learning_rate": 5.176470588235295e-06,
      "loss": 0.7538,
      "step": 76200
    },
    {
      "epoch": 4482.941176470588,
      "grad_norm": 16.83249282836914,
      "learning_rate": 5.170588235294118e-06,
      "loss": 0.8029,
      "step": 76210
    },
    {
      "epoch": 4483.529411764706,
      "grad_norm": 20.482707977294922,
      "learning_rate": 5.1647058823529415e-06,
      "loss": 0.7248,
      "step": 76220
    },
    {
      "epoch": 4484.117647058823,
      "grad_norm": 25.817825317382812,
      "learning_rate": 5.158823529411765e-06,
      "loss": 0.7309,
      "step": 76230
    },
    {
      "epoch": 4484.705882352941,
      "grad_norm": 22.06804656982422,
      "learning_rate": 5.152941176470588e-06,
      "loss": 0.8006,
      "step": 76240
    },
    {
      "epoch": 4485.294117647059,
      "grad_norm": 19.926513671875,
      "learning_rate": 5.147058823529412e-06,
      "loss": 0.6068,
      "step": 76250
    },
    {
      "epoch": 4485.882352941177,
      "grad_norm": 19.31785774230957,
      "learning_rate": 5.141176470588235e-06,
      "loss": 0.6651,
      "step": 76260
    },
    {
      "epoch": 4486.470588235294,
      "grad_norm": 19.518701553344727,
      "learning_rate": 5.135294117647059e-06,
      "loss": 0.7341,
      "step": 76270
    },
    {
      "epoch": 4487.058823529412,
      "grad_norm": 20.067611694335938,
      "learning_rate": 5.129411764705883e-06,
      "loss": 0.7413,
      "step": 76280
    },
    {
      "epoch": 4487.64705882353,
      "grad_norm": 14.041975021362305,
      "learning_rate": 5.123529411764706e-06,
      "loss": 0.6586,
      "step": 76290
    },
    {
      "epoch": 4488.235294117647,
      "grad_norm": 27.0621280670166,
      "learning_rate": 5.11764705882353e-06,
      "loss": 0.5955,
      "step": 76300
    },
    {
      "epoch": 4488.823529411765,
      "grad_norm": 17.739770889282227,
      "learning_rate": 5.1117647058823535e-06,
      "loss": 0.6783,
      "step": 76310
    },
    {
      "epoch": 4489.411764705882,
      "grad_norm": 19.910808563232422,
      "learning_rate": 5.105882352941177e-06,
      "loss": 0.6584,
      "step": 76320
    },
    {
      "epoch": 4490.0,
      "grad_norm": 25.841781616210938,
      "learning_rate": 5.1e-06,
      "loss": 0.6522,
      "step": 76330
    },
    {
      "epoch": 4490.588235294118,
      "grad_norm": 21.51433563232422,
      "learning_rate": 5.094117647058823e-06,
      "loss": 0.6886,
      "step": 76340
    },
    {
      "epoch": 4491.176470588235,
      "grad_norm": 25.628108978271484,
      "learning_rate": 5.088235294117647e-06,
      "loss": 0.7636,
      "step": 76350
    },
    {
      "epoch": 4491.764705882353,
      "grad_norm": 20.200441360473633,
      "learning_rate": 5.082352941176471e-06,
      "loss": 0.7232,
      "step": 76360
    },
    {
      "epoch": 4492.35294117647,
      "grad_norm": 21.709444046020508,
      "learning_rate": 5.076470588235294e-06,
      "loss": 0.7025,
      "step": 76370
    },
    {
      "epoch": 4492.941176470588,
      "grad_norm": 23.72380256652832,
      "learning_rate": 5.070588235294118e-06,
      "loss": 0.7773,
      "step": 76380
    },
    {
      "epoch": 4493.529411764706,
      "grad_norm": 15.955587387084961,
      "learning_rate": 5.064705882352942e-06,
      "loss": 0.6815,
      "step": 76390
    },
    {
      "epoch": 4494.117647058823,
      "grad_norm": 30.817516326904297,
      "learning_rate": 5.0588235294117654e-06,
      "loss": 0.6872,
      "step": 76400
    },
    {
      "epoch": 4494.705882352941,
      "grad_norm": 21.59185028076172,
      "learning_rate": 5.0529411764705884e-06,
      "loss": 0.6939,
      "step": 76410
    },
    {
      "epoch": 4495.294117647059,
      "grad_norm": 16.69156837463379,
      "learning_rate": 5.0470588235294114e-06,
      "loss": 0.5139,
      "step": 76420
    },
    {
      "epoch": 4495.882352941177,
      "grad_norm": 19.15717887878418,
      "learning_rate": 5.041176470588235e-06,
      "loss": 0.5697,
      "step": 76430
    },
    {
      "epoch": 4496.470588235294,
      "grad_norm": 16.65659523010254,
      "learning_rate": 5.035294117647059e-06,
      "loss": 0.6567,
      "step": 76440
    },
    {
      "epoch": 4497.058823529412,
      "grad_norm": 20.485628128051758,
      "learning_rate": 5.029411764705882e-06,
      "loss": 0.7394,
      "step": 76450
    },
    {
      "epoch": 4497.64705882353,
      "grad_norm": 17.902666091918945,
      "learning_rate": 5.023529411764706e-06,
      "loss": 0.6688,
      "step": 76460
    },
    {
      "epoch": 4498.235294117647,
      "grad_norm": 15.243324279785156,
      "learning_rate": 5.01764705882353e-06,
      "loss": 0.631,
      "step": 76470
    },
    {
      "epoch": 4498.823529411765,
      "grad_norm": 29.211294174194336,
      "learning_rate": 5.011764705882354e-06,
      "loss": 0.7494,
      "step": 76480
    },
    {
      "epoch": 4499.411764705882,
      "grad_norm": 25.745458602905273,
      "learning_rate": 5.0058823529411766e-06,
      "loss": 0.6782,
      "step": 76490
    },
    {
      "epoch": 4500.0,
      "grad_norm": 25.993932723999023,
      "learning_rate": 5e-06,
      "loss": 0.6541,
      "step": 76500
    },
    {
      "epoch": 4500.588235294118,
      "grad_norm": 24.178390502929688,
      "learning_rate": 4.994117647058823e-06,
      "loss": 0.6542,
      "step": 76510
    },
    {
      "epoch": 4501.176470588235,
      "grad_norm": 17.68684959411621,
      "learning_rate": 4.988235294117647e-06,
      "loss": 0.61,
      "step": 76520
    },
    {
      "epoch": 4501.764705882353,
      "grad_norm": 22.395605087280273,
      "learning_rate": 4.98235294117647e-06,
      "loss": 0.6856,
      "step": 76530
    },
    {
      "epoch": 4502.35294117647,
      "grad_norm": 20.462770462036133,
      "learning_rate": 4.976470588235294e-06,
      "loss": 0.7743,
      "step": 76540
    },
    {
      "epoch": 4502.941176470588,
      "grad_norm": 17.993907928466797,
      "learning_rate": 4.970588235294118e-06,
      "loss": 0.6882,
      "step": 76550
    },
    {
      "epoch": 4503.529411764706,
      "grad_norm": 23.912736892700195,
      "learning_rate": 4.964705882352942e-06,
      "loss": 0.6609,
      "step": 76560
    },
    {
      "epoch": 4504.117647058823,
      "grad_norm": 19.40883445739746,
      "learning_rate": 4.958823529411765e-06,
      "loss": 0.6459,
      "step": 76570
    },
    {
      "epoch": 4504.705882352941,
      "grad_norm": 16.292003631591797,
      "learning_rate": 4.9529411764705885e-06,
      "loss": 0.6378,
      "step": 76580
    },
    {
      "epoch": 4505.294117647059,
      "grad_norm": 26.401798248291016,
      "learning_rate": 4.947058823529412e-06,
      "loss": 0.702,
      "step": 76590
    },
    {
      "epoch": 4505.882352941177,
      "grad_norm": 17.201250076293945,
      "learning_rate": 4.941176470588235e-06,
      "loss": 0.6289,
      "step": 76600
    },
    {
      "epoch": 4506.470588235294,
      "grad_norm": 19.843347549438477,
      "learning_rate": 4.935294117647059e-06,
      "loss": 0.6481,
      "step": 76610
    },
    {
      "epoch": 4507.058823529412,
      "grad_norm": 20.582523345947266,
      "learning_rate": 4.929411764705882e-06,
      "loss": 0.7747,
      "step": 76620
    },
    {
      "epoch": 4507.64705882353,
      "grad_norm": 15.040810585021973,
      "learning_rate": 4.923529411764706e-06,
      "loss": 0.6733,
      "step": 76630
    },
    {
      "epoch": 4508.235294117647,
      "grad_norm": 19.95775032043457,
      "learning_rate": 4.91764705882353e-06,
      "loss": 0.7768,
      "step": 76640
    },
    {
      "epoch": 4508.823529411765,
      "grad_norm": 19.925378799438477,
      "learning_rate": 4.911764705882353e-06,
      "loss": 0.7883,
      "step": 76650
    },
    {
      "epoch": 4509.411764705882,
      "grad_norm": 16.719219207763672,
      "learning_rate": 4.905882352941177e-06,
      "loss": 0.7475,
      "step": 76660
    },
    {
      "epoch": 4510.0,
      "grad_norm": 29.444936752319336,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.6769,
      "step": 76670
    },
    {
      "epoch": 4510.588235294118,
      "grad_norm": 21.271764755249023,
      "learning_rate": 4.894117647058824e-06,
      "loss": 0.7509,
      "step": 76680
    },
    {
      "epoch": 4511.176470588235,
      "grad_norm": 20.47981834411621,
      "learning_rate": 4.888235294117647e-06,
      "loss": 0.6246,
      "step": 76690
    },
    {
      "epoch": 4511.764705882353,
      "grad_norm": 23.957963943481445,
      "learning_rate": 4.88235294117647e-06,
      "loss": 0.6997,
      "step": 76700
    },
    {
      "epoch": 4512.35294117647,
      "grad_norm": 25.43058204650879,
      "learning_rate": 4.876470588235294e-06,
      "loss": 0.7295,
      "step": 76710
    },
    {
      "epoch": 4512.941176470588,
      "grad_norm": 25.1601619720459,
      "learning_rate": 4.870588235294118e-06,
      "loss": 0.6844,
      "step": 76720
    },
    {
      "epoch": 4513.529411764706,
      "grad_norm": 23.18813133239746,
      "learning_rate": 4.864705882352941e-06,
      "loss": 0.6635,
      "step": 76730
    },
    {
      "epoch": 4514.117647058823,
      "grad_norm": 19.04612922668457,
      "learning_rate": 4.858823529411765e-06,
      "loss": 0.6527,
      "step": 76740
    },
    {
      "epoch": 4514.705882352941,
      "grad_norm": 27.175647735595703,
      "learning_rate": 4.852941176470589e-06,
      "loss": 0.7266,
      "step": 76750
    },
    {
      "epoch": 4515.294117647059,
      "grad_norm": 18.867963790893555,
      "learning_rate": 4.8470588235294125e-06,
      "loss": 0.6823,
      "step": 76760
    },
    {
      "epoch": 4515.882352941177,
      "grad_norm": 15.345388412475586,
      "learning_rate": 4.8411764705882355e-06,
      "loss": 0.6829,
      "step": 76770
    },
    {
      "epoch": 4516.470588235294,
      "grad_norm": 18.704349517822266,
      "learning_rate": 4.835294117647059e-06,
      "loss": 0.542,
      "step": 76780
    },
    {
      "epoch": 4517.058823529412,
      "grad_norm": 25.223752975463867,
      "learning_rate": 4.829411764705882e-06,
      "loss": 0.7463,
      "step": 76790
    },
    {
      "epoch": 4517.64705882353,
      "grad_norm": 22.91250228881836,
      "learning_rate": 4.823529411764706e-06,
      "loss": 0.7408,
      "step": 76800
    },
    {
      "epoch": 4518.235294117647,
      "grad_norm": 15.730995178222656,
      "learning_rate": 4.817647058823529e-06,
      "loss": 0.6142,
      "step": 76810
    },
    {
      "epoch": 4518.823529411765,
      "grad_norm": 21.53192901611328,
      "learning_rate": 4.811764705882353e-06,
      "loss": 0.6673,
      "step": 76820
    },
    {
      "epoch": 4519.411764705882,
      "grad_norm": 21.592750549316406,
      "learning_rate": 4.805882352941177e-06,
      "loss": 0.7186,
      "step": 76830
    },
    {
      "epoch": 4520.0,
      "grad_norm": 18.859865188598633,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.6442,
      "step": 76840
    },
    {
      "epoch": 4520.588235294118,
      "grad_norm": 22.897979736328125,
      "learning_rate": 4.794117647058824e-06,
      "loss": 0.7307,
      "step": 76850
    },
    {
      "epoch": 4521.176470588235,
      "grad_norm": 23.550689697265625,
      "learning_rate": 4.7882352941176475e-06,
      "loss": 0.7061,
      "step": 76860
    },
    {
      "epoch": 4521.764705882353,
      "grad_norm": 24.37902069091797,
      "learning_rate": 4.782352941176471e-06,
      "loss": 0.6778,
      "step": 76870
    },
    {
      "epoch": 4522.35294117647,
      "grad_norm": 18.581090927124023,
      "learning_rate": 4.776470588235294e-06,
      "loss": 0.6972,
      "step": 76880
    },
    {
      "epoch": 4522.941176470588,
      "grad_norm": 17.046348571777344,
      "learning_rate": 4.770588235294117e-06,
      "loss": 0.6032,
      "step": 76890
    },
    {
      "epoch": 4523.529411764706,
      "grad_norm": 26.222131729125977,
      "learning_rate": 4.764705882352941e-06,
      "loss": 0.6571,
      "step": 76900
    },
    {
      "epoch": 4524.117647058823,
      "grad_norm": 20.159826278686523,
      "learning_rate": 4.758823529411765e-06,
      "loss": 0.6554,
      "step": 76910
    },
    {
      "epoch": 4524.705882352941,
      "grad_norm": 24.74380874633789,
      "learning_rate": 4.752941176470589e-06,
      "loss": 0.7274,
      "step": 76920
    },
    {
      "epoch": 4525.294117647059,
      "grad_norm": 14.703670501708984,
      "learning_rate": 4.747058823529412e-06,
      "loss": 0.6189,
      "step": 76930
    },
    {
      "epoch": 4525.882352941177,
      "grad_norm": 22.47896385192871,
      "learning_rate": 4.741176470588236e-06,
      "loss": 0.6127,
      "step": 76940
    },
    {
      "epoch": 4526.470588235294,
      "grad_norm": 15.937638282775879,
      "learning_rate": 4.7352941176470594e-06,
      "loss": 0.6553,
      "step": 76950
    },
    {
      "epoch": 4527.058823529412,
      "grad_norm": 25.630956649780273,
      "learning_rate": 4.729411764705883e-06,
      "loss": 0.6777,
      "step": 76960
    },
    {
      "epoch": 4527.64705882353,
      "grad_norm": 21.266138076782227,
      "learning_rate": 4.723529411764705e-06,
      "loss": 0.6258,
      "step": 76970
    },
    {
      "epoch": 4528.235294117647,
      "grad_norm": 21.808155059814453,
      "learning_rate": 4.717647058823529e-06,
      "loss": 0.6758,
      "step": 76980
    },
    {
      "epoch": 4528.823529411765,
      "grad_norm": 14.959704399108887,
      "learning_rate": 4.711764705882353e-06,
      "loss": 0.6475,
      "step": 76990
    },
    {
      "epoch": 4529.411764705882,
      "grad_norm": 20.070619583129883,
      "learning_rate": 4.705882352941177e-06,
      "loss": 0.6521,
      "step": 77000
    },
    {
      "epoch": 4530.0,
      "grad_norm": 16.249649047851562,
      "learning_rate": 4.7e-06,
      "loss": 0.7388,
      "step": 77010
    },
    {
      "epoch": 4530.588235294118,
      "grad_norm": 18.707992553710938,
      "learning_rate": 4.694117647058824e-06,
      "loss": 0.5958,
      "step": 77020
    },
    {
      "epoch": 4531.176470588235,
      "grad_norm": 18.530887603759766,
      "learning_rate": 4.6882352941176476e-06,
      "loss": 0.7656,
      "step": 77030
    },
    {
      "epoch": 4531.764705882353,
      "grad_norm": 21.79921531677246,
      "learning_rate": 4.682352941176471e-06,
      "loss": 0.6278,
      "step": 77040
    },
    {
      "epoch": 4532.35294117647,
      "grad_norm": 17.505359649658203,
      "learning_rate": 4.676470588235294e-06,
      "loss": 0.6568,
      "step": 77050
    },
    {
      "epoch": 4532.941176470588,
      "grad_norm": 20.12024688720703,
      "learning_rate": 4.670588235294117e-06,
      "loss": 0.6281,
      "step": 77060
    },
    {
      "epoch": 4533.529411764706,
      "grad_norm": 22.58823013305664,
      "learning_rate": 4.664705882352941e-06,
      "loss": 0.6773,
      "step": 77070
    },
    {
      "epoch": 4534.117647058823,
      "grad_norm": 14.388566017150879,
      "learning_rate": 4.658823529411765e-06,
      "loss": 0.6141,
      "step": 77080
    },
    {
      "epoch": 4534.705882352941,
      "grad_norm": 17.18194580078125,
      "learning_rate": 4.652941176470588e-06,
      "loss": 0.6234,
      "step": 77090
    },
    {
      "epoch": 4535.294117647059,
      "grad_norm": 12.626830101013184,
      "learning_rate": 4.647058823529412e-06,
      "loss": 0.7387,
      "step": 77100
    },
    {
      "epoch": 4535.882352941177,
      "grad_norm": 17.605953216552734,
      "learning_rate": 4.641176470588236e-06,
      "loss": 0.6647,
      "step": 77110
    },
    {
      "epoch": 4536.470588235294,
      "grad_norm": 20.99357795715332,
      "learning_rate": 4.6352941176470595e-06,
      "loss": 0.6929,
      "step": 77120
    },
    {
      "epoch": 4537.058823529412,
      "grad_norm": 16.77127456665039,
      "learning_rate": 4.6294117647058825e-06,
      "loss": 0.7265,
      "step": 77130
    },
    {
      "epoch": 4537.64705882353,
      "grad_norm": 16.34929084777832,
      "learning_rate": 4.623529411764706e-06,
      "loss": 0.7255,
      "step": 77140
    },
    {
      "epoch": 4538.235294117647,
      "grad_norm": 26.860519409179688,
      "learning_rate": 4.617647058823529e-06,
      "loss": 0.6187,
      "step": 77150
    },
    {
      "epoch": 4538.823529411765,
      "grad_norm": 21.369142532348633,
      "learning_rate": 4.611764705882353e-06,
      "loss": 0.6934,
      "step": 77160
    },
    {
      "epoch": 4539.411764705882,
      "grad_norm": 21.742753982543945,
      "learning_rate": 4.605882352941176e-06,
      "loss": 0.7331,
      "step": 77170
    },
    {
      "epoch": 4540.0,
      "grad_norm": 29.501161575317383,
      "learning_rate": 4.6e-06,
      "loss": 0.6441,
      "step": 77180
    },
    {
      "epoch": 4540.588235294118,
      "grad_norm": 26.40095329284668,
      "learning_rate": 4.594117647058824e-06,
      "loss": 0.675,
      "step": 77190
    },
    {
      "epoch": 4541.176470588235,
      "grad_norm": 21.24917984008789,
      "learning_rate": 4.588235294117648e-06,
      "loss": 0.6645,
      "step": 77200
    },
    {
      "epoch": 4541.764705882353,
      "grad_norm": 18.22720718383789,
      "learning_rate": 4.582352941176471e-06,
      "loss": 0.6737,
      "step": 77210
    },
    {
      "epoch": 4542.35294117647,
      "grad_norm": 15.544368743896484,
      "learning_rate": 4.5764705882352945e-06,
      "loss": 0.6778,
      "step": 77220
    },
    {
      "epoch": 4542.941176470588,
      "grad_norm": 17.719200134277344,
      "learning_rate": 4.570588235294118e-06,
      "loss": 0.6411,
      "step": 77230
    },
    {
      "epoch": 4543.529411764706,
      "grad_norm": 17.729122161865234,
      "learning_rate": 4.564705882352941e-06,
      "loss": 0.6692,
      "step": 77240
    },
    {
      "epoch": 4544.117647058823,
      "grad_norm": 16.66547966003418,
      "learning_rate": 4.558823529411764e-06,
      "loss": 0.6794,
      "step": 77250
    },
    {
      "epoch": 4544.705882352941,
      "grad_norm": 20.209936141967773,
      "learning_rate": 4.552941176470588e-06,
      "loss": 0.5849,
      "step": 77260
    },
    {
      "epoch": 4545.294117647059,
      "grad_norm": 17.55440330505371,
      "learning_rate": 4.547058823529412e-06,
      "loss": 0.595,
      "step": 77270
    },
    {
      "epoch": 4545.882352941177,
      "grad_norm": 15.070162773132324,
      "learning_rate": 4.541176470588236e-06,
      "loss": 0.681,
      "step": 77280
    },
    {
      "epoch": 4546.470588235294,
      "grad_norm": 21.356550216674805,
      "learning_rate": 4.535294117647059e-06,
      "loss": 0.6082,
      "step": 77290
    },
    {
      "epoch": 4547.058823529412,
      "grad_norm": 12.37132740020752,
      "learning_rate": 4.529411764705883e-06,
      "loss": 0.6899,
      "step": 77300
    },
    {
      "epoch": 4547.64705882353,
      "grad_norm": 16.604610443115234,
      "learning_rate": 4.5235294117647065e-06,
      "loss": 0.6835,
      "step": 77310
    },
    {
      "epoch": 4548.235294117647,
      "grad_norm": 18.790311813354492,
      "learning_rate": 4.51764705882353e-06,
      "loss": 0.6524,
      "step": 77320
    },
    {
      "epoch": 4548.823529411765,
      "grad_norm": 21.040523529052734,
      "learning_rate": 4.5117647058823524e-06,
      "loss": 0.6971,
      "step": 77330
    },
    {
      "epoch": 4549.411764705882,
      "grad_norm": 16.573558807373047,
      "learning_rate": 4.505882352941176e-06,
      "loss": 0.7379,
      "step": 77340
    },
    {
      "epoch": 4550.0,
      "grad_norm": 36.8937873840332,
      "learning_rate": 4.5e-06,
      "loss": 0.6186,
      "step": 77350
    },
    {
      "epoch": 4550.588235294118,
      "grad_norm": 19.996362686157227,
      "learning_rate": 4.494117647058824e-06,
      "loss": 0.6395,
      "step": 77360
    },
    {
      "epoch": 4551.176470588235,
      "grad_norm": 22.75899887084961,
      "learning_rate": 4.488235294117647e-06,
      "loss": 0.7166,
      "step": 77370
    },
    {
      "epoch": 4551.764705882353,
      "grad_norm": 24.70992660522461,
      "learning_rate": 4.482352941176471e-06,
      "loss": 0.6531,
      "step": 77380
    },
    {
      "epoch": 4552.35294117647,
      "grad_norm": 19.10613441467285,
      "learning_rate": 4.476470588235295e-06,
      "loss": 0.7465,
      "step": 77390
    },
    {
      "epoch": 4552.941176470588,
      "grad_norm": 19.126850128173828,
      "learning_rate": 4.4705882352941184e-06,
      "loss": 0.599,
      "step": 77400
    },
    {
      "epoch": 4553.529411764706,
      "grad_norm": 29.469392776489258,
      "learning_rate": 4.4647058823529414e-06,
      "loss": 0.649,
      "step": 77410
    },
    {
      "epoch": 4554.117647058823,
      "grad_norm": 17.860658645629883,
      "learning_rate": 4.458823529411764e-06,
      "loss": 0.6138,
      "step": 77420
    },
    {
      "epoch": 4554.705882352941,
      "grad_norm": 13.360804557800293,
      "learning_rate": 4.452941176470588e-06,
      "loss": 0.5806,
      "step": 77430
    },
    {
      "epoch": 4555.294117647059,
      "grad_norm": 16.936025619506836,
      "learning_rate": 4.447058823529412e-06,
      "loss": 0.7415,
      "step": 77440
    },
    {
      "epoch": 4555.882352941177,
      "grad_norm": 21.97136878967285,
      "learning_rate": 4.441176470588235e-06,
      "loss": 0.63,
      "step": 77450
    },
    {
      "epoch": 4556.470588235294,
      "grad_norm": 20.231639862060547,
      "learning_rate": 4.435294117647059e-06,
      "loss": 0.6135,
      "step": 77460
    },
    {
      "epoch": 4557.058823529412,
      "grad_norm": 19.088302612304688,
      "learning_rate": 4.429411764705883e-06,
      "loss": 0.6118,
      "step": 77470
    },
    {
      "epoch": 4557.64705882353,
      "grad_norm": 19.07181739807129,
      "learning_rate": 4.423529411764707e-06,
      "loss": 0.7274,
      "step": 77480
    },
    {
      "epoch": 4558.235294117647,
      "grad_norm": 14.42785358428955,
      "learning_rate": 4.4176470588235296e-06,
      "loss": 0.7203,
      "step": 77490
    },
    {
      "epoch": 4558.823529411765,
      "grad_norm": 34.33103942871094,
      "learning_rate": 4.411764705882353e-06,
      "loss": 0.6423,
      "step": 77500
    },
    {
      "epoch": 4559.411764705882,
      "grad_norm": 24.787160873413086,
      "learning_rate": 4.405882352941176e-06,
      "loss": 0.6664,
      "step": 77510
    },
    {
      "epoch": 4560.0,
      "grad_norm": 20.41029167175293,
      "learning_rate": 4.4e-06,
      "loss": 0.6921,
      "step": 77520
    },
    {
      "epoch": 4560.588235294118,
      "grad_norm": 21.022357940673828,
      "learning_rate": 4.394117647058823e-06,
      "loss": 0.6753,
      "step": 77530
    },
    {
      "epoch": 4561.176470588235,
      "grad_norm": 18.37657928466797,
      "learning_rate": 4.388235294117647e-06,
      "loss": 0.7088,
      "step": 77540
    },
    {
      "epoch": 4561.764705882353,
      "grad_norm": 19.516828536987305,
      "learning_rate": 4.382352941176471e-06,
      "loss": 0.8175,
      "step": 77550
    },
    {
      "epoch": 4562.35294117647,
      "grad_norm": 19.71351432800293,
      "learning_rate": 4.376470588235295e-06,
      "loss": 0.5966,
      "step": 77560
    },
    {
      "epoch": 4562.941176470588,
      "grad_norm": 20.187366485595703,
      "learning_rate": 4.370588235294118e-06,
      "loss": 0.7501,
      "step": 77570
    },
    {
      "epoch": 4563.529411764706,
      "grad_norm": 13.338044166564941,
      "learning_rate": 4.3647058823529415e-06,
      "loss": 0.6803,
      "step": 77580
    },
    {
      "epoch": 4564.117647058823,
      "grad_norm": 27.93821907043457,
      "learning_rate": 4.358823529411765e-06,
      "loss": 0.6818,
      "step": 77590
    },
    {
      "epoch": 4564.705882352941,
      "grad_norm": 17.851909637451172,
      "learning_rate": 4.352941176470588e-06,
      "loss": 0.6268,
      "step": 77600
    },
    {
      "epoch": 4565.294117647059,
      "grad_norm": 26.01736068725586,
      "learning_rate": 4.347058823529411e-06,
      "loss": 0.6206,
      "step": 77610
    },
    {
      "epoch": 4565.882352941177,
      "grad_norm": 18.242698669433594,
      "learning_rate": 4.341176470588235e-06,
      "loss": 0.6597,
      "step": 77620
    },
    {
      "epoch": 4566.470588235294,
      "grad_norm": 17.44048500061035,
      "learning_rate": 4.335294117647059e-06,
      "loss": 0.5906,
      "step": 77630
    },
    {
      "epoch": 4567.058823529412,
      "grad_norm": 22.586641311645508,
      "learning_rate": 4.329411764705883e-06,
      "loss": 0.7187,
      "step": 77640
    },
    {
      "epoch": 4567.64705882353,
      "grad_norm": 23.384517669677734,
      "learning_rate": 4.323529411764706e-06,
      "loss": 0.6774,
      "step": 77650
    },
    {
      "epoch": 4568.235294117647,
      "grad_norm": 21.143085479736328,
      "learning_rate": 4.31764705882353e-06,
      "loss": 0.6872,
      "step": 77660
    },
    {
      "epoch": 4568.823529411765,
      "grad_norm": 17.698049545288086,
      "learning_rate": 4.3117647058823535e-06,
      "loss": 0.6361,
      "step": 77670
    },
    {
      "epoch": 4569.411764705882,
      "grad_norm": 17.96908950805664,
      "learning_rate": 4.305882352941177e-06,
      "loss": 0.5883,
      "step": 77680
    },
    {
      "epoch": 4570.0,
      "grad_norm": 19.423519134521484,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.6118,
      "step": 77690
    },
    {
      "epoch": 4570.588235294118,
      "grad_norm": 19.079362869262695,
      "learning_rate": 4.294117647058823e-06,
      "loss": 0.6824,
      "step": 77700
    },
    {
      "epoch": 4571.176470588235,
      "grad_norm": 21.456350326538086,
      "learning_rate": 4.288235294117647e-06,
      "loss": 0.6863,
      "step": 77710
    },
    {
      "epoch": 4571.764705882353,
      "grad_norm": 23.009111404418945,
      "learning_rate": 4.282352941176471e-06,
      "loss": 0.6101,
      "step": 77720
    },
    {
      "epoch": 4572.35294117647,
      "grad_norm": 21.191328048706055,
      "learning_rate": 4.276470588235294e-06,
      "loss": 0.6182,
      "step": 77730
    },
    {
      "epoch": 4572.941176470588,
      "grad_norm": 15.178473472595215,
      "learning_rate": 4.270588235294118e-06,
      "loss": 0.691,
      "step": 77740
    },
    {
      "epoch": 4573.529411764706,
      "grad_norm": 17.85072135925293,
      "learning_rate": 4.264705882352942e-06,
      "loss": 0.7135,
      "step": 77750
    },
    {
      "epoch": 4574.117647058823,
      "grad_norm": 20.22771644592285,
      "learning_rate": 4.2588235294117655e-06,
      "loss": 0.6992,
      "step": 77760
    },
    {
      "epoch": 4574.705882352941,
      "grad_norm": 20.512367248535156,
      "learning_rate": 4.2529411764705885e-06,
      "loss": 0.674,
      "step": 77770
    },
    {
      "epoch": 4575.294117647059,
      "grad_norm": 20.867074966430664,
      "learning_rate": 4.247058823529412e-06,
      "loss": 0.7427,
      "step": 77780
    },
    {
      "epoch": 4575.882352941177,
      "grad_norm": 23.742347717285156,
      "learning_rate": 4.241176470588235e-06,
      "loss": 0.6622,
      "step": 77790
    },
    {
      "epoch": 4576.470588235294,
      "grad_norm": 27.54064178466797,
      "learning_rate": 4.235294117647059e-06,
      "loss": 0.6259,
      "step": 77800
    },
    {
      "epoch": 4577.058823529412,
      "grad_norm": 23.504770278930664,
      "learning_rate": 4.229411764705882e-06,
      "loss": 0.642,
      "step": 77810
    },
    {
      "epoch": 4577.64705882353,
      "grad_norm": 30.33664321899414,
      "learning_rate": 4.223529411764706e-06,
      "loss": 0.6679,
      "step": 77820
    },
    {
      "epoch": 4578.235294117647,
      "grad_norm": 18.40439796447754,
      "learning_rate": 4.21764705882353e-06,
      "loss": 0.6508,
      "step": 77830
    },
    {
      "epoch": 4578.823529411765,
      "grad_norm": 20.99951171875,
      "learning_rate": 4.211764705882354e-06,
      "loss": 0.6899,
      "step": 77840
    },
    {
      "epoch": 4579.411764705882,
      "grad_norm": 21.741300582885742,
      "learning_rate": 4.205882352941177e-06,
      "loss": 0.6639,
      "step": 77850
    },
    {
      "epoch": 4580.0,
      "grad_norm": 29.830711364746094,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.7673,
      "step": 77860
    },
    {
      "epoch": 4580.588235294118,
      "grad_norm": 21.088045120239258,
      "learning_rate": 4.194117647058824e-06,
      "loss": 0.5687,
      "step": 77870
    },
    {
      "epoch": 4581.176470588235,
      "grad_norm": 21.327409744262695,
      "learning_rate": 4.188235294117647e-06,
      "loss": 0.7026,
      "step": 77880
    },
    {
      "epoch": 4581.764705882353,
      "grad_norm": 19.427093505859375,
      "learning_rate": 4.18235294117647e-06,
      "loss": 0.6371,
      "step": 77890
    },
    {
      "epoch": 4582.35294117647,
      "grad_norm": 14.214592933654785,
      "learning_rate": 4.176470588235294e-06,
      "loss": 0.5732,
      "step": 77900
    },
    {
      "epoch": 4582.941176470588,
      "grad_norm": 21.528989791870117,
      "learning_rate": 4.170588235294118e-06,
      "loss": 0.6829,
      "step": 77910
    },
    {
      "epoch": 4583.529411764706,
      "grad_norm": 20.067859649658203,
      "learning_rate": 4.164705882352942e-06,
      "loss": 0.7551,
      "step": 77920
    },
    {
      "epoch": 4584.117647058823,
      "grad_norm": 18.52799415588379,
      "learning_rate": 4.158823529411765e-06,
      "loss": 0.7358,
      "step": 77930
    },
    {
      "epoch": 4584.705882352941,
      "grad_norm": 22.252975463867188,
      "learning_rate": 4.152941176470589e-06,
      "loss": 0.6763,
      "step": 77940
    },
    {
      "epoch": 4585.294117647059,
      "grad_norm": 16.751998901367188,
      "learning_rate": 4.147058823529412e-06,
      "loss": 0.6491,
      "step": 77950
    },
    {
      "epoch": 4585.882352941177,
      "grad_norm": 17.896257400512695,
      "learning_rate": 4.141176470588235e-06,
      "loss": 0.6057,
      "step": 77960
    },
    {
      "epoch": 4586.470588235294,
      "grad_norm": 11.410886764526367,
      "learning_rate": 4.135294117647058e-06,
      "loss": 0.6102,
      "step": 77970
    },
    {
      "epoch": 4587.058823529412,
      "grad_norm": 25.20159339904785,
      "learning_rate": 4.129411764705882e-06,
      "loss": 0.6162,
      "step": 77980
    },
    {
      "epoch": 4587.64705882353,
      "grad_norm": 20.318090438842773,
      "learning_rate": 4.123529411764706e-06,
      "loss": 0.7561,
      "step": 77990
    },
    {
      "epoch": 4588.235294117647,
      "grad_norm": 20.87317657470703,
      "learning_rate": 4.11764705882353e-06,
      "loss": 0.6768,
      "step": 78000
    },
    {
      "epoch": 4588.823529411765,
      "grad_norm": 19.1638240814209,
      "learning_rate": 4.111764705882353e-06,
      "loss": 0.6192,
      "step": 78010
    },
    {
      "epoch": 4589.411764705882,
      "grad_norm": 22.548734664916992,
      "learning_rate": 4.105882352941177e-06,
      "loss": 0.6771,
      "step": 78020
    },
    {
      "epoch": 4590.0,
      "grad_norm": 18.716415405273438,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.6527,
      "step": 78030
    },
    {
      "epoch": 4590.588235294118,
      "grad_norm": 17.431108474731445,
      "learning_rate": 4.094117647058824e-06,
      "loss": 0.6693,
      "step": 78040
    },
    {
      "epoch": 4591.176470588235,
      "grad_norm": 20.993825912475586,
      "learning_rate": 4.088235294117647e-06,
      "loss": 0.7147,
      "step": 78050
    },
    {
      "epoch": 4591.764705882353,
      "grad_norm": 16.356103897094727,
      "learning_rate": 4.08235294117647e-06,
      "loss": 0.6882,
      "step": 78060
    },
    {
      "epoch": 4592.35294117647,
      "grad_norm": 23.346839904785156,
      "learning_rate": 4.076470588235294e-06,
      "loss": 0.7145,
      "step": 78070
    },
    {
      "epoch": 4592.941176470588,
      "grad_norm": 18.406787872314453,
      "learning_rate": 4.070588235294118e-06,
      "loss": 0.64,
      "step": 78080
    },
    {
      "epoch": 4593.529411764706,
      "grad_norm": 18.45808982849121,
      "learning_rate": 4.064705882352941e-06,
      "loss": 0.7101,
      "step": 78090
    },
    {
      "epoch": 4594.117647058823,
      "grad_norm": 23.056081771850586,
      "learning_rate": 4.058823529411765e-06,
      "loss": 0.7434,
      "step": 78100
    },
    {
      "epoch": 4594.705882352941,
      "grad_norm": 14.014982223510742,
      "learning_rate": 4.052941176470589e-06,
      "loss": 0.6163,
      "step": 78110
    },
    {
      "epoch": 4595.294117647059,
      "grad_norm": 25.546131134033203,
      "learning_rate": 4.0470588235294125e-06,
      "loss": 0.6423,
      "step": 78120
    },
    {
      "epoch": 4595.882352941177,
      "grad_norm": 24.119394302368164,
      "learning_rate": 4.0411764705882355e-06,
      "loss": 0.6949,
      "step": 78130
    },
    {
      "epoch": 4596.470588235294,
      "grad_norm": 16.276954650878906,
      "learning_rate": 4.035294117647059e-06,
      "loss": 0.6792,
      "step": 78140
    },
    {
      "epoch": 4597.058823529412,
      "grad_norm": 10.274131774902344,
      "learning_rate": 4.029411764705882e-06,
      "loss": 0.6861,
      "step": 78150
    },
    {
      "epoch": 4597.64705882353,
      "grad_norm": 22.1119441986084,
      "learning_rate": 4.023529411764706e-06,
      "loss": 0.6968,
      "step": 78160
    },
    {
      "epoch": 4598.235294117647,
      "grad_norm": 19.74462127685547,
      "learning_rate": 4.017647058823529e-06,
      "loss": 0.6912,
      "step": 78170
    },
    {
      "epoch": 4598.823529411765,
      "grad_norm": 20.164278030395508,
      "learning_rate": 4.011764705882353e-06,
      "loss": 0.5952,
      "step": 78180
    },
    {
      "epoch": 4599.411764705882,
      "grad_norm": 31.46984100341797,
      "learning_rate": 4.005882352941177e-06,
      "loss": 0.6359,
      "step": 78190
    },
    {
      "epoch": 4600.0,
      "grad_norm": 21.69554328918457,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.6894,
      "step": 78200
    },
    {
      "epoch": 4600.588235294118,
      "grad_norm": 16.893327713012695,
      "learning_rate": 3.994117647058824e-06,
      "loss": 0.7098,
      "step": 78210
    },
    {
      "epoch": 4601.176470588235,
      "grad_norm": 18.49802589416504,
      "learning_rate": 3.9882352941176475e-06,
      "loss": 0.6781,
      "step": 78220
    },
    {
      "epoch": 4601.764705882353,
      "grad_norm": 14.528182029724121,
      "learning_rate": 3.982352941176471e-06,
      "loss": 0.6082,
      "step": 78230
    },
    {
      "epoch": 4602.35294117647,
      "grad_norm": 19.64234161376953,
      "learning_rate": 3.976470588235294e-06,
      "loss": 0.5989,
      "step": 78240
    },
    {
      "epoch": 4602.941176470588,
      "grad_norm": 14.568533897399902,
      "learning_rate": 3.970588235294117e-06,
      "loss": 0.647,
      "step": 78250
    },
    {
      "epoch": 4603.529411764706,
      "grad_norm": 22.5153865814209,
      "learning_rate": 3.964705882352941e-06,
      "loss": 0.6533,
      "step": 78260
    },
    {
      "epoch": 4604.117647058823,
      "grad_norm": 19.805551528930664,
      "learning_rate": 3.958823529411765e-06,
      "loss": 0.7136,
      "step": 78270
    },
    {
      "epoch": 4604.705882352941,
      "grad_norm": 19.2856502532959,
      "learning_rate": 3.952941176470589e-06,
      "loss": 0.7735,
      "step": 78280
    },
    {
      "epoch": 4605.294117647059,
      "grad_norm": 23.170089721679688,
      "learning_rate": 3.947058823529412e-06,
      "loss": 0.5814,
      "step": 78290
    },
    {
      "epoch": 4605.882352941177,
      "grad_norm": 20.463775634765625,
      "learning_rate": 3.941176470588236e-06,
      "loss": 0.6028,
      "step": 78300
    },
    {
      "epoch": 4606.470588235294,
      "grad_norm": 28.782657623291016,
      "learning_rate": 3.9352941176470595e-06,
      "loss": 0.7536,
      "step": 78310
    },
    {
      "epoch": 4607.058823529412,
      "grad_norm": 22.028318405151367,
      "learning_rate": 3.9294117647058824e-06,
      "loss": 0.6756,
      "step": 78320
    },
    {
      "epoch": 4607.64705882353,
      "grad_norm": 17.166088104248047,
      "learning_rate": 3.9235294117647054e-06,
      "loss": 0.6896,
      "step": 78330
    },
    {
      "epoch": 4608.235294117647,
      "grad_norm": 20.39141082763672,
      "learning_rate": 3.917647058823529e-06,
      "loss": 0.5674,
      "step": 78340
    },
    {
      "epoch": 4608.823529411765,
      "grad_norm": 22.324743270874023,
      "learning_rate": 3.911764705882353e-06,
      "loss": 0.7122,
      "step": 78350
    },
    {
      "epoch": 4609.411764705882,
      "grad_norm": 19.626468658447266,
      "learning_rate": 3.905882352941177e-06,
      "loss": 0.7104,
      "step": 78360
    },
    {
      "epoch": 4610.0,
      "grad_norm": 22.222333908081055,
      "learning_rate": 3.9e-06,
      "loss": 0.6444,
      "step": 78370
    },
    {
      "epoch": 4610.588235294118,
      "grad_norm": 19.112241744995117,
      "learning_rate": 3.894117647058824e-06,
      "loss": 0.6337,
      "step": 78380
    },
    {
      "epoch": 4611.176470588235,
      "grad_norm": 20.76026153564453,
      "learning_rate": 3.888235294117648e-06,
      "loss": 0.7977,
      "step": 78390
    },
    {
      "epoch": 4611.764705882353,
      "grad_norm": 16.650222778320312,
      "learning_rate": 3.882352941176471e-06,
      "loss": 0.5673,
      "step": 78400
    },
    {
      "epoch": 4612.35294117647,
      "grad_norm": 19.37076187133789,
      "learning_rate": 3.876470588235294e-06,
      "loss": 0.6775,
      "step": 78410
    },
    {
      "epoch": 4612.941176470588,
      "grad_norm": 23.581512451171875,
      "learning_rate": 3.870588235294117e-06,
      "loss": 0.6709,
      "step": 78420
    },
    {
      "epoch": 4613.529411764706,
      "grad_norm": 23.186595916748047,
      "learning_rate": 3.864705882352941e-06,
      "loss": 0.7052,
      "step": 78430
    },
    {
      "epoch": 4614.117647058823,
      "grad_norm": 23.561906814575195,
      "learning_rate": 3.858823529411765e-06,
      "loss": 0.6622,
      "step": 78440
    },
    {
      "epoch": 4614.705882352941,
      "grad_norm": 17.71864128112793,
      "learning_rate": 3.852941176470588e-06,
      "loss": 0.6928,
      "step": 78450
    },
    {
      "epoch": 4615.294117647059,
      "grad_norm": 12.537312507629395,
      "learning_rate": 3.847058823529412e-06,
      "loss": 0.6077,
      "step": 78460
    },
    {
      "epoch": 4615.882352941177,
      "grad_norm": 23.807767868041992,
      "learning_rate": 3.841176470588236e-06,
      "loss": 0.6824,
      "step": 78470
    },
    {
      "epoch": 4616.470588235294,
      "grad_norm": 18.26055335998535,
      "learning_rate": 3.8352941176470596e-06,
      "loss": 0.6958,
      "step": 78480
    },
    {
      "epoch": 4617.058823529412,
      "grad_norm": 20.6617488861084,
      "learning_rate": 3.8294117647058826e-06,
      "loss": 0.7072,
      "step": 78490
    },
    {
      "epoch": 4617.64705882353,
      "grad_norm": 22.89480972290039,
      "learning_rate": 3.823529411764706e-06,
      "loss": 0.6727,
      "step": 78500
    },
    {
      "epoch": 4618.235294117647,
      "grad_norm": 20.747522354125977,
      "learning_rate": 3.817647058823529e-06,
      "loss": 0.7518,
      "step": 78510
    },
    {
      "epoch": 4618.823529411765,
      "grad_norm": 19.286922454833984,
      "learning_rate": 3.811764705882353e-06,
      "loss": 0.736,
      "step": 78520
    },
    {
      "epoch": 4619.411764705882,
      "grad_norm": 18.43977165222168,
      "learning_rate": 3.8058823529411766e-06,
      "loss": 0.7145,
      "step": 78530
    },
    {
      "epoch": 4620.0,
      "grad_norm": 14.284517288208008,
      "learning_rate": 3.8e-06,
      "loss": 0.6812,
      "step": 78540
    },
    {
      "epoch": 4620.588235294118,
      "grad_norm": 16.667043685913086,
      "learning_rate": 3.794117647058824e-06,
      "loss": 0.5001,
      "step": 78550
    },
    {
      "epoch": 4621.176470588235,
      "grad_norm": 24.941905975341797,
      "learning_rate": 3.7882352941176473e-06,
      "loss": 0.6984,
      "step": 78560
    },
    {
      "epoch": 4621.764705882353,
      "grad_norm": 23.204715728759766,
      "learning_rate": 3.7823529411764707e-06,
      "loss": 0.6676,
      "step": 78570
    },
    {
      "epoch": 4622.35294117647,
      "grad_norm": 19.04815673828125,
      "learning_rate": 3.7764705882352945e-06,
      "loss": 0.7123,
      "step": 78580
    },
    {
      "epoch": 4622.941176470588,
      "grad_norm": 23.41701316833496,
      "learning_rate": 3.770588235294118e-06,
      "loss": 0.6463,
      "step": 78590
    },
    {
      "epoch": 4623.529411764706,
      "grad_norm": 31.186384201049805,
      "learning_rate": 3.764705882352941e-06,
      "loss": 0.7297,
      "step": 78600
    },
    {
      "epoch": 4624.117647058823,
      "grad_norm": 20.675413131713867,
      "learning_rate": 3.7588235294117648e-06,
      "loss": 0.6071,
      "step": 78610
    },
    {
      "epoch": 4624.705882352941,
      "grad_norm": 19.914152145385742,
      "learning_rate": 3.752941176470588e-06,
      "loss": 0.6674,
      "step": 78620
    },
    {
      "epoch": 4625.294117647059,
      "grad_norm": 27.561277389526367,
      "learning_rate": 3.747058823529412e-06,
      "loss": 0.7798,
      "step": 78630
    },
    {
      "epoch": 4625.882352941177,
      "grad_norm": 20.888011932373047,
      "learning_rate": 3.7411764705882354e-06,
      "loss": 0.7397,
      "step": 78640
    },
    {
      "epoch": 4626.470588235294,
      "grad_norm": 13.99294376373291,
      "learning_rate": 3.7352941176470593e-06,
      "loss": 0.623,
      "step": 78650
    },
    {
      "epoch": 4627.058823529412,
      "grad_norm": 14.973944664001465,
      "learning_rate": 3.7294117647058827e-06,
      "loss": 0.6646,
      "step": 78660
    },
    {
      "epoch": 4627.64705882353,
      "grad_norm": 20.507183074951172,
      "learning_rate": 3.723529411764706e-06,
      "loss": 0.5774,
      "step": 78670
    },
    {
      "epoch": 4628.235294117647,
      "grad_norm": 15.572371482849121,
      "learning_rate": 3.71764705882353e-06,
      "loss": 0.5956,
      "step": 78680
    },
    {
      "epoch": 4628.823529411765,
      "grad_norm": 17.853261947631836,
      "learning_rate": 3.711764705882353e-06,
      "loss": 0.6323,
      "step": 78690
    },
    {
      "epoch": 4629.411764705882,
      "grad_norm": 17.42315101623535,
      "learning_rate": 3.7058823529411763e-06,
      "loss": 0.6789,
      "step": 78700
    },
    {
      "epoch": 4630.0,
      "grad_norm": 17.87445640563965,
      "learning_rate": 3.7e-06,
      "loss": 0.7439,
      "step": 78710
    },
    {
      "epoch": 4630.588235294118,
      "grad_norm": 26.997159957885742,
      "learning_rate": 3.6941176470588236e-06,
      "loss": 0.6024,
      "step": 78720
    },
    {
      "epoch": 4631.176470588235,
      "grad_norm": 21.320526123046875,
      "learning_rate": 3.6882352941176474e-06,
      "loss": 0.6495,
      "step": 78730
    },
    {
      "epoch": 4631.764705882353,
      "grad_norm": 14.636082649230957,
      "learning_rate": 3.682352941176471e-06,
      "loss": 0.7008,
      "step": 78740
    },
    {
      "epoch": 4632.35294117647,
      "grad_norm": 20.212614059448242,
      "learning_rate": 3.6764705882352942e-06,
      "loss": 0.786,
      "step": 78750
    },
    {
      "epoch": 4632.941176470588,
      "grad_norm": 21.268207550048828,
      "learning_rate": 3.670588235294118e-06,
      "loss": 0.6797,
      "step": 78760
    },
    {
      "epoch": 4633.529411764706,
      "grad_norm": 24.444046020507812,
      "learning_rate": 3.6647058823529415e-06,
      "loss": 0.6693,
      "step": 78770
    },
    {
      "epoch": 4634.117647058823,
      "grad_norm": 18.84099769592285,
      "learning_rate": 3.6588235294117645e-06,
      "loss": 0.6104,
      "step": 78780
    },
    {
      "epoch": 4634.705882352941,
      "grad_norm": 23.27049446105957,
      "learning_rate": 3.6529411764705883e-06,
      "loss": 0.6552,
      "step": 78790
    },
    {
      "epoch": 4635.294117647059,
      "grad_norm": 21.099632263183594,
      "learning_rate": 3.6470588235294117e-06,
      "loss": 0.5771,
      "step": 78800
    },
    {
      "epoch": 4635.882352941177,
      "grad_norm": 20.924474716186523,
      "learning_rate": 3.6411764705882355e-06,
      "loss": 0.695,
      "step": 78810
    },
    {
      "epoch": 4636.470588235294,
      "grad_norm": 21.554075241088867,
      "learning_rate": 3.635294117647059e-06,
      "loss": 0.6889,
      "step": 78820
    },
    {
      "epoch": 4637.058823529412,
      "grad_norm": 16.812469482421875,
      "learning_rate": 3.6294117647058824e-06,
      "loss": 0.6649,
      "step": 78830
    },
    {
      "epoch": 4637.64705882353,
      "grad_norm": 22.47829818725586,
      "learning_rate": 3.623529411764706e-06,
      "loss": 0.7218,
      "step": 78840
    },
    {
      "epoch": 4638.235294117647,
      "grad_norm": 17.995838165283203,
      "learning_rate": 3.6176470588235296e-06,
      "loss": 0.6579,
      "step": 78850
    },
    {
      "epoch": 4638.823529411765,
      "grad_norm": 22.543214797973633,
      "learning_rate": 3.6117647058823534e-06,
      "loss": 0.6943,
      "step": 78860
    },
    {
      "epoch": 4639.411764705882,
      "grad_norm": 19.156131744384766,
      "learning_rate": 3.605882352941177e-06,
      "loss": 0.7245,
      "step": 78870
    },
    {
      "epoch": 4640.0,
      "grad_norm": 16.338470458984375,
      "learning_rate": 3.6e-06,
      "loss": 0.6127,
      "step": 78880
    },
    {
      "epoch": 4640.588235294118,
      "grad_norm": 19.41968536376953,
      "learning_rate": 3.5941176470588237e-06,
      "loss": 0.5831,
      "step": 78890
    },
    {
      "epoch": 4641.176470588235,
      "grad_norm": 22.46674919128418,
      "learning_rate": 3.588235294117647e-06,
      "loss": 0.6494,
      "step": 78900
    },
    {
      "epoch": 4641.764705882353,
      "grad_norm": 22.100852966308594,
      "learning_rate": 3.582352941176471e-06,
      "loss": 0.7769,
      "step": 78910
    },
    {
      "epoch": 4642.35294117647,
      "grad_norm": 17.8243408203125,
      "learning_rate": 3.5764705882352943e-06,
      "loss": 0.7185,
      "step": 78920
    },
    {
      "epoch": 4642.941176470588,
      "grad_norm": 24.135541915893555,
      "learning_rate": 3.5705882352941177e-06,
      "loss": 0.7367,
      "step": 78930
    },
    {
      "epoch": 4643.529411764706,
      "grad_norm": 15.986944198608398,
      "learning_rate": 3.5647058823529416e-06,
      "loss": 0.6072,
      "step": 78940
    },
    {
      "epoch": 4644.117647058823,
      "grad_norm": 22.232906341552734,
      "learning_rate": 3.558823529411765e-06,
      "loss": 0.5847,
      "step": 78950
    },
    {
      "epoch": 4644.705882352941,
      "grad_norm": 14.499015808105469,
      "learning_rate": 3.552941176470589e-06,
      "loss": 0.6511,
      "step": 78960
    },
    {
      "epoch": 4645.294117647059,
      "grad_norm": 12.629515647888184,
      "learning_rate": 3.547058823529412e-06,
      "loss": 0.6491,
      "step": 78970
    },
    {
      "epoch": 4645.882352941177,
      "grad_norm": 22.707204818725586,
      "learning_rate": 3.5411764705882352e-06,
      "loss": 0.7035,
      "step": 78980
    },
    {
      "epoch": 4646.470588235294,
      "grad_norm": 18.57402801513672,
      "learning_rate": 3.535294117647059e-06,
      "loss": 0.5875,
      "step": 78990
    },
    {
      "epoch": 4647.058823529412,
      "grad_norm": 16.929601669311523,
      "learning_rate": 3.5294117647058825e-06,
      "loss": 0.6567,
      "step": 79000
    },
    {
      "epoch": 4647.64705882353,
      "grad_norm": 15.337357521057129,
      "learning_rate": 3.523529411764706e-06,
      "loss": 0.5406,
      "step": 79010
    },
    {
      "epoch": 4648.235294117647,
      "grad_norm": 19.07568359375,
      "learning_rate": 3.5176470588235297e-06,
      "loss": 0.7822,
      "step": 79020
    },
    {
      "epoch": 4648.823529411765,
      "grad_norm": 19.00067138671875,
      "learning_rate": 3.511764705882353e-06,
      "loss": 0.631,
      "step": 79030
    },
    {
      "epoch": 4649.411764705882,
      "grad_norm": 22.40581512451172,
      "learning_rate": 3.505882352941177e-06,
      "loss": 0.7044,
      "step": 79040
    },
    {
      "epoch": 4650.0,
      "grad_norm": 20.680025100708008,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.6997,
      "step": 79050
    },
    {
      "epoch": 4650.588235294118,
      "grad_norm": 19.1990966796875,
      "learning_rate": 3.4941176470588234e-06,
      "loss": 0.688,
      "step": 79060
    },
    {
      "epoch": 4651.176470588235,
      "grad_norm": 30.51384162902832,
      "learning_rate": 3.488235294117647e-06,
      "loss": 0.7062,
      "step": 79070
    },
    {
      "epoch": 4651.764705882353,
      "grad_norm": 18.060640335083008,
      "learning_rate": 3.4823529411764706e-06,
      "loss": 0.6941,
      "step": 79080
    },
    {
      "epoch": 4652.35294117647,
      "grad_norm": 13.648228645324707,
      "learning_rate": 3.4764705882352944e-06,
      "loss": 0.6679,
      "step": 79090
    },
    {
      "epoch": 4652.941176470588,
      "grad_norm": 16.12962532043457,
      "learning_rate": 3.470588235294118e-06,
      "loss": 0.5718,
      "step": 79100
    },
    {
      "epoch": 4653.529411764706,
      "grad_norm": 27.196186065673828,
      "learning_rate": 3.4647058823529413e-06,
      "loss": 0.7059,
      "step": 79110
    },
    {
      "epoch": 4654.117647058823,
      "grad_norm": 19.80187225341797,
      "learning_rate": 3.458823529411765e-06,
      "loss": 0.6434,
      "step": 79120
    },
    {
      "epoch": 4654.705882352941,
      "grad_norm": 17.79449462890625,
      "learning_rate": 3.4529411764705885e-06,
      "loss": 0.6946,
      "step": 79130
    },
    {
      "epoch": 4655.294117647059,
      "grad_norm": 24.73889923095703,
      "learning_rate": 3.4470588235294123e-06,
      "loss": 0.6432,
      "step": 79140
    },
    {
      "epoch": 4655.882352941177,
      "grad_norm": 14.855855941772461,
      "learning_rate": 3.4411764705882353e-06,
      "loss": 0.6331,
      "step": 79150
    },
    {
      "epoch": 4656.470588235294,
      "grad_norm": 22.231904983520508,
      "learning_rate": 3.4352941176470587e-06,
      "loss": 0.6987,
      "step": 79160
    },
    {
      "epoch": 4657.058823529412,
      "grad_norm": 24.94940185546875,
      "learning_rate": 3.4294117647058826e-06,
      "loss": 0.7684,
      "step": 79170
    },
    {
      "epoch": 4657.64705882353,
      "grad_norm": 26.40939712524414,
      "learning_rate": 3.423529411764706e-06,
      "loss": 0.6337,
      "step": 79180
    },
    {
      "epoch": 4658.235294117647,
      "grad_norm": 18.38504981994629,
      "learning_rate": 3.4176470588235294e-06,
      "loss": 0.6662,
      "step": 79190
    },
    {
      "epoch": 4658.823529411765,
      "grad_norm": 19.12273406982422,
      "learning_rate": 3.4117647058823532e-06,
      "loss": 0.7094,
      "step": 79200
    },
    {
      "epoch": 4659.411764705882,
      "grad_norm": 19.826534271240234,
      "learning_rate": 3.4058823529411766e-06,
      "loss": 0.6854,
      "step": 79210
    },
    {
      "epoch": 4660.0,
      "grad_norm": 24.15103530883789,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.6598,
      "step": 79220
    },
    {
      "epoch": 4660.588235294118,
      "grad_norm": 21.73373031616211,
      "learning_rate": 3.394117647058824e-06,
      "loss": 0.7069,
      "step": 79230
    },
    {
      "epoch": 4661.176470588235,
      "grad_norm": 19.07459831237793,
      "learning_rate": 3.388235294117647e-06,
      "loss": 0.6179,
      "step": 79240
    },
    {
      "epoch": 4661.764705882353,
      "grad_norm": 25.005632400512695,
      "learning_rate": 3.3823529411764707e-06,
      "loss": 0.6412,
      "step": 79250
    },
    {
      "epoch": 4662.35294117647,
      "grad_norm": 17.07298469543457,
      "learning_rate": 3.376470588235294e-06,
      "loss": 0.6423,
      "step": 79260
    },
    {
      "epoch": 4662.941176470588,
      "grad_norm": 27.04705238342285,
      "learning_rate": 3.3705882352941175e-06,
      "loss": 0.6884,
      "step": 79270
    },
    {
      "epoch": 4663.529411764706,
      "grad_norm": 20.836111068725586,
      "learning_rate": 3.3647058823529414e-06,
      "loss": 0.6012,
      "step": 79280
    },
    {
      "epoch": 4664.117647058823,
      "grad_norm": 19.75202178955078,
      "learning_rate": 3.3588235294117648e-06,
      "loss": 0.7053,
      "step": 79290
    },
    {
      "epoch": 4664.705882352941,
      "grad_norm": 21.106931686401367,
      "learning_rate": 3.3529411764705886e-06,
      "loss": 0.7227,
      "step": 79300
    },
    {
      "epoch": 4665.294117647059,
      "grad_norm": 22.137760162353516,
      "learning_rate": 3.347058823529412e-06,
      "loss": 0.6775,
      "step": 79310
    },
    {
      "epoch": 4665.882352941177,
      "grad_norm": 14.529629707336426,
      "learning_rate": 3.341176470588236e-06,
      "loss": 0.7168,
      "step": 79320
    },
    {
      "epoch": 4666.470588235294,
      "grad_norm": 24.708465576171875,
      "learning_rate": 3.335294117647059e-06,
      "loss": 0.6981,
      "step": 79330
    },
    {
      "epoch": 4667.058823529412,
      "grad_norm": 16.854095458984375,
      "learning_rate": 3.3294117647058823e-06,
      "loss": 0.7563,
      "step": 79340
    },
    {
      "epoch": 4667.64705882353,
      "grad_norm": 11.865373611450195,
      "learning_rate": 3.323529411764706e-06,
      "loss": 0.6347,
      "step": 79350
    },
    {
      "epoch": 4668.235294117647,
      "grad_norm": 24.664480209350586,
      "learning_rate": 3.3176470588235295e-06,
      "loss": 0.6403,
      "step": 79360
    },
    {
      "epoch": 4668.823529411765,
      "grad_norm": 24.537961959838867,
      "learning_rate": 3.311764705882353e-06,
      "loss": 0.7205,
      "step": 79370
    },
    {
      "epoch": 4669.411764705882,
      "grad_norm": 22.857463836669922,
      "learning_rate": 3.3058823529411768e-06,
      "loss": 0.6504,
      "step": 79380
    },
    {
      "epoch": 4670.0,
      "grad_norm": 16.824827194213867,
      "learning_rate": 3.3e-06,
      "loss": 0.5858,
      "step": 79390
    },
    {
      "epoch": 4670.588235294118,
      "grad_norm": 17.91585922241211,
      "learning_rate": 3.294117647058824e-06,
      "loss": 0.6385,
      "step": 79400
    },
    {
      "epoch": 4671.176470588235,
      "grad_norm": 20.32994270324707,
      "learning_rate": 3.2882352941176474e-06,
      "loss": 0.6941,
      "step": 79410
    },
    {
      "epoch": 4671.764705882353,
      "grad_norm": 21.05927276611328,
      "learning_rate": 3.2823529411764704e-06,
      "loss": 0.6203,
      "step": 79420
    },
    {
      "epoch": 4672.35294117647,
      "grad_norm": 29.354814529418945,
      "learning_rate": 3.2764705882352942e-06,
      "loss": 0.8248,
      "step": 79430
    },
    {
      "epoch": 4672.941176470588,
      "grad_norm": 20.788524627685547,
      "learning_rate": 3.2705882352941176e-06,
      "loss": 0.7141,
      "step": 79440
    },
    {
      "epoch": 4673.529411764706,
      "grad_norm": 17.10187339782715,
      "learning_rate": 3.264705882352941e-06,
      "loss": 0.6493,
      "step": 79450
    },
    {
      "epoch": 4674.117647058823,
      "grad_norm": 23.24786949157715,
      "learning_rate": 3.258823529411765e-06,
      "loss": 0.5789,
      "step": 79460
    },
    {
      "epoch": 4674.705882352941,
      "grad_norm": 17.81733512878418,
      "learning_rate": 3.2529411764705883e-06,
      "loss": 0.6669,
      "step": 79470
    },
    {
      "epoch": 4675.294117647059,
      "grad_norm": 22.740406036376953,
      "learning_rate": 3.247058823529412e-06,
      "loss": 0.5968,
      "step": 79480
    },
    {
      "epoch": 4675.882352941177,
      "grad_norm": 18.610389709472656,
      "learning_rate": 3.2411764705882356e-06,
      "loss": 0.6817,
      "step": 79490
    },
    {
      "epoch": 4676.470588235294,
      "grad_norm": 17.9660587310791,
      "learning_rate": 3.2352941176470594e-06,
      "loss": 0.6634,
      "step": 79500
    },
    {
      "epoch": 4677.058823529412,
      "grad_norm": 21.98909568786621,
      "learning_rate": 3.2294117647058824e-06,
      "loss": 0.6394,
      "step": 79510
    },
    {
      "epoch": 4677.64705882353,
      "grad_norm": 17.551525115966797,
      "learning_rate": 3.2235294117647058e-06,
      "loss": 0.615,
      "step": 79520
    },
    {
      "epoch": 4678.235294117647,
      "grad_norm": 16.471181869506836,
      "learning_rate": 3.217647058823529e-06,
      "loss": 0.657,
      "step": 79530
    },
    {
      "epoch": 4678.823529411765,
      "grad_norm": 23.262285232543945,
      "learning_rate": 3.211764705882353e-06,
      "loss": 0.6737,
      "step": 79540
    },
    {
      "epoch": 4679.411764705882,
      "grad_norm": 20.043521881103516,
      "learning_rate": 3.2058823529411764e-06,
      "loss": 0.6686,
      "step": 79550
    },
    {
      "epoch": 4680.0,
      "grad_norm": 20.972518920898438,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.5881,
      "step": 79560
    },
    {
      "epoch": 4680.588235294118,
      "grad_norm": 19.42405891418457,
      "learning_rate": 3.1941176470588237e-06,
      "loss": 0.661,
      "step": 79570
    },
    {
      "epoch": 4681.176470588235,
      "grad_norm": 20.75340461730957,
      "learning_rate": 3.1882352941176475e-06,
      "loss": 0.6533,
      "step": 79580
    },
    {
      "epoch": 4681.764705882353,
      "grad_norm": 22.116941452026367,
      "learning_rate": 3.182352941176471e-06,
      "loss": 0.6213,
      "step": 79590
    },
    {
      "epoch": 4682.35294117647,
      "grad_norm": 21.75478744506836,
      "learning_rate": 3.176470588235294e-06,
      "loss": 0.6631,
      "step": 79600
    },
    {
      "epoch": 4682.941176470588,
      "grad_norm": 20.187808990478516,
      "learning_rate": 3.1705882352941178e-06,
      "loss": 0.58,
      "step": 79610
    },
    {
      "epoch": 4683.529411764706,
      "grad_norm": 17.678451538085938,
      "learning_rate": 3.164705882352941e-06,
      "loss": 0.5864,
      "step": 79620
    },
    {
      "epoch": 4684.117647058823,
      "grad_norm": 24.363784790039062,
      "learning_rate": 3.1588235294117646e-06,
      "loss": 0.6654,
      "step": 79630
    },
    {
      "epoch": 4684.705882352941,
      "grad_norm": 20.137245178222656,
      "learning_rate": 3.1529411764705884e-06,
      "loss": 0.7325,
      "step": 79640
    },
    {
      "epoch": 4685.294117647059,
      "grad_norm": 22.784326553344727,
      "learning_rate": 3.147058823529412e-06,
      "loss": 0.7288,
      "step": 79650
    },
    {
      "epoch": 4685.882352941177,
      "grad_norm": 28.687110900878906,
      "learning_rate": 3.1411764705882357e-06,
      "loss": 0.7169,
      "step": 79660
    },
    {
      "epoch": 4686.470588235294,
      "grad_norm": 18.997936248779297,
      "learning_rate": 3.135294117647059e-06,
      "loss": 0.6345,
      "step": 79670
    },
    {
      "epoch": 4687.058823529412,
      "grad_norm": 22.387409210205078,
      "learning_rate": 3.129411764705883e-06,
      "loss": 0.7603,
      "step": 79680
    },
    {
      "epoch": 4687.64705882353,
      "grad_norm": 19.019084930419922,
      "learning_rate": 3.123529411764706e-06,
      "loss": 0.6906,
      "step": 79690
    },
    {
      "epoch": 4688.235294117647,
      "grad_norm": 19.24388313293457,
      "learning_rate": 3.1176470588235297e-06,
      "loss": 0.6354,
      "step": 79700
    },
    {
      "epoch": 4688.823529411765,
      "grad_norm": 21.231517791748047,
      "learning_rate": 3.111764705882353e-06,
      "loss": 0.6231,
      "step": 79710
    },
    {
      "epoch": 4689.411764705882,
      "grad_norm": 28.5092830657959,
      "learning_rate": 3.1058823529411766e-06,
      "loss": 0.6382,
      "step": 79720
    },
    {
      "epoch": 4690.0,
      "grad_norm": 36.62431335449219,
      "learning_rate": 3.1e-06,
      "loss": 0.7091,
      "step": 79730
    },
    {
      "epoch": 4690.588235294118,
      "grad_norm": 20.6209774017334,
      "learning_rate": 3.094117647058824e-06,
      "loss": 0.6093,
      "step": 79740
    },
    {
      "epoch": 4691.176470588235,
      "grad_norm": 27.60816764831543,
      "learning_rate": 3.0882352941176472e-06,
      "loss": 0.635,
      "step": 79750
    },
    {
      "epoch": 4691.764705882353,
      "grad_norm": 21.51812171936035,
      "learning_rate": 3.0823529411764706e-06,
      "loss": 0.6912,
      "step": 79760
    },
    {
      "epoch": 4692.35294117647,
      "grad_norm": 20.835628509521484,
      "learning_rate": 3.076470588235294e-06,
      "loss": 0.7007,
      "step": 79770
    },
    {
      "epoch": 4692.941176470588,
      "grad_norm": 24.14398193359375,
      "learning_rate": 3.070588235294118e-06,
      "loss": 0.6968,
      "step": 79780
    },
    {
      "epoch": 4693.529411764706,
      "grad_norm": 18.11985969543457,
      "learning_rate": 3.0647058823529413e-06,
      "loss": 0.6674,
      "step": 79790
    },
    {
      "epoch": 4694.117647058823,
      "grad_norm": 14.636200904846191,
      "learning_rate": 3.058823529411765e-06,
      "loss": 0.642,
      "step": 79800
    },
    {
      "epoch": 4694.705882352941,
      "grad_norm": 22.8028507232666,
      "learning_rate": 3.052941176470588e-06,
      "loss": 0.6417,
      "step": 79810
    },
    {
      "epoch": 4695.294117647059,
      "grad_norm": 19.707265853881836,
      "learning_rate": 3.047058823529412e-06,
      "loss": 0.6143,
      "step": 79820
    },
    {
      "epoch": 4695.882352941177,
      "grad_norm": 24.32803726196289,
      "learning_rate": 3.0411764705882353e-06,
      "loss": 0.7513,
      "step": 79830
    },
    {
      "epoch": 4696.470588235294,
      "grad_norm": 20.352163314819336,
      "learning_rate": 3.035294117647059e-06,
      "loss": 0.6347,
      "step": 79840
    },
    {
      "epoch": 4697.058823529412,
      "grad_norm": 23.508926391601562,
      "learning_rate": 3.029411764705882e-06,
      "loss": 0.6453,
      "step": 79850
    },
    {
      "epoch": 4697.64705882353,
      "grad_norm": 19.77782440185547,
      "learning_rate": 3.023529411764706e-06,
      "loss": 0.7144,
      "step": 79860
    },
    {
      "epoch": 4698.235294117647,
      "grad_norm": 19.744779586791992,
      "learning_rate": 3.0176470588235294e-06,
      "loss": 0.6526,
      "step": 79870
    },
    {
      "epoch": 4698.823529411765,
      "grad_norm": 20.280580520629883,
      "learning_rate": 3.0117647058823533e-06,
      "loss": 0.5506,
      "step": 79880
    },
    {
      "epoch": 4699.411764705882,
      "grad_norm": 21.70244598388672,
      "learning_rate": 3.0058823529411767e-06,
      "loss": 0.6295,
      "step": 79890
    },
    {
      "epoch": 4700.0,
      "grad_norm": 27.701276779174805,
      "learning_rate": 3e-06,
      "loss": 0.6131,
      "step": 79900
    },
    {
      "epoch": 4700.588235294118,
      "grad_norm": 15.776652336120605,
      "learning_rate": 2.9941176470588235e-06,
      "loss": 0.6764,
      "step": 79910
    },
    {
      "epoch": 4701.176470588235,
      "grad_norm": 19.65470314025879,
      "learning_rate": 2.9882352941176473e-06,
      "loss": 0.6641,
      "step": 79920
    },
    {
      "epoch": 4701.764705882353,
      "grad_norm": 17.282915115356445,
      "learning_rate": 2.9823529411764707e-06,
      "loss": 0.6222,
      "step": 79930
    },
    {
      "epoch": 4702.35294117647,
      "grad_norm": 17.243206024169922,
      "learning_rate": 2.976470588235294e-06,
      "loss": 0.6157,
      "step": 79940
    },
    {
      "epoch": 4702.941176470588,
      "grad_norm": 21.01000213623047,
      "learning_rate": 2.9705882352941176e-06,
      "loss": 0.6821,
      "step": 79950
    },
    {
      "epoch": 4703.529411764706,
      "grad_norm": 19.089677810668945,
      "learning_rate": 2.9647058823529414e-06,
      "loss": 0.7346,
      "step": 79960
    },
    {
      "epoch": 4704.117647058823,
      "grad_norm": 32.79267120361328,
      "learning_rate": 2.958823529411765e-06,
      "loss": 0.6694,
      "step": 79970
    },
    {
      "epoch": 4704.705882352941,
      "grad_norm": 14.281557083129883,
      "learning_rate": 2.9529411764705886e-06,
      "loss": 0.6245,
      "step": 79980
    },
    {
      "epoch": 4705.294117647059,
      "grad_norm": 15.109893798828125,
      "learning_rate": 2.9470588235294116e-06,
      "loss": 0.5338,
      "step": 79990
    },
    {
      "epoch": 4705.882352941177,
      "grad_norm": 22.760841369628906,
      "learning_rate": 2.9411764705882355e-06,
      "loss": 0.6138,
      "step": 80000
    },
    {
      "epoch": 4706.470588235294,
      "grad_norm": 21.0024471282959,
      "learning_rate": 2.935294117647059e-06,
      "loss": 0.7048,
      "step": 80010
    },
    {
      "epoch": 4707.058823529412,
      "grad_norm": 18.261892318725586,
      "learning_rate": 2.9294117647058827e-06,
      "loss": 0.7055,
      "step": 80020
    },
    {
      "epoch": 4707.64705882353,
      "grad_norm": 16.037036895751953,
      "learning_rate": 2.9235294117647057e-06,
      "loss": 0.5807,
      "step": 80030
    },
    {
      "epoch": 4708.235294117647,
      "grad_norm": 22.931747436523438,
      "learning_rate": 2.9176470588235295e-06,
      "loss": 0.606,
      "step": 80040
    },
    {
      "epoch": 4708.823529411765,
      "grad_norm": 27.172313690185547,
      "learning_rate": 2.911764705882353e-06,
      "loss": 0.616,
      "step": 80050
    },
    {
      "epoch": 4709.411764705882,
      "grad_norm": 11.2083740234375,
      "learning_rate": 2.9058823529411768e-06,
      "loss": 0.5823,
      "step": 80060
    },
    {
      "epoch": 4710.0,
      "grad_norm": 22.89110565185547,
      "learning_rate": 2.9e-06,
      "loss": 0.8383,
      "step": 80070
    },
    {
      "epoch": 4710.588235294118,
      "grad_norm": 16.707035064697266,
      "learning_rate": 2.8941176470588236e-06,
      "loss": 0.6206,
      "step": 80080
    },
    {
      "epoch": 4711.176470588235,
      "grad_norm": 21.364463806152344,
      "learning_rate": 2.888235294117647e-06,
      "loss": 0.7148,
      "step": 80090
    },
    {
      "epoch": 4711.764705882353,
      "grad_norm": 19.157573699951172,
      "learning_rate": 2.882352941176471e-06,
      "loss": 0.6781,
      "step": 80100
    },
    {
      "epoch": 4712.35294117647,
      "grad_norm": 26.208377838134766,
      "learning_rate": 2.8764705882352943e-06,
      "loss": 0.6235,
      "step": 80110
    },
    {
      "epoch": 4712.941176470588,
      "grad_norm": 27.626461029052734,
      "learning_rate": 2.8705882352941177e-06,
      "loss": 0.6843,
      "step": 80120
    },
    {
      "epoch": 4713.529411764706,
      "grad_norm": 19.848249435424805,
      "learning_rate": 2.864705882352941e-06,
      "loss": 0.6848,
      "step": 80130
    },
    {
      "epoch": 4714.117647058823,
      "grad_norm": 18.14862823486328,
      "learning_rate": 2.858823529411765e-06,
      "loss": 0.5146,
      "step": 80140
    },
    {
      "epoch": 4714.705882352941,
      "grad_norm": 24.987083435058594,
      "learning_rate": 2.8529411764705883e-06,
      "loss": 0.8185,
      "step": 80150
    },
    {
      "epoch": 4715.294117647059,
      "grad_norm": 13.860883712768555,
      "learning_rate": 2.847058823529412e-06,
      "loss": 0.5593,
      "step": 80160
    },
    {
      "epoch": 4715.882352941177,
      "grad_norm": 16.268461227416992,
      "learning_rate": 2.841176470588235e-06,
      "loss": 0.6739,
      "step": 80170
    },
    {
      "epoch": 4716.470588235294,
      "grad_norm": 22.83698844909668,
      "learning_rate": 2.835294117647059e-06,
      "loss": 0.6836,
      "step": 80180
    },
    {
      "epoch": 4717.058823529412,
      "grad_norm": 17.667306900024414,
      "learning_rate": 2.8294117647058824e-06,
      "loss": 0.6223,
      "step": 80190
    },
    {
      "epoch": 4717.64705882353,
      "grad_norm": 20.517791748046875,
      "learning_rate": 2.8235294117647062e-06,
      "loss": 0.6249,
      "step": 80200
    },
    {
      "epoch": 4718.235294117647,
      "grad_norm": 18.879913330078125,
      "learning_rate": 2.8176470588235292e-06,
      "loss": 0.5722,
      "step": 80210
    },
    {
      "epoch": 4718.823529411765,
      "grad_norm": 20.768800735473633,
      "learning_rate": 2.811764705882353e-06,
      "loss": 0.7019,
      "step": 80220
    },
    {
      "epoch": 4719.411764705882,
      "grad_norm": 26.370018005371094,
      "learning_rate": 2.8058823529411765e-06,
      "loss": 0.6855,
      "step": 80230
    },
    {
      "epoch": 4720.0,
      "grad_norm": 22.6679744720459,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.5952,
      "step": 80240
    },
    {
      "epoch": 4720.588235294118,
      "grad_norm": 24.525293350219727,
      "learning_rate": 2.7941176470588237e-06,
      "loss": 0.5798,
      "step": 80250
    },
    {
      "epoch": 4721.176470588235,
      "grad_norm": 21.084592819213867,
      "learning_rate": 2.788235294117647e-06,
      "loss": 0.7233,
      "step": 80260
    },
    {
      "epoch": 4721.764705882353,
      "grad_norm": 19.3879337310791,
      "learning_rate": 2.7823529411764705e-06,
      "loss": 0.6657,
      "step": 80270
    },
    {
      "epoch": 4722.35294117647,
      "grad_norm": 15.59480094909668,
      "learning_rate": 2.7764705882352944e-06,
      "loss": 0.6978,
      "step": 80280
    },
    {
      "epoch": 4722.941176470588,
      "grad_norm": 18.43106460571289,
      "learning_rate": 2.7705882352941178e-06,
      "loss": 0.675,
      "step": 80290
    },
    {
      "epoch": 4723.529411764706,
      "grad_norm": 27.4904842376709,
      "learning_rate": 2.7647058823529416e-06,
      "loss": 0.6592,
      "step": 80300
    },
    {
      "epoch": 4724.117647058823,
      "grad_norm": 18.44843101501465,
      "learning_rate": 2.7588235294117646e-06,
      "loss": 0.678,
      "step": 80310
    },
    {
      "epoch": 4724.705882352941,
      "grad_norm": 17.474458694458008,
      "learning_rate": 2.7529411764705884e-06,
      "loss": 0.5946,
      "step": 80320
    },
    {
      "epoch": 4725.294117647059,
      "grad_norm": 18.343158721923828,
      "learning_rate": 2.747058823529412e-06,
      "loss": 0.6423,
      "step": 80330
    },
    {
      "epoch": 4725.882352941177,
      "grad_norm": 17.76335906982422,
      "learning_rate": 2.7411764705882357e-06,
      "loss": 0.6697,
      "step": 80340
    },
    {
      "epoch": 4726.470588235294,
      "grad_norm": 19.43720817565918,
      "learning_rate": 2.7352941176470587e-06,
      "loss": 0.5857,
      "step": 80350
    },
    {
      "epoch": 4727.058823529412,
      "grad_norm": 17.702415466308594,
      "learning_rate": 2.7294117647058825e-06,
      "loss": 0.5911,
      "step": 80360
    },
    {
      "epoch": 4727.64705882353,
      "grad_norm": 20.17037582397461,
      "learning_rate": 2.723529411764706e-06,
      "loss": 0.5943,
      "step": 80370
    },
    {
      "epoch": 4728.235294117647,
      "grad_norm": 26.02420997619629,
      "learning_rate": 2.7176470588235297e-06,
      "loss": 0.7386,
      "step": 80380
    },
    {
      "epoch": 4728.823529411765,
      "grad_norm": 13.384181022644043,
      "learning_rate": 2.711764705882353e-06,
      "loss": 0.612,
      "step": 80390
    },
    {
      "epoch": 4729.411764705882,
      "grad_norm": 14.39702320098877,
      "learning_rate": 2.7058823529411766e-06,
      "loss": 0.6446,
      "step": 80400
    },
    {
      "epoch": 4730.0,
      "grad_norm": 18.205909729003906,
      "learning_rate": 2.7e-06,
      "loss": 0.5753,
      "step": 80410
    },
    {
      "epoch": 4730.588235294118,
      "grad_norm": 17.67511749267578,
      "learning_rate": 2.694117647058824e-06,
      "loss": 0.6876,
      "step": 80420
    },
    {
      "epoch": 4731.176470588235,
      "grad_norm": 18.972749710083008,
      "learning_rate": 2.6882352941176472e-06,
      "loss": 0.7048,
      "step": 80430
    },
    {
      "epoch": 4731.764705882353,
      "grad_norm": 21.214845657348633,
      "learning_rate": 2.6823529411764706e-06,
      "loss": 0.6764,
      "step": 80440
    },
    {
      "epoch": 4732.35294117647,
      "grad_norm": 17.524600982666016,
      "learning_rate": 2.676470588235294e-06,
      "loss": 0.78,
      "step": 80450
    },
    {
      "epoch": 4732.941176470588,
      "grad_norm": 22.525634765625,
      "learning_rate": 2.670588235294118e-06,
      "loss": 0.6411,
      "step": 80460
    },
    {
      "epoch": 4733.529411764706,
      "grad_norm": 19.6806697845459,
      "learning_rate": 2.6647058823529413e-06,
      "loss": 0.6142,
      "step": 80470
    },
    {
      "epoch": 4734.117647058823,
      "grad_norm": 20.81259536743164,
      "learning_rate": 2.658823529411765e-06,
      "loss": 0.5947,
      "step": 80480
    },
    {
      "epoch": 4734.705882352941,
      "grad_norm": 10.532815933227539,
      "learning_rate": 2.652941176470588e-06,
      "loss": 0.6756,
      "step": 80490
    },
    {
      "epoch": 4735.294117647059,
      "grad_norm": 16.94624137878418,
      "learning_rate": 2.647058823529412e-06,
      "loss": 0.647,
      "step": 80500
    },
    {
      "epoch": 4735.882352941177,
      "grad_norm": 19.243558883666992,
      "learning_rate": 2.6411764705882354e-06,
      "loss": 0.6289,
      "step": 80510
    },
    {
      "epoch": 4736.470588235294,
      "grad_norm": 23.40838050842285,
      "learning_rate": 2.635294117647059e-06,
      "loss": 0.6435,
      "step": 80520
    },
    {
      "epoch": 4737.058823529412,
      "grad_norm": 16.698348999023438,
      "learning_rate": 2.629411764705882e-06,
      "loss": 0.6164,
      "step": 80530
    },
    {
      "epoch": 4737.64705882353,
      "grad_norm": 19.878263473510742,
      "learning_rate": 2.623529411764706e-06,
      "loss": 0.6692,
      "step": 80540
    },
    {
      "epoch": 4738.235294117647,
      "grad_norm": 25.175439834594727,
      "learning_rate": 2.6176470588235294e-06,
      "loss": 0.6654,
      "step": 80550
    },
    {
      "epoch": 4738.823529411765,
      "grad_norm": 21.060667037963867,
      "learning_rate": 2.6117647058823533e-06,
      "loss": 0.6575,
      "step": 80560
    },
    {
      "epoch": 4739.411764705882,
      "grad_norm": 17.54447364807129,
      "learning_rate": 2.6058823529411767e-06,
      "loss": 0.6017,
      "step": 80570
    },
    {
      "epoch": 4740.0,
      "grad_norm": 28.484344482421875,
      "learning_rate": 2.6e-06,
      "loss": 0.7044,
      "step": 80580
    },
    {
      "epoch": 4740.588235294118,
      "grad_norm": 31.344985961914062,
      "learning_rate": 2.5941176470588235e-06,
      "loss": 0.6565,
      "step": 80590
    },
    {
      "epoch": 4741.176470588235,
      "grad_norm": 30.064943313598633,
      "learning_rate": 2.5882352941176473e-06,
      "loss": 0.6613,
      "step": 80600
    },
    {
      "epoch": 4741.764705882353,
      "grad_norm": 20.582300186157227,
      "learning_rate": 2.5823529411764708e-06,
      "loss": 0.5795,
      "step": 80610
    },
    {
      "epoch": 4742.35294117647,
      "grad_norm": 24.38083839416504,
      "learning_rate": 2.576470588235294e-06,
      "loss": 0.6307,
      "step": 80620
    },
    {
      "epoch": 4742.941176470588,
      "grad_norm": 19.15243148803711,
      "learning_rate": 2.5705882352941176e-06,
      "loss": 0.6524,
      "step": 80630
    },
    {
      "epoch": 4743.529411764706,
      "grad_norm": 29.133676528930664,
      "learning_rate": 2.5647058823529414e-06,
      "loss": 0.7445,
      "step": 80640
    },
    {
      "epoch": 4744.117647058823,
      "grad_norm": 22.48151397705078,
      "learning_rate": 2.558823529411765e-06,
      "loss": 0.71,
      "step": 80650
    },
    {
      "epoch": 4744.705882352941,
      "grad_norm": 16.674501419067383,
      "learning_rate": 2.5529411764705887e-06,
      "loss": 0.6589,
      "step": 80660
    },
    {
      "epoch": 4745.294117647059,
      "grad_norm": 19.819271087646484,
      "learning_rate": 2.5470588235294116e-06,
      "loss": 0.5633,
      "step": 80670
    },
    {
      "epoch": 4745.882352941177,
      "grad_norm": 19.240697860717773,
      "learning_rate": 2.5411764705882355e-06,
      "loss": 0.6028,
      "step": 80680
    },
    {
      "epoch": 4746.470588235294,
      "grad_norm": 21.45989418029785,
      "learning_rate": 2.535294117647059e-06,
      "loss": 0.6258,
      "step": 80690
    },
    {
      "epoch": 4747.058823529412,
      "grad_norm": 30.557823181152344,
      "learning_rate": 2.5294117647058827e-06,
      "loss": 0.7031,
      "step": 80700
    },
    {
      "epoch": 4747.64705882353,
      "grad_norm": 13.573490142822266,
      "learning_rate": 2.5235294117647057e-06,
      "loss": 0.6413,
      "step": 80710
    },
    {
      "epoch": 4748.235294117647,
      "grad_norm": 22.990291595458984,
      "learning_rate": 2.5176470588235295e-06,
      "loss": 0.6193,
      "step": 80720
    },
    {
      "epoch": 4748.823529411765,
      "grad_norm": 23.139875411987305,
      "learning_rate": 2.511764705882353e-06,
      "loss": 0.6534,
      "step": 80730
    },
    {
      "epoch": 4749.411764705882,
      "grad_norm": 21.525957107543945,
      "learning_rate": 2.505882352941177e-06,
      "loss": 0.6229,
      "step": 80740
    },
    {
      "epoch": 4750.0,
      "grad_norm": 23.207578659057617,
      "learning_rate": 2.5e-06,
      "loss": 0.6669,
      "step": 80750
    },
    {
      "epoch": 4750.588235294118,
      "grad_norm": 26.317699432373047,
      "learning_rate": 2.4941176470588236e-06,
      "loss": 0.6131,
      "step": 80760
    },
    {
      "epoch": 4751.176470588235,
      "grad_norm": 18.33123016357422,
      "learning_rate": 2.488235294117647e-06,
      "loss": 0.6517,
      "step": 80770
    },
    {
      "epoch": 4751.764705882353,
      "grad_norm": 18.356525421142578,
      "learning_rate": 2.482352941176471e-06,
      "loss": 0.6121,
      "step": 80780
    },
    {
      "epoch": 4752.35294117647,
      "grad_norm": 13.345441818237305,
      "learning_rate": 2.4764705882352943e-06,
      "loss": 0.6867,
      "step": 80790
    },
    {
      "epoch": 4752.941176470588,
      "grad_norm": 13.940317153930664,
      "learning_rate": 2.4705882352941177e-06,
      "loss": 0.6255,
      "step": 80800
    },
    {
      "epoch": 4753.529411764706,
      "grad_norm": 22.82643699645996,
      "learning_rate": 2.464705882352941e-06,
      "loss": 0.6685,
      "step": 80810
    },
    {
      "epoch": 4754.117647058823,
      "grad_norm": 21.474435806274414,
      "learning_rate": 2.458823529411765e-06,
      "loss": 0.6149,
      "step": 80820
    },
    {
      "epoch": 4754.705882352941,
      "grad_norm": 13.967558860778809,
      "learning_rate": 2.4529411764705883e-06,
      "loss": 0.5591,
      "step": 80830
    },
    {
      "epoch": 4755.294117647059,
      "grad_norm": 19.784053802490234,
      "learning_rate": 2.447058823529412e-06,
      "loss": 0.671,
      "step": 80840
    },
    {
      "epoch": 4755.882352941177,
      "grad_norm": 29.413192749023438,
      "learning_rate": 2.441176470588235e-06,
      "loss": 0.6549,
      "step": 80850
    },
    {
      "epoch": 4756.470588235294,
      "grad_norm": 25.209548950195312,
      "learning_rate": 2.435294117647059e-06,
      "loss": 0.7558,
      "step": 80860
    },
    {
      "epoch": 4757.058823529412,
      "grad_norm": 19.508325576782227,
      "learning_rate": 2.4294117647058824e-06,
      "loss": 0.6014,
      "step": 80870
    },
    {
      "epoch": 4757.64705882353,
      "grad_norm": 21.723365783691406,
      "learning_rate": 2.4235294117647062e-06,
      "loss": 0.7191,
      "step": 80880
    },
    {
      "epoch": 4758.235294117647,
      "grad_norm": 16.585676193237305,
      "learning_rate": 2.4176470588235297e-06,
      "loss": 0.5925,
      "step": 80890
    },
    {
      "epoch": 4758.823529411765,
      "grad_norm": 18.884004592895508,
      "learning_rate": 2.411764705882353e-06,
      "loss": 0.7049,
      "step": 80900
    },
    {
      "epoch": 4759.411764705882,
      "grad_norm": 22.107702255249023,
      "learning_rate": 2.4058823529411765e-06,
      "loss": 0.7078,
      "step": 80910
    },
    {
      "epoch": 4760.0,
      "grad_norm": 22.963491439819336,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.729,
      "step": 80920
    },
    {
      "epoch": 4760.588235294118,
      "grad_norm": 19.418533325195312,
      "learning_rate": 2.3941176470588237e-06,
      "loss": 0.6884,
      "step": 80930
    },
    {
      "epoch": 4761.176470588235,
      "grad_norm": 15.875200271606445,
      "learning_rate": 2.388235294117647e-06,
      "loss": 0.6288,
      "step": 80940
    },
    {
      "epoch": 4761.764705882353,
      "grad_norm": 19.120431900024414,
      "learning_rate": 2.3823529411764705e-06,
      "loss": 0.5972,
      "step": 80950
    },
    {
      "epoch": 4762.35294117647,
      "grad_norm": 17.16977882385254,
      "learning_rate": 2.3764705882352944e-06,
      "loss": 0.5803,
      "step": 80960
    },
    {
      "epoch": 4762.941176470588,
      "grad_norm": 15.946696281433105,
      "learning_rate": 2.370588235294118e-06,
      "loss": 0.7011,
      "step": 80970
    },
    {
      "epoch": 4763.529411764706,
      "grad_norm": 22.96849822998047,
      "learning_rate": 2.3647058823529416e-06,
      "loss": 0.6273,
      "step": 80980
    },
    {
      "epoch": 4764.117647058823,
      "grad_norm": 33.43320083618164,
      "learning_rate": 2.3588235294117646e-06,
      "loss": 0.7328,
      "step": 80990
    },
    {
      "epoch": 4764.705882352941,
      "grad_norm": 20.420495986938477,
      "learning_rate": 2.3529411764705885e-06,
      "loss": 0.6833,
      "step": 81000
    },
    {
      "epoch": 4765.294117647059,
      "grad_norm": 22.602741241455078,
      "learning_rate": 2.347058823529412e-06,
      "loss": 0.6652,
      "step": 81010
    },
    {
      "epoch": 4765.882352941177,
      "grad_norm": 25.786428451538086,
      "learning_rate": 2.3411764705882357e-06,
      "loss": 0.6311,
      "step": 81020
    },
    {
      "epoch": 4766.470588235294,
      "grad_norm": 20.636112213134766,
      "learning_rate": 2.3352941176470587e-06,
      "loss": 0.6664,
      "step": 81030
    },
    {
      "epoch": 4767.058823529412,
      "grad_norm": 23.19099998474121,
      "learning_rate": 2.3294117647058825e-06,
      "loss": 0.6896,
      "step": 81040
    },
    {
      "epoch": 4767.64705882353,
      "grad_norm": 27.57229232788086,
      "learning_rate": 2.323529411764706e-06,
      "loss": 0.7018,
      "step": 81050
    },
    {
      "epoch": 4768.235294117647,
      "grad_norm": 20.897592544555664,
      "learning_rate": 2.3176470588235298e-06,
      "loss": 0.5545,
      "step": 81060
    },
    {
      "epoch": 4768.823529411765,
      "grad_norm": 24.169897079467773,
      "learning_rate": 2.311764705882353e-06,
      "loss": 0.6632,
      "step": 81070
    },
    {
      "epoch": 4769.411764705882,
      "grad_norm": 27.89570426940918,
      "learning_rate": 2.3058823529411766e-06,
      "loss": 0.6839,
      "step": 81080
    },
    {
      "epoch": 4770.0,
      "grad_norm": 22.079072952270508,
      "learning_rate": 2.3e-06,
      "loss": 0.6767,
      "step": 81090
    },
    {
      "epoch": 4770.588235294118,
      "grad_norm": 14.95148754119873,
      "learning_rate": 2.294117647058824e-06,
      "loss": 0.6694,
      "step": 81100
    },
    {
      "epoch": 4771.176470588235,
      "grad_norm": 13.725770950317383,
      "learning_rate": 2.2882352941176472e-06,
      "loss": 0.666,
      "step": 81110
    },
    {
      "epoch": 4771.764705882353,
      "grad_norm": 18.20772933959961,
      "learning_rate": 2.2823529411764707e-06,
      "loss": 0.6366,
      "step": 81120
    },
    {
      "epoch": 4772.35294117647,
      "grad_norm": 27.590145111083984,
      "learning_rate": 2.276470588235294e-06,
      "loss": 0.6683,
      "step": 81130
    },
    {
      "epoch": 4772.941176470588,
      "grad_norm": 20.931941986083984,
      "learning_rate": 2.270588235294118e-06,
      "loss": 0.7223,
      "step": 81140
    },
    {
      "epoch": 4773.529411764706,
      "grad_norm": 15.808677673339844,
      "learning_rate": 2.2647058823529413e-06,
      "loss": 0.6344,
      "step": 81150
    },
    {
      "epoch": 4774.117647058823,
      "grad_norm": 23.513050079345703,
      "learning_rate": 2.258823529411765e-06,
      "loss": 0.7247,
      "step": 81160
    },
    {
      "epoch": 4774.705882352941,
      "grad_norm": 19.74049949645996,
      "learning_rate": 2.252941176470588e-06,
      "loss": 0.6865,
      "step": 81170
    },
    {
      "epoch": 4775.294117647059,
      "grad_norm": 18.70172119140625,
      "learning_rate": 2.247058823529412e-06,
      "loss": 0.656,
      "step": 81180
    },
    {
      "epoch": 4775.882352941177,
      "grad_norm": 15.876372337341309,
      "learning_rate": 2.2411764705882354e-06,
      "loss": 0.7847,
      "step": 81190
    },
    {
      "epoch": 4776.470588235294,
      "grad_norm": 26.282987594604492,
      "learning_rate": 2.2352941176470592e-06,
      "loss": 0.7478,
      "step": 81200
    },
    {
      "epoch": 4777.058823529412,
      "grad_norm": 22.556602478027344,
      "learning_rate": 2.229411764705882e-06,
      "loss": 0.6404,
      "step": 81210
    },
    {
      "epoch": 4777.64705882353,
      "grad_norm": 25.6209659576416,
      "learning_rate": 2.223529411764706e-06,
      "loss": 0.6501,
      "step": 81220
    },
    {
      "epoch": 4778.235294117647,
      "grad_norm": 18.598140716552734,
      "learning_rate": 2.2176470588235295e-06,
      "loss": 0.6315,
      "step": 81230
    },
    {
      "epoch": 4778.823529411765,
      "grad_norm": 20.938650131225586,
      "learning_rate": 2.2117647058823533e-06,
      "loss": 0.6863,
      "step": 81240
    },
    {
      "epoch": 4779.411764705882,
      "grad_norm": 20.09213638305664,
      "learning_rate": 2.2058823529411767e-06,
      "loss": 0.6687,
      "step": 81250
    },
    {
      "epoch": 4780.0,
      "grad_norm": 18.640155792236328,
      "learning_rate": 2.2e-06,
      "loss": 0.7135,
      "step": 81260
    },
    {
      "epoch": 4780.588235294118,
      "grad_norm": 17.432790756225586,
      "learning_rate": 2.1941176470588235e-06,
      "loss": 0.6988,
      "step": 81270
    },
    {
      "epoch": 4781.176470588235,
      "grad_norm": 21.273046493530273,
      "learning_rate": 2.1882352941176474e-06,
      "loss": 0.5966,
      "step": 81280
    },
    {
      "epoch": 4781.764705882353,
      "grad_norm": 28.561498641967773,
      "learning_rate": 2.1823529411764708e-06,
      "loss": 0.8023,
      "step": 81290
    },
    {
      "epoch": 4782.35294117647,
      "grad_norm": 24.367473602294922,
      "learning_rate": 2.176470588235294e-06,
      "loss": 0.6641,
      "step": 81300
    },
    {
      "epoch": 4782.941176470588,
      "grad_norm": 14.496091842651367,
      "learning_rate": 2.1705882352941176e-06,
      "loss": 0.5887,
      "step": 81310
    },
    {
      "epoch": 4783.529411764706,
      "grad_norm": 18.935340881347656,
      "learning_rate": 2.1647058823529414e-06,
      "loss": 0.7609,
      "step": 81320
    },
    {
      "epoch": 4784.117647058823,
      "grad_norm": 17.65507698059082,
      "learning_rate": 2.158823529411765e-06,
      "loss": 0.6012,
      "step": 81330
    },
    {
      "epoch": 4784.705882352941,
      "grad_norm": 18.467947006225586,
      "learning_rate": 2.1529411764705887e-06,
      "loss": 0.7008,
      "step": 81340
    },
    {
      "epoch": 4785.294117647059,
      "grad_norm": 15.856489181518555,
      "learning_rate": 2.1470588235294117e-06,
      "loss": 0.6623,
      "step": 81350
    },
    {
      "epoch": 4785.882352941177,
      "grad_norm": 19.904218673706055,
      "learning_rate": 2.1411764705882355e-06,
      "loss": 0.644,
      "step": 81360
    },
    {
      "epoch": 4786.470588235294,
      "grad_norm": 16.26809310913086,
      "learning_rate": 2.135294117647059e-06,
      "loss": 0.657,
      "step": 81370
    },
    {
      "epoch": 4787.058823529412,
      "grad_norm": 21.770429611206055,
      "learning_rate": 2.1294117647058827e-06,
      "loss": 0.6527,
      "step": 81380
    },
    {
      "epoch": 4787.64705882353,
      "grad_norm": 18.541982650756836,
      "learning_rate": 2.123529411764706e-06,
      "loss": 0.594,
      "step": 81390
    },
    {
      "epoch": 4788.235294117647,
      "grad_norm": 16.483556747436523,
      "learning_rate": 2.1176470588235296e-06,
      "loss": 0.6744,
      "step": 81400
    },
    {
      "epoch": 4788.823529411765,
      "grad_norm": 24.281829833984375,
      "learning_rate": 2.111764705882353e-06,
      "loss": 0.6556,
      "step": 81410
    },
    {
      "epoch": 4789.411764705882,
      "grad_norm": 22.46451187133789,
      "learning_rate": 2.105882352941177e-06,
      "loss": 0.6909,
      "step": 81420
    },
    {
      "epoch": 4790.0,
      "grad_norm": 22.92943000793457,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.6233,
      "step": 81430
    },
    {
      "epoch": 4790.588235294118,
      "grad_norm": 16.28145980834961,
      "learning_rate": 2.0941176470588236e-06,
      "loss": 0.7735,
      "step": 81440
    },
    {
      "epoch": 4791.176470588235,
      "grad_norm": 22.560657501220703,
      "learning_rate": 2.088235294117647e-06,
      "loss": 0.6983,
      "step": 81450
    },
    {
      "epoch": 4791.764705882353,
      "grad_norm": 21.4332218170166,
      "learning_rate": 2.082352941176471e-06,
      "loss": 0.6908,
      "step": 81460
    },
    {
      "epoch": 4792.35294117647,
      "grad_norm": 17.238441467285156,
      "learning_rate": 2.0764705882352943e-06,
      "loss": 0.7075,
      "step": 81470
    },
    {
      "epoch": 4792.941176470588,
      "grad_norm": 14.613910675048828,
      "learning_rate": 2.0705882352941177e-06,
      "loss": 0.599,
      "step": 81480
    },
    {
      "epoch": 4793.529411764706,
      "grad_norm": 20.4371280670166,
      "learning_rate": 2.064705882352941e-06,
      "loss": 0.6665,
      "step": 81490
    },
    {
      "epoch": 4794.117647058823,
      "grad_norm": 16.91837501525879,
      "learning_rate": 2.058823529411765e-06,
      "loss": 0.6926,
      "step": 81500
    },
    {
      "epoch": 4794.705882352941,
      "grad_norm": 19.79886817932129,
      "learning_rate": 2.0529411764705884e-06,
      "loss": 0.6475,
      "step": 81510
    },
    {
      "epoch": 4795.294117647059,
      "grad_norm": 30.59619903564453,
      "learning_rate": 2.047058823529412e-06,
      "loss": 0.7689,
      "step": 81520
    },
    {
      "epoch": 4795.882352941177,
      "grad_norm": 19.408864974975586,
      "learning_rate": 2.041176470588235e-06,
      "loss": 0.6776,
      "step": 81530
    },
    {
      "epoch": 4796.470588235294,
      "grad_norm": 23.032123565673828,
      "learning_rate": 2.035294117647059e-06,
      "loss": 0.6895,
      "step": 81540
    },
    {
      "epoch": 4797.058823529412,
      "grad_norm": 16.94528579711914,
      "learning_rate": 2.0294117647058824e-06,
      "loss": 0.6818,
      "step": 81550
    },
    {
      "epoch": 4797.64705882353,
      "grad_norm": 21.888830184936523,
      "learning_rate": 2.0235294117647063e-06,
      "loss": 0.6195,
      "step": 81560
    },
    {
      "epoch": 4798.235294117647,
      "grad_norm": 19.72364616394043,
      "learning_rate": 2.0176470588235297e-06,
      "loss": 0.6853,
      "step": 81570
    },
    {
      "epoch": 4798.823529411765,
      "grad_norm": 20.149215698242188,
      "learning_rate": 2.011764705882353e-06,
      "loss": 0.6549,
      "step": 81580
    },
    {
      "epoch": 4799.411764705882,
      "grad_norm": 23.80294418334961,
      "learning_rate": 2.0058823529411765e-06,
      "loss": 0.6109,
      "step": 81590
    },
    {
      "epoch": 4800.0,
      "grad_norm": 36.05647659301758,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5884,
      "step": 81600
    },
    {
      "epoch": 4800.588235294118,
      "grad_norm": 22.91493797302246,
      "learning_rate": 1.9941176470588237e-06,
      "loss": 0.6724,
      "step": 81610
    },
    {
      "epoch": 4801.176470588235,
      "grad_norm": 17.786434173583984,
      "learning_rate": 1.988235294117647e-06,
      "loss": 0.5537,
      "step": 81620
    },
    {
      "epoch": 4801.764705882353,
      "grad_norm": 26.04404640197754,
      "learning_rate": 1.9823529411764706e-06,
      "loss": 0.7334,
      "step": 81630
    },
    {
      "epoch": 4802.35294117647,
      "grad_norm": 18.31936264038086,
      "learning_rate": 1.9764705882352944e-06,
      "loss": 0.7132,
      "step": 81640
    },
    {
      "epoch": 4802.941176470588,
      "grad_norm": 18.611045837402344,
      "learning_rate": 1.970588235294118e-06,
      "loss": 0.6591,
      "step": 81650
    },
    {
      "epoch": 4803.529411764706,
      "grad_norm": 18.886037826538086,
      "learning_rate": 1.9647058823529412e-06,
      "loss": 0.6125,
      "step": 81660
    },
    {
      "epoch": 4804.117647058823,
      "grad_norm": 19.099943161010742,
      "learning_rate": 1.9588235294117646e-06,
      "loss": 0.6515,
      "step": 81670
    },
    {
      "epoch": 4804.705882352941,
      "grad_norm": 19.635786056518555,
      "learning_rate": 1.9529411764705885e-06,
      "loss": 0.7005,
      "step": 81680
    },
    {
      "epoch": 4805.294117647059,
      "grad_norm": 21.08835220336914,
      "learning_rate": 1.947058823529412e-06,
      "loss": 0.6985,
      "step": 81690
    },
    {
      "epoch": 4805.882352941177,
      "grad_norm": 20.506776809692383,
      "learning_rate": 1.9411764705882353e-06,
      "loss": 0.6066,
      "step": 81700
    },
    {
      "epoch": 4806.470588235294,
      "grad_norm": 19.569997787475586,
      "learning_rate": 1.9352941176470587e-06,
      "loss": 0.6589,
      "step": 81710
    },
    {
      "epoch": 4807.058823529412,
      "grad_norm": 23.651691436767578,
      "learning_rate": 1.9294117647058825e-06,
      "loss": 0.5319,
      "step": 81720
    },
    {
      "epoch": 4807.64705882353,
      "grad_norm": 18.103195190429688,
      "learning_rate": 1.923529411764706e-06,
      "loss": 0.6714,
      "step": 81730
    },
    {
      "epoch": 4808.235294117647,
      "grad_norm": 19.03569984436035,
      "learning_rate": 1.9176470588235298e-06,
      "loss": 0.6637,
      "step": 81740
    },
    {
      "epoch": 4808.823529411765,
      "grad_norm": 21.412689208984375,
      "learning_rate": 1.911764705882353e-06,
      "loss": 0.7338,
      "step": 81750
    },
    {
      "epoch": 4809.411764705882,
      "grad_norm": 19.835891723632812,
      "learning_rate": 1.9058823529411764e-06,
      "loss": 0.7139,
      "step": 81760
    },
    {
      "epoch": 4810.0,
      "grad_norm": 19.61680030822754,
      "learning_rate": 1.9e-06,
      "loss": 0.6988,
      "step": 81770
    },
    {
      "epoch": 4810.588235294118,
      "grad_norm": 18.137222290039062,
      "learning_rate": 1.8941176470588236e-06,
      "loss": 0.6096,
      "step": 81780
    },
    {
      "epoch": 4811.176470588235,
      "grad_norm": 29.35160255432129,
      "learning_rate": 1.8882352941176473e-06,
      "loss": 0.5669,
      "step": 81790
    },
    {
      "epoch": 4811.764705882353,
      "grad_norm": 22.104698181152344,
      "learning_rate": 1.8823529411764705e-06,
      "loss": 0.7313,
      "step": 81800
    },
    {
      "epoch": 4812.35294117647,
      "grad_norm": 14.421262741088867,
      "learning_rate": 1.876470588235294e-06,
      "loss": 0.6265,
      "step": 81810
    },
    {
      "epoch": 4812.941176470588,
      "grad_norm": 16.815601348876953,
      "learning_rate": 1.8705882352941177e-06,
      "loss": 0.6737,
      "step": 81820
    },
    {
      "epoch": 4813.529411764706,
      "grad_norm": 19.066085815429688,
      "learning_rate": 1.8647058823529413e-06,
      "loss": 0.6403,
      "step": 81830
    },
    {
      "epoch": 4814.117647058823,
      "grad_norm": 17.86440086364746,
      "learning_rate": 1.858823529411765e-06,
      "loss": 0.6769,
      "step": 81840
    },
    {
      "epoch": 4814.705882352941,
      "grad_norm": 19.916425704956055,
      "learning_rate": 1.8529411764705882e-06,
      "loss": 0.7392,
      "step": 81850
    },
    {
      "epoch": 4815.294117647059,
      "grad_norm": 25.965110778808594,
      "learning_rate": 1.8470588235294118e-06,
      "loss": 0.6405,
      "step": 81860
    },
    {
      "epoch": 4815.882352941177,
      "grad_norm": 18.063692092895508,
      "learning_rate": 1.8411764705882354e-06,
      "loss": 0.6536,
      "step": 81870
    },
    {
      "epoch": 4816.470588235294,
      "grad_norm": 25.884742736816406,
      "learning_rate": 1.835294117647059e-06,
      "loss": 0.6179,
      "step": 81880
    },
    {
      "epoch": 4817.058823529412,
      "grad_norm": 17.35892105102539,
      "learning_rate": 1.8294117647058822e-06,
      "loss": 0.6142,
      "step": 81890
    },
    {
      "epoch": 4817.64705882353,
      "grad_norm": 25.340906143188477,
      "learning_rate": 1.8235294117647058e-06,
      "loss": 0.7338,
      "step": 81900
    },
    {
      "epoch": 4818.235294117647,
      "grad_norm": 16.27914810180664,
      "learning_rate": 1.8176470588235295e-06,
      "loss": 0.6347,
      "step": 81910
    },
    {
      "epoch": 4818.823529411765,
      "grad_norm": 25.4862060546875,
      "learning_rate": 1.811764705882353e-06,
      "loss": 0.5879,
      "step": 81920
    },
    {
      "epoch": 4819.411764705882,
      "grad_norm": 17.73908805847168,
      "learning_rate": 1.8058823529411767e-06,
      "loss": 0.5994,
      "step": 81930
    },
    {
      "epoch": 4820.0,
      "grad_norm": 14.555842399597168,
      "learning_rate": 1.8e-06,
      "loss": 0.6653,
      "step": 81940
    },
    {
      "epoch": 4820.588235294118,
      "grad_norm": 25.051454544067383,
      "learning_rate": 1.7941176470588235e-06,
      "loss": 0.7667,
      "step": 81950
    },
    {
      "epoch": 4821.176470588235,
      "grad_norm": 17.77153968811035,
      "learning_rate": 1.7882352941176472e-06,
      "loss": 0.5987,
      "step": 81960
    },
    {
      "epoch": 4821.764705882353,
      "grad_norm": 16.842823028564453,
      "learning_rate": 1.7823529411764708e-06,
      "loss": 0.6306,
      "step": 81970
    },
    {
      "epoch": 4822.35294117647,
      "grad_norm": 20.674516677856445,
      "learning_rate": 1.7764705882352944e-06,
      "loss": 0.5892,
      "step": 81980
    },
    {
      "epoch": 4822.941176470588,
      "grad_norm": 17.435829162597656,
      "learning_rate": 1.7705882352941176e-06,
      "loss": 0.5931,
      "step": 81990
    },
    {
      "epoch": 4823.529411764706,
      "grad_norm": 25.224382400512695,
      "learning_rate": 1.7647058823529412e-06,
      "loss": 0.6245,
      "step": 82000
    },
    {
      "epoch": 4824.117647058823,
      "grad_norm": 20.879301071166992,
      "learning_rate": 1.7588235294117649e-06,
      "loss": 0.6526,
      "step": 82010
    },
    {
      "epoch": 4824.705882352941,
      "grad_norm": 13.677312850952148,
      "learning_rate": 1.7529411764705885e-06,
      "loss": 0.6858,
      "step": 82020
    },
    {
      "epoch": 4825.294117647059,
      "grad_norm": 25.667259216308594,
      "learning_rate": 1.7470588235294117e-06,
      "loss": 0.632,
      "step": 82030
    },
    {
      "epoch": 4825.882352941177,
      "grad_norm": 32.59233474731445,
      "learning_rate": 1.7411764705882353e-06,
      "loss": 0.722,
      "step": 82040
    },
    {
      "epoch": 4826.470588235294,
      "grad_norm": 24.798437118530273,
      "learning_rate": 1.735294117647059e-06,
      "loss": 0.5931,
      "step": 82050
    },
    {
      "epoch": 4827.058823529412,
      "grad_norm": 30.028215408325195,
      "learning_rate": 1.7294117647058825e-06,
      "loss": 0.7225,
      "step": 82060
    },
    {
      "epoch": 4827.64705882353,
      "grad_norm": 14.480143547058105,
      "learning_rate": 1.7235294117647062e-06,
      "loss": 0.7055,
      "step": 82070
    },
    {
      "epoch": 4828.235294117647,
      "grad_norm": 18.40107536315918,
      "learning_rate": 1.7176470588235294e-06,
      "loss": 0.6676,
      "step": 82080
    },
    {
      "epoch": 4828.823529411765,
      "grad_norm": 19.975439071655273,
      "learning_rate": 1.711764705882353e-06,
      "loss": 0.6151,
      "step": 82090
    },
    {
      "epoch": 4829.411764705882,
      "grad_norm": 15.401872634887695,
      "learning_rate": 1.7058823529411766e-06,
      "loss": 0.5368,
      "step": 82100
    },
    {
      "epoch": 4830.0,
      "grad_norm": 23.598581314086914,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.6183,
      "step": 82110
    },
    {
      "epoch": 4830.588235294118,
      "grad_norm": 14.604777336120605,
      "learning_rate": 1.6941176470588234e-06,
      "loss": 0.6421,
      "step": 82120
    },
    {
      "epoch": 4831.176470588235,
      "grad_norm": 21.75442886352539,
      "learning_rate": 1.688235294117647e-06,
      "loss": 0.7437,
      "step": 82130
    },
    {
      "epoch": 4831.764705882353,
      "grad_norm": 22.426715850830078,
      "learning_rate": 1.6823529411764707e-06,
      "loss": 0.5793,
      "step": 82140
    },
    {
      "epoch": 4832.35294117647,
      "grad_norm": 15.760770797729492,
      "learning_rate": 1.6764705882352943e-06,
      "loss": 0.6329,
      "step": 82150
    },
    {
      "epoch": 4832.941176470588,
      "grad_norm": 32.99555587768555,
      "learning_rate": 1.670588235294118e-06,
      "loss": 0.635,
      "step": 82160
    },
    {
      "epoch": 4833.529411764706,
      "grad_norm": 39.909324645996094,
      "learning_rate": 1.6647058823529411e-06,
      "loss": 0.749,
      "step": 82170
    },
    {
      "epoch": 4834.117647058823,
      "grad_norm": 21.30601692199707,
      "learning_rate": 1.6588235294117648e-06,
      "loss": 0.5839,
      "step": 82180
    },
    {
      "epoch": 4834.705882352941,
      "grad_norm": 14.819856643676758,
      "learning_rate": 1.6529411764705884e-06,
      "loss": 0.6574,
      "step": 82190
    },
    {
      "epoch": 4835.294117647059,
      "grad_norm": 24.52944564819336,
      "learning_rate": 1.647058823529412e-06,
      "loss": 0.6791,
      "step": 82200
    },
    {
      "epoch": 4835.882352941177,
      "grad_norm": 24.468353271484375,
      "learning_rate": 1.6411764705882352e-06,
      "loss": 0.6816,
      "step": 82210
    },
    {
      "epoch": 4836.470588235294,
      "grad_norm": 19.83083724975586,
      "learning_rate": 1.6352941176470588e-06,
      "loss": 0.6684,
      "step": 82220
    },
    {
      "epoch": 4837.058823529412,
      "grad_norm": 18.781883239746094,
      "learning_rate": 1.6294117647058824e-06,
      "loss": 0.617,
      "step": 82230
    },
    {
      "epoch": 4837.64705882353,
      "grad_norm": 20.422712326049805,
      "learning_rate": 1.623529411764706e-06,
      "loss": 0.6594,
      "step": 82240
    },
    {
      "epoch": 4838.235294117647,
      "grad_norm": 29.957088470458984,
      "learning_rate": 1.6176470588235297e-06,
      "loss": 0.6332,
      "step": 82250
    },
    {
      "epoch": 4838.823529411765,
      "grad_norm": 21.00945281982422,
      "learning_rate": 1.6117647058823529e-06,
      "loss": 0.6493,
      "step": 82260
    },
    {
      "epoch": 4839.411764705882,
      "grad_norm": 28.400602340698242,
      "learning_rate": 1.6058823529411765e-06,
      "loss": 0.5778,
      "step": 82270
    },
    {
      "epoch": 4840.0,
      "grad_norm": 25.907310485839844,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.5688,
      "step": 82280
    },
    {
      "epoch": 4840.588235294118,
      "grad_norm": 20.53876304626465,
      "learning_rate": 1.5941176470588238e-06,
      "loss": 0.6558,
      "step": 82290
    },
    {
      "epoch": 4841.176470588235,
      "grad_norm": 24.19171142578125,
      "learning_rate": 1.588235294117647e-06,
      "loss": 0.7696,
      "step": 82300
    },
    {
      "epoch": 4841.764705882353,
      "grad_norm": 21.620275497436523,
      "learning_rate": 1.5823529411764706e-06,
      "loss": 0.7039,
      "step": 82310
    },
    {
      "epoch": 4842.35294117647,
      "grad_norm": 24.170562744140625,
      "learning_rate": 1.5764705882352942e-06,
      "loss": 0.7295,
      "step": 82320
    },
    {
      "epoch": 4842.941176470588,
      "grad_norm": 16.108312606811523,
      "learning_rate": 1.5705882352941178e-06,
      "loss": 0.5908,
      "step": 82330
    },
    {
      "epoch": 4843.529411764706,
      "grad_norm": 16.405874252319336,
      "learning_rate": 1.5647058823529415e-06,
      "loss": 0.6479,
      "step": 82340
    },
    {
      "epoch": 4844.117647058823,
      "grad_norm": 16.348522186279297,
      "learning_rate": 1.5588235294117649e-06,
      "loss": 0.7059,
      "step": 82350
    },
    {
      "epoch": 4844.705882352941,
      "grad_norm": 16.072872161865234,
      "learning_rate": 1.5529411764705883e-06,
      "loss": 0.6904,
      "step": 82360
    },
    {
      "epoch": 4845.294117647059,
      "grad_norm": 18.851787567138672,
      "learning_rate": 1.547058823529412e-06,
      "loss": 0.6481,
      "step": 82370
    },
    {
      "epoch": 4845.882352941177,
      "grad_norm": 26.976716995239258,
      "learning_rate": 1.5411764705882353e-06,
      "loss": 0.7826,
      "step": 82380
    },
    {
      "epoch": 4846.470588235294,
      "grad_norm": 19.636150360107422,
      "learning_rate": 1.535294117647059e-06,
      "loss": 0.6043,
      "step": 82390
    },
    {
      "epoch": 4847.058823529412,
      "grad_norm": 26.793384552001953,
      "learning_rate": 1.5294117647058826e-06,
      "loss": 0.7029,
      "step": 82400
    },
    {
      "epoch": 4847.64705882353,
      "grad_norm": 20.72931480407715,
      "learning_rate": 1.523529411764706e-06,
      "loss": 0.6476,
      "step": 82410
    },
    {
      "epoch": 4848.235294117647,
      "grad_norm": 18.69862174987793,
      "learning_rate": 1.5176470588235296e-06,
      "loss": 0.6198,
      "step": 82420
    },
    {
      "epoch": 4848.823529411765,
      "grad_norm": 14.543549537658691,
      "learning_rate": 1.511764705882353e-06,
      "loss": 0.7064,
      "step": 82430
    },
    {
      "epoch": 4849.411764705882,
      "grad_norm": 19.967376708984375,
      "learning_rate": 1.5058823529411766e-06,
      "loss": 0.6339,
      "step": 82440
    },
    {
      "epoch": 4850.0,
      "grad_norm": 29.48239517211914,
      "learning_rate": 1.5e-06,
      "loss": 0.6889,
      "step": 82450
    },
    {
      "epoch": 4850.588235294118,
      "grad_norm": 18.299102783203125,
      "learning_rate": 1.4941176470588237e-06,
      "loss": 0.5868,
      "step": 82460
    },
    {
      "epoch": 4851.176470588235,
      "grad_norm": 25.829811096191406,
      "learning_rate": 1.488235294117647e-06,
      "loss": 0.6039,
      "step": 82470
    },
    {
      "epoch": 4851.764705882353,
      "grad_norm": 25.139440536499023,
      "learning_rate": 1.4823529411764707e-06,
      "loss": 0.6124,
      "step": 82480
    },
    {
      "epoch": 4852.35294117647,
      "grad_norm": 23.938583374023438,
      "learning_rate": 1.4764705882352943e-06,
      "loss": 0.7551,
      "step": 82490
    },
    {
      "epoch": 4852.941176470588,
      "grad_norm": 19.816509246826172,
      "learning_rate": 1.4705882352941177e-06,
      "loss": 0.6327,
      "step": 82500
    },
    {
      "epoch": 4853.529411764706,
      "grad_norm": 18.426437377929688,
      "learning_rate": 1.4647058823529414e-06,
      "loss": 0.6995,
      "step": 82510
    },
    {
      "epoch": 4854.117647058823,
      "grad_norm": 12.993247032165527,
      "learning_rate": 1.4588235294117648e-06,
      "loss": 0.6142,
      "step": 82520
    },
    {
      "epoch": 4854.705882352941,
      "grad_norm": 18.772113800048828,
      "learning_rate": 1.4529411764705884e-06,
      "loss": 0.6852,
      "step": 82530
    },
    {
      "epoch": 4855.294117647059,
      "grad_norm": 23.095409393310547,
      "learning_rate": 1.4470588235294118e-06,
      "loss": 0.7534,
      "step": 82540
    },
    {
      "epoch": 4855.882352941177,
      "grad_norm": 17.89740562438965,
      "learning_rate": 1.4411764705882354e-06,
      "loss": 0.6883,
      "step": 82550
    },
    {
      "epoch": 4856.470588235294,
      "grad_norm": 28.05010414123535,
      "learning_rate": 1.4352941176470588e-06,
      "loss": 0.7458,
      "step": 82560
    },
    {
      "epoch": 4857.058823529412,
      "grad_norm": 23.300527572631836,
      "learning_rate": 1.4294117647058825e-06,
      "loss": 0.6035,
      "step": 82570
    },
    {
      "epoch": 4857.64705882353,
      "grad_norm": 18.186059951782227,
      "learning_rate": 1.423529411764706e-06,
      "loss": 0.6309,
      "step": 82580
    },
    {
      "epoch": 4858.235294117647,
      "grad_norm": 20.286869049072266,
      "learning_rate": 1.4176470588235295e-06,
      "loss": 0.708,
      "step": 82590
    },
    {
      "epoch": 4858.823529411765,
      "grad_norm": 23.60569190979004,
      "learning_rate": 1.4117647058823531e-06,
      "loss": 0.5662,
      "step": 82600
    },
    {
      "epoch": 4859.411764705882,
      "grad_norm": 18.019350051879883,
      "learning_rate": 1.4058823529411765e-06,
      "loss": 0.7297,
      "step": 82610
    },
    {
      "epoch": 4860.0,
      "grad_norm": 16.859657287597656,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.6468,
      "step": 82620
    },
    {
      "epoch": 4860.588235294118,
      "grad_norm": 19.697425842285156,
      "learning_rate": 1.3941176470588236e-06,
      "loss": 0.7047,
      "step": 82630
    },
    {
      "epoch": 4861.176470588235,
      "grad_norm": 14.412353515625,
      "learning_rate": 1.3882352941176472e-06,
      "loss": 0.6506,
      "step": 82640
    },
    {
      "epoch": 4861.764705882353,
      "grad_norm": 20.105884552001953,
      "learning_rate": 1.3823529411764708e-06,
      "loss": 0.7009,
      "step": 82650
    },
    {
      "epoch": 4862.35294117647,
      "grad_norm": 23.307790756225586,
      "learning_rate": 1.3764705882352942e-06,
      "loss": 0.6741,
      "step": 82660
    },
    {
      "epoch": 4862.941176470588,
      "grad_norm": 19.519046783447266,
      "learning_rate": 1.3705882352941178e-06,
      "loss": 0.7607,
      "step": 82670
    },
    {
      "epoch": 4863.529411764706,
      "grad_norm": 19.980348587036133,
      "learning_rate": 1.3647058823529413e-06,
      "loss": 0.7303,
      "step": 82680
    },
    {
      "epoch": 4864.117647058823,
      "grad_norm": 18.982892990112305,
      "learning_rate": 1.3588235294117649e-06,
      "loss": 0.6465,
      "step": 82690
    },
    {
      "epoch": 4864.705882352941,
      "grad_norm": 20.126995086669922,
      "learning_rate": 1.3529411764705883e-06,
      "loss": 0.7428,
      "step": 82700
    },
    {
      "epoch": 4865.294117647059,
      "grad_norm": 27.52391242980957,
      "learning_rate": 1.347058823529412e-06,
      "loss": 0.6237,
      "step": 82710
    },
    {
      "epoch": 4865.882352941177,
      "grad_norm": 19.927663803100586,
      "learning_rate": 1.3411764705882353e-06,
      "loss": 0.6189,
      "step": 82720
    },
    {
      "epoch": 4866.470588235294,
      "grad_norm": 19.920427322387695,
      "learning_rate": 1.335294117647059e-06,
      "loss": 0.6827,
      "step": 82730
    },
    {
      "epoch": 4867.058823529412,
      "grad_norm": 19.783708572387695,
      "learning_rate": 1.3294117647058826e-06,
      "loss": 0.5992,
      "step": 82740
    },
    {
      "epoch": 4867.64705882353,
      "grad_norm": 23.214553833007812,
      "learning_rate": 1.323529411764706e-06,
      "loss": 0.6445,
      "step": 82750
    },
    {
      "epoch": 4868.235294117647,
      "grad_norm": 21.446826934814453,
      "learning_rate": 1.3176470588235296e-06,
      "loss": 0.6255,
      "step": 82760
    },
    {
      "epoch": 4868.823529411765,
      "grad_norm": 27.01414680480957,
      "learning_rate": 1.311764705882353e-06,
      "loss": 0.6426,
      "step": 82770
    },
    {
      "epoch": 4869.411764705882,
      "grad_norm": 17.98564910888672,
      "learning_rate": 1.3058823529411766e-06,
      "loss": 0.6116,
      "step": 82780
    },
    {
      "epoch": 4870.0,
      "grad_norm": 30.373727798461914,
      "learning_rate": 1.3e-06,
      "loss": 0.5889,
      "step": 82790
    },
    {
      "epoch": 4870.588235294118,
      "grad_norm": 22.353166580200195,
      "learning_rate": 1.2941176470588237e-06,
      "loss": 0.7167,
      "step": 82800
    },
    {
      "epoch": 4871.176470588235,
      "grad_norm": 24.53864288330078,
      "learning_rate": 1.288235294117647e-06,
      "loss": 0.6561,
      "step": 82810
    },
    {
      "epoch": 4871.764705882353,
      "grad_norm": 20.501283645629883,
      "learning_rate": 1.2823529411764707e-06,
      "loss": 0.6914,
      "step": 82820
    },
    {
      "epoch": 4872.35294117647,
      "grad_norm": 18.935306549072266,
      "learning_rate": 1.2764705882352943e-06,
      "loss": 0.642,
      "step": 82830
    },
    {
      "epoch": 4872.941176470588,
      "grad_norm": 26.27843475341797,
      "learning_rate": 1.2705882352941177e-06,
      "loss": 0.6492,
      "step": 82840
    },
    {
      "epoch": 4873.529411764706,
      "grad_norm": 14.897013664245605,
      "learning_rate": 1.2647058823529414e-06,
      "loss": 0.6232,
      "step": 82850
    },
    {
      "epoch": 4874.117647058823,
      "grad_norm": 17.686168670654297,
      "learning_rate": 1.2588235294117648e-06,
      "loss": 0.6382,
      "step": 82860
    },
    {
      "epoch": 4874.705882352941,
      "grad_norm": 19.9759521484375,
      "learning_rate": 1.2529411764705884e-06,
      "loss": 0.6822,
      "step": 82870
    },
    {
      "epoch": 4875.294117647059,
      "grad_norm": 13.263544082641602,
      "learning_rate": 1.2470588235294118e-06,
      "loss": 0.5499,
      "step": 82880
    },
    {
      "epoch": 4875.882352941177,
      "grad_norm": 37.497825622558594,
      "learning_rate": 1.2411764705882354e-06,
      "loss": 0.8336,
      "step": 82890
    },
    {
      "epoch": 4876.470588235294,
      "grad_norm": 22.498899459838867,
      "learning_rate": 1.2352941176470588e-06,
      "loss": 0.6509,
      "step": 82900
    },
    {
      "epoch": 4877.058823529412,
      "grad_norm": 20.257719039916992,
      "learning_rate": 1.2294117647058825e-06,
      "loss": 0.7454,
      "step": 82910
    },
    {
      "epoch": 4877.64705882353,
      "grad_norm": 18.556203842163086,
      "learning_rate": 1.223529411764706e-06,
      "loss": 0.6308,
      "step": 82920
    },
    {
      "epoch": 4878.235294117647,
      "grad_norm": 22.99816131591797,
      "learning_rate": 1.2176470588235295e-06,
      "loss": 0.623,
      "step": 82930
    },
    {
      "epoch": 4878.823529411765,
      "grad_norm": 22.02849578857422,
      "learning_rate": 1.2117647058823531e-06,
      "loss": 0.6469,
      "step": 82940
    },
    {
      "epoch": 4879.411764705882,
      "grad_norm": 15.271291732788086,
      "learning_rate": 1.2058823529411765e-06,
      "loss": 0.7192,
      "step": 82950
    },
    {
      "epoch": 4880.0,
      "grad_norm": 27.166133880615234,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6982,
      "step": 82960
    },
    {
      "epoch": 4880.588235294118,
      "grad_norm": 29.086578369140625,
      "learning_rate": 1.1941176470588236e-06,
      "loss": 0.6324,
      "step": 82970
    },
    {
      "epoch": 4881.176470588235,
      "grad_norm": 20.63494873046875,
      "learning_rate": 1.1882352941176472e-06,
      "loss": 0.6306,
      "step": 82980
    },
    {
      "epoch": 4881.764705882353,
      "grad_norm": 18.848045349121094,
      "learning_rate": 1.1823529411764708e-06,
      "loss": 0.6615,
      "step": 82990
    },
    {
      "epoch": 4882.35294117647,
      "grad_norm": 21.9718017578125,
      "learning_rate": 1.1764705882352942e-06,
      "loss": 0.6329,
      "step": 83000
    },
    {
      "epoch": 4882.941176470588,
      "grad_norm": 22.683101654052734,
      "learning_rate": 1.1705882352941178e-06,
      "loss": 0.6553,
      "step": 83010
    },
    {
      "epoch": 4883.529411764706,
      "grad_norm": 23.274810791015625,
      "learning_rate": 1.1647058823529413e-06,
      "loss": 0.6945,
      "step": 83020
    },
    {
      "epoch": 4884.117647058823,
      "grad_norm": 23.119766235351562,
      "learning_rate": 1.1588235294117649e-06,
      "loss": 0.7248,
      "step": 83030
    },
    {
      "epoch": 4884.705882352941,
      "grad_norm": 22.067907333374023,
      "learning_rate": 1.1529411764705883e-06,
      "loss": 0.696,
      "step": 83040
    },
    {
      "epoch": 4885.294117647059,
      "grad_norm": 16.10346794128418,
      "learning_rate": 1.147058823529412e-06,
      "loss": 0.6391,
      "step": 83050
    },
    {
      "epoch": 4885.882352941177,
      "grad_norm": 22.665029525756836,
      "learning_rate": 1.1411764705882353e-06,
      "loss": 0.6689,
      "step": 83060
    },
    {
      "epoch": 4886.470588235294,
      "grad_norm": 21.587055206298828,
      "learning_rate": 1.135294117647059e-06,
      "loss": 0.6304,
      "step": 83070
    },
    {
      "epoch": 4887.058823529412,
      "grad_norm": 22.17017364501953,
      "learning_rate": 1.1294117647058826e-06,
      "loss": 0.6631,
      "step": 83080
    },
    {
      "epoch": 4887.64705882353,
      "grad_norm": 19.61390495300293,
      "learning_rate": 1.123529411764706e-06,
      "loss": 0.666,
      "step": 83090
    },
    {
      "epoch": 4888.235294117647,
      "grad_norm": 30.550846099853516,
      "learning_rate": 1.1176470588235296e-06,
      "loss": 0.543,
      "step": 83100
    },
    {
      "epoch": 4888.823529411765,
      "grad_norm": 14.628443717956543,
      "learning_rate": 1.111764705882353e-06,
      "loss": 0.6205,
      "step": 83110
    },
    {
      "epoch": 4889.411764705882,
      "grad_norm": 23.531936645507812,
      "learning_rate": 1.1058823529411766e-06,
      "loss": 0.6877,
      "step": 83120
    },
    {
      "epoch": 4890.0,
      "grad_norm": 23.486143112182617,
      "learning_rate": 1.1e-06,
      "loss": 0.6444,
      "step": 83130
    },
    {
      "epoch": 4890.588235294118,
      "grad_norm": 14.963345527648926,
      "learning_rate": 1.0941176470588237e-06,
      "loss": 0.6179,
      "step": 83140
    },
    {
      "epoch": 4891.176470588235,
      "grad_norm": 23.476242065429688,
      "learning_rate": 1.088235294117647e-06,
      "loss": 0.6571,
      "step": 83150
    },
    {
      "epoch": 4891.764705882353,
      "grad_norm": 17.745107650756836,
      "learning_rate": 1.0823529411764707e-06,
      "loss": 0.6368,
      "step": 83160
    },
    {
      "epoch": 4892.35294117647,
      "grad_norm": 16.252126693725586,
      "learning_rate": 1.0764705882352943e-06,
      "loss": 0.7079,
      "step": 83170
    },
    {
      "epoch": 4892.941176470588,
      "grad_norm": 26.6908016204834,
      "learning_rate": 1.0705882352941177e-06,
      "loss": 0.7059,
      "step": 83180
    },
    {
      "epoch": 4893.529411764706,
      "grad_norm": 13.84255313873291,
      "learning_rate": 1.0647058823529414e-06,
      "loss": 0.6494,
      "step": 83190
    },
    {
      "epoch": 4894.117647058823,
      "grad_norm": 15.125049591064453,
      "learning_rate": 1.0588235294117648e-06,
      "loss": 0.6446,
      "step": 83200
    },
    {
      "epoch": 4894.705882352941,
      "grad_norm": 32.414920806884766,
      "learning_rate": 1.0529411764705884e-06,
      "loss": 0.6776,
      "step": 83210
    },
    {
      "epoch": 4895.294117647059,
      "grad_norm": 17.823646545410156,
      "learning_rate": 1.0470588235294118e-06,
      "loss": 0.6697,
      "step": 83220
    },
    {
      "epoch": 4895.882352941177,
      "grad_norm": 12.451179504394531,
      "learning_rate": 1.0411764705882354e-06,
      "loss": 0.6157,
      "step": 83230
    },
    {
      "epoch": 4896.470588235294,
      "grad_norm": 24.712690353393555,
      "learning_rate": 1.0352941176470589e-06,
      "loss": 0.6444,
      "step": 83240
    },
    {
      "epoch": 4897.058823529412,
      "grad_norm": 23.379390716552734,
      "learning_rate": 1.0294117647058825e-06,
      "loss": 0.6611,
      "step": 83250
    },
    {
      "epoch": 4897.64705882353,
      "grad_norm": 17.3198299407959,
      "learning_rate": 1.023529411764706e-06,
      "loss": 0.5931,
      "step": 83260
    },
    {
      "epoch": 4898.235294117647,
      "grad_norm": 23.957128524780273,
      "learning_rate": 1.0176470588235295e-06,
      "loss": 0.6117,
      "step": 83270
    },
    {
      "epoch": 4898.823529411765,
      "grad_norm": 18.318403244018555,
      "learning_rate": 1.0117647058823531e-06,
      "loss": 0.6334,
      "step": 83280
    },
    {
      "epoch": 4899.411764705882,
      "grad_norm": 17.94083595275879,
      "learning_rate": 1.0058823529411765e-06,
      "loss": 0.6188,
      "step": 83290
    },
    {
      "epoch": 4900.0,
      "grad_norm": 25.807050704956055,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.7122,
      "step": 83300
    },
    {
      "epoch": 4900.588235294118,
      "grad_norm": 22.532567977905273,
      "learning_rate": 9.941176470588236e-07,
      "loss": 0.7203,
      "step": 83310
    },
    {
      "epoch": 4901.176470588235,
      "grad_norm": 20.113027572631836,
      "learning_rate": 9.882352941176472e-07,
      "loss": 0.6215,
      "step": 83320
    },
    {
      "epoch": 4901.764705882353,
      "grad_norm": 20.449800491333008,
      "learning_rate": 9.823529411764706e-07,
      "loss": 0.6061,
      "step": 83330
    },
    {
      "epoch": 4902.35294117647,
      "grad_norm": 23.01700782775879,
      "learning_rate": 9.764705882352942e-07,
      "loss": 0.6746,
      "step": 83340
    },
    {
      "epoch": 4902.941176470588,
      "grad_norm": 26.371061325073242,
      "learning_rate": 9.705882352941176e-07,
      "loss": 0.7322,
      "step": 83350
    },
    {
      "epoch": 4903.529411764706,
      "grad_norm": 22.40038299560547,
      "learning_rate": 9.647058823529413e-07,
      "loss": 0.6818,
      "step": 83360
    },
    {
      "epoch": 4904.117647058823,
      "grad_norm": 26.330387115478516,
      "learning_rate": 9.588235294117649e-07,
      "loss": 0.6266,
      "step": 83370
    },
    {
      "epoch": 4904.705882352941,
      "grad_norm": 19.8239803314209,
      "learning_rate": 9.529411764705882e-07,
      "loss": 0.749,
      "step": 83380
    },
    {
      "epoch": 4905.294117647059,
      "grad_norm": 20.165019989013672,
      "learning_rate": 9.470588235294118e-07,
      "loss": 0.6179,
      "step": 83390
    },
    {
      "epoch": 4905.882352941177,
      "grad_norm": 16.77768898010254,
      "learning_rate": 9.411764705882352e-07,
      "loss": 0.6969,
      "step": 83400
    },
    {
      "epoch": 4906.470588235294,
      "grad_norm": 21.305917739868164,
      "learning_rate": 9.352941176470589e-07,
      "loss": 0.6143,
      "step": 83410
    },
    {
      "epoch": 4907.058823529412,
      "grad_norm": 15.185848236083984,
      "learning_rate": 9.294117647058825e-07,
      "loss": 0.5415,
      "step": 83420
    },
    {
      "epoch": 4907.64705882353,
      "grad_norm": 20.765851974487305,
      "learning_rate": 9.235294117647059e-07,
      "loss": 0.6123,
      "step": 83430
    },
    {
      "epoch": 4908.235294117647,
      "grad_norm": 22.597148895263672,
      "learning_rate": 9.176470588235295e-07,
      "loss": 0.6661,
      "step": 83440
    },
    {
      "epoch": 4908.823529411765,
      "grad_norm": 21.499805450439453,
      "learning_rate": 9.117647058823529e-07,
      "loss": 0.634,
      "step": 83450
    },
    {
      "epoch": 4909.411764705882,
      "grad_norm": 18.43699836730957,
      "learning_rate": 9.058823529411765e-07,
      "loss": 0.6322,
      "step": 83460
    },
    {
      "epoch": 4910.0,
      "grad_norm": 18.48396873474121,
      "learning_rate": 9e-07,
      "loss": 0.6663,
      "step": 83470
    },
    {
      "epoch": 4910.588235294118,
      "grad_norm": 21.463682174682617,
      "learning_rate": 8.941176470588236e-07,
      "loss": 0.7217,
      "step": 83480
    },
    {
      "epoch": 4911.176470588235,
      "grad_norm": 24.427431106567383,
      "learning_rate": 8.882352941176472e-07,
      "loss": 0.6741,
      "step": 83490
    },
    {
      "epoch": 4911.764705882353,
      "grad_norm": 27.9637393951416,
      "learning_rate": 8.823529411764706e-07,
      "loss": 0.6723,
      "step": 83500
    },
    {
      "epoch": 4912.35294117647,
      "grad_norm": 22.556385040283203,
      "learning_rate": 8.764705882352942e-07,
      "loss": 0.6392,
      "step": 83510
    },
    {
      "epoch": 4912.941176470588,
      "grad_norm": 21.90988540649414,
      "learning_rate": 8.705882352941177e-07,
      "loss": 0.776,
      "step": 83520
    },
    {
      "epoch": 4913.529411764706,
      "grad_norm": 22.61054039001465,
      "learning_rate": 8.647058823529413e-07,
      "loss": 0.6502,
      "step": 83530
    },
    {
      "epoch": 4914.117647058823,
      "grad_norm": 26.578079223632812,
      "learning_rate": 8.588235294117647e-07,
      "loss": 0.6162,
      "step": 83540
    },
    {
      "epoch": 4914.705882352941,
      "grad_norm": 20.846654891967773,
      "learning_rate": 8.529411764705883e-07,
      "loss": 0.5586,
      "step": 83550
    },
    {
      "epoch": 4915.294117647059,
      "grad_norm": 20.202959060668945,
      "learning_rate": 8.470588235294117e-07,
      "loss": 0.7016,
      "step": 83560
    },
    {
      "epoch": 4915.882352941177,
      "grad_norm": 18.99308204650879,
      "learning_rate": 8.411764705882353e-07,
      "loss": 0.6927,
      "step": 83570
    },
    {
      "epoch": 4916.470588235294,
      "grad_norm": 21.70757293701172,
      "learning_rate": 8.35294117647059e-07,
      "loss": 0.5941,
      "step": 83580
    },
    {
      "epoch": 4917.058823529412,
      "grad_norm": 19.992443084716797,
      "learning_rate": 8.294117647058824e-07,
      "loss": 0.6736,
      "step": 83590
    },
    {
      "epoch": 4917.64705882353,
      "grad_norm": 23.774768829345703,
      "learning_rate": 8.23529411764706e-07,
      "loss": 0.6307,
      "step": 83600
    },
    {
      "epoch": 4918.235294117647,
      "grad_norm": 17.228139877319336,
      "learning_rate": 8.176470588235294e-07,
      "loss": 0.6471,
      "step": 83610
    },
    {
      "epoch": 4918.823529411765,
      "grad_norm": 20.737258911132812,
      "learning_rate": 8.11764705882353e-07,
      "loss": 0.6038,
      "step": 83620
    },
    {
      "epoch": 4919.411764705882,
      "grad_norm": 14.535374641418457,
      "learning_rate": 8.058823529411764e-07,
      "loss": 0.576,
      "step": 83630
    },
    {
      "epoch": 4920.0,
      "grad_norm": 36.572898864746094,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.65,
      "step": 83640
    },
    {
      "epoch": 4920.588235294118,
      "grad_norm": 18.07529067993164,
      "learning_rate": 7.941176470588235e-07,
      "loss": 0.7013,
      "step": 83650
    },
    {
      "epoch": 4921.176470588235,
      "grad_norm": 22.619792938232422,
      "learning_rate": 7.882352941176471e-07,
      "loss": 0.6134,
      "step": 83660
    },
    {
      "epoch": 4921.764705882353,
      "grad_norm": 19.445648193359375,
      "learning_rate": 7.823529411764707e-07,
      "loss": 0.621,
      "step": 83670
    },
    {
      "epoch": 4922.35294117647,
      "grad_norm": 33.17913818359375,
      "learning_rate": 7.764705882352941e-07,
      "loss": 0.6704,
      "step": 83680
    },
    {
      "epoch": 4922.941176470588,
      "grad_norm": 18.724685668945312,
      "learning_rate": 7.705882352941177e-07,
      "loss": 0.662,
      "step": 83690
    },
    {
      "epoch": 4923.529411764706,
      "grad_norm": 16.781858444213867,
      "learning_rate": 7.647058823529413e-07,
      "loss": 0.6645,
      "step": 83700
    },
    {
      "epoch": 4924.117647058823,
      "grad_norm": 18.670499801635742,
      "learning_rate": 7.588235294117648e-07,
      "loss": 0.5882,
      "step": 83710
    },
    {
      "epoch": 4924.705882352941,
      "grad_norm": 15.869852066040039,
      "learning_rate": 7.529411764705883e-07,
      "loss": 0.6779,
      "step": 83720
    },
    {
      "epoch": 4925.294117647059,
      "grad_norm": 14.92857551574707,
      "learning_rate": 7.470588235294118e-07,
      "loss": 0.68,
      "step": 83730
    },
    {
      "epoch": 4925.882352941177,
      "grad_norm": 23.16310691833496,
      "learning_rate": 7.411764705882353e-07,
      "loss": 0.7144,
      "step": 83740
    },
    {
      "epoch": 4926.470588235294,
      "grad_norm": 24.816329956054688,
      "learning_rate": 7.352941176470589e-07,
      "loss": 0.6622,
      "step": 83750
    },
    {
      "epoch": 4927.058823529412,
      "grad_norm": 21.20867919921875,
      "learning_rate": 7.294117647058824e-07,
      "loss": 0.7515,
      "step": 83760
    },
    {
      "epoch": 4927.64705882353,
      "grad_norm": 22.729841232299805,
      "learning_rate": 7.235294117647059e-07,
      "loss": 0.6271,
      "step": 83770
    },
    {
      "epoch": 4928.235294117647,
      "grad_norm": 19.657798767089844,
      "learning_rate": 7.176470588235294e-07,
      "loss": 0.6388,
      "step": 83780
    },
    {
      "epoch": 4928.823529411765,
      "grad_norm": 21.248380661010742,
      "learning_rate": 7.11764705882353e-07,
      "loss": 0.6838,
      "step": 83790
    },
    {
      "epoch": 4929.411764705882,
      "grad_norm": 19.453340530395508,
      "learning_rate": 7.058823529411766e-07,
      "loss": 0.7099,
      "step": 83800
    },
    {
      "epoch": 4930.0,
      "grad_norm": 23.806474685668945,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.6213,
      "step": 83810
    },
    {
      "epoch": 4930.588235294118,
      "grad_norm": 21.28053092956543,
      "learning_rate": 6.941176470588236e-07,
      "loss": 0.7794,
      "step": 83820
    },
    {
      "epoch": 4931.176470588235,
      "grad_norm": 25.112449645996094,
      "learning_rate": 6.882352941176471e-07,
      "loss": 0.6343,
      "step": 83830
    },
    {
      "epoch": 4931.764705882353,
      "grad_norm": 21.023353576660156,
      "learning_rate": 6.823529411764706e-07,
      "loss": 0.6692,
      "step": 83840
    },
    {
      "epoch": 4932.35294117647,
      "grad_norm": 25.117633819580078,
      "learning_rate": 6.764705882352941e-07,
      "loss": 0.5357,
      "step": 83850
    },
    {
      "epoch": 4932.941176470588,
      "grad_norm": 20.443632125854492,
      "learning_rate": 6.705882352941177e-07,
      "loss": 0.652,
      "step": 83860
    },
    {
      "epoch": 4933.529411764706,
      "grad_norm": 22.36895179748535,
      "learning_rate": 6.647058823529413e-07,
      "loss": 0.6264,
      "step": 83870
    },
    {
      "epoch": 4934.117647058823,
      "grad_norm": 31.162532806396484,
      "learning_rate": 6.588235294117648e-07,
      "loss": 0.7411,
      "step": 83880
    },
    {
      "epoch": 4934.705882352941,
      "grad_norm": 24.630455017089844,
      "learning_rate": 6.529411764705883e-07,
      "loss": 0.7168,
      "step": 83890
    },
    {
      "epoch": 4935.294117647059,
      "grad_norm": 19.70574951171875,
      "learning_rate": 6.470588235294118e-07,
      "loss": 0.6339,
      "step": 83900
    },
    {
      "epoch": 4935.882352941177,
      "grad_norm": 18.06941795349121,
      "learning_rate": 6.411764705882354e-07,
      "loss": 0.6085,
      "step": 83910
    },
    {
      "epoch": 4936.470588235294,
      "grad_norm": 20.185379028320312,
      "learning_rate": 6.352941176470589e-07,
      "loss": 0.7253,
      "step": 83920
    },
    {
      "epoch": 4937.058823529412,
      "grad_norm": 19.79907989501953,
      "learning_rate": 6.294117647058824e-07,
      "loss": 0.6422,
      "step": 83930
    },
    {
      "epoch": 4937.64705882353,
      "grad_norm": 20.8925724029541,
      "learning_rate": 6.235294117647059e-07,
      "loss": 0.5988,
      "step": 83940
    },
    {
      "epoch": 4938.235294117647,
      "grad_norm": 27.457998275756836,
      "learning_rate": 6.176470588235294e-07,
      "loss": 0.6197,
      "step": 83950
    },
    {
      "epoch": 4938.823529411765,
      "grad_norm": 15.295443534851074,
      "learning_rate": 6.11764705882353e-07,
      "loss": 0.6875,
      "step": 83960
    },
    {
      "epoch": 4939.411764705882,
      "grad_norm": 18.611812591552734,
      "learning_rate": 6.058823529411766e-07,
      "loss": 0.5811,
      "step": 83970
    },
    {
      "epoch": 4940.0,
      "grad_norm": 23.766910552978516,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.5481,
      "step": 83980
    },
    {
      "epoch": 4940.588235294118,
      "grad_norm": 17.266157150268555,
      "learning_rate": 5.941176470588236e-07,
      "loss": 0.6625,
      "step": 83990
    },
    {
      "epoch": 4941.176470588235,
      "grad_norm": 15.10311222076416,
      "learning_rate": 5.882352941176471e-07,
      "loss": 0.6783,
      "step": 84000
    },
    {
      "epoch": 4941.764705882353,
      "grad_norm": 24.98343276977539,
      "learning_rate": 5.823529411764706e-07,
      "loss": 0.6736,
      "step": 84010
    },
    {
      "epoch": 4942.35294117647,
      "grad_norm": 20.88799476623535,
      "learning_rate": 5.764705882352941e-07,
      "loss": 0.6316,
      "step": 84020
    },
    {
      "epoch": 4942.941176470588,
      "grad_norm": 22.377912521362305,
      "learning_rate": 5.705882352941177e-07,
      "loss": 0.6922,
      "step": 84030
    },
    {
      "epoch": 4943.529411764706,
      "grad_norm": 19.273487091064453,
      "learning_rate": 5.647058823529413e-07,
      "loss": 0.7079,
      "step": 84040
    },
    {
      "epoch": 4944.117647058823,
      "grad_norm": 20.198694229125977,
      "learning_rate": 5.588235294117648e-07,
      "loss": 0.7331,
      "step": 84050
    },
    {
      "epoch": 4944.705882352941,
      "grad_norm": 14.649646759033203,
      "learning_rate": 5.529411764705883e-07,
      "loss": 0.6829,
      "step": 84060
    },
    {
      "epoch": 4945.294117647059,
      "grad_norm": 18.97890853881836,
      "learning_rate": 5.470588235294118e-07,
      "loss": 0.6647,
      "step": 84070
    },
    {
      "epoch": 4945.882352941177,
      "grad_norm": 25.534149169921875,
      "learning_rate": 5.411764705882354e-07,
      "loss": 0.6652,
      "step": 84080
    },
    {
      "epoch": 4946.470588235294,
      "grad_norm": 20.290624618530273,
      "learning_rate": 5.352941176470589e-07,
      "loss": 0.6064,
      "step": 84090
    },
    {
      "epoch": 4947.058823529412,
      "grad_norm": 22.339019775390625,
      "learning_rate": 5.294117647058824e-07,
      "loss": 0.5868,
      "step": 84100
    },
    {
      "epoch": 4947.64705882353,
      "grad_norm": 17.265605926513672,
      "learning_rate": 5.235294117647059e-07,
      "loss": 0.6355,
      "step": 84110
    },
    {
      "epoch": 4948.235294117647,
      "grad_norm": 27.72126579284668,
      "learning_rate": 5.176470588235294e-07,
      "loss": 0.681,
      "step": 84120
    },
    {
      "epoch": 4948.823529411765,
      "grad_norm": 23.099838256835938,
      "learning_rate": 5.11764705882353e-07,
      "loss": 0.6669,
      "step": 84130
    },
    {
      "epoch": 4949.411764705882,
      "grad_norm": 22.30946922302246,
      "learning_rate": 5.058823529411766e-07,
      "loss": 0.6291,
      "step": 84140
    },
    {
      "epoch": 4950.0,
      "grad_norm": 16.673364639282227,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.6012,
      "step": 84150
    },
    {
      "epoch": 4950.588235294118,
      "grad_norm": 15.549086570739746,
      "learning_rate": 4.941176470588236e-07,
      "loss": 0.7124,
      "step": 84160
    },
    {
      "epoch": 4951.176470588235,
      "grad_norm": 16.660690307617188,
      "learning_rate": 4.882352941176471e-07,
      "loss": 0.6502,
      "step": 84170
    },
    {
      "epoch": 4951.764705882353,
      "grad_norm": 20.01597023010254,
      "learning_rate": 4.823529411764706e-07,
      "loss": 0.5709,
      "step": 84180
    },
    {
      "epoch": 4952.35294117647,
      "grad_norm": 22.505449295043945,
      "learning_rate": 4.764705882352941e-07,
      "loss": 0.6463,
      "step": 84190
    },
    {
      "epoch": 4952.941176470588,
      "grad_norm": 21.171274185180664,
      "learning_rate": 4.705882352941176e-07,
      "loss": 0.7062,
      "step": 84200
    },
    {
      "epoch": 4953.529411764706,
      "grad_norm": 15.751822471618652,
      "learning_rate": 4.6470588235294124e-07,
      "loss": 0.7575,
      "step": 84210
    },
    {
      "epoch": 4954.117647058823,
      "grad_norm": 20.51425552368164,
      "learning_rate": 4.5882352941176476e-07,
      "loss": 0.6302,
      "step": 84220
    },
    {
      "epoch": 4954.705882352941,
      "grad_norm": 15.976255416870117,
      "learning_rate": 4.529411764705883e-07,
      "loss": 0.6233,
      "step": 84230
    },
    {
      "epoch": 4955.294117647059,
      "grad_norm": 23.697132110595703,
      "learning_rate": 4.470588235294118e-07,
      "loss": 0.6693,
      "step": 84240
    },
    {
      "epoch": 4955.882352941177,
      "grad_norm": 20.362266540527344,
      "learning_rate": 4.411764705882353e-07,
      "loss": 0.5904,
      "step": 84250
    },
    {
      "epoch": 4956.470588235294,
      "grad_norm": 18.873397827148438,
      "learning_rate": 4.352941176470588e-07,
      "loss": 0.6309,
      "step": 84260
    },
    {
      "epoch": 4957.058823529412,
      "grad_norm": 31.693889617919922,
      "learning_rate": 4.2941176470588234e-07,
      "loss": 0.664,
      "step": 84270
    },
    {
      "epoch": 4957.64705882353,
      "grad_norm": 27.106327056884766,
      "learning_rate": 4.2352941176470586e-07,
      "loss": 0.5753,
      "step": 84280
    },
    {
      "epoch": 4958.235294117647,
      "grad_norm": 20.79647445678711,
      "learning_rate": 4.176470588235295e-07,
      "loss": 0.5635,
      "step": 84290
    },
    {
      "epoch": 4958.823529411765,
      "grad_norm": 15.875322341918945,
      "learning_rate": 4.11764705882353e-07,
      "loss": 0.6843,
      "step": 84300
    },
    {
      "epoch": 4959.411764705882,
      "grad_norm": 22.784555435180664,
      "learning_rate": 4.058823529411765e-07,
      "loss": 0.7339,
      "step": 84310
    },
    {
      "epoch": 4960.0,
      "grad_norm": 18.83144760131836,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.5445,
      "step": 84320
    },
    {
      "epoch": 4960.588235294118,
      "grad_norm": 17.36690902709961,
      "learning_rate": 3.9411764705882355e-07,
      "loss": 0.5806,
      "step": 84330
    },
    {
      "epoch": 4961.176470588235,
      "grad_norm": 20.15949058532715,
      "learning_rate": 3.8823529411764707e-07,
      "loss": 0.6499,
      "step": 84340
    },
    {
      "epoch": 4961.764705882353,
      "grad_norm": 21.864757537841797,
      "learning_rate": 3.8235294117647064e-07,
      "loss": 0.657,
      "step": 84350
    },
    {
      "epoch": 4962.35294117647,
      "grad_norm": 21.620882034301758,
      "learning_rate": 3.7647058823529416e-07,
      "loss": 0.6089,
      "step": 84360
    },
    {
      "epoch": 4962.941176470588,
      "grad_norm": 24.2153263092041,
      "learning_rate": 3.705882352941177e-07,
      "loss": 0.7324,
      "step": 84370
    },
    {
      "epoch": 4963.529411764706,
      "grad_norm": 22.614524841308594,
      "learning_rate": 3.647058823529412e-07,
      "loss": 0.7473,
      "step": 84380
    },
    {
      "epoch": 4964.117647058823,
      "grad_norm": 22.03362274169922,
      "learning_rate": 3.588235294117647e-07,
      "loss": 0.6411,
      "step": 84390
    },
    {
      "epoch": 4964.705882352941,
      "grad_norm": 25.421579360961914,
      "learning_rate": 3.529411764705883e-07,
      "loss": 0.6477,
      "step": 84400
    },
    {
      "epoch": 4965.294117647059,
      "grad_norm": 17.052162170410156,
      "learning_rate": 3.470588235294118e-07,
      "loss": 0.6781,
      "step": 84410
    },
    {
      "epoch": 4965.882352941177,
      "grad_norm": 20.874731063842773,
      "learning_rate": 3.411764705882353e-07,
      "loss": 0.5906,
      "step": 84420
    },
    {
      "epoch": 4966.470588235294,
      "grad_norm": 16.6162166595459,
      "learning_rate": 3.3529411764705883e-07,
      "loss": 0.641,
      "step": 84430
    },
    {
      "epoch": 4967.058823529412,
      "grad_norm": 20.58562469482422,
      "learning_rate": 3.294117647058824e-07,
      "loss": 0.7321,
      "step": 84440
    },
    {
      "epoch": 4967.64705882353,
      "grad_norm": 17.27956771850586,
      "learning_rate": 3.235294117647059e-07,
      "loss": 0.6391,
      "step": 84450
    },
    {
      "epoch": 4968.235294117647,
      "grad_norm": 22.453609466552734,
      "learning_rate": 3.1764705882352943e-07,
      "loss": 0.6131,
      "step": 84460
    },
    {
      "epoch": 4968.823529411765,
      "grad_norm": 20.794492721557617,
      "learning_rate": 3.1176470588235295e-07,
      "loss": 0.6108,
      "step": 84470
    },
    {
      "epoch": 4969.411764705882,
      "grad_norm": 26.841697692871094,
      "learning_rate": 3.058823529411765e-07,
      "loss": 0.6756,
      "step": 84480
    },
    {
      "epoch": 4970.0,
      "grad_norm": 27.098817825317383,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.7057,
      "step": 84490
    },
    {
      "epoch": 4970.588235294118,
      "grad_norm": 18.191423416137695,
      "learning_rate": 2.9411764705882356e-07,
      "loss": 0.6447,
      "step": 84500
    },
    {
      "epoch": 4971.176470588235,
      "grad_norm": 19.060665130615234,
      "learning_rate": 2.882352941176471e-07,
      "loss": 0.7803,
      "step": 84510
    },
    {
      "epoch": 4971.764705882353,
      "grad_norm": 19.14217185974121,
      "learning_rate": 2.8235294117647064e-07,
      "loss": 0.5707,
      "step": 84520
    },
    {
      "epoch": 4972.35294117647,
      "grad_norm": 17.935420989990234,
      "learning_rate": 2.7647058823529416e-07,
      "loss": 0.7333,
      "step": 84530
    },
    {
      "epoch": 4972.941176470588,
      "grad_norm": 24.1849365234375,
      "learning_rate": 2.705882352941177e-07,
      "loss": 0.7081,
      "step": 84540
    },
    {
      "epoch": 4973.529411764706,
      "grad_norm": 13.522832870483398,
      "learning_rate": 2.647058823529412e-07,
      "loss": 0.6218,
      "step": 84550
    },
    {
      "epoch": 4974.117647058823,
      "grad_norm": 30.55705451965332,
      "learning_rate": 2.588235294117647e-07,
      "loss": 0.627,
      "step": 84560
    },
    {
      "epoch": 4974.705882352941,
      "grad_norm": 24.90593719482422,
      "learning_rate": 2.529411764705883e-07,
      "loss": 0.5946,
      "step": 84570
    },
    {
      "epoch": 4975.294117647059,
      "grad_norm": 16.668048858642578,
      "learning_rate": 2.470588235294118e-07,
      "loss": 0.5965,
      "step": 84580
    },
    {
      "epoch": 4975.882352941177,
      "grad_norm": 21.09597396850586,
      "learning_rate": 2.411764705882353e-07,
      "loss": 0.707,
      "step": 84590
    },
    {
      "epoch": 4976.470588235294,
      "grad_norm": 15.911262512207031,
      "learning_rate": 2.352941176470588e-07,
      "loss": 0.691,
      "step": 84600
    },
    {
      "epoch": 4977.058823529412,
      "grad_norm": 17.01024055480957,
      "learning_rate": 2.2941176470588238e-07,
      "loss": 0.5986,
      "step": 84610
    },
    {
      "epoch": 4977.64705882353,
      "grad_norm": 16.59499740600586,
      "learning_rate": 2.235294117647059e-07,
      "loss": 0.6239,
      "step": 84620
    },
    {
      "epoch": 4978.235294117647,
      "grad_norm": 15.934880256652832,
      "learning_rate": 2.176470588235294e-07,
      "loss": 0.6253,
      "step": 84630
    },
    {
      "epoch": 4978.823529411765,
      "grad_norm": 20.708772659301758,
      "learning_rate": 2.1176470588235293e-07,
      "loss": 0.6099,
      "step": 84640
    },
    {
      "epoch": 4979.411764705882,
      "grad_norm": 14.763330459594727,
      "learning_rate": 2.058823529411765e-07,
      "loss": 0.728,
      "step": 84650
    },
    {
      "epoch": 4980.0,
      "grad_norm": 21.58174705505371,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.5823,
      "step": 84660
    },
    {
      "epoch": 4980.588235294118,
      "grad_norm": 24.326210021972656,
      "learning_rate": 1.9411764705882353e-07,
      "loss": 0.6557,
      "step": 84670
    },
    {
      "epoch": 4981.176470588235,
      "grad_norm": 21.03679656982422,
      "learning_rate": 1.8823529411764708e-07,
      "loss": 0.5813,
      "step": 84680
    },
    {
      "epoch": 4981.764705882353,
      "grad_norm": 16.693744659423828,
      "learning_rate": 1.823529411764706e-07,
      "loss": 0.6625,
      "step": 84690
    },
    {
      "epoch": 4982.35294117647,
      "grad_norm": 21.14436149597168,
      "learning_rate": 1.7647058823529414e-07,
      "loss": 0.7132,
      "step": 84700
    },
    {
      "epoch": 4982.941176470588,
      "grad_norm": 14.772256851196289,
      "learning_rate": 1.7058823529411766e-07,
      "loss": 0.5192,
      "step": 84710
    },
    {
      "epoch": 4983.529411764706,
      "grad_norm": 16.79252052307129,
      "learning_rate": 1.647058823529412e-07,
      "loss": 0.5957,
      "step": 84720
    },
    {
      "epoch": 4984.117647058823,
      "grad_norm": 20.347490310668945,
      "learning_rate": 1.5882352941176472e-07,
      "loss": 0.707,
      "step": 84730
    },
    {
      "epoch": 4984.705882352941,
      "grad_norm": 18.63768768310547,
      "learning_rate": 1.5294117647058826e-07,
      "loss": 0.7069,
      "step": 84740
    },
    {
      "epoch": 4985.294117647059,
      "grad_norm": 17.573184967041016,
      "learning_rate": 1.4705882352941178e-07,
      "loss": 0.6717,
      "step": 84750
    },
    {
      "epoch": 4985.882352941177,
      "grad_norm": 24.708650588989258,
      "learning_rate": 1.4117647058823532e-07,
      "loss": 0.7091,
      "step": 84760
    },
    {
      "epoch": 4986.470588235294,
      "grad_norm": 16.729110717773438,
      "learning_rate": 1.3529411764705884e-07,
      "loss": 0.674,
      "step": 84770
    },
    {
      "epoch": 4987.058823529412,
      "grad_norm": 14.71167278289795,
      "learning_rate": 1.2941176470588236e-07,
      "loss": 0.5664,
      "step": 84780
    },
    {
      "epoch": 4987.64705882353,
      "grad_norm": 18.44908905029297,
      "learning_rate": 1.235294117647059e-07,
      "loss": 0.7312,
      "step": 84790
    },
    {
      "epoch": 4988.235294117647,
      "grad_norm": 17.18975257873535,
      "learning_rate": 1.176470588235294e-07,
      "loss": 0.5859,
      "step": 84800
    },
    {
      "epoch": 4988.823529411765,
      "grad_norm": 21.11067771911621,
      "learning_rate": 1.1176470588235295e-07,
      "loss": 0.6454,
      "step": 84810
    },
    {
      "epoch": 4989.411764705882,
      "grad_norm": 22.785263061523438,
      "learning_rate": 1.0588235294117647e-07,
      "loss": 0.6224,
      "step": 84820
    },
    {
      "epoch": 4990.0,
      "grad_norm": 17.088294982910156,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 0.733,
      "step": 84830
    },
    {
      "epoch": 4990.588235294118,
      "grad_norm": 17.002796173095703,
      "learning_rate": 9.411764705882354e-08,
      "loss": 0.5927,
      "step": 84840
    },
    {
      "epoch": 4991.176470588235,
      "grad_norm": 18.79875373840332,
      "learning_rate": 8.823529411764707e-08,
      "loss": 0.6323,
      "step": 84850
    },
    {
      "epoch": 4991.764705882353,
      "grad_norm": 26.713764190673828,
      "learning_rate": 8.23529411764706e-08,
      "loss": 0.6945,
      "step": 84860
    },
    {
      "epoch": 4992.35294117647,
      "grad_norm": 19.265338897705078,
      "learning_rate": 7.647058823529413e-08,
      "loss": 0.6602,
      "step": 84870
    },
    {
      "epoch": 4992.941176470588,
      "grad_norm": 16.98318099975586,
      "learning_rate": 7.058823529411766e-08,
      "loss": 0.6283,
      "step": 84880
    },
    {
      "epoch": 4993.529411764706,
      "grad_norm": 17.439708709716797,
      "learning_rate": 6.470588235294118e-08,
      "loss": 0.5854,
      "step": 84890
    },
    {
      "epoch": 4994.117647058823,
      "grad_norm": 22.399534225463867,
      "learning_rate": 5.88235294117647e-08,
      "loss": 0.7092,
      "step": 84900
    },
    {
      "epoch": 4994.705882352941,
      "grad_norm": 23.869916915893555,
      "learning_rate": 5.294117647058823e-08,
      "loss": 0.6379,
      "step": 84910
    },
    {
      "epoch": 4995.294117647059,
      "grad_norm": 20.782344818115234,
      "learning_rate": 4.705882352941177e-08,
      "loss": 0.7468,
      "step": 84920
    },
    {
      "epoch": 4995.882352941177,
      "grad_norm": 16.745073318481445,
      "learning_rate": 4.11764705882353e-08,
      "loss": 0.6139,
      "step": 84930
    },
    {
      "epoch": 4996.470588235294,
      "grad_norm": 14.641977310180664,
      "learning_rate": 3.529411764705883e-08,
      "loss": 0.6826,
      "step": 84940
    },
    {
      "epoch": 4997.058823529412,
      "grad_norm": 22.63477897644043,
      "learning_rate": 2.941176470588235e-08,
      "loss": 0.5897,
      "step": 84950
    },
    {
      "epoch": 4997.64705882353,
      "grad_norm": 23.886280059814453,
      "learning_rate": 2.3529411764705885e-08,
      "loss": 0.71,
      "step": 84960
    },
    {
      "epoch": 4998.235294117647,
      "grad_norm": 27.540220260620117,
      "learning_rate": 1.7647058823529415e-08,
      "loss": 0.7004,
      "step": 84970
    },
    {
      "epoch": 4998.823529411765,
      "grad_norm": 28.523273468017578,
      "learning_rate": 1.1764705882352942e-08,
      "loss": 0.6304,
      "step": 84980
    }
  ],
  "logging_steps": 10,
  "max_steps": 85000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5000,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 59561783820288.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
