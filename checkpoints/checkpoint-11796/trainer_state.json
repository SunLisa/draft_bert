{
  "best_metric": 3.6637704372406006,
  "best_model_checkpoint": "./checkpoints\\checkpoint-11796",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 11796,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00508646998982706,
      "grad_norm": 1.9585089683532715,
      "learning_rate": 4.999491353001017e-05,
      "loss": 5.5444,
      "step": 10
    },
    {
      "epoch": 0.01017293997965412,
      "grad_norm": 1.7009533643722534,
      "learning_rate": 4.998982706002035e-05,
      "loss": 5.5018,
      "step": 20
    },
    {
      "epoch": 0.015259409969481181,
      "grad_norm": 1.4952682256698608,
      "learning_rate": 4.998474059003052e-05,
      "loss": 5.464,
      "step": 30
    },
    {
      "epoch": 0.02034587995930824,
      "grad_norm": 1.4348140954971313,
      "learning_rate": 4.997965412004069e-05,
      "loss": 5.4295,
      "step": 40
    },
    {
      "epoch": 0.0254323499491353,
      "grad_norm": 1.1581816673278809,
      "learning_rate": 4.9974567650050865e-05,
      "loss": 5.4153,
      "step": 50
    },
    {
      "epoch": 0.030518819938962362,
      "grad_norm": 1.569205403327942,
      "learning_rate": 4.996948118006104e-05,
      "loss": 5.3885,
      "step": 60
    },
    {
      "epoch": 0.03560528992878942,
      "grad_norm": 1.3471859693527222,
      "learning_rate": 4.996439471007121e-05,
      "loss": 5.3574,
      "step": 70
    },
    {
      "epoch": 0.04069175991861648,
      "grad_norm": 1.1945815086364746,
      "learning_rate": 4.995930824008139e-05,
      "loss": 5.3385,
      "step": 80
    },
    {
      "epoch": 0.04577822990844354,
      "grad_norm": 1.233575701713562,
      "learning_rate": 4.995422177009156e-05,
      "loss": 5.32,
      "step": 90
    },
    {
      "epoch": 0.0508646998982706,
      "grad_norm": 1.2731891870498657,
      "learning_rate": 4.994913530010173e-05,
      "loss": 5.2897,
      "step": 100
    },
    {
      "epoch": 0.05595116988809766,
      "grad_norm": 1.095229148864746,
      "learning_rate": 4.9944048830111905e-05,
      "loss": 5.2792,
      "step": 110
    },
    {
      "epoch": 0.061037639877924724,
      "grad_norm": 1.194229006767273,
      "learning_rate": 4.9938962360122075e-05,
      "loss": 5.2332,
      "step": 120
    },
    {
      "epoch": 0.06612410986775177,
      "grad_norm": 1.4456599950790405,
      "learning_rate": 4.9933875890132245e-05,
      "loss": 5.2304,
      "step": 130
    },
    {
      "epoch": 0.07121057985757884,
      "grad_norm": 1.413408637046814,
      "learning_rate": 4.992878942014242e-05,
      "loss": 5.2069,
      "step": 140
    },
    {
      "epoch": 0.0762970498474059,
      "grad_norm": 1.565890908241272,
      "learning_rate": 4.99237029501526e-05,
      "loss": 5.19,
      "step": 150
    },
    {
      "epoch": 0.08138351983723296,
      "grad_norm": 1.3241851329803467,
      "learning_rate": 4.9918616480162775e-05,
      "loss": 5.1631,
      "step": 160
    },
    {
      "epoch": 0.08646998982706001,
      "grad_norm": 1.2150278091430664,
      "learning_rate": 4.9913530010172945e-05,
      "loss": 5.1278,
      "step": 170
    },
    {
      "epoch": 0.09155645981688708,
      "grad_norm": 1.2402805089950562,
      "learning_rate": 4.9908443540183115e-05,
      "loss": 5.1118,
      "step": 180
    },
    {
      "epoch": 0.09664292980671414,
      "grad_norm": 1.0788289308547974,
      "learning_rate": 4.990335707019329e-05,
      "loss": 5.1017,
      "step": 190
    },
    {
      "epoch": 0.1017293997965412,
      "grad_norm": 1.2812668085098267,
      "learning_rate": 4.989827060020346e-05,
      "loss": 5.0792,
      "step": 200
    },
    {
      "epoch": 0.10681586978636826,
      "grad_norm": 1.1210490465164185,
      "learning_rate": 4.989318413021363e-05,
      "loss": 5.0611,
      "step": 210
    },
    {
      "epoch": 0.11190233977619532,
      "grad_norm": 1.0064949989318848,
      "learning_rate": 4.988809766022381e-05,
      "loss": 5.0538,
      "step": 220
    },
    {
      "epoch": 0.11698880976602238,
      "grad_norm": 1.5250755548477173,
      "learning_rate": 4.988301119023398e-05,
      "loss": 4.9935,
      "step": 230
    },
    {
      "epoch": 0.12207527975584945,
      "grad_norm": 1.2483479976654053,
      "learning_rate": 4.9877924720244154e-05,
      "loss": 5.0059,
      "step": 240
    },
    {
      "epoch": 0.1271617497456765,
      "grad_norm": 0.9861146807670593,
      "learning_rate": 4.987283825025433e-05,
      "loss": 4.986,
      "step": 250
    },
    {
      "epoch": 0.13224821973550355,
      "grad_norm": 1.2158012390136719,
      "learning_rate": 4.98677517802645e-05,
      "loss": 4.9749,
      "step": 260
    },
    {
      "epoch": 0.1373346897253306,
      "grad_norm": 1.07657790184021,
      "learning_rate": 4.986266531027467e-05,
      "loss": 4.9685,
      "step": 270
    },
    {
      "epoch": 0.14242115971515767,
      "grad_norm": 1.1107748746871948,
      "learning_rate": 4.985757884028485e-05,
      "loss": 4.9142,
      "step": 280
    },
    {
      "epoch": 0.14750762970498474,
      "grad_norm": 1.2115124464035034,
      "learning_rate": 4.985249237029502e-05,
      "loss": 4.9266,
      "step": 290
    },
    {
      "epoch": 0.1525940996948118,
      "grad_norm": 1.0620908737182617,
      "learning_rate": 4.984740590030519e-05,
      "loss": 4.8965,
      "step": 300
    },
    {
      "epoch": 0.15768056968463887,
      "grad_norm": 1.3235645294189453,
      "learning_rate": 4.9842319430315364e-05,
      "loss": 4.8439,
      "step": 310
    },
    {
      "epoch": 0.16276703967446593,
      "grad_norm": 1.256437063217163,
      "learning_rate": 4.9837232960325534e-05,
      "loss": 4.8681,
      "step": 320
    },
    {
      "epoch": 0.167853509664293,
      "grad_norm": 1.0278855562210083,
      "learning_rate": 4.983214649033571e-05,
      "loss": 4.8625,
      "step": 330
    },
    {
      "epoch": 0.17293997965412003,
      "grad_norm": 1.2098768949508667,
      "learning_rate": 4.982706002034588e-05,
      "loss": 4.8021,
      "step": 340
    },
    {
      "epoch": 0.1780264496439471,
      "grad_norm": 1.29997718334198,
      "learning_rate": 4.982197355035606e-05,
      "loss": 4.7978,
      "step": 350
    },
    {
      "epoch": 0.18311291963377416,
      "grad_norm": 1.237276315689087,
      "learning_rate": 4.981688708036623e-05,
      "loss": 4.7685,
      "step": 360
    },
    {
      "epoch": 0.18819938962360122,
      "grad_norm": 1.0698118209838867,
      "learning_rate": 4.9811800610376404e-05,
      "loss": 4.761,
      "step": 370
    },
    {
      "epoch": 0.19328585961342828,
      "grad_norm": 0.9539771676063538,
      "learning_rate": 4.9806714140386574e-05,
      "loss": 4.7812,
      "step": 380
    },
    {
      "epoch": 0.19837232960325535,
      "grad_norm": 0.9739999771118164,
      "learning_rate": 4.9801627670396743e-05,
      "loss": 4.7757,
      "step": 390
    },
    {
      "epoch": 0.2034587995930824,
      "grad_norm": 1.190354347229004,
      "learning_rate": 4.979654120040692e-05,
      "loss": 4.7024,
      "step": 400
    },
    {
      "epoch": 0.20854526958290945,
      "grad_norm": 1.0434629917144775,
      "learning_rate": 4.979145473041709e-05,
      "loss": 4.7009,
      "step": 410
    },
    {
      "epoch": 0.2136317395727365,
      "grad_norm": 1.2235456705093384,
      "learning_rate": 4.978636826042727e-05,
      "loss": 4.7084,
      "step": 420
    },
    {
      "epoch": 0.21871820956256358,
      "grad_norm": 1.4132357835769653,
      "learning_rate": 4.9781281790437437e-05,
      "loss": 4.6598,
      "step": 430
    },
    {
      "epoch": 0.22380467955239064,
      "grad_norm": 0.9326409697532654,
      "learning_rate": 4.977619532044761e-05,
      "loss": 4.6951,
      "step": 440
    },
    {
      "epoch": 0.2288911495422177,
      "grad_norm": 1.050897240638733,
      "learning_rate": 4.977110885045779e-05,
      "loss": 4.6434,
      "step": 450
    },
    {
      "epoch": 0.23397761953204477,
      "grad_norm": 1.0765312910079956,
      "learning_rate": 4.976602238046796e-05,
      "loss": 4.5976,
      "step": 460
    },
    {
      "epoch": 0.23906408952187183,
      "grad_norm": 1.2914726734161377,
      "learning_rate": 4.976093591047813e-05,
      "loss": 4.6072,
      "step": 470
    },
    {
      "epoch": 0.2441505595116989,
      "grad_norm": 1.1661275625228882,
      "learning_rate": 4.9755849440488306e-05,
      "loss": 4.6281,
      "step": 480
    },
    {
      "epoch": 0.24923702950152593,
      "grad_norm": 1.1334600448608398,
      "learning_rate": 4.9750762970498476e-05,
      "loss": 4.594,
      "step": 490
    },
    {
      "epoch": 0.254323499491353,
      "grad_norm": 0.956544816493988,
      "learning_rate": 4.9745676500508646e-05,
      "loss": 4.5851,
      "step": 500
    },
    {
      "epoch": 0.25940996948118006,
      "grad_norm": 1.0469050407409668,
      "learning_rate": 4.974059003051882e-05,
      "loss": 4.5527,
      "step": 510
    },
    {
      "epoch": 0.2644964394710071,
      "grad_norm": 1.0449284315109253,
      "learning_rate": 4.973550356052899e-05,
      "loss": 4.5929,
      "step": 520
    },
    {
      "epoch": 0.2695829094608342,
      "grad_norm": 1.2400704622268677,
      "learning_rate": 4.973041709053917e-05,
      "loss": 4.5203,
      "step": 530
    },
    {
      "epoch": 0.2746693794506612,
      "grad_norm": 0.9109295606613159,
      "learning_rate": 4.9725330620549346e-05,
      "loss": 4.5771,
      "step": 540
    },
    {
      "epoch": 0.2797558494404883,
      "grad_norm": 1.120289921760559,
      "learning_rate": 4.9720244150559516e-05,
      "loss": 4.4708,
      "step": 550
    },
    {
      "epoch": 0.28484231943031535,
      "grad_norm": 0.9644604325294495,
      "learning_rate": 4.9715157680569686e-05,
      "loss": 4.539,
      "step": 560
    },
    {
      "epoch": 0.28992878942014244,
      "grad_norm": 1.2098420858383179,
      "learning_rate": 4.971007121057986e-05,
      "loss": 4.5541,
      "step": 570
    },
    {
      "epoch": 0.2950152594099695,
      "grad_norm": 0.9133832454681396,
      "learning_rate": 4.970498474059003e-05,
      "loss": 4.5027,
      "step": 580
    },
    {
      "epoch": 0.30010172939979657,
      "grad_norm": 0.8536376953125,
      "learning_rate": 4.96998982706002e-05,
      "loss": 4.4776,
      "step": 590
    },
    {
      "epoch": 0.3051881993896236,
      "grad_norm": 0.881970226764679,
      "learning_rate": 4.969481180061038e-05,
      "loss": 4.4669,
      "step": 600
    },
    {
      "epoch": 0.31027466937945064,
      "grad_norm": 1.07752525806427,
      "learning_rate": 4.968972533062055e-05,
      "loss": 4.4765,
      "step": 610
    },
    {
      "epoch": 0.31536113936927773,
      "grad_norm": 1.1978000402450562,
      "learning_rate": 4.9684638860630726e-05,
      "loss": 4.5212,
      "step": 620
    },
    {
      "epoch": 0.32044760935910477,
      "grad_norm": 1.0351076126098633,
      "learning_rate": 4.96795523906409e-05,
      "loss": 4.4634,
      "step": 630
    },
    {
      "epoch": 0.32553407934893186,
      "grad_norm": 0.9918464422225952,
      "learning_rate": 4.967446592065107e-05,
      "loss": 4.3966,
      "step": 640
    },
    {
      "epoch": 0.3306205493387589,
      "grad_norm": 1.0583484172821045,
      "learning_rate": 4.966937945066124e-05,
      "loss": 4.5152,
      "step": 650
    },
    {
      "epoch": 0.335707019328586,
      "grad_norm": 1.057262659072876,
      "learning_rate": 4.966429298067142e-05,
      "loss": 4.4448,
      "step": 660
    },
    {
      "epoch": 0.340793489318413,
      "grad_norm": 1.005101203918457,
      "learning_rate": 4.965920651068159e-05,
      "loss": 4.4238,
      "step": 670
    },
    {
      "epoch": 0.34587995930824006,
      "grad_norm": 0.9661780595779419,
      "learning_rate": 4.9654120040691765e-05,
      "loss": 4.3938,
      "step": 680
    },
    {
      "epoch": 0.35096642929806715,
      "grad_norm": 0.987967848777771,
      "learning_rate": 4.9649033570701935e-05,
      "loss": 4.3688,
      "step": 690
    },
    {
      "epoch": 0.3560528992878942,
      "grad_norm": 1.2252821922302246,
      "learning_rate": 4.9643947100712105e-05,
      "loss": 4.3957,
      "step": 700
    },
    {
      "epoch": 0.3611393692777213,
      "grad_norm": 1.2010424137115479,
      "learning_rate": 4.963886063072228e-05,
      "loss": 4.4713,
      "step": 710
    },
    {
      "epoch": 0.3662258392675483,
      "grad_norm": 1.1826695203781128,
      "learning_rate": 4.963377416073245e-05,
      "loss": 4.3908,
      "step": 720
    },
    {
      "epoch": 0.3713123092573754,
      "grad_norm": 1.2025344371795654,
      "learning_rate": 4.962868769074263e-05,
      "loss": 4.4077,
      "step": 730
    },
    {
      "epoch": 0.37639877924720244,
      "grad_norm": 1.0716888904571533,
      "learning_rate": 4.9623601220752805e-05,
      "loss": 4.3235,
      "step": 740
    },
    {
      "epoch": 0.3814852492370295,
      "grad_norm": 0.9116369485855103,
      "learning_rate": 4.9618514750762975e-05,
      "loss": 4.3541,
      "step": 750
    },
    {
      "epoch": 0.38657171922685657,
      "grad_norm": 1.0556226968765259,
      "learning_rate": 4.9613428280773145e-05,
      "loss": 4.3337,
      "step": 760
    },
    {
      "epoch": 0.3916581892166836,
      "grad_norm": 0.9915569424629211,
      "learning_rate": 4.960834181078332e-05,
      "loss": 4.4359,
      "step": 770
    },
    {
      "epoch": 0.3967446592065107,
      "grad_norm": 1.0425828695297241,
      "learning_rate": 4.960325534079349e-05,
      "loss": 4.3788,
      "step": 780
    },
    {
      "epoch": 0.40183112919633773,
      "grad_norm": 0.860231339931488,
      "learning_rate": 4.959816887080366e-05,
      "loss": 4.4026,
      "step": 790
    },
    {
      "epoch": 0.4069175991861648,
      "grad_norm": 0.9238550066947937,
      "learning_rate": 4.959308240081384e-05,
      "loss": 4.3583,
      "step": 800
    },
    {
      "epoch": 0.41200406917599186,
      "grad_norm": 1.0526010990142822,
      "learning_rate": 4.958799593082401e-05,
      "loss": 4.3531,
      "step": 810
    },
    {
      "epoch": 0.4170905391658189,
      "grad_norm": 1.1386793851852417,
      "learning_rate": 4.9582909460834184e-05,
      "loss": 4.322,
      "step": 820
    },
    {
      "epoch": 0.422177009155646,
      "grad_norm": 1.1627914905548096,
      "learning_rate": 4.957782299084436e-05,
      "loss": 4.2742,
      "step": 830
    },
    {
      "epoch": 0.427263479145473,
      "grad_norm": 1.2516367435455322,
      "learning_rate": 4.957273652085453e-05,
      "loss": 4.2997,
      "step": 840
    },
    {
      "epoch": 0.4323499491353001,
      "grad_norm": 1.1359845399856567,
      "learning_rate": 4.95676500508647e-05,
      "loss": 4.3493,
      "step": 850
    },
    {
      "epoch": 0.43743641912512715,
      "grad_norm": 1.4033501148223877,
      "learning_rate": 4.956256358087488e-05,
      "loss": 4.3401,
      "step": 860
    },
    {
      "epoch": 0.44252288911495424,
      "grad_norm": 1.059059500694275,
      "learning_rate": 4.955747711088505e-05,
      "loss": 4.2688,
      "step": 870
    },
    {
      "epoch": 0.4476093591047813,
      "grad_norm": 1.1205987930297852,
      "learning_rate": 4.955239064089522e-05,
      "loss": 4.3024,
      "step": 880
    },
    {
      "epoch": 0.4526958290946083,
      "grad_norm": 1.1574594974517822,
      "learning_rate": 4.9547304170905394e-05,
      "loss": 4.3315,
      "step": 890
    },
    {
      "epoch": 0.4577822990844354,
      "grad_norm": 0.9600428938865662,
      "learning_rate": 4.9542217700915564e-05,
      "loss": 4.2731,
      "step": 900
    },
    {
      "epoch": 0.46286876907426244,
      "grad_norm": 1.2586506605148315,
      "learning_rate": 4.953713123092574e-05,
      "loss": 4.2756,
      "step": 910
    },
    {
      "epoch": 0.46795523906408953,
      "grad_norm": 1.302880883216858,
      "learning_rate": 4.953204476093592e-05,
      "loss": 4.2366,
      "step": 920
    },
    {
      "epoch": 0.47304170905391657,
      "grad_norm": 1.7742618322372437,
      "learning_rate": 4.952695829094609e-05,
      "loss": 4.2277,
      "step": 930
    },
    {
      "epoch": 0.47812817904374366,
      "grad_norm": 1.244584083557129,
      "learning_rate": 4.952187182095626e-05,
      "loss": 4.2625,
      "step": 940
    },
    {
      "epoch": 0.4832146490335707,
      "grad_norm": 1.0780242681503296,
      "learning_rate": 4.9516785350966434e-05,
      "loss": 4.2794,
      "step": 950
    },
    {
      "epoch": 0.4883011190233978,
      "grad_norm": 1.3982036113739014,
      "learning_rate": 4.9511698880976604e-05,
      "loss": 4.2644,
      "step": 960
    },
    {
      "epoch": 0.4933875890132248,
      "grad_norm": 1.6520549058914185,
      "learning_rate": 4.950661241098678e-05,
      "loss": 4.2507,
      "step": 970
    },
    {
      "epoch": 0.49847405900305186,
      "grad_norm": 0.9269866347312927,
      "learning_rate": 4.950152594099695e-05,
      "loss": 4.2482,
      "step": 980
    },
    {
      "epoch": 0.503560528992879,
      "grad_norm": 1.022214412689209,
      "learning_rate": 4.949643947100712e-05,
      "loss": 4.2032,
      "step": 990
    },
    {
      "epoch": 0.508646998982706,
      "grad_norm": 1.2174458503723145,
      "learning_rate": 4.94913530010173e-05,
      "loss": 4.2069,
      "step": 1000
    },
    {
      "epoch": 0.513733468972533,
      "grad_norm": 1.3003227710723877,
      "learning_rate": 4.9486266531027467e-05,
      "loss": 4.2186,
      "step": 1010
    },
    {
      "epoch": 0.5188199389623601,
      "grad_norm": 1.5599457025527954,
      "learning_rate": 4.948118006103764e-05,
      "loss": 4.2508,
      "step": 1020
    },
    {
      "epoch": 0.5239064089521872,
      "grad_norm": 1.1026594638824463,
      "learning_rate": 4.947609359104782e-05,
      "loss": 4.2113,
      "step": 1030
    },
    {
      "epoch": 0.5289928789420142,
      "grad_norm": 1.0611540079116821,
      "learning_rate": 4.947100712105799e-05,
      "loss": 4.2,
      "step": 1040
    },
    {
      "epoch": 0.5340793489318413,
      "grad_norm": 1.2830498218536377,
      "learning_rate": 4.946592065106816e-05,
      "loss": 4.1854,
      "step": 1050
    },
    {
      "epoch": 0.5391658189216684,
      "grad_norm": 1.0995076894760132,
      "learning_rate": 4.9460834181078336e-05,
      "loss": 4.2178,
      "step": 1060
    },
    {
      "epoch": 0.5442522889114955,
      "grad_norm": 1.1036007404327393,
      "learning_rate": 4.9455747711088506e-05,
      "loss": 4.1803,
      "step": 1070
    },
    {
      "epoch": 0.5493387589013224,
      "grad_norm": 1.3285057544708252,
      "learning_rate": 4.9450661241098676e-05,
      "loss": 4.211,
      "step": 1080
    },
    {
      "epoch": 0.5544252288911495,
      "grad_norm": 1.3299161195755005,
      "learning_rate": 4.944557477110885e-05,
      "loss": 4.188,
      "step": 1090
    },
    {
      "epoch": 0.5595116988809766,
      "grad_norm": 1.1388696432113647,
      "learning_rate": 4.944048830111902e-05,
      "loss": 4.2074,
      "step": 1100
    },
    {
      "epoch": 0.5645981688708036,
      "grad_norm": 1.0197968482971191,
      "learning_rate": 4.94354018311292e-05,
      "loss": 4.1387,
      "step": 1110
    },
    {
      "epoch": 0.5696846388606307,
      "grad_norm": 1.29355788230896,
      "learning_rate": 4.9430315361139376e-05,
      "loss": 4.1868,
      "step": 1120
    },
    {
      "epoch": 0.5747711088504578,
      "grad_norm": 1.3100465536117554,
      "learning_rate": 4.9425228891149546e-05,
      "loss": 4.1097,
      "step": 1130
    },
    {
      "epoch": 0.5798575788402849,
      "grad_norm": 1.1799715757369995,
      "learning_rate": 4.9420142421159716e-05,
      "loss": 4.1706,
      "step": 1140
    },
    {
      "epoch": 0.5849440488301119,
      "grad_norm": 1.2754360437393188,
      "learning_rate": 4.941505595116989e-05,
      "loss": 4.2091,
      "step": 1150
    },
    {
      "epoch": 0.590030518819939,
      "grad_norm": 1.024857759475708,
      "learning_rate": 4.940996948118006e-05,
      "loss": 4.1714,
      "step": 1160
    },
    {
      "epoch": 0.595116988809766,
      "grad_norm": 1.5393249988555908,
      "learning_rate": 4.940488301119023e-05,
      "loss": 4.1591,
      "step": 1170
    },
    {
      "epoch": 0.6002034587995931,
      "grad_norm": 1.2726995944976807,
      "learning_rate": 4.939979654120041e-05,
      "loss": 4.1788,
      "step": 1180
    },
    {
      "epoch": 0.6052899287894201,
      "grad_norm": 1.228076696395874,
      "learning_rate": 4.939471007121058e-05,
      "loss": 4.1604,
      "step": 1190
    },
    {
      "epoch": 0.6103763987792472,
      "grad_norm": 1.3904155492782593,
      "learning_rate": 4.9389623601220756e-05,
      "loss": 4.1445,
      "step": 1200
    },
    {
      "epoch": 0.6154628687690743,
      "grad_norm": 1.4824177026748657,
      "learning_rate": 4.938453713123093e-05,
      "loss": 4.2071,
      "step": 1210
    },
    {
      "epoch": 0.6205493387589013,
      "grad_norm": 1.1739479303359985,
      "learning_rate": 4.93794506612411e-05,
      "loss": 4.2445,
      "step": 1220
    },
    {
      "epoch": 0.6256358087487284,
      "grad_norm": 1.1372936964035034,
      "learning_rate": 4.937436419125128e-05,
      "loss": 4.1683,
      "step": 1230
    },
    {
      "epoch": 0.6307222787385555,
      "grad_norm": 1.6379144191741943,
      "learning_rate": 4.936927772126145e-05,
      "loss": 4.2249,
      "step": 1240
    },
    {
      "epoch": 0.6358087487283826,
      "grad_norm": 1.340135097503662,
      "learning_rate": 4.936419125127162e-05,
      "loss": 4.209,
      "step": 1250
    },
    {
      "epoch": 0.6408952187182095,
      "grad_norm": 1.7233679294586182,
      "learning_rate": 4.9359104781281795e-05,
      "loss": 4.1444,
      "step": 1260
    },
    {
      "epoch": 0.6459816887080366,
      "grad_norm": 1.2008136510849,
      "learning_rate": 4.9354018311291965e-05,
      "loss": 4.1167,
      "step": 1270
    },
    {
      "epoch": 0.6510681586978637,
      "grad_norm": 1.4558030366897583,
      "learning_rate": 4.9348931841302135e-05,
      "loss": 4.2075,
      "step": 1280
    },
    {
      "epoch": 0.6561546286876907,
      "grad_norm": 1.1162328720092773,
      "learning_rate": 4.934384537131231e-05,
      "loss": 4.1029,
      "step": 1290
    },
    {
      "epoch": 0.6612410986775178,
      "grad_norm": 1.3151744604110718,
      "learning_rate": 4.933875890132248e-05,
      "loss": 4.2085,
      "step": 1300
    },
    {
      "epoch": 0.6663275686673449,
      "grad_norm": 1.8034764528274536,
      "learning_rate": 4.933367243133266e-05,
      "loss": 4.1405,
      "step": 1310
    },
    {
      "epoch": 0.671414038657172,
      "grad_norm": 1.2844878435134888,
      "learning_rate": 4.9328585961342835e-05,
      "loss": 4.1374,
      "step": 1320
    },
    {
      "epoch": 0.676500508646999,
      "grad_norm": 1.515757441520691,
      "learning_rate": 4.9323499491353005e-05,
      "loss": 4.1828,
      "step": 1330
    },
    {
      "epoch": 0.681586978636826,
      "grad_norm": 1.2853244543075562,
      "learning_rate": 4.9318413021363175e-05,
      "loss": 4.1547,
      "step": 1340
    },
    {
      "epoch": 0.6866734486266531,
      "grad_norm": 1.2468208074569702,
      "learning_rate": 4.931332655137335e-05,
      "loss": 4.1324,
      "step": 1350
    },
    {
      "epoch": 0.6917599186164801,
      "grad_norm": 1.2399073839187622,
      "learning_rate": 4.930824008138352e-05,
      "loss": 4.2425,
      "step": 1360
    },
    {
      "epoch": 0.6968463886063072,
      "grad_norm": 1.524145245552063,
      "learning_rate": 4.930315361139369e-05,
      "loss": 4.1049,
      "step": 1370
    },
    {
      "epoch": 0.7019328585961343,
      "grad_norm": 1.2506672143936157,
      "learning_rate": 4.929806714140387e-05,
      "loss": 4.0974,
      "step": 1380
    },
    {
      "epoch": 0.7070193285859614,
      "grad_norm": 1.8325817584991455,
      "learning_rate": 4.929298067141404e-05,
      "loss": 4.1568,
      "step": 1390
    },
    {
      "epoch": 0.7121057985757884,
      "grad_norm": 1.0411946773529053,
      "learning_rate": 4.9287894201424214e-05,
      "loss": 4.0931,
      "step": 1400
    },
    {
      "epoch": 0.7171922685656155,
      "grad_norm": 2.354320526123047,
      "learning_rate": 4.928280773143439e-05,
      "loss": 4.0813,
      "step": 1410
    },
    {
      "epoch": 0.7222787385554426,
      "grad_norm": 1.5156207084655762,
      "learning_rate": 4.927772126144456e-05,
      "loss": 4.1268,
      "step": 1420
    },
    {
      "epoch": 0.7273652085452695,
      "grad_norm": 1.4479687213897705,
      "learning_rate": 4.927263479145473e-05,
      "loss": 4.1882,
      "step": 1430
    },
    {
      "epoch": 0.7324516785350966,
      "grad_norm": 1.4128406047821045,
      "learning_rate": 4.926754832146491e-05,
      "loss": 4.1001,
      "step": 1440
    },
    {
      "epoch": 0.7375381485249237,
      "grad_norm": 1.3605612516403198,
      "learning_rate": 4.926246185147508e-05,
      "loss": 4.0763,
      "step": 1450
    },
    {
      "epoch": 0.7426246185147508,
      "grad_norm": 1.2540223598480225,
      "learning_rate": 4.925737538148525e-05,
      "loss": 4.1341,
      "step": 1460
    },
    {
      "epoch": 0.7477110885045778,
      "grad_norm": 2.1127843856811523,
      "learning_rate": 4.9252288911495424e-05,
      "loss": 4.0764,
      "step": 1470
    },
    {
      "epoch": 0.7527975584944049,
      "grad_norm": 1.5416548252105713,
      "learning_rate": 4.9247202441505594e-05,
      "loss": 4.0845,
      "step": 1480
    },
    {
      "epoch": 0.757884028484232,
      "grad_norm": 1.9581156969070435,
      "learning_rate": 4.924211597151577e-05,
      "loss": 4.0685,
      "step": 1490
    },
    {
      "epoch": 0.762970498474059,
      "grad_norm": 1.3578999042510986,
      "learning_rate": 4.923702950152595e-05,
      "loss": 4.1751,
      "step": 1500
    },
    {
      "epoch": 0.768056968463886,
      "grad_norm": 1.1729875802993774,
      "learning_rate": 4.923194303153612e-05,
      "loss": 4.1706,
      "step": 1510
    },
    {
      "epoch": 0.7731434384537131,
      "grad_norm": 1.3527897596359253,
      "learning_rate": 4.9226856561546294e-05,
      "loss": 4.0323,
      "step": 1520
    },
    {
      "epoch": 0.7782299084435402,
      "grad_norm": 1.3012056350708008,
      "learning_rate": 4.9221770091556464e-05,
      "loss": 4.1256,
      "step": 1530
    },
    {
      "epoch": 0.7833163784333672,
      "grad_norm": 1.7433809041976929,
      "learning_rate": 4.9216683621566634e-05,
      "loss": 4.1122,
      "step": 1540
    },
    {
      "epoch": 0.7884028484231943,
      "grad_norm": 1.2754298448562622,
      "learning_rate": 4.921159715157681e-05,
      "loss": 4.0388,
      "step": 1550
    },
    {
      "epoch": 0.7934893184130214,
      "grad_norm": 1.454426884651184,
      "learning_rate": 4.920651068158698e-05,
      "loss": 4.0637,
      "step": 1560
    },
    {
      "epoch": 0.7985757884028484,
      "grad_norm": 1.352404236793518,
      "learning_rate": 4.920142421159715e-05,
      "loss": 4.0225,
      "step": 1570
    },
    {
      "epoch": 0.8036622583926755,
      "grad_norm": 1.3086756467819214,
      "learning_rate": 4.919633774160733e-05,
      "loss": 4.0847,
      "step": 1580
    },
    {
      "epoch": 0.8087487283825026,
      "grad_norm": 1.6145128011703491,
      "learning_rate": 4.91912512716175e-05,
      "loss": 4.0928,
      "step": 1590
    },
    {
      "epoch": 0.8138351983723296,
      "grad_norm": 1.6500154733657837,
      "learning_rate": 4.918616480162767e-05,
      "loss": 4.0666,
      "step": 1600
    },
    {
      "epoch": 0.8189216683621566,
      "grad_norm": 1.7474112510681152,
      "learning_rate": 4.918107833163785e-05,
      "loss": 4.1195,
      "step": 1610
    },
    {
      "epoch": 0.8240081383519837,
      "grad_norm": 1.3464937210083008,
      "learning_rate": 4.917599186164802e-05,
      "loss": 4.0685,
      "step": 1620
    },
    {
      "epoch": 0.8290946083418108,
      "grad_norm": 1.6267820596694946,
      "learning_rate": 4.917090539165819e-05,
      "loss": 4.12,
      "step": 1630
    },
    {
      "epoch": 0.8341810783316378,
      "grad_norm": 1.491764783859253,
      "learning_rate": 4.9165818921668366e-05,
      "loss": 4.1061,
      "step": 1640
    },
    {
      "epoch": 0.8392675483214649,
      "grad_norm": 1.5330615043640137,
      "learning_rate": 4.9160732451678536e-05,
      "loss": 4.0751,
      "step": 1650
    },
    {
      "epoch": 0.844354018311292,
      "grad_norm": 1.8004283905029297,
      "learning_rate": 4.9155645981688706e-05,
      "loss": 4.1599,
      "step": 1660
    },
    {
      "epoch": 0.8494404883011191,
      "grad_norm": 1.5963318347930908,
      "learning_rate": 4.915055951169888e-05,
      "loss": 4.0844,
      "step": 1670
    },
    {
      "epoch": 0.854526958290946,
      "grad_norm": 1.2102068662643433,
      "learning_rate": 4.914547304170905e-05,
      "loss": 4.0799,
      "step": 1680
    },
    {
      "epoch": 0.8596134282807731,
      "grad_norm": 1.4623253345489502,
      "learning_rate": 4.914038657171923e-05,
      "loss": 4.0773,
      "step": 1690
    },
    {
      "epoch": 0.8646998982706002,
      "grad_norm": 1.9084428548812866,
      "learning_rate": 4.9135300101729406e-05,
      "loss": 4.0293,
      "step": 1700
    },
    {
      "epoch": 0.8697863682604272,
      "grad_norm": 1.6433120965957642,
      "learning_rate": 4.9130213631739576e-05,
      "loss": 4.0589,
      "step": 1710
    },
    {
      "epoch": 0.8748728382502543,
      "grad_norm": 1.7939417362213135,
      "learning_rate": 4.9125127161749746e-05,
      "loss": 4.0127,
      "step": 1720
    },
    {
      "epoch": 0.8799593082400814,
      "grad_norm": 2.129500150680542,
      "learning_rate": 4.912004069175992e-05,
      "loss": 4.0082,
      "step": 1730
    },
    {
      "epoch": 0.8850457782299085,
      "grad_norm": 1.7895512580871582,
      "learning_rate": 4.911495422177009e-05,
      "loss": 4.0832,
      "step": 1740
    },
    {
      "epoch": 0.8901322482197355,
      "grad_norm": 2.037670135498047,
      "learning_rate": 4.910986775178026e-05,
      "loss": 4.0918,
      "step": 1750
    },
    {
      "epoch": 0.8952187182095626,
      "grad_norm": 1.5972774028778076,
      "learning_rate": 4.910478128179044e-05,
      "loss": 4.1087,
      "step": 1760
    },
    {
      "epoch": 0.9003051881993896,
      "grad_norm": 2.431774616241455,
      "learning_rate": 4.909969481180061e-05,
      "loss": 4.0779,
      "step": 1770
    },
    {
      "epoch": 0.9053916581892166,
      "grad_norm": 1.5935935974121094,
      "learning_rate": 4.9094608341810786e-05,
      "loss": 4.1047,
      "step": 1780
    },
    {
      "epoch": 0.9104781281790437,
      "grad_norm": 1.4424139261245728,
      "learning_rate": 4.908952187182096e-05,
      "loss": 4.0734,
      "step": 1790
    },
    {
      "epoch": 0.9155645981688708,
      "grad_norm": 1.7132132053375244,
      "learning_rate": 4.908443540183113e-05,
      "loss": 4.067,
      "step": 1800
    },
    {
      "epoch": 0.9206510681586979,
      "grad_norm": 1.969544768333435,
      "learning_rate": 4.907934893184131e-05,
      "loss": 4.0115,
      "step": 1810
    },
    {
      "epoch": 0.9257375381485249,
      "grad_norm": 1.8443292379379272,
      "learning_rate": 4.907426246185148e-05,
      "loss": 4.046,
      "step": 1820
    },
    {
      "epoch": 0.930824008138352,
      "grad_norm": 1.3656728267669678,
      "learning_rate": 4.906917599186165e-05,
      "loss": 4.0337,
      "step": 1830
    },
    {
      "epoch": 0.9359104781281791,
      "grad_norm": 1.4383519887924194,
      "learning_rate": 4.9064089521871825e-05,
      "loss": 4.1255,
      "step": 1840
    },
    {
      "epoch": 0.940996948118006,
      "grad_norm": 2.0288400650024414,
      "learning_rate": 4.9059003051881995e-05,
      "loss": 4.0633,
      "step": 1850
    },
    {
      "epoch": 0.9460834181078331,
      "grad_norm": 1.3376163244247437,
      "learning_rate": 4.9053916581892165e-05,
      "loss": 4.0134,
      "step": 1860
    },
    {
      "epoch": 0.9511698880976602,
      "grad_norm": 2.0227010250091553,
      "learning_rate": 4.904883011190234e-05,
      "loss": 4.0001,
      "step": 1870
    },
    {
      "epoch": 0.9562563580874873,
      "grad_norm": 2.045563220977783,
      "learning_rate": 4.904374364191252e-05,
      "loss": 3.9864,
      "step": 1880
    },
    {
      "epoch": 0.9613428280773143,
      "grad_norm": 2.089078426361084,
      "learning_rate": 4.903865717192269e-05,
      "loss": 4.0078,
      "step": 1890
    },
    {
      "epoch": 0.9664292980671414,
      "grad_norm": 1.4491204023361206,
      "learning_rate": 4.9033570701932865e-05,
      "loss": 4.1164,
      "step": 1900
    },
    {
      "epoch": 0.9715157680569685,
      "grad_norm": 2.1739985942840576,
      "learning_rate": 4.9028484231943035e-05,
      "loss": 4.0323,
      "step": 1910
    },
    {
      "epoch": 0.9766022380467956,
      "grad_norm": 1.5660347938537598,
      "learning_rate": 4.9023397761953205e-05,
      "loss": 3.9893,
      "step": 1920
    },
    {
      "epoch": 0.9816887080366226,
      "grad_norm": 1.423434853553772,
      "learning_rate": 4.901831129196338e-05,
      "loss": 4.0604,
      "step": 1930
    },
    {
      "epoch": 0.9867751780264497,
      "grad_norm": 2.103409767150879,
      "learning_rate": 4.901322482197355e-05,
      "loss": 3.9823,
      "step": 1940
    },
    {
      "epoch": 0.9918616480162767,
      "grad_norm": 1.478571891784668,
      "learning_rate": 4.900813835198372e-05,
      "loss": 4.0099,
      "step": 1950
    },
    {
      "epoch": 0.9969481180061037,
      "grad_norm": 2.681691884994507,
      "learning_rate": 4.90030518819939e-05,
      "loss": 4.0214,
      "step": 1960
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.9944815635681152,
      "eval_runtime": 2.637,
      "eval_samples_per_second": 1052.336,
      "eval_steps_per_second": 131.589,
      "step": 1966
    },
    {
      "epoch": 1.002034587995931,
      "grad_norm": 1.5439386367797852,
      "learning_rate": 4.899796541200407e-05,
      "loss": 3.962,
      "step": 1970
    },
    {
      "epoch": 1.007121057985758,
      "grad_norm": 1.8132206201553345,
      "learning_rate": 4.8992878942014244e-05,
      "loss": 4.0555,
      "step": 1980
    },
    {
      "epoch": 1.0122075279755849,
      "grad_norm": 1.8010733127593994,
      "learning_rate": 4.898779247202442e-05,
      "loss": 4.0329,
      "step": 1990
    },
    {
      "epoch": 1.017293997965412,
      "grad_norm": 2.2718300819396973,
      "learning_rate": 4.898270600203459e-05,
      "loss": 4.0538,
      "step": 2000
    },
    {
      "epoch": 1.022380467955239,
      "grad_norm": 1.835971474647522,
      "learning_rate": 4.897761953204476e-05,
      "loss": 4.0172,
      "step": 2010
    },
    {
      "epoch": 1.027466937945066,
      "grad_norm": 1.8991100788116455,
      "learning_rate": 4.897253306205494e-05,
      "loss": 4.0043,
      "step": 2020
    },
    {
      "epoch": 1.0325534079348933,
      "grad_norm": 2.124861478805542,
      "learning_rate": 4.896744659206511e-05,
      "loss": 3.9585,
      "step": 2030
    },
    {
      "epoch": 1.0376398779247202,
      "grad_norm": 1.7480398416519165,
      "learning_rate": 4.8962360122075284e-05,
      "loss": 4.0108,
      "step": 2040
    },
    {
      "epoch": 1.0427263479145472,
      "grad_norm": 3.101414203643799,
      "learning_rate": 4.8957273652085454e-05,
      "loss": 4.0298,
      "step": 2050
    },
    {
      "epoch": 1.0478128179043744,
      "grad_norm": 2.015698194503784,
      "learning_rate": 4.8952187182095624e-05,
      "loss": 4.0388,
      "step": 2060
    },
    {
      "epoch": 1.0528992878942014,
      "grad_norm": 1.9052571058273315,
      "learning_rate": 4.89471007121058e-05,
      "loss": 4.0278,
      "step": 2070
    },
    {
      "epoch": 1.0579857578840284,
      "grad_norm": 2.2769241333007812,
      "learning_rate": 4.894201424211598e-05,
      "loss": 4.0523,
      "step": 2080
    },
    {
      "epoch": 1.0630722278738556,
      "grad_norm": 1.8658063411712646,
      "learning_rate": 4.893692777212615e-05,
      "loss": 4.0593,
      "step": 2090
    },
    {
      "epoch": 1.0681586978636826,
      "grad_norm": 1.8654835224151611,
      "learning_rate": 4.8931841302136324e-05,
      "loss": 4.013,
      "step": 2100
    },
    {
      "epoch": 1.0732451678535098,
      "grad_norm": 1.6722782850265503,
      "learning_rate": 4.8926754832146494e-05,
      "loss": 3.9998,
      "step": 2110
    },
    {
      "epoch": 1.0783316378433367,
      "grad_norm": 2.4552009105682373,
      "learning_rate": 4.8921668362156664e-05,
      "loss": 3.9824,
      "step": 2120
    },
    {
      "epoch": 1.0834181078331637,
      "grad_norm": 3.1544547080993652,
      "learning_rate": 4.891658189216684e-05,
      "loss": 3.9884,
      "step": 2130
    },
    {
      "epoch": 1.088504577822991,
      "grad_norm": 2.443894624710083,
      "learning_rate": 4.891149542217701e-05,
      "loss": 4.0525,
      "step": 2140
    },
    {
      "epoch": 1.093591047812818,
      "grad_norm": 1.3459848165512085,
      "learning_rate": 4.890640895218718e-05,
      "loss": 3.9722,
      "step": 2150
    },
    {
      "epoch": 1.0986775178026449,
      "grad_norm": 2.145956039428711,
      "learning_rate": 4.890132248219736e-05,
      "loss": 3.9298,
      "step": 2160
    },
    {
      "epoch": 1.103763987792472,
      "grad_norm": 1.4805586338043213,
      "learning_rate": 4.889623601220753e-05,
      "loss": 4.0408,
      "step": 2170
    },
    {
      "epoch": 1.108850457782299,
      "grad_norm": 1.902617335319519,
      "learning_rate": 4.88911495422177e-05,
      "loss": 3.9652,
      "step": 2180
    },
    {
      "epoch": 1.113936927772126,
      "grad_norm": 2.057846784591675,
      "learning_rate": 4.888606307222788e-05,
      "loss": 3.9803,
      "step": 2190
    },
    {
      "epoch": 1.1190233977619533,
      "grad_norm": 1.7683820724487305,
      "learning_rate": 4.888097660223805e-05,
      "loss": 3.973,
      "step": 2200
    },
    {
      "epoch": 1.1241098677517802,
      "grad_norm": 2.033362627029419,
      "learning_rate": 4.887589013224822e-05,
      "loss": 4.0147,
      "step": 2210
    },
    {
      "epoch": 1.1291963377416074,
      "grad_norm": 1.5628010034561157,
      "learning_rate": 4.8870803662258396e-05,
      "loss": 3.8702,
      "step": 2220
    },
    {
      "epoch": 1.1342828077314344,
      "grad_norm": 2.378798246383667,
      "learning_rate": 4.8865717192268566e-05,
      "loss": 3.964,
      "step": 2230
    },
    {
      "epoch": 1.1393692777212614,
      "grad_norm": 2.093319892883301,
      "learning_rate": 4.8860630722278736e-05,
      "loss": 4.0114,
      "step": 2240
    },
    {
      "epoch": 1.1444557477110886,
      "grad_norm": 2.6308491230010986,
      "learning_rate": 4.885554425228891e-05,
      "loss": 4.0142,
      "step": 2250
    },
    {
      "epoch": 1.1495422177009156,
      "grad_norm": 2.726793050765991,
      "learning_rate": 4.885045778229908e-05,
      "loss": 4.0365,
      "step": 2260
    },
    {
      "epoch": 1.1546286876907426,
      "grad_norm": 2.5190415382385254,
      "learning_rate": 4.884537131230926e-05,
      "loss": 3.9481,
      "step": 2270
    },
    {
      "epoch": 1.1597151576805698,
      "grad_norm": 2.6700727939605713,
      "learning_rate": 4.8840284842319436e-05,
      "loss": 3.9796,
      "step": 2280
    },
    {
      "epoch": 1.1648016276703967,
      "grad_norm": 2.0161519050598145,
      "learning_rate": 4.8835198372329606e-05,
      "loss": 3.9047,
      "step": 2290
    },
    {
      "epoch": 1.1698880976602237,
      "grad_norm": 5.05422306060791,
      "learning_rate": 4.883011190233978e-05,
      "loss": 4.0376,
      "step": 2300
    },
    {
      "epoch": 1.174974567650051,
      "grad_norm": 2.1214497089385986,
      "learning_rate": 4.882502543234995e-05,
      "loss": 3.944,
      "step": 2310
    },
    {
      "epoch": 1.180061037639878,
      "grad_norm": 1.6276074647903442,
      "learning_rate": 4.881993896236012e-05,
      "loss": 3.9378,
      "step": 2320
    },
    {
      "epoch": 1.1851475076297049,
      "grad_norm": 1.5870987176895142,
      "learning_rate": 4.88148524923703e-05,
      "loss": 3.9637,
      "step": 2330
    },
    {
      "epoch": 1.190233977619532,
      "grad_norm": 2.0462143421173096,
      "learning_rate": 4.880976602238047e-05,
      "loss": 3.9431,
      "step": 2340
    },
    {
      "epoch": 1.195320447609359,
      "grad_norm": 2.922961711883545,
      "learning_rate": 4.880467955239064e-05,
      "loss": 3.9334,
      "step": 2350
    },
    {
      "epoch": 1.200406917599186,
      "grad_norm": 1.9806867837905884,
      "learning_rate": 4.8799593082400816e-05,
      "loss": 3.9257,
      "step": 2360
    },
    {
      "epoch": 1.2054933875890133,
      "grad_norm": 2.251918077468872,
      "learning_rate": 4.879450661241099e-05,
      "loss": 4.068,
      "step": 2370
    },
    {
      "epoch": 1.2105798575788402,
      "grad_norm": 2.077712297439575,
      "learning_rate": 4.878942014242116e-05,
      "loss": 3.9651,
      "step": 2380
    },
    {
      "epoch": 1.2156663275686674,
      "grad_norm": 2.7639362812042236,
      "learning_rate": 4.878433367243134e-05,
      "loss": 3.9453,
      "step": 2390
    },
    {
      "epoch": 1.2207527975584944,
      "grad_norm": 3.0231478214263916,
      "learning_rate": 4.877924720244151e-05,
      "loss": 3.967,
      "step": 2400
    },
    {
      "epoch": 1.2258392675483214,
      "grad_norm": 2.7401299476623535,
      "learning_rate": 4.877416073245168e-05,
      "loss": 3.8482,
      "step": 2410
    },
    {
      "epoch": 1.2309257375381486,
      "grad_norm": 2.4448115825653076,
      "learning_rate": 4.8769074262461855e-05,
      "loss": 3.9619,
      "step": 2420
    },
    {
      "epoch": 1.2360122075279756,
      "grad_norm": 2.610426664352417,
      "learning_rate": 4.8763987792472025e-05,
      "loss": 3.8927,
      "step": 2430
    },
    {
      "epoch": 1.2410986775178026,
      "grad_norm": 2.169914960861206,
      "learning_rate": 4.8758901322482195e-05,
      "loss": 3.9062,
      "step": 2440
    },
    {
      "epoch": 1.2461851475076298,
      "grad_norm": 1.9923087358474731,
      "learning_rate": 4.875381485249237e-05,
      "loss": 3.9691,
      "step": 2450
    },
    {
      "epoch": 1.2512716174974567,
      "grad_norm": 1.861140489578247,
      "learning_rate": 4.874872838250255e-05,
      "loss": 3.9622,
      "step": 2460
    },
    {
      "epoch": 1.2563580874872837,
      "grad_norm": 2.7623417377471924,
      "learning_rate": 4.874364191251272e-05,
      "loss": 4.0254,
      "step": 2470
    },
    {
      "epoch": 1.261444557477111,
      "grad_norm": 2.691542148590088,
      "learning_rate": 4.8738555442522895e-05,
      "loss": 3.978,
      "step": 2480
    },
    {
      "epoch": 1.266531027466938,
      "grad_norm": 2.4289944171905518,
      "learning_rate": 4.8733468972533065e-05,
      "loss": 3.9974,
      "step": 2490
    },
    {
      "epoch": 1.2716174974567651,
      "grad_norm": 1.9797954559326172,
      "learning_rate": 4.8728382502543235e-05,
      "loss": 3.9843,
      "step": 2500
    },
    {
      "epoch": 1.276703967446592,
      "grad_norm": 2.807321310043335,
      "learning_rate": 4.872329603255341e-05,
      "loss": 3.9557,
      "step": 2510
    },
    {
      "epoch": 1.281790437436419,
      "grad_norm": 2.413821220397949,
      "learning_rate": 4.871820956256358e-05,
      "loss": 3.9128,
      "step": 2520
    },
    {
      "epoch": 1.286876907426246,
      "grad_norm": 2.500555992126465,
      "learning_rate": 4.871312309257375e-05,
      "loss": 4.0065,
      "step": 2530
    },
    {
      "epoch": 1.2919633774160733,
      "grad_norm": 2.4103293418884277,
      "learning_rate": 4.870803662258393e-05,
      "loss": 3.9594,
      "step": 2540
    },
    {
      "epoch": 1.2970498474059002,
      "grad_norm": 2.211968421936035,
      "learning_rate": 4.8702950152594105e-05,
      "loss": 3.9439,
      "step": 2550
    },
    {
      "epoch": 1.3021363173957274,
      "grad_norm": 1.8204437494277954,
      "learning_rate": 4.8697863682604274e-05,
      "loss": 3.8638,
      "step": 2560
    },
    {
      "epoch": 1.3072227873855544,
      "grad_norm": 2.9566521644592285,
      "learning_rate": 4.869277721261445e-05,
      "loss": 3.9171,
      "step": 2570
    },
    {
      "epoch": 1.3123092573753814,
      "grad_norm": 2.778378963470459,
      "learning_rate": 4.868769074262462e-05,
      "loss": 3.9873,
      "step": 2580
    },
    {
      "epoch": 1.3173957273652086,
      "grad_norm": 2.2707302570343018,
      "learning_rate": 4.86826042726348e-05,
      "loss": 3.897,
      "step": 2590
    },
    {
      "epoch": 1.3224821973550356,
      "grad_norm": 2.2261154651641846,
      "learning_rate": 4.867751780264497e-05,
      "loss": 3.9147,
      "step": 2600
    },
    {
      "epoch": 1.3275686673448628,
      "grad_norm": 2.997498035430908,
      "learning_rate": 4.867243133265514e-05,
      "loss": 3.9852,
      "step": 2610
    },
    {
      "epoch": 1.3326551373346898,
      "grad_norm": 2.813624143600464,
      "learning_rate": 4.8667344862665314e-05,
      "loss": 3.9209,
      "step": 2620
    },
    {
      "epoch": 1.3377416073245167,
      "grad_norm": 2.183887004852295,
      "learning_rate": 4.8662258392675484e-05,
      "loss": 4.0062,
      "step": 2630
    },
    {
      "epoch": 1.3428280773143437,
      "grad_norm": 1.90959632396698,
      "learning_rate": 4.8657171922685654e-05,
      "loss": 3.9272,
      "step": 2640
    },
    {
      "epoch": 1.347914547304171,
      "grad_norm": 2.0961248874664307,
      "learning_rate": 4.865208545269583e-05,
      "loss": 3.9862,
      "step": 2650
    },
    {
      "epoch": 1.353001017293998,
      "grad_norm": 3.1883022785186768,
      "learning_rate": 4.864699898270601e-05,
      "loss": 3.9133,
      "step": 2660
    },
    {
      "epoch": 1.3580874872838251,
      "grad_norm": 3.58553409576416,
      "learning_rate": 4.864191251271618e-05,
      "loss": 3.9675,
      "step": 2670
    },
    {
      "epoch": 1.363173957273652,
      "grad_norm": 2.64648699760437,
      "learning_rate": 4.8636826042726354e-05,
      "loss": 3.9066,
      "step": 2680
    },
    {
      "epoch": 1.368260427263479,
      "grad_norm": 3.3454434871673584,
      "learning_rate": 4.8631739572736524e-05,
      "loss": 3.9468,
      "step": 2690
    },
    {
      "epoch": 1.3733468972533063,
      "grad_norm": 2.500591278076172,
      "learning_rate": 4.8626653102746694e-05,
      "loss": 3.8322,
      "step": 2700
    },
    {
      "epoch": 1.3784333672431333,
      "grad_norm": 2.02000093460083,
      "learning_rate": 4.862156663275687e-05,
      "loss": 3.9598,
      "step": 2710
    },
    {
      "epoch": 1.3835198372329605,
      "grad_norm": 3.3726367950439453,
      "learning_rate": 4.861648016276704e-05,
      "loss": 3.9125,
      "step": 2720
    },
    {
      "epoch": 1.3886063072227874,
      "grad_norm": 3.754370927810669,
      "learning_rate": 4.861139369277721e-05,
      "loss": 3.9597,
      "step": 2730
    },
    {
      "epoch": 1.3936927772126144,
      "grad_norm": 2.5952484607696533,
      "learning_rate": 4.860630722278739e-05,
      "loss": 3.8853,
      "step": 2740
    },
    {
      "epoch": 1.3987792472024414,
      "grad_norm": 3.5110955238342285,
      "learning_rate": 4.860122075279756e-05,
      "loss": 3.9132,
      "step": 2750
    },
    {
      "epoch": 1.4038657171922686,
      "grad_norm": 2.1168408393859863,
      "learning_rate": 4.859613428280773e-05,
      "loss": 3.9108,
      "step": 2760
    },
    {
      "epoch": 1.4089521871820956,
      "grad_norm": 2.5722568035125732,
      "learning_rate": 4.859104781281791e-05,
      "loss": 3.9318,
      "step": 2770
    },
    {
      "epoch": 1.4140386571719228,
      "grad_norm": 2.604870557785034,
      "learning_rate": 4.858596134282808e-05,
      "loss": 3.9304,
      "step": 2780
    },
    {
      "epoch": 1.4191251271617498,
      "grad_norm": 3.0501840114593506,
      "learning_rate": 4.858087487283825e-05,
      "loss": 3.8976,
      "step": 2790
    },
    {
      "epoch": 1.4242115971515767,
      "grad_norm": 3.528156280517578,
      "learning_rate": 4.8575788402848426e-05,
      "loss": 3.9468,
      "step": 2800
    },
    {
      "epoch": 1.4292980671414037,
      "grad_norm": 2.1598286628723145,
      "learning_rate": 4.8570701932858596e-05,
      "loss": 3.8417,
      "step": 2810
    },
    {
      "epoch": 1.434384537131231,
      "grad_norm": 2.7477807998657227,
      "learning_rate": 4.8565615462868766e-05,
      "loss": 3.8616,
      "step": 2820
    },
    {
      "epoch": 1.439471007121058,
      "grad_norm": 2.1633222103118896,
      "learning_rate": 4.856052899287894e-05,
      "loss": 3.9145,
      "step": 2830
    },
    {
      "epoch": 1.4445574771108851,
      "grad_norm": 3.123568296432495,
      "learning_rate": 4.855544252288912e-05,
      "loss": 3.8761,
      "step": 2840
    },
    {
      "epoch": 1.449643947100712,
      "grad_norm": 2.3529410362243652,
      "learning_rate": 4.8550356052899296e-05,
      "loss": 3.9291,
      "step": 2850
    },
    {
      "epoch": 1.454730417090539,
      "grad_norm": 2.1702964305877686,
      "learning_rate": 4.8545269582909466e-05,
      "loss": 3.9836,
      "step": 2860
    },
    {
      "epoch": 1.4598168870803663,
      "grad_norm": 6.299477577209473,
      "learning_rate": 4.8540183112919636e-05,
      "loss": 3.8715,
      "step": 2870
    },
    {
      "epoch": 1.4649033570701933,
      "grad_norm": 1.9678689241409302,
      "learning_rate": 4.853509664292981e-05,
      "loss": 3.8707,
      "step": 2880
    },
    {
      "epoch": 1.4699898270600205,
      "grad_norm": 2.019385576248169,
      "learning_rate": 4.853001017293998e-05,
      "loss": 3.8956,
      "step": 2890
    },
    {
      "epoch": 1.4750762970498474,
      "grad_norm": 2.589125633239746,
      "learning_rate": 4.852492370295015e-05,
      "loss": 3.856,
      "step": 2900
    },
    {
      "epoch": 1.4801627670396744,
      "grad_norm": 2.6016361713409424,
      "learning_rate": 4.851983723296033e-05,
      "loss": 3.8752,
      "step": 2910
    },
    {
      "epoch": 1.4852492370295014,
      "grad_norm": 1.8504096269607544,
      "learning_rate": 4.85147507629705e-05,
      "loss": 3.8341,
      "step": 2920
    },
    {
      "epoch": 1.4903357070193286,
      "grad_norm": 2.361558437347412,
      "learning_rate": 4.850966429298067e-05,
      "loss": 3.903,
      "step": 2930
    },
    {
      "epoch": 1.4954221770091556,
      "grad_norm": 2.77169132232666,
      "learning_rate": 4.8504577822990846e-05,
      "loss": 3.9602,
      "step": 2940
    },
    {
      "epoch": 1.5005086469989828,
      "grad_norm": 2.268280506134033,
      "learning_rate": 4.849949135300102e-05,
      "loss": 3.8591,
      "step": 2950
    },
    {
      "epoch": 1.5055951169888098,
      "grad_norm": 2.139366388320923,
      "learning_rate": 4.849440488301119e-05,
      "loss": 3.8656,
      "step": 2960
    },
    {
      "epoch": 1.5106815869786367,
      "grad_norm": 2.2815325260162354,
      "learning_rate": 4.848931841302137e-05,
      "loss": 3.9288,
      "step": 2970
    },
    {
      "epoch": 1.5157680569684637,
      "grad_norm": 3.4343669414520264,
      "learning_rate": 4.848423194303154e-05,
      "loss": 3.8513,
      "step": 2980
    },
    {
      "epoch": 1.520854526958291,
      "grad_norm": 1.9289299249649048,
      "learning_rate": 4.847914547304171e-05,
      "loss": 3.9442,
      "step": 2990
    },
    {
      "epoch": 1.5259409969481181,
      "grad_norm": 3.3456056118011475,
      "learning_rate": 4.8474059003051885e-05,
      "loss": 3.8703,
      "step": 3000
    },
    {
      "epoch": 1.5310274669379451,
      "grad_norm": 3.178622007369995,
      "learning_rate": 4.8468972533062055e-05,
      "loss": 3.8683,
      "step": 3010
    },
    {
      "epoch": 1.536113936927772,
      "grad_norm": 2.766413450241089,
      "learning_rate": 4.8463886063072225e-05,
      "loss": 3.8761,
      "step": 3020
    },
    {
      "epoch": 1.541200406917599,
      "grad_norm": 2.2417070865631104,
      "learning_rate": 4.84587995930824e-05,
      "loss": 3.9268,
      "step": 3030
    },
    {
      "epoch": 1.5462868769074263,
      "grad_norm": 2.6189918518066406,
      "learning_rate": 4.845371312309258e-05,
      "loss": 3.9012,
      "step": 3040
    },
    {
      "epoch": 1.5513733468972533,
      "grad_norm": 1.843309760093689,
      "learning_rate": 4.844862665310275e-05,
      "loss": 3.9043,
      "step": 3050
    },
    {
      "epoch": 1.5564598168870805,
      "grad_norm": 3.2836098670959473,
      "learning_rate": 4.8443540183112925e-05,
      "loss": 3.8718,
      "step": 3060
    },
    {
      "epoch": 1.5615462868769074,
      "grad_norm": 2.080643892288208,
      "learning_rate": 4.8438453713123095e-05,
      "loss": 3.8666,
      "step": 3070
    },
    {
      "epoch": 1.5666327568667344,
      "grad_norm": 2.10274600982666,
      "learning_rate": 4.8433367243133265e-05,
      "loss": 3.9361,
      "step": 3080
    },
    {
      "epoch": 1.5717192268565614,
      "grad_norm": 3.276583671569824,
      "learning_rate": 4.842828077314344e-05,
      "loss": 3.8648,
      "step": 3090
    },
    {
      "epoch": 1.5768056968463886,
      "grad_norm": 2.5222623348236084,
      "learning_rate": 4.842319430315361e-05,
      "loss": 3.89,
      "step": 3100
    },
    {
      "epoch": 1.5818921668362158,
      "grad_norm": 2.2307581901550293,
      "learning_rate": 4.841810783316379e-05,
      "loss": 3.8316,
      "step": 3110
    },
    {
      "epoch": 1.5869786368260428,
      "grad_norm": 3.31823468208313,
      "learning_rate": 4.841302136317396e-05,
      "loss": 3.8627,
      "step": 3120
    },
    {
      "epoch": 1.5920651068158698,
      "grad_norm": 3.992385149002075,
      "learning_rate": 4.8407934893184135e-05,
      "loss": 3.8764,
      "step": 3130
    },
    {
      "epoch": 1.5971515768056967,
      "grad_norm": 3.4003562927246094,
      "learning_rate": 4.840284842319431e-05,
      "loss": 3.8693,
      "step": 3140
    },
    {
      "epoch": 1.602238046795524,
      "grad_norm": 1.8446124792099,
      "learning_rate": 4.839776195320448e-05,
      "loss": 3.9082,
      "step": 3150
    },
    {
      "epoch": 1.607324516785351,
      "grad_norm": 2.199260711669922,
      "learning_rate": 4.839267548321465e-05,
      "loss": 3.9227,
      "step": 3160
    },
    {
      "epoch": 1.6124109867751781,
      "grad_norm": 2.4016170501708984,
      "learning_rate": 4.838758901322483e-05,
      "loss": 3.8335,
      "step": 3170
    },
    {
      "epoch": 1.6174974567650051,
      "grad_norm": 2.32049298286438,
      "learning_rate": 4.8382502543235e-05,
      "loss": 3.8902,
      "step": 3180
    },
    {
      "epoch": 1.622583926754832,
      "grad_norm": 2.3983724117279053,
      "learning_rate": 4.837741607324517e-05,
      "loss": 3.782,
      "step": 3190
    },
    {
      "epoch": 1.627670396744659,
      "grad_norm": 2.9348959922790527,
      "learning_rate": 4.8372329603255344e-05,
      "loss": 3.8834,
      "step": 3200
    },
    {
      "epoch": 1.6327568667344863,
      "grad_norm": 2.4127590656280518,
      "learning_rate": 4.8367243133265514e-05,
      "loss": 3.8701,
      "step": 3210
    },
    {
      "epoch": 1.6378433367243135,
      "grad_norm": 2.564382553100586,
      "learning_rate": 4.836215666327569e-05,
      "loss": 3.8865,
      "step": 3220
    },
    {
      "epoch": 1.6429298067141405,
      "grad_norm": 2.2278523445129395,
      "learning_rate": 4.835707019328586e-05,
      "loss": 3.9217,
      "step": 3230
    },
    {
      "epoch": 1.6480162767039674,
      "grad_norm": 3.367666006088257,
      "learning_rate": 4.835198372329604e-05,
      "loss": 3.8624,
      "step": 3240
    },
    {
      "epoch": 1.6531027466937944,
      "grad_norm": 2.4702365398406982,
      "learning_rate": 4.834689725330621e-05,
      "loss": 3.8736,
      "step": 3250
    },
    {
      "epoch": 1.6581892166836214,
      "grad_norm": 4.007655143737793,
      "learning_rate": 4.8341810783316384e-05,
      "loss": 3.8503,
      "step": 3260
    },
    {
      "epoch": 1.6632756866734486,
      "grad_norm": 3.676967144012451,
      "learning_rate": 4.8336724313326554e-05,
      "loss": 3.8622,
      "step": 3270
    },
    {
      "epoch": 1.6683621566632758,
      "grad_norm": 2.4301090240478516,
      "learning_rate": 4.8331637843336724e-05,
      "loss": 3.8197,
      "step": 3280
    },
    {
      "epoch": 1.6734486266531028,
      "grad_norm": 3.7800233364105225,
      "learning_rate": 4.83265513733469e-05,
      "loss": 3.8499,
      "step": 3290
    },
    {
      "epoch": 1.6785350966429298,
      "grad_norm": 3.440908432006836,
      "learning_rate": 4.832146490335707e-05,
      "loss": 3.8585,
      "step": 3300
    },
    {
      "epoch": 1.6836215666327567,
      "grad_norm": 2.779834508895874,
      "learning_rate": 4.831637843336724e-05,
      "loss": 3.9192,
      "step": 3310
    },
    {
      "epoch": 1.688708036622584,
      "grad_norm": 2.7942798137664795,
      "learning_rate": 4.831129196337742e-05,
      "loss": 3.8236,
      "step": 3320
    },
    {
      "epoch": 1.693794506612411,
      "grad_norm": 2.871171236038208,
      "learning_rate": 4.830620549338759e-05,
      "loss": 3.8742,
      "step": 3330
    },
    {
      "epoch": 1.6988809766022381,
      "grad_norm": 2.3870112895965576,
      "learning_rate": 4.830111902339776e-05,
      "loss": 3.8669,
      "step": 3340
    },
    {
      "epoch": 1.7039674465920651,
      "grad_norm": 4.039936542510986,
      "learning_rate": 4.829603255340794e-05,
      "loss": 3.917,
      "step": 3350
    },
    {
      "epoch": 1.709053916581892,
      "grad_norm": 3.6603236198425293,
      "learning_rate": 4.829094608341811e-05,
      "loss": 3.8463,
      "step": 3360
    },
    {
      "epoch": 1.714140386571719,
      "grad_norm": 2.9734718799591064,
      "learning_rate": 4.828585961342828e-05,
      "loss": 3.8646,
      "step": 3370
    },
    {
      "epoch": 1.7192268565615463,
      "grad_norm": 2.11411452293396,
      "learning_rate": 4.8280773143438456e-05,
      "loss": 3.82,
      "step": 3380
    },
    {
      "epoch": 1.7243133265513735,
      "grad_norm": 2.353867530822754,
      "learning_rate": 4.8275686673448626e-05,
      "loss": 3.8583,
      "step": 3390
    },
    {
      "epoch": 1.7293997965412005,
      "grad_norm": 4.884743690490723,
      "learning_rate": 4.82706002034588e-05,
      "loss": 3.8391,
      "step": 3400
    },
    {
      "epoch": 1.7344862665310274,
      "grad_norm": 2.719420909881592,
      "learning_rate": 4.826551373346897e-05,
      "loss": 3.8841,
      "step": 3410
    },
    {
      "epoch": 1.7395727365208544,
      "grad_norm": 3.1500298976898193,
      "learning_rate": 4.826042726347915e-05,
      "loss": 3.9163,
      "step": 3420
    },
    {
      "epoch": 1.7446592065106816,
      "grad_norm": 3.3296663761138916,
      "learning_rate": 4.8255340793489326e-05,
      "loss": 3.8473,
      "step": 3430
    },
    {
      "epoch": 1.7497456765005086,
      "grad_norm": 2.9359757900238037,
      "learning_rate": 4.8250254323499496e-05,
      "loss": 3.891,
      "step": 3440
    },
    {
      "epoch": 1.7548321464903358,
      "grad_norm": 2.534726858139038,
      "learning_rate": 4.8245167853509666e-05,
      "loss": 3.8741,
      "step": 3450
    },
    {
      "epoch": 1.7599186164801628,
      "grad_norm": 3.068474531173706,
      "learning_rate": 4.824008138351984e-05,
      "loss": 3.8028,
      "step": 3460
    },
    {
      "epoch": 1.7650050864699898,
      "grad_norm": 2.4252750873565674,
      "learning_rate": 4.823499491353001e-05,
      "loss": 3.8805,
      "step": 3470
    },
    {
      "epoch": 1.7700915564598168,
      "grad_norm": 2.3944170475006104,
      "learning_rate": 4.822990844354018e-05,
      "loss": 3.8117,
      "step": 3480
    },
    {
      "epoch": 1.775178026449644,
      "grad_norm": 2.5047669410705566,
      "learning_rate": 4.822482197355036e-05,
      "loss": 3.8235,
      "step": 3490
    },
    {
      "epoch": 1.7802644964394712,
      "grad_norm": 3.883359432220459,
      "learning_rate": 4.821973550356053e-05,
      "loss": 3.8281,
      "step": 3500
    },
    {
      "epoch": 1.7853509664292981,
      "grad_norm": 3.714405059814453,
      "learning_rate": 4.8214649033570706e-05,
      "loss": 3.7933,
      "step": 3510
    },
    {
      "epoch": 1.7904374364191251,
      "grad_norm": 2.343216896057129,
      "learning_rate": 4.820956256358088e-05,
      "loss": 3.8734,
      "step": 3520
    },
    {
      "epoch": 1.795523906408952,
      "grad_norm": 2.821183443069458,
      "learning_rate": 4.820447609359105e-05,
      "loss": 3.7989,
      "step": 3530
    },
    {
      "epoch": 1.8006103763987793,
      "grad_norm": 3.595947742462158,
      "learning_rate": 4.819938962360122e-05,
      "loss": 3.8246,
      "step": 3540
    },
    {
      "epoch": 1.8056968463886063,
      "grad_norm": 3.038120746612549,
      "learning_rate": 4.81943031536114e-05,
      "loss": 3.7674,
      "step": 3550
    },
    {
      "epoch": 1.8107833163784335,
      "grad_norm": 2.997492790222168,
      "learning_rate": 4.818921668362157e-05,
      "loss": 3.8784,
      "step": 3560
    },
    {
      "epoch": 1.8158697863682605,
      "grad_norm": 2.9268176555633545,
      "learning_rate": 4.818413021363174e-05,
      "loss": 3.8091,
      "step": 3570
    },
    {
      "epoch": 1.8209562563580874,
      "grad_norm": 2.6341984272003174,
      "learning_rate": 4.8179043743641915e-05,
      "loss": 3.7538,
      "step": 3580
    },
    {
      "epoch": 1.8260427263479144,
      "grad_norm": 3.3541431427001953,
      "learning_rate": 4.8173957273652085e-05,
      "loss": 3.7694,
      "step": 3590
    },
    {
      "epoch": 1.8311291963377416,
      "grad_norm": 3.993887186050415,
      "learning_rate": 4.8168870803662255e-05,
      "loss": 3.8071,
      "step": 3600
    },
    {
      "epoch": 1.8362156663275688,
      "grad_norm": 2.154033899307251,
      "learning_rate": 4.816378433367243e-05,
      "loss": 3.8258,
      "step": 3610
    },
    {
      "epoch": 1.8413021363173958,
      "grad_norm": 2.2375717163085938,
      "learning_rate": 4.815869786368261e-05,
      "loss": 3.8102,
      "step": 3620
    },
    {
      "epoch": 1.8463886063072228,
      "grad_norm": 2.48102068901062,
      "learning_rate": 4.815361139369278e-05,
      "loss": 3.8123,
      "step": 3630
    },
    {
      "epoch": 1.8514750762970498,
      "grad_norm": 2.991802215576172,
      "learning_rate": 4.8148524923702955e-05,
      "loss": 3.8595,
      "step": 3640
    },
    {
      "epoch": 1.8565615462868768,
      "grad_norm": 3.0669162273406982,
      "learning_rate": 4.8143438453713125e-05,
      "loss": 3.7474,
      "step": 3650
    },
    {
      "epoch": 1.861648016276704,
      "grad_norm": 2.7170755863189697,
      "learning_rate": 4.81383519837233e-05,
      "loss": 3.7943,
      "step": 3660
    },
    {
      "epoch": 1.8667344862665312,
      "grad_norm": 3.6425702571868896,
      "learning_rate": 4.813326551373347e-05,
      "loss": 3.833,
      "step": 3670
    },
    {
      "epoch": 1.8718209562563581,
      "grad_norm": 2.2230606079101562,
      "learning_rate": 4.812817904374364e-05,
      "loss": 3.863,
      "step": 3680
    },
    {
      "epoch": 1.8769074262461851,
      "grad_norm": 2.9699721336364746,
      "learning_rate": 4.812309257375382e-05,
      "loss": 3.8508,
      "step": 3690
    },
    {
      "epoch": 1.881993896236012,
      "grad_norm": 2.8466637134552,
      "learning_rate": 4.811800610376399e-05,
      "loss": 3.8133,
      "step": 3700
    },
    {
      "epoch": 1.8870803662258393,
      "grad_norm": 3.647420883178711,
      "learning_rate": 4.8112919633774165e-05,
      "loss": 3.8016,
      "step": 3710
    },
    {
      "epoch": 1.8921668362156663,
      "grad_norm": 3.686527729034424,
      "learning_rate": 4.810783316378434e-05,
      "loss": 3.8999,
      "step": 3720
    },
    {
      "epoch": 1.8972533062054935,
      "grad_norm": 2.251647710800171,
      "learning_rate": 4.810274669379451e-05,
      "loss": 3.7827,
      "step": 3730
    },
    {
      "epoch": 1.9023397761953205,
      "grad_norm": 3.028019428253174,
      "learning_rate": 4.809766022380468e-05,
      "loss": 3.7994,
      "step": 3740
    },
    {
      "epoch": 1.9074262461851474,
      "grad_norm": 3.234785318374634,
      "learning_rate": 4.809257375381486e-05,
      "loss": 3.8321,
      "step": 3750
    },
    {
      "epoch": 1.9125127161749744,
      "grad_norm": 3.6764044761657715,
      "learning_rate": 4.808748728382503e-05,
      "loss": 3.8554,
      "step": 3760
    },
    {
      "epoch": 1.9175991861648016,
      "grad_norm": 3.336867570877075,
      "learning_rate": 4.80824008138352e-05,
      "loss": 3.7937,
      "step": 3770
    },
    {
      "epoch": 1.9226856561546288,
      "grad_norm": 3.244828701019287,
      "learning_rate": 4.8077314343845374e-05,
      "loss": 3.789,
      "step": 3780
    },
    {
      "epoch": 1.9277721261444558,
      "grad_norm": 2.5136430263519287,
      "learning_rate": 4.8072227873855544e-05,
      "loss": 3.7683,
      "step": 3790
    },
    {
      "epoch": 1.9328585961342828,
      "grad_norm": 5.260274410247803,
      "learning_rate": 4.806714140386572e-05,
      "loss": 3.8014,
      "step": 3800
    },
    {
      "epoch": 1.9379450661241098,
      "grad_norm": 3.4694857597351074,
      "learning_rate": 4.80620549338759e-05,
      "loss": 3.8004,
      "step": 3810
    },
    {
      "epoch": 1.943031536113937,
      "grad_norm": 2.9836699962615967,
      "learning_rate": 4.805696846388607e-05,
      "loss": 3.7309,
      "step": 3820
    },
    {
      "epoch": 1.948118006103764,
      "grad_norm": 3.3965353965759277,
      "learning_rate": 4.805188199389624e-05,
      "loss": 3.8469,
      "step": 3830
    },
    {
      "epoch": 1.9532044760935912,
      "grad_norm": 3.9317257404327393,
      "learning_rate": 4.8046795523906414e-05,
      "loss": 3.8225,
      "step": 3840
    },
    {
      "epoch": 1.9582909460834181,
      "grad_norm": 5.062075614929199,
      "learning_rate": 4.8041709053916584e-05,
      "loss": 3.7682,
      "step": 3850
    },
    {
      "epoch": 1.9633774160732451,
      "grad_norm": 3.7126967906951904,
      "learning_rate": 4.8036622583926754e-05,
      "loss": 3.892,
      "step": 3860
    },
    {
      "epoch": 1.968463886063072,
      "grad_norm": 4.513299465179443,
      "learning_rate": 4.803153611393693e-05,
      "loss": 3.8108,
      "step": 3870
    },
    {
      "epoch": 1.9735503560528993,
      "grad_norm": 2.908249616622925,
      "learning_rate": 4.80264496439471e-05,
      "loss": 3.8366,
      "step": 3880
    },
    {
      "epoch": 1.9786368260427265,
      "grad_norm": 5.220104694366455,
      "learning_rate": 4.802136317395727e-05,
      "loss": 3.826,
      "step": 3890
    },
    {
      "epoch": 1.9837232960325535,
      "grad_norm": 3.4382638931274414,
      "learning_rate": 4.801627670396745e-05,
      "loss": 3.7856,
      "step": 3900
    },
    {
      "epoch": 1.9888097660223805,
      "grad_norm": 3.960299015045166,
      "learning_rate": 4.801119023397762e-05,
      "loss": 3.843,
      "step": 3910
    },
    {
      "epoch": 1.9938962360122074,
      "grad_norm": 5.232454776763916,
      "learning_rate": 4.80061037639878e-05,
      "loss": 3.7792,
      "step": 3920
    },
    {
      "epoch": 1.9989827060020344,
      "grad_norm": 2.63761043548584,
      "learning_rate": 4.800101729399797e-05,
      "loss": 3.8942,
      "step": 3930
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.822338104248047,
      "eval_runtime": 2.6649,
      "eval_samples_per_second": 1041.326,
      "eval_steps_per_second": 130.213,
      "step": 3932
    },
    {
      "epoch": 2.004069175991862,
      "grad_norm": 3.3009870052337646,
      "learning_rate": 4.799593082400814e-05,
      "loss": 3.7859,
      "step": 3940
    },
    {
      "epoch": 2.009155645981689,
      "grad_norm": 2.3990509510040283,
      "learning_rate": 4.7990844354018317e-05,
      "loss": 3.6946,
      "step": 3950
    },
    {
      "epoch": 2.014242115971516,
      "grad_norm": 3.3798155784606934,
      "learning_rate": 4.7985757884028486e-05,
      "loss": 3.824,
      "step": 3960
    },
    {
      "epoch": 2.019328585961343,
      "grad_norm": 4.404094696044922,
      "learning_rate": 4.7980671414038656e-05,
      "loss": 3.8429,
      "step": 3970
    },
    {
      "epoch": 2.0244150559511698,
      "grad_norm": 3.420275926589966,
      "learning_rate": 4.797558494404883e-05,
      "loss": 3.8101,
      "step": 3980
    },
    {
      "epoch": 2.0295015259409968,
      "grad_norm": 3.7091214656829834,
      "learning_rate": 4.7970498474059e-05,
      "loss": 3.7007,
      "step": 3990
    },
    {
      "epoch": 2.034587995930824,
      "grad_norm": 5.376887321472168,
      "learning_rate": 4.796541200406918e-05,
      "loss": 3.789,
      "step": 4000
    },
    {
      "epoch": 2.039674465920651,
      "grad_norm": 2.9828460216522217,
      "learning_rate": 4.7960325534079356e-05,
      "loss": 3.9175,
      "step": 4010
    },
    {
      "epoch": 2.044760935910478,
      "grad_norm": 3.4961628913879395,
      "learning_rate": 4.7955239064089526e-05,
      "loss": 3.7196,
      "step": 4020
    },
    {
      "epoch": 2.049847405900305,
      "grad_norm": 3.5820329189300537,
      "learning_rate": 4.7950152594099696e-05,
      "loss": 3.721,
      "step": 4030
    },
    {
      "epoch": 2.054933875890132,
      "grad_norm": 3.178952932357788,
      "learning_rate": 4.794506612410987e-05,
      "loss": 3.7873,
      "step": 4040
    },
    {
      "epoch": 2.0600203458799595,
      "grad_norm": 4.098269462585449,
      "learning_rate": 4.793997965412004e-05,
      "loss": 3.7829,
      "step": 4050
    },
    {
      "epoch": 2.0651068158697865,
      "grad_norm": 3.055269479751587,
      "learning_rate": 4.793489318413021e-05,
      "loss": 3.7502,
      "step": 4060
    },
    {
      "epoch": 2.0701932858596135,
      "grad_norm": 2.66963529586792,
      "learning_rate": 4.792980671414039e-05,
      "loss": 3.8583,
      "step": 4070
    },
    {
      "epoch": 2.0752797558494405,
      "grad_norm": 3.066627264022827,
      "learning_rate": 4.792472024415056e-05,
      "loss": 3.8102,
      "step": 4080
    },
    {
      "epoch": 2.0803662258392674,
      "grad_norm": 3.998633623123169,
      "learning_rate": 4.7919633774160736e-05,
      "loss": 3.7599,
      "step": 4090
    },
    {
      "epoch": 2.0854526958290944,
      "grad_norm": 3.2907497882843018,
      "learning_rate": 4.791454730417091e-05,
      "loss": 3.7392,
      "step": 4100
    },
    {
      "epoch": 2.090539165818922,
      "grad_norm": 3.4080381393432617,
      "learning_rate": 4.790946083418108e-05,
      "loss": 3.7252,
      "step": 4110
    },
    {
      "epoch": 2.095625635808749,
      "grad_norm": 3.682750701904297,
      "learning_rate": 4.790437436419125e-05,
      "loss": 3.7968,
      "step": 4120
    },
    {
      "epoch": 2.100712105798576,
      "grad_norm": 3.1150565147399902,
      "learning_rate": 4.789928789420143e-05,
      "loss": 3.8283,
      "step": 4130
    },
    {
      "epoch": 2.105798575788403,
      "grad_norm": 3.0871498584747314,
      "learning_rate": 4.78942014242116e-05,
      "loss": 3.8037,
      "step": 4140
    },
    {
      "epoch": 2.1108850457782298,
      "grad_norm": 3.301619291305542,
      "learning_rate": 4.788911495422177e-05,
      "loss": 3.8016,
      "step": 4150
    },
    {
      "epoch": 2.1159715157680568,
      "grad_norm": 5.515467643737793,
      "learning_rate": 4.7884028484231945e-05,
      "loss": 3.7351,
      "step": 4160
    },
    {
      "epoch": 2.121057985757884,
      "grad_norm": 2.8090403079986572,
      "learning_rate": 4.7878942014242115e-05,
      "loss": 3.7198,
      "step": 4170
    },
    {
      "epoch": 2.126144455747711,
      "grad_norm": 4.266490459442139,
      "learning_rate": 4.787385554425229e-05,
      "loss": 3.7448,
      "step": 4180
    },
    {
      "epoch": 2.131230925737538,
      "grad_norm": 4.604043483734131,
      "learning_rate": 4.786876907426246e-05,
      "loss": 3.8793,
      "step": 4190
    },
    {
      "epoch": 2.136317395727365,
      "grad_norm": 3.466778516769409,
      "learning_rate": 4.786368260427264e-05,
      "loss": 3.7978,
      "step": 4200
    },
    {
      "epoch": 2.141403865717192,
      "grad_norm": 3.406524896621704,
      "learning_rate": 4.7858596134282815e-05,
      "loss": 3.672,
      "step": 4210
    },
    {
      "epoch": 2.1464903357070195,
      "grad_norm": 3.2007479667663574,
      "learning_rate": 4.7853509664292985e-05,
      "loss": 3.7458,
      "step": 4220
    },
    {
      "epoch": 2.1515768056968465,
      "grad_norm": 4.488451957702637,
      "learning_rate": 4.7848423194303155e-05,
      "loss": 3.7815,
      "step": 4230
    },
    {
      "epoch": 2.1566632756866735,
      "grad_norm": 4.18934440612793,
      "learning_rate": 4.784333672431333e-05,
      "loss": 3.768,
      "step": 4240
    },
    {
      "epoch": 2.1617497456765005,
      "grad_norm": 4.366158962249756,
      "learning_rate": 4.78382502543235e-05,
      "loss": 3.6843,
      "step": 4250
    },
    {
      "epoch": 2.1668362156663274,
      "grad_norm": 4.3815202713012695,
      "learning_rate": 4.783316378433367e-05,
      "loss": 3.8156,
      "step": 4260
    },
    {
      "epoch": 2.1719226856561544,
      "grad_norm": 2.2758522033691406,
      "learning_rate": 4.782807731434385e-05,
      "loss": 3.7765,
      "step": 4270
    },
    {
      "epoch": 2.177009155645982,
      "grad_norm": 3.107891082763672,
      "learning_rate": 4.782299084435402e-05,
      "loss": 3.7466,
      "step": 4280
    },
    {
      "epoch": 2.182095625635809,
      "grad_norm": 3.354189157485962,
      "learning_rate": 4.7817904374364195e-05,
      "loss": 3.7595,
      "step": 4290
    },
    {
      "epoch": 2.187182095625636,
      "grad_norm": 3.506438970565796,
      "learning_rate": 4.781281790437437e-05,
      "loss": 3.7106,
      "step": 4300
    },
    {
      "epoch": 2.192268565615463,
      "grad_norm": 3.2889552116394043,
      "learning_rate": 4.780773143438454e-05,
      "loss": 3.7488,
      "step": 4310
    },
    {
      "epoch": 2.1973550356052898,
      "grad_norm": 2.5478384494781494,
      "learning_rate": 4.780264496439471e-05,
      "loss": 3.8366,
      "step": 4320
    },
    {
      "epoch": 2.202441505595117,
      "grad_norm": 3.5290231704711914,
      "learning_rate": 4.779755849440489e-05,
      "loss": 3.7865,
      "step": 4330
    },
    {
      "epoch": 2.207527975584944,
      "grad_norm": 3.3598546981811523,
      "learning_rate": 4.779247202441506e-05,
      "loss": 3.8662,
      "step": 4340
    },
    {
      "epoch": 2.212614445574771,
      "grad_norm": 3.1380226612091064,
      "learning_rate": 4.778738555442523e-05,
      "loss": 3.7645,
      "step": 4350
    },
    {
      "epoch": 2.217700915564598,
      "grad_norm": 2.6953513622283936,
      "learning_rate": 4.7782299084435404e-05,
      "loss": 3.8181,
      "step": 4360
    },
    {
      "epoch": 2.222787385554425,
      "grad_norm": 3.2051572799682617,
      "learning_rate": 4.7777212614445574e-05,
      "loss": 3.6611,
      "step": 4370
    },
    {
      "epoch": 2.227873855544252,
      "grad_norm": 3.859872341156006,
      "learning_rate": 4.777212614445575e-05,
      "loss": 3.787,
      "step": 4380
    },
    {
      "epoch": 2.2329603255340795,
      "grad_norm": 4.926129341125488,
      "learning_rate": 4.776703967446593e-05,
      "loss": 3.7507,
      "step": 4390
    },
    {
      "epoch": 2.2380467955239065,
      "grad_norm": 2.5377938747406006,
      "learning_rate": 4.77619532044761e-05,
      "loss": 3.6993,
      "step": 4400
    },
    {
      "epoch": 2.2431332655137335,
      "grad_norm": 4.288913249969482,
      "learning_rate": 4.775686673448627e-05,
      "loss": 3.8625,
      "step": 4410
    },
    {
      "epoch": 2.2482197355035605,
      "grad_norm": 3.177579164505005,
      "learning_rate": 4.7751780264496444e-05,
      "loss": 3.6934,
      "step": 4420
    },
    {
      "epoch": 2.2533062054933874,
      "grad_norm": 3.5934462547302246,
      "learning_rate": 4.7746693794506614e-05,
      "loss": 3.7367,
      "step": 4430
    },
    {
      "epoch": 2.258392675483215,
      "grad_norm": 3.7067041397094727,
      "learning_rate": 4.7741607324516784e-05,
      "loss": 3.7389,
      "step": 4440
    },
    {
      "epoch": 2.263479145473042,
      "grad_norm": 3.248070240020752,
      "learning_rate": 4.773652085452696e-05,
      "loss": 3.7455,
      "step": 4450
    },
    {
      "epoch": 2.268565615462869,
      "grad_norm": 4.805527687072754,
      "learning_rate": 4.773143438453713e-05,
      "loss": 3.7711,
      "step": 4460
    },
    {
      "epoch": 2.273652085452696,
      "grad_norm": 4.811013698577881,
      "learning_rate": 4.772634791454731e-05,
      "loss": 3.8216,
      "step": 4470
    },
    {
      "epoch": 2.278738555442523,
      "grad_norm": 4.649559020996094,
      "learning_rate": 4.7721261444557483e-05,
      "loss": 3.7682,
      "step": 4480
    },
    {
      "epoch": 2.2838250254323498,
      "grad_norm": 4.402706623077393,
      "learning_rate": 4.7716174974567653e-05,
      "loss": 3.7639,
      "step": 4490
    },
    {
      "epoch": 2.288911495422177,
      "grad_norm": 4.548746585845947,
      "learning_rate": 4.771108850457783e-05,
      "loss": 3.7394,
      "step": 4500
    },
    {
      "epoch": 2.293997965412004,
      "grad_norm": 3.320382833480835,
      "learning_rate": 4.7706002034588e-05,
      "loss": 3.7268,
      "step": 4510
    },
    {
      "epoch": 2.299084435401831,
      "grad_norm": 2.421847105026245,
      "learning_rate": 4.770091556459817e-05,
      "loss": 3.7891,
      "step": 4520
    },
    {
      "epoch": 2.304170905391658,
      "grad_norm": 3.777848958969116,
      "learning_rate": 4.7695829094608347e-05,
      "loss": 3.7576,
      "step": 4530
    },
    {
      "epoch": 2.309257375381485,
      "grad_norm": 4.143126487731934,
      "learning_rate": 4.7690742624618516e-05,
      "loss": 3.7131,
      "step": 4540
    },
    {
      "epoch": 2.3143438453713125,
      "grad_norm": 6.256880760192871,
      "learning_rate": 4.7685656154628686e-05,
      "loss": 3.6934,
      "step": 4550
    },
    {
      "epoch": 2.3194303153611395,
      "grad_norm": 4.804635524749756,
      "learning_rate": 4.768056968463886e-05,
      "loss": 3.716,
      "step": 4560
    },
    {
      "epoch": 2.3245167853509665,
      "grad_norm": 2.7259509563446045,
      "learning_rate": 4.767548321464903e-05,
      "loss": 3.641,
      "step": 4570
    },
    {
      "epoch": 2.3296032553407935,
      "grad_norm": 5.308385848999023,
      "learning_rate": 4.767039674465921e-05,
      "loss": 3.655,
      "step": 4580
    },
    {
      "epoch": 2.3346897253306205,
      "grad_norm": 3.2066640853881836,
      "learning_rate": 4.7665310274669386e-05,
      "loss": 3.8049,
      "step": 4590
    },
    {
      "epoch": 2.3397761953204474,
      "grad_norm": 3.672332286834717,
      "learning_rate": 4.7660223804679556e-05,
      "loss": 3.7147,
      "step": 4600
    },
    {
      "epoch": 2.3448626653102744,
      "grad_norm": 3.9188222885131836,
      "learning_rate": 4.7655137334689726e-05,
      "loss": 3.795,
      "step": 4610
    },
    {
      "epoch": 2.349949135300102,
      "grad_norm": 3.4528794288635254,
      "learning_rate": 4.76500508646999e-05,
      "loss": 3.715,
      "step": 4620
    },
    {
      "epoch": 2.355035605289929,
      "grad_norm": 4.446424961090088,
      "learning_rate": 4.764496439471007e-05,
      "loss": 3.6899,
      "step": 4630
    },
    {
      "epoch": 2.360122075279756,
      "grad_norm": 3.785942554473877,
      "learning_rate": 4.763987792472024e-05,
      "loss": 3.7736,
      "step": 4640
    },
    {
      "epoch": 2.365208545269583,
      "grad_norm": 5.180270195007324,
      "learning_rate": 4.763479145473042e-05,
      "loss": 3.6831,
      "step": 4650
    },
    {
      "epoch": 2.3702950152594098,
      "grad_norm": 4.773013114929199,
      "learning_rate": 4.762970498474059e-05,
      "loss": 3.8022,
      "step": 4660
    },
    {
      "epoch": 2.375381485249237,
      "grad_norm": 4.565668106079102,
      "learning_rate": 4.7624618514750766e-05,
      "loss": 3.7295,
      "step": 4670
    },
    {
      "epoch": 2.380467955239064,
      "grad_norm": 3.4002201557159424,
      "learning_rate": 4.761953204476094e-05,
      "loss": 3.7945,
      "step": 4680
    },
    {
      "epoch": 2.385554425228891,
      "grad_norm": 3.350419521331787,
      "learning_rate": 4.761444557477111e-05,
      "loss": 3.7091,
      "step": 4690
    },
    {
      "epoch": 2.390640895218718,
      "grad_norm": 4.265203475952148,
      "learning_rate": 4.760935910478128e-05,
      "loss": 3.7394,
      "step": 4700
    },
    {
      "epoch": 2.395727365208545,
      "grad_norm": 4.637005805969238,
      "learning_rate": 4.760427263479146e-05,
      "loss": 3.7654,
      "step": 4710
    },
    {
      "epoch": 2.400813835198372,
      "grad_norm": 3.4709601402282715,
      "learning_rate": 4.759918616480163e-05,
      "loss": 3.7143,
      "step": 4720
    },
    {
      "epoch": 2.4059003051881995,
      "grad_norm": 3.3729593753814697,
      "learning_rate": 4.7594099694811805e-05,
      "loss": 3.751,
      "step": 4730
    },
    {
      "epoch": 2.4109867751780265,
      "grad_norm": 4.194859027862549,
      "learning_rate": 4.7589013224821975e-05,
      "loss": 3.7014,
      "step": 4740
    },
    {
      "epoch": 2.4160732451678535,
      "grad_norm": 5.186005592346191,
      "learning_rate": 4.7583926754832145e-05,
      "loss": 3.7729,
      "step": 4750
    },
    {
      "epoch": 2.4211597151576805,
      "grad_norm": 3.5192174911499023,
      "learning_rate": 4.757884028484232e-05,
      "loss": 3.7645,
      "step": 4760
    },
    {
      "epoch": 2.4262461851475075,
      "grad_norm": 3.952028751373291,
      "learning_rate": 4.75737538148525e-05,
      "loss": 3.7307,
      "step": 4770
    },
    {
      "epoch": 2.431332655137335,
      "grad_norm": 4.758773326873779,
      "learning_rate": 4.756866734486267e-05,
      "loss": 3.7452,
      "step": 4780
    },
    {
      "epoch": 2.436419125127162,
      "grad_norm": 4.455031871795654,
      "learning_rate": 4.7563580874872845e-05,
      "loss": 3.7642,
      "step": 4790
    },
    {
      "epoch": 2.441505595116989,
      "grad_norm": 2.650963544845581,
      "learning_rate": 4.7558494404883015e-05,
      "loss": 3.8022,
      "step": 4800
    },
    {
      "epoch": 2.446592065106816,
      "grad_norm": 4.416297912597656,
      "learning_rate": 4.7553407934893185e-05,
      "loss": 3.7897,
      "step": 4810
    },
    {
      "epoch": 2.451678535096643,
      "grad_norm": 3.3899331092834473,
      "learning_rate": 4.754832146490336e-05,
      "loss": 3.7809,
      "step": 4820
    },
    {
      "epoch": 2.4567650050864698,
      "grad_norm": 4.730326175689697,
      "learning_rate": 4.754323499491353e-05,
      "loss": 3.6782,
      "step": 4830
    },
    {
      "epoch": 2.461851475076297,
      "grad_norm": 3.9171133041381836,
      "learning_rate": 4.75381485249237e-05,
      "loss": 3.664,
      "step": 4840
    },
    {
      "epoch": 2.466937945066124,
      "grad_norm": 3.900848627090454,
      "learning_rate": 4.753306205493388e-05,
      "loss": 3.6963,
      "step": 4850
    },
    {
      "epoch": 2.472024415055951,
      "grad_norm": 3.6779797077178955,
      "learning_rate": 4.752797558494405e-05,
      "loss": 3.7904,
      "step": 4860
    },
    {
      "epoch": 2.477110885045778,
      "grad_norm": 5.714904308319092,
      "learning_rate": 4.7522889114954225e-05,
      "loss": 3.7113,
      "step": 4870
    },
    {
      "epoch": 2.482197355035605,
      "grad_norm": 4.858906269073486,
      "learning_rate": 4.75178026449644e-05,
      "loss": 3.669,
      "step": 4880
    },
    {
      "epoch": 2.4872838250254325,
      "grad_norm": 3.5522751808166504,
      "learning_rate": 4.751271617497457e-05,
      "loss": 3.6697,
      "step": 4890
    },
    {
      "epoch": 2.4923702950152595,
      "grad_norm": 3.6095802783966064,
      "learning_rate": 4.750762970498474e-05,
      "loss": 3.7363,
      "step": 4900
    },
    {
      "epoch": 2.4974567650050865,
      "grad_norm": 5.125812530517578,
      "learning_rate": 4.750254323499492e-05,
      "loss": 3.7243,
      "step": 4910
    },
    {
      "epoch": 2.5025432349949135,
      "grad_norm": 5.69280481338501,
      "learning_rate": 4.749745676500509e-05,
      "loss": 3.7737,
      "step": 4920
    },
    {
      "epoch": 2.5076297049847405,
      "grad_norm": 2.7839348316192627,
      "learning_rate": 4.749237029501526e-05,
      "loss": 3.7156,
      "step": 4930
    },
    {
      "epoch": 2.5127161749745675,
      "grad_norm": 5.089852809906006,
      "learning_rate": 4.7487283825025434e-05,
      "loss": 3.7611,
      "step": 4940
    },
    {
      "epoch": 2.517802644964395,
      "grad_norm": 5.812798500061035,
      "learning_rate": 4.7482197355035604e-05,
      "loss": 3.7123,
      "step": 4950
    },
    {
      "epoch": 2.522889114954222,
      "grad_norm": 3.87550687789917,
      "learning_rate": 4.747711088504578e-05,
      "loss": 3.778,
      "step": 4960
    },
    {
      "epoch": 2.527975584944049,
      "grad_norm": 4.252837657928467,
      "learning_rate": 4.747202441505596e-05,
      "loss": 3.769,
      "step": 4970
    },
    {
      "epoch": 2.533062054933876,
      "grad_norm": 3.3828322887420654,
      "learning_rate": 4.746693794506613e-05,
      "loss": 3.7077,
      "step": 4980
    },
    {
      "epoch": 2.538148524923703,
      "grad_norm": 4.0107831954956055,
      "learning_rate": 4.74618514750763e-05,
      "loss": 3.6857,
      "step": 4990
    },
    {
      "epoch": 2.5432349949135302,
      "grad_norm": 5.351294040679932,
      "learning_rate": 4.7456765005086474e-05,
      "loss": 3.6802,
      "step": 5000
    },
    {
      "epoch": 2.548321464903357,
      "grad_norm": 4.689877033233643,
      "learning_rate": 4.7451678535096644e-05,
      "loss": 3.6626,
      "step": 5010
    },
    {
      "epoch": 2.553407934893184,
      "grad_norm": 3.8211734294891357,
      "learning_rate": 4.744659206510682e-05,
      "loss": 3.6476,
      "step": 5020
    },
    {
      "epoch": 2.558494404883011,
      "grad_norm": 4.359163761138916,
      "learning_rate": 4.744150559511699e-05,
      "loss": 3.7347,
      "step": 5030
    },
    {
      "epoch": 2.563580874872838,
      "grad_norm": 3.2256479263305664,
      "learning_rate": 4.743641912512716e-05,
      "loss": 3.6519,
      "step": 5040
    },
    {
      "epoch": 2.568667344862665,
      "grad_norm": 3.401954174041748,
      "learning_rate": 4.743133265513734e-05,
      "loss": 3.6615,
      "step": 5050
    },
    {
      "epoch": 2.573753814852492,
      "grad_norm": 3.7059149742126465,
      "learning_rate": 4.7426246185147514e-05,
      "loss": 3.7818,
      "step": 5060
    },
    {
      "epoch": 2.5788402848423195,
      "grad_norm": 5.809062957763672,
      "learning_rate": 4.7421159715157683e-05,
      "loss": 3.7481,
      "step": 5070
    },
    {
      "epoch": 2.5839267548321465,
      "grad_norm": 5.7823052406311035,
      "learning_rate": 4.741607324516786e-05,
      "loss": 3.6964,
      "step": 5080
    },
    {
      "epoch": 2.5890132248219735,
      "grad_norm": 3.965182304382324,
      "learning_rate": 4.741098677517803e-05,
      "loss": 3.6505,
      "step": 5090
    },
    {
      "epoch": 2.5940996948118005,
      "grad_norm": 3.550030469894409,
      "learning_rate": 4.74059003051882e-05,
      "loss": 3.7102,
      "step": 5100
    },
    {
      "epoch": 2.599186164801628,
      "grad_norm": 2.77231764793396,
      "learning_rate": 4.7400813835198377e-05,
      "loss": 3.6702,
      "step": 5110
    },
    {
      "epoch": 2.604272634791455,
      "grad_norm": 5.389247894287109,
      "learning_rate": 4.7395727365208546e-05,
      "loss": 3.673,
      "step": 5120
    },
    {
      "epoch": 2.609359104781282,
      "grad_norm": 4.422870635986328,
      "learning_rate": 4.7390640895218716e-05,
      "loss": 3.6684,
      "step": 5130
    },
    {
      "epoch": 2.614445574771109,
      "grad_norm": 5.599137306213379,
      "learning_rate": 4.738555442522889e-05,
      "loss": 3.7615,
      "step": 5140
    },
    {
      "epoch": 2.619532044760936,
      "grad_norm": 5.155229091644287,
      "learning_rate": 4.738046795523906e-05,
      "loss": 3.7898,
      "step": 5150
    },
    {
      "epoch": 2.624618514750763,
      "grad_norm": 4.242743492126465,
      "learning_rate": 4.737538148524924e-05,
      "loss": 3.7476,
      "step": 5160
    },
    {
      "epoch": 2.62970498474059,
      "grad_norm": 4.456991672515869,
      "learning_rate": 4.7370295015259416e-05,
      "loss": 3.6668,
      "step": 5170
    },
    {
      "epoch": 2.634791454730417,
      "grad_norm": 4.164248943328857,
      "learning_rate": 4.7365208545269586e-05,
      "loss": 3.7105,
      "step": 5180
    },
    {
      "epoch": 2.639877924720244,
      "grad_norm": 4.192049026489258,
      "learning_rate": 4.7360122075279756e-05,
      "loss": 3.6862,
      "step": 5190
    },
    {
      "epoch": 2.644964394710071,
      "grad_norm": 4.411014556884766,
      "learning_rate": 4.735503560528993e-05,
      "loss": 3.6542,
      "step": 5200
    },
    {
      "epoch": 2.650050864699898,
      "grad_norm": 6.183542728424072,
      "learning_rate": 4.73499491353001e-05,
      "loss": 3.7023,
      "step": 5210
    },
    {
      "epoch": 2.6551373346897256,
      "grad_norm": 5.223053455352783,
      "learning_rate": 4.734486266531027e-05,
      "loss": 3.6974,
      "step": 5220
    },
    {
      "epoch": 2.6602238046795526,
      "grad_norm": 6.425203800201416,
      "learning_rate": 4.733977619532045e-05,
      "loss": 3.6601,
      "step": 5230
    },
    {
      "epoch": 2.6653102746693795,
      "grad_norm": 3.5007314682006836,
      "learning_rate": 4.733468972533062e-05,
      "loss": 3.6727,
      "step": 5240
    },
    {
      "epoch": 2.6703967446592065,
      "grad_norm": 4.957117557525635,
      "learning_rate": 4.7329603255340796e-05,
      "loss": 3.693,
      "step": 5250
    },
    {
      "epoch": 2.6754832146490335,
      "grad_norm": 4.623505115509033,
      "learning_rate": 4.732451678535097e-05,
      "loss": 3.7079,
      "step": 5260
    },
    {
      "epoch": 2.6805696846388605,
      "grad_norm": 3.95219087600708,
      "learning_rate": 4.731943031536114e-05,
      "loss": 3.6843,
      "step": 5270
    },
    {
      "epoch": 2.6856561546286875,
      "grad_norm": 7.213279724121094,
      "learning_rate": 4.731434384537132e-05,
      "loss": 3.6372,
      "step": 5280
    },
    {
      "epoch": 2.690742624618515,
      "grad_norm": 4.484387397766113,
      "learning_rate": 4.730925737538149e-05,
      "loss": 3.6865,
      "step": 5290
    },
    {
      "epoch": 2.695829094608342,
      "grad_norm": 4.676220893859863,
      "learning_rate": 4.730417090539166e-05,
      "loss": 3.7287,
      "step": 5300
    },
    {
      "epoch": 2.700915564598169,
      "grad_norm": 5.956389427185059,
      "learning_rate": 4.7299084435401835e-05,
      "loss": 3.6351,
      "step": 5310
    },
    {
      "epoch": 2.706002034587996,
      "grad_norm": 3.696568489074707,
      "learning_rate": 4.7293997965412005e-05,
      "loss": 3.7946,
      "step": 5320
    },
    {
      "epoch": 2.7110885045778232,
      "grad_norm": 4.7207136154174805,
      "learning_rate": 4.7288911495422175e-05,
      "loss": 3.6459,
      "step": 5330
    },
    {
      "epoch": 2.7161749745676502,
      "grad_norm": 4.2314558029174805,
      "learning_rate": 4.728382502543235e-05,
      "loss": 3.691,
      "step": 5340
    },
    {
      "epoch": 2.721261444557477,
      "grad_norm": 3.279736042022705,
      "learning_rate": 4.727873855544253e-05,
      "loss": 3.7023,
      "step": 5350
    },
    {
      "epoch": 2.726347914547304,
      "grad_norm": 3.727085828781128,
      "learning_rate": 4.72736520854527e-05,
      "loss": 3.6622,
      "step": 5360
    },
    {
      "epoch": 2.731434384537131,
      "grad_norm": 5.499291896820068,
      "learning_rate": 4.7268565615462875e-05,
      "loss": 3.6241,
      "step": 5370
    },
    {
      "epoch": 2.736520854526958,
      "grad_norm": 3.705845355987549,
      "learning_rate": 4.7263479145473045e-05,
      "loss": 3.6156,
      "step": 5380
    },
    {
      "epoch": 2.741607324516785,
      "grad_norm": 3.802474021911621,
      "learning_rate": 4.7258392675483215e-05,
      "loss": 3.6701,
      "step": 5390
    },
    {
      "epoch": 2.7466937945066126,
      "grad_norm": 4.997459411621094,
      "learning_rate": 4.725330620549339e-05,
      "loss": 3.6839,
      "step": 5400
    },
    {
      "epoch": 2.7517802644964395,
      "grad_norm": 5.709918975830078,
      "learning_rate": 4.724821973550356e-05,
      "loss": 3.6577,
      "step": 5410
    },
    {
      "epoch": 2.7568667344862665,
      "grad_norm": 4.70246696472168,
      "learning_rate": 4.724313326551373e-05,
      "loss": 3.7086,
      "step": 5420
    },
    {
      "epoch": 2.7619532044760935,
      "grad_norm": 3.8968029022216797,
      "learning_rate": 4.723804679552391e-05,
      "loss": 3.7816,
      "step": 5430
    },
    {
      "epoch": 2.767039674465921,
      "grad_norm": 4.845094203948975,
      "learning_rate": 4.7232960325534085e-05,
      "loss": 3.7089,
      "step": 5440
    },
    {
      "epoch": 2.772126144455748,
      "grad_norm": 4.331329345703125,
      "learning_rate": 4.7227873855544255e-05,
      "loss": 3.7702,
      "step": 5450
    },
    {
      "epoch": 2.777212614445575,
      "grad_norm": 4.5414042472839355,
      "learning_rate": 4.722278738555443e-05,
      "loss": 3.7594,
      "step": 5460
    },
    {
      "epoch": 2.782299084435402,
      "grad_norm": 3.7642195224761963,
      "learning_rate": 4.72177009155646e-05,
      "loss": 3.7254,
      "step": 5470
    },
    {
      "epoch": 2.787385554425229,
      "grad_norm": 5.4843292236328125,
      "learning_rate": 4.721261444557477e-05,
      "loss": 3.62,
      "step": 5480
    },
    {
      "epoch": 2.792472024415056,
      "grad_norm": 5.650673866271973,
      "learning_rate": 4.720752797558495e-05,
      "loss": 3.6561,
      "step": 5490
    },
    {
      "epoch": 2.797558494404883,
      "grad_norm": 3.9296176433563232,
      "learning_rate": 4.720244150559512e-05,
      "loss": 3.6567,
      "step": 5500
    },
    {
      "epoch": 2.8026449643947102,
      "grad_norm": 5.057713985443115,
      "learning_rate": 4.719735503560529e-05,
      "loss": 3.6691,
      "step": 5510
    },
    {
      "epoch": 2.807731434384537,
      "grad_norm": 5.678650856018066,
      "learning_rate": 4.7192268565615464e-05,
      "loss": 3.6463,
      "step": 5520
    },
    {
      "epoch": 2.812817904374364,
      "grad_norm": 5.302112102508545,
      "learning_rate": 4.7187182095625634e-05,
      "loss": 3.6625,
      "step": 5530
    },
    {
      "epoch": 2.817904374364191,
      "grad_norm": 4.528802871704102,
      "learning_rate": 4.718209562563581e-05,
      "loss": 3.66,
      "step": 5540
    },
    {
      "epoch": 2.822990844354018,
      "grad_norm": 5.758062839508057,
      "learning_rate": 4.717700915564599e-05,
      "loss": 3.6774,
      "step": 5550
    },
    {
      "epoch": 2.8280773143438456,
      "grad_norm": 3.8056657314300537,
      "learning_rate": 4.717192268565616e-05,
      "loss": 3.656,
      "step": 5560
    },
    {
      "epoch": 2.8331637843336726,
      "grad_norm": 4.489880561828613,
      "learning_rate": 4.7166836215666334e-05,
      "loss": 3.6592,
      "step": 5570
    },
    {
      "epoch": 2.8382502543234995,
      "grad_norm": 5.6020355224609375,
      "learning_rate": 4.7161749745676504e-05,
      "loss": 3.6587,
      "step": 5580
    },
    {
      "epoch": 2.8433367243133265,
      "grad_norm": 4.388919830322266,
      "learning_rate": 4.7156663275686674e-05,
      "loss": 3.6172,
      "step": 5590
    },
    {
      "epoch": 2.8484231943031535,
      "grad_norm": 5.279253005981445,
      "learning_rate": 4.715157680569685e-05,
      "loss": 3.6641,
      "step": 5600
    },
    {
      "epoch": 2.8535096642929805,
      "grad_norm": 4.987951755523682,
      "learning_rate": 4.714649033570702e-05,
      "loss": 3.6629,
      "step": 5610
    },
    {
      "epoch": 2.8585961342828075,
      "grad_norm": 3.9382975101470947,
      "learning_rate": 4.714140386571719e-05,
      "loss": 3.6729,
      "step": 5620
    },
    {
      "epoch": 2.863682604272635,
      "grad_norm": 4.747213840484619,
      "learning_rate": 4.713631739572737e-05,
      "loss": 3.6258,
      "step": 5630
    },
    {
      "epoch": 2.868769074262462,
      "grad_norm": 5.04433012008667,
      "learning_rate": 4.7131230925737544e-05,
      "loss": 3.6859,
      "step": 5640
    },
    {
      "epoch": 2.873855544252289,
      "grad_norm": 4.856659412384033,
      "learning_rate": 4.7126144455747713e-05,
      "loss": 3.6226,
      "step": 5650
    },
    {
      "epoch": 2.878942014242116,
      "grad_norm": 4.189613342285156,
      "learning_rate": 4.712105798575789e-05,
      "loss": 3.6887,
      "step": 5660
    },
    {
      "epoch": 2.8840284842319432,
      "grad_norm": 4.049076557159424,
      "learning_rate": 4.711597151576806e-05,
      "loss": 3.6765,
      "step": 5670
    },
    {
      "epoch": 2.8891149542217702,
      "grad_norm": 3.7918357849121094,
      "learning_rate": 4.711088504577823e-05,
      "loss": 3.6691,
      "step": 5680
    },
    {
      "epoch": 2.894201424211597,
      "grad_norm": 4.2135162353515625,
      "learning_rate": 4.7105798575788407e-05,
      "loss": 3.6714,
      "step": 5690
    },
    {
      "epoch": 2.899287894201424,
      "grad_norm": 5.489110946655273,
      "learning_rate": 4.7100712105798576e-05,
      "loss": 3.6798,
      "step": 5700
    },
    {
      "epoch": 2.904374364191251,
      "grad_norm": 5.34439754486084,
      "learning_rate": 4.7095625635808746e-05,
      "loss": 3.6785,
      "step": 5710
    },
    {
      "epoch": 2.909460834181078,
      "grad_norm": 4.0993332862854,
      "learning_rate": 4.709053916581892e-05,
      "loss": 3.6565,
      "step": 5720
    },
    {
      "epoch": 2.914547304170905,
      "grad_norm": 4.631868839263916,
      "learning_rate": 4.70854526958291e-05,
      "loss": 3.7402,
      "step": 5730
    },
    {
      "epoch": 2.9196337741607326,
      "grad_norm": 3.4633841514587402,
      "learning_rate": 4.708036622583927e-05,
      "loss": 3.6674,
      "step": 5740
    },
    {
      "epoch": 2.9247202441505595,
      "grad_norm": 5.54067325592041,
      "learning_rate": 4.7075279755849446e-05,
      "loss": 3.6828,
      "step": 5750
    },
    {
      "epoch": 2.9298067141403865,
      "grad_norm": 3.4729666709899902,
      "learning_rate": 4.7070193285859616e-05,
      "loss": 3.6672,
      "step": 5760
    },
    {
      "epoch": 2.9348931841302135,
      "grad_norm": 4.912230491638184,
      "learning_rate": 4.7065106815869786e-05,
      "loss": 3.6515,
      "step": 5770
    },
    {
      "epoch": 2.939979654120041,
      "grad_norm": 3.7959399223327637,
      "learning_rate": 4.706002034587996e-05,
      "loss": 3.7472,
      "step": 5780
    },
    {
      "epoch": 2.945066124109868,
      "grad_norm": 4.85549259185791,
      "learning_rate": 4.705493387589013e-05,
      "loss": 3.6709,
      "step": 5790
    },
    {
      "epoch": 2.950152594099695,
      "grad_norm": 4.189378261566162,
      "learning_rate": 4.70498474059003e-05,
      "loss": 3.6489,
      "step": 5800
    },
    {
      "epoch": 2.955239064089522,
      "grad_norm": 3.500896453857422,
      "learning_rate": 4.704476093591048e-05,
      "loss": 3.615,
      "step": 5810
    },
    {
      "epoch": 2.960325534079349,
      "grad_norm": 4.711471080780029,
      "learning_rate": 4.703967446592065e-05,
      "loss": 3.6782,
      "step": 5820
    },
    {
      "epoch": 2.965412004069176,
      "grad_norm": 7.09427547454834,
      "learning_rate": 4.7034587995930826e-05,
      "loss": 3.6456,
      "step": 5830
    },
    {
      "epoch": 2.970498474059003,
      "grad_norm": 3.952336311340332,
      "learning_rate": 4.7029501525941e-05,
      "loss": 3.6552,
      "step": 5840
    },
    {
      "epoch": 2.9755849440488302,
      "grad_norm": 4.944598197937012,
      "learning_rate": 4.702441505595117e-05,
      "loss": 3.5345,
      "step": 5850
    },
    {
      "epoch": 2.980671414038657,
      "grad_norm": 6.228686332702637,
      "learning_rate": 4.701932858596135e-05,
      "loss": 3.5936,
      "step": 5860
    },
    {
      "epoch": 2.985757884028484,
      "grad_norm": 4.323083400726318,
      "learning_rate": 4.701424211597152e-05,
      "loss": 3.5843,
      "step": 5870
    },
    {
      "epoch": 2.990844354018311,
      "grad_norm": 5.371728897094727,
      "learning_rate": 4.700915564598169e-05,
      "loss": 3.7016,
      "step": 5880
    },
    {
      "epoch": 2.9959308240081386,
      "grad_norm": 4.7863569259643555,
      "learning_rate": 4.7004069175991865e-05,
      "loss": 3.6439,
      "step": 5890
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.7252442836761475,
      "eval_runtime": 2.8692,
      "eval_samples_per_second": 967.165,
      "eval_steps_per_second": 120.939,
      "step": 5898
    },
    {
      "epoch": 3.0010172939979656,
      "grad_norm": 7.583024501800537,
      "learning_rate": 4.6998982706002035e-05,
      "loss": 3.6023,
      "step": 5900
    },
    {
      "epoch": 3.0061037639877926,
      "grad_norm": 4.859519958496094,
      "learning_rate": 4.6993896236012205e-05,
      "loss": 3.5988,
      "step": 5910
    },
    {
      "epoch": 3.0111902339776195,
      "grad_norm": 3.942944288253784,
      "learning_rate": 4.698880976602238e-05,
      "loss": 3.6202,
      "step": 5920
    },
    {
      "epoch": 3.0162767039674465,
      "grad_norm": 4.281228542327881,
      "learning_rate": 4.698372329603256e-05,
      "loss": 3.6121,
      "step": 5930
    },
    {
      "epoch": 3.0213631739572735,
      "grad_norm": 3.435305595397949,
      "learning_rate": 4.697863682604273e-05,
      "loss": 3.6494,
      "step": 5940
    },
    {
      "epoch": 3.026449643947101,
      "grad_norm": 3.6954398155212402,
      "learning_rate": 4.6973550356052905e-05,
      "loss": 3.6734,
      "step": 5950
    },
    {
      "epoch": 3.031536113936928,
      "grad_norm": 4.779880046844482,
      "learning_rate": 4.6968463886063075e-05,
      "loss": 3.6766,
      "step": 5960
    },
    {
      "epoch": 3.036622583926755,
      "grad_norm": 4.524289131164551,
      "learning_rate": 4.6963377416073245e-05,
      "loss": 3.6474,
      "step": 5970
    },
    {
      "epoch": 3.041709053916582,
      "grad_norm": 5.124083995819092,
      "learning_rate": 4.695829094608342e-05,
      "loss": 3.5596,
      "step": 5980
    },
    {
      "epoch": 3.046795523906409,
      "grad_norm": 4.635921955108643,
      "learning_rate": 4.695320447609359e-05,
      "loss": 3.6808,
      "step": 5990
    },
    {
      "epoch": 3.051881993896236,
      "grad_norm": 4.853470325469971,
      "learning_rate": 4.694811800610376e-05,
      "loss": 3.5641,
      "step": 6000
    },
    {
      "epoch": 3.0569684638860632,
      "grad_norm": 4.713903427124023,
      "learning_rate": 4.694303153611394e-05,
      "loss": 3.6332,
      "step": 6010
    },
    {
      "epoch": 3.0620549338758902,
      "grad_norm": 5.001492023468018,
      "learning_rate": 4.6937945066124115e-05,
      "loss": 3.5625,
      "step": 6020
    },
    {
      "epoch": 3.067141403865717,
      "grad_norm": 6.610650539398193,
      "learning_rate": 4.6932858596134285e-05,
      "loss": 3.6483,
      "step": 6030
    },
    {
      "epoch": 3.072227873855544,
      "grad_norm": 4.805732250213623,
      "learning_rate": 4.692777212614446e-05,
      "loss": 3.604,
      "step": 6040
    },
    {
      "epoch": 3.077314343845371,
      "grad_norm": 6.504794597625732,
      "learning_rate": 4.692268565615463e-05,
      "loss": 3.6102,
      "step": 6050
    },
    {
      "epoch": 3.082400813835198,
      "grad_norm": 9.153931617736816,
      "learning_rate": 4.69175991861648e-05,
      "loss": 3.5449,
      "step": 6060
    },
    {
      "epoch": 3.0874872838250256,
      "grad_norm": 6.519296169281006,
      "learning_rate": 4.691251271617498e-05,
      "loss": 3.6389,
      "step": 6070
    },
    {
      "epoch": 3.0925737538148526,
      "grad_norm": 4.770105361938477,
      "learning_rate": 4.690742624618515e-05,
      "loss": 3.6886,
      "step": 6080
    },
    {
      "epoch": 3.0976602238046795,
      "grad_norm": 5.810426235198975,
      "learning_rate": 4.6902339776195324e-05,
      "loss": 3.6273,
      "step": 6090
    },
    {
      "epoch": 3.1027466937945065,
      "grad_norm": 4.306407451629639,
      "learning_rate": 4.6897253306205494e-05,
      "loss": 3.6181,
      "step": 6100
    },
    {
      "epoch": 3.1078331637843335,
      "grad_norm": 5.578598499298096,
      "learning_rate": 4.6892166836215664e-05,
      "loss": 3.6889,
      "step": 6110
    },
    {
      "epoch": 3.112919633774161,
      "grad_norm": 5.577609539031982,
      "learning_rate": 4.688708036622584e-05,
      "loss": 3.6188,
      "step": 6120
    },
    {
      "epoch": 3.118006103763988,
      "grad_norm": 4.331940174102783,
      "learning_rate": 4.688199389623602e-05,
      "loss": 3.5675,
      "step": 6130
    },
    {
      "epoch": 3.123092573753815,
      "grad_norm": 5.435168743133545,
      "learning_rate": 4.687690742624619e-05,
      "loss": 3.5775,
      "step": 6140
    },
    {
      "epoch": 3.128179043743642,
      "grad_norm": 4.688910484313965,
      "learning_rate": 4.6871820956256364e-05,
      "loss": 3.6808,
      "step": 6150
    },
    {
      "epoch": 3.133265513733469,
      "grad_norm": 5.937453269958496,
      "learning_rate": 4.6866734486266534e-05,
      "loss": 3.629,
      "step": 6160
    },
    {
      "epoch": 3.138351983723296,
      "grad_norm": 4.7829155921936035,
      "learning_rate": 4.6861648016276704e-05,
      "loss": 3.572,
      "step": 6170
    },
    {
      "epoch": 3.1434384537131232,
      "grad_norm": 4.767241954803467,
      "learning_rate": 4.685656154628688e-05,
      "loss": 3.6488,
      "step": 6180
    },
    {
      "epoch": 3.1485249237029502,
      "grad_norm": 4.491647243499756,
      "learning_rate": 4.685147507629705e-05,
      "loss": 3.6865,
      "step": 6190
    },
    {
      "epoch": 3.153611393692777,
      "grad_norm": 7.464101791381836,
      "learning_rate": 4.684638860630722e-05,
      "loss": 3.6062,
      "step": 6200
    },
    {
      "epoch": 3.158697863682604,
      "grad_norm": 4.401134967803955,
      "learning_rate": 4.68413021363174e-05,
      "loss": 3.6582,
      "step": 6210
    },
    {
      "epoch": 3.163784333672431,
      "grad_norm": 8.509295463562012,
      "learning_rate": 4.6836215666327574e-05,
      "loss": 3.6492,
      "step": 6220
    },
    {
      "epoch": 3.1688708036622586,
      "grad_norm": 5.979337215423584,
      "learning_rate": 4.6831129196337743e-05,
      "loss": 3.6675,
      "step": 6230
    },
    {
      "epoch": 3.1739572736520856,
      "grad_norm": 4.641373157501221,
      "learning_rate": 4.682604272634792e-05,
      "loss": 3.6731,
      "step": 6240
    },
    {
      "epoch": 3.1790437436419126,
      "grad_norm": 4.351202487945557,
      "learning_rate": 4.682095625635809e-05,
      "loss": 3.6228,
      "step": 6250
    },
    {
      "epoch": 3.1841302136317395,
      "grad_norm": 4.611045837402344,
      "learning_rate": 4.681586978636826e-05,
      "loss": 3.635,
      "step": 6260
    },
    {
      "epoch": 3.1892166836215665,
      "grad_norm": 3.8160417079925537,
      "learning_rate": 4.6810783316378437e-05,
      "loss": 3.6503,
      "step": 6270
    },
    {
      "epoch": 3.1943031536113935,
      "grad_norm": 6.671133518218994,
      "learning_rate": 4.6805696846388606e-05,
      "loss": 3.6312,
      "step": 6280
    },
    {
      "epoch": 3.199389623601221,
      "grad_norm": 4.400428771972656,
      "learning_rate": 4.6800610376398776e-05,
      "loss": 3.5836,
      "step": 6290
    },
    {
      "epoch": 3.204476093591048,
      "grad_norm": 6.711611270904541,
      "learning_rate": 4.679552390640895e-05,
      "loss": 3.6643,
      "step": 6300
    },
    {
      "epoch": 3.209562563580875,
      "grad_norm": 5.938518047332764,
      "learning_rate": 4.679043743641913e-05,
      "loss": 3.5879,
      "step": 6310
    },
    {
      "epoch": 3.214649033570702,
      "grad_norm": 5.224584102630615,
      "learning_rate": 4.67853509664293e-05,
      "loss": 3.6751,
      "step": 6320
    },
    {
      "epoch": 3.219735503560529,
      "grad_norm": 4.686142444610596,
      "learning_rate": 4.6780264496439476e-05,
      "loss": 3.5793,
      "step": 6330
    },
    {
      "epoch": 3.2248219735503563,
      "grad_norm": 5.041260719299316,
      "learning_rate": 4.6775178026449646e-05,
      "loss": 3.6131,
      "step": 6340
    },
    {
      "epoch": 3.2299084435401832,
      "grad_norm": 6.784351825714111,
      "learning_rate": 4.677009155645982e-05,
      "loss": 3.5302,
      "step": 6350
    },
    {
      "epoch": 3.2349949135300102,
      "grad_norm": 4.719609260559082,
      "learning_rate": 4.676500508646999e-05,
      "loss": 3.6024,
      "step": 6360
    },
    {
      "epoch": 3.240081383519837,
      "grad_norm": 4.837835788726807,
      "learning_rate": 4.675991861648016e-05,
      "loss": 3.6316,
      "step": 6370
    },
    {
      "epoch": 3.245167853509664,
      "grad_norm": 3.860846519470215,
      "learning_rate": 4.675483214649034e-05,
      "loss": 3.5562,
      "step": 6380
    },
    {
      "epoch": 3.250254323499491,
      "grad_norm": 4.719042778015137,
      "learning_rate": 4.674974567650051e-05,
      "loss": 3.6236,
      "step": 6390
    },
    {
      "epoch": 3.2553407934893186,
      "grad_norm": 5.3294572830200195,
      "learning_rate": 4.6744659206510686e-05,
      "loss": 3.5858,
      "step": 6400
    },
    {
      "epoch": 3.2604272634791456,
      "grad_norm": 4.688499450683594,
      "learning_rate": 4.673957273652086e-05,
      "loss": 3.6385,
      "step": 6410
    },
    {
      "epoch": 3.2655137334689726,
      "grad_norm": 5.017598628997803,
      "learning_rate": 4.673448626653103e-05,
      "loss": 3.5617,
      "step": 6420
    },
    {
      "epoch": 3.2706002034587995,
      "grad_norm": 4.725494384765625,
      "learning_rate": 4.67293997965412e-05,
      "loss": 3.5331,
      "step": 6430
    },
    {
      "epoch": 3.2756866734486265,
      "grad_norm": 4.814544200897217,
      "learning_rate": 4.672431332655138e-05,
      "loss": 3.6042,
      "step": 6440
    },
    {
      "epoch": 3.280773143438454,
      "grad_norm": 5.200475692749023,
      "learning_rate": 4.671922685656155e-05,
      "loss": 3.6681,
      "step": 6450
    },
    {
      "epoch": 3.285859613428281,
      "grad_norm": 6.304190635681152,
      "learning_rate": 4.671414038657172e-05,
      "loss": 3.6586,
      "step": 6460
    },
    {
      "epoch": 3.290946083418108,
      "grad_norm": 6.093272686004639,
      "learning_rate": 4.6709053916581895e-05,
      "loss": 3.5281,
      "step": 6470
    },
    {
      "epoch": 3.296032553407935,
      "grad_norm": 5.949427604675293,
      "learning_rate": 4.6703967446592065e-05,
      "loss": 3.6669,
      "step": 6480
    },
    {
      "epoch": 3.301119023397762,
      "grad_norm": 5.326529026031494,
      "learning_rate": 4.6698880976602235e-05,
      "loss": 3.5815,
      "step": 6490
    },
    {
      "epoch": 3.306205493387589,
      "grad_norm": 6.130629062652588,
      "learning_rate": 4.669379450661241e-05,
      "loss": 3.6712,
      "step": 6500
    },
    {
      "epoch": 3.311291963377416,
      "grad_norm": 7.084123134613037,
      "learning_rate": 4.668870803662259e-05,
      "loss": 3.6649,
      "step": 6510
    },
    {
      "epoch": 3.3163784333672433,
      "grad_norm": 4.31793737411499,
      "learning_rate": 4.668362156663276e-05,
      "loss": 3.6104,
      "step": 6520
    },
    {
      "epoch": 3.3214649033570702,
      "grad_norm": 6.770806789398193,
      "learning_rate": 4.6678535096642935e-05,
      "loss": 3.5508,
      "step": 6530
    },
    {
      "epoch": 3.326551373346897,
      "grad_norm": 7.9012956619262695,
      "learning_rate": 4.6673448626653105e-05,
      "loss": 3.5376,
      "step": 6540
    },
    {
      "epoch": 3.331637843336724,
      "grad_norm": 4.781438827514648,
      "learning_rate": 4.6668362156663275e-05,
      "loss": 3.5428,
      "step": 6550
    },
    {
      "epoch": 3.3367243133265516,
      "grad_norm": 4.8690314292907715,
      "learning_rate": 4.666327568667345e-05,
      "loss": 3.5954,
      "step": 6560
    },
    {
      "epoch": 3.3418107833163786,
      "grad_norm": 3.661860704421997,
      "learning_rate": 4.665818921668362e-05,
      "loss": 3.6351,
      "step": 6570
    },
    {
      "epoch": 3.3468972533062056,
      "grad_norm": 5.75223445892334,
      "learning_rate": 4.665310274669379e-05,
      "loss": 3.5385,
      "step": 6580
    },
    {
      "epoch": 3.3519837232960326,
      "grad_norm": 3.6822266578674316,
      "learning_rate": 4.664801627670397e-05,
      "loss": 3.6236,
      "step": 6590
    },
    {
      "epoch": 3.3570701932858595,
      "grad_norm": 7.664994239807129,
      "learning_rate": 4.6642929806714145e-05,
      "loss": 3.6336,
      "step": 6600
    },
    {
      "epoch": 3.3621566632756865,
      "grad_norm": 5.162015438079834,
      "learning_rate": 4.6637843336724315e-05,
      "loss": 3.5759,
      "step": 6610
    },
    {
      "epoch": 3.3672431332655135,
      "grad_norm": 5.551575660705566,
      "learning_rate": 4.663275686673449e-05,
      "loss": 3.6228,
      "step": 6620
    },
    {
      "epoch": 3.372329603255341,
      "grad_norm": 5.163883209228516,
      "learning_rate": 4.662767039674466e-05,
      "loss": 3.6341,
      "step": 6630
    },
    {
      "epoch": 3.377416073245168,
      "grad_norm": 6.469604969024658,
      "learning_rate": 4.662258392675484e-05,
      "loss": 3.5981,
      "step": 6640
    },
    {
      "epoch": 3.382502543234995,
      "grad_norm": 4.385519981384277,
      "learning_rate": 4.661749745676501e-05,
      "loss": 3.5927,
      "step": 6650
    },
    {
      "epoch": 3.387589013224822,
      "grad_norm": 5.208214282989502,
      "learning_rate": 4.661241098677518e-05,
      "loss": 3.5765,
      "step": 6660
    },
    {
      "epoch": 3.392675483214649,
      "grad_norm": 4.088645935058594,
      "learning_rate": 4.6607324516785354e-05,
      "loss": 3.5992,
      "step": 6670
    },
    {
      "epoch": 3.3977619532044763,
      "grad_norm": 5.166849613189697,
      "learning_rate": 4.6602238046795524e-05,
      "loss": 3.579,
      "step": 6680
    },
    {
      "epoch": 3.4028484231943033,
      "grad_norm": 6.5324506759643555,
      "learning_rate": 4.65971515768057e-05,
      "loss": 3.6316,
      "step": 6690
    },
    {
      "epoch": 3.4079348931841302,
      "grad_norm": 4.6760783195495605,
      "learning_rate": 4.659206510681588e-05,
      "loss": 3.6492,
      "step": 6700
    },
    {
      "epoch": 3.413021363173957,
      "grad_norm": 6.862030982971191,
      "learning_rate": 4.658697863682605e-05,
      "loss": 3.5725,
      "step": 6710
    },
    {
      "epoch": 3.418107833163784,
      "grad_norm": 5.874502658843994,
      "learning_rate": 4.658189216683622e-05,
      "loss": 3.5419,
      "step": 6720
    },
    {
      "epoch": 3.423194303153611,
      "grad_norm": 6.032994270324707,
      "learning_rate": 4.6576805696846394e-05,
      "loss": 3.6009,
      "step": 6730
    },
    {
      "epoch": 3.4282807731434386,
      "grad_norm": 4.199117183685303,
      "learning_rate": 4.6571719226856564e-05,
      "loss": 3.6195,
      "step": 6740
    },
    {
      "epoch": 3.4333672431332656,
      "grad_norm": 6.940876007080078,
      "learning_rate": 4.6566632756866734e-05,
      "loss": 3.6071,
      "step": 6750
    },
    {
      "epoch": 3.4384537131230926,
      "grad_norm": 6.1621012687683105,
      "learning_rate": 4.656154628687691e-05,
      "loss": 3.5854,
      "step": 6760
    },
    {
      "epoch": 3.4435401831129195,
      "grad_norm": 4.376574516296387,
      "learning_rate": 4.655645981688708e-05,
      "loss": 3.5615,
      "step": 6770
    },
    {
      "epoch": 3.4486266531027465,
      "grad_norm": 4.592933177947998,
      "learning_rate": 4.655137334689725e-05,
      "loss": 3.6003,
      "step": 6780
    },
    {
      "epoch": 3.453713123092574,
      "grad_norm": 4.879818916320801,
      "learning_rate": 4.654628687690743e-05,
      "loss": 3.6257,
      "step": 6790
    },
    {
      "epoch": 3.458799593082401,
      "grad_norm": 4.794998645782471,
      "learning_rate": 4.6541200406917604e-05,
      "loss": 3.5651,
      "step": 6800
    },
    {
      "epoch": 3.463886063072228,
      "grad_norm": 5.426793098449707,
      "learning_rate": 4.6536113936927773e-05,
      "loss": 3.6279,
      "step": 6810
    },
    {
      "epoch": 3.468972533062055,
      "grad_norm": 4.63775110244751,
      "learning_rate": 4.653102746693795e-05,
      "loss": 3.5838,
      "step": 6820
    },
    {
      "epoch": 3.474059003051882,
      "grad_norm": 6.2726240158081055,
      "learning_rate": 4.652594099694812e-05,
      "loss": 3.6179,
      "step": 6830
    },
    {
      "epoch": 3.479145473041709,
      "grad_norm": 4.428628921508789,
      "learning_rate": 4.652085452695829e-05,
      "loss": 3.5676,
      "step": 6840
    },
    {
      "epoch": 3.4842319430315363,
      "grad_norm": 4.96235466003418,
      "learning_rate": 4.6515768056968467e-05,
      "loss": 3.5855,
      "step": 6850
    },
    {
      "epoch": 3.4893184130213633,
      "grad_norm": 6.148820400238037,
      "learning_rate": 4.6510681586978636e-05,
      "loss": 3.5367,
      "step": 6860
    },
    {
      "epoch": 3.4944048830111902,
      "grad_norm": 7.199848175048828,
      "learning_rate": 4.6505595116988806e-05,
      "loss": 3.6036,
      "step": 6870
    },
    {
      "epoch": 3.499491353001017,
      "grad_norm": 6.699007987976074,
      "learning_rate": 4.650050864699898e-05,
      "loss": 3.6144,
      "step": 6880
    },
    {
      "epoch": 3.504577822990844,
      "grad_norm": 5.319265842437744,
      "learning_rate": 4.649542217700916e-05,
      "loss": 3.6197,
      "step": 6890
    },
    {
      "epoch": 3.5096642929806716,
      "grad_norm": 6.041663646697998,
      "learning_rate": 4.6490335707019336e-05,
      "loss": 3.503,
      "step": 6900
    },
    {
      "epoch": 3.5147507629704986,
      "grad_norm": 5.700667381286621,
      "learning_rate": 4.6485249237029506e-05,
      "loss": 3.5554,
      "step": 6910
    },
    {
      "epoch": 3.5198372329603256,
      "grad_norm": 5.902184009552002,
      "learning_rate": 4.6480162767039676e-05,
      "loss": 3.5574,
      "step": 6920
    },
    {
      "epoch": 3.5249237029501526,
      "grad_norm": 4.570537567138672,
      "learning_rate": 4.647507629704985e-05,
      "loss": 3.5995,
      "step": 6930
    },
    {
      "epoch": 3.5300101729399795,
      "grad_norm": 5.666985511779785,
      "learning_rate": 4.646998982706002e-05,
      "loss": 3.4969,
      "step": 6940
    },
    {
      "epoch": 3.5350966429298065,
      "grad_norm": 7.348017692565918,
      "learning_rate": 4.646490335707019e-05,
      "loss": 3.6009,
      "step": 6950
    },
    {
      "epoch": 3.5401831129196335,
      "grad_norm": 5.705157279968262,
      "learning_rate": 4.645981688708037e-05,
      "loss": 3.6524,
      "step": 6960
    },
    {
      "epoch": 3.545269582909461,
      "grad_norm": 6.622408866882324,
      "learning_rate": 4.645473041709054e-05,
      "loss": 3.6017,
      "step": 6970
    },
    {
      "epoch": 3.550356052899288,
      "grad_norm": 7.1577911376953125,
      "learning_rate": 4.6449643947100716e-05,
      "loss": 3.5671,
      "step": 6980
    },
    {
      "epoch": 3.555442522889115,
      "grad_norm": 7.3192901611328125,
      "learning_rate": 4.644455747711089e-05,
      "loss": 3.5647,
      "step": 6990
    },
    {
      "epoch": 3.560528992878942,
      "grad_norm": 5.097104072570801,
      "learning_rate": 4.643947100712106e-05,
      "loss": 3.4403,
      "step": 7000
    },
    {
      "epoch": 3.5656154628687693,
      "grad_norm": 5.256007671356201,
      "learning_rate": 4.643438453713123e-05,
      "loss": 3.5134,
      "step": 7010
    },
    {
      "epoch": 3.5707019328585963,
      "grad_norm": 8.457737922668457,
      "learning_rate": 4.642929806714141e-05,
      "loss": 3.5688,
      "step": 7020
    },
    {
      "epoch": 3.5757884028484233,
      "grad_norm": 6.196751117706299,
      "learning_rate": 4.642421159715158e-05,
      "loss": 3.5507,
      "step": 7030
    },
    {
      "epoch": 3.5808748728382502,
      "grad_norm": 7.598966121673584,
      "learning_rate": 4.641912512716175e-05,
      "loss": 3.5545,
      "step": 7040
    },
    {
      "epoch": 3.585961342828077,
      "grad_norm": 5.845785140991211,
      "learning_rate": 4.6414038657171925e-05,
      "loss": 3.5091,
      "step": 7050
    },
    {
      "epoch": 3.591047812817904,
      "grad_norm": 6.829281330108643,
      "learning_rate": 4.6408952187182095e-05,
      "loss": 3.5377,
      "step": 7060
    },
    {
      "epoch": 3.596134282807731,
      "grad_norm": 7.449661731719971,
      "learning_rate": 4.640386571719227e-05,
      "loss": 3.5916,
      "step": 7070
    },
    {
      "epoch": 3.6012207527975586,
      "grad_norm": 5.581085205078125,
      "learning_rate": 4.639877924720244e-05,
      "loss": 3.6195,
      "step": 7080
    },
    {
      "epoch": 3.6063072227873856,
      "grad_norm": 9.375107765197754,
      "learning_rate": 4.639369277721262e-05,
      "loss": 3.515,
      "step": 7090
    },
    {
      "epoch": 3.6113936927772126,
      "grad_norm": 6.638445854187012,
      "learning_rate": 4.638860630722279e-05,
      "loss": 3.5836,
      "step": 7100
    },
    {
      "epoch": 3.6164801627670395,
      "grad_norm": 5.276569366455078,
      "learning_rate": 4.6383519837232965e-05,
      "loss": 3.5954,
      "step": 7110
    },
    {
      "epoch": 3.621566632756867,
      "grad_norm": 6.055764198303223,
      "learning_rate": 4.6378433367243135e-05,
      "loss": 3.5648,
      "step": 7120
    },
    {
      "epoch": 3.626653102746694,
      "grad_norm": 4.181941509246826,
      "learning_rate": 4.6373346897253305e-05,
      "loss": 3.6137,
      "step": 7130
    },
    {
      "epoch": 3.631739572736521,
      "grad_norm": 5.305240631103516,
      "learning_rate": 4.636826042726348e-05,
      "loss": 3.4938,
      "step": 7140
    },
    {
      "epoch": 3.636826042726348,
      "grad_norm": 7.194119930267334,
      "learning_rate": 4.636317395727365e-05,
      "loss": 3.5962,
      "step": 7150
    },
    {
      "epoch": 3.641912512716175,
      "grad_norm": 5.363950729370117,
      "learning_rate": 4.635808748728383e-05,
      "loss": 3.4839,
      "step": 7160
    },
    {
      "epoch": 3.646998982706002,
      "grad_norm": 5.105096340179443,
      "learning_rate": 4.6353001017294e-05,
      "loss": 3.533,
      "step": 7170
    },
    {
      "epoch": 3.652085452695829,
      "grad_norm": 8.620826721191406,
      "learning_rate": 4.6347914547304175e-05,
      "loss": 3.5804,
      "step": 7180
    },
    {
      "epoch": 3.6571719226856563,
      "grad_norm": 7.782473087310791,
      "learning_rate": 4.634282807731435e-05,
      "loss": 3.5759,
      "step": 7190
    },
    {
      "epoch": 3.6622583926754833,
      "grad_norm": 6.057344436645508,
      "learning_rate": 4.633774160732452e-05,
      "loss": 3.5642,
      "step": 7200
    },
    {
      "epoch": 3.6673448626653102,
      "grad_norm": 6.224895000457764,
      "learning_rate": 4.633265513733469e-05,
      "loss": 3.5709,
      "step": 7210
    },
    {
      "epoch": 3.672431332655137,
      "grad_norm": 5.362300872802734,
      "learning_rate": 4.632756866734487e-05,
      "loss": 3.6043,
      "step": 7220
    },
    {
      "epoch": 3.6775178026449646,
      "grad_norm": 5.777322769165039,
      "learning_rate": 4.632248219735504e-05,
      "loss": 3.5394,
      "step": 7230
    },
    {
      "epoch": 3.6826042726347916,
      "grad_norm": 6.153487205505371,
      "learning_rate": 4.631739572736521e-05,
      "loss": 3.5048,
      "step": 7240
    },
    {
      "epoch": 3.6876907426246186,
      "grad_norm": 5.022377967834473,
      "learning_rate": 4.6312309257375384e-05,
      "loss": 3.5802,
      "step": 7250
    },
    {
      "epoch": 3.6927772126144456,
      "grad_norm": 6.604250907897949,
      "learning_rate": 4.6307222787385554e-05,
      "loss": 3.56,
      "step": 7260
    },
    {
      "epoch": 3.6978636826042726,
      "grad_norm": 6.2935662269592285,
      "learning_rate": 4.630213631739573e-05,
      "loss": 3.5526,
      "step": 7270
    },
    {
      "epoch": 3.7029501525940995,
      "grad_norm": 6.017805099487305,
      "learning_rate": 4.629704984740591e-05,
      "loss": 3.5665,
      "step": 7280
    },
    {
      "epoch": 3.7080366225839265,
      "grad_norm": 6.805716037750244,
      "learning_rate": 4.629196337741608e-05,
      "loss": 3.5369,
      "step": 7290
    },
    {
      "epoch": 3.713123092573754,
      "grad_norm": 5.785092830657959,
      "learning_rate": 4.628687690742625e-05,
      "loss": 3.5705,
      "step": 7300
    },
    {
      "epoch": 3.718209562563581,
      "grad_norm": 6.968968391418457,
      "learning_rate": 4.6281790437436424e-05,
      "loss": 3.5707,
      "step": 7310
    },
    {
      "epoch": 3.723296032553408,
      "grad_norm": 4.476573944091797,
      "learning_rate": 4.6276703967446594e-05,
      "loss": 3.6302,
      "step": 7320
    },
    {
      "epoch": 3.728382502543235,
      "grad_norm": 6.818967342376709,
      "learning_rate": 4.6271617497456764e-05,
      "loss": 3.5776,
      "step": 7330
    },
    {
      "epoch": 3.7334689725330623,
      "grad_norm": 4.902641773223877,
      "learning_rate": 4.626653102746694e-05,
      "loss": 3.542,
      "step": 7340
    },
    {
      "epoch": 3.7385554425228893,
      "grad_norm": 7.175673484802246,
      "learning_rate": 4.626144455747711e-05,
      "loss": 3.6294,
      "step": 7350
    },
    {
      "epoch": 3.7436419125127163,
      "grad_norm": 7.035593509674072,
      "learning_rate": 4.625635808748729e-05,
      "loss": 3.5111,
      "step": 7360
    },
    {
      "epoch": 3.7487283825025433,
      "grad_norm": 4.818260192871094,
      "learning_rate": 4.6251271617497464e-05,
      "loss": 3.5356,
      "step": 7370
    },
    {
      "epoch": 3.7538148524923702,
      "grad_norm": 4.316448211669922,
      "learning_rate": 4.6246185147507634e-05,
      "loss": 3.5158,
      "step": 7380
    },
    {
      "epoch": 3.758901322482197,
      "grad_norm": 5.205474853515625,
      "learning_rate": 4.6241098677517803e-05,
      "loss": 3.5076,
      "step": 7390
    },
    {
      "epoch": 3.763987792472024,
      "grad_norm": 4.981058597564697,
      "learning_rate": 4.623601220752798e-05,
      "loss": 3.5112,
      "step": 7400
    },
    {
      "epoch": 3.7690742624618516,
      "grad_norm": 9.11139965057373,
      "learning_rate": 4.623092573753815e-05,
      "loss": 3.5791,
      "step": 7410
    },
    {
      "epoch": 3.7741607324516786,
      "grad_norm": 6.497097492218018,
      "learning_rate": 4.622583926754832e-05,
      "loss": 3.5984,
      "step": 7420
    },
    {
      "epoch": 3.7792472024415056,
      "grad_norm": 5.455554962158203,
      "learning_rate": 4.6220752797558497e-05,
      "loss": 3.5153,
      "step": 7430
    },
    {
      "epoch": 3.7843336724313326,
      "grad_norm": 5.274377346038818,
      "learning_rate": 4.6215666327568666e-05,
      "loss": 3.6236,
      "step": 7440
    },
    {
      "epoch": 3.78942014242116,
      "grad_norm": 5.3269124031066895,
      "learning_rate": 4.621057985757884e-05,
      "loss": 3.5068,
      "step": 7450
    },
    {
      "epoch": 3.794506612410987,
      "grad_norm": 4.999887466430664,
      "learning_rate": 4.620549338758901e-05,
      "loss": 3.5454,
      "step": 7460
    },
    {
      "epoch": 3.799593082400814,
      "grad_norm": 5.569624900817871,
      "learning_rate": 4.620040691759919e-05,
      "loss": 3.5025,
      "step": 7470
    },
    {
      "epoch": 3.804679552390641,
      "grad_norm": 5.212758541107178,
      "learning_rate": 4.6195320447609366e-05,
      "loss": 3.5044,
      "step": 7480
    },
    {
      "epoch": 3.809766022380468,
      "grad_norm": 5.46865701675415,
      "learning_rate": 4.6190233977619536e-05,
      "loss": 3.5141,
      "step": 7490
    },
    {
      "epoch": 3.814852492370295,
      "grad_norm": 6.264723777770996,
      "learning_rate": 4.6185147507629706e-05,
      "loss": 3.5176,
      "step": 7500
    },
    {
      "epoch": 3.819938962360122,
      "grad_norm": 6.231154918670654,
      "learning_rate": 4.618006103763988e-05,
      "loss": 3.4819,
      "step": 7510
    },
    {
      "epoch": 3.8250254323499493,
      "grad_norm": 7.268845558166504,
      "learning_rate": 4.617497456765005e-05,
      "loss": 3.5521,
      "step": 7520
    },
    {
      "epoch": 3.8301119023397763,
      "grad_norm": 6.45893669128418,
      "learning_rate": 4.616988809766022e-05,
      "loss": 3.5357,
      "step": 7530
    },
    {
      "epoch": 3.8351983723296033,
      "grad_norm": 6.147562503814697,
      "learning_rate": 4.61648016276704e-05,
      "loss": 3.5249,
      "step": 7540
    },
    {
      "epoch": 3.8402848423194302,
      "grad_norm": 5.291461944580078,
      "learning_rate": 4.615971515768057e-05,
      "loss": 3.5391,
      "step": 7550
    },
    {
      "epoch": 3.845371312309257,
      "grad_norm": 6.942288398742676,
      "learning_rate": 4.6154628687690746e-05,
      "loss": 3.6022,
      "step": 7560
    },
    {
      "epoch": 3.8504577822990846,
      "grad_norm": 4.945714950561523,
      "learning_rate": 4.614954221770092e-05,
      "loss": 3.5635,
      "step": 7570
    },
    {
      "epoch": 3.8555442522889116,
      "grad_norm": 9.11294937133789,
      "learning_rate": 4.614445574771109e-05,
      "loss": 3.4485,
      "step": 7580
    },
    {
      "epoch": 3.8606307222787386,
      "grad_norm": 4.3927531242370605,
      "learning_rate": 4.613936927772126e-05,
      "loss": 3.5657,
      "step": 7590
    },
    {
      "epoch": 3.8657171922685656,
      "grad_norm": 6.62286901473999,
      "learning_rate": 4.613428280773144e-05,
      "loss": 3.5646,
      "step": 7600
    },
    {
      "epoch": 3.8708036622583926,
      "grad_norm": 7.678708076477051,
      "learning_rate": 4.612919633774161e-05,
      "loss": 3.549,
      "step": 7610
    },
    {
      "epoch": 3.8758901322482195,
      "grad_norm": 8.059137344360352,
      "learning_rate": 4.612410986775178e-05,
      "loss": 3.5159,
      "step": 7620
    },
    {
      "epoch": 3.8809766022380465,
      "grad_norm": 5.620709419250488,
      "learning_rate": 4.6119023397761955e-05,
      "loss": 3.5623,
      "step": 7630
    },
    {
      "epoch": 3.886063072227874,
      "grad_norm": 6.80388069152832,
      "learning_rate": 4.6113936927772125e-05,
      "loss": 3.5126,
      "step": 7640
    },
    {
      "epoch": 3.891149542217701,
      "grad_norm": 6.889127254486084,
      "learning_rate": 4.61088504577823e-05,
      "loss": 3.4742,
      "step": 7650
    },
    {
      "epoch": 3.896236012207528,
      "grad_norm": 5.844050884246826,
      "learning_rate": 4.610376398779248e-05,
      "loss": 3.6426,
      "step": 7660
    },
    {
      "epoch": 3.901322482197355,
      "grad_norm": 5.526522636413574,
      "learning_rate": 4.609867751780265e-05,
      "loss": 3.5746,
      "step": 7670
    },
    {
      "epoch": 3.9064089521871823,
      "grad_norm": 7.178666591644287,
      "learning_rate": 4.609359104781282e-05,
      "loss": 3.5512,
      "step": 7680
    },
    {
      "epoch": 3.9114954221770093,
      "grad_norm": 5.448190212249756,
      "learning_rate": 4.6088504577822995e-05,
      "loss": 3.5583,
      "step": 7690
    },
    {
      "epoch": 3.9165818921668363,
      "grad_norm": 7.872321605682373,
      "learning_rate": 4.6083418107833165e-05,
      "loss": 3.5339,
      "step": 7700
    },
    {
      "epoch": 3.9216683621566633,
      "grad_norm": 5.739564895629883,
      "learning_rate": 4.607833163784334e-05,
      "loss": 3.5863,
      "step": 7710
    },
    {
      "epoch": 3.9267548321464902,
      "grad_norm": 7.415482997894287,
      "learning_rate": 4.607324516785351e-05,
      "loss": 3.5802,
      "step": 7720
    },
    {
      "epoch": 3.931841302136317,
      "grad_norm": 6.35541296005249,
      "learning_rate": 4.606815869786368e-05,
      "loss": 3.4715,
      "step": 7730
    },
    {
      "epoch": 3.936927772126144,
      "grad_norm": 6.134989261627197,
      "learning_rate": 4.606307222787386e-05,
      "loss": 3.4953,
      "step": 7740
    },
    {
      "epoch": 3.9420142421159716,
      "grad_norm": 4.735561847686768,
      "learning_rate": 4.605798575788403e-05,
      "loss": 3.5409,
      "step": 7750
    },
    {
      "epoch": 3.9471007121057986,
      "grad_norm": 7.810882568359375,
      "learning_rate": 4.6052899287894205e-05,
      "loss": 3.4791,
      "step": 7760
    },
    {
      "epoch": 3.9521871820956256,
      "grad_norm": 8.222249984741211,
      "learning_rate": 4.604781281790438e-05,
      "loss": 3.4796,
      "step": 7770
    },
    {
      "epoch": 3.9572736520854526,
      "grad_norm": 4.569162845611572,
      "learning_rate": 4.604272634791455e-05,
      "loss": 3.5221,
      "step": 7780
    },
    {
      "epoch": 3.96236012207528,
      "grad_norm": 6.029212951660156,
      "learning_rate": 4.603763987792472e-05,
      "loss": 3.5498,
      "step": 7790
    },
    {
      "epoch": 3.967446592065107,
      "grad_norm": 6.914393901824951,
      "learning_rate": 4.60325534079349e-05,
      "loss": 3.4702,
      "step": 7800
    },
    {
      "epoch": 3.972533062054934,
      "grad_norm": 5.658518314361572,
      "learning_rate": 4.602746693794507e-05,
      "loss": 3.5559,
      "step": 7810
    },
    {
      "epoch": 3.977619532044761,
      "grad_norm": 4.597030162811279,
      "learning_rate": 4.602238046795524e-05,
      "loss": 3.5152,
      "step": 7820
    },
    {
      "epoch": 3.982706002034588,
      "grad_norm": 7.656993865966797,
      "learning_rate": 4.6017293997965414e-05,
      "loss": 3.5653,
      "step": 7830
    },
    {
      "epoch": 3.987792472024415,
      "grad_norm": 5.765600204467773,
      "learning_rate": 4.6012207527975584e-05,
      "loss": 3.5138,
      "step": 7840
    },
    {
      "epoch": 3.992878942014242,
      "grad_norm": 5.834529876708984,
      "learning_rate": 4.600712105798576e-05,
      "loss": 3.5151,
      "step": 7850
    },
    {
      "epoch": 3.9979654120040693,
      "grad_norm": 5.882931709289551,
      "learning_rate": 4.600203458799594e-05,
      "loss": 3.4387,
      "step": 7860
    },
    {
      "epoch": 4.0,
      "eval_loss": 3.682547092437744,
      "eval_runtime": 2.7269,
      "eval_samples_per_second": 1017.624,
      "eval_steps_per_second": 127.249,
      "step": 7864
    },
    {
      "epoch": 4.003051881993896,
      "grad_norm": 5.696679592132568,
      "learning_rate": 4.599694811800611e-05,
      "loss": 3.5395,
      "step": 7870
    },
    {
      "epoch": 4.008138351983724,
      "grad_norm": 7.439525127410889,
      "learning_rate": 4.599186164801628e-05,
      "loss": 3.5478,
      "step": 7880
    },
    {
      "epoch": 4.013224821973551,
      "grad_norm": 7.7172627449035645,
      "learning_rate": 4.5986775178026454e-05,
      "loss": 3.5519,
      "step": 7890
    },
    {
      "epoch": 4.018311291963378,
      "grad_norm": 6.432249546051025,
      "learning_rate": 4.5981688708036624e-05,
      "loss": 3.5054,
      "step": 7900
    },
    {
      "epoch": 4.023397761953205,
      "grad_norm": 6.983066082000732,
      "learning_rate": 4.5976602238046794e-05,
      "loss": 3.4648,
      "step": 7910
    },
    {
      "epoch": 4.028484231943032,
      "grad_norm": 6.731037616729736,
      "learning_rate": 4.597151576805697e-05,
      "loss": 3.5688,
      "step": 7920
    },
    {
      "epoch": 4.033570701932859,
      "grad_norm": 5.829944133758545,
      "learning_rate": 4.596642929806714e-05,
      "loss": 3.5034,
      "step": 7930
    },
    {
      "epoch": 4.038657171922686,
      "grad_norm": 6.856215953826904,
      "learning_rate": 4.596134282807732e-05,
      "loss": 3.4485,
      "step": 7940
    },
    {
      "epoch": 4.043743641912513,
      "grad_norm": 7.135995388031006,
      "learning_rate": 4.5956256358087494e-05,
      "loss": 3.5118,
      "step": 7950
    },
    {
      "epoch": 4.0488301119023395,
      "grad_norm": 6.787564277648926,
      "learning_rate": 4.5951169888097664e-05,
      "loss": 3.5648,
      "step": 7960
    },
    {
      "epoch": 4.0539165818921665,
      "grad_norm": 5.2697224617004395,
      "learning_rate": 4.594608341810784e-05,
      "loss": 3.5025,
      "step": 7970
    },
    {
      "epoch": 4.0590030518819935,
      "grad_norm": 8.408817291259766,
      "learning_rate": 4.594099694811801e-05,
      "loss": 3.4802,
      "step": 7980
    },
    {
      "epoch": 4.064089521871821,
      "grad_norm": 8.241003036499023,
      "learning_rate": 4.593591047812818e-05,
      "loss": 3.482,
      "step": 7990
    },
    {
      "epoch": 4.069175991861648,
      "grad_norm": 7.665555477142334,
      "learning_rate": 4.593082400813836e-05,
      "loss": 3.5327,
      "step": 8000
    },
    {
      "epoch": 4.074262461851475,
      "grad_norm": 7.697659969329834,
      "learning_rate": 4.5925737538148527e-05,
      "loss": 3.5564,
      "step": 8010
    },
    {
      "epoch": 4.079348931841302,
      "grad_norm": 8.690217971801758,
      "learning_rate": 4.5920651068158696e-05,
      "loss": 3.5187,
      "step": 8020
    },
    {
      "epoch": 4.084435401831129,
      "grad_norm": 6.476851940155029,
      "learning_rate": 4.591556459816887e-05,
      "loss": 3.5528,
      "step": 8030
    },
    {
      "epoch": 4.089521871820956,
      "grad_norm": 4.610379695892334,
      "learning_rate": 4.591047812817904e-05,
      "loss": 3.4914,
      "step": 8040
    },
    {
      "epoch": 4.094608341810783,
      "grad_norm": 7.771125316619873,
      "learning_rate": 4.590539165818922e-05,
      "loss": 3.5258,
      "step": 8050
    },
    {
      "epoch": 4.09969481180061,
      "grad_norm": 5.73425817489624,
      "learning_rate": 4.5900305188199396e-05,
      "loss": 3.5533,
      "step": 8060
    },
    {
      "epoch": 4.104781281790437,
      "grad_norm": 6.67058801651001,
      "learning_rate": 4.5895218718209566e-05,
      "loss": 3.5051,
      "step": 8070
    },
    {
      "epoch": 4.109867751780264,
      "grad_norm": 6.453938007354736,
      "learning_rate": 4.5890132248219736e-05,
      "loss": 3.468,
      "step": 8080
    },
    {
      "epoch": 4.114954221770091,
      "grad_norm": 5.88721227645874,
      "learning_rate": 4.588504577822991e-05,
      "loss": 3.5389,
      "step": 8090
    },
    {
      "epoch": 4.120040691759919,
      "grad_norm": 6.570307731628418,
      "learning_rate": 4.587995930824008e-05,
      "loss": 3.4452,
      "step": 8100
    },
    {
      "epoch": 4.125127161749746,
      "grad_norm": 5.611356735229492,
      "learning_rate": 4.587487283825025e-05,
      "loss": 3.4966,
      "step": 8110
    },
    {
      "epoch": 4.130213631739573,
      "grad_norm": 6.96566104888916,
      "learning_rate": 4.586978636826043e-05,
      "loss": 3.5291,
      "step": 8120
    },
    {
      "epoch": 4.1353001017294,
      "grad_norm": 7.933901786804199,
      "learning_rate": 4.58646998982706e-05,
      "loss": 3.4813,
      "step": 8130
    },
    {
      "epoch": 4.140386571719227,
      "grad_norm": 5.545637607574463,
      "learning_rate": 4.5859613428280776e-05,
      "loss": 3.552,
      "step": 8140
    },
    {
      "epoch": 4.145473041709054,
      "grad_norm": 6.902024269104004,
      "learning_rate": 4.585452695829095e-05,
      "loss": 3.4831,
      "step": 8150
    },
    {
      "epoch": 4.150559511698881,
      "grad_norm": 8.349004745483398,
      "learning_rate": 4.584944048830112e-05,
      "loss": 3.468,
      "step": 8160
    },
    {
      "epoch": 4.155645981688708,
      "grad_norm": 4.874841690063477,
      "learning_rate": 4.584435401831129e-05,
      "loss": 3.5194,
      "step": 8170
    },
    {
      "epoch": 4.160732451678535,
      "grad_norm": 5.534297943115234,
      "learning_rate": 4.583926754832147e-05,
      "loss": 3.5145,
      "step": 8180
    },
    {
      "epoch": 4.165818921668362,
      "grad_norm": 6.5342936515808105,
      "learning_rate": 4.583418107833164e-05,
      "loss": 3.5465,
      "step": 8190
    },
    {
      "epoch": 4.170905391658189,
      "grad_norm": 6.877661228179932,
      "learning_rate": 4.582909460834181e-05,
      "loss": 3.5069,
      "step": 8200
    },
    {
      "epoch": 4.175991861648017,
      "grad_norm": 5.838846683502197,
      "learning_rate": 4.5824008138351985e-05,
      "loss": 3.4646,
      "step": 8210
    },
    {
      "epoch": 4.181078331637844,
      "grad_norm": 5.677132606506348,
      "learning_rate": 4.5818921668362155e-05,
      "loss": 3.4656,
      "step": 8220
    },
    {
      "epoch": 4.186164801627671,
      "grad_norm": 7.908148765563965,
      "learning_rate": 4.581383519837233e-05,
      "loss": 3.4607,
      "step": 8230
    },
    {
      "epoch": 4.191251271617498,
      "grad_norm": 6.314956188201904,
      "learning_rate": 4.580874872838251e-05,
      "loss": 3.4616,
      "step": 8240
    },
    {
      "epoch": 4.196337741607325,
      "grad_norm": 7.86068058013916,
      "learning_rate": 4.580366225839268e-05,
      "loss": 3.5131,
      "step": 8250
    },
    {
      "epoch": 4.201424211597152,
      "grad_norm": 5.117482662200928,
      "learning_rate": 4.5798575788402855e-05,
      "loss": 3.4784,
      "step": 8260
    },
    {
      "epoch": 4.206510681586979,
      "grad_norm": 6.023205280303955,
      "learning_rate": 4.5793489318413025e-05,
      "loss": 3.4265,
      "step": 8270
    },
    {
      "epoch": 4.211597151576806,
      "grad_norm": 7.292799949645996,
      "learning_rate": 4.5788402848423195e-05,
      "loss": 3.4721,
      "step": 8280
    },
    {
      "epoch": 4.216683621566633,
      "grad_norm": 7.190707683563232,
      "learning_rate": 4.578331637843337e-05,
      "loss": 3.5505,
      "step": 8290
    },
    {
      "epoch": 4.2217700915564595,
      "grad_norm": 6.749729156494141,
      "learning_rate": 4.577822990844354e-05,
      "loss": 3.506,
      "step": 8300
    },
    {
      "epoch": 4.2268565615462865,
      "grad_norm": 7.345061302185059,
      "learning_rate": 4.577314343845371e-05,
      "loss": 3.4624,
      "step": 8310
    },
    {
      "epoch": 4.2319430315361135,
      "grad_norm": 8.302069664001465,
      "learning_rate": 4.576805696846389e-05,
      "loss": 3.4777,
      "step": 8320
    },
    {
      "epoch": 4.237029501525941,
      "grad_norm": 7.307460308074951,
      "learning_rate": 4.5762970498474065e-05,
      "loss": 3.4481,
      "step": 8330
    },
    {
      "epoch": 4.242115971515768,
      "grad_norm": 8.880420684814453,
      "learning_rate": 4.5757884028484235e-05,
      "loss": 3.4313,
      "step": 8340
    },
    {
      "epoch": 4.247202441505595,
      "grad_norm": 8.772892951965332,
      "learning_rate": 4.575279755849441e-05,
      "loss": 3.4719,
      "step": 8350
    },
    {
      "epoch": 4.252288911495422,
      "grad_norm": 7.044968128204346,
      "learning_rate": 4.574771108850458e-05,
      "loss": 3.5015,
      "step": 8360
    },
    {
      "epoch": 4.257375381485249,
      "grad_norm": 7.717684268951416,
      "learning_rate": 4.574262461851475e-05,
      "loss": 3.4443,
      "step": 8370
    },
    {
      "epoch": 4.262461851475076,
      "grad_norm": 5.569419860839844,
      "learning_rate": 4.573753814852493e-05,
      "loss": 3.5175,
      "step": 8380
    },
    {
      "epoch": 4.267548321464903,
      "grad_norm": 9.139665603637695,
      "learning_rate": 4.57324516785351e-05,
      "loss": 3.4793,
      "step": 8390
    },
    {
      "epoch": 4.27263479145473,
      "grad_norm": 6.840276718139648,
      "learning_rate": 4.572736520854527e-05,
      "loss": 3.4976,
      "step": 8400
    },
    {
      "epoch": 4.277721261444557,
      "grad_norm": 9.281371116638184,
      "learning_rate": 4.5722278738555444e-05,
      "loss": 3.4081,
      "step": 8410
    },
    {
      "epoch": 4.282807731434384,
      "grad_norm": 7.087295055389404,
      "learning_rate": 4.5717192268565614e-05,
      "loss": 3.557,
      "step": 8420
    },
    {
      "epoch": 4.287894201424211,
      "grad_norm": 6.432158470153809,
      "learning_rate": 4.571210579857579e-05,
      "loss": 3.4804,
      "step": 8430
    },
    {
      "epoch": 4.292980671414039,
      "grad_norm": 7.989634990692139,
      "learning_rate": 4.570701932858597e-05,
      "loss": 3.5354,
      "step": 8440
    },
    {
      "epoch": 4.298067141403866,
      "grad_norm": 8.7486572265625,
      "learning_rate": 4.570193285859614e-05,
      "loss": 3.3316,
      "step": 8450
    },
    {
      "epoch": 4.303153611393693,
      "grad_norm": 6.547328948974609,
      "learning_rate": 4.569684638860631e-05,
      "loss": 3.481,
      "step": 8460
    },
    {
      "epoch": 4.30824008138352,
      "grad_norm": 6.48618221282959,
      "learning_rate": 4.5691759918616484e-05,
      "loss": 3.4517,
      "step": 8470
    },
    {
      "epoch": 4.313326551373347,
      "grad_norm": 6.9151387214660645,
      "learning_rate": 4.5686673448626654e-05,
      "loss": 3.4846,
      "step": 8480
    },
    {
      "epoch": 4.318413021363174,
      "grad_norm": 8.77987289428711,
      "learning_rate": 4.5681586978636824e-05,
      "loss": 3.5076,
      "step": 8490
    },
    {
      "epoch": 4.323499491353001,
      "grad_norm": 7.875593662261963,
      "learning_rate": 4.5676500508647e-05,
      "loss": 3.4863,
      "step": 8500
    },
    {
      "epoch": 4.328585961342828,
      "grad_norm": 6.400660514831543,
      "learning_rate": 4.567141403865717e-05,
      "loss": 3.4977,
      "step": 8510
    },
    {
      "epoch": 4.333672431332655,
      "grad_norm": 5.12528657913208,
      "learning_rate": 4.566632756866735e-05,
      "loss": 3.4756,
      "step": 8520
    },
    {
      "epoch": 4.338758901322482,
      "grad_norm": 6.7897257804870605,
      "learning_rate": 4.5661241098677524e-05,
      "loss": 3.3882,
      "step": 8530
    },
    {
      "epoch": 4.343845371312309,
      "grad_norm": 7.742665767669678,
      "learning_rate": 4.5656154628687694e-05,
      "loss": 3.4606,
      "step": 8540
    },
    {
      "epoch": 4.348931841302137,
      "grad_norm": 6.026894569396973,
      "learning_rate": 4.565106815869787e-05,
      "loss": 3.5058,
      "step": 8550
    },
    {
      "epoch": 4.354018311291964,
      "grad_norm": 7.406733512878418,
      "learning_rate": 4.564598168870804e-05,
      "loss": 3.4474,
      "step": 8560
    },
    {
      "epoch": 4.359104781281791,
      "grad_norm": 5.7472124099731445,
      "learning_rate": 4.564089521871821e-05,
      "loss": 3.4928,
      "step": 8570
    },
    {
      "epoch": 4.364191251271618,
      "grad_norm": 6.1093645095825195,
      "learning_rate": 4.563580874872839e-05,
      "loss": 3.417,
      "step": 8580
    },
    {
      "epoch": 4.369277721261445,
      "grad_norm": 8.887754440307617,
      "learning_rate": 4.5630722278738557e-05,
      "loss": 3.4739,
      "step": 8590
    },
    {
      "epoch": 4.374364191251272,
      "grad_norm": 5.5056633949279785,
      "learning_rate": 4.5625635808748726e-05,
      "loss": 3.4902,
      "step": 8600
    },
    {
      "epoch": 4.379450661241099,
      "grad_norm": 8.776473999023438,
      "learning_rate": 4.56205493387589e-05,
      "loss": 3.4383,
      "step": 8610
    },
    {
      "epoch": 4.384537131230926,
      "grad_norm": 12.802504539489746,
      "learning_rate": 4.561546286876908e-05,
      "loss": 3.4436,
      "step": 8620
    },
    {
      "epoch": 4.389623601220753,
      "grad_norm": 5.422558784484863,
      "learning_rate": 4.561037639877925e-05,
      "loss": 3.4911,
      "step": 8630
    },
    {
      "epoch": 4.3947100712105795,
      "grad_norm": 8.648914337158203,
      "learning_rate": 4.5605289928789426e-05,
      "loss": 3.4686,
      "step": 8640
    },
    {
      "epoch": 4.3997965412004065,
      "grad_norm": 7.750622272491455,
      "learning_rate": 4.5600203458799596e-05,
      "loss": 3.4595,
      "step": 8650
    },
    {
      "epoch": 4.404883011190234,
      "grad_norm": 8.931674003601074,
      "learning_rate": 4.5595116988809766e-05,
      "loss": 3.4817,
      "step": 8660
    },
    {
      "epoch": 4.409969481180061,
      "grad_norm": 5.857328414916992,
      "learning_rate": 4.559003051881994e-05,
      "loss": 3.379,
      "step": 8670
    },
    {
      "epoch": 4.415055951169888,
      "grad_norm": 10.644730567932129,
      "learning_rate": 4.558494404883011e-05,
      "loss": 3.4553,
      "step": 8680
    },
    {
      "epoch": 4.420142421159715,
      "grad_norm": 7.72205114364624,
      "learning_rate": 4.557985757884028e-05,
      "loss": 3.423,
      "step": 8690
    },
    {
      "epoch": 4.425228891149542,
      "grad_norm": 7.861493110656738,
      "learning_rate": 4.557477110885046e-05,
      "loss": 3.5308,
      "step": 8700
    },
    {
      "epoch": 4.430315361139369,
      "grad_norm": 10.250804901123047,
      "learning_rate": 4.556968463886063e-05,
      "loss": 3.4332,
      "step": 8710
    },
    {
      "epoch": 4.435401831129196,
      "grad_norm": 9.085840225219727,
      "learning_rate": 4.5564598168870806e-05,
      "loss": 3.4776,
      "step": 8720
    },
    {
      "epoch": 4.440488301119023,
      "grad_norm": 8.356430053710938,
      "learning_rate": 4.555951169888098e-05,
      "loss": 3.4658,
      "step": 8730
    },
    {
      "epoch": 4.44557477110885,
      "grad_norm": 6.438593864440918,
      "learning_rate": 4.555442522889115e-05,
      "loss": 3.4122,
      "step": 8740
    },
    {
      "epoch": 4.450661241098677,
      "grad_norm": 6.882085800170898,
      "learning_rate": 4.554933875890132e-05,
      "loss": 3.3572,
      "step": 8750
    },
    {
      "epoch": 4.455747711088504,
      "grad_norm": 8.223237037658691,
      "learning_rate": 4.55442522889115e-05,
      "loss": 3.4867,
      "step": 8760
    },
    {
      "epoch": 4.460834181078331,
      "grad_norm": 10.363363265991211,
      "learning_rate": 4.553916581892167e-05,
      "loss": 3.4631,
      "step": 8770
    },
    {
      "epoch": 4.465920651068159,
      "grad_norm": 7.76474142074585,
      "learning_rate": 4.5534079348931846e-05,
      "loss": 3.4099,
      "step": 8780
    },
    {
      "epoch": 4.471007121057986,
      "grad_norm": 6.035660266876221,
      "learning_rate": 4.5528992878942015e-05,
      "loss": 3.5282,
      "step": 8790
    },
    {
      "epoch": 4.476093591047813,
      "grad_norm": 8.37148666381836,
      "learning_rate": 4.5523906408952185e-05,
      "loss": 3.4237,
      "step": 8800
    },
    {
      "epoch": 4.48118006103764,
      "grad_norm": 9.595413208007812,
      "learning_rate": 4.551881993896236e-05,
      "loss": 3.3798,
      "step": 8810
    },
    {
      "epoch": 4.486266531027467,
      "grad_norm": 9.972001075744629,
      "learning_rate": 4.551373346897254e-05,
      "loss": 3.3891,
      "step": 8820
    },
    {
      "epoch": 4.491353001017294,
      "grad_norm": 6.743781089782715,
      "learning_rate": 4.550864699898271e-05,
      "loss": 3.5214,
      "step": 8830
    },
    {
      "epoch": 4.496439471007121,
      "grad_norm": 7.265984535217285,
      "learning_rate": 4.5503560528992885e-05,
      "loss": 3.465,
      "step": 8840
    },
    {
      "epoch": 4.501525940996948,
      "grad_norm": 9.915414810180664,
      "learning_rate": 4.5498474059003055e-05,
      "loss": 3.4719,
      "step": 8850
    },
    {
      "epoch": 4.506612410986775,
      "grad_norm": 8.241029739379883,
      "learning_rate": 4.5493387589013225e-05,
      "loss": 3.4857,
      "step": 8860
    },
    {
      "epoch": 4.511698880976602,
      "grad_norm": 8.134593963623047,
      "learning_rate": 4.54883011190234e-05,
      "loss": 3.4377,
      "step": 8870
    },
    {
      "epoch": 4.51678535096643,
      "grad_norm": 8.605182647705078,
      "learning_rate": 4.548321464903357e-05,
      "loss": 3.4001,
      "step": 8880
    },
    {
      "epoch": 4.521871820956257,
      "grad_norm": 10.543033599853516,
      "learning_rate": 4.547812817904374e-05,
      "loss": 3.5066,
      "step": 8890
    },
    {
      "epoch": 4.526958290946084,
      "grad_norm": 8.078709602355957,
      "learning_rate": 4.547304170905392e-05,
      "loss": 3.4245,
      "step": 8900
    },
    {
      "epoch": 4.532044760935911,
      "grad_norm": 8.793431282043457,
      "learning_rate": 4.5467955239064095e-05,
      "loss": 3.4561,
      "step": 8910
    },
    {
      "epoch": 4.537131230925738,
      "grad_norm": 8.877737998962402,
      "learning_rate": 4.5462868769074265e-05,
      "loss": 3.4705,
      "step": 8920
    },
    {
      "epoch": 4.542217700915565,
      "grad_norm": 7.629286766052246,
      "learning_rate": 4.545778229908444e-05,
      "loss": 3.4751,
      "step": 8930
    },
    {
      "epoch": 4.547304170905392,
      "grad_norm": 7.7086076736450195,
      "learning_rate": 4.545269582909461e-05,
      "loss": 3.4642,
      "step": 8940
    },
    {
      "epoch": 4.552390640895219,
      "grad_norm": 7.560568332672119,
      "learning_rate": 4.544760935910478e-05,
      "loss": 3.4221,
      "step": 8950
    },
    {
      "epoch": 4.557477110885046,
      "grad_norm": 6.356943130493164,
      "learning_rate": 4.544252288911496e-05,
      "loss": 3.4515,
      "step": 8960
    },
    {
      "epoch": 4.562563580874873,
      "grad_norm": 7.457127094268799,
      "learning_rate": 4.543743641912513e-05,
      "loss": 3.4878,
      "step": 8970
    },
    {
      "epoch": 4.5676500508646996,
      "grad_norm": 6.367786884307861,
      "learning_rate": 4.54323499491353e-05,
      "loss": 3.4584,
      "step": 8980
    },
    {
      "epoch": 4.5727365208545265,
      "grad_norm": 10.294775009155273,
      "learning_rate": 4.5427263479145474e-05,
      "loss": 3.4518,
      "step": 8990
    },
    {
      "epoch": 4.577822990844354,
      "grad_norm": 6.925987720489502,
      "learning_rate": 4.5422177009155644e-05,
      "loss": 3.4573,
      "step": 9000
    },
    {
      "epoch": 4.582909460834181,
      "grad_norm": 9.72468090057373,
      "learning_rate": 4.541709053916582e-05,
      "loss": 3.4615,
      "step": 9010
    },
    {
      "epoch": 4.587995930824008,
      "grad_norm": 7.277684688568115,
      "learning_rate": 4.5412004069176e-05,
      "loss": 3.5261,
      "step": 9020
    },
    {
      "epoch": 4.593082400813835,
      "grad_norm": 8.789225578308105,
      "learning_rate": 4.540691759918617e-05,
      "loss": 3.4422,
      "step": 9030
    },
    {
      "epoch": 4.598168870803662,
      "grad_norm": 9.132124900817871,
      "learning_rate": 4.540183112919634e-05,
      "loss": 3.3697,
      "step": 9040
    },
    {
      "epoch": 4.603255340793489,
      "grad_norm": 9.38601303100586,
      "learning_rate": 4.5396744659206514e-05,
      "loss": 3.4617,
      "step": 9050
    },
    {
      "epoch": 4.608341810783316,
      "grad_norm": 7.389291286468506,
      "learning_rate": 4.5391658189216684e-05,
      "loss": 3.4407,
      "step": 9060
    },
    {
      "epoch": 4.613428280773143,
      "grad_norm": 7.182735443115234,
      "learning_rate": 4.538657171922686e-05,
      "loss": 3.4555,
      "step": 9070
    },
    {
      "epoch": 4.61851475076297,
      "grad_norm": 8.08858585357666,
      "learning_rate": 4.538148524923703e-05,
      "loss": 3.4242,
      "step": 9080
    },
    {
      "epoch": 4.623601220752797,
      "grad_norm": 9.837270736694336,
      "learning_rate": 4.53763987792472e-05,
      "loss": 3.4403,
      "step": 9090
    },
    {
      "epoch": 4.628687690742625,
      "grad_norm": 8.107197761535645,
      "learning_rate": 4.537131230925738e-05,
      "loss": 3.4998,
      "step": 9100
    },
    {
      "epoch": 4.633774160732452,
      "grad_norm": 8.040487289428711,
      "learning_rate": 4.5366225839267554e-05,
      "loss": 3.4844,
      "step": 9110
    },
    {
      "epoch": 4.638860630722279,
      "grad_norm": 8.661519050598145,
      "learning_rate": 4.5361139369277724e-05,
      "loss": 3.4773,
      "step": 9120
    },
    {
      "epoch": 4.643947100712106,
      "grad_norm": 7.067198276519775,
      "learning_rate": 4.53560528992879e-05,
      "loss": 3.4678,
      "step": 9130
    },
    {
      "epoch": 4.649033570701933,
      "grad_norm": 7.373317718505859,
      "learning_rate": 4.535096642929807e-05,
      "loss": 3.487,
      "step": 9140
    },
    {
      "epoch": 4.65412004069176,
      "grad_norm": 8.952664375305176,
      "learning_rate": 4.534587995930824e-05,
      "loss": 3.4,
      "step": 9150
    },
    {
      "epoch": 4.659206510681587,
      "grad_norm": 6.957043170928955,
      "learning_rate": 4.534079348931842e-05,
      "loss": 3.4165,
      "step": 9160
    },
    {
      "epoch": 4.664292980671414,
      "grad_norm": 7.385956764221191,
      "learning_rate": 4.5335707019328587e-05,
      "loss": 3.4986,
      "step": 9170
    },
    {
      "epoch": 4.669379450661241,
      "grad_norm": 8.847443580627441,
      "learning_rate": 4.5330620549338756e-05,
      "loss": 3.3991,
      "step": 9180
    },
    {
      "epoch": 4.674465920651068,
      "grad_norm": 8.921510696411133,
      "learning_rate": 4.532553407934893e-05,
      "loss": 3.387,
      "step": 9190
    },
    {
      "epoch": 4.679552390640895,
      "grad_norm": 5.965644359588623,
      "learning_rate": 4.532044760935911e-05,
      "loss": 3.5382,
      "step": 9200
    },
    {
      "epoch": 4.684638860630722,
      "grad_norm": 6.969622611999512,
      "learning_rate": 4.531536113936928e-05,
      "loss": 3.3679,
      "step": 9210
    },
    {
      "epoch": 4.689725330620549,
      "grad_norm": 7.57749080657959,
      "learning_rate": 4.5310274669379456e-05,
      "loss": 3.395,
      "step": 9220
    },
    {
      "epoch": 4.694811800610377,
      "grad_norm": 6.542710304260254,
      "learning_rate": 4.5305188199389626e-05,
      "loss": 3.4391,
      "step": 9230
    },
    {
      "epoch": 4.699898270600204,
      "grad_norm": 7.5784807205200195,
      "learning_rate": 4.5300101729399796e-05,
      "loss": 3.4806,
      "step": 9240
    },
    {
      "epoch": 4.704984740590031,
      "grad_norm": 6.456599712371826,
      "learning_rate": 4.529501525940997e-05,
      "loss": 3.5053,
      "step": 9250
    },
    {
      "epoch": 4.710071210579858,
      "grad_norm": 6.538114547729492,
      "learning_rate": 4.528992878942014e-05,
      "loss": 3.4178,
      "step": 9260
    },
    {
      "epoch": 4.715157680569685,
      "grad_norm": 8.753795623779297,
      "learning_rate": 4.528484231943031e-05,
      "loss": 3.4396,
      "step": 9270
    },
    {
      "epoch": 4.720244150559512,
      "grad_norm": 8.9151611328125,
      "learning_rate": 4.527975584944049e-05,
      "loss": 3.4548,
      "step": 9280
    },
    {
      "epoch": 4.725330620549339,
      "grad_norm": 6.75811243057251,
      "learning_rate": 4.5274669379450666e-05,
      "loss": 3.4676,
      "step": 9290
    },
    {
      "epoch": 4.730417090539166,
      "grad_norm": 5.821547031402588,
      "learning_rate": 4.5269582909460836e-05,
      "loss": 3.4346,
      "step": 9300
    },
    {
      "epoch": 4.735503560528993,
      "grad_norm": 5.982168197631836,
      "learning_rate": 4.526449643947101e-05,
      "loss": 3.4206,
      "step": 9310
    },
    {
      "epoch": 4.7405900305188196,
      "grad_norm": 8.34958267211914,
      "learning_rate": 4.525940996948118e-05,
      "loss": 3.4191,
      "step": 9320
    },
    {
      "epoch": 4.745676500508647,
      "grad_norm": 7.6805901527404785,
      "learning_rate": 4.525432349949136e-05,
      "loss": 3.3965,
      "step": 9330
    },
    {
      "epoch": 4.750762970498474,
      "grad_norm": 8.454001426696777,
      "learning_rate": 4.524923702950153e-05,
      "loss": 3.3419,
      "step": 9340
    },
    {
      "epoch": 4.755849440488301,
      "grad_norm": 8.188343048095703,
      "learning_rate": 4.52441505595117e-05,
      "loss": 3.5124,
      "step": 9350
    },
    {
      "epoch": 4.760935910478128,
      "grad_norm": 8.950751304626465,
      "learning_rate": 4.5239064089521876e-05,
      "loss": 3.4223,
      "step": 9360
    },
    {
      "epoch": 4.766022380467955,
      "grad_norm": 9.847806930541992,
      "learning_rate": 4.5233977619532045e-05,
      "loss": 3.3539,
      "step": 9370
    },
    {
      "epoch": 4.771108850457782,
      "grad_norm": 6.817256927490234,
      "learning_rate": 4.5228891149542215e-05,
      "loss": 3.4986,
      "step": 9380
    },
    {
      "epoch": 4.776195320447609,
      "grad_norm": 8.241292953491211,
      "learning_rate": 4.522380467955239e-05,
      "loss": 3.4582,
      "step": 9390
    },
    {
      "epoch": 4.781281790437436,
      "grad_norm": 7.211737155914307,
      "learning_rate": 4.521871820956257e-05,
      "loss": 3.398,
      "step": 9400
    },
    {
      "epoch": 4.786368260427263,
      "grad_norm": 10.099587440490723,
      "learning_rate": 4.521363173957274e-05,
      "loss": 3.4068,
      "step": 9410
    },
    {
      "epoch": 4.79145473041709,
      "grad_norm": 9.291696548461914,
      "learning_rate": 4.5208545269582915e-05,
      "loss": 3.3966,
      "step": 9420
    },
    {
      "epoch": 4.796541200406917,
      "grad_norm": 8.879063606262207,
      "learning_rate": 4.5203458799593085e-05,
      "loss": 3.3861,
      "step": 9430
    },
    {
      "epoch": 4.801627670396744,
      "grad_norm": 5.990861415863037,
      "learning_rate": 4.5198372329603255e-05,
      "loss": 3.3656,
      "step": 9440
    },
    {
      "epoch": 4.806714140386572,
      "grad_norm": 10.332392692565918,
      "learning_rate": 4.519328585961343e-05,
      "loss": 3.4341,
      "step": 9450
    },
    {
      "epoch": 4.811800610376399,
      "grad_norm": 9.404748916625977,
      "learning_rate": 4.51881993896236e-05,
      "loss": 3.3905,
      "step": 9460
    },
    {
      "epoch": 4.816887080366226,
      "grad_norm": 9.398710250854492,
      "learning_rate": 4.518311291963377e-05,
      "loss": 3.3496,
      "step": 9470
    },
    {
      "epoch": 4.821973550356053,
      "grad_norm": 10.12822151184082,
      "learning_rate": 4.517802644964395e-05,
      "loss": 3.4069,
      "step": 9480
    },
    {
      "epoch": 4.82706002034588,
      "grad_norm": 8.296046257019043,
      "learning_rate": 4.5172939979654125e-05,
      "loss": 3.4047,
      "step": 9490
    },
    {
      "epoch": 4.832146490335707,
      "grad_norm": 7.27692985534668,
      "learning_rate": 4.5167853509664295e-05,
      "loss": 3.4799,
      "step": 9500
    },
    {
      "epoch": 4.837232960325534,
      "grad_norm": 6.795954704284668,
      "learning_rate": 4.516276703967447e-05,
      "loss": 3.3804,
      "step": 9510
    },
    {
      "epoch": 4.842319430315361,
      "grad_norm": 7.342233657836914,
      "learning_rate": 4.515768056968464e-05,
      "loss": 3.471,
      "step": 9520
    },
    {
      "epoch": 4.847405900305188,
      "grad_norm": 5.689978122711182,
      "learning_rate": 4.515259409969481e-05,
      "loss": 3.458,
      "step": 9530
    },
    {
      "epoch": 4.852492370295015,
      "grad_norm": 9.231182098388672,
      "learning_rate": 4.514750762970499e-05,
      "loss": 3.3644,
      "step": 9540
    },
    {
      "epoch": 4.857578840284843,
      "grad_norm": 6.887257099151611,
      "learning_rate": 4.514242115971516e-05,
      "loss": 3.4188,
      "step": 9550
    },
    {
      "epoch": 4.86266531027467,
      "grad_norm": 9.625636100769043,
      "learning_rate": 4.513733468972533e-05,
      "loss": 3.3807,
      "step": 9560
    },
    {
      "epoch": 4.867751780264497,
      "grad_norm": 7.341774940490723,
      "learning_rate": 4.5132248219735504e-05,
      "loss": 3.4628,
      "step": 9570
    },
    {
      "epoch": 4.872838250254324,
      "grad_norm": 9.670441627502441,
      "learning_rate": 4.512716174974568e-05,
      "loss": 3.3221,
      "step": 9580
    },
    {
      "epoch": 4.877924720244151,
      "grad_norm": 8.147854804992676,
      "learning_rate": 4.512207527975586e-05,
      "loss": 3.4089,
      "step": 9590
    },
    {
      "epoch": 4.883011190233978,
      "grad_norm": 11.208895683288574,
      "learning_rate": 4.511698880976603e-05,
      "loss": 3.4554,
      "step": 9600
    },
    {
      "epoch": 4.888097660223805,
      "grad_norm": 7.076759338378906,
      "learning_rate": 4.51119023397762e-05,
      "loss": 3.4768,
      "step": 9610
    },
    {
      "epoch": 4.893184130213632,
      "grad_norm": 7.467263221740723,
      "learning_rate": 4.5106815869786374e-05,
      "loss": 3.4078,
      "step": 9620
    },
    {
      "epoch": 4.898270600203459,
      "grad_norm": 5.993905067443848,
      "learning_rate": 4.5101729399796544e-05,
      "loss": 3.3875,
      "step": 9630
    },
    {
      "epoch": 4.903357070193286,
      "grad_norm": 8.5457124710083,
      "learning_rate": 4.5096642929806714e-05,
      "loss": 3.4183,
      "step": 9640
    },
    {
      "epoch": 4.908443540183113,
      "grad_norm": 8.266600608825684,
      "learning_rate": 4.509155645981689e-05,
      "loss": 3.4442,
      "step": 9650
    },
    {
      "epoch": 4.9135300101729396,
      "grad_norm": 7.88046932220459,
      "learning_rate": 4.508646998982706e-05,
      "loss": 3.3544,
      "step": 9660
    },
    {
      "epoch": 4.918616480162767,
      "grad_norm": 9.515092849731445,
      "learning_rate": 4.508138351983723e-05,
      "loss": 3.5051,
      "step": 9670
    },
    {
      "epoch": 4.923702950152594,
      "grad_norm": 10.328898429870605,
      "learning_rate": 4.507629704984741e-05,
      "loss": 3.3576,
      "step": 9680
    },
    {
      "epoch": 4.928789420142421,
      "grad_norm": 8.191904067993164,
      "learning_rate": 4.5071210579857584e-05,
      "loss": 3.4015,
      "step": 9690
    },
    {
      "epoch": 4.933875890132248,
      "grad_norm": 9.719797134399414,
      "learning_rate": 4.5066124109867754e-05,
      "loss": 3.4232,
      "step": 9700
    },
    {
      "epoch": 4.938962360122075,
      "grad_norm": 8.77003002166748,
      "learning_rate": 4.506103763987793e-05,
      "loss": 3.316,
      "step": 9710
    },
    {
      "epoch": 4.944048830111902,
      "grad_norm": 7.335244178771973,
      "learning_rate": 4.50559511698881e-05,
      "loss": 3.4611,
      "step": 9720
    },
    {
      "epoch": 4.949135300101729,
      "grad_norm": 6.774808883666992,
      "learning_rate": 4.505086469989827e-05,
      "loss": 3.3914,
      "step": 9730
    },
    {
      "epoch": 4.954221770091556,
      "grad_norm": 6.645503044128418,
      "learning_rate": 4.504577822990845e-05,
      "loss": 3.3872,
      "step": 9740
    },
    {
      "epoch": 4.959308240081383,
      "grad_norm": 8.210997581481934,
      "learning_rate": 4.5040691759918617e-05,
      "loss": 3.3891,
      "step": 9750
    },
    {
      "epoch": 4.96439471007121,
      "grad_norm": 7.470862865447998,
      "learning_rate": 4.5035605289928786e-05,
      "loss": 3.4412,
      "step": 9760
    },
    {
      "epoch": 4.969481180061038,
      "grad_norm": 7.658910274505615,
      "learning_rate": 4.503051881993896e-05,
      "loss": 3.4044,
      "step": 9770
    },
    {
      "epoch": 4.974567650050865,
      "grad_norm": 9.376871109008789,
      "learning_rate": 4.502543234994914e-05,
      "loss": 3.2793,
      "step": 9780
    },
    {
      "epoch": 4.979654120040692,
      "grad_norm": 8.251885414123535,
      "learning_rate": 4.502034587995931e-05,
      "loss": 3.398,
      "step": 9790
    },
    {
      "epoch": 4.984740590030519,
      "grad_norm": 7.661507606506348,
      "learning_rate": 4.5015259409969486e-05,
      "loss": 3.3721,
      "step": 9800
    },
    {
      "epoch": 4.989827060020346,
      "grad_norm": 7.2751784324646,
      "learning_rate": 4.5010172939979656e-05,
      "loss": 3.4614,
      "step": 9810
    },
    {
      "epoch": 4.994913530010173,
      "grad_norm": 7.3902459144592285,
      "learning_rate": 4.5005086469989826e-05,
      "loss": 3.4271,
      "step": 9820
    },
    {
      "epoch": 5.0,
      "grad_norm": 13.9072847366333,
      "learning_rate": 4.5e-05,
      "loss": 3.3877,
      "step": 9830
    },
    {
      "epoch": 5.0,
      "eval_loss": 3.6719284057617188,
      "eval_runtime": 2.7431,
      "eval_samples_per_second": 1011.623,
      "eval_steps_per_second": 126.498,
      "step": 9830
    },
    {
      "epoch": 5.005086469989827,
      "grad_norm": 7.9567790031433105,
      "learning_rate": 4.499491353001017e-05,
      "loss": 3.3439,
      "step": 9840
    },
    {
      "epoch": 5.010172939979654,
      "grad_norm": 8.787237167358398,
      "learning_rate": 4.498982706002034e-05,
      "loss": 3.4455,
      "step": 9850
    },
    {
      "epoch": 5.015259409969481,
      "grad_norm": 8.89844036102295,
      "learning_rate": 4.498474059003052e-05,
      "loss": 3.3618,
      "step": 9860
    },
    {
      "epoch": 5.020345879959308,
      "grad_norm": 6.023423671722412,
      "learning_rate": 4.4979654120040696e-05,
      "loss": 3.3615,
      "step": 9870
    },
    {
      "epoch": 5.025432349949135,
      "grad_norm": 8.555155754089355,
      "learning_rate": 4.497456765005087e-05,
      "loss": 3.3757,
      "step": 9880
    },
    {
      "epoch": 5.030518819938963,
      "grad_norm": 8.894885063171387,
      "learning_rate": 4.496948118006104e-05,
      "loss": 3.4623,
      "step": 9890
    },
    {
      "epoch": 5.03560528992879,
      "grad_norm": 9.594351768493652,
      "learning_rate": 4.496439471007121e-05,
      "loss": 3.3697,
      "step": 9900
    },
    {
      "epoch": 5.040691759918617,
      "grad_norm": 14.147628784179688,
      "learning_rate": 4.495930824008139e-05,
      "loss": 3.292,
      "step": 9910
    },
    {
      "epoch": 5.045778229908444,
      "grad_norm": 7.516112327575684,
      "learning_rate": 4.495422177009156e-05,
      "loss": 3.4049,
      "step": 9920
    },
    {
      "epoch": 5.050864699898271,
      "grad_norm": 8.008264541625977,
      "learning_rate": 4.494913530010173e-05,
      "loss": 3.3821,
      "step": 9930
    },
    {
      "epoch": 5.055951169888098,
      "grad_norm": 9.882140159606934,
      "learning_rate": 4.4944048830111906e-05,
      "loss": 3.4062,
      "step": 9940
    },
    {
      "epoch": 5.061037639877925,
      "grad_norm": 13.335467338562012,
      "learning_rate": 4.4938962360122075e-05,
      "loss": 3.3017,
      "step": 9950
    },
    {
      "epoch": 5.066124109867752,
      "grad_norm": 11.549736022949219,
      "learning_rate": 4.493387589013225e-05,
      "loss": 3.4247,
      "step": 9960
    },
    {
      "epoch": 5.071210579857579,
      "grad_norm": 8.189862251281738,
      "learning_rate": 4.492878942014242e-05,
      "loss": 3.3668,
      "step": 9970
    },
    {
      "epoch": 5.076297049847406,
      "grad_norm": 8.225774765014648,
      "learning_rate": 4.49237029501526e-05,
      "loss": 3.3872,
      "step": 9980
    },
    {
      "epoch": 5.081383519837233,
      "grad_norm": 6.003359317779541,
      "learning_rate": 4.491861648016277e-05,
      "loss": 3.4136,
      "step": 9990
    },
    {
      "epoch": 5.0864699898270604,
      "grad_norm": 8.378362655639648,
      "learning_rate": 4.4913530010172945e-05,
      "loss": 3.3772,
      "step": 10000
    },
    {
      "epoch": 5.091556459816887,
      "grad_norm": 9.08553409576416,
      "learning_rate": 4.4908443540183115e-05,
      "loss": 3.4507,
      "step": 10010
    },
    {
      "epoch": 5.096642929806714,
      "grad_norm": 7.368302345275879,
      "learning_rate": 4.4903357070193285e-05,
      "loss": 3.3575,
      "step": 10020
    },
    {
      "epoch": 5.101729399796541,
      "grad_norm": 8.320717811584473,
      "learning_rate": 4.489827060020346e-05,
      "loss": 3.3418,
      "step": 10030
    },
    {
      "epoch": 5.106815869786368,
      "grad_norm": 8.783220291137695,
      "learning_rate": 4.489318413021363e-05,
      "loss": 3.4001,
      "step": 10040
    },
    {
      "epoch": 5.111902339776195,
      "grad_norm": 7.794065952301025,
      "learning_rate": 4.48880976602238e-05,
      "loss": 3.453,
      "step": 10050
    },
    {
      "epoch": 5.116988809766022,
      "grad_norm": 8.192094802856445,
      "learning_rate": 4.488301119023398e-05,
      "loss": 3.3681,
      "step": 10060
    },
    {
      "epoch": 5.122075279755849,
      "grad_norm": 7.488438606262207,
      "learning_rate": 4.4877924720244155e-05,
      "loss": 3.4124,
      "step": 10070
    },
    {
      "epoch": 5.127161749745676,
      "grad_norm": 9.813347816467285,
      "learning_rate": 4.4872838250254325e-05,
      "loss": 3.3748,
      "step": 10080
    },
    {
      "epoch": 5.132248219735503,
      "grad_norm": 7.967130184173584,
      "learning_rate": 4.48677517802645e-05,
      "loss": 3.41,
      "step": 10090
    },
    {
      "epoch": 5.13733468972533,
      "grad_norm": 6.782693386077881,
      "learning_rate": 4.486266531027467e-05,
      "loss": 3.3778,
      "step": 10100
    },
    {
      "epoch": 5.142421159715157,
      "grad_norm": 7.84092378616333,
      "learning_rate": 4.485757884028484e-05,
      "loss": 3.4061,
      "step": 10110
    },
    {
      "epoch": 5.147507629704985,
      "grad_norm": 5.829002380371094,
      "learning_rate": 4.485249237029502e-05,
      "loss": 3.2953,
      "step": 10120
    },
    {
      "epoch": 5.152594099694812,
      "grad_norm": 6.801539897918701,
      "learning_rate": 4.484740590030519e-05,
      "loss": 3.3162,
      "step": 10130
    },
    {
      "epoch": 5.157680569684639,
      "grad_norm": 8.075284004211426,
      "learning_rate": 4.4842319430315364e-05,
      "loss": 3.3222,
      "step": 10140
    },
    {
      "epoch": 5.162767039674466,
      "grad_norm": 9.009367942810059,
      "learning_rate": 4.4837232960325534e-05,
      "loss": 3.3519,
      "step": 10150
    },
    {
      "epoch": 5.167853509664293,
      "grad_norm": 9.120413780212402,
      "learning_rate": 4.483214649033571e-05,
      "loss": 3.3542,
      "step": 10160
    },
    {
      "epoch": 5.17293997965412,
      "grad_norm": 7.851102352142334,
      "learning_rate": 4.482706002034589e-05,
      "loss": 3.4059,
      "step": 10170
    },
    {
      "epoch": 5.178026449643947,
      "grad_norm": 8.88165283203125,
      "learning_rate": 4.482197355035606e-05,
      "loss": 3.344,
      "step": 10180
    },
    {
      "epoch": 5.183112919633774,
      "grad_norm": 8.889517784118652,
      "learning_rate": 4.481688708036623e-05,
      "loss": 3.4032,
      "step": 10190
    },
    {
      "epoch": 5.188199389623601,
      "grad_norm": 8.39539623260498,
      "learning_rate": 4.4811800610376404e-05,
      "loss": 3.3277,
      "step": 10200
    },
    {
      "epoch": 5.193285859613428,
      "grad_norm": 8.665796279907227,
      "learning_rate": 4.4806714140386574e-05,
      "loss": 3.4252,
      "step": 10210
    },
    {
      "epoch": 5.198372329603256,
      "grad_norm": 10.705954551696777,
      "learning_rate": 4.4801627670396744e-05,
      "loss": 3.3138,
      "step": 10220
    },
    {
      "epoch": 5.203458799593083,
      "grad_norm": 10.920796394348145,
      "learning_rate": 4.479654120040692e-05,
      "loss": 3.3161,
      "step": 10230
    },
    {
      "epoch": 5.20854526958291,
      "grad_norm": 9.953409194946289,
      "learning_rate": 4.479145473041709e-05,
      "loss": 3.3515,
      "step": 10240
    },
    {
      "epoch": 5.213631739572737,
      "grad_norm": 9.429926872253418,
      "learning_rate": 4.478636826042727e-05,
      "loss": 3.3914,
      "step": 10250
    },
    {
      "epoch": 5.218718209562564,
      "grad_norm": 10.035264015197754,
      "learning_rate": 4.4781281790437444e-05,
      "loss": 3.3721,
      "step": 10260
    },
    {
      "epoch": 5.223804679552391,
      "grad_norm": 10.723214149475098,
      "learning_rate": 4.4776195320447614e-05,
      "loss": 3.3597,
      "step": 10270
    },
    {
      "epoch": 5.228891149542218,
      "grad_norm": 9.610581398010254,
      "learning_rate": 4.4771108850457784e-05,
      "loss": 3.4669,
      "step": 10280
    },
    {
      "epoch": 5.233977619532045,
      "grad_norm": 9.203573226928711,
      "learning_rate": 4.476602238046796e-05,
      "loss": 3.3818,
      "step": 10290
    },
    {
      "epoch": 5.239064089521872,
      "grad_norm": 9.906189918518066,
      "learning_rate": 4.476093591047813e-05,
      "loss": 3.3805,
      "step": 10300
    },
    {
      "epoch": 5.244150559511699,
      "grad_norm": 7.550098896026611,
      "learning_rate": 4.47558494404883e-05,
      "loss": 3.3852,
      "step": 10310
    },
    {
      "epoch": 5.249237029501526,
      "grad_norm": 8.183079719543457,
      "learning_rate": 4.475076297049848e-05,
      "loss": 3.4084,
      "step": 10320
    },
    {
      "epoch": 5.254323499491353,
      "grad_norm": 10.722709655761719,
      "learning_rate": 4.4745676500508647e-05,
      "loss": 3.3396,
      "step": 10330
    },
    {
      "epoch": 5.2594099694811804,
      "grad_norm": 8.980326652526855,
      "learning_rate": 4.4740590030518816e-05,
      "loss": 3.4066,
      "step": 10340
    },
    {
      "epoch": 5.264496439471007,
      "grad_norm": 9.439671516418457,
      "learning_rate": 4.473550356052899e-05,
      "loss": 3.3253,
      "step": 10350
    },
    {
      "epoch": 5.269582909460834,
      "grad_norm": 5.883491039276123,
      "learning_rate": 4.473041709053917e-05,
      "loss": 3.3322,
      "step": 10360
    },
    {
      "epoch": 5.274669379450661,
      "grad_norm": 7.728847503662109,
      "learning_rate": 4.472533062054934e-05,
      "loss": 3.3758,
      "step": 10370
    },
    {
      "epoch": 5.279755849440488,
      "grad_norm": 7.2566938400268555,
      "learning_rate": 4.4720244150559516e-05,
      "loss": 3.3584,
      "step": 10380
    },
    {
      "epoch": 5.284842319430315,
      "grad_norm": 9.463056564331055,
      "learning_rate": 4.4715157680569686e-05,
      "loss": 3.3334,
      "step": 10390
    },
    {
      "epoch": 5.289928789420142,
      "grad_norm": 9.359456062316895,
      "learning_rate": 4.471007121057986e-05,
      "loss": 3.2811,
      "step": 10400
    },
    {
      "epoch": 5.295015259409969,
      "grad_norm": 8.876591682434082,
      "learning_rate": 4.470498474059003e-05,
      "loss": 3.4604,
      "step": 10410
    },
    {
      "epoch": 5.300101729399796,
      "grad_norm": 8.443236351013184,
      "learning_rate": 4.46998982706002e-05,
      "loss": 3.3761,
      "step": 10420
    },
    {
      "epoch": 5.305188199389623,
      "grad_norm": 7.779739856719971,
      "learning_rate": 4.469481180061038e-05,
      "loss": 3.4225,
      "step": 10430
    },
    {
      "epoch": 5.31027466937945,
      "grad_norm": 8.223272323608398,
      "learning_rate": 4.468972533062055e-05,
      "loss": 3.4115,
      "step": 10440
    },
    {
      "epoch": 5.315361139369278,
      "grad_norm": 10.142083168029785,
      "learning_rate": 4.4684638860630726e-05,
      "loss": 3.452,
      "step": 10450
    },
    {
      "epoch": 5.320447609359105,
      "grad_norm": 9.125947952270508,
      "learning_rate": 4.46795523906409e-05,
      "loss": 3.4257,
      "step": 10460
    },
    {
      "epoch": 5.325534079348932,
      "grad_norm": 7.132338523864746,
      "learning_rate": 4.467446592065107e-05,
      "loss": 3.3429,
      "step": 10470
    },
    {
      "epoch": 5.330620549338759,
      "grad_norm": 7.125019550323486,
      "learning_rate": 4.466937945066124e-05,
      "loss": 3.2622,
      "step": 10480
    },
    {
      "epoch": 5.335707019328586,
      "grad_norm": 11.581009864807129,
      "learning_rate": 4.466429298067142e-05,
      "loss": 3.351,
      "step": 10490
    },
    {
      "epoch": 5.340793489318413,
      "grad_norm": 6.582719326019287,
      "learning_rate": 4.465920651068159e-05,
      "loss": 3.2933,
      "step": 10500
    },
    {
      "epoch": 5.34587995930824,
      "grad_norm": 9.592324256896973,
      "learning_rate": 4.465412004069176e-05,
      "loss": 3.322,
      "step": 10510
    },
    {
      "epoch": 5.350966429298067,
      "grad_norm": 8.43247127532959,
      "learning_rate": 4.4649033570701936e-05,
      "loss": 3.4415,
      "step": 10520
    },
    {
      "epoch": 5.356052899287894,
      "grad_norm": 12.023277282714844,
      "learning_rate": 4.4643947100712105e-05,
      "loss": 3.3526,
      "step": 10530
    },
    {
      "epoch": 5.361139369277721,
      "grad_norm": 9.103813171386719,
      "learning_rate": 4.463886063072228e-05,
      "loss": 3.4279,
      "step": 10540
    },
    {
      "epoch": 5.366225839267548,
      "grad_norm": 6.361832618713379,
      "learning_rate": 4.463377416073246e-05,
      "loss": 3.3832,
      "step": 10550
    },
    {
      "epoch": 5.371312309257376,
      "grad_norm": 10.655050277709961,
      "learning_rate": 4.462868769074263e-05,
      "loss": 3.3426,
      "step": 10560
    },
    {
      "epoch": 5.376398779247203,
      "grad_norm": 10.224148750305176,
      "learning_rate": 4.46236012207528e-05,
      "loss": 3.3462,
      "step": 10570
    },
    {
      "epoch": 5.38148524923703,
      "grad_norm": 11.408224105834961,
      "learning_rate": 4.4618514750762975e-05,
      "loss": 3.2704,
      "step": 10580
    },
    {
      "epoch": 5.386571719226857,
      "grad_norm": 9.483844757080078,
      "learning_rate": 4.4613428280773145e-05,
      "loss": 3.3895,
      "step": 10590
    },
    {
      "epoch": 5.391658189216684,
      "grad_norm": 7.130020618438721,
      "learning_rate": 4.4608341810783315e-05,
      "loss": 3.4029,
      "step": 10600
    },
    {
      "epoch": 5.396744659206511,
      "grad_norm": 7.745676040649414,
      "learning_rate": 4.460325534079349e-05,
      "loss": 3.3698,
      "step": 10610
    },
    {
      "epoch": 5.401831129196338,
      "grad_norm": 10.487146377563477,
      "learning_rate": 4.459816887080366e-05,
      "loss": 3.2955,
      "step": 10620
    },
    {
      "epoch": 5.406917599186165,
      "grad_norm": 9.193432807922363,
      "learning_rate": 4.459308240081383e-05,
      "loss": 3.3925,
      "step": 10630
    },
    {
      "epoch": 5.412004069175992,
      "grad_norm": 8.540753364562988,
      "learning_rate": 4.458799593082401e-05,
      "loss": 3.3679,
      "step": 10640
    },
    {
      "epoch": 5.417090539165819,
      "grad_norm": 10.630615234375,
      "learning_rate": 4.4582909460834185e-05,
      "loss": 3.3183,
      "step": 10650
    },
    {
      "epoch": 5.422177009155646,
      "grad_norm": 8.636068344116211,
      "learning_rate": 4.4577822990844355e-05,
      "loss": 3.3556,
      "step": 10660
    },
    {
      "epoch": 5.4272634791454735,
      "grad_norm": 9.008451461791992,
      "learning_rate": 4.457273652085453e-05,
      "loss": 3.3082,
      "step": 10670
    },
    {
      "epoch": 5.4323499491353004,
      "grad_norm": 7.712475299835205,
      "learning_rate": 4.45676500508647e-05,
      "loss": 3.327,
      "step": 10680
    },
    {
      "epoch": 5.437436419125127,
      "grad_norm": 8.044414520263672,
      "learning_rate": 4.456256358087488e-05,
      "loss": 3.4002,
      "step": 10690
    },
    {
      "epoch": 5.442522889114954,
      "grad_norm": 8.008727073669434,
      "learning_rate": 4.455747711088505e-05,
      "loss": 3.333,
      "step": 10700
    },
    {
      "epoch": 5.447609359104781,
      "grad_norm": 6.735044002532959,
      "learning_rate": 4.455239064089522e-05,
      "loss": 3.3364,
      "step": 10710
    },
    {
      "epoch": 5.452695829094608,
      "grad_norm": 9.523913383483887,
      "learning_rate": 4.4547304170905394e-05,
      "loss": 3.3695,
      "step": 10720
    },
    {
      "epoch": 5.457782299084435,
      "grad_norm": 9.524142265319824,
      "learning_rate": 4.4542217700915564e-05,
      "loss": 3.2907,
      "step": 10730
    },
    {
      "epoch": 5.462868769074262,
      "grad_norm": 9.6463623046875,
      "learning_rate": 4.453713123092574e-05,
      "loss": 3.2932,
      "step": 10740
    },
    {
      "epoch": 5.467955239064089,
      "grad_norm": 9.36671257019043,
      "learning_rate": 4.453204476093592e-05,
      "loss": 3.3517,
      "step": 10750
    },
    {
      "epoch": 5.473041709053916,
      "grad_norm": 9.570650100708008,
      "learning_rate": 4.452695829094609e-05,
      "loss": 3.3564,
      "step": 10760
    },
    {
      "epoch": 5.478128179043743,
      "grad_norm": 10.900553703308105,
      "learning_rate": 4.452187182095626e-05,
      "loss": 3.395,
      "step": 10770
    },
    {
      "epoch": 5.48321464903357,
      "grad_norm": 8.267682075500488,
      "learning_rate": 4.4516785350966434e-05,
      "loss": 3.3565,
      "step": 10780
    },
    {
      "epoch": 5.488301119023398,
      "grad_norm": 6.798012733459473,
      "learning_rate": 4.4511698880976604e-05,
      "loss": 3.3898,
      "step": 10790
    },
    {
      "epoch": 5.493387589013225,
      "grad_norm": 9.496233940124512,
      "learning_rate": 4.4506612410986774e-05,
      "loss": 3.3927,
      "step": 10800
    },
    {
      "epoch": 5.498474059003052,
      "grad_norm": 9.266633987426758,
      "learning_rate": 4.450152594099695e-05,
      "loss": 3.2898,
      "step": 10810
    },
    {
      "epoch": 5.503560528992879,
      "grad_norm": 8.694616317749023,
      "learning_rate": 4.449643947100712e-05,
      "loss": 3.3541,
      "step": 10820
    },
    {
      "epoch": 5.508646998982706,
      "grad_norm": 10.764841079711914,
      "learning_rate": 4.44913530010173e-05,
      "loss": 3.3413,
      "step": 10830
    },
    {
      "epoch": 5.513733468972533,
      "grad_norm": 12.022740364074707,
      "learning_rate": 4.4486266531027474e-05,
      "loss": 3.291,
      "step": 10840
    },
    {
      "epoch": 5.51881993896236,
      "grad_norm": 7.828083038330078,
      "learning_rate": 4.4481180061037644e-05,
      "loss": 3.2283,
      "step": 10850
    },
    {
      "epoch": 5.523906408952187,
      "grad_norm": 9.341636657714844,
      "learning_rate": 4.4476093591047814e-05,
      "loss": 3.3576,
      "step": 10860
    },
    {
      "epoch": 5.528992878942014,
      "grad_norm": 9.401507377624512,
      "learning_rate": 4.447100712105799e-05,
      "loss": 3.3562,
      "step": 10870
    },
    {
      "epoch": 5.534079348931841,
      "grad_norm": 10.16762924194336,
      "learning_rate": 4.446592065106816e-05,
      "loss": 3.2606,
      "step": 10880
    },
    {
      "epoch": 5.539165818921669,
      "grad_norm": 9.584733963012695,
      "learning_rate": 4.446083418107833e-05,
      "loss": 3.3123,
      "step": 10890
    },
    {
      "epoch": 5.544252288911496,
      "grad_norm": 10.775392532348633,
      "learning_rate": 4.445574771108851e-05,
      "loss": 3.3168,
      "step": 10900
    },
    {
      "epoch": 5.549338758901323,
      "grad_norm": 10.052096366882324,
      "learning_rate": 4.4450661241098677e-05,
      "loss": 3.289,
      "step": 10910
    },
    {
      "epoch": 5.55442522889115,
      "grad_norm": 9.882708549499512,
      "learning_rate": 4.444557477110885e-05,
      "loss": 3.2876,
      "step": 10920
    },
    {
      "epoch": 5.559511698880977,
      "grad_norm": 12.903475761413574,
      "learning_rate": 4.444048830111902e-05,
      "loss": 3.2578,
      "step": 10930
    },
    {
      "epoch": 5.564598168870804,
      "grad_norm": 7.134469509124756,
      "learning_rate": 4.44354018311292e-05,
      "loss": 3.2762,
      "step": 10940
    },
    {
      "epoch": 5.569684638860631,
      "grad_norm": 10.457711219787598,
      "learning_rate": 4.4430315361139376e-05,
      "loss": 3.3346,
      "step": 10950
    },
    {
      "epoch": 5.574771108850458,
      "grad_norm": 11.453181266784668,
      "learning_rate": 4.4425228891149546e-05,
      "loss": 3.3907,
      "step": 10960
    },
    {
      "epoch": 5.579857578840285,
      "grad_norm": 8.242410659790039,
      "learning_rate": 4.4420142421159716e-05,
      "loss": 3.3826,
      "step": 10970
    },
    {
      "epoch": 5.584944048830112,
      "grad_norm": 10.822739601135254,
      "learning_rate": 4.441505595116989e-05,
      "loss": 3.3547,
      "step": 10980
    },
    {
      "epoch": 5.590030518819939,
      "grad_norm": 11.870401382446289,
      "learning_rate": 4.440996948118006e-05,
      "loss": 3.3461,
      "step": 10990
    },
    {
      "epoch": 5.595116988809766,
      "grad_norm": 7.754425525665283,
      "learning_rate": 4.440488301119023e-05,
      "loss": 3.3348,
      "step": 11000
    },
    {
      "epoch": 5.6002034587995935,
      "grad_norm": 10.82386302947998,
      "learning_rate": 4.439979654120041e-05,
      "loss": 3.3586,
      "step": 11010
    },
    {
      "epoch": 5.6052899287894205,
      "grad_norm": 8.672690391540527,
      "learning_rate": 4.439471007121058e-05,
      "loss": 3.2891,
      "step": 11020
    },
    {
      "epoch": 5.610376398779247,
      "grad_norm": 10.940115928649902,
      "learning_rate": 4.4389623601220756e-05,
      "loss": 3.3483,
      "step": 11030
    },
    {
      "epoch": 5.615462868769074,
      "grad_norm": 8.015664100646973,
      "learning_rate": 4.438453713123093e-05,
      "loss": 3.4545,
      "step": 11040
    },
    {
      "epoch": 5.620549338758901,
      "grad_norm": 8.609061241149902,
      "learning_rate": 4.43794506612411e-05,
      "loss": 3.3364,
      "step": 11050
    },
    {
      "epoch": 5.625635808748728,
      "grad_norm": 8.95185375213623,
      "learning_rate": 4.437436419125127e-05,
      "loss": 3.3531,
      "step": 11060
    },
    {
      "epoch": 5.630722278738555,
      "grad_norm": 7.477101802825928,
      "learning_rate": 4.436927772126145e-05,
      "loss": 3.3797,
      "step": 11070
    },
    {
      "epoch": 5.635808748728382,
      "grad_norm": 11.06296443939209,
      "learning_rate": 4.436419125127162e-05,
      "loss": 3.2687,
      "step": 11080
    },
    {
      "epoch": 5.640895218718209,
      "grad_norm": 7.0605597496032715,
      "learning_rate": 4.435910478128179e-05,
      "loss": 3.3735,
      "step": 11090
    },
    {
      "epoch": 5.645981688708036,
      "grad_norm": 9.048935890197754,
      "learning_rate": 4.4354018311291966e-05,
      "loss": 3.3826,
      "step": 11100
    },
    {
      "epoch": 5.651068158697864,
      "grad_norm": 9.549798011779785,
      "learning_rate": 4.4348931841302135e-05,
      "loss": 3.3,
      "step": 11110
    },
    {
      "epoch": 5.656154628687691,
      "grad_norm": 8.588309288024902,
      "learning_rate": 4.434384537131231e-05,
      "loss": 3.3629,
      "step": 11120
    },
    {
      "epoch": 5.661241098677518,
      "grad_norm": 8.058106422424316,
      "learning_rate": 4.433875890132249e-05,
      "loss": 3.4014,
      "step": 11130
    },
    {
      "epoch": 5.666327568667345,
      "grad_norm": 10.738755226135254,
      "learning_rate": 4.433367243133266e-05,
      "loss": 3.3039,
      "step": 11140
    },
    {
      "epoch": 5.671414038657172,
      "grad_norm": 10.400997161865234,
      "learning_rate": 4.432858596134283e-05,
      "loss": 3.2921,
      "step": 11150
    },
    {
      "epoch": 5.676500508646999,
      "grad_norm": 8.08258056640625,
      "learning_rate": 4.4323499491353005e-05,
      "loss": 3.2978,
      "step": 11160
    },
    {
      "epoch": 5.681586978636826,
      "grad_norm": 9.636003494262695,
      "learning_rate": 4.4318413021363175e-05,
      "loss": 3.3483,
      "step": 11170
    },
    {
      "epoch": 5.686673448626653,
      "grad_norm": 7.847623348236084,
      "learning_rate": 4.4313326551373345e-05,
      "loss": 3.3171,
      "step": 11180
    },
    {
      "epoch": 5.69175991861648,
      "grad_norm": 9.59643840789795,
      "learning_rate": 4.430824008138352e-05,
      "loss": 3.3215,
      "step": 11190
    },
    {
      "epoch": 5.696846388606307,
      "grad_norm": 9.010138511657715,
      "learning_rate": 4.430315361139369e-05,
      "loss": 3.3469,
      "step": 11200
    },
    {
      "epoch": 5.701932858596134,
      "grad_norm": 10.77082633972168,
      "learning_rate": 4.429806714140387e-05,
      "loss": 3.2313,
      "step": 11210
    },
    {
      "epoch": 5.707019328585961,
      "grad_norm": 8.151232719421387,
      "learning_rate": 4.4292980671414045e-05,
      "loss": 3.3291,
      "step": 11220
    },
    {
      "epoch": 5.712105798575788,
      "grad_norm": 9.16970443725586,
      "learning_rate": 4.4287894201424215e-05,
      "loss": 3.2572,
      "step": 11230
    },
    {
      "epoch": 5.717192268565616,
      "grad_norm": 11.333077430725098,
      "learning_rate": 4.428280773143439e-05,
      "loss": 3.3699,
      "step": 11240
    },
    {
      "epoch": 5.722278738555443,
      "grad_norm": 11.12392807006836,
      "learning_rate": 4.427772126144456e-05,
      "loss": 3.2718,
      "step": 11250
    },
    {
      "epoch": 5.72736520854527,
      "grad_norm": 9.172706604003906,
      "learning_rate": 4.427263479145473e-05,
      "loss": 3.359,
      "step": 11260
    },
    {
      "epoch": 5.732451678535097,
      "grad_norm": 11.577558517456055,
      "learning_rate": 4.426754832146491e-05,
      "loss": 3.2735,
      "step": 11270
    },
    {
      "epoch": 5.737538148524924,
      "grad_norm": 11.478280067443848,
      "learning_rate": 4.426246185147508e-05,
      "loss": 3.3528,
      "step": 11280
    },
    {
      "epoch": 5.742624618514751,
      "grad_norm": 13.171431541442871,
      "learning_rate": 4.425737538148525e-05,
      "loss": 3.2745,
      "step": 11290
    },
    {
      "epoch": 5.747711088504578,
      "grad_norm": 8.938261985778809,
      "learning_rate": 4.4252288911495424e-05,
      "loss": 3.3341,
      "step": 11300
    },
    {
      "epoch": 5.752797558494405,
      "grad_norm": 10.897587776184082,
      "learning_rate": 4.4247202441505594e-05,
      "loss": 3.2312,
      "step": 11310
    },
    {
      "epoch": 5.757884028484232,
      "grad_norm": 10.139737129211426,
      "learning_rate": 4.424211597151577e-05,
      "loss": 3.3423,
      "step": 11320
    },
    {
      "epoch": 5.762970498474059,
      "grad_norm": 10.771903991699219,
      "learning_rate": 4.423702950152595e-05,
      "loss": 3.3103,
      "step": 11330
    },
    {
      "epoch": 5.7680569684638865,
      "grad_norm": 9.907981872558594,
      "learning_rate": 4.423194303153612e-05,
      "loss": 3.3144,
      "step": 11340
    },
    {
      "epoch": 5.7731434384537135,
      "grad_norm": 8.606064796447754,
      "learning_rate": 4.422685656154629e-05,
      "loss": 3.288,
      "step": 11350
    },
    {
      "epoch": 5.7782299084435405,
      "grad_norm": 8.361233711242676,
      "learning_rate": 4.4221770091556464e-05,
      "loss": 3.3832,
      "step": 11360
    },
    {
      "epoch": 5.783316378433367,
      "grad_norm": 8.858541488647461,
      "learning_rate": 4.4216683621566634e-05,
      "loss": 3.2854,
      "step": 11370
    },
    {
      "epoch": 5.788402848423194,
      "grad_norm": 8.017171859741211,
      "learning_rate": 4.4211597151576804e-05,
      "loss": 3.2785,
      "step": 11380
    },
    {
      "epoch": 5.793489318413021,
      "grad_norm": 11.969258308410645,
      "learning_rate": 4.420651068158698e-05,
      "loss": 3.2457,
      "step": 11390
    },
    {
      "epoch": 5.798575788402848,
      "grad_norm": 10.202942848205566,
      "learning_rate": 4.420142421159715e-05,
      "loss": 3.3004,
      "step": 11400
    },
    {
      "epoch": 5.803662258392675,
      "grad_norm": 10.957554817199707,
      "learning_rate": 4.419633774160733e-05,
      "loss": 3.2722,
      "step": 11410
    },
    {
      "epoch": 5.808748728382502,
      "grad_norm": 8.270360946655273,
      "learning_rate": 4.4191251271617504e-05,
      "loss": 3.2521,
      "step": 11420
    },
    {
      "epoch": 5.813835198372329,
      "grad_norm": 8.591361999511719,
      "learning_rate": 4.4186164801627674e-05,
      "loss": 3.3187,
      "step": 11430
    },
    {
      "epoch": 5.818921668362156,
      "grad_norm": 13.481123924255371,
      "learning_rate": 4.4181078331637844e-05,
      "loss": 3.305,
      "step": 11440
    },
    {
      "epoch": 5.824008138351983,
      "grad_norm": 14.472496032714844,
      "learning_rate": 4.417599186164802e-05,
      "loss": 3.2616,
      "step": 11450
    },
    {
      "epoch": 5.829094608341811,
      "grad_norm": 9.587061882019043,
      "learning_rate": 4.417090539165819e-05,
      "loss": 3.3253,
      "step": 11460
    },
    {
      "epoch": 5.834181078331638,
      "grad_norm": 12.283282279968262,
      "learning_rate": 4.416581892166836e-05,
      "loss": 3.2938,
      "step": 11470
    },
    {
      "epoch": 5.839267548321465,
      "grad_norm": 9.572628021240234,
      "learning_rate": 4.416073245167854e-05,
      "loss": 3.2496,
      "step": 11480
    },
    {
      "epoch": 5.844354018311292,
      "grad_norm": 9.23408031463623,
      "learning_rate": 4.4155645981688707e-05,
      "loss": 3.3274,
      "step": 11490
    },
    {
      "epoch": 5.849440488301119,
      "grad_norm": 14.417545318603516,
      "learning_rate": 4.415055951169888e-05,
      "loss": 3.2937,
      "step": 11500
    },
    {
      "epoch": 5.854526958290946,
      "grad_norm": 8.926292419433594,
      "learning_rate": 4.414547304170906e-05,
      "loss": 3.294,
      "step": 11510
    },
    {
      "epoch": 5.859613428280773,
      "grad_norm": 12.539822578430176,
      "learning_rate": 4.414038657171923e-05,
      "loss": 3.2641,
      "step": 11520
    },
    {
      "epoch": 5.8646998982706,
      "grad_norm": 11.359343528747559,
      "learning_rate": 4.4135300101729407e-05,
      "loss": 3.2704,
      "step": 11530
    },
    {
      "epoch": 5.869786368260427,
      "grad_norm": 8.480549812316895,
      "learning_rate": 4.4130213631739576e-05,
      "loss": 3.3313,
      "step": 11540
    },
    {
      "epoch": 5.874872838250254,
      "grad_norm": 9.010926246643066,
      "learning_rate": 4.4125127161749746e-05,
      "loss": 3.3384,
      "step": 11550
    },
    {
      "epoch": 5.879959308240082,
      "grad_norm": 9.482748985290527,
      "learning_rate": 4.412004069175992e-05,
      "loss": 3.329,
      "step": 11560
    },
    {
      "epoch": 5.885045778229909,
      "grad_norm": 8.854823112487793,
      "learning_rate": 4.411495422177009e-05,
      "loss": 3.2712,
      "step": 11570
    },
    {
      "epoch": 5.890132248219736,
      "grad_norm": 11.604650497436523,
      "learning_rate": 4.410986775178026e-05,
      "loss": 3.3553,
      "step": 11580
    },
    {
      "epoch": 5.895218718209563,
      "grad_norm": 12.020246505737305,
      "learning_rate": 4.410478128179044e-05,
      "loss": 3.321,
      "step": 11590
    },
    {
      "epoch": 5.90030518819939,
      "grad_norm": 13.222394943237305,
      "learning_rate": 4.409969481180061e-05,
      "loss": 3.2785,
      "step": 11600
    },
    {
      "epoch": 5.905391658189217,
      "grad_norm": 8.09487247467041,
      "learning_rate": 4.4094608341810786e-05,
      "loss": 3.3072,
      "step": 11610
    },
    {
      "epoch": 5.910478128179044,
      "grad_norm": 10.938811302185059,
      "learning_rate": 4.408952187182096e-05,
      "loss": 3.2016,
      "step": 11620
    },
    {
      "epoch": 5.915564598168871,
      "grad_norm": 10.162993431091309,
      "learning_rate": 4.408443540183113e-05,
      "loss": 3.2129,
      "step": 11630
    },
    {
      "epoch": 5.920651068158698,
      "grad_norm": 9.318120002746582,
      "learning_rate": 4.40793489318413e-05,
      "loss": 3.1827,
      "step": 11640
    },
    {
      "epoch": 5.925737538148525,
      "grad_norm": 13.590348243713379,
      "learning_rate": 4.407426246185148e-05,
      "loss": 3.2526,
      "step": 11650
    },
    {
      "epoch": 5.930824008138352,
      "grad_norm": 8.646023750305176,
      "learning_rate": 4.406917599186165e-05,
      "loss": 3.1699,
      "step": 11660
    },
    {
      "epoch": 5.935910478128179,
      "grad_norm": 9.792922019958496,
      "learning_rate": 4.406408952187182e-05,
      "loss": 3.2753,
      "step": 11670
    },
    {
      "epoch": 5.940996948118006,
      "grad_norm": 10.818516731262207,
      "learning_rate": 4.4059003051881996e-05,
      "loss": 3.3989,
      "step": 11680
    },
    {
      "epoch": 5.9460834181078335,
      "grad_norm": 8.641620635986328,
      "learning_rate": 4.4053916581892165e-05,
      "loss": 3.3195,
      "step": 11690
    },
    {
      "epoch": 5.9511698880976605,
      "grad_norm": 10.680644035339355,
      "learning_rate": 4.404883011190234e-05,
      "loss": 3.2914,
      "step": 11700
    },
    {
      "epoch": 5.956256358087487,
      "grad_norm": 10.870576858520508,
      "learning_rate": 4.404374364191252e-05,
      "loss": 3.2912,
      "step": 11710
    },
    {
      "epoch": 5.961342828077314,
      "grad_norm": 9.983887672424316,
      "learning_rate": 4.403865717192269e-05,
      "loss": 3.2835,
      "step": 11720
    },
    {
      "epoch": 5.966429298067141,
      "grad_norm": 12.87192440032959,
      "learning_rate": 4.403357070193286e-05,
      "loss": 3.2574,
      "step": 11730
    },
    {
      "epoch": 5.971515768056968,
      "grad_norm": 8.533270835876465,
      "learning_rate": 4.4028484231943035e-05,
      "loss": 3.2379,
      "step": 11740
    },
    {
      "epoch": 5.976602238046795,
      "grad_norm": 9.024237632751465,
      "learning_rate": 4.4023397761953205e-05,
      "loss": 3.3316,
      "step": 11750
    },
    {
      "epoch": 5.981688708036622,
      "grad_norm": 10.567879676818848,
      "learning_rate": 4.401831129196338e-05,
      "loss": 3.2574,
      "step": 11760
    },
    {
      "epoch": 5.986775178026449,
      "grad_norm": 10.429017066955566,
      "learning_rate": 4.401322482197355e-05,
      "loss": 3.222,
      "step": 11770
    },
    {
      "epoch": 5.991861648016277,
      "grad_norm": 10.473714828491211,
      "learning_rate": 4.400813835198372e-05,
      "loss": 3.288,
      "step": 11780
    },
    {
      "epoch": 5.996948118006104,
      "grad_norm": 7.6641693115234375,
      "learning_rate": 4.40030518819939e-05,
      "loss": 3.2724,
      "step": 11790
    },
    {
      "epoch": 6.0,
      "eval_loss": 3.6637704372406006,
      "eval_runtime": 2.6533,
      "eval_samples_per_second": 1045.881,
      "eval_steps_per_second": 130.782,
      "step": 11796
    }
  ],
  "logging_steps": 10,
  "max_steps": 98300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3920998809600.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
