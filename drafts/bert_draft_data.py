# -*- coding: utf-8 -*-
"""bert_draft_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cTZjs_5u_NKzvOSHgvlP7pFjr9p_kNuT
"""

import requests

# Define the API endpoint for heroes
url = "https://api.opendota.com/api/heroes"

# Make the GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    heroes_data = response.json()

    # Optionally, print just the first few to get a feel
    print("Sample Heroes:")
    for hero in heroes_data[:5]:  # adjust the slice as needed
        print(f"Hero ID: {hero['id']}, Name: {hero['localized_name']}")
else:
    print(f"Failed to retrieve heroes. Status code: {response.status_code}")

hero_ids = set()
for hero in heroes_data:
    hero_ids.add(hero['id'])
len(hero_ids)

hero_id_to_dense = {hero_id: idx for idx, hero_id in enumerate(hero_ids)}

def build_dense_vocab(hero_ids):
    hero_ids = sorted(hero_ids)
    dense_map = {hid: idx for idx, hid in enumerate(hero_ids)}
    num_heroes = len(hero_ids)

    vocab = {}
    reverse_vocab = {}

    for hero_id, dense_id in dense_map.items():
        pick_token = f"pick_{hero_id}"
        ban_token = f"ban_{hero_id}"

        vocab[pick_token] = dense_id
        vocab[ban_token] = dense_id + num_heroes

        reverse_vocab[dense_id] = pick_token
        reverse_vocab[dense_id + num_heroes] = ban_token

    # Add special tokens
    special_tokens = {
        '[PAD]': num_heroes * 2,
        '[MASK]': num_heroes * 2 + 1,
        '[CLS]': num_heroes * 2 + 2,
        '[SEP]': num_heroes * 2 + 3
    }

    for token, token_id in special_tokens.items():
        vocab[token] = token_id
        reverse_vocab[token_id] = token

    return vocab, reverse_vocab, dense_map

vocab, reverse_vocab, dense_map = build_dense_vocab(hero_ids)

vocab['[PAD]']

class DraftTokenizer:
    def __init__(self, hero_ids):
        # Build dense map from hero_id to 0...N-1
        self.hero_ids = sorted(hero_ids)
        self.hero_id_to_dense = {hid: idx for idx, hid in enumerate(self.hero_ids)}
        self.num_heroes = len(self.hero_ids)

        # Build vocab and reverse vocab
        self.vocab = {}
        self.reverse_vocab = {}

        for hero_id, dense_id in self.hero_id_to_dense.items():
            pick_token = f"pick_{hero_id}"
            ban_token = f"ban_{hero_id}"

            self.vocab[pick_token] = dense_id
            self.vocab[ban_token] = dense_id + self.num_heroes

            self.reverse_vocab[dense_id] = pick_token
            self.reverse_vocab[dense_id + self.num_heroes] = ban_token

        # Special tokens
        self.special_tokens = {
            '[PAD]': self.num_heroes * 2,
            '[MASK]': self.num_heroes * 2 + 1,
            '[CLS]': self.num_heroes * 2 + 2,
            '[SEP]': self.num_heroes * 2 + 3
        }

        for token, token_id in self.special_tokens.items():
            self.vocab[token] = token_id
            self.reverse_vocab[token_id] = token

    def encode_action(self, action):
        """Convert a single action dict to its token ID."""
        action_type = 'pick' if action['is_pick'] else 'ban'
        hero_id = action['hero_id']
        token_str = f"{action_type}_{hero_id}"

        return {
            'token_str': token_str,
            'token_id': self.vocab.get(token_str),
            'team': action['team'],
            'order': action['order'],
            'is_pick': action['is_pick']
        }

    def encode_sequence(self, actions):
        """Convert a list of actions to a tokenized sequence."""
        sorted_actions = sorted(actions, key=lambda x: x['order'])
        return [self.encode_action(a) for a in sorted_actions if self.encode_action(a)['token_id'] is not None]

    def decode_token_id(self, token_id):
        """Convert a token ID back to its string form."""
        return self.reverse_vocab.get(token_id, '[UNK]')

    def vocab_size(self):
        return len(self.vocab)

tokenizer = DraftTokenizer(hero_ids)

import requests

# Define the API endpoint for leagues
url = "https://api.opendota.com/api/leagues"

# Make the GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    leagues_data = response.json()

    # Optionally, print just the first few to get a feel
    print("Sample Leagues:")
    for league in leagues_data[:5]:  # adjust the slice as needed
        print(f"League ID: {league['leagueid']}, Name: {league['name']}, Tier: {league.get('tier', 'N/A')}")
else:
    print(f"Failed to retrieve leagues. Status code: {response.status_code}")

leagues_data[9]



#8303157854

import requests

# Define the match ID and API endpoint
match_id = 8303157854
url = f"https://api.opendota.com/api/matches/{match_id}"

# Make the GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    match_data = response.json()
    print("Match Data:")
    print(match_data)
else:
    print(f"Failed to retrieve data. Status code: {response.status_code}")

match_data['picks_bans']



import os
import time
import json
import requests

def fetch_data_from_league(league_id, save_dir="dota_matches"):
    os.makedirs(save_dir, exist_ok=True)

    match_list_url = f"https://api.opendota.com/api/leagues/{league_id}/matches"
    match_list_response = requests.get(match_list_url)

    if match_list_response.status_code != 200:
        print(f"Failed to retrieve matches for league {league_id}. Status code: {match_list_response.status_code}")
        return []

    matches = match_list_response.json()
    match_data_list = []

    for match in matches:
        match_id = match['match_id']
        match_url = f"https://api.opendota.com/api/matches/{match_id}"
        match_response = requests.get(match_url)

        if match_response.status_code == 200:
            match_data = match_response.json()
            match_data_list.append(match_data)

            with open(f"{save_dir}/match_{match_id}.json", "w") as f:
                json.dump(match_data, f, indent=2)
            print(f"Saved match {match_id}")
        else:
            print(f"Failed to retrieve match {match_id}. Status code: {match_response.status_code}")

        time.sleep(1.5)  # Be kind to OpenDota API limits

    return match_data_list

!ls dota_matches

matches = fetch_data_from_league(18210)

for league in leagues_data:
    print(league['leagueid'])
    #fetch_data_from_league(league['leagueid'])

tokens = tokenizer.encode_sequence(match_data['picks_bans'])

tokens

[t['token_id'] for t in tokens]

def make_training_example(token_ids, max_len=32, pad_token_id=252):
    # Pad or truncate to max_len
    input_ids = token_ids[:max_len]
    input_ids += [pad_token_id] * (max_len - len(input_ids))

    # Shifted labels (next token prediction)
    labels = input_ids[1:] + [pad_token_id]

    attention_mask = [1 if token != pad_token_id else 0 for token in input_ids]

    return {
        'input_ids': input_ids,
        'labels': labels,
        'attention_mask': attention_mask
    }

from torch.utils.data import Dataset

class DraftSequenceDataset(Dataset):
    def __init__(self, list_of_token_ids, max_len=32, pad_token_id=252):
        self.examples = [make_training_example(seq, max_len, pad_token_id)
                         for seq in list_of_token_ids]

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {k: torch.tensor(v) for k, v in self.examples[idx].items()}

dataset = DraftSequenceDataset([[t['token_id'] for t in tokens]])  # wrap your one sequence in a list for now

len(vocab)

from transformers import BertConfig

config = BertConfig(
    vocab_size=256,             # set to match your tokenizer.vocab_size()
    hidden_size=128,
    num_attention_heads=4,
    num_hidden_layers=2,
    intermediate_size=512,
    max_position_embeddings=32,
    pad_token_id=252
)

from transformers import BertForMaskedLM

model = BertForMaskedLM(config)

from transformers import Trainer, TrainingArguments
import torch
training_args = TrainingArguments(
    output_dir='./draftbert',
    overwrite_output_dir=True,
    num_train_epochs=50,
    per_device_train_batch_size=1,
    save_steps=10,
    save_total_limit=1,
    logging_dir='./logs',
    logging_steps=5
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

trainer.train()

tokens[:2]



from transformers import BertConfig

config = BertConfig(
    vocab_size=256,               # from your tokenizer
    hidden_size=128,
    num_attention_heads=4,
    num_hidden_layers=4,
    intermediate_size=512,
    max_position_embeddings=32,   # max length of draft sequence
    pad_token_id=252,               # match your tokenizer's [PAD]
)

class DraftDataset(Dataset):
    def __init__(self, sequences, max_len=32, pad_token_id=252):
        self.max_len = max_len
        self.pad_token_id = pad_token_id
        self.examples = [self.process(seq) for seq in sequences]

    def process(self, token_objs):
        input_ids = [x['token_id'] for x in token_objs]
        position_ids = [x['order'] for x in token_objs]
        team_ids = [x['team'] for x in token_objs]
        type_ids = [int(x['is_pick']) for x in token_objs]

        input_ids += [self.pad_token_id] * (self.max_len - len(input_ids))
        position_ids += [0] * (self.max_len - len(position_ids))
        team_ids += [0] * (self.max_len - len(team_ids))
        type_ids += [0] * (self.max_len - len(type_ids))

        labels = input_ids[1:] + [self.pad_token_id]
        attention_mask = [1 if i != self.pad_token_id else 0 for i in input_ids]
        attention_mask = torch.tensor(attention_mask, dtype=torch.bool)
        return {
            'input_ids': input_ids,
            'labels': labels,
            'position_ids': position_ids,
            'team_ids': team_ids,
            'type_ids': type_ids,
            'attention_mask': attention_mask
        }

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, idx):
        return {k: torch.tensor(v) for k, v in self.examples[idx].items()}

dataset = DraftDataset([tokens])

import torch.nn as nn
class DraftEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.team_embeddings = nn.Embedding(2, config.hidden_size)
        self.type_embeddings = nn.Embedding(2, config.hidden_size)

        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, position_ids, team_ids, type_ids):
        x = (
            self.token_embeddings(input_ids)
            + self.position_embeddings(position_ids)
            + self.team_embeddings(team_ids)
            + self.type_embeddings(type_ids)
        )
        return self.dropout(self.LayerNorm(x))

from transformers.models.bert.modeling_bert import BertModel, BertEncoder, BertPooler
from transformers.modeling_outputs import BaseModelOutputWithPooling
class DraftBertModel(BertModel):
    def __init__(self, config):
        super().__init__(config)
        self.embeddings = DraftEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config) if getattr(config, "add_pooling_layer", False) else None

    def forward(self, input_ids, attention_mask=None, position_ids=None, team_ids=None, type_ids=None):
        # 👇 Inject custom embeddings
        embedding_output = self.embeddings(
            input_ids=input_ids,
            position_ids=position_ids,
            team_ids=team_ids,
            type_ids=type_ids
        )

        # Standard BERT encoder behavior
        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=attention_mask,
        )

        sequence_output = encoder_outputs[0]

        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None



        return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output
        )

dataset[0]

from transformers import BertPreTrainedModel
from transformers.models.bert.modeling_bert import BertOnlyMLMHead

class DraftBertForMaskedLM(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = DraftBertModel(config)
        self.cls = BertOnlyMLMHead(config)
        self.init_weights()

    def forward(self, input_ids, attention_mask=None, position_ids=None, team_ids=None, type_ids=None, labels=None):
        # Pass everything through the model
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            team_ids=team_ids,
            type_ids=type_ids
        )

        sequence_output = outputs.last_hidden_state
        prediction_scores = self.cls(sequence_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

        return {
            "loss": loss,
            "logits": prediction_scores
        }

config.add_pooling_layer = True

model = DraftBertForMaskedLM(config)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./checkpoints",
    save_strategy="no",  # or "epoch", "steps"
    num_train_epochs=50,
    #per_device_train_batch_size=4,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=1,
    #evaluation_strategy="no",
    save_safetensors=False,  # ✅ disables "safe serialization"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,  # must return input_ids, labels, attention_mask, position_ids, team_ids, type_ids
)

trainer.train()